Forecasting. Predicting what's going to happen based on what we already know. This has already
been a focus of our last two lectures and our tool for that exploration has been regression
and the idea that we can model the outputs value as a linear combination of our inputs.
Not all input-output relations are linear, of course, but we've found some clever ways
to apply linear regression to many nonlinear relationships too, using transformation of
variables and polynomial regression. But in a lot of circumstances, our situation is
more specific. We're looking at historical data gathered over time with one observation
for each point in time. Our goal is to use this data to figure out what's going to happen
next, as well as we can. Data of this type is called time series data, and to have any
hope of making progress when predicting a time series, we have to make a rather obvious assumption
that what's gone on in the past is a decent model for what will happen in the future.
To get started, let's look at some historical data on U.S. housing starts, a month-by-month
record of how many new homes had their construction start in each month. Housing starts is generally
considered to be a leading indicator of the economy as a whole.
We'll start by visualizing the data. For a time series, we do this by making a line
graph. The horizontal axis is time, and we connect the dots, where each dot represents
the use at U.S. housing starts for that month. Here's what the data looks like from 1990
through the end of 2000. This is a wonderful data set for introducing some key ideas in
time series analysis. The basic strategy is to decompose the time series into different
collections of different components. Each component will capture one aspect of the historical
behavior of the series, one part of the pattern. So, take a look at this graph. What patterns
catch your eye? If you're like most people, one of the first things that you'll notice
is the up and down bouncing of the data of the series. You can see that this variation
is far from random. Each January, new housing starts tank, then climb rapidly in the spring
months, reaching a peak in the summer. Given the weather patterns in North America, this
makes sense, and would have every reason to expect this kind of variation to continue
into the future. Well, congratulations. You've just identified the first component of our
time series decomposition, the seasonal component. Seasonal components are patterns that repeat
over and over, always with a fixed duration, just like the four seasons. But the period
of repetition doesn't have to be a year. Sunspot activities vary over an 11-year cycle. The
number of babies born in U.S. hospitals shows a strong seasonality of seven days, with about
10% fewer babies being born on weekend days than weekdays. Birth rates drop on holidays,
too, for that matter. Kids being born might not know what a weekend or a holiday is, but
the doctors who schedule the surgical deliveries certainly do. Some people reserve the word
seasonal only for variations with a period of a year or more, but others, like me, apply
the term to any regular variation of fixed duration. Getting a handle on seasonality
is important in two ways. First, if you're looking at making an accurate forecast of
what's going to happen at some point in the future, such as how much electricity you can
expect Chicago to need next January, then you'd better include a seasonal variation
in that forecast. But the second reason that it's important to understand seasonality is
the mirror reverse of this. When trying to make sense of the past, we don't want seasonal
fluctuations to conceal other, more persistent trends. This is certainly the case with our
housing starts, and why the government reports seasonally adjusted measures of growth. Yeah,
we know that this January is going to be worse for housing starts than was the preceding
June, but taking into account that they are a January and a June, which one looks better?
We'll be looking at some ways to do exactly this in a few minutes. But for now, let's
go back to our time series graph and see what else we can find. The other obvious pattern
in the data, once seasonality is accounted for, is that it appears to be a steady increase
in housing starts. In fact, we can apply simple linear regression to this line to see how
well the linear trend fits the data. Like this. Here, x is measured in months, with
x equals 1 being January 1990, x equals 13 is January 1991, and so on. Your first impression
might be that this is a terribly unimpressive performance by linear regression, with r squared
being only about 0.36, about 36% of the variation in housing starts can be late at the doorstep
of the steady passage of time. That leaves 64% unaccounted for. But this is the sort
of thing we'd expect, actually. The data has a very strong seasonal component. The trend
line is going to completely ignore seasonal effects. In the sense of tracking the center
of the data, the regression line actually seems to be doing rather well. For our current
example, the regression line would be the second component of our time series, the trend
component. We've already seen that not all data demonstrates a linear trend, and in
general trend components can actually be quite complicated. We'll take a look at some less
well-behaved examples in a few minutes. There's a third component that arises with
some time series called a cyclic component. As the name suggests, it tracks cyclic variation.
While cyclic variation can be thought of as including regular seasonality as one of its
subtypes, it's clearer to say that cyclic variation refers to longer term fluctuations
that lack the regularity of seasonal variation. Business cycles are a good example, growth,
recovery, recession, recovery, but a variable onset, intensity, and duration. Here's the
U.S. unemployment rate since 1948, and it's hard to miss the repeated peak and trough
nature of the graphs. But while the general character of the cycle is known, their highs
and lows over long stretches of time, their intensity and duration generally vary from
cycle to cycle. Our data for housing starts doesn't show any cyclic variation. In fact,
many short-term and medium-range techniques for forecasting of two years or less don't
include a cyclic component. So we're currently modeling our housing starts as a seasonal
component overlaid on a linear trend. Anything else? Well, yes. Just like in regression,
a time series almost never perfectly matches the real-world data. Whatever variation is
left unexplained is identified as the error component. The error component essentially
consists of all the residuals that we talked about in the regression. The component captures
all of the variation between what the model predicts and what actually happens. When you
do a good job with your forecasting, there shouldn't be any significant pattern to the
error component. Patterns in the error component mean that the errors contain more information.
Information we could have squeezed out of them and included in the other components of the
forecast. Everything I've said so far applies to almost every time series forecasting technique,
but there are a large number of such techniques, and the variety exists for a reason. Different
time series display different characteristics, and the processes that generate them may dictate
the restrictions on the kind of model that we use. Let's look at an example. I'm going
to show you some time series data for the earnings on an initial investment of $100, and I want
you to guess what happens next. In this one, it looks like the earnings get to about 13,
then level out. Best guess for the future? Keeping level. How about this one? After some
original hard times, we see a long trend of growth, although we've had some backslotting
from time to time. Evidently, something good happened around day 85, triggering an earnings
spurt. Best guess? Continued growth. Okay, one more. What do you think? Well, my thought
is stay away from this one. While we've got bumps and wiggles, we see a downward trend
that is, if anything, getting worse. That's pretty easy, wasn't it? Except that in every
case but the first, we've got the trend quite wrong. All three of these datasets were generated
in exactly the same way. Like this. Flip a coin. If it comes up heads, earnings increase
by $1 for that day. If it comes up tails, earnings decrease by $1. The symmetry of the
process means that over any period of time, your chances of gaining a certain amount of
money and losing that same amount of money are exactly the same. That is, on average,
you break even. This kind of process is called a simple random walk. If I understand that
the process driving my observations is such a random walk, there won't be a trend. And
so my forecast model should not include a trend component. A simple random walk like
this is an example of a stationary process and the mean and variance of such a process
don't change over time. In fact, the single best forecast for what happens next in all
three of these pictures is what's called the naive forecast. That simply means that our
forecast for future times repeats whatever the earnings were today. It's like forecasting
the weather in the future as more of the same. Now, in real life, almost no one proposes
the naive forecast seriously, but its simplicity makes it a good benchmark for stationary time
series data. And with difficult data, it can be surprisingly difficult to beat it. Some
people think that the short term fluctuations in the Dow Jones industrial average are essentially
a random walk. So you can understand why predicting the stock market is such a challenge.
This random walk example brings back into focus another idea from simple linear regression,
confidence intervals, and prediction intervals. If the future ups and downs of my earnings
are really nothing more than coin flips, it is absolutely true that the average value
of the earnings at all future points in time across all possible futures would be whatever
they are today. But of course, we're not going to see all possible futures. We're only going
to see one. And a terribly important question is, how far from our predicted value can the
observed value reasonably be expected to go? This is a question for prediction intervals.
For our last random walk example, the naive forecast with the 80% prediction interval would
look like this. The horizontal line shows the naive forecast, and the actual value for
each point in the future has an 80% probability of falling in that gray zone. As you can see,
other than the current value of the earnings, $16 below where we started, the history of
the random walk is irrelevant to the prediction. Still, we can say that it's less than 10%
likely that we'll get back to above negative 3 or so by day 300. The shaded area covers
80% of the possible futures. So there are 10% that fall above the gray zone, and 10%
that fall below it. And that's really about the best we can do with a random walk.
Okay. So not every model includes all the possible components. Some data shows virtually
no seasonality, some virtually no trend. But suppose you have data that includes both.
How do you combine them? Two common approaches are additive and multiplicative models. And
the names are well chosen. In an additive model, you see that the observed value is
the sum of the trend component, the seasonal component, and the error component. This is
good when the seasonal component stays pretty constant over time. If you multiply these
pieces together, instead of adding them, a better choice when the seasonal component's
magnitude varies with the trend, you get a multiplicative model. For example, imagine
that we're tracking revenues generated by businesses in a beachside city in Virginia
that's been growing over time. Well, it expects the revenues to be growing, too. Although
we'd also expect revenues to show a seasonal variation. During the warm months, the city
is going to be raking in money from tourists as well as locals. In the winter, it has to
get by with local trade. So as the city grows, what's going to happen to the size of the
seasonal variation? Well, that depends. If the growth of the city is almost all due to
the growth in year-round population, say, due to a computer firm moving into the area,
then the dollar variation in revenue due to tourism won't vary much as the years pass.
In this case, we'd want to use an additive model. The seasonal component adds about the
same amount of business each summer, year after year. On the other hand, if the growth
were spread across sectors of the city's economy all over, we'd want a multiplicative model.
Here, a 10% increase in overall revenues for the city would also show a 10% increase in
the size of the seasonal fluctuations. A multiplier of 1 is the base, and a multiplier of 1.3 for
July would mean that revenues in July are 30% higher than the base level.
Let's return to the housing starts data and apply this new information. The size of the
seasonal variation is just about constant, even though the average number of starts per
month has increased by about 50% over the 10 years, so we want to use an additive model.
Our forecast at any point in time is going to be the sum of the trend component and the
seasonal component. Any deviation between the predicted and the actual values will be captured
by the error component. Here is maybe the simplest way to make this forecast, taking advantage
of the fact that our data seemed to be displaying a linear trend. First, we use linear regression
to fit a straight line to our data. We already did this a moment ago. Now, subtract the linear
regression prediction from the observed data values at each point in time. Numerically,
this is easy to do since we have an equation for the regression line and historical data
values. Graphically, we're just removing the trend and focusing on how the data wiggles
up and down around the trend line. The result, the detrended data, looks like this.
Okay, now we'll capture the seasonal variation. The simplest way to do this is to come up
with one seasonal index for every month of our forecast. For example, we can see on the
seasonal graph that January is consistently low, about negative 30, so the seasonal index
for January is going to be about negative 30. To be more precise, we're going to average
the values of all the January observations on this detrended graph and use the results
as the seasonal adjustment or seasonal index for January. It comes out to be negative 29.2,
meaning that the average January is about 29,200 houses below the regression line that
we just found. May ends up being the busiest month with a seasonal index of 17.9. There
are an average 17,900 more housing starts in May than you'd guess just by looking at
the regression equation. When you put all these seasonal indices together, you've created
the seasonal component of our forecast. Here it is, superimposed on our last graph, the
detrended data series. The blue wiggle shows the actual historic data moving up and down
around the trend line. The red wiggle is our seasonal component, repeating the same pattern
over and over. The dip that you see on the red line at each January is the seasonal index
of negative 29.2 because the average of all of the historical blue line Januaries was
negative 29.2. Because it repeats over and over, this seasonal forecast can easily be
extended into the future. The seasonal component for every January is the same, for every February
is the same, and so on. This is the simplest kind of seasonal forecast to do. And as you
can see, it does a pretty good job. But not a perfect one. We can see the discrepancy
by looking at the vertical separation between the blue line and the red line, blue minus
red. That'll be the error component. The stuff that our model doesn't explain. Here
it is, in a line graph of its own. And what we're seeing here is pretty much what we're
supposed to see. The first couple of months have noticeably high errors. Housing starts
were consistently higher than we would have expected. But beyond that point, there seems
to be no pattern to our errors. They hover around zero, zigging and zagging in what appears
to be a random walk. And happily, they tend to be small. This suggests that we've gotten
out of our data about as much as we can. So, let's see how well the forecast actually
did when compared to the real data. I'm going to construct our forecast with the additive
model we discussed, a linear trend modified by adding a seasonal component. Here's the
forecast, superimposed on the actual data. And you can see how well it did in the 90s.
And you can also see what it predicts for the next five years. Based on how well our
forecast has done so far, I'm feeling pretty good about the reliability of our prediction.
Not surprisingly, people who do forecasting like this for a living want to be a bit more
quantitative when assessing the performance of a forecast. One common measure is called
the mean absolute deviation, or MAD. Start with finding out how much each forecast value
differed from the actual historical value, the error for that point in time. If it's
negative, take its absolute value to make it positive, so an error of plus five or minus
five still counts as an absolute error of five. Finally, average all these absolute errors.
For our housing starts data, the MAD turns out to be about 7.3, or 7,300 houses. That
is, sometimes our prediction is too high, sometimes it's too low, but on average, we
miss the mark by 7,300 houses during the 11 years for which we have data. Coming to a
cold, learning that the MAD is 7,300 houses might not be as informative as you'd like.
Is that a lot or a little? Because of this, people often also report the MAPE, or mean
absolute percentage error. To do this, find the percent that each forecast was wrong,
take the absolute value of these in case they're negative, then average all the percents. For
our data, it comes out to be 6.8%. Not bad. Again, sometimes we're too high, sometimes
we're too low, but on average, we miss the actual housing starts by 6.8%. MAD and MAPE
are both particularly popular in the manufacturing sector. But the most common way to characterize
the fit of a forecast should sound quite familiar to you from our discussion of regression.
That's MSE, mean squared error. That is, take the error for each observation, square each
of those errors, then average all the squared errors together. That's MSE.
We talked about the advantages of MSE when we discussed regression. It has much nicer
statistical and calculus properties than do MAD or MAPE. It falls short though when you
try to interpret it simply. I gave you a nice intuitive interpretation of what an MAD of
7.3 and an MAPE of 6.8 mean. It's much harder to explain what MSE of our housing forecast
actually means. It's 88.7 million square houses. Yes, I said square houses. The error was in
houses and we squared that before we averaged. Obviously, you can't compare these different
measures of forecast quality to one another. It's that's comparing apples to oranges or
apples squared. What you can do is to compare two different forecasts using the same measure
of quality. The one with the lower MAD or MAPE or MSE should be the better forecast.
Will all the different measures always agree on the winner? They often do, but sadly, no.
MSE tends to care about the consistency of a forecast. If you're spot on a lot of the
time, but occasionally make howling errors, MSE gives you a big penalty for the mistakes.
That is, its squaring step turns a very bad error into a very, very bad, bad error. MAD
and MAPE are more forgiving with the occasional howler. In many applications, people would
prefer a lot of small errors to the occasional large one, which is one of the non-computational
reasons why MSE is often the preferred measure. I'd rather have my budget for my business
off by $5,000 per month every month than to have it be exactly correct in January through
November and $60,000 off in December. MAD says that both of these forecasts are equally
good. MSE says that the end of the year surprise is 12 times worse than the small consistent
errors. But no matter which measure you use, if you look more closely at how we assess
the quality of the forecast, you might notice I've played a bit of a shell game. The calculations
don't talk about what's going to happen after the historic data I've got. They talk about
what's already happened. In a sense, I was forecasting the past. I'm deciding how good
my model is at how well it fit the historic data when I use that data to make the forecast
to begin with. Isn't that kind of like forecasting yesterday's weather? Well, yes. And I think
no. I applied my forecasting techniques to model the 132 months of historical data that
I had available. That's what I used to assess its quality to compute the MSE and so on.
The model itself had a linear trend component and a seasonal component. To specify this
model, I had to come up with the value of 14 quantities, the slope of the regression
line, the intercept of that line, and 12 seasonal indices for the 12 months. Actually, since
the seasonal indices will add to zero when a problem like this, I only needed to specify
11 of them. So, 13 numbers total, what are often called model parameters, had to be found.
At the end of our discussion of polynomial regression, we saw the danger of overfitting
the data, which really means using a model with too many parameters to explain the set
with too little data. But here our model has 13 parameters, and we're using it to explain
the data set more than 10 times that size, 132 months. So the fact that we got a very
good forecast over that entire range gives us some confidence in its quality. Still,
the proof really is in how well the forecast does on new data. What happens after 2000?
Well, we know our forecast out through the end of 2005. How well did it do in predicting
the actual events of the early 21st century? Take a look. The model characterized the patterns
of the past, extended them into the future, and the result was, I think, a very nice forecast,
indeed. Unfortunately, not every time a series is so well behaved. In fact, even this series
has some surprises for us. Let's look at housing starts over a larger time horizon,
from 1959 to 2013. Here's that bigger picture. The part in green is the part we've already
been discussing, and you can see that once you move past 2005, the forecast that we made
has a lot of problems. The huge decline in housing starts that began in 2006 and persisted
for the next two years for starters. This is one of the dangers that you face when you're
working with historical data alone. It lacks a structural model of what the factors are
that are influencing the evolution of the data outputs. Time series forecast is based
on the idea that the past is a good model for the future. When something fundamental
about that situation changes, if you can't anticipate it, you can be left with forecasts
that are really quite dreadful. Extending our graph over a much longer number of years
shows that the business cycle or other cyclic variation can turn out to be important even
for short-term forecasts. One reason the business cycle is often ignored is that economists
find it so hard to specify precisely. But just by inspecting the extended graph, we
can see that housing starts peaked and turned downward three times, in 1973, 1978, and 1986.
And we can see that the level of those peaks together define a fairly narrow horizontal
zone, or we might have begun to anticipate a possible cyclic downturn, maybe lower,
as in the 1986 level, or maybe higher, at the 1973 level. Clearly, the better we understand
the past, the better chance we have of predicting the future, and sometimes, such as in economic
analysis. We're also trying to make sense of the past for its own sake. But how do we
get a trend? How does something like this? A common way of characterizing the trend is
by using a simple moving average. Simply put, we peg the trend at some point in time by averaging
together some number of observations near that point in time. If you're trying to forecast
a future with monthly data, you might average together the 12 most recent months to get
the forecast of what happens next. If you're trying to make sense of historic data, you
might choose a set of observations centered on your current month to average.
Here's an example for our housing data. It's a simple moving average forecast with n equals
12, meaning that the preceding 12 observations are used to predict the new value. The actual
data is in blue, the forecast is in red. And the forecast doesn't do a very good job of
capturing the details of the actual events. Its most obvious fault is that it ignores
seasonality. And it should. Remember how we got this forecast? We predicted starts in
any given month as being the average of the starts over the preceding 12 months. By averaging
over the preceding years, we wiped out any seasonality effects from our forecast. And
really, that was the idea. If we're interested in how the market for housing starts was doing
over a span of years, we don't want to focus on the seasonal variation that we know is
going to be present. That's why the government always uses de-seasonalized data when viewing
housing starts as a barometer for the economy. They also control for the number of so-called
trading days in each month. If we're trying to do a trend seasonal decomposition of data,
the red line gives us the trend line that we could use for our trend component. And on
top of this, this component, we could put a seasonal variation, computed as we did in
our original example. There's one more problem, though. Since each simple moving average forecast
is just the average of the preceding 12 months, we don't get a forecast until 12 months have
gone by. More importantly, we can only use the technique to forecast one month in advance.
If we want to go further than that, and we probably do, we need more advanced techniques.
And a plethora of them exist, each suited to different kinds of series. Some are simple.
Weighted moving average takes the simple moving average that we just did, but gives each observation
a different weight. These weights tell us the relative importance of the values that
are used in computing the forecast. If what happened one time period ago has twice as
much influence as what happened two time periods ago, then the weights would reflect this.
W1 would be twice as big as W2. In a 12-month weighted moving average, that would be weights
W1 through W12. W1 isn't the weight for January. It's the weight for whatever happened one
month ago. So W12 tells us the relative importance of what happened a year ago. With data that's
strongly seasonal like ours, W12 would probably be the largest weight. That's because what
will happen this month is probably most strongly tied to what happened at the same month last
year. If the data isn't seasonal, then what happens this month might be most strongly
tied to whatever happened last month. In that case, W1 would be the largest, and the weights
would grow smaller as we move backward in time from the time which we're making the
forecast. Of course, you'd have to find the weights that make the forecast fit the historical
data as well as possible. That is, you'd like to find the values of the weights that
minimize some measurement of error, like the MSE in the forecast. This is an example of
an optimization problem since we're trying to find the best weights for our model. The
middle section of this course is going to be dedicated to setting up and solving optimization
problems like this. By the time we get there, you'll be finding the best weights is going
to be a piece of cake. It's worth noting though, something that we'll see again and
again in this course, the synergy among the different analytic techniques. All our tools
fit together. Time series forecasting, for example, might use linear regression for the
trend component, or might need nonlinear optimization for finding the weights of the weighted moving
average. Here's a close relative of the weighted moving average called simple exponential smoothing.
You can think of it as a weighted moving average in which the weights grow smaller and smaller
in a geometric fashion as we move back in time. For example, consecutive weights might differ
by a factor of 2. W1 is a half, W2 is a quarter, W3 is an eighth, and so on. The shrinkage
rate is generally conveyed by giving the forecast's smoothing constant, which is called alpha.
If each weight is 90% of the weight before it, the value of alpha is 1 minus 90%, or
0.1. Exponential smoothing is an extremely simple forecasting technique, but it's the
basis for a lot more of more sophisticated and complicated approaches. It can be surprising
that time series works so well. In fact, even when the data does not show noticeable seasonal
or trend components, time series can still be useful in two regards. First, you can look
for near term correlations between the residuals of a forecast in an identical series of residuals
to space displaced one or more periods in time, what are called the lagged residuals.
Such correlations are called auto-correlations, and are quite common in many time series.
On one hand, auto-correlation has an important impact on confidence and prediction intervals
obtained from the forecast. It will generally lead to intervals that are narrower than they
should be. That means that auto-correlation in the data can give us excessive confidence
in our forecast. Sometimes this problem can be addressed by
differencing, creating a new data set that tracks not the value of the quantity of our
time, but it's changed from one time period to the next. Moreover, a series may show for
example that the residual in a forecast at one time period tends to be strongly linked
to the residual three time periods before. Such information can be used with more complicated
techniques to make a better forecast. Second, time series techniques like the moving
average and exponential smoothing techniques can remove high frequency variations in the
data and allow longer term drifts to become evident. A business cycle, for example, such
as we found in the data for housing starts, might be noticed first by looking at a smooth
curve. Sometimes we're lucky. Sometimes we know enough about the mechanism and process
that we're able to build a structural model of it, a mathematical representation of how
it interacts with the outside world and aspects of itself. In such cases, tools such as optimization
and simulation can give us insights into what we might expect from the process, including
behavior we've never yet observed. But often we're not that lucky. Sometimes all we have
to go on is historical data and very little insight into the mechanisms that actually
drive the process underlying it. In these cases, time series forecasting can often fill the
gap. It's especially good for forecasting short term economic demand for a product or
service such as airline seats, electricity or water usage. So long as we can trust the
assumption that the past is a good indicator of the future, time series forecasting can
be a powerful tool for quantifying what that future may hold.
