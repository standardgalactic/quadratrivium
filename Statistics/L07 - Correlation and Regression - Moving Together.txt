Welcome back to Meaning From Data, Statistics Made Clear.
As you recall, we're in the midst of a four lecture sequence about how to describe, summarize,
and organize data if we have all the data in the population with which we're interested.
In this lecture, we're going to talk about a fundamental strategy with which we attempt
to understand our world, and that is by identifying examples of cause and effect, and other cases
where two varying quantities are related to each other.
When this comes up, literally all the time in physics, you hit a ball and it travels
in a certain predictable trajectory.
If you put a hot object in the room, it's going to cool at a certain rate.
If you bend a lens at a certain way, it's going to focus the image at a certain distance.
So those are examples in physics of correlations between one thing and another.
In everyday life, social situations, you have lots of examples of cause and effect.
If a student studies more, that student may possibly do better on the next test.
Income of a person is related to the amount of education that they have.
But you could certainly ask the question of whether the increase in the income is a result
of the knowledge or skills they gained from education.
When seeking a cause and effect relationship, maybe the education actually allows the person
to be more productive, actually able to do things that allow them to make money.
But maybe it's really other factors that demonstrate that correlation that has nothing
to do with the actual acquired skills of education.
So in any case, in all of these examples, whether it really is cause and effect or just
moving together, what's involved is a pair of numbers or qualities that each member of
the population has that we're trying to compare.
If a cause and effect relationship actually is operating, then the two features or the
numbers that describe the features will move together.
The manifestation of cause and effect will often be a list of pairs of numbers.
The first numbers will have some relation, the first and the pairs will have some relation
to the second number in the pairs.
So in this lecture, what we're going to do is try to identify and describe the relationships
of pairs of numbers.
And statistics is a tool which we're going to try to develop.
We're going to, in this lecture, develop concepts by which we can look at a list of pairs of
numbers and decide whether or not there really is some relationship to how those numbers
are moving.
So there may be a relationship, by the way, whether or not there is a direct causal reason
for one thing to be moving in a way similar to the other.
And it can often be a real challenge to distinguish between a cause and effect relationship or
just another kind of correspondence that were actually caused is not involved.
I'll give you an example.
There's certainly a correspondence between SAT scores of incoming college freshmen and
grade point averages.
But it's not true that getting a higher score on one's SAT actually causes the student to
get a better grade.
In other words, the grade is not computed by saying, OK, what was your SAT score?
Therefore, you get a better grade.
So it doesn't cause the higher grade, but nevertheless, a person with SAT scores higher
often gets higher grade point averages.
So the two quantities exhibit a correlation, meaning that they move together, but neither
one actually causes the other.
So a goal of statistical analysis is to see whether two quantitative qualities of members
of a population are correlated.
That is, move together.
And we should say now that the question of causation is really outside the realm of statistics.
Statistics results certainly can be induced to bolster an argument about causation.
Because if you are claiming there's causation, you should be able to find evidence that has
two things that are moving in step.
But statistics job is to measure to what extent they're moving together, not to say
whether or not one is causing the other.
That's a different question.
So the first step that we want to take in discussing these pairs of numbers and seeing
whether they move together is a visual one.
Let's see how we can represent pairs of numbers in an effective visual way.
So here is a visual representation of such a phenomenon, namely this is a description,
a visual representation of data about a bunch of students.
Each student is represented by a dot, and the dot occurs, is placed for each student
by looking at the SAT score of that student, and then moving up on the line to where the
grade point average of the student is in college.
So this is the SAT score, which was a test taken before college started, and this is
the college grade point average after the student has been in college.
So this is sort of a cloud of dots.
And you can see that this cloud of dots generally moves up as you move to the right.
In other words, higher SAT scores seem to be associated with higher grade point averages.
But certainly the dots don't lie on a perfect line by any means, as you can see, because
not every student with a particular SAT score is going to get that particular grade, and
getting a higher SAT score doesn't guarantee, as you see, doesn't guarantee a higher grade.
See, this person got a 1200 SAT, but has a 1.8 grade point average, whereas this one
had less than 1000 SAT, this person right here, and has a 3.5 grade point average.
So it's not at all a perfect correlation.
We're going to try to quantify the extent to which these two variables are related by
giving a number, defining a number that's called the correlation, that summarizes the
extent to which these two varying quantities, in this case SAT and grade point average in
college, are moving together.
When one quantity increases, we want the other quantity to tend to increase.
That is, if when you increase the SAT, you increase the grade point average, that's called
positive correlation.
When the cloud goes up to the right, if you have the opposite, that is, when you increase
one value, the other one goes down, then that's called negative correlation.
So SAT scores and grade point averages, they are examples of a positive correlation.
An example of a negative correlation would be that if you look at the amount of, say,
the number of cigarettes a person smokes versus their life expectancy, well, that would be
a negative correlation.
The more cigarettes, the lower life expectancy, so it would be a downward cloud.
So a scatterplot of, is the first thing that we're looking at, and it gives a visual representation
of the concept that we're trying to get at, whether the cloud is moving together.
Well, we're going to next think about trying to develop a formula for correlation, and
the way that we are going to develop this formula is to first look at the two varying
quantities individually.
In our example, we'll look at the SAT scores individually, and then we'll look at the grade
point averages individually.
And by looking at them individually, we can do things that we've done in the previous
lectures.
We can compute the mean of the SAT scores, and we can compute the standard deviation
of the SAT scores of this collection of students.
And for example, let's just look at our example here.
In this example, the mean of just the SAT scores, that is how these scores here, the
mean of those scores turns out to be 1,070.
That's the mean.
And the standard deviation is 163.
So remember, the standard deviation is some measure of how, on average, how far apart
a typical score is from the mean of the whole population.
So there are some students who have a score that say one standard deviation above the mean,
and that would be the sum of the mean plus the standard deviation of 1233.
So a person who got a SAT score of 1233 would lie somewhere on this line here, be a dot
above this line.
Now the grade point average can be looked at individually also.
So the grade point average has a mean of 2.6, and its standard deviation is 0.78.
So the mean grade point average is something, 2.6, standard deviation, the amount of spread
in the grades just viewed by themselves is 0.78.
So the strategy by which we think about this concept of correlation is really to ask a
question about whether the student's grade point average is how closely it's associated
with the SATs, and it's measured in standard deviations away from the mean.
In other words, you see, if you have a student whose grade point average, whose SAT scores
was one standard deviation from above the mean of the SAT scores, you're going to ask the
question is the student's grade point average one standard deviation above the mean of the
grade point averages.
In other words, in this particular case, where if you had a student with an SAT score,
one standard deviation above the mean, we saw before that that student would have scored
1233.
Of course, it wasn't really 1233 because the SAT scores just always end in zero.
But if you had a student in that range of about 1233 SAT score, then you're asking the question
would that person's grade point average also be one standard deviation above the mean grade
point average, which would be a 3.38?
And if so, then we're having a correlation between the two because they're distance above
the mean measured by standard deviations above the mean, which you may recall was called
the Z score of that datum, how many standard deviations away from the mean it was.
That's the way we measure the units in comparing, in this case, SAT scores with grade point
average because you see they're on different scales.
The numbers aren't the same, SAT scores are a thousand, and GPAs are some number between
zero and four.
So that's the way to make sure that the units can be compared.
Well, we'll say that a data set is perfectly correlated if the distance from the mean measured
in standard deviations of the one variable corresponds exactly to the number of standard
deviations above the mean of the other variable.
So let's give some examples now.
So a perfect positive correlation then would draw a figure where the scatter plot really
consisted of an exact straight line, all the dots lay on a straight line.
That is a perfect positive correlation.
A perfect negative correlation would occur when all of the dots lie on a straight line
but heading down instead of up.
So this would be a case that we could rephrase by saying that the Z score for this variable
is always the negative of the Z score for the vertical variable.
Now let's look at the definition of correlation and to try to get a numerical measure for
how it is we're going to compare the two varying quantities, the first and the second of these
paired variables.
Now this once again has a sort of a formidable look to it but let me just walk you through
it and see what it is that all these symbols mean.
The X's refer to the quantities that are changing in the case of the horizontal first component
of these paired variables.
So the X's in our previous example would be the SAT scores, individual SAT scores.
And X bar you may recall was the mean of the whole set of in this case SAT scores.
And then S sub X is the standard deviation of the horizontal value.
So this S sub X is the standard deviation and you can see that actually when you take
the difference between a value and the mean and divide by the standard deviation all that's
telling you is how many standard deviations away from the mean you're getting.
So if this difference for example were one standard deviation then dividing by then the
difference would be equal to one standard deviation and so when you divide by the standard
deviation you'd get one.
So this is really just the way of saying that this is what we called before the Z score,
the number of standard deviation deviations away from the mean.
Likewise, the second fraction here is measuring exactly the same thing for the second component
number of this pair, the number of standard deviations above the mean for the vertical
axis collection of data.
You multiply these two things together, the Z scores from each, and add them up, remember
this capital sigma just means add up all of the values that we get going from throughout
all of the data.
And then we take sort of an average, it would be an exact mean if we divided by n because
you have n data points, instead we divide by n minus 1, this is slightly technical reason
why we divide by n minus 1 instead of n.
The answer, morally speaking, this is just the mean of the, it's an average of the mean
of this product and this is traditionally called R and this is the definition of the
technical definition in statistics of the word correlation.
So this is the correlation of two data sets X and Y and this is, these down here are just
reminders of the definition of the standard deviations for the two data sets.
Now, let's do a specific example so that we can measure what this correlation, what kind
of a number it gives us.
So let me do an example.
So here's an example.
We have, in this case, our example has four points to it, here they are, and these are
ostrich eggs versus ostrich hens in a ostrich farm, you see four ostrich farms.
And this ostrich farm has three hens and that hen laid a hundred and some eggs, just a little
over a hundred eggs.
And this farm had five hens and those five hens had close to 200 eggs.
This farm had six hens and had a little over 200 eggs.
This farm had seven hens and had 300 and some eggs.
So these are ostrich eggs, you probably didn't realize that you would learn about ostriches
in this lecture.
And by the way, I learned that ostriches have on average about 35 eggs per year and that's
something you may not have known.
Some people don't know that and seem to get along perfectly well without knowing that
apparently.
So here are the different farms.
We have the number of hens in each farm and then down here we have the number of eggs
that each one produced.
And we deal then, again, the farms individually, that is the number of eggs per farm, we have
a mean of 5.25 hens per farm.
And then here we have the mean of 209 eggs per farm.
And so we do the calculation associated with finding this correlation, which means subtracting
each number from the mean.
So we have 3 minus 5.25 gives negative 2.25.
Back here, I'll go back to our formula, which you see the first thing you do is you subtract
for each value, you subtract the mean.
So we subtract, we get these values, we get the difference in the y's from the mean, we
find the z scores, these are the z scores for each one.
And when we take the z scores, we multiply the z scores together, we add them together,
we divide by n minus 1, remember 3, we're dividing by n minus 1, we have a total of 4 farms,
so we divide by 3, and we get 0.92.
So the correlation of this data set then is 0.92, which means geometrically that the
data lie pretty close to a line, that we can draw a straight line and the data points are
pretty close to that line.
And the higher correlation closer to 1 is corresponding to a higher data point.
So the first thing to know is that correlation always produces a number between 1 and minus
1.
All that complicated stuff, you know, you take all those z scores and multiply them together
and you add them and then you divide by the thing.
It turns out it always gives a number between 1 and minus 1.
It's not obvious why it does it, but it is true.
And if there's an exact correlation between the two things, either positive or negative,
then the correlation will be plus 1 or minus 1.
It's plus 1 if the data lie on an exact straight line.
If the correlation is minus 1, if the data lie exactly on a straight line that's descending.
So those are the two extreme cases.
And now let's look at some examples where we have a correlation that's between the two.
Now here's an example that I found interesting.
This is an example that is comparing in a population of adult men in the United States.
It is comparing the weight of the men on this axis with their thigh circumference in this
axis.
Now the reason I find this sort of interesting is because, and by the way, so let me first
say that this, you can see that there's a strong correlation between these two measures.
In other words, as for heavier men, their thigh circumference seems to be increasing
at a almost along a straight line here.
So instead of weighing yourself, what you could do is just measure your thigh, and you
get a pretty good estimate for how much person weighs.
The reason I'm sort of surprised at this is that it seemed to me that the thigh, since
it's just the circumference, you'd think that there would be like a cubing issue involved.
That if you increase your weight, your thigh by a little bit, that really increases your
radius of your thigh, and you think you might say to square maybe your weight instead.
But these are the facts.
We cannot dispute them.
So the correlation here is 0.87, meaning that these data do lie more or less close to a
straight line.
Let's look at some other examples.
Here's one where the correlation is not as strong.
It does seem to be going generally up to the right, but you can see that no line that you
chose gets the points clustered very close to that line.
Here's an example where the correlation is a negative correlation.
You can see that as the scatter plot tends to have the points going down to the right,
and if you draw any line in there, they're quite close to the line, but not exactly on
it.
Here's an example that was obtained by simply taking random numbers.
So we let the computer simply take random numbers and plotted them, and it has a correlation
very close to 0.
So a 0 correlation just means something where there's no real predictive value of one of
the components, the first horizontal value of each pair, with the second.
They just sort of are random, and then you can see that.
The measure of correlation is the technical definition of correlation as all mathematical
and scientific terms has a very specific meaning.
Often mathematics or science attempts to capture ordinary experience.
In this case, we're trying to capture the idea that two things are moving together,
but then we made a technical definition that talked about that, and the technical definition,
to some extent, captures our human interest in that word, in that concept, but to some
extent it doesn't.
Once you've made the technical definition, you have something whose meaning you must
then cope with in its own terms.
So here's an example where the English meaning of the term may differ from what you may not
feel this was a good thing to call, in this case, correlation 0.
So let's look at this example.
This is an example that has exactly six data points.
Five of them line up completely perfectly, 1, 1, 2, 2, 3, 3, 4, 4, and 5, 5.
These are exactly on a straight line, perfectly correlated, but then there's just one more
data point.
Here it is.
Now the correlation of these data turn out to be exactly zero.
In other words, if you plug those numbers into the formula for correlation, you get zero.
Now do we, in the human way, want to say that those two things have correlation zero?
Well in English, in regular life, you probably would say, look, there's a lot of connection
between those data, because for five out of the six of them, there was a very, very close
connection.
They were identical to each other.
So this is a cautionary note to realize that there's nothing magic about mathematics, there's
nothing magic about statistics.
The reason for defining these concepts is to capture some idea, but it may or may not
capture the idea that you want perfectly, and then you have to use judgment when you
apply the idea.
There's a wonderful example about correlation that is very interesting, particularly for
a person of my age and gender.
In 1969, the Vietnam War was raging, and there was a draft lottery.
The lottery was conducted in the following way.
It was conducted in 1969 to determine who would be drafted into military service in
the next year, 1970, and the way it worked was very simple.
There were capsules made for every day of the year, every possible birthday, January
1st, January 2nd, January 3rd.
These balls were put into a container.
It was shaken around, and then somebody reached in and took out the capsules one at a time.
And the first one that came out, whatever the date was on that capsule, that person
with that birthday would be drafted first, be susceptible first to the draft.
And then they reached in.
The second one was chosen.
You follow me?
So the idea was a way to try to randomize the order in which people would be drafted
based on their birth dates.
Well, here are the data.
The data looked very random, as you would expect.
Now, so let me explain what this means.
On the horizontal axis, it's the birth date.
That is the day of the year, the number day of the year.
On the y-axis here, it's the order in which it was selected from the jar.
You follow me?
It looks like it's a random collection, but if you put it into the formula, you discover
that the correlation is actually negative 0.23.
And negative 0.23 turns out to be an answer that is statistically unlikely to happen by
if it were truly random.
It's unlikely to happen by true randomness.
So this is an example where, in fact, probably the balls were not actually mixed up sufficiently.
That the first ones were put in first, and when they were taken out, the later ones in
the year tended to be taken out earlier.
So this idea of correlation had a huge effect on a lot of people.
And here you can see some examples that September 14th was the first day chosen.
On my birthday, July 10th was the 284th birthday, so I did not go to Vietnam.
We've drawn in all of our examples, including this example of the Vietnam War, we drew a
line that sort of captured how the data were moving.
That line that we're drawing is called a regression line.
It's a line in which it, in a sense, most closely approximates the direction of the data.
And the way it works is this.
For every data point, every dot, you look at the distance to the vertical distance to
this approximation line.
And you choose the line such that the sum of the squares of those distances is minimized.
And that's called the least squares regression line.
So there's a unique straight line that minimizes that sum of the distance of the sum of the
squares.
The distance between a data point, the vertical distance between the data point and the line
is called the residual because it's the extra amount, the part that is different from your
summary line, the regression line.
A regression line summarizes the data with one line.
Sometimes you have more variables than just one explanatory variable.
Instead of just the grade point average in the SAT scores to predict grade point averages
in college, you might have other qualities about a student.
You know their SAT score, but you also know their high school grade point average.
So you could take both of those things into account and then use that to compare it to
the grade point average.
And having more information about the incoming student, those are called explanatory variables
to explain a response variable, the grade point average in college.
And we can make a really attractive graph of this using a way of capturing what really
is a three-dimensional object in the plane.
Namely, we have the scatter plot as before with SAT scores on this axis and the high
school grade point average this time on this axis.
And now instead of bringing the third dimension out, which conceptually it actually is, we
just color the higher grade point averages with the reds and the lower grade point averages
with the blues.
And you can therefore this picture, colorful picture, has to be in color, shows you a way
of talking about what's called multiple regression, where you have more than one explanatory variable
that is explaining a response variable.
Well, in this lecture then, we have discussed one of the fundamental ways of trying to understand
our world, which is looking for the consequences of cause and effect.
We looked at statistical correlation of two variables, and then we looked at this regression
line, the least squares regression line, which was a way to summarize the relationship between
two varying quantities.
The statistical concept of correlation helps us to identify and quantify the extent to
which paired qualities of members of a population are varying together.
Correlation does not indicate cause and effect, but it does indicate an association.
And one of the, by the way, the prime misuses of statistical information is to mistakenly
infer cause and effect from correlation.
I'll look forward to seeing you next time when we'll talk about probability.
