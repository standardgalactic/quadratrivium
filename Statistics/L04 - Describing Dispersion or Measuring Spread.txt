Welcome back to Meaning From Data, Statistics Made Clear.
In this lecture, we begin a series of four lectures concerning how we extract meaning
from data when we know all the data of the population that we're interested in.
And then in later lectures, 8-12, we'll consider the question of how to infer information about
the whole set of the population when we just have information about some of the members
of the population, a sample.
So to describe a set of data, we have to confront the challenge of taking a list of numbers
and putting some structure on them through which we can garner meaning.
So as we begin in this lecture, we're going to take the principles for organizing, describing,
and summarizing data, and when we have all the data in the whole population, and figuring
out how to look at issues of it.
And one of the basic features of a set of data that we'd like to describe is how closely
the numbers in that set of data are clustered together or how widely they're spread apart,
how dispersed they are.
And this measure of the variability of the data is going to be accomplished by developing
a few different concepts of dispersion or spread.
And so in this lecture, the goal of this lecture is to define and explore the standard
deviation.
So we'll define that later in the lecture, and that's a measure of how widely data from
a data set are dispersed.
So the various methods of what we'll start by, first of all, when we're talking about
dispersion, we're talking about dispersion from something, and that's a central value.
And so what we're going to do is talk first about some of the measures of central tendency
that we've already, in fact, met before.
So the first thing that we're going to do is review the two one number summaries of data
that we already met in lecture two, namely the mean and the median.
The mean and the median, although they both measure central tendency, they're both one
number summaries, notice that neither one of them speaks to the issue of how dispersed
the data are.
So let's, first of all, review the mean and the median.
The median, remember, is the number that you get if you list all of the data and you take
the number that's halfway up that list.
So that's the median.
And it represents, in some sense, the data by a single number.
And notice that the median is not affected by outliers, outliers meaning values that are
really extreme, because if you have a list of numbers, and if this number is in the center,
well if you change the biggest number to be much, much, much, much bigger, it doesn't affect
that number at all.
So the median is not affected by outliers at all.
But the mean, on the other hand, is another measure of center, and it's the most common
one, at least when you think of the word average, you probably are thinking of the mean.
The mean, remember, is the sum of the data divided by the number of elements in the data.
So here is the actual equation for the mean.
And it's often represented, by the way, by x with a bar over it, or by the Greek letter
mu, depending on the context which we'll talk about later.
So the x bar, which is the mean, is the sum of all of the data divided by the number of
data points that you have.
So if you have salary data about how much salary each of 100 people makes in a company,
you would add up those 100 salary numbers and divide by 100 to get the mean.
And there's a fancy notation for this, which is this capital letter sigma, i going from
1 to n of x of i, the sigma being the letter s really stands for sum.
It means add up.
So it adds up, i going from 1 to n means that you take this little letter i and make it
a 1, then a 2, then a 3, all the way up to n.
So if we had the salaries, the salaries would be i going from 1 to 100 of x of 1, x of i,
and then that would be shorthand for this equation right here.
So that's just a little notational thing, and don't worry about it.
But the mean has a physical property that is rather interesting.
And that can be described by putting a histogram of the data and then taking this physical
action, which I'll describe.
Here's a histogram of a collection of salaries from a given company.
And you can see that the histogram, the heights of each of these bars indicate how many of
the people make a salary between, in this case, $60,000 and $70,000.
And so this is two people make that salary or three and so on.
So this is a histogram of the salary data.
The mean is the place that you would put a fulcrum if you thought of this as a teeter
trotter.
So the mean is the place at which the data balance.
You see?
So the data balance, in this case, the mean is $44,000.
So if we put the fulcrum right here, the data would balance, OK?
Now notice that if you have a teeter trotter, I mean in real life you have a teeter trotter.
And by the way, I have to say specifically that the teeter trotter that we're thinking
of doesn't weigh anything itself.
That is, the board doesn't weigh anything.
And that what we're really doing is just putting one, say, coin at each place where
each data, each datum is.
And you just lay it on the board that is marked by the numbers.
And so you could have a stack of three if three of them have the same value here and
a stack of two.
Well notice from your own experience of teeter trotters that if you put the little kid way
out on the teeter trotter, it'll balance a heavier person that's nearby.
The effect of that is that that's a physical manifestation of the fact that the mean is
susceptible to being strongly influenced by outliers.
If you put a high number in, there's a big effect on the mean.
So this, by the way, is a histogram of exactly the same data set that we had before.
And that's interesting just in its own right.
Notice that the only thing that's happened is that the x-axis here, the horizontal axis
has changed where this is now $100,000 right here.
So all the data are crammed together.
So here we go.
Here I went back.
There are the same data.
This one goes from $10,000 up to $90,000.
And then in this one, it's exactly the same thing except they're all crammed together.
The reason I want to do this is I want to illustrate an outlier by adding one more value
way out here.
You see this?
Million-dollar salary.
The way to think of this, by the way, is that this might be a company and most people
make amounts of money that have a range between, in this case, the $20,000 and $90,000.
But then we've got the CEO of a company who's making a million dollars.
So this is an example where the effect of that one salary, including that CEO salary,
the effect is that the mean now is $64,000.
Whereas without the salary of the CEO, it was only $44,000.
So that one salary change made a big impact on the mean.
So the mean, then, the way this is phrased is that the mean is susceptible to the influence
of outliers.
So a single number summary, in order to decide what single number summary that we want to
use, if any, we need to see its properties.
And one thing that we've seen is that one of the properties is that outliers have a
big effect on the mean, but don't have a big effect on the median, and that can influence
it.
And so let me give some examples.
A single number summary, it would not be useful in the following situation.
Suppose a farmer has 100 chickens and 100 cows, then you could say that the mean number
of legs per animal is three.
But probably not a useful thing, it loses some of the information about the animals.
Okay, so here's an example.
Suppose you looked at houses that were selling in a certain area, and you were thinking about
getting a sense of the prices of houses in that area, and you found that the median was
$191,000.
That probably is a better measure of central tendency of house sale prices than the mean.
Because often if you have, say, one house that's sold for several million dollars, it
could have a big effect on the mean of the housing prices, and it would give you a distorted
view of what the, so to speak, average or typical house was selling for.
So in that case, you would say the median was a better thing to use.
We saw the salaries at a company where, once again, if you were thinking of being employed
at the company, probably looking at the median salary would be more representative of what
you're likely to make than looking at the mean salary, which included an outlier.
If you look at an example like the heights of adult men in the United States where the
graph has this nice symmetric distribution about a center value, the mean is a very good
way to summarize that kind of data set.
But none of these things talks about the distribution of the data.
And the distribution of the data is important.
That is not only where the center is, but how spread out those data are is an important
issue.
For example, and here's a case we've met already before, was the case of the distributions
of the batting averages of batters in the year 2000 versus the year 1920.
In the year 2000, the batting averages were more squished together.
They were not as widely spread.
That is to say the people's batting averages were more similar to one another.
There's less variation.
From the 1920 batting averages, they were more spread out.
In the lecture 16 on sports where we asked the question, who is the best hitter in the
history of baseball, it's going to be very important to realize what the significance
is and the consequences of having the data spread out or more widely dispersed like they
were in 1920 or closer together like they are in the year 2000, even though they have
the same center.
One of the issues has to do with how likely are we to see a batter get a 400 batting average
in the season in the future and we'll see why it would be an unexpected occurrence in
our lifetimes.
But that's for lecture 16 to deal with.
But it illustrates the idea that the dispersion of the data, the variety of the data is an
important thing for us to look at.
Because much of the information about a set of data is lost if we just look at the mean
or the median to summarize it.
Neither of these tells us how widely the data are dispersed.
Now we can look at some things to tell something about the dispersion, namely we can look at
the maximum value of a data set and the minimum value.
We can look at the quartiles of a data set.
Those are all measures of dispersion that give us some sense of how dispersed the data
are.
And in this example, here is a collection of data that actually two sets of data, where
here we have a lot of data in the center and then two collections of data that are to the
sides.
Whereas in the second data set, there's almost nothing in the center and much more data are
on the two sides.
Many more data points are on the two sides.
Look what happens when we summarize that with a box plot.
The box plot that we get are identical in those two cases because the box plot is just
telling us where the minimum value is, where the maximum value is, what the median is,
and where the first and third quartiles are.
And we chose these data sets to have exactly the same maximum as one another, a maximum
as one another, minimum as one another, and the quartiles both in both cases lay at exactly
the same places.
So this illustrates the idea that the box plot strategy for talking about dispersion
misses some of the main character of the data set.
Namely, this data set, number two, the difference between the values is bigger than, on average,
than the data set for data one because a lot of the data in data set one are all centered
at the same place.
So our goal then is to try to actually find a way to measure this dispersion of data.
Now a histogram, by the way, does give us a better illustration of how it is that the
data are spread out because it shows where every place is on the data.
But what a histogram does not do is give us a single numerical number that tells us how
widely spaced data are.
So that's the challenge that we now turn to.
We now turn to the challenge of describing some numerical measures of dispersion, how
widely spaced, how spread out data are.
Now let's think about this.
Let's suppose, you know, I mean all these things are made by humans, by the way, of
course, by just thinking about what we're trying to do and then making a definition
that tries to capture what we're after.
So in the case of trying to measure how spread out data are, what is sort of a natural way
to do that?
So let's just think for a minute.
If you have a collection of data and think of it like, say, the salaries at a company,
how are you going to measure how widely spread those data are?
Well, a natural way, I would think the very first way that you'd think of doing this would
be to say, let's take the mean value, the center value, the mean of the salaries, and
then for every salary, let's just see how far away it is from the mean.
You follow me?
Does that make sense?
You take the mean and you just say how far it is for each single salary.
You say, well, this one is quite a bit lower.
This one's quite a bit higher.
Now, by the way, we should take the absolute value.
So we're talking about the distance away.
We don't want them to cancel out because we're trying to measure how far away they are from
the mean.
So we take their absolute value and we could, a measure, a numerical measure of how far
they are from the mean would be to say, what's the mean of their distances from the mean?
You follow me?
What's the mean of the distances away from the mean?
So let's look at this example.
Here's an example that has just four data points.
These numbers are minus six, minus four, four, and six.
Very simple set of four data points.
The mean of these four data points is zero.
I chose them to balance at zero.
You can see that there's a minus four and a plus four, minus six and a plus six.
They're going to balance at zero.
So zero is the mean.
And the average deviation from the mean, that is the average distance away from the mean,
is what?
Well, the four values, the minus six is six away, minus four is four away, plus four is
four away from the mean, which is zero, six is six away, dividing by four, we get five.
That is the average distance away from the mean.
And it seems like a good measure of dispersion.
However, it turns out that this measure of dispersion is not the most commonly used measure
of dispersion.
The most commonly discussed measure of dispersion is called the standard deviation.
And it's a little more complicated than this, and I'll tell you why in a few minutes.
So our goal is to find a numerical measure of how spread out the data are so that you
get a bigger number if they're more spread out and a smaller number if they're more
clustered together.
And the standard deviation accomplishes that, just as, by the way, the average deviation
accomplished it.
But let me, first of all, tell you how to get the standard deviation, and then we'll
discuss why it's the measure that is most commonly used.
So here's the definition of the standard deviation.
The first thing that we do is we go through our collection of data, and for each datum,
look at the difference between it and the mean.
Mu is the mean, by the way.
So in this fancy formula, mu is the mean and xi are the different data values in our collection.
So for example, if we're talking about salary, for each salary, you take the difference between
the salary and the average salary, the mean salary, and then square it.
So you square the difference from the mean, and add up all of those squared differences
from the mean value, and then take the mean of those, that is, divide by n.
So this number, which is designated sigma squared, is called the variance.
This value is the average squared distance away from the mean.
So it's exactly like the average deviation that we talked about before, except that we're
squaring each of those differences, square each difference, and then just divide, take
the mean of those squared differences.
And the square root of that is called the standard deviation.
So the standard deviation, we do some squaring, then add up the squaring numbers, and then
take the square root to get the standard deviation.
And you can see that it's basically, just as before, a measure of how far away a typical
value is from the mean.
Or maybe I shouldn't say a typical value, but I should say it's the mean of the square
root of some of the square distances from the mean.
So this is the definition of the standard deviation, and let's just do an example to
ground it in reality here.
First we take our same collection of four numbers, so minus six, minus four, plus four,
plus six, and once again, the mean of these numbers is zero.
They balance at zero if we put them on a teeter-totter.
And let's go ahead and compute the standard deviation.
The average deviation, which we did before, remember, was five, and the standard deviation,
remember what we do, we take the square of each distance from the mean, so we take six
squared for our minus six, we square that and get six squared, four squared, four squared,
six squared, and add up those squares.
By the way, squares are always positive numbers.
That's one reason to square, is that you don't have to worry about absolute value signs.
So you square it, divide by four, and then take the square root.
And we get 5.1, slightly bigger than the average deviation, and that's typical, that
you will get a number that's slightly bigger than just taking the average distance of the
points to the mean.
And by the way, just to remind you of the terminology, this number before we took the
square root, whose value is 26, is called the variance.
So it's the mean of the square distances from the mean, so it's the variance.
So the standard deviation, and by the way, we're using the letters that are typical,
customarily used for standard deviation, sigma for the standard deviation, or s, sigma squared
or s squared for the variance.
Now I have to say something now so that to avoid getting lots of emails telling me why
I'm wrong on the definition of the standard deviation.
Very often, the definition of the standard deviation will have a division by n minus
1 for the variance and the standard deviation instead of n.
In other words, that you take the sum of the squared differences from the mean, add them
up, and instead of dividing by n, divide by n minus 1.
That is the correct definition when you're taking a sample of a population.
If you're looking at the entire population, which is what we're concerned with in this
lecture, where we know all of the data from the population, then it's appropriate to divide
by n.
And in a future lecture, we'll see what the difference is and why we divide by n minus
1 if we're taking a sample.
You may wonder why it is that we choose to use the standard deviation rather than the
average deviation.
Remember that the average deviation was very simple, the simple idea.
You just took the average distance from the mean to each data point and took the average,
and that would say how far on average the data are spread from the mean.
Why do we need to go to this trouble of squaring the difference between the mean and each data
point, adding them up, dividing by n to get the variance and taking the square root?
Why would go to all that trouble?
Well one of the answers is that if we take just the average distance to the mean, notice
that it has the following property, that if we look at our data set, minus 6, minus 4,
4, 6, if you took the average distance from 0, the mean, you would get 5.
Look what happens if you take the average distance from the 0.1.
You see what happens?
The minus 6 gives us 7, the minus 4 gives 5, that's 12 altogether, and then each of the
upper ones is reduced by 1 because we're at 1 instead of 0.
So the 4 just gives a distance of 3 and the 6 just gives a distance of 5, so that's 8
more.
So you notice that the total, 12 plus 8, is 20 again, which divided by the 4 data points,
4, gives an average value again of 5.
So we got the same average distance from 0 as we got average distance from 1, or in fact
2, or minus 1, or any number between minus 4 and 4, in fact, would always give us that
same average distance.
So taking the average deviation doesn't help us to find a unique, the unique mean, whereas
the standard deviation has the property that it minimizes the deviation.
That is, if you take the mean value and you take the sum of the square distances from
the mean, and you take that mean of those, you divide by n to get the variance, and you
take the square root, that that number is minimized by choosing the mean as the value
from which you're taking these differences and squaring them.
I wanted to demonstrate this by some rather complicated looking work.
If we choose a central value, and we think of it as m, and we think about taking the
sum of the squares of the distances from m to each of our data points in turn, and then
we add up those squares, our goal is to say, what number m will make that number the minimum
among all possible m's, and we're going to find out that it's actually the mean.
And this fancy work here is trying to minimize the sum of the square distances, and all we
do is we take the sum of the square distances, multiply out that square by the algebra that
you may or may not remember from high school.
And then what we do is we, this, I'll tell you, the reason I'm doing this is I'm demonstrating
that calculus actually plays a role in statistics, and you don't have to follow any of this.
So I'm just going to do it very quickly.
Taking the derivative gives us this sum, if we're trying to maximize a value in calculus,
what we do is we set the derivative equal to zero, which occurs when this equation is
true, solving for m, we see that we get exactly the mean.
So this is an example where calculus actually shows us that the mean has that special value
of being the only place where the sum of the square distances is minimized.
So that, so it's unique, and that's one reason for using, it's a technical reason, but it's
a reason for using, one of the reasons for using the standard deviation and variance instead
of just the average distance.
Lesson aside, and you can safely ignore it.
Let me just point out that the salaries of a company, the ones that were less than $100,000,
we see that the standard deviation is $19,000.
So that's a measure of roughly how far a typical salary in that range differs from the mean,
$19,000.
But look and see what happens when we add the CEO's salary.
So we go through this computation where you have the CEO making a million dollars.
What happens?
The mean goes up to $64,000, and the standard deviation jumps to $138,000.
So it's saying that the standard deviation is far higher than it was before.
So the standard deviation is susceptible to a strong influence by an outlier.
Well in this lecture then, what we have discussed is the ways of measuring dispersions and difference
of dispersions.
And we had the measure of the standard deviation, which then can be used.
And the way we would use it is to compare sets of data.
For example, we might want to compare the salary distributions at companies in Japan
compared to the way salaries are distributed in the United States.
And we would find that the standard deviation of salaries in companies in the United States
is much smaller than the standard deviation, I mean in Japan, is much smaller than typical
ones in the United States.
Their CEO's salaries tend to be lower, and that's one thing that the standard deviation records.
We saw different measures of looking at dispersion.
One was the five number summary, the minimum, the maximum, first quartile, median, and third
quartile.
We saw that the box plots that are associated with that give us some sense of the dispersion
of data, but it didn't give us a numerical measure and it didn't follow the whole distribution.
The standard deviation is a numerical measure of roughly how far the data are on average
from the mean.
So this was one of the basic concepts in statistics.
And in the next lecture, so in this lecture we were talking about, in our triumvirate
of shape, center, and spread, in this lecture we were talking about spread and also center.
In the next lecture, we're going to take on the challenge of shape.
I look forward to seeing you then.
