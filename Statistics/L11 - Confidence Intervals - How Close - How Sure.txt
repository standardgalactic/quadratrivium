Welcome back to Meaning From Data, Statistics Made Clear.
Today we're going to talk about one of the most common statistical statements that we
read in the newspapers, which is the headline that we see every election, which will say
something like this.
Candidate A will receive 59% of the vote with a margin of error of plus or minus 3%.
So in this lecture, we'll see what a statement like that actually means, and also why that
statement is just simply incomplete as it's written.
It simply does not contain a meaning if it didn't have some other presumption that is
not stated in that statement.
So the format of the lecture will be to just look at some examples that will make this
hopefully clear.
And we'll begin with the concept of an election where we have two candidates, imaginatively
named A and B.
So we are thinking about trying to find out how the population of all the voters in the
country are likely to vote, and our strategy is quite simple.
What we do is we just take 1,000 people at random, always at random of course, and we
ask each one how he or she will vote in the upcoming election.
And one of the points of this lecture is going to be to answer the question, how many people
do we need to ask to be how confident?
Remember that in some sense, the goal of statistical analysis can be summarized as how close and
how confident.
And we're going, in this lecture on confidence intervals, we're going to answer those questions.
How close is our guess going to be, that is what the margin of error is going to be,
and how confident are we that in fact we are close enough?
Now in our example here, talking about statistical polling before an election, we're going to
make the following assumptions, none of which incidentally is actually true in the real
world, which is a big issue that we'll talk about later.
But first of all, let's assume that everybody tells the truth.
Second, let's assume that every person that we ask actually answers the question.
You can see that none of these is actually true.
But third is that all the people that we ask are in fact chosen completely randomly from
the general population.
They're always all voters.
They actually will vote.
But these are issues that are very problematic in practice.
And so what this lecture really is about is the mathematics of how we do confidence intervals,
which is what's actually reported.
But there are other issues that we have to realize that in reality make the answers
to how accurately we can assess future voting trends that make it all the more difficult
to actually ascertain.
Well the goal as is often the case when we're taking a sample from a large population is
that what we really want to know is how the voting population at large, all the voters
in the country, are going to vote.
Maybe there are 100 million voters.
We once again don't really care about the specific say thousand people whom we're asking.
I mean there's nothing special about them except as they are trying to give us evidence
for how we are going to make a guess about how the 100 million people are going to vote.
So this lecture completes the promise really from lecture three, the introduction to statistical
inference, where we first describe the concept of gathering from a sample of a population
an answer about the whole population or answering the question how close and how confident we're
going to quantify those ideas.
So let's begin with our example and as I said we'll assume that we have an election
coming up and we're going to ask a certain number of people how they will vote and try
to ascertain what we're going to declare as that headline in the paper of how close
we are to guessing the total voting population and how confident we are that that is correct.
So let's begin with an example of a poll that would not actually be undertaken in real life
because it's too small.
So suppose that we have a poll of only 10 people.
You randomly choose 10 people and suppose that the actual answer is that 60% of people
are for candidate A. Then what would we expect when we take a poll of 10 people as how often
would we expect six of those 10 to say that they're for candidate A. How often would we
expect seven of them to say they're for candidate A. Five of them. Four, six, nine, ten.
We could get any number between zero and ten for candidate A. We're only picking ten people.
But it's more likely that we will get six people for candidate A and somewhat less likely
that we get five and here you see the dots are an accurate representation of what proportion
of samples of size 10 have zero people for candidate A, one person for candidate A, two
people for candidate A, three, four, five, six, seven, eight, nine and ten.
So if you choose just 10 people and ask them you can see that it is rather likely that
you'll get between four and eight people for candidate A if the reality is that in the
population 60% are for candidate A.
But look what happens if we take a larger number of people. Then the proportion of our
samples of size, in this case this diagram has to do with samples of size 100, a much
higher percentage of samples of size 100 are closer to the 60% reality of the voting population.
Likewise, if we go on and take a sample of size 1000 then the proportion of groups of
size 1000 whose fraction of people for candidate A, they tend to get closer to the reality
of the whole population. So this is a basic principle and by the way I have to confess
to you if this sort of seems like deja vu it's true. We've done this before and in fact
the whole purpose in a way of our previous lectures on probability and were to illustrate
this idea that as you take larger samples those larger samples are having a higher fraction
of the samples that are closer to the actual population mean.
So here we go and you can see in this diagram we have all of these represented. That is
this low line here shows us that samples of size 10 are more apt to include samples that
differ from the actual correct answer of 6 out of 10 compared to the green ones which
were samples of size 100 and then this tall one which is samples of size 1000. As we get
more people we're more apt to get a good representation of the world. However we want
to point out that we may not get the good representation of the world it's certainly
possible to ask 100 people at random and all of them are for candidate A or all of them
are for candidate B even though 60% of the people in the world are for candidate A.
So let's assume that after taking our poll and now let's assume that we don't know the
answer. We don't know that 60% of the population is for candidate A because in reality when
we're taking a poll we don't know that. So suppose that we do our poll we ask 1000 people
and 59% of them that is 590 of those people whom we asked say I will vote for candidate
A. So the question that we're going to answer in this lecture is what can we conclude from
that and so here is what we conclude. We say okay in the actual world there is some percentage
of the people who are for candidate A. Maybe only 30% of people are for candidate A. If
only 30% of the people were for candidate A what's the chance that I would have gotten
a sample where 59% are for candidate A or 590 out of 1000 are for candidate A if on average
in the whole population only 30% are for candidate A. The answer is it's a very remote
chance very remote chance and so the hypothetical work that we're doing is to say if the actual
reality in the world were that 65% were for candidate A what's the chance that I only
got 59% for candidate A in my sample. If the reality were only 55% for candidate A what's
the chance that I got 59% so what we'll do is just look at the following diagrams. This
first diagram describes a hypothetical situation where we assume that in reality 53% of the
population is for candidate A. The graph shows us what proportion of samples of size 1000
have various percentages for candidate A. We can see that if 53% of all voters are for
candidate A then most samples of size 1000 have somewhere between about 500 people that's
50% and about 560 or 56% for candidate A. Remember the last lecture about hypothesis
testing. Well let's do a hypothesis test here. Let's consider the null hypothesis that the
world really contains 53% for candidate A. Now we took a sample of 1000 and found that
590 or 59% of that sample were for candidate A. In doing this hypothesis testing remember
that the strategy was to hypothesize something about the world and then see whether the evidence
we find would be rare under that hypothesis. In this case where we hypothesize that 53%
of all voters are for candidate A then we can look at this graph to see whether getting
59% would be an unusual event and we can see that 59% lies way out on that thin part of
the graph. Specifically we would reject the null hypothesis that 53% of voters are for
candidate A when we found that our random sample had 59% for candidate A because less
than 5% of samples of size 1000 are that far or further away from the hypothesized value
of 53%. In the language of hypothesis testing we would reject the null hypothesis that 53%
of the population is for candidate A at a significance level of 5%. Okay so 53% should
be rejected. How about 54%? We entertain the hypothesis that the true proportion of the
population for A is 54% but we also reject that hypothesis because our sample proportion
of 59% is still too far away. 55%? We reject that also. But look what happens when we
consider 56%, the red curve. At about 56%, it's actually 55.9% by the way, we find that
our sample of 59% would not be sufficiently far from 56%. That is sufficiently unlikely
to occur by chance for us to reject the null hypothesis that the true population proportion
for candidate A is 56%. So that boundary value, that breakpoint where the percentage hypothesized
is close enough to our sample percentage that we would not reject it as a null hypothesis
is the lower end of the confidence interval that we are creating. In this case that value
is 55.9%. Now let's now move on. We'll consider 57%, 58%, 59%, 60%, 61% as hypothetical values
for the true population proportion. For those values, getting a random sample of 59% is
right among the most expected values for most samples of size 1000. So we would definitely
not reject any of those values in a hypothesis test. So all those values are in the confidence
interval. We would continue considering hypothetical worlds until we reach the first value, that's
this blue curve, and in this case it's about 62%, actually 62.1%, where the sample that
we found of 59% for candidate A would cause us to reject the null hypothesis that the
true population proportion is, in this case, 62.1% for candidate A. So okay, let's just
review the whole situation. We took a sample of 1000 voters and found that 59% of them
are for candidate A. Our feeling is that it's likely that the actual proportion of the population
that is for candidate A is 59% or not too far from 59%. The proportion of the population
for A probably lies somewhere in an interval centered around 59%. We came up with the interval
from a little under 56% to a little over 62%. That interval is called the 95% confidence
interval because there's a 95% chance that the true population proportion will be somewhere
in that interval. Or stated another way, when we got a random sample of size 1000 in which
590 people, or that's 59%, were for candidate A, then the interval, 59% plus or minus 3.1%
is the set of all percentages that we would not reject as a null hypothesis in doing a
hypothesis test at the 5% level of significance. So when we take a sample, we can produce a
confidence interval that we expect to contain the actual population proportion 95% of the
time. So we did some simulations here on a computer that did exactly this kind of experiment.
That is to say, we took from a population where the actual population mean was 60%,
we took groups of 1000 and saw what the probability, I mean what percentage of those people were
for candidate A, and then put a margin of 3% on each side. And then that gave us, or
actually a little more than 3%, that gave us this 95% confidence interval. And you can
see that what happened here is that all of these black lines, each one of these is an
experiment of choosing 1000 people at random from a population where 60% are for candidate
A. And for each of these lines, the ones that are black have the property that their
span does in fact include the true population percentage, which is 60% down here. In other
words, if I draw the straight line up, you'd see that the straight line does hit each of
the black lines. But on 313 of our simulations, we got intervals that did not include the
true population. What that is telling us is that when we read this headline in the newspaper
that says that the voting, the people who are for candidate A are 59% plus or minus
3%, well, 5% of the time by luck alone, that interval will fail to contain the actual population
mean. Now why do I say 95% confident? Notice that this is a word that is not mentioned
in the newspaper headline. The newspaper headline said, it's 59% plus or minus 3% margin of
error. The implication when it is not mentioned how confident you are is that you're 95%
confident. And the reason for that is purely historical and purely arbitrary. There is
no reason why you couldn't have chosen 96% confident or 90% confident. So let's think
about what this means. Suppose that you wanted to be more confident. You wanted to have an
interval that contained the correct proportion of the population at least 99% of the time
instead of just 95% of the time. If you took your sample of 1,000 and 590 people were for
candidate A, you would have to declare just a much wider plus or minus interval. You see,
because if you have a wider interval, you're more apt to contain the true population mean,
and therefore you can make yourself 99% confident on that by having those two red and blue graphs
spread out so that the 59 is in the much thinner part of those two graphs. You follow me? So that
it would happen only 1% of the time by chance alone that you'd get 590 people for candidate
A if you were out to say maybe 65%. Okay, so this illustrates the idea that a confidence
interval doesn't always capture the population mean, but 95% of the time on average it does,
and here are some more simulations that in this case of all these simulations only one time did
it fail to do it, whereas in this simulation too, this time on five different occasions,
the simulation failed to capture the population mean. So this is the basic idea of confidence
intervals. Now I'm going to describe how many people you need to take a sample of in order to be
how confident. So what we're going to do is go through an example to illustrate how you choose
how big a sample you need to take, and we're going to have the goal, the first goal will be how can
we get a confidence interval that is only 3% off on one side or the other. How many people do we
really need in reality to take in order to have a 95% chance of being within 3% of the true population
mean. So we'll start off with this diagram that illustrates what's happening in reality. We have
a certain number of people for candidate A and a certain number of people for candidate B, and in
this case you can see that roughly 60% are for candidate A, but we're just assuming we don't
know what that percentage is, but it's somewhere close to maybe a few more for candidate A.
So we can draw a histogram, a very simple histogram, where we just put all of the people
who are for candidate A at the point one and all of the people for candidate B at the point zero,
and then if we take the mean of those numbers we get 0.6, so the mean is telling us the percentage
of people for candidate A. The mean is whatever this height is when the two heights add up to 100,
you see the two heights add up to the 100%, and we're putting the number on this side are the
ones for candidate A at the one, and at zero we're putting the other people. So the computation
of the mean, if suppose that we have P is the proportion of people, so it's a fraction like
0.6, if 60% are for candidate A, then the number of these 100 million voters who we put at
number one are the proportion of people times 100 million, and likewise we put 100 million times
one minus P, those are the remaining people are put at zero in our histogram. And so computing
the mean is just done by taking this 100 million people times P, 100 million times P is the number
of people who are sitting at one, so we multiply that by one, and then 100 million times this is
zero, of course that's just zero, the 100 millions cancel and we just get P. So this is why when you
have that little histogram where the bar at one has P percentage of the people and the bar at the
other one has the rest, then the mean actually is P, so that illustrates that fact. Now
we have then a distribution, and when we have a distribution we can find the standard deviation
of the distribution by remembering that the definition of the standard deviation.
Remember that the way you get the standard deviation is you first find the sum of the
squared differences from the mean, and then you take that the average, the mean of those
squared differences from the mean, that's the variance, and then we took the square root of
the variance to get the standard deviation. So let's just, we'll go through this in detail.
So if you have this number of people, 100 million times P people, which is the number of people in
that bar at one, each of those people is one minus P distance from the mean of the population,
because you see each person over here is at one and here's the mean which is at P,
and so the difference between these two is one minus P, that's that difference.
So we square the difference of the distance between one and the population mean, and
each person who's at one contributes such a contribution to the sum of the square distances.
Remember we're taking each individual in the population, taking its distance to the mean,
and squaring it, and adding them up, and then we'll take the average of those,
the average of those, the mean of those. And so we then look at the people who are at zero.
The people at zero, right here, each person who's at zero is distance P away from the mean.
So if we square that distance, P, we get P squared. So the number of people, 100 m times
one minus P is the number of people each of whom is P distance from the mean. If we just do a little
bit of algebra here, we see that this is the answer, it's 100 million times P times one minus P,
and the variance, as you recall, is when we take the number of people in the population,
which is 100 m, the 100 m's cancel and we just end up with P times one minus P. So P times one
minus P is the variance, that is the average square distance from each person to the mean.
The square root of that is the standard deviation, square root of P times one minus P.
The goal that we're trying to accomplish is to ask how big a sample size n do we need to take
for us to have a 95% confidence that we're within 3% of the true population mean,
the true portion of the population. Well, we need to remember a couple of things. First of all,
in a normal distribution, you may recall that 95% of the material under a normal curve lies
within two standard deviations of the mean. Remember that from lecture six. So our real goal
is to have a distribution so that that is a distribution of our samples, those peak curves
that we saw before, we want a distribution of our samples so that two times the standard deviation
of the sample is less than 3%. And that way we'll know that 95% of the time we're within
3% of the true value. So here we go. If we take n people as a sample, you may remember that we've
discussed briefly the central limit theorem. The central limit theorem said that if you have a
distribution and you take a sample, you take all samples of size n, a fixed sample of size n,
and for each sample of size n, you take the mean of those and you get a value. So you take every
sample size n, you take the mean, take sample size n, take the mean that we saw that the
central limit theorem states that the standard deviation of the samples of size n are smaller
than the standard deviation of the original distribution that you started with in a predictable
way, namely you divide by the square root of n. It becomes thinner by dividing by the square root
of n. So in other words, if we take samples of size say 100 from a distribution and we take the
every group of 100, we take the mean and that's a value. Any ever group from 100, we take the
mean and that's a value. We get a distribution. The distribution will become bell shaped. That's
also part of the central limit theorem. It becomes bell shaped and it's the width of it,
the standard deviation of it, is going to be 1 tenth of the standard deviation of the original
population if we take samples of size 100 because the square root of 100 is 10. You see?
So here's, in our example here, we know the standard deviation of our original just
population that has two bars, people at one, people at zero. Its standard deviation, which we
just computed, was the square root of p times 1 minus p. So the standard deviation of the,
if we take samples of size n, the standard deviation is going to be only one square root
of nth of that, that is we divide by the square root of n. The square root of p times 1 minus p
is never going to be bigger than one half and this is just some arithmetic. Suppose p, remember p
is a number between zero and one. If p is a half, we have a half times a half. The square root of
a half times a half is a half. If we have a number like p is equal to 0.6, then we have 0.6
times 0.4, which is 0.24, which is less than 0.25, and so its square root is smaller than a half.
You see? So you never, so this value here is never bigger than a half, so we can just
see that this standard deviation of the means of sample size n is never bigger than one over two
times the square root of n, one half times one over the square root of n. So you can see that
it's getting smaller. Our goal is to make two times the standard deviation of our sample less
than 3%. So just doing the plugging in for sigma is the standard deviation and this is an upper
bound for the standard deviation. So two times that is less than 3%, which is equivalent to
just doing some multiplying through. The two's cancel, square root of n comes over here.
Square root of n has to be bigger than one over 0.03 and just squaring it to get n.
n has to be bigger than or equal to 1,111. So all of this work and I certainly don't
imagine that you necessarily followed every step of that, but the, all of this work tells us that
if we take a sample of size 1,111, then we can be 95% confident that the 3% interval,
a plus or minus 3 error, will capture the actual population proportion. So this is,
I know this is a little technical, but it gives you at least the flavor of how these
things are done. Suppose that you wanted to be much more confident. For example, suppose you
wanted to be 99.7% confident that a 3% interval contained the correct answer. Well, then you do
exactly the same analysis. You're trying to find out how big the n would have to be.
Remember that in a normal distribution, 3 times sigma is the 99.7% level. In other words,
in a normal distribution, 99.7% of the material is within 3 standard deviations of the mean.
So we're looking at the, at the sample mean, samples of size n and, and we want
3 standard deviations to be less than 3%. And doing the exact same work we did before,
we see that the sample size would have to be bigger than or equal to 2,500. I, I wanted to
point out just one other feature and then we'll stop. And the, the feature is that notice that the
size of the sample in order to get confidence that we're getting close to the correct answer
is not determined by the total population. It didn't matter that there were 100 million people,
total voters, or just 1 million people, total voters, the numbers that we were getting were
just determined by an absolute number. And the analogy for this is the following. Suppose you
were creating chicken soup, you know, you're, you're, you're cooking soup, you don't create soup,
but you're cooking soup for your family and you have a little saucepan and you want to know
whether or not it's salty. What do you do? You stir it up, you take a spoon, you taste it and see,
is it salty? If it's perfect, it's perfect. Now suppose the next day you're in, in, you're
volunteering at the local soup kitchen, you have a big vat of soup that's going to feed hundreds.
What do you do to test how salty it is? You stir it up, you take a spoon out, and you taste it,
and you say, is it the right saltiness? If, if so, fine. You don't have to take
the same proportion out of this big vat. You don't have to taste a whole, you know,
big terrine of soup in order to tell whether it's the right saltiness. So that's the,
the analogy to saying that the absolute size of the sample is what tells us the confidence.
So in this lecture, we've, we've seen the, we've, we've gotten to the promise of how to tell how
close we are and how confident we can be that we are that close. See you next time.
