Welcome back to Meaning From Data, Statistics Made Clear.
Today's lecture is on hypothesis testing, which is one of the most common statistical techniques.
It's used in every application area in which statistics is applied,
which is basically everything, in education, psychology, all the social sciences,
the natural sciences, in medicine, in law, everywhere.
Hypothesis testing is one of the workhorses of statistical inference,
and the goal of this lecture is crystal clear,
and that is to illustrate and explain the logic of hypothesis testing.
And basically that's the core of the idea of statistical inference in general.
So let me first give you this analogy about hypothesis testing,
which I actually presented earlier in lecture three.
Hypothesis testing is like the American criminal justice system.
The goal is, in the American criminal justice system,
what happens is that the defendant is viewed as innocent
until sufficient evidence is presented,
which would make the presumption of innocence an untenable assumption.
And that is the core of hypothesis testing.
And by the way, in the criminal justice system, you see,
if you had a defendant that's on trial,
the judge says to the jury, says, this person is innocent,
and when evidence comes up, what kind of evidence comes up?
The evidence that comes up is a witness says,
I saw somebody who looks like this guy who was right near the scene of the crime.
I saw somebody who was driving a car that looked like the car that this person has.
These kinds of bits of evidence, each one is saying,
if this person is innocent, what's the chance that these observations would have been made?
And then you look at the evidence, and if it's extremely rare,
then you have to discard the concept of innocence.
Well, that is exactly the analogy for hypothesis testing.
And by the way, there's another part of this hypothesis testing that is also apt,
and that is that you only try people who are, at least some people, view as guilty.
They're put on trial, not just a random person,
but a person who somebody, the prosecutor, feels is guilty.
And then there are challenges to say, under the assumption that the person is innocent,
is there sufficient evidence to show that that assumption is wrong?
Okay, so now what we're going to do now is talk about hypothesis testing,
and you'll see that it's exactly analogous to this legal system in really all these ways.
And the strategy of this lecture is that I'm going to give a couple of examples
and just work through them the way a hypothesis testing scenario works to demonstrate the ideas.
So the first example is the following. Suppose that I take a penny.
So this is a penny, and I'm going to spin the penny and see whether it lands heads or tails.
So here it is. I'm going to spin the penny, and it'll spin around,
and after spinning it'll land heads or tails.
In this case, it landed heads.
Originally, we might think that it had an equal likelihood of landing heads or tails.
But I'll tell you now that I don't believe that.
In fact, it's not true, and if you actually try it at home,
you will discover that it's not true that it lands equally heads and tails.
So if I'm trying to prove that it's not a fair process,
that it's not equally likely to land heads or tails,
I'm going to undertake a hypothesis test.
I make the hypothesis that in fact it is equally likely to land heads or tails.
That's my hypothesis.
And notice that it's the hypothesis that's really contrary to what I really believe to be the case.
Okay, so I make the hypothesis that it has an equal likelihood to land heads or tails.
Its probability is equally heads or tails.
And then what do I do?
I undertake some experiments.
And the experiments that I'm going to undertake is to simply spin the penny a lot of times.
And I'm going to spin the penny a certain number of times and then record the results.
The concept of this hypothesis testing is to say,
is the result of spinning these pennies a hundred times,
did the result that I obtain,
is it extremely a rare result to have occurred if the penny were in fact equally likely to land heads or tails?
So before I spin the penny, before I've done any of the actual experiment,
what I do is I think,
and what I think about is what are the likely outcomes,
what would be the distribution of outcomes of my experiment
if the penny were equally likely to land heads or tails?
So let's look here and see what I mean by that.
Here is a graph that shows what would happen if I spun a fair coin.
That is one where the probability of landing heads were in fact one-half.
This tells me if I spun it a hundred times, many, many, many times.
That is, I did it a hundred times, many times.
You know, like millions and millions and billions of times.
This tells me the proportion of those times that the number of heads coming up would be the different values.
Very infrequently, with a fair coin, the number of heads coming up would be between 0 and 30.
It could happen with a fair coin because every time there's a 50% chance that it lands head,
and it could happen that when you do it a hundred times,
it's only 30 times that it comes out heads.
That could happen.
But almost all the time, that's this region here,
almost all the time that the outcome will lie somewhere between,
as you see in this graph, between 40 and 60.
That would be the expected outcome of spinning coins a hundred times.
Follow me?
All of this discussion has taken place, notice, before we actually spin any penny.
That's the characteristic of hypothesis testing.
We see in the abstract what distribution of outcomes we would expect if the hypothesis were true.
The hypothesis is called the null hypothesis.
The null hypothesis is that the coin is fair in this case.
Okay.
And now we make a decision.
Before we do any experiment, we make a decision and we say,
I'm going to reject the null hypothesis that the coin is fair.
In other words, I'm going to declare this coin to be guilty if the outcome of my experiment is sufficiently rare.
And I describe how rare.
I'll say 5% rare.
Before I start the experiment, I'll say,
if the result I get would only happen 5% or fewer times with a fair coin,
then I'm going to call this coin guilty.
I'm going to say I'm going to reject the null hypothesis that the probability is even heads or tails.
And I'm going to declare that, in fact, it's not a fair, I shouldn't say fair.
It's just that it doesn't have a 50% probability of landing heads.
Okay.
So here's what we do.
So we do the experiment.
Now, after doing the experiment, we find that only 39 times does the coin end up being a heads.
When we do that, we look at our graph and we say, ah, look at this, 39 times.
That looks way over here.
It's not in the center region.
In fact, that only 39 times did it land heads indicates that it would be a very rare event, quite rare.
You see, because it's right here on the graph, quite rare that you'd get only 39 heads if the coin were fair.
Now, what we record is what's called a p-value.
The p-value is the probability that the outcome would be as extreme or more extreme than what we actually observed in the experiment.
In other words, we ask ourselves by looking at this graph, what's the probability that a fair coin would have 39 or fewer heads
or the same distance away from that center value 50, that's 11 higher, 61, 61 or higher, numbers of heads.
We look at that probability, which is just the fraction of the area under the curve on the outside.
Here, the part that's sort of colored red, but probably hard to see because it's so small,
that that region is so small that the probability is very unlikely to happen that such an occurrence would occur.
We say this is strong evidence that our initial hypothesis was wrong and we reject the null hypothesis.
That's the logic of hypothesis testing.
Here is just a general graph that describes a null hypothesis.
It describes the region in which we're going to reject the null hypothesis if the experiment comes out in that unlikely region.
That is the general form of the hypothesis testing.
The p-value is the probability that assuming that the null hypothesis is true,
the probability that the experiment would result in a value as extreme or more extreme than the value that actually resulted from the experiment.
By the way, let me just say, I think that what's interesting about this hypothesis testing is it's rather convoluted logic.
I don't know if you see it this way, but I see it as rather a strange kind of logic that you hypothesize a certain world
and then you get evidence to show that it's rare and then reject the concept of the world that you had before.
That's a rather complicated thing, and yet that is the way that most statistical inference operates.
If you read papers in psychology or sociology or education, all of them are based on this kind of hypothesis testing.
Let me give you another example right now having to do with medical experiments.
Suppose that you have a medication and you wish to test the efficacy of the medication.
You want to know if it works.
To ground our discussion in a specific example, although made up,
suppose you have a cream that's supposed to cure athlete's foot.
How do you go about experimenting with such a thing to see if it works?
Well, the strategy is this.
First of all, the basic idea is going to be that you're going to put the cream on people who have athlete's foot and see if they get better.
That's certainly the basic idea.
But there's something about this medical condition and many others, and that is that people get better anyway.
That certain percentage of people will be cured from athlete's foot in, say, a week without any treatment at all.
So let's suppose that it's known already that 40% of people who get athlete's foot will be cured in a week.
That that's just a statistic that you already know, so let's assume that's a given.
Then we want to test this cream and to see if the cream is efficacious.
Well, what do we do?
The first thing that we do is we decide on a level of significance that we're going to accept
as a reason to reject the null hypothesis that our cream does nothing.
See, our goal is to show that the cream works.
So what's our null hypothesis?
Our null hypothesis is that the cream has no effect, and we're trying to show that's wrong.
And then we also state a level of significance, meaning that if the p-value for the outcome of our experiment is less than 5% say,
we choose it arbitrarily, what percent is going to be the significance level that we'll accept.
So we declare in advance if the outcome of the experiment would happen only 5% of the time under the conditions of the null hypothesis that the cream had no effect,
then we'll reject that null hypothesis and accept the fact that the cream has an effect.
The words statistically significant mean that the outcome of the experiment was something whose p-value,
that's the probability of its being that far or further from the hypothesized world,
was less than the level of significance that was declared before the experiment was undertaken.
Very commonly that p-value is chosen to be 5%, and the reason for that is completely arbitrary.
It's historical, it's arbitrary, and you can decide for yourself whether or not that seems to you to be a good threshold for deciding significance.
Now by the way, in other experiments you may choose a different level of significance.
For the experiment say, I will not reject the null hypothesis unless it exceeds a 1%, it's less than 1%.
By the way, a lower p-value is stronger evidence, right?
Because if a lower p-value says that the probability of getting that result was even less likely than given the null hypothesis,
then with a higher p-value.
So let's go ahead and do this example.
Before we start the experiment, we can once again draw a diagram that illustrates what proportion of patients would spontaneously be cured in a week,
who start with athletes would be cured in a week, if they were not treated at all.
So this is a diagram that captures the concept of the null hypothesis.
Namely, we expect on average 40% of people would be cured, but occasionally many more would be cured because it's just random, whether an individual is cured or not.
So sometimes 50 or more would be cured, sometimes none would be cured, but this is a histogram that describes how many people we would expect to be cured under the hypothesis that the cream has no effect.
Now we apply the cream to 100 patients and we ask ourselves what the result is.
Well, suppose that the result is 51.
51% of the people actually were cured.
You notice that you expected about 40% on average would be cured, in this case 51% of the people were cured.
The red markings here, which I know are hard to see, represent the 5% significance level that we chose before the experiment.
We chose to say that if the result that we got is rarer at the level of 5%, then we were going to reject the null hypothesis that the medication has no effect.
Well, the result 51%, as you can see, it lies in the red and we can actually compute it, it's that the p-value is .032, meaning that the probability of being 11 off from the expected 40, either less or more, it would happen only 3.2% of the time by accident.
By accident alone.
And so we count this as strong evidence that we should reject the null hypothesis that the medication doesn't have any effect.
Now, there's an issue about how results of this sort are reported.
When a study undertakes to decide, for example, whether this medication works or not, they will report the p-value.
So they will say in the article that this medication works with a p-value of .032, meaning that the actual outcome of the experiment was .032.
The standard method for doing hypothesis testing requires a previous description of what the threshold of significance is.
So statistically significant, you'd have to say, we will call the result statistically significant if the p-value is less than .05.
That should have been declared before the experiment was done.
That's the standard way.
That's the definition of statistically significant.
And by the way, while we're on the definition of statistically significant, the word significant when used in statistics does not mean important.
It means that it signifies something.
It's an old word.
Apparently, the word significant used back in like the year 1900, what meant in common parlance that it signified something, that it mattered.
Not that it was important.
And so that since the meaning of the word in ordinary English has changed over the century, we sometimes think of, oh, it's statistically significant.
Therefore, it's important.
And that's not what it really means.
The technical meaning is exactly what I've described.
It means that the hypothesis test has come out to be what it is.
That is within the previously defined threshold.
And it could be important if you're talking about something important, but it might not be important.
That's not really the definition of significance in statistics.
OK.
But it turns out that what happens in reality is that people are disinclined to follow the rigorous method of hypothesis testing.
That is to declare in advance a level of significance that they're going to say is the threshold for their concluding that they can reject the null hypothesis.
Instead, what they tend to do is do the experiment and then get a p-value and then, so to speak, decide that, well, yes, that sounded pretty significant to them.
And the reality of this has actually become known.
And I wanted to read to you a statement from the American Psychological Association concerning this issue of the p-values.
And so this is now from their publication manual that talks about the practice of the use of statistics in psychology.
And it says the following.
Many journal editors and reviewers continued to insist upon the range method of reporting p-values for tests to make the a priori significance level more obvious.
The range method, meaning declaring in advance that we will accept any evidence that is any p-value less than 0.05.
But because most researchers do not, in actual practice, select their supposedly a priori criterion for how unlikely a result has to be to be deemed significant,
it is more honest to report the exact probability.
So the result is, I found it sort of just interesting that the reality of the way people actually practice statistics actually is now sort of codified in practice.
Let me give you another example that brings up one more wrinkle, and that is the following.
Suppose that somebody asserts that the average number of calories that an adult American male eats is 2400 calories.
Suppose that that's the assertion.
And you accept that and you want to do an experiment to decide whether or not that's a reasonable statement.
So you undertake an experiment and you get, say, 25 random people.
You're wise enough to take adult men at random in selecting your sample, as we discussed in the last lecture.
You take 25 people at random and you ascertain what their caloric intake is.
And you discover that each of those 25, they have an average caloric intake of 2500 calories.
Well, what can you conclude from that evidence?
Notice that there's a new wrinkle here that's being introduced in this example.
And the new wrinkle is that if you thought about drawing your diagram of the expected results from the null hypothesis of 2400 calories as the average intake of the American male,
notice that you would have trouble drawing that diagram as accurately as we drew the diagram for the flipped coins.
And the reason is that there's something else we don't know about caloric intake, and that is the standard deviation.
We don't know how much variety there is among the population as to their caloric intake.
So there are really two things that we don't know.
In the case of caloric intake, the two things are we don't know the mean, but we also don't know the standard deviation.
So this becomes an additional problem.
In the case of the coin flips, we actually knew all that.
Everything was completely determined because it was just exactly 50% each time and we could actually draw the whole graph.
So now in our minds, when we think about the null hypothesis that the 2400 is the average intake of calories for the average American male,
we don't know the standard deviation of the caloric intake of the adult American males in the whole population.
If the standard deviation is very big, then we would expect from a sample of 25 that the standard deviation of those samples might be rather spread out,
such as this blue curve, the low blue curve.
On the other hand, if the American intake of calories among adult men were rather closely clustered to one another,
then when we take a sample of 25, we would expect to have a distribution that is more taller and peaked, like the black curve.
So not knowing the deviation, the variety, means that when we take our sample, the sample has to do two things for us.
It has to tell us what the mean is, and it has to give an indication of the mean,
but it also has to give us some sort of indication of the spread of the data.
So now we have to think.
See, we have to think as usual.
It requires our thinking in advance about hypotheticals, and the hypotheticals in this case are the following.
And by the way, let's assume that the experiment that we're going to do is to take 25 American males and see their caloric intake.
Let's think about it.
If those 25 people all consume calories that are very close to one another,
and for example, suppose all of them consumed calories between 2475 and 2525 to exaggerate.
Suppose they were all very closely clustered together.
Well, then that would be an indication that, in fact, 2400 may be wrong because of the fact that we've got an indication from a random sample,
not only that the mean was at 2500, but that they were also clustered together, you see, and that 2400 would be quite a ways away.
On the other hand, with the same mean of 2500, suppose the 25 people had a caloric intake that spread from 1200 up to 4000 and were sort of evenly spread around there.
Well, then that would be an indication that, in fact, the caloric intake of people has a rather wide standard deviation,
which would tend to indicate that choosing 25 people at random and having their average be 100 off from 2500 to 2400 may not be that unusual.
It turns out that we can cope with both of these things using an idea called the t-distribution, which is called the student's t-distribution that was invented by William Gossett.
And I wanted to tell you a little story about William Gossett.
William Gossett was a statistician who worked at the Guinness Brewing Company at the turn of the century, 1900 century, that is, in about 1900.
And he was working in the company and he wanted to publish statistical, he was a statistician there talking about yeast and so on.
And he was working on statistics and wanted to write articles in a statistics magazine journal.
But the Guinness Brewing Company didn't allow him to do that because they thought it was giving away trade secrets.
So he published under the name of student.
He published a lot of articles and did very important work like talking about this t-distribution we were just talking about and did this anonymously for his whole life.
And apparently the Guinness Company did not know until he died in 1937 that he was the author of these seminal works in statistics but were under the name student.
So this is known in all the textbooks as student's t-test.
So hypothesis testing then is a strategy that's absolutely fundamental in statistical inference where we make a null hypothesis.
We assign a threshold for statistical significance.
We do an experiment to ascertain the p-value that is how rare the event would be under the presumption of the null hypothesis.
And then we reject the null hypothesis if the p-value of the experiment is smaller than the threshold that we declared.
This is an introduction to this strategy of statistical thinking that is absolutely fundamental to the way that we do statistical inference.
Next time we'll talk about confidence intervals.
See you then.
