Welcome back to Meaning From Data, Statistics Made Clear.
In this lecture, we're going to talk about the bell curve.
Recall that when we know all the data in a population, what we're trying to understand
about the distribution is its shape, its center, and its spread.
And an effective way of understanding a set of data is to compare the data to some model
shape.
So one of the principle methods for describing distributions of real data is to describe
some known idealized distribution.
For example, in the last lecture, we introduced a couple of such families of shapes of distributions.
The Poisson distributions and the binomial distributions as well as the uniform distributions.
So our strategy then is to use these ideal distributions and use them as models and then
compare the real dataset with one of these as the paradigmatic model of what we're after.
Well the most famous shape of distributions ever is the bell shaped curve, which is called
the Gaussian or the normal distribution.
And so the family of curves, that is the normal curves or the Gaussian curves, is the topic
of this lecture.
I think the best introduction to the normal curve is just to look at a bunch of examples
from all sorts of sources to see how common this shape arises in the world of how commonly
it arises.
How common it is or how commonly it arises.
So later in the lecture we'll talk about why it's so common.
But for now let's just look at a bunch of examples.
So here is the first example, is the histogram of the heights of male adult American males.
Here is the next example, the heights of American females.
The next example is the histogram of the year 1920 Major League batting averages.
Here's another example, histogram of the year 2000 batting averages.
Here's an example, this will take a little bit of explanation, this is an example of
the following.
Suppose that you have a deck of cards and you take up five cards from this deck.
So it's like a poker hand, except I'm not going to be talking about the way poker hands
are evaluated.
Instead, let's just evaluate the cards 1, 2, 3, 4, 5, 6 up to 13, where that is the
jack is 11 and the queen is 12 and the king is 13.
And then you take five cards, your dealt five cards and just take the average value from
those five cards.
So that is the average of those numbers, is called the average value of those cards.
And if we looked at all possible five card hands of that sort and looked at their average
value, we would get a histogram that looks like this.
Notice again that all of these examples have this characteristic bell shape to them.
From the last lecture, we saw an example that the binomial distributions had this bell shaped
character to them.
The binomial distributions, you may recall that we talked about taking all samples of
size 50 of say 20 year olds and seeing what fraction of these different samples had one
graduate, two graduates, three graduates and so on.
And drawing a histogram of that gave us once again this bell shaped curve.
In this lecture, what we're going to try to understand is what the normal distributions
are.
We'll describe a mathematical description of them, give a specific mathematical equation
that gives this bell shaped curve as its graph, and then we'll see why it is that these kinds
of things arise.
We'll also give a method for creating distributions automatically from any starting distribution
doing a process that gives a bunch of bell shaped normal distributions that arise from
it.
But we'll begin our introduction to this normal or Gaussian curve by looking at a little bit
of history.
The term Gaussian celebrates the famous German mathematician Karl Friedrich Gauss, who lived
from 1777 until 1855.
In 1801, he was a young man and there was an observation, observations were taken of
the first asteroid ever observed, the asteroid series.
After a few observations, the series passed behind the sun and the question was how to
find it after it emerged.
Well Gauss was able to predict the future position, where to look for it after it passed
the sun by looking at the past measurements and these past observations and realizing
that observations, like all measurements, include errors.
They're always off, all measurements are off.
What Gauss was able to do was to fit a curve into the observed data that would minimize
the error between the curve and the known observations.
In doing this process, it involved this kind of distribution of errors.
In fact, this distribution of errors had this bell shaped curve to it and it was so commonly
associated with errors, such as in measurements, that an old name for the Gaussian or normal
distribution was the error distribution.
That's just because measurement errors are so common and they always produce a normal
distribution about some center value, so to speak, the real value.
The error curve suggests one of the reasons that this symmetric value arises, that it's
somehow an error on one or the other side of some correct value, but you can't quite
get it right.
It turned out that although Gaussian distributions were associated with errors, later they became
associated with the distribution of other kinds of things in nature.
The first person to apply the Gaussian or normal distribution to other kinds of arenas
was a gentleman by the name of Lambert Adolf Jacques Kettelay, who lived from 1796 to 1874.
He was the first to apply this normal distribution to settings other than errors.
In his book, Se l'homme et le développement de ses facultés assais d'un physique social
in 1835, Kettelay introduced the concept of the average man, l'homme moyen.
Well the concept imagines that you have an average person around which all the people
in the distribution are distributed according to a normal distribution.
And for many characteristics of people, such as height, chest size, if you draw a histogram
of the data about all people in a population, sure enough you get a bell shaped kind of
a curve.
Incidentally, Kettelay introduced the body mass index, which is still used today to
define obesity.
Well but the normal distribution of these measured characteristics was in the 19th century
was taken almost as a law of nature.
People who had extreme values, such as being very tall, began to be thought of as being
abnormal.
They deviated from the normal.
So in fact maybe this is one reason that some people don't like the normal curve, because
in a way it defined abnormal.
Before that, I don't know what the psychology was, but maybe there is some substance to
that history.
Let's talk for a minute about why the Gaussian distribution will arise so frequently.
One reason is that many times if we're taking some value, and that's what we're measuring,
and that value is actually the result of a lot of little influences that are randomly
increasing the value or decreasing the final value, then that's likely to give a distribution
that has a center value and then some pushing some values to the left and some to the right.
Let's take an example, the heights of men.
Well if you think about what it is that causes us to be a certain height, in a way presumably
the reason has to do with a lot of small genetic and environmental influences that sort of
independently push us either a little taller or a little shorter.
So they give pluses and minuses to our height that tend to sort of average out to the average.
So if you have a large population, we'd expect that most people would have the influences
sort of balancing out and sort of canceling to give it sort of an average height, whereas
a few people would sort of randomly have a preponderance of these influences that would
lead them to say be extremely tall.
So this is a way of understanding in general why we would expect this sort of bell shaped
curve for large populations in measurements such as height.
If we take errors in measurement or random noise that often occurs, well what's happening
in random noise or measurement errors?
Well it's a combining of errors that sometimes give things to the plus side of the error
and sometimes the minus side, and that tends to accumulate into this kind of normal distribution.
Another example, when lots of people take tests, you expect some to be above average
and some below average.
Now of course they're probably not always just guessing their answers to these multiple
choice tests, but maybe that's a good approximation to some of them.
But in any case, all of these examples lead us to expect Gaussian distributions in many
cases and in the same way that there were physical characteristics that led us to expect
a Poisson distribution in certain settings because of having in the case of Poisson things
like the random arrival of cars at a toll booth, we expected it to generate a distribution
of a certain shape.
Well we can now say that the Gaussian distribution will arise when we're in a setting in which
there are influences that are equally likely to give more on the plus side than the minus
side.
Okay, but now let's take the step of trying to describe this normal or Gaussian distribution
with some more precision.
And so we'll begin by just making the simple observation that the normal distribution is
bell shaped.
In order to describe the family of distributions, once again this is a whole family of bell
shaped curves, let's just notice some things about how these distributions can be related
to each other.
For example, here are the heights of American males and here are the heights of American
females.
And you can see that the difference in these two graphs, I'm sort of flipping back and
forth between them, the difference is they're basically the same shape, the difference is
just where they're centered.
One centered at about 68 inches, the males and the females are centered at about 64 inches.
So one of the ways in which two Gaussian distributions can differ is simply where they're centered.
This is a smooth Gaussian distribution.
We'll talk about the actual formula that gives this distribution, but we'll just use
it to see that you can shift it over and it is actually an exact match shifting over
and that gives an example of how the centers can be altered.
Now let's look at another pair of graphs that will illustrate one other characteristic in
which bell shaped curves can differ.
If we look at the batting averages for batters in the year 2000, we have this characteristic
bell shape and if we compare it to the batting averages of the year 1920, we have a similar
bell shape but it's more spread out.
And in fact, we talked about this before that in this case it's the standard deviation of
the, that differs.
The standard deviation for the 1920 batting averages is bigger than the standard deviation
of the batting averages from the year 2000.
The batting averages in the year 2000 are more clustered together.
And so these are two characteristics of normal distributions.
Their center and their spread can differ.
Well that's all that can differ.
The normal distribution is completely determined in fact by where it's centered and what its
standard deviation is, how spread out it is.
That determines an exact formula for a bell shaped curve.
So let's now turn to actually looking at the formula for the bell shaped curve.
This is a very formidable looking formula.
And I want to, and I put up these formulas not for the purpose of understanding every
detail of the formulas, but just to realize that these formulas actually exist and you
could plug them into your calculator and actually get an answer or have it draw a graph for
you.
But let me just mention something about this formula here, that it involves several things.
One sort of interesting feature is that it has two constants in it, namely e and the
pi, the famous ratio between the diameter and the circumference of the circle, pi.
And e and pi are the two most famous constants in mathematics.
So it's interesting that the normal curve contains both of these very famous constants.
But in this hard looking description of the normal curve, there are two other letters.
There's a mu and a sigma.
Mu is the mean.
That is to say the center point for the bell curve and sigma is the standard deviation.
So it's a measure of how spread out the curve is.
So that means that we can choose any specific values for mu and for sigma, well, as long
as the sigma is a positive number, you can choose any positive standard deviation and
any value at all for the center, for mu, and you will get a bell shaped curve.
If you plug it into your calculator, you will get a specific distribution, a specific graph.
So when you do type that formula in, you get these graphs, some of which you've seen before.
But there is a feature of all the graphs that make the normal distribution useful for analyzing
data.
For example, if we look at this normal distribution, so here we plugged into our calculator that
the normal has mean zero, it's a normal distribution with mean zero and standard deviation one,
but we get this nice curve here.
Now there's a particular point right here that I want you to focus on.
Notice something that happens to this curve.
It curves up like this, and then it's peaked, and then it comes back down.
It's all symmetrical, in this case around zero.
And let me ask you to look at the way that the shape of the curve changes.
If you start out, the curve goes like this, and see my hand, it's tapering up, and then
it gets steep up.
Do you see how my hand is showing concavity, concave up?
It goes concave up, and then at the peak, it goes concave down, and then it goes down
and concave up again on the other side.
Well at some point it changes from being concave up to concave down.
That exact place where it changes is the value that is precisely one standard deviation from
the mean.
So in the curve where we have standard deviation one, you can see that the place right here
that is marked is the place where the concavity is changing from concave up to concave down.
Any normal distribution will have the property that a certain fraction of the area under
the curve lies within one standard deviation away from the mean.
In other words, there are always, no matter what normal curve that you have, the amount
of material between one standard deviation on the plus side of the mean and one standard
deviation on the negative side of the mean is equal to 68% of the area under the curve.
So the effect of that is that it allows us to get some understanding about, for example,
what fraction of the population is likely to have certain heights if we're looking at
a population that's described by a normal distribution.
Likewise, in a normal curve, you always are going to have 95% of the material that lie
within two standard deviations of the mean, and you'll have 99.7% of the material that
lies within three standard deviations of the mean.
So the effect of that is that you can predict how much material is, for example, further
away than two standard deviations from the mean.
So let's look, for example, at the case of heights of men.
Well, since we know something about the heights of men that they are well approximated by
a normal curve, when we compute their center and we compute their standard deviation, we
can estimate what fraction of men are going to be between 5' 9' and 6' tall, for example.
So if the 5' 9' is the mean height of adult men and the standard deviation is 3", those
are approximately correct, then we would expect that 68% of men have heights that are within
one standard deviation of the mean.
Well one standard deviation is 3".
That means that we would expect between 5' 6' and 6' tall, that's 3' less and greater
than the mean, we would expect that 68% of men are in that height range.
And within two standard deviations, so we go 3' lower yet, so from 5' 3' tall to 6'
3' tall, we would expect about 95% of men to have heights that fall in that range.
Because those heights are the ones that are within two standard deviations of the mean.
And any, if you have different values of sigma and mu, then you will, those different values
approximate the different data sets.
In other words, for heights, the mu was 5' 9", and the standard deviation was 3", 3".
But we saw that you can have a normal, you can have a Gaussian bell shaped curve that
has any center and any standard deviation.
One excellent use for the normal curve is that it allows us to compare items that come
from two different normally distributed collections, and we can make the comparison in a meaningful
way.
And the meaningful way is to measure how many standard deviations an element is above
the mean or below the mean in one group compared to the other.
And let me give, let me pin it down a little bit.
In lecture 16, which is about sports, what we're going to do is try to compare baseball
players from different eras.
In 1920, we saw a graph before, in 1920, the mean was 265, that was the mean batting average.
And the standard deviation was .05, so 50 in the way that you usually think of batting
averages was the standard deviation.
In the year 2000, the mean was very close to the same thing, it was 266, but the standard
deviation was smaller.
It was 38.038.
So if we want to compare the performance of a batter from 1920 to a batter from 19, from
the year 2000, we can compare these two by looking at the histograms of the 1920 batting
averages and the histogram of the year 2000 batting averages and putting them next to
each other in this way.
And look at these two players, Joe Jackson from the year 1920 had a batting average of
382.
Moises Alou had a batting average of 355 in the year 2000.
But remember the standard deviations in the year 1920 was 50, whereas the standard deviation
in the year 2000 was only 38.
So both of these values represent a player who was 2.3 standard deviations above the
mean.
So you see what I mean that by looking at this distribution as a normal distribution,
we can say how far above the mean the performance was of these two players.
So in a way, the Joe Jackson's performance relative to his compatriots in 1920 is comparable
to Moises Alou's performance of 355 in the year 2000, even though the batting average
here was only 355 and the batting average was higher, 382, both of them are 2.3 standard
deviations above the mean.
So in a way, that's a way to compare the data from different normal distributions.
And such a measure that is measuring a value of a datum by how many standard deviations
it is from the mean is called its Z score.
So it's a way of normalizing and comparing where a particular individual is within the
distribution of values.
And let me point out here just with some pictures that this concept of how much of the material
in an under a normal distribution lies between a certain one standard deviation below the
mean and one standard deviation above the mean.
It doesn't matter what the shape of the normal distribution is.
It doesn't matter if it is a squat distribution that has standard deviation two.
So this is one standard deviation above the from the mean.
And consequently, this is 68% of the material under the curve because it's one standard
deviation away from the from the mean.
Here's a standard deviation of 1.75.
And so if we take the material that just is one standard deviation up below and above,
it's still 68%.
So the point is that if you look at standard deviation of 0.75, that's a more peaked curve.
Still the material that's within one standard deviation of the mean is going to contain
68% of the material.
And if it's very, very thin like this, standard deviation of 0.5, then still the material
between that's one standard deviation from the mean, 68%.
So that's why the comparisons you see from things with different normal distributions
and different spreads, we can get a sense of how much material is close to a certain
distance from the mean.
Let's look at the binomial distribution.
We saw in the last lecture had this bell shaped curve to it.
But let's go on to the average value of cards.
Remember that if you have a collection of cards and you just take one card from the
deck, the value of each one is it's going to have a uniform distribution.
If you take two cards and take their average, then among all the collections of two cards
that you take, you will see that the distribution of average values has this kind of a shape
to it.
And when we put more cards in, for example, three card hands, you take all possible ways
of taking three cards from a deck, and then you take the average value where the values
are just 1, 2, 3 up to 13.
You take the average of those three and you're saying, what proportion of those averages
are, for example, 6?
And you say, well, about 15% have value 6.
What have value 7?
About 17 or 18% have value 7 as the average value among all these groups of three cards.
The point is that, and we'll go on, four card hand is even more peaked, five card hand
is even more peaked.
The point is that no matter what you start with, as you take more samples of larger size
and take their average value, you get a distribution that looks very much like a normal curve.
And you can see here that we've superimposed this exact normal curve, which is the smooth
curve, onto these dots, and you can see that the dots line up almost exactly onto that
normal curve.
So in fact, it doesn't matter what distribution you start with, if you take collections of
a certain size, like sample size 2 and you take their average, and then sample size 3
and you take their average, and sample size, in this case, 10, and you take their average,
you get something that is closer and closer to a normal distribution.
And in fact, you can say specifically how wide that normal distribution is.
That is what the standard deviation is of the approximating normal distribution.
And it is the, if you have something of a samples of size n, it turns out that the standard
deviation of that approximating normal curve will be 1 over the square root of n times
the standard deviation that you started with.
In other words, you're getting more and more peaked as you take bigger samples.
And this is going to be crucial when we discuss the issues of statistical inference.
These are actually the workhorses for statistical inference.
And by the way, the fact that you get a normal approximation and its standard deviation is
decreasing by the square root of n as you take samples of size n is called the central
limit theorem.
It's central to statistical inference.
Well, in this lecture, we have introduced the famous bell-shaped curve.
And this normal or Gaussian distribution arises in a lot of settings, like heights of man,
scores on test, measurements.
And when you have many small factors that are contributing, an error pushing it up or
down, you can expect a normal distribution to arise.
One of the main ways that we try to deal with distributions is to approximate a distribution
by choosing one of a well-known family of distributions, such as the Gaussian or Poisson
or binomial or uniform distributions.
And then choosing the parameter values so that we have an approximation that does in
fact fit the data.
It's at the right place on the graph.
It's centered at the right place.
It has the right standard deviation.
So if we have a physical reason for expecting a certain kind of distribution, then our challenge
is to get a good summary, is to find the values of the parameters.
And that's one of the big goals of statistical inference.
If we know the mean and the standard deviation of a normal distribution, then we know for
sure that 68% of the data are going to be within one standard deviation, 95% are going
to be within two standard deviations, and 99.7% of the data will be within three standard
deviations of the mean.
If you can talk about the number of standard deviations above or below the mean allows
you to compare elements from two different distributions, and that's called their Z-score.
That makes a meaningful comparison.
In the next lecture, we're going to talk about correlation and regression.
I look forward to seeing you then.
