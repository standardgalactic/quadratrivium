Welcome back to Meaning From Data, Statistics Made Clear, and this lecture we're going to
continue our four lecture series that concerns the principles and the concepts involved in
trying to organize, describe, and summarize a collection of data when we know all of the
data in a population.
As you recall, in lecture four, we emphasized two things.
One was the measures of center, which were the median and the mean, and we also talked
about measures of how spread out the data were.
In particular, we introduced the concepts of the variance and the standard deviation.
So in this lecture, we are going to aim for discussion of the shapes of the data.
How do we go about describing the shape of the data?
So remember that the big picture of how to describe a distribution was to describe three
things, the shape, the center, and the spread.
Well, so we'll start as usual with our discussion by looking at histograms.
When we take a histogram of a set of data, we get a shape because where the size of the
different bars will describe the shape of the data.
So our strategy in this lecture will be first to identify sort of general characteristics
of shapes, and then we're going to later discuss specific kinds of shapes that actually have
mathematical descriptions.
So the general characteristics of shapes are things like being symmetric, for example,
but then the specific ones will come later, which have particular mathematical formulas.
So let's begin by looking at an example.
Here is an example of a specific set of data.
These are, again, the heights of American males, and you can see that this is an example
of a set of data that is symmetrical about the center.
So it's an illustration of a concept of symmetry, and it could be described by drawing a smooth
curve that approximates it, which is a curve like this.
Now here's another example.
We're just going to go through some quick examples.
This is a histogram of house prices.
The histogram of house prices have a different kind of characteristic shape.
In this case, it's a shape where there are more data values over here to the right, and
the extra values to the right that prevent it from being symmetrical make the shape of
this data skewed to the right.
So one of the terms that we use to describe shapes is being skewed to the right.
And by the way, I always used to forget whether the skewed to the right meant that the big
part of the tall part was to the right, or the tall part was to the left, and it means
actually that the thin part, the long part, is in the direction that you say, like skewed
to the right, means the extra long thin parts are to the right.
And we can once again draw a general smooth curve that gives a sense of a skewed kind
of distribution.
So these are just terms that are associated with general shape characteristics.
Here's another collection, a collection of bimodal distribution.
It has two peaks, and these by, of course, meaning two.
And these were the SAT scores, remember, from the school that gave the test both to seventh
graders and to eleventh graders, and that's why we have these two different peaks.
Well these are generic characteristics of shapes, and they're handy summaries, and they
help to give a sense of the shape.
But sometimes a shape has a specific mathematical formula description, which then gives a lot
of valuable information.
But let me, before we go on to talk about these shapes that have a mathematical description,
let me give a little bit of rather pessimistic view of our enterprise.
And that is that any possible shape is a possible shape of a distribution.
In other words, if you look out at the horizon, and you're in Colorado, and you look out at
the horizon, and you see these mountain peaks, and you say, isn't that a beautiful sunset?
Well, that could be a description of a histogram of a collection of data, right?
You just put little bars at exactly those heights, and that could be it.
Or the New York skyline, the same thing, that could be a histogram of data.
So we have to be realistic about our hopes here.
We're trying to describe every possible shape, and what we're going to do in order to be
somewhat successful is to realize that in real situations, collections of data often
have shapes that have certain common characteristics.
And that's the kind of thing that we're going to be trying to capture.
So let's start right now with the simplest shape that a distribution can possibly have.
And that is a flat line.
That's the simplest.
So let's start with something really simple.
Flat line.
These distributions are called uniform distributions, because it's a completely flat line.
So for example, let's suppose that we take a die, like this die, and we roll it a thousand
times, and we take a record of how many times it came up, one, how many times, two, three,
four, five, and six.
Well, we have an expectation about how the values of those throwing of the dies would
come out.
We would expect to get about the same number of ones, twos, threes, fours, fives, and six.
And in fact, we did this.
And here are examples of four times when we threw a thousand dies each time and recorded
how many ones, twos, threes, fours, fives, and six occurred in each of those four experiments.
Now first of all, I have to confess we didn't really throw the dice.
We used a computer to simulate the randomness, which is useful and easier.
But I just wanted to make a few observations about these results.
First of all, as expected, in all cases of throwing a thousand dies in all of these cases,
we expect that we have about the same number of each of the six numbers.
Now it's possible, by the way, just by random luck that all of them would come out to be
a three.
That's possible, extremely unlikely.
But we expect them to be about the same number for each one, and that's correct.
Now these things are approximated by this distribution.
In our minds, we say, really, we should have them exactly the same.
That's sort of the correct approximation of the concept of the world that is occurring
when we throw dice that are equally likely to come on each side.
And there's a mathematical formula associated with this distribution.
It's a very simple formula.
The formula is that for every value, the answer is the same.
C is a constant.
You get the same value.
But let me point out something about this value in the case of dice.
We take one die, and we throw it a thousand times.
Then what is the expectation for how many times each individual number should arise?
Well it's a sixth of a thousand.
If you divide six into a thousand, you get one hundred sixty-six and two-thirds.
You don't get a whole number.
You get one hundred sixty-six and two-thirds.
Well what does that mean?
That means that the expected value, the uniform distribution that is approximating the actual
answers, cannot actually be the answer for any experiment because the dice never come
up a hundred and sixty-six and two-thirds time.
That's not going to happen.
It's a whole number.
So the point is that the approximation of this experiment is a function whose value is
not ever going to be precise.
And in fact there's a great quotation about models.
What we're describing here are models of a distribution.
George Box said the following quote, all models are wrong.
Some models are useful.
And this uniform distribution illustrates that.
We cannot expect that the model is exactly right, but it's useful because it captures
a concept of what the shape of the data really are likely to be.
And what we're going to be talking about later in this lecture, and in fact as an important
principle of statistics in general, is that if you make certain kinds of assumptions about
the processes that are generating a collection of data, we are led to expect certain kinds
of shapes.
And one thing that we're going to explore now is certain kinds of generating processes
that lead to different kinds of shapes.
And before we go on to that though, let me just make one quick observation.
The uniform distribution is not the uniform distribution.
It's actually a whole family of distributions, namely one for every constant value.
In the case of dice, and throwing them 1,000 times, the constant value was 166 and 2 thirds.
Whereas if we were doing something else, even as simple as throwing the dice a different
number of times, like 6,000 times, then the uniform approximation would be 1,000 occurrences
of each of the numbers.
So it's actually a family of distributions, and that's going to be also characteristic
of the different shapes that we'll discuss now.
So let's take now a different kind of a situation and see what kind of shape distribution we'll
expect to be generated from that physical occurrence.
We talked before about this experiment that we could do of going to a toll booth in the
middle of a road and measuring, say for every Wednesday between 3 o'clock and 4.40, that's
100 minutes.
We sat down at the toll booth, and we measured for every minute how many cars arrived during
each of those 100 minutes.
And then we did that for 10 Wednesdays in a row to get 1,000 occurrences of cars arriving
for each minute.
So then we could make a graph that captured that data where we said, well, how many times,
for how many minute intervals, did no car arrive?
And for how many minute intervals did one car arrive?
For how many minute intervals did two cars arrive?
Three cars, four cars, five cars, six cars?
Well, that gives us a collection of graphs.
And here, once again, and as before, this is actually a collection of four computer simulations
of this occurrence.
And the computer simulation is assuming that on average, six cars arrive per minute.
That is, we just took all of the total number of cars that arrived during that 1,000 minutes,
and there were 6,000 cars.
So the average number of cars per minute was six cars per minute.
But some minutes none arrived, some one arrived, and two, three, four, five, and sometimes
many arrived.
So let's just make sure that you understand that each of these histograms, so you see
this eight here.
What this is saying, the height of the histogram here, which is about, oh, it looks to me to
be about maybe 95, says that there were 95 intervals of these 1,000 intervals in which
exactly eight cars arrived.
You follow me?
And then if we look at, for example, here the number six, let's see, four, five, six,
that's the tallest one.
There were 175 minute intervals in which exactly six cars arrived.
Now these distributions are a little bit different from the distributions, the shape of the distributions,
for example, of heights of adult men.
And it may be a little bit difficult to detect the difference, but these are skewed to the
right.
Our values that come off to the right here, it's a little hard to tell.
But there is an abstract shape that we can figure out by virtue of a mathematical analysis,
making assumptions about what's happening with the cars, namely that we're assuming
that the arrival of one car doesn't influence how many additional cars come.
They're coming more or less independently random times and averaging six cars per minute.
So there's a mathematical formula here, which I will show you.
Here is the mathematical formula that you can deduce from abstract terms that predicts
how many cars will arrive.
That is how many intervals will have exactly K number of cars arriving.
For how many minutes there will be exactly K cars arriving.
So let me give you specific examples so you can see.
So six is the number of cars per minute, the mean of that.
That's the mean number of cars that arrive per minute.
And out of a thousand intervals, we can predict how in how many of those intervals we would
expect exactly four cars to arrive by plugging the number four, you see the number four here
and here, six, the occurrences of the six are telling us the mean overall of the number
of cars per minute.
And then the number four is asking the question, how many of these thousand intervals will
have exactly four cars arriving?
And the answer is 133.85 and this, if we want to know how many intervals we expect to have
exactly nine cars arriving, you know, a little bit above the average, we would expect 68.84.
Now of course, we're not going to have 68.84 intervals because that's a fraction and we're
talking about a whole numbers.
So this is giving once again an approximation and here we have the ideal Poisson distribution
for this experiment and you can see how many cars we would expect, how many intervals we
would expect zero cars to arrive, 2.48.
How many one, you see, so for each one, so and how many intervals would expect exactly
five cars to arrive, 160.62.
So we can now go back to our graphs, our simulations and superimpose the smooth curve that is
the abstract mathematical approximation that we expect to approximate our data and you
can see, you see this is the same red curve that's superimposed on each of these four
data sets and you can see that they all approximate the data quite well.
The value of having a mathematical approximation such as this is that it allows us to analyze
such things as the distribution mathematically.
We can ask questions such as how many times would we expect more than nine people to arrive
at the toll booth during a minute and that can affect our decisions, for example, business
decisions about how many toll booth places you should have on this highway.
So let's, if you change the mean number of cars that arrive per minute, you'll get of
course a different Poisson distribution.
Here you have tens instead of sixes in these two places and once again, this is going to
predict how many, how many intervals you would expect exactly K cars to arrive out of your
thousand intervals if the average were 10 cars per minute rather than six cars per minute.
So once again, you see you have a related shape.
It's a related shape of what your expected shape of the distribution is.
It's related.
This one is the shape for an average of six cars per minute and this is the shape for
ten cars per minute.
So you can see that the distributions, and by the way, here's the simulation data, simulated
data for a mean of ten cars per minute and you can see that we've superimposed these
graphs, the same graph, the model graph onto the actual data and you can see that it fits
quite well.
And this is the general equation for the Poisson distribution that tells, in this case, the
proportion of the intervals in which you would expect exactly K cars if the average number
of cars per minute is lambda.
The fun thing about looking at families of distributions is that it allows you to take
situations in nature and realize that the data that you get from those situations are
likely to be modeled by one member of this family of distributions.
And let me give you an example.
So this is an example that we saw before.
This is the number of deaths by mule kicks in the Prussian army over a period of 200
couriers.
In 109 of those couriers, no deaths occurred.
In 65, one death occurred.
In 22 couriers, two soldiers died for mule kicks, etc.
And we can graph this number of deaths by mule kicks in this way because it has the same
abstract features that the car arrivals did, namely the mule kicks are going to occur randomly.
The effect of one person dying from a mule kick doesn't either increase or decrease the
chance of another death occurring during the next month, say.
It doesn't have an effect.
There are independent events.
And so when you have that kind of situation where the probability of being killed by
a mule kick is basically the same over short periods of time and that they don't affect
each other, they're independent, that's when the graph that you get the distribution is
going to be approximated by a Poisson distribution.
So and sure enough, we can find the Poisson distribution.
This is a curve that comes down here that approximates the distribution.
In this case, the mean is 0.61.
Remember the mean is that that term that was changed.
We looked at Poisson distribution for 6 and we look at the Poisson distribution for 10.
And now this is the Poisson distribution for 0.61.
And notice it, by the way, it has a slightly different character, namely instead of coming
down toward zero at zero, it's higher.
It turns out when the mean is 1 is sort of a break point where the character of the curve
changes a little bit, but it's in the same family.
Another example of where you'd expect a Poisson distribution, a person has done a study of
the number of outbreaks of wars in years for 432 years from 1500 to 1931.
For each year, counting how many wars occurred during those periods.
And you can make a graph and you would expect it to be approximated by the Poisson distribution
in how many years was there no war and how many years one war and so on.
So the point is that when you have a family of distributions, you expect that family of distributions
to occur because it's modeling a kind of behavior that the mathematical formula is capturing.
So the goal here is not so much to think about the Poisson distribution, especially importantly,
but instead to realize the concept of thinking about a family of distributions.
A related kind of distribution are the exponential distributions.
Once again, this is a family of shapes.
The exponential distribution occurs, many of you are familiar with this kind of example.
This is an example that occurs when if you take a radioactive element like strontium 89,
and if you have this strontium, you would expect a certain amount of it to become not radioactive over time.
And it has a half life, meaning that in a certain period of time, in this case, 53 days,
you would expect half of the material to have decayed and quit being radioactive.
And an individual atom, see, has no memory.
It's probability of becoming non-radioactive in the next 53 days is 50%,
but if it's made it through 100 days already without turning non-radioactive, it doesn't have any memory about that.
It's probability is still 50% that in the next 53 days it's going to decay.
So that's under those kinds of circumstances, one finds an exponential distribution.
And there are, once again, many different situations in which you would expect one member of the family of exponential distributions.
It's not just one distribution, there are steeper distributions.
Namely, instead of the half life being 53 days, suppose the half life were one day,
then you would have a steeper decay of your material than you'd expect to have less material quicker.
Another family of distributions that is very important are the binomial distributions.
And this is another family of distributions, and let's illustrate it by an example.
About two-thirds of students who enter high school in the United States actually graduate from high school.
That's roughly correct.
Now, suppose that we are having a reality TV show, or we're constructing a reality TV show,
and so we need a lot of participants to start, and we want to get a random group of 50, say, 20-year-olds,
and we take this group of 20-year-olds, and we might want to know the sort of education level among those 50.
So we might want to ask the question of how many of these groups of 50 are going to have how many graduates?
So in a lot of the groups, about 33 of them will have graduated from high school,
because two-thirds of 50 is 33 and a third, and that's the sort of average number you'd expect.
But in some of the groups of 50, there won't be any graduates, right?
And in some groups of 50, all of them will have graduated.
So what we can do is we can consider all the 20-year-olds in the country, that's one big population,
and consider all of the groups of 50, every possible group of 50 that we could select.
And for every group of 50, we could ask the question, how many graduates are in that group of 50?
And then we can make a histogram that records that information here, which gives us, basically,
for every number between zero and 50, it tells us how many groups of 50 have that many graduates in it.
So for example, if we looked at 30, we'd ask ourselves, how many of these groups of 50 have exactly 30 graduates?
The numbers are absolutely enormous. They're astronomical.
The number of groups of 50 from the population of 20-year-olds would be so enormous that it would be meaningful to us.
The numbers would be too big.
So instead of that, we measure what proportion of all of these groups of 50 have exactly 30 members in them.
So if we take 30 and we go straight up, I think it's this one here, it shows us that about 7% of the groups of 50,
of all possible groups of 50, have exactly 30 graduates in them.
And if we go up to 31 and go up here, and 33 would be one of these top ones, about 12%,
.12 is about 12%, or a little less than that, of the groups of 50 would have exactly 33 graduates in them.
Well, there are many situations in which we would expect data to have this kind of a distribution.
And by the way, this is an example of a binomial distribution.
Whenever we look at all possible samples from a given population, we will get a binomial distribution.
For example, if we consider all collections of 50 cars in the country, and for each collection of 50, we count how many are blue.
We'll get a binomial distribution.
If we consider all the collections of 150 people in the United States today and ask how many of them have a cold, we'll get a binomial distribution.
Consider all the possible ways that if you flip a coin 15 times, it could land in certain ways.
Here's a chart that would show some examples.
If you flip a coin 15 times, you could get this outcome, a head, then a tail, then a head, then two, you know, several tails, and then heads and so on.
And for each one, for each possible collection of 15 coin flips, for every single possible one, you could count how many heads there were in that collection.
And if you did, you would get a binomial distribution that was a histogram that said what proportion of all possible groups of 15 has zero heads, one head, two heads, three heads, four heads, five heads,
and the middle of the distribution would be centered right about at between seven and eight, you see, because you'd expect about half of them, most of the collections of 15 would have half of them heads.
But there would be some fraction of the distributions that would have 10 heads or 15 heads, like this one has 10.
So if we take a collection of a size of 1600, so this is an example, again, of a binomial distribution where we have an election coming up,
and we ask 1600 people from a large population, how many will vote yes on a particular proposition?
If 60% are actually for the proposition, we would expect a distribution that looks like this.
It would be peaked at the 60%, which is the correct answer, but there would be some of our collections of 1600 that have fewer and some have more.
And this is just a blow-up of that same distribution that shows that most of the high part of the distribution lies between getting 920 of the 1600 who are actually for proposition B.
The binomial distribution has a complicated looking formula here.
The point of showing this is just to show that you can actually compute the proportion of the samples that has exactly k elements in each of the samples.
And then this is a chart that just shows that if we have a sample size of 10 and 60% of the people are safe for a proposition,
we would expect this kind of a fraction of those samples of 10 to have 6.
There would be about 30% of the, let's see, 25% of the samples of size 10 would have exactly 6,
but 22% would have exactly 7, and we'd have about 20% that have exactly 5 among samples of size 10, and here we actually have them.
So in this lecture, then, we have explored ways of describing the shapes of distributions.
And there were generic features that we could describe, namely that a distribution is skewed or that it's symmetrical or that it's bimodal has two peaks.
And then we talked about specific families of distributions that were modeled by actual mathematical formulas.
And these included the uniform distributions where we could have just flat line distributions or the Poisson distributions that arose in particular circumstances of random arrivals.
And then we had the exponential distributions that arose, for example, in exponential decay of radioactive materials.
And then we talked about the binomial distributions that arose from the idea of taking samples from a set.
The basic strategy for describing the shape of a set of data, then, is to find some sort of mathematical model that approximates the histogram of the data.
In our next lecture, we'll explore the most famous shape of the distributions of them all, namely the bell-shaped curve.
See you then.
