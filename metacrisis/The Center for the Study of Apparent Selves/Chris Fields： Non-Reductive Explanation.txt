Okay we're starting. So next we've got Chris Fields. Chris is a faculty member at our
Allen Discovery Center. He's going to tell us about non-reductive explanation. Okay thank you Mike
and thanks Thomas for letting me make some brief remarks about this kind of ad hoc.
I wanted to give this presentation after hearing Richard's presentation yesterday
and then engaging in some discussion of that presentation from Richard last night over dinner
and just briefly I want to talk about the structures, the structure of non-reductive
explanation and use the free energy principle as a particular example of a kind of non-reductive
approach to science. And to start I'll just say that I think as scientists and as the broader
community we've been exposed to a persistent mythology that originates really in the 1930s
with people like Carl Hempel that science has to be reductive and therefore science is reductive.
It is because it has to be not the other way around and we therefore in a way give lip service
to science being reductive even though we don't think it is and even though it isn't in fact.
And I actually think that most science isn't reductive but even though it isn't reductive
people talk about it as if it is reductive even the people who are doing it because they think
that's what they're supposed to do. And so when we think about Richard's slide which I didn't have
time to actually replicate and I'm not sure that I could replicate it quite as artful
a way that he does where he shows science leading us into this kind of abyss of reductive
meaninglessness or whatever. I think this is a construct that in a sense has been
projected onto science by philosophers and scientists have bought into it even against
their best instincts. So I'm using slides that I put together for a talk that I'm giving in a
few weeks for a quantum information theory class that I'm doing online. So there's a little bit
of background that I'll include here but it's just background to bring you up to speed on some
of the vocabulary. So I'm doing everything in the context of a very simple setting in which we use
quantum information theory. And the extremely simple setting is one in which there are two
systems you can call them agents if you want to. I always think of them that way that are
exchanging information across some mutual boundary. And these two agents make up the
entire universe of interest so we can treat them as an isolated system. We can treat the joint
two agent system as an isolated system. So each of the agents is obviously an open system. It's
interacting with the other agent and the interactions defined at this particular boundary.
And the agents are always called Alice and Bob in quantum theory.
And we were able to formulate a couple of years ago working with Mike and Carl and some other
people. The free energy principle in this super simple quantum information theory
theoretic setting. And the way that we ended up being able to formulate it is that two interacting
systems will tend to behave in a way that familiarly allows them to maximize their
predictive power. What are they trying to predict? They're trying to predict the information that
they'll receive across this boundary. How do they do that? They do that by mutually evolving
toward a shared set of observation and action capabilities which amounts to sharing a language
in some informal sense that could be made more formal. But we won't worry about how to do that.
Now to the extent that agents perfectly share their observation and action capabilities
so that they in fact perfectly reproduce each other's inputs, it turns out that they are then
by definition no longer conditionally independent. So they're entangled. Now what that means is that
the free energy principle is actually a classical approximation of the principle of
uniterity which is just the principle of information and conservation which is the core
principle of quantum theory. So the free energy principle when it's expressed in quantum theory
becomes a classical limit if you will of this very general principle that in fact defines
this as quantum theory. So it's kind of nice in that it tells us how the free energy principle
which was originally formulated in neuroscience and then classical physics fits into this deeper
if you will theoretical framework. And a nice outcome of this is that constraints on observation
and in particular the constraint that one can't deploy all of one's operational or
action capabilities simultaneously. That constraint gives you effective non-commutativity between
your observations and actions. So you can observe an act this way or you can observe an act this
way but you can't do both. As soon as you're in that situation it induces compartmentalization
because if I can't do both at the same time I can't regard them as the same operation
which is what they would be if they commuted. And so I have to draw a boundary between them
and that enforces internal classical communication. It enforces time delays such as Adam was showing
us are very critical and so it effectively creates a multi-component agent and induces these new
internal communication pathways. And one outcome of that is you automatically get some sort of
control system which allocates free energy resources to the different components so you
get something that looks a bit like attention. So this is all just background for the theoretical
setting. What I really want to talk about is explanation strategies and ask the question
what's the difference between reductionist theories and scale-free theories and the FEP is a
I think canonical example now of a scale-free theory. So reductive theories are all based on
this idea that what little things are doing explains what big things are doing. And that
because the behavior of the little things explains the behavior of the big things,
the behavior of the big things is in some sense emergent or epiphenomenal or not very interesting
or it doesn't matter or what have you. So this is the reasoning that gives rise to
Richard's dissent and to the abyss. That what we actually see at our scale doesn't really make any
difference because what's really going on is Adam's and the boy or something like that or
Kline fields or who knows what. And so the core assumption that any reductionist approach is making
is that there is some fundamental scale and at that fundamental scale what's happening is interesting
and that there are a bunch of fundamental laws or dynamics or mechanisms or something
that are defined down at that fundamental scale and that once you're given that you've got everything
else. So that's how reductive theories work and the philosophically precise formulation
of reductionism is that there are actually laws of nature that allow you to infer from
this micro-scale behavior up to macro-scale behavior and by saying laws of nature I mean
not just an inferential procedure that we came up with but rather an actual law that's been there
since the big bang. So nature itself does this inference to the macro-scale and there's nothing
we can do about it. So let me contrast that with a scale free theory. So in a scale free theory
what that what it means to say that the the theory of scale free is that exactly the same
theory applies at multiple scales. So for example taking the free energy principle as an example
little things behave according to the free energy principle and big things behave according to the
free energy principle and I can write down the free energy principle and that's it. All I have to do
now is change what the variables refer to and I can have a micro-scale description, a mesoscale
description, a macro-scale description. They're all exactly the same dynamics applying at different
scales. So what does that tell us about systems as opposed to dynamics? Well we've got these
macro-scale entities. We've got a bunch of micro-scale entities. There's got to be some sort of
relationship between them and the only plausible relationship is some kind of relationship that
we can bundle under the term implementation fairly broadly conceived. So we know for example that
our bodies are full of cells and those cells are executing some sorts of behaviors
and we know that somehow what the cells are doing is is implementing something about how
we are behaving. Of course I'm stealing this term from computer science and we'll come back
to computer science to define it but I'm relying right now on a kind of intuitive notion of what
implementation means that these parts are behaving in ways that
in some sense manifest in the behavior that we see at the macro-scale but their behavior is not
our behavior. Their environment is not our environment so on. They're experiencing and
thinking about a completely different environment than the environment we're experiencing and
thinking about in free energy terms. So the model for scale free theories is as above so below.
Whatever a theory that you come up with to describe the macro-scale
is going to be the area of the micro-scale and vice versa.
So what matters then if we go back to this picture I showed you at first of agents interacting
across a boundary what distinguishes the scale is just how densely information is encoded on this
boundary. If you want to think of the boundary as a spatial structure it's not necessary to do that.
The theory is in fact very compatible with the idea that space is something that emerges on the
boundary and doesn't exist anyplace else. So it's very consistent with a picture in which
there's no such thing as observer-independent classical information and no such thing as
observer-independent space hunt. But if you think about the boundary as having a scale then we can
treat it as an energy scale, a length scale, a time scale or any combination of those. So if you
think just in terms of physics you get a picture like this where you have the Planck energy which
is very large much much higher than we can achieve with say the LHC and it corresponds to the Planck
length which is very small and if you have that kind of encoding at that density then
the interacting objects are black holes by definition. And you can decrease the energy
scale which corresponds to increasing the length scale and so our natural scale is about a meter
in size and so our natural energy scale is this very small energy
10 to the minus 5 electron volts which is tiny, tiny, tiny. And if you compare that to say the
energy scale of visual photons, visual photons are 10 to the 5th times that they're about an
electron volt. So it's no surprise that light seems very energetic to us because it's much,
much larger than the energy associated with the scale of beings that are our size. So scale
is this thing that we can manipulate and if we think about a scale-free theory it's just a
theory that applies anywhere in this picture. So you know we're all very familiar with a picture
like this. This is a classic reductionist theory. It says look all the action here is
on the left hand extreme the very initial instant of the Big Bang that's occurring within roughly
the plank time and everything happens then and then we have this inflationary period that just
makes things bigger and then things become a little bit bigger and more diffuse and there's not even
light at that point and then we start to get the formation of atoms and and elementary particles
and then atoms and then conglomerations of atoms and on and on and on and on. And all of that physics
has to be encoded in this initial instant of creation of whatever you want. And so people
construct bouncing cosmologies and things like that where these sorts of instants are repeated over
and over again and you get different physical constants in each one and on and on. But all of
this is a reductive picture. All of this is saying the action in the universe is at this tiny scale
and that's all that really matters. Everything else is emergence where emergence just says
some process happened but how that process unfolds is actually embedded in these initial conditions.
So let's contrast that with a kind of explanation that's truly scale-free. It doesn't look
anything like this. And to do that contrast I have to draw a picture. So let's think of this
physical system that the observer Alice is observing. She's observing the state of something
that we've called Bob and that state evolves by some physical process. But Alice doesn't see that
evolving state. Alice sees what Bob writes on her boundary which is this script B. So
Bob might write on the boundary using some procedure that I've labeled procedure one
in which case Alice sees some encoding that Bob has written and it evolves through time.
And the blue upper arrow here is just the process by which Bob writes on the boundary.
That's his action process which Alice can't see. She just sees the boundary.
And Alice can come up with a theory of how the boundary evolves. And that's going to be some
sort of Markov process for example because we're talking about mapping some bit array
into some other bit array, some data into some other data. So if she can develop a theory
that's at the scale of this encoding. So that can happen but it also can be the case that Bob uses
some other process to write on the boundary at a different scale with a different scale of encoding
which produces a different bit array that has a different dimension. And that bit array can also
evolve through time. And Alice can construct a theory of how that bit array evolves.
And now the question becomes how does Alice relate these two theories?
Say a very high density theory that would correspond to
micro scale events and a very low density theory that would correspond to macro scale events.
Well Alice has to come up with a map that says objects at this micro scale have some sort of
relationship to objects at this macro scale because I'm observing the same boundary.
These data are being produced by the same process but their data at different scales.
And that map, that map between scales we would like to remain constant.
Otherwise the world really doesn't make sense at all. And that map is essentially a semantic map
because it relates what look like objects and processes at one scale to what look like objects
and processes at another scale. So I will claim that this process of looking at multiple
distinct scale representations of the behavior of one system
and developing semantic maps between them is extremely familiar to us.
And in fact we deal with that mapping all the time because this is exactly
what we do in computer science. It's exactly the process of observing a device,
you know, something like that. And observing it at multiple scales
and looking at different mappings from some underlying physics that we don't observe,
right? We're not constantly looking at our laptops with electron microscopes or the LHC.
We're not examining the micro scale behavior. We're only comparing different
kinds of macro scale behavior even if we're looking at the relationship between
what's happening in the transistors and what's happening on the screen. Those are both macro
scale descriptions. And when we think about programming languages, when we think about
compilers, for example, we're building exactly these kinds of semantic maps between these
representations of some underlying dynamics that are defined at different scales.
So computer science itself gives us a model of non-reductive explanation.
You know, what this allows me to do is very different from what
understanding the circuitry allows me to do. For example, Mike was telling
a story that I've heard him tell before about an applicant for a software engineering position
who only talks about electrons. This is not the person who's going to be a very good programmer,
unless of course they're a god of some kind. We're not interested in the behaviors or theories
generated by gods. We're only interested in behaviors and theories that are generated by
observers like us. So this is how multi-scale interpretive theory always works. It's always
a process of constructing semantic maps. So we can ask now about what's the relationship
between some tower of semantic maps that we build to understand how things relate
across scales. And here's the kind of tower that you might get in biology if you're applying the
free energy principle at multiple scales from the scale of interacting molecular pathways
within a cell up to the scale of ecosystems kind of colliding in some biosphere. And
I would advance the hypothesis at least that for biological systems the relationship between
these scales in a sense is trivial. That not only are we executing the free energy principle at
each scale, but the relationships between scales are even formal equivalents so that we would expect
to see things like cooperation, competition, coercion, deception, etc. at any scale from the
scale of pathways, certainly at the scale of cells, organisms, social groups, and even perhaps
ecosystems. So it would make sense in a picture like this to do kind of what Adam illustrated in
the previous talk, which is to apply these sociological concepts to, for example, cell biology.
Why not think of cell biology in terms of communities of agents who want to communicate
with each other but who may not quite want to reveal everything, who may have different levels
of trust, or who may actually want to take control and tell other cells what to do.
And Mike and I have explored these sorts of ideas in various papers,
but I think it's at least worth entertaining the hypothesis that in biology we have this sort of
not just scale-free behavior, but in a sense metascale-free behavior that even the semantic
maps are scale-free. So I'll just close with this comment that if physics and computer science
are actually the same thing, then our job as physicists is to build a physics that describes
the behavior of physicists and in particular build a physics that explicitly describes
the processes of observation and action. And this is why I think quantum information theory is such
a huge advance and why I think it will actually change the way quantum theory is viewed and used
and maybe finally break the hold of shut up and calculate on physics,
because it forces us to come to terms with interaction and observation being the same thing
and having the same description. And of course it exposes us immediately to the gentleman depicted
at the bottom who reminds us that consistency and completeness are in interesting theories,
not actually compatible. And I think an open area of exploration and one thing that a colleague and
I am working on is to what extent are interesting theories undecidable in girdle sense. So thank
you. I just wanted to make those remarks kind of in response to what we've heard so far in this
meeting. Thanks a lot Chris. I really enjoyed that really important points I think. I'll start
with a comment which follows from the comment I had also about Adam's talk which is on the use of
terms from sociology or psychology and so on which tend to be quite loaded and which you might intend
a particular way but listeners or readers might take another way that isn't licensed by the actual
things you're looking at. And I think there's a tendency when we see the same type of behavior
at different scales to say that the thing at either the high scale, the psychology and sociology
that we see is just this mechanistic thing happening. It's sort of a reduction or deflation
of the concepts like a belief or so on. Or we ascribe beliefs and desires in a kind of anthropomorphic
way to simple systems. And I think an alternative to that rather than saying A is a type of B or
B is a type of A is to just say A and B are both instantiations of C in a different way.
So it gets beyond I think the concerns that misunderstandings that can arise from either
the deflation or the inflation of things at either end. So that's the comment. The question is whether
to get past the idea of causal comprehensiveness or completeness at the lowest levels of supposedly
the fundamental levels of forces and particles or fields or whatever you need to have indeterminacy
at that level. And so that's the, I think that's true. I don't think you can get away from reductionism
if determinism at that level is true. But I know not everybody thinks that's true.
Okay. I'll actually comment on both of your comments. One is natural languages are obviously
necessary. But they're also a terrible hindrance to communication. And mathematics is nicer
from an ambiguity standpoint. But it also raises similar semantic problems,
which for example is evidenced by the measurement problem in quantum theory, which is a semantic problem.
So I think we're just stuck with this. And yes, we can wriggle around and try to get around it and
try to say things in ways that are very clear. But language is
deeply insufficient for communication. So that said, to your question in indeterminacy,
I am among the camp that views all uncertainty as observer relative.
And I, you know, I actually go farther than that and claim that all classical information is observer
relative. So in that sense, I see uncertainty as being fundamental
in a certain precise sense, which is that, and let me go back to this first picture actually.
Okay.
If you have a system, its dynamics can be perfectly deterministic.
Its dynamics can be perfectly information conserving. So if you think of a universe
which is closed, so it's not interacting with anything. So there's no information flowing
into it. And there's no information flowing out of it. Then that's heaven, right? That's a place
where nothing ever happens. So there's no uncertainty, but there's no observers either.
As soon as you slice it, and you say, okay, I've defined a boundary, and there's
there's one system on one side of the boundary and one system on the other side of the boundary.
Then those two observers have deep uncertainty. And not only do they have deep uncertainty
about the future, they're extremely powerful, no go theorems that immediately apply.
So if you have this situation with Alice and Bob, then it's easy to show that
Alice can't determine Bob's dimension. She can't even tell how many degrees of
freedom are out there. She can't determine the dimension of her boundary.
She can't determine whether she has a conditionally independent state.
So Alice doesn't even know that she's not entangled with Bob.
So it's not just a matter of statistical uncertainty now. There's deep, deep epistemic
uncertainty. So the answer about uncertainty is yes and no. It's fundamental and completely
observer relative, at least in my view.
Great. So let me add one thing to that.
Lots and lots of theories, especially in the popular press,
appeal to randomness as some sort of explanation. Randomness is never an explanation of anything
in this view. There's no such thing as objective randomness.
Thanks, Chris. I'm going to go over here.
First of all, I disagree with that, and I'm looking forward to talking about that last statement,
because I think there is ontological uncertainty, and I think it's prevalent,
but that's a conversation for another day. I've thought about the renormalization
group. I've thought about that theory a little bit, and I admit to fundamental ignorance. It's
very hard stuff, so I'm going to maybe misrepresent it a little bit. It seems to me that if I understand
it correctly, when you apply renormalization theory, what you're saying is there are a class of,
if you want, systems that can all be understood according to the same kind of general rule framework,
but there are many, many such classes, and certainly not all systems can be renormalized
to all other systems. While I appreciate the
goal of trying to find classes of systems that across scales of time and space
are fundamentally similar according to whatever block diagonalization you're going to apply,
those sets of sets are very specific. I guess my question is, what's the
explanatory power do we get out of, first of all, making the statement that there is scale
invariance as a general statement, and secondly, how do you do the work to figure out which
systems are well-conduced to scale invariant explanations without doing something like the
hard work of mathematicalizing them all and testing against a renormalization group? I'll end
there, but also to recommend a philosopher called Robert Baderman, who's done a lot of work on this.
His interesting most introductory stuff is to say that there are certain things where
the mesoscale is what matters. For example, steel beams, what you really need to look at
is the domains of atoms and anything smaller than that doesn't matter. When you do multi-scale
modeling, you often find that there are scales that matter and that they're different from other
scales. I think some of your work, actually, Mike, on information theory also indicates
that in time-length scales, there are some places where you really get more bang for your buck
by looking at a particular scale than trying to make a scale-independent argument.
Oh, yeah. I'll be like Mark here. I'll also say there are two responses here.
Response one is I completely agree with you. There's no substitute for actually doing the work.
I'm trying to describe here what it would be like to have a scale-free approach to
explanation as opposed to a reductive approach. What would we expect that to look like in principle?
But yes, applying things like the free, we can argue that the free energy principle is completely
universal. But what does that mean? It doesn't mean much of anything until you actually apply it
in some system and say, how do things really work? For example, what atoms trying to do?
To get to your other point about, say, causal efficacy being at a particular scale or being
primarily at a particular scale or the information that's useful being at a particular scale,
I think this is always relative to the statement that I'm an observer. I have particular goals.
They may be explanatory goals. They may be instrumental goals. I'm looking at this system
and I'm evaluating models that I can construct of that system at different scales.
So this is taking this very simple picture and extending it very far in the direction of
describing real science, but very far in the direction that's actually very difficult
if you want to carry through with it. Because it requires saying, how do I make measurements
of this physical system at all these different scales? And how do I make measurements of what
I see as the surrounding of that system at these different scales? What do I define as noise?
So I'm trying to make observer relative here all of the things that we typically,
for very good reasons, treat as objective. And that's extremely difficult. And I don't know
whether we'll ever be able to actually pull much of that off from the theory point of view.
It may just turn out to be hopelessly attractable. So we may always be forced back into this
very convenient instrumental way of thinking of things as objective.
And even representing them as objective in our theorizing, even if we know that that is
not really right or not really justified by consistency within some deep framework.
I mean, this may just be Gertl's revenge or Heraclitus's revenge.
Good question. Richard, please.
Thank you. Thank you so much, Chris. I really wish I was there for
evening conversations and lunchtime conversations. I'm really missed being there with you guys.
So I'm totally on board. My inclination is towards a scale free cognitive model of everything,
you know, cognition all the way down sort of thing. So I'm totally on board with a scale free
framing of things. I wonder, just so a couple of points on that you can just pick up on whatever
you feel like. So of course, we could have a multi-scale theory that wasn't
invariant over scales. And actually, the programming one illustrates that point too. So
high level programming language way of describing what's going on inside my computer
is a legitimate way of one particular scale of describing what's going on.
And the level of electronic gates is another way, but they're not the same theory, I don't think.
Anyway, of course, it's neater if it is the same at all scales. I think it's fair to say that the
majority of science, the cosmology, biology, neuroscience, we don't is not scale free, though
it does suppose a lowest causal scale that's privileged in how things work.
But more importantly, obviously, I'm open to desperate to do science in a different way,
and I don't want science to be tied to being done in a reductionist way.
So I have questions about now more specifically about free energy principle and a scale free
application thereof. And this is just sort of almost a stream of consciousness thing.
So questions like, is a scale free free energy principle still ultimately tied to an idea of
what persists exists? Is it still ultimately tied to a fear of me not existing and a contraction of
care? Is it still based on natural selection principles? I think not because you talk about
social groups and ecosystems doing the same kind of scale free stuff, which natural selection
doesn't apply. So it's not driven by that. Does it avoid an eliminativism of the real
phenomena or questions we have? Does we're all just trying to minimize free energy at all scales?
Does that statement help us find reason or affect or purpose or love or meaning or avoid
a crisis of meaning or expand a scope of care? So I'm totally fine with it's
science doesn't have to be reductionist. I wonder whether the real offering that the
free energy principle or your way of thinking about things has to offer might be more to do
with not just that it's scale free, but more to do with things like the observer dependence and
observer relative rather than being not reductionist. Yeah, I think a great benefit of the free
energy principle way of thinking and this is a benefit that I think wasn't recognized for a long
time is that it's intrinsically a theory of multi-agent communication and it may be very
asymmetrical. It may be computationally very asymmetrical multi-agent communication,
maybe semantically very asymmetrical multi-agent communication. It is at the quantitative level
informationally symmetric multi-agent communication, but it does
cast questions in science into this communicative framework and that's what in fact most closely
relates it to quantum information theory, which does the same thing. So I think that is a big
change in the way we think about things and you know, as some of us were saying last night,
there's a big distinction between doing science as a meaningful activity and reading about it in
the newspaper and I think a lot of the meaningfulness of science is lost on people who read about it
in the newspaper and that's a question that we can think about trying to improve the situation on.
And I'm not sure that science itself can certainly help in that process, but a theoretical
formalism is not the answer to that particular problem.
Just briefly if I could, I think there is a way in which Richard's
use of reductionism actually still applies, I think in this context and this is a different
use of that term that comes, it's very widespread in religious studies, but you'll also find it
across other disciplines, cultural history and so on. And so one way, just a very quick example
for this is let's say I'm trying to account for what we mean by religion, like what's the
essential feature of religion, what's the essential feature of religion, what's the
essential feature of religion, and a typical modernist account where that would be that it is
about an individual, a particular kind of experience that individuals have that involves
in some fashion knowing or experiencing or embodying the divine, the sacred god, what have you.
So that scale of analysis is actually at the individual, but then one can come along with
a Durkheimian account of religion as a social phenomenon, and the scale of analysis is not
the individual, it's actually a social group. And that would be called reductionist,
which is admittedly kind of odd because the scale is actually larger, but that's called
a reductionist explanation because what it's done is it's taken a phenomenon like an entity,
it's kind of ontologically deflationary, second entity like the sacred or god or the divine and
said actually we don't need that, so it's a limitivist, we don't need god or that or the spirit
or whatever. This thing that we thought gave us meaning, for example, we don't need that,
we have a better explanation, which is the Durkheimian social explanation of religion.
So that's called reductionist because it is doing, making the same thing,
so that's called reductionist because it is making this limitivist move,
and the point, the kind of general point then becomes science's reductionist in the sense that
it's eliminating the existence of these entities that we've been using to produce meaning for
ourselves. And so it's not about necessarily a scale here, it's actually this kind of ontological
move, in the sense that our traditional means, the entities that we've used to produce meaning
in our lives have all been eliminated. So it's a peculiar use of reductionism, but I think it's
important for us to know that in this sense what Richard is saying from the standpoint of religious
studies still very much holds, which is that in that limitivist sense, the reductionism of science
has led to a crisis of meaning. Okay, that's really cool because I always use the term reductionism
in this sort of philosophy of science sense, and I've always wondered what people were talking about
when they talked about reductionism in a way that obviously had no overlap with
the old Carl Hempel et al. sense of reductionism, because they defined it very precisely in terms
of bridge laws and all that stuff, none of which I think makes any sense, but that's how they defined
it. So okay, so I can understand that the word reductionism could be used to mean what
eliminativism means, and yes, this doesn't talk about that at all, right? And I would think of
the case that you describe as a sort of a case of theory replacement, right? All theory replacements,
maybe that's too strong, at least many theory replacements are in some sense eliminative.
So there's a deep sense in which quantum theory, for example, gets rid of the idea of particles.
So they're gone, right? It eliminates that idea. It's still a very useful idea. We still use it
all the time, but we only use it with a certain amount of trepidation because we know that it
doesn't really apply. So it seems to me that what you're describing is a case of theory replacement
and that has kind of the generic features of theory replacement in terms of being
eliminative. And there are always people who are deeply attached to
concepts from some theory or other. And if they're replaced, yeah, that can be difficult. But
I guess I would see that as what's happening in this case.
Okay. Thanks again for the opportunity to do this. I appreciate it.
