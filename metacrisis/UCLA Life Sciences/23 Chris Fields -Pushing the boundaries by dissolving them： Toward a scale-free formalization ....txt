What I want to talk about today is how much can we learn in a very generic sense about
biology from looking at very simple physical systems.
So I want to start with the free energy principle that Carl Friston introduced in the very first
talk, and I formulated it here in a way that looks as much like a tautology as possible,
so that you can see just how simple this principle is. It basically says anything that persists
has got to behave in a way that contributes to persistence, and if it doesn't, it stops persisting
and that's that. So I want to study the effects of this in the simplest possible physical model,
and whereas Carl presented the FEP in terms of classical dynamical systems theory,
I'm going to use the language of quantum information theory because it's much,
much simpler, and I'm going to think about the simplest possible situation, which is some system,
which I'll call S, that interacts with some environment, which I'll call E, through a boundary,
which I'll call B, and this boundary is a boundary in the total joint state space,
so in quantum theory that will be a Hilbert space, but you can think of it as just the collection
of all of the degrees of freedom that are of interest, and I'm going to make two
simplifying assumptions. Assumption number one is that E is everything that's not S,
so if I think about my interaction with my environment, my environment is just not me,
so that includes everything that isn't me, and this simplifying assumption buys us
something important. It means that the joint system is isolated because I built anything
that we could consider to be an extra component into E, so we just have a bipartite decomposition.
The other simplifying assumption I'm going to make is that everything inside is finite.
What does that mean? It means that the energy flows across this boundary have to be finite,
so no infinite energy flows, so no singularities, and stuff like that,
so I'll ask two questions. What does this tell us about the structure of S, and what does it
tell us about the structure of the interaction, and I'm going to ask these questions just by
looking at simple symmetries and then how they're broken, so here's symmetry number one.
The FEP is a universal statement, as Friston showed in his 2019 paper, and since we've shown
using the quantum theoretic formulation, and what that means, what universality means here,
is just that I can exchange these labels, and I can label the left side system E and the right
side system S. All that means is that because this is an isolated system, the thing I previously
called S is the environment of E, and so that means the free energy principle applies to both of them.
Both of these systems are acting to maintain the integrity of this boundary that separates them,
because it's maintaining the integrity of the boundary that makes them persistent systems.
If I take the boundary away, both systems disappear. All I have is the joint system left,
so persistence requires the integrity of this boundary, and that's what the systems act to
assure. So symmetry number two is conservation of information, and I can appeal from a quantum
theoretic point of view to unitarity, since this is an isolated system. Unitarity is just the
principle that information is conserved, but we can also think of this in terms of Newton's third
law. Anything that S does to E, it can expect an equal and opposite response, i.e. for E to do
something to S, and since these are energetically balanced, they're informationally balanced.
The other perspective on this, which is very useful, is because I've required that everything
inside is finite, I can use the holographic principle, which was originally developed for
talking about black holes, but what it says is that the information coding capacity of any
boundary is finite, and quantitatively, it puts a number to the minimum size for encoding one bit,
but we don't need to worry about that here. In the case of a black hole, it's four-plank areas,
which are very small, and that defines a black hole, actually. Anything that has that encoding
capacity as a black hole and anything that has a smaller encoding density is not, but what this
does by requiring a finite information capacity is it induces a pixelation on this boundary,
so that creates a discrete topology mathematically, and it gives it a finite dimension,
and we know exactly what that dimension is. It's the dimension of the interaction operator
which I've called HSE, that's just a Hamiltonian operator, and it also tells us exactly what
information the boundary encodes. The eigenvalues of HSE are indimensional numbers, or have an
indimensional binary representation, and so we can think of the boundary as encoding the eigenvalues
of the operator that crosses it, and eigenvalues of the Hamiltonian are energy values, so any of
these interactions boundaries actually encode a representation of the energy that's being exchanged
across the boundary. Now this tells us something very important, which is that S can obtain exactly
in bits about E and vice versa, so the amount of information that the system can get about its
environment is exactly in bits and vice versa, so the environment can learn exactly in bits about
the system, so that's a very powerful symmetry. Symmetry number three is a little bit more subtle,
but not a whole lot, and it depends on this fact that if S and E are separable,
which in quantum theory means if they're not entangled, but classically the analog is if
their states are conditionally independent, which if you recall from Carl's talk as a requirement of
the FEP, then the dimensions, which you can read just as the number of degrees of freedom,
but formally it's the Hilbert space dimension, of the system and the dimension of its environment
must be a lot bigger than the dimension of the boundary, otherwise they'll cease to be
conditionally independent. Now if that's true, we can write this operator, this Hamiltonian operator
that represents the interaction in a particular really simple form, which is a bunch of thermodynamic
stuff, KT and a constant, times a sum of simple operators, each of which you can think of is
acting on one bit, and I can, is there a back button on this I hope? So there's a picture of what
these M operators look like, each one acts on one quantum bit that's in the boundary,
and it reads one classical bit off of that, and then prepares that some classical bit that the
system wants to prepare. So it's a very simple representation of the operator, and since this is
a sum, it means that I can rearrange these M operators any way I want to, I can effectively
relabel the indices any way I want to, and what that does is just swap what ones and zeros are
written in what positions on the boundary, and since the boundary encodes an eigenvalue of energy,
when I move ones and zeros around, I change the eigenvalue, so I change the value of the energy
that's coming across the boundary from the perspective of the system. All that means is
that I'm moving the zero point of energy up and down, and I can move that point up and down
arbitrarily without changing anything about the physics. So this exchange symmetry doesn't do
anything to the interaction, it just changes the numerical value that's associated with it on one
side of the boundary, and this is actually required for separability. If I lose this ability to freely
choose what basis I'm operating in, so how I've labeled the boundary with bits,
then it turns out that induces entanglement, or that's just a way of defining entanglement,
so I have to have this free choice of moving things around. Okay, so those are three simple
symmetries of any generic physical interaction between finite systems, given this condition of
separability, in other words conditional independence of their states. So now we can start talking about
biology and what we can learn from this, and the bottom line is that any system that's actually
interesting breaks this third symmetry, and what I'll spend the rest of the talk doing is looking
at ways this third symmetry is broken, and how they give us bits of information that's interesting
from a biological point of view. So if you don't break this third symmetry, you just have a quantum
noise interaction. There's nothing outside the system to contribute any classical perturbation,
so there's no classical noise at all in this picture. So this thermodynamic interaction is just
quantum noise, but any system that acts irreversibly on its environment, so it does something to its
environment that affects what its environment is going to do to it, that is what I mean by irreversible
here, has to burn some free energy to do so, and the reason is just Landauers principle, that if I
write a bit irreversibly, that costs energy. I can read bits all I want, as a previous talk pointed
out actually, but writing bits costs energy, and that energy can only come from one place, it can
only come from the environment, because I'm working in a closed joint system, so if S is
going to get extra free energy, it has to get it from E. So this breaks the symmetry of the
interaction if I'm doing anything irreversible, because I have to say, okay, I'm going to dedicate
that bit of the boundary to a free energy supply, and we've all got perfect examples of that,
the power cord in your computer is a thermodynamic sector of its boundary, and you don't do any
information processing with that free energy that comes in, it powers the information processing
that you do by typing on the keyboard and playing with the mouse and all of that.
Your food supply is an example of a thermodynamic sector of your boundary.
So this is the first symmetry breaking that actually characterizes life, it's got to get free energy
from some place, so that it can act on its environment in a way that actually influences its
environment. And if you recall from Carl's presentation, that action on the environment is
half of the active inference equation, learning is the other half. So now let's think about memory.
If I want to act on my environment, and I want to store some observed consequence of my action,
then I've got to write a bit someplace. And since I'm working in a quantum theoretic setting,
the only place I can put classical information is on the boundary. Now, maybe I'm taking that
boundary and pulling some of it inside me, and I'll show you a picture of it, and I'm
taking that boundary and pulling some of it inside me. And I'll show you a picture of that
a little bit later on. But wherever that bit of boundary is, I've got to write this classical
information on the boundary. So I have to break the symmetry again by allocating some sector of
the boundary to the memory that I write. And that does an interesting thing to the inside of the
system. It says that the inside of the system has got to implement a clock, that a simple clock
that counts bits. Because it has to be able to distinguish information from then, from information
from now. And that requires a time variable. So it has to implement this little internal clock.
And this clock turns out to be what in the theory is called a quantum reference frame. It's a
physical object that encodes some standard, which is internal to the system and specific to the
system for time. So what happens as soon as we have a sector like this for memory? Well, it means
that we now have a metabolic trade-off. We've gotten free energy from the environment. And now
we have to decide how to use it. We can allocate it to perceptions of getting information and then
acting on the environment on the basis of that information. Or we can allocate it to memory.
And if I have a fixed amount of free energy, I can sort of turn it up between perception and
memory and try to find a place that's good. But if I want to keep getting information from the
environment and processing it, I better not spend too much money on memory. So what do systems do?
The solution we see over and over again is they coarse-grain their memory.
By coarse-grain the memory, I write fewer bits to represent a past event. So I represented it
lower resolution. And you see examples of this trade-off everywhere. In your computer, you don't
coarse-grain your memory. You want to keep an exact record. But up here, you do coarse-grain your
memory. And in fact, all organisms that I know of coarse-grain their memories. So they bias their
free energy budget toward getting information. Okay. So now let's think about a system that
doesn't just look at its environment and remember things. It looks at its environment and it sees
particular objects that are distinct. So I look out at you and I see a bunch of people. I see some
tables and chairs. I see lights, doors, walls, all this stuff. So I'm identifying a lot of objects
right now. And if I want to identify an object, then I have to do two things. I have to have
some kind of detector that says this is an object and this is the same object next time I look. And
it's the same object next time I look. But I have to have another detector that says, okay, there's
some states that have changed. And you can see this really clearly with the scientific instrument
where if I tell you to go in the lab and read the voltmeter, the first thing you got to do is find the
voltmeter before you can read it. And then reading it is a different process that assumes that you
found it. So every time I see an object, I have to allocate two sectors of my boundary. One for what
I keep constant to identify the object. And the other one to look at its variable, the variable
of interest or the variable that I can measure without seeing it as a different object. And in
physics, the variable is always called a pointer state because of all meters that had pointers.
So I call these the reference sector and the pointer sector. So if I see an object, I've
automatically made two more sector divisions in both my information space and my memory space.
Because in my memory space, I've got to write down that I saw this state of that object. So I have
to write two distinct records. And I don't want them to get messed up. Okay. And all of this requires
free energy because I've got to keep these sector boundaries fixed. So
now I can think about identifying more than one object. And anytime I deploy operators
that can't be deployed at exactly the same time, I've got to break the symmetry again
and write a different set of sectors in my memory. And I use the word non-co-deployable here because
I may not be able to deploy these operators because they really don't commute in some fundamental
physics way like upspin and sideways spin. But I may not be able to deploy them simultaneously
just because I don't have the free energy to do it. And that's just as bad. Right? If I don't have
the free energy, I can deploy this one and then I have to wait a little bit and deploy that one.
I can't do them at the same time. And it turns out that if you can deploy two operators simultaneously,
you can just do the and or the or and have one operator. But if they're not co-deployable,
then you have to represent them separately and define their own sectors. So there are many, many,
many examples in human perception and other perception and physical measurement and on and
on and on of operators that you can't deploy simultaneously. So what does this do? It makes
this trade-off much, much more complicated because it's cut the boundary up into many sectors
and given a free energy constraint, it means that you have to make decisions about what sector
you're looking at. And you typically don't want to completely ignore sectors of your boundary.
So you just don't focus attention on them. So what this tells us is that any system,
essentially, that can look at its environment in more than a couple of ways and has memory,
has to have an attention system, even if it's a very simple attention system.
And attention here just means control, control of what I'm deploying to look at the environment
and control of how I move free energy resources around between my computational resources.
Okay. So what does this tell us about S's internal structure? And I'll spend the rest of the talk
on this question of what can we learn about any system, any generic physical system
that has these symmetry breaking capabilities? And in particular, what can we learn about
any organism since organisms break all of these symmetries? Well, what we can first rely on is
a theorem. This is a quantum theoretic theorem. And I should back up and say that
a feature of this whole discussion is that when we look at the state space, there's no requirement
that it has any geometric degrees of freedom. So I'm doing this in a representation that doesn't
assume anything about space time. So in the classical FEP, you typically assume that the
parts of a system are located in different space compartments. And that's part of what keeps them
conditionally independent. But in quantum theory, you don't need to do that. You don't have to assume
anything about space time at all. And as we'll see, we're going to get space time out of this
instead of having to put it in from the beginning. So we have this theorem that says if I have two
computational resources, I'll call them Q1 and Q2, and they can't be deployed simultaneously,
then we have to keep them separate inside the system. And by separate, I mean not entangled.
Because if they're entangled, I won't be able to distinguish them. But I've made the assumption
that I can deploy them individually. So you've got to be separate. They can't be entangled.
And that means that they're compartmentalized in a way that forces them to only communicate
classically. So they can only communicate by exchanging bit strings if they're not entangled.
Well, what does that mean? It means I have to have boundaries inside my system that divide these
compartments that can only communicate classically. So physics alone, nothing else,
if you assume you have the cognitive or the computational capability to break the symmetry
of your boundary, the physics gives you compartmentalization with boundaries that can only be crossed
with classical information. And here's a very simple stripped down system that has a memory
compartment and two compartments that let it look at its environment and think of these as both
being split into a reference detector and a pointer detector. So this is a little thing that can
identify two different objects and remember something. And even at this level of simplicity,
because of this theorem about classical information and compartmentalization,
you have to have a control compartment, which is separate from these perception and memory
compartments, and that manages this attention system, which is allocating free energy and saying,
don't do this, do that now. And some colleagues and I have done a general theory of how to
represent these control structures, and it turns out you can do it with tensor networks. And you
can describe control with tensor networks only if you have this kind of compartmentalization that
keeps things that can't be co-deployed in unentangled compartments that can only communicate
classically. So the theory holds together quite nicely, and you get this compartmentalization
for free. So let's look at a single compartment and what its relation with its environment is.
And I've picked one of the compartments that looks at the external environment E,
but since it has this internal boundary that enforces classical communication,
it also sees other internal compartments that are like it, i.e., other compartments that do
perception and action. And then it also sees this compartment behind it that does control.
And so if you think of this in a free energy principle way, where you're interested in,
how is the variational free energy, so how is surprise or unpredictability or risk
or however you want to describe the VFE, how is that distributed across the boundary?
And in the case of this little compartment, the bit that faces the exterior environment is going
to encode the highest VFE because it's the least predictable system. The compartments on both sides
are compartments that are a whole lot like me, right? They do the same thing I do. So it's easier
for me to predict what they're going to do and how they're going to behave and what kind of
information they're going to send across the boundary because they're a lot like me.
And I don't need to know they're a lot like me. I just need to recognize that I'm not getting a
lot of surprise from those compartments on the side. Now the control system behind me may work
in a completely different way than I do, although from a very generic point of view,
you can represent it using the same kind of mathematics. So it's probably got a little bit
higher VFE than the ones on the side, but a lot less than the one in front, i.e. the environment.
So if these sectors behave differently, and I have a way of detecting what VFE is on what
part of the boundary, then from my point of view as a little compartment, I can distinguish between
the parts of my environment that I'm not too worried about, my cousins if you will, and the part
behind me that occasionally tells me what to do and the part in front of me that's very uncertain
that my job is to measure and act on. So here's a more familiar view of what one of these systems
looks like. There's an outside environment, there's an outer ring of things that face that outside
environment, and there's an inside that does all the control functions and allocates free energy.
And one of these outside compartments, of course, is the free energy supply, the power supply,
that doesn't have a measurement function, it's just a source. And this picture applies at whatever
scale you like. I haven't made any scale assumptions, so these compartments can represent pathways inside
a cell, they can represent cells inside an organism, they can represent tissues, they can represent
organisms in a community, whatever you want. The FEP is a scale free model of any physical
system that's interacting with any environment. So to get to this sort of structure, I would
claim that all you really need is an operation that does copy and diversify. So if you have a
little system that's in an environment, and it can make a copy of itself that's a little bit
different, then it can change its environment by adding this copy to it. And what does that do?
It gives it a piece of its environment that suddenly has a lot more predictability than the
rest of the environment. So it now has a piece of the environment that's kind of friendly.
So if it makes a bunch of these copies and kind of surrounds itself with copies, then its environment
suddenly looks really nice, right? Its environment's now highly predictable. The hard problem
of predicting what the external environment's going to do has been shoved out one layer to
these copies. Now let's look at exactly this same thing, but look at it from the point of view
of a boundary that would be outside of the copies. Now initially, that boundary doesn't
do anything, right? Because there are no copies. But the free energy at that boundary,
which now surrounds mostly the environment, is the free energy that is created by part
of the environment riding on another part of the environment. Now as some of the copies get into it,
the outer part of the environment still sees mostly stuff that's like it. But now there are
these odd things in it that are doing something interesting. And as I fill that annulus up with
the odd things that are interesting, then the environment on the outside sees a very different
system. It suddenly sees a system that looks multicellular and that has some kind of collective
behavior. But no part of the system inside that boundary can detect the collective behavior
or represent the collective behavior beyond the function of saying, I need to give more free
energy over here. I'm going to give more free energy over here. So other than this control
function. So the control function is the very basic representation of the self, but it's not
a representation of the whole self. It's just a representation of the part of the self that
provides free energy and needs free energy. Okay, so let's take this a little bit further.
We've broken our exchange symmetry on the boundary. We've made distinctions between data sectors.
Something that allows us to do is put some information in one sector and put the same
information in another sector so we can create an error correcting code. And error correcting
codes are the basis for languages. And I'll give you an example of an error correcting code. If I
just write 10 at different positions in space, then I can obscure a bunch of it and still make
out that it's 10. And I can do the same thing in time. So a resource for error correcting codes
is space time. But if I have an error correcting code, if I can write some data here and write
some data here and tell them apart, then I've got the beginnings of a representation of a spatial
difference between part of my boundary and some other part of my boundary. So if I have an error
correcting code, I can start to build space time from my own perspective. So I can start to build
what's important about space time, which is an ability to measure space and time.
And this is a very popular idea in quantum gravity. There are dozens of papers per month
talking about building space time from error correcting codes. So it's a place where biology
and physics really come together in that a thing we don't really investigate outside of
mammals and the hippocampus and all that is how systems organize space, what their detector
for space is, and how much space they can see. So they can see, can they see a radial coordinate
on a two-dimensional or an angular coordinate on a two-dimensional boundary? Can they see a radial
coordinate going away from them? We should be asking those questions about E. coli or yeast
or perimusium, which I expect can represent space pretty well. Okay, so to sum up,
distinguishable boundary sectors allow redundant encoding in his error correction.
If I have that plus some internal logic, I can have a representation of space.
And if I have morphological complexity, so if I can put detectors on lots of little parts
of my boundary and tell them apart from the inside, which my control system has to do to allocate
free energy, then I can have a very complicated representation of space. So if you think about
a neuron that has a branching into a tree, it's sampling lots of parts of space.
So as it brings those signals together, it loses that spatial information, but it keeps
part of it implicitly in things like time differences between signals and phase differences
between, phase differences between signals. Okay, so what does this tell us about living systems?
It tells us that all living systems at all scales co-evolve with their environments,
which are in fact agents. That's the symmetry of the FEP. It tells us that even minimal cognition,
seeing more than one object that's distinguishable requires internal compartmentalization.
It tells us that we'll expect metabolic trade-offs at every scale,
which are going to drive systems to different degrees of coarse-graining at different scales.
It tells us that competing resources always require an attention system, and it tells us,
I think most importantly, that detecting objects and memory and a spatial representation
are essentially three sides of the same coin, and you can make it four sides by adding error
correcting codes. So those are all in some sense the same concept if you think about
them from a physical point of view instead of a psychological point of view.
Now this will be the only bit of data that I show you, but a consequence is that you just expect
this massively increasing control system complexity as the number of different things you can measure
go up, and this is a plot we published a couple of years ago between the log of the area of a cell
and the number of bits that that cell's energy budget allows it to process at one kilohertz.
Cells have incredibly small energy budgets for the massive amount of information processing they do,
which leads us to suggest that they're using coherence for a lot of their information processing,
not classical manipulation. And the slope of this line is above two, so it's way bigger than
elementary, and that suggests that they're putting a lot of information on internal boundaries,
not their external boundary. And up here the eukaryotes have very, very large pathways,
you're all familiar with that interconnect, and the prokaryotes have pretty simple pathways,
and not nearly as many of them. So they're not burning nearly as much energy.
So thanks to my collaborators Carl, who spoke yesterday, Mike will speak later today,
and Jim Blaisberg and Antonino Marchiano. Thank you.
Thank you very much. We have a couple of minutes before the break. One or two questions.
Okay, I'll give it a go. Thank you. So you mentioned at the beginning the symmetry between the system
and the environment that if S is the system, then on the other side of the boundary that's
equally a system for the environment that we're the other way around. Does the compartmentalization
of the boundary into thermal, informational, and memory, is that symmetric too? Is one system's
boundary have the same compartments when you flip it around, and is the free energy source for one,
it's presumably not the free energy source for the other. That compartment would be the info
part of that to that side. Do you understand what I'm asking? Yes, and it's a beautiful question,
and it all gets back to that remark about free choice of basis being required to
avoid entanglement. So each side gets to sector their boundary any way they want to. They're
completely free to sector their boundary in any way. So I may have lots of informative sectors,
my environment may just have a thermodynamic sector. So everything I do to my environment
may look like noise to the environment, even though it looks like signal to me.
The system in the environment can have equally complex
sets of sensors and actors and all of that, but they may be completely misaligned.
So they don't communicate in the same language at all. But the interesting case
is what happens as the perception action looks on the two sides of the boundary start
to become aligned? Well, the free energy principle is driving all systems to minimize their free
energy on both sides. So they can minimize their free energy by aligning all of their detectors
so that what I write is what the environment expects and what the environment writes is what
I expect. And of course we do that, we call it language. Now think of the limit, think of a
system interacting with an environment, and it approaches this synchrony in reading and writing
and so perfect predictability over its entire boundary. Well, as it starts to get perfect
predictability in its boundary, it has to do less and less work because what the environment tells
it is always what it would have told the environment anyway. So it needs less and less free energy to
process information because it's never surprised. So it can shrink its fuel sector, if you will,
down toward the limit of zero. And so you can end the limit approach a situation where both system
and environment are writing on their mutual boundary in exactly the same way. So each one is
exactly satisfying the other's expectations. Now that's of course the limit that the FEP says
we're all aiming for. The FEP says we're all trying to minimize our VFE, minimize the difference
between our predictions and what we actually see. But what is that limit? In quantum theory,
where we don't make this assumption about space time, the limit of exact synchrony is entanglement.
So we cease to be separate systems. So the FEP is basically a classical limit of the principle
of uniterity, which just says that all systems will approach entanglement. We have a very good question.
Yeah, there's a question in the chat.
It is, does this mean the environment has some quote, interest in maintaining the boundary? And
if so, why? Yes, if the environment doesn't maintain the boundary, it ceases to exist as a system.
The environment and the system are co-defined, in other words. Thank you very much. Okay,
thank you to all the speakers.
