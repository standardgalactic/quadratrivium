Let's see. Okay, please go ahead and start.
All right. Well, thanks everybody for coming. I'm excited to present on some of this today.
So yeah, it's just going to be sort of a short tutorial on Bayesian data analysis.
And let's see if my slides don't work. And so the general outline for today, I wanted to
kind of start because I think a lot of people don't really start kind of from the Bayesian
perspective. We sort of start with sort of classical frequentist stats and then get introduced
to Bayes later. And so I think with that comes some conceptual barriers that I'd like to just point
out. And so I'll start there and then talk about what I think sort of the core of the Bayesian
philosophy is, which will turn generative thinking. And then I'll go through sort of a workflow of
how you might apply some of these methods to your own data with sort of an example from
some work I've done in the past. I was actually I was originally going to go through like
different like, you know, how to do a t-test or how to do a regression and some things. But as
we'll see when I talk about conceptual barriers, it actually kind of worked against some of those
barriers. I was trying to sort of break down a little bit. And so I'm going to mostly follow
one example, but try to hit on all of the core concepts throughout that example.
And if you want to, I put together an R notebook. And there's also an HTML file. So if you want to
follow along with some of the code that I use to generate like the figures and some of the analyses,
you can just take a snapshot of this QR code. And this will take you to a Dropbox folder.
And you can go to the code folder in there and then take a look at the files there.
So I'll just leave that for a sec. Then I'll move on. Okay, so in terms of conceptual barriers,
I think these are just some things that you tend to think of if we come from
the Bayesian perspective from the frequented side. And these aren't necessarily untrue.
It's just sometimes I think they can kind of limit what we can do with Bayes and sort of
it gets in the way of using it fully. And the first one is that Bayesian data analysis is just
sort of adding priors to traditional frequentist models. And so you hear about,
you know, we have a likelihood and we have a prior. And typically, you know, I can do a t-test,
I can do a Bayesian t-test, I can do a Bayesian regression and Nova. And all I need to do is
just add some prior information. And then it sort of makes it Bayesian. And it's technically true.
But I think there's a lot more to doing Bayesian data analysis than just that. And
and it really requires us to maybe take a different perspective from some of the classical
approaches. And so I hope that the example that I'll walk through today will kind of
demonstrate that idea. And then another barrier I think is that, you know, Bayesian probability
or prior information in general is subjective. And it's sort of just inherently biases our
statistical inference. And I think a great example of this is there was a paper that came out kind
of recently, you can see here, so Bayesian estimation with informative priors is indistinguishable
from data falsification. Right. And so I think this tends to be sort of like the snap judgment
that people make. And it's like technically true, like you can kind of, you know, having really
strong prior information that you might put into a Bayesian analysis can have a similar effect is
sort of like making up data. But again, I think this this sort of limits what how we kind of
think about Bayesian modeling in general. And so I hope that going through the example today,
I can sort of touch on some of these conceptual barriers and show that there's a little more
to it than that. Okay. And so so then it comes to what what would I say Bayesian modeling actually
is. And for me, the the general idea here is that we want to develop what I'll call a generative
model, which I'll explain more detail. But this is a model that can kind of reproduce data or
reproduce like all of the variables that we're interested in making inference on for a given
application. And so the the basic sort of maybe a definition that I could give here of a generative
model is a little more technical is a single or a joint probability distribution that we can use
to actually simulate forward the data that we're going to be observing in our experiment or whatever
analysis that we're conducting. And within this, this includes a few different things, but it
includes not only the observed data that we see. So like, maybe behavioral responses that we get
in an experiment or like self report questionnaire types of responses that we might observe. So not
only includes that, but it also includes how the parameters of our models are distributed.
And again, I know this, if for those who might be unfamiliar with Bayesian methods, some of these
ideas might be a little abstract, but we'll get into sort of more concrete examples as we go on.
And then finally, this includes also just all of the relationships that I'm interested in
estimating. So like if I'm interested in group differences, or something of that nature, kind
of a generative model is a model that can kind of capture all of those things that I'm wanting to
kind of understand better. Okay, and as I'm going through this, you can also feel free to ask any
questions. And so one one other thing to note here as we're discussing this is that the model that
we develop, we want it to be sort of closely aligned with our theory that we're trying to test
or sort of scrutinize as possible. And so I think that is one of the really big benefits of doing
Bayesian data analysis is that we're able to kind of develop models that more closely align with our
theories. And I want to kind of walk through that today in this example to hopefully convey that to
some extent. Okay, and so now in terms of like how we actually conduct Bayesian data analysis,
there's actually a lot of really good papers that have come out recently on this, but
on this general idea of a Bayesian workflow, and we'll sort of walk through kind of the steps that
you might take when you're approaching a problem from a Bayesian perspective. And these will still
be kind of conceptual, but then we'll make them concrete in the example as we go through.
So the general steps are to first we have to actually define that model of you know how
basically if we can if we can kind of create a model that can simulate data that look kind of like
or similar to what we might actually observe in our experiment or our study, whatever it might be,
then we have specified a generative model. So if you've ever for example,
like gone into R and simulated data from a normal distribution, you might sort of take like 10
draws or 10 samples from a normal distribution. I like that would be a normal like a generative model
of sort of observations that come from a normal distribution. And so so that's the basic idea
that we want to do when we specify a generative model, but we want to do it in such a way that
it's kind of capturing all of the different patterns in the data that we are interested in.
And so when we're talking about data, typically we'll talk about like a likelihood,
and then we'll also talk about priors, which are the information that we bring to inference
that specifies how the parameters in our likelihood are going to be distributed themselves. And
again, I know it's kind of abstract for those who may not be familiar with some of these ideas,
but we'll again get into some specific examples as we go on. So that's sort of defining what the
model is, is the first step. And then the second step is to kind of simulate from it, right? So
once we've, once we've kind of defined the model, you can actually just kind of sample from it just
to see if it can generate data that looks like what we might expect. And so so basically we want
the model to kind of like, but before we even like fit it to data or estimate parameters of the
model, we want to be sure that the model can sort of produce sort of sort of patterns that aren't
like strange in some way. And again, I know it's abstract, but we'll get to an example of where
this is really important actually in our application today. And if it's not the case, like if we
simulate some data from the model, and it looks kind of totally unlike what we would expect it to
look like, then we kind of go back to step one and rethink what what we're doing. And then
the actual Bayesian part comes in when we're actually estimating the parameters from these
models. And so, you know, we're defining the model we assigned some prior distributions.
And then what we do after that is we say, okay, now that I've defined this, what should I believe
or how should I update my priors after observing the data? And then we call those the posteriors.
And that's where sort of Bayes rule comes into play. And so, so after we obtained that information,
the next step then is to sort of do something similar to what we did in step two here,
where we simulated from the prior to see what things look like. But now we're saying, okay,
now that we've learned something from the data, we've updated our priors, we have maybe more
precise sort of estimates of the parameters in the model that we're interested in. Now let's see
if we can sort of simulate data that looks more consistent with with what we actually observed
than than previously. And in this stage, we might find that, you know, a whole host of things could
go wrong, like maybe we made a bug in our code, or we sort of like the model was just doing a really
bad job. And so if that's the case, then we might have to go back to step one. And so what I wanted
to sort of demonstrate with this idea of a Bayesian workflow is that, you know, it's sort of this
iterative process where we'll sort of come up with a model of how we think something might work.
And then we were kind of going through these checks to see if it's producing behavior that we
would expect. And then and then this way at the end of the day, we can be more confident in any
inferences that we might derive from from the model. Okay, so that's the basic idea that we
want to do. And for our example, I'll talk about sort of a specific problem. And here, you know,
I'm in the, you know, my interests are in clinical psychology, I'm really interested in
impulsivity. And so we're going to use this example here where, you know, maybe the inference that I
want to make is like this difference in impulsivity between people with versus without a history of
chronic substance use. And so so we could approach this question in many different ways. And in this
case, I have some data from the Iowa gambling task, I'm not sure how many people here are familiar
with this task. But the basic idea here is that we have a we have four choices that you can make on
each trial. And these are just these different decks. So ABC and D. And what I'm showing you on,
I guess the y axis here, if we think of this as a graph, is like what you could expect if you were
to pick each deck on each trial. Right. So you get different amounts. And at the end, if you were to
pick deck A and B in the long term, the expected value of continuing to pick those decks, you end
up like losing a lot of money in a task. And so we consider A and B, like bad decks, for lack of a
better word. And then C and D lead to these long term gains. Right. And so we consider those good
decks. Right. And so this is sort of the the the observed data that we have. And the task is,
you know, whether or not you picked a good or a bad deck on each trial. Right. And I just want to
know if there's a difference between groups and sort of your your likelihood of picking a good or
a bad deck. So in this data set, so this is data from from we young on, and it's a paper 2014,
and we have 48 healthy controls and 38 individuals with past stimulant use disorder. And so at the
end of the day, the inference that we want to make is, you know, whether or not there's a group
difference in how they're selecting good versus bad decks. So that's the basic idea here. All
right. So to begin, I thought it would be useful just to go through how we might like typically
analyze this type of data, and then contrast that with the sort of Bayesian approach that we'll
discuss. And so here, so this is just an example of sort of data in like a long format that we might
collect from this task. And, you know, we have some various different columns that we might have
like here we have trial number. So here we're just showing the first 10 trials for this one
participant. And they're in the control group. And then we have this last column over here that
indicates whether or not they selected a bad deck on this trial. Right. So it's zero if they
selected a good deck and a one if they selected the bad deck. And we're interested in this column
and normally how we might analyze this is I might just sort of take these and I'll compute
like the proportion of these that are one for each person. So like for each person I,
and, you know, maybe for this one participant, it's 0.43. So, you know, 43 of the 100 trials,
they selected the bad deck. Right. So that might be sort of what we would do to sort of get some
information on how this individual person is sort of behaving. And then we do this for each person.
And then here I might demonstrate, you know, I might just plot out a histogram. Here we have
that proportion for each person and color coded by group. And what you can see is that so this
gray distribution is of the controls. And the red is of those in the amphetamine
use group. And I mean, you know, they're all kind of like between 0.2, like there's this
maybe one person who's got like a little higher than the rest. There doesn't really seem to be any
like particular pattern here. But we might traditionally we might analyze this if we wanted
to approach this from the from the point of view of sort of estimating mean differences in groups,
we might do a t test. Right. So that would be a pretty natural way to try to answer this question.
And if we do that to sample t test, we get like a p value of 0.08. And then we might sort of from
this conclude that well, there's not a significant difference between groups. And so we're sort of
done there. And that's really the way that we might traditionally approach this type of data.
Right. And so when we think about how we want to approach this question from the Bayesian
perspective, I think it's useful to kind of back up a little bit and think about like what I'll
call the data generating process, which includes all of our knowledge about like the experiment that
we conducted, like the paradigm itself, but then also like any theories that we have about
like why a person might be choosing, you know, good or bad decks, like more or less.
And so some basic things that we know about data collection. This is just sort of going back to
some of the things that we already went over when I showed you the data. But, you know, we know that
there are 100 trials or not elections, I should say selections made by each person. And that the
observed outcomes that we see are binary, right, like either chose good or chose bad. So that's
sort of information that we want to be able to, like we would like to be able to simulate
like zero or zeros and ones, like whether or not you're selecting good or bad on each trial.
So that's what our model should be kind of capturing. And then we might be considering
some things just about impulsivity and behavior in general. And so, you know, coming from sort of
some of the clinical theories, like maybe, you know, we believe that each person has some like
latent impulsivity trait or like propensity to engage in impulsive behaviors. And we think that,
you know, just a person who's high in impulsivity or who engages in impulsive behavior quite frequently,
it's not all the time, you know, it's probabilistic because behavior is kind of
really complex and kind of random. But that's sort of what we think we think there's something
underlying the person that gives rise to this behavior that we observe in the task. And in this
case, we believe that, you know, people who are higher in impulsivity should maybe produce
like more of the bad deck selections across the trials in this current context. Right.
There's some other stuff that we might consider like, you know, there's some ideas that trade
impulsivity is this dimensional trait that, you know, I'm, I have a certain level of impulsivity
and you have certain level of impulsivity. And we're all sort of along this continuous distribution.
So we might, we'll kind of come back to that point later, but that's also stuff that we might
want to consider when we're kind of thinking about a model here. Okay, so these are just some
basic ideas that we might kind of start with. And then, so kind of revisiting the data, what we
observe in the task in our experiment is just, you know, this column zeros and ones, right.
And so the sort of first question to ask here is like, how do we come up with a way to kind of
like simulate zeros and ones, like within a person? And like, how can we generate that data?
Like what is a model that could produce that? And one of the ways that we can actually think about
this is we can view each of these trials. So, so here, like that is just indicating whether or not
a person selected like the bad deck in T is just the trial. So like bad for T one for this,
so we're only thinking of this one person for now. So like bad T when T equals one is just zero,
and then T two is zero and sort of so on. And we can think of each of these trials as us sort of
like flipping a coin, right. And you can imagine that instead of landing heads or tails, it's one
for bad equals one and zero. And then underlying that is this, this value P, which is sort of the
probability of a one arising as opposed to a zero. And it's sort of like a biased coin flip,
right. So if I had like a loaded die or like a low, I guess a loaded coin, then, you know,
a higher value for P would mean that that ones are occurring more frequently. And so you can
actually do this in R for those of you who are familiar, I'm not going to like go through the
code in a ton of detail, but I just wanted to demonstrate sort of how you might approach this.
And you can kind of ignore this part or actually doesn't have like something that just
does this function. So I kind of made my own real real quick here. But what we can do is we
can kind of simulate like a Bernoulli trial, like one flip of this bias coin with if the underlying
probability is 0.1, we can simulate like 100 trials. And what that might look like is something
like this, right, like, so we're going to get some some zeros and ones over here. And what we could
then do is say, okay, like we simulated this data. And then if I just kind of compute the proportion
of those that are bad, like we can imagine like maybe it's 0.05. And then you can imagine like
doing this again. Right. So we simulate another time. And, you know, maybe now it's like 0.08.
Right. And so, you know, we could we could do this over and over again. And what we would end up with
in this case is sort of a bunch of different sort of observed proportions of bad decks selected,
despite the fact that like P was equal to 0.1 in all of these cases. Right. And so I think that's
one of the important concepts about like building some of these models is that,
you know, by doing this, we're sort of making the assumption that, you know, even if there is this,
you know, the single value P that underlies someone's behavior, you know, if they were to kind
of do the task multiple times, you know, if we don't assume that they're learning or something
like that, even if P is the same, we're going to see different, you're going to sort of like
realize or observe different outcomes just because, because this is probabilistic, it's like flipping
a coin. And so this is just doing that repeatedly 100 times. And this is the distribution that you
get. I'm highlighting 0.5 here just to show you kind of give sort of like a midline that you might
see. But you can also kind of look at what this might look like across different values for P.
Right. So, so if P is 0.5, you might get a distribution that looks something like this,
or 0.3, something like this. And in all cases, you know, what we actually would observe
and every, for any given like run through the task, you know, it could have come from many
different values for P. And when we are doing a Bayesian data analysis, what we want to know is
say, you know, given this is my model of the behavior, like what is the most likely value
for P that could have generated it? And that's sort of the core question that we're answering
when we're doing Bayesian data analysis. And so the idea here, so this model is essentially what
we might call the likelihood in a Bayesian data analysis. And then what we need to do then is
think about, you know, we've specified how the observed data might arise from some underlying
probability, but then like where does this probability come from? And so that's where
the prior comes into play. And I know this kind of looks strange, but I'll talk about this in a
second. But we have, so we have our likelihood here, and then our prior. And again, we're still
thinking about, you know, just one person's data. And what we're assuming here is that, so this is,
we're assuming this normal distribution on this probability, but it's on a different scale than
then the natural scale. And so basically, you know, P needs to be between zero and one because
it's a probability. And so we need some way of assigning a prior distribution to it that will
actually capture that assumption. And so when you, if you've ever done logistic regression before,
you've probably done something you've unintentionally used this before. And so this, so you can show
what this prior looks like on logit P here, right? So this is just what it might look like. This is
just our sort of typical normal distribution that we're kind of familiar with. But then if you were
to show what this actually looks like on the scale of P, so it's approximately uniform. And so that's
why I chose this like sort of weird number. And so basically what this prior is saying in this
context is that, you know, like before we've seen any data, we haven't looked at any of the participants,
you know, we think the probability of sort of choosing a good versus a bad deck could be anywhere
between zero and one. And any value is kind of equally likely, right? And that's sort of the
idea that this prior is capturing. And right, so this basically what's happening is if you look
through the notebook that I linked, I go into a little bit more detail on exactly what the
transformations are here and how to better interpret that. So I'd encourage you to check
that out if you're interested in. But I think the core concept here is that, you know, we've
assigned a prior distribution on P and it's more or less uniform across any value. And so we don't
really expect anything in particular. Okay. And so because that's a little abstract, right, we've
talked about what the prior might look like on the parameter P. But what does that actually look like
in terms of like the data that we might observe in the actual experiment. And this is where the
concept of a prior predictive check or simulation comes into play. And so the basic idea here is
that, you know, I may first start by sampling this normal distribution. So in R, I would
say like just draw one sample from a normal distribution with a mean of zero, and then a
standard deviation of this strange number here. And then I'll get some value. And then I plug
that into the likelihood. And then I simulate the sort of observed data from the likelihood.
So that's sort of the, the steps that we might take. And then if you do that for all of the 100
trials, then I could summarize that just like I did in the previous slides when I was computing
sort of the proportion that we observe of bad deck selections. And then you could do that over and
over again. Right. So taking different samples from the prior kind of running it through the
likelihood here. And then at the end of the day, what we get is what we'll call this prior
predictive distribution, which tells us like what our model actually expects to see in the observed
data that we're going to make inference on. And so if we basically do this, so if we do this,
I think I ran this like 100 times or more. What you'll see here is the pink here, the lighter
red is the simulated quantities that kind of come from this of the proportion of bad deck selections.
And then I'm just for scale, I'm putting this, this is our random or subject 103, just the random
person that I'm using as the example here. This was like the number that they actually selected.
So, so at this stage, we haven't actually like estimated the model parameters. We haven't updated
them. The model is unaware of what the observed data should look like. But, but we're just sort
of sampling from the prior and the likelihood just to see like before, before we see any data,
like what are some things that our model might predict about your like a random participant.
And the key kind of thing to note here is that it's, it's kind of uniform, right? And it should
be because our prior was uniform is that, you know, it's, we're sort of equally likely to see
a person pick all bad or pick all good or pick like half and half or really anywhere in between.
And so this is sort of like what the model expects to see in the data before we, before we see any.
And so that's, so this is a useful thing to do just to get a sense of like,
do I have, have I selected priors that are reasonable for the inference that I'm trying to
make? Because I think one thing that can easily go wrong is when, I mean, at least when I first
started doing Bayesian data analysis kind of stuff, you, you sort of don't want to
make your priors too informative, right? Like I showed you that paper at the very beginning
that said something along the lines of, you know, if you're using informative prior information,
then you're, you're doing data falsification. And I don't want to do that. And so, so you might say
like, okay, well, I'm going to make my prior like really diffuse, right? Like this normal
distribution with like this really big standard deviation. And I think that's like pretty typical
when, when people maybe start to do this. But if you, if you, if you assign this, this prior and
you do these prior predictive simulations, what you actually get is something that looks more like
this. And so kind of, so sort of just doing the same thing, but instead of using that previous
distribution, I'm using this normal zero 10 on the on the P. And what ends up happening is because
of some of the weird transformations that are going on here, the model is actually predicting
before seeing any data that it's much more likely that people are going to be like choosing all
bad decks or all good decks and sort of almost all or almost all bad or good, but very little in
between. And so, so this is sort of one of the reasons why I think doing, you know, kind of
after specifying a model, just sort of like drawing some samples from it, just to see what it's
making predictions about can be really useful and really important when we're trying to better
understand the implications of our assumptions. Because this is probably not good, it's going
to give us some inference that is maybe biased a little bit. Okay. And so, so the key concept to
kind of take away from this part is just that, you know, it may look not informative because
it's really wide, but actually this can be quite informative at times. Okay. So now you've seen,
I've changed it to this normal at 01. And I can kind of try this again and see what happens.
And this time, what I'm getting is something that's sort of more concentrated around 0.5,
which in this case, if we think back to the task, so in this task, if you were to respond
completely randomly on each trial, like, we would expect you to have like, you know,
half and half good and bad selections. And so it's probably more reasonable to assume that,
you know, we expect kind of more of the observations to be kind of closer to 0.5 than
all one or all bad decks or all good decks. And because, you know, that would sort of
require our participants to learn, like really rapidly what the sort of best responses in the
task are. And that's pretty unreasonable assumption, at least in this context. And so,
so this prior sort of more conservative, it's sort of saying that we are going to expect to see
people sort of more having a preference for either or, but not one or the other.
And that's, I'm, I'm more happy with that. And so I've put my little happy face here.
And I think the more important thing is that we don't have something that looks like this,
which is completely maybe not what I expect. And I mean, unless it is like maybe you do expect
everyone to pick one or the other option, but, but that wasn't my expectation for this. And so
this is more consistent with what I would say my domain knowledge is in this context.
Okay. So, so after we're kind of happy with the, the model itself, the next step is to actually
fit it to data. And so I have a lot of examples in that notebook that I shared at the beginning.
But what I wanted to show you is sort of just like a little snippet of the code that's actually
used to do this. And I'm using this package that I'll talk a little bit more about at the end,
called rethinking, but it allows you to specify the model pretty similar to what you see in
the notation here, just kind of nice. So I'm sort of defining the model, getting just this
subject's data, and then kind of fitting the model to the data. And so this is actually where
all of the sort of Bayes rule stuff comes into play. And, you know, I wasn't actually going to
show Bayes rule or anything. So this might be the first talk you ever see where somebody talks
about Bayes, but doesn't show the Bayesian rule. Because I actually don't have to,
I don't have to do it by hand, right? I sort of define my model, and then I can estimate it.
And what's happening is the model is estimating, you know, given that I've observed the data,
now what should I update my priors to look like? And then we call that a posterior at the end.
And so what ends up happening? So if we take this model, and I'm just showing you again the
prior predictions for this one person, right? So this is what we had in the slide before.
But then after fitting it, if you do the same thing that we did before, but instead of sampling
from the prior, we sample from the posterior to see like what the model expects after seeing the
data, you get something that looks more like this. So basically what's happened here is that
the model, we've learned something about this individual, this person's probability of responding
to bad decks. And because of that, now our predictions for what this person might,
like what the observed data for this person might look like, are now sort of more precise and centered
around the value that we actually observed them choose. But I think it's important to notice that
there's still some, you know, we're not completely certain. So the model is acknowledging that if
this person were to kind of take the task again, you know, we could see anything reasonably within
this distribution and that would still be within our expectation, right? And so this is sort of how
I usually go through my analyses is looking at, you know, what does the model predict before?
What does it predict after? Like we've learned something about this participant. And now we might
want to like interpret the parameters of the model in some way, right? So, you know, the whole idea
here was to estimate that P, so this probability of responding to, of making bad deck selections,
and then see if that's different across people. But in this case, we'll start with just this one
person and we'll say, so I'll show you here, this is the prior distribution on P for this individual
versus the posterior. And what you can see is that this, so this prior is just this normal
distribution. And it's pretty, pretty wide. But then after we see the data, the posterior has now
become more concentrated and much sort of denser around where the actual proportion of bad deck
selections was for this person. And so, so I wanted to get in some info about Bayes factors. And so
this is where I'll talk about some Bayes factors because, you know, one of the things that we
might want to do here, if we wanted to do some form of like null hypothesis, or point null
hypothesis type of testing in the Bayesian framework, this is one of the ways that you
can use Bayes factors. And so, for example, or so here, maybe our alternative might be that
like a person is just like responding randomly in the task, which could be like the P is equal to
point five, right? They're just they're not even looking at the decks, they're just picking whatever.
And so our null might be that P is equal to point five. And then our alternative might be that it's
not equal to point five. And what you can actually do, we can compute a Bayes factor that tells
us that gives us the evidence for one or the other. And one of the ways that you can do this is
actually, I like the visual appeal of doing this, but there's this idea called the savage
sticky density ratio for computing a Bayes factor. And what what this does is all we have to do is
look at at our null value at point five, at what the posterior what the height of the posterior is
versus the height of the prior. And that ratio gives us the Bayes factor for the null over the
alternative. And in this case, the value is like 2.23. And what this actually translates to is,
you know, there are different ways we can think about this, but the data are about
two times more likely under the null after having observed the data than what we expected beforehand.
And so the Bayes factor used in this way kind of tells us like how much we've learned or how
much we've updated our expectations of a certain parameter, like add a specific value. And and
this is sort of, you know, there are ways to interpret this like, like people have cut off
similar to what we might have for like, large, small, medium effect sizes. I'm not going to
actually go over those. But I have a link in the in the R notebook, if you wanted to take a look at
that. Because I actually don't I don't really use Bayes factors in my own research. But but they
can be useful to sort of tell me how much I've learned or how much my expectations have been
updated after seeing the data. And here I'm using them sort of testing this idea of this hypothesis
that that like P is equal to some null versus versus or point five versus not, but they can
also be used for model comparison, which is something that John Krzyski is going to talk
quite extensively about next week. Now another thing, another concept that's often used in
Bayesian data analysis, this is the idea of a highest density interval. And so so here, so this
is an idea that's similar to the concept of a confidence interval. But it's a little different
in the interpretation, and I put it here. But basically, so if I say I have a 95%
highest density interval, what it means is that the interval contains 95% of the distribution.
And all of the points inside it are more likely, or more probable than all of the points outside
of it. So it's sort of like, if you think of the mode being the most likely point, the HDI is like
an interval of the most likely points. And I know this is also John Krzyski uses a lot of these.
In his doing Bayesian data analysis book, he talks a lot about like how to do hypothesis
testing with HDIs. But we can also use them similarly to how we might use confidence intervals,
just sort of kind of ascertain like how certain we are in our estimate of PI, because here you
can see that it's quite wide, right? So PI could still be kind of within this interval somewhere.
And we'd need more data to be more sure where it is exactly.
Okay, so that was all thinking about one participant, right? And it seems like we've
done a lot of work just to analyze one participant's data. And so what I'm going to make a little
change, you'll see a small change in the notation. So just like look carefully.
All right. And so what you'll see is I added an I here. So now we have participant I and trial T.
And now we're going to estimate a different P for each participant. And we're going to use the same
prior. So basically what I can show you now is like before I was showing you the prior predictions
on the scale of just one person and I was showing a histogram. But now that I've talked about the
highest density interval, that's what I'm showing you here for each person. And the light is sort
it's the 80% HDI and the dark is the 50% HDI. And what I'm showing you is the prior predictions
for all participants, like with this assumed prior. So really, this is exactly the same
information that I was showing for one participant, but just for all of them at the same time.
And I'm also putting down their actual proportion of bad deck selections, just to give you a scale
of sort of what we're dealing with in terms of the actual observed data. But what you'll see
is the predictions for each participant are basically the same. Our prior is sort of centered
around 0.5. So we don't really know. But again, these seem reasonable based on our expectations
before seeing the data. And then what happens is when we fit the model to data, what should happen
is our prediction should get sort of more centered around the observed data. So the simulated should
start to match the observed. And so if I fit this model to data, which I have the example for
that again in our notebook, you'll see in the code that it's actually, it's very similar to what
this looks like. It looks very similar to the code I showed before. What happens now is our
posterior predictions become kind of shrunken around the actual observed data that we saw.
And so I have them ordered here just so that you can kind of see what's happening.
There's not really a whole lot to take away from this other than that, after fitting the model
to data, our posteriors become more precise for each person. And we're able to sort of estimate
or sort of simulate their data with more fidelity. And I don't know, maybe this person is like somewhat
more impulsive than the rest of the group up here. But that's about all I can take away from this plot.
And so this is so now, so now we've talked about like how we might analyze multiple participants,
but you know, remember, we were interested in group differences. And so how might we
ask that question, right? Because right now, we're just sort of estimating,
you know, each person's value for P, but not really doing anything with the groups. And
and so that's where we'll take this next step, which, which is actually pretty interesting. So,
so again, I'll make a small change in the notation here. And so also watch. Okay, so before
when I was showing this, I just, you know, when we were deciding our prior, I just sort of like
set the mean to zero and standard deviation to one. But we can actually estimate those. So we can
estimate the parameters in the prior distribution, which may seem strange. I'll get into some more
information about that soon. But the basic idea is that now our prior distribution on the person
level effects now functions as sort of our group level model. So we can actually use this to estimate
the differences between group means. So the basic idea here is that for each person,
we have this, this sort of regression equation. And group I here is just going to be an indicator
that is zero, if you're in the control group, and one, if you're in the substance use group.
And so what this means is that, you know, so if we just picture this as a control participant,
this participant I, then what happens is this is zero, and then new I is just beta not. And so
what that means, so that's the mean. So beta not is sort of the mean of the control group.
And then when this is turned on, we have the mean of the control group plus this beta weight here.
And so this is essentially a group difference beta one. And so this is beta one is now the
difference between the control and substance use group. And so so now as opposed to again,
as opposed to just setting this to some value, we're actually estimating it with this regression
model here. And this is essentially a logistic regression model. And so so beta zero and beta
one are on the same scale, like the law god scale that if you're familiar with logistic regression,
then then this is sort of the idea here. And now, so so yeah, we have our beta weights should have
put that up earlier. So we have our beta weights. And now you might say, well, now, you know,
so now we have this group level model, it's like sort of still a prior, it's like a prior for P,
but but now we need to put priors on these other parameters, which is the standard deviation,
and the beta weights. And I won't get into too much detail there, but we can do that in the same
way that we've done for other parameters in the model. Right. So before we said like normal zero
one is on P. But now we're just assigning the prior distributions to the coefficients in this
equation. And here, the sigma needs to be positive. And so that's why there's a log here.
But again, it's hard to interpret this, right? Like if I just look at this model,
and say like what would happen if I were to simulate data from this?
It's kind of hard to say, right? So that's why we can just sort of run our prior predictions.
Oh, and I forgot to mention so. So I've sort of snuck in hierarchical basing analysis here,
I think we we often might hear this word. And it seems like, you know, there might be a lot
going on there. But really, this idea of like estimating parameters in a prior. This is what
makes this model hierarchical. So now, like each person comes from a group, and it could be the
control or the substance use group. And that group is going to become the prior. And so it
sort of constrains what we might observe for each person. But again, we want to sort of see
what this model suggests. And so we can kind of go back and run our prior predictive checks. And
this looks pretty much identical to what we had before. So it shouldn't look too much different.
I mean, one thing that is noticeable if you go compare this to the one that we had before is
there's a little more uncertainty actually, because now the sort of prior predictive intervals stand
across like even this really impulsive person, which I don't think they did before. And that's
because now we have like the uncertainty and beta not the uncertainty and beta one. And so we have
more uncertainty. So there's sort of more uncertainty in the prior predictions. But then
after we fit it to data, which again, there's example code of that in the R notebook,
then we got these posterior predictions, which again, look pretty similar to what we had when
we were not estimating sort of these group differences with this sort of form of the model
up here. But you know, given that the model seems to be able to capture the data relatively well
here. Now we can go back and try to interpret the parameters. Right. So similar to what we did
before, we plotted the prior versus the posteriors for for P for one person. Now we're interested in
beta not beta one, which are telling us something about the group level effects.
And so, so these, these coefficients are, you know, between, they're sort of unbounded. So
they're not between zero and one like P is. But in this case, as you might be able to see, so at
our null, where, where so our null hypothesis here, I show beta not here, it's not really of
interest. What's really interest is this group difference parameter. Right. So our null hypothesis
might be that the group difference is equal to zero, no alternative would be that it's not equal
to zero. And then the ratio of the posterior density to the prior density gives us that, that
base factor. And I didn't actually even bother calculating it here, because as you can see,
the dots are like right on top of each other. And so the base factor in this case is approximately
one. And, and we've essentially come to like what that means is that we don't really have convincing
evidence in favor of the null. But we don't have convincing evidence for the alternative either,
we really for this, for this null value of zero difference in the group means, we've essentially
learned nothing at that specific value from our prior to our posterior. And I think this, I like
this example, because it demonstrates something about base factors, which is, you know, clearly,
we've learned a lot like this posterior has become much more precise. And so we have a better idea
of like what this group difference might be. But in terms of the base factor in this case,
we haven't learned much at all. And so this is why I think some of these multiple different methods
of thinking about Bayesian models can be helpful, because then, you know, I could look at the HDI,
which I know in this picture, it's really small. But looking at the HDIs here, in this case,
again, the 95% high density interval does contain zero. So, you know, if we're kind of treating this
similar to how we would a confidence interval, then we might sort of say that we failed to reject
the null. But in the Bayesian world, you know, we can still say that the majority of the posterior
is over zero. So it's, you know, there's potentially a group difference, but it's not really clear,
right? Like maybe if we had more data, we could be more certain and make more
calibrated inference on that. But there's a lot of uncertainty right now. And it seems that more
or less the groups are performing pretty similarly. And so, you know, so we went through all that,
we came to kind of a similar conclusion as we did with the sort of traditional
frequencies T test approach that I described at the very beginning, right? And the main conclusion
that we kind of are taking away from this analysis is that, you know, we didn't really find convincing
evidence of group level mean differences between our groups. And the last slide, there was some
evidence that if I were to go back that like the HDI was sort of below zero for the controls,
like that their group mean in which this indicates that they're choosing the good decks more
frequently than the bad decks on average. But again, there wasn't really a difference between
groups. And so, so it's something that we found there. But there's also just a lot of heterogeneity,
right? So the participants were very different from one another. And that itself is suggestive of
some future directions that we might take the model in. And so that's actually something that I
wanted to maybe touch on at the end. And this is where I think sort of the idea of doing Bayesian
data analysis is that, you know, we've gone through all this work to define this model. And now it's
really easy to think about how we might extend it to potentially better account for the data or to
better match our theory. And this is something that I think sometimes is missing with the traditional
approach, right? We might find that there's an all effect. And, you know, we might have to move on
to thinking about like different problems. But in with the Bayesian data analysis, now we can
start to think about like different ways to analyze the data. Like, for example, I, I, before
when I showed you we were estimating a difference in group level means, we assumed that the standard
deviation was the same for groups. But we could do something very similar to what we did earlier
for the group level standard deviation. And so, so maybe, you know, if we found that there, like
one group was highly variable and the other was not like that could be interesting in and of itself.
We also might try to account for some learning effects in the task. So maybe as opposed to just
estimating like a single probability of certain person performing like a bad or a good deck
selection, like we might try to estimate changes in that across trials, because maybe that's actually
more consistent with sort of the theory of impulsivity in this case. And, you know, another
thing that kind of is brought up here is that if we really think that impulsivity is normally
distributed at the population, well, does it really make sense to assume that or to estimate
like a difference between normal distributions and the group means of both like our substance use
group and our control group, like maybe it would make more sense to approach this question from
an individual difference perspective. So, so like as opposed to just estimating like a group
difference, you know, maybe, maybe we're interested in some like continuous like self report trait
that that we have and where we can look at sort of the relationship between some continuous trait
measure and the like the underlying probability of responding bad for each person. So something that
looks kind of more like this, right. And so, so those are just some some ideas for possible
extensions that we might make to the model to kind of better capture some of our theories and allow
us to dig deeper or dig deeper into the problem. Okay, so, so I wanted to kind of like wrap up
what we went over today. So, so just in terms of like what Bayesian data analysis allows us to do,
I mean, we really have to think about how the data that we're analyzing might arise or how
they might be generated. And I think that's like a really big key benefit of actually doing Bayesian
data analysis and taking up this philosophy because we start to realize what assumptions we're
making and how they might be sort of different from what our theories are suggesting. And this
allows us to kind of explore our data and in much more detail. And by doing this, we can actually
construct models that better align with our theories. So we can, you know, we developed this model
today where, you know, we could we're estimating what's going on for each participant, but then
also the groups that those participants were within. And so we're able to sort of explore
the data and understand it at a level that's just not afforded by other methods when we were just
doing that t test. And then a few other things is, you know, the fact that we can use these models
to simulate data is actually really useful for us to not only understand like the, like how our
statistical models perform, but also like what the implications of the models are for our theories.
Right. And I think we kind of touched on some of that when we were doing those simulations earlier
with the priors. And then I just wanted to final note, like for people who are really interested
in digging more, I know this was like a really fast course sort of overview of like what a sort of
Bayesian data analysis might look like. But there's this book statistical rethinking that I think
does a really good job of like introducing the core concepts and sort of the philosophy of Bayesian
stats that sometimes are hard to get in other more advanced textbooks. And despite that, it goes
through some pretty interesting and complicated models, but it kind of builds off some really
simple principles that, you know, you don't even have to have taken a statistics class,
I think, to to start working through this book. He also has lectures available online if you're
interested in. So it's been a big influence for me. So I thought I'd give a shout out here.
But yeah, so that wraps up what I had to share with everyone today. So I guess I'll I guess I
should stop sharing my screen real quick. Yeah, so I'm happy to talk about any any questions,
like I said, I know it's kind of fast going over all that information. So thank you very much,
thank you very much, Nate. It's great. Nate, please. Questions for Nate, please ask.
Or if not, I'm going to start. So anyway, thank you very much. I'll do a very nice talks, you know,
it's kind of a lot of stuff. And I did part of it for the hierarchy base. And in particular,
I'd like to give you a last slide to wrap up so that when you do Bayesian analysis,
it actually forces you to think more about this, you know, the the models and also
theoretical hypothesis of this and that, you know, not like the hypothesis testing,
which is very simple minded, and you get the p value and then in the terms of interpreters,
well, you know, you can go a lot beyond that, try this, try that, you know, and that's very good.
And I think that's really the power of the Bayesian analysis.
But thank you for that. But I think I kind of disagree with your final conclusion where you
said that, you know, when you look at this a beta posterior, and that includes the hypothesis,
you know, includes the zero, you say no, but I think it's that actually p values say we can
not say no versus alternative, look at the beta one posterior, it tells you that actually it looks
like there's the effect. Okay, good difference. Okay. Yeah. And which is you don't get that
information from my process testing, right. So you think that there's some indication that
there's a maybe that the the the impossibility and that can play a role for this, this, this,
the, the patient group, for example. Yeah. And so I think that's, I mean, I don't want to say
there's no, no, no, I think there's some indication of there's a good effect, good differences.
It was their discretion not center on the zeroes can talk more to the right, right.
Right. No, definitely. And I would agree. And I think, yeah, so like the Bayes factor sort of
suggested we hadn't really learned anything about that particular null. But it was true,
like the HDI was like, you know, the majority of the posteriors over zero, and that does tell us
something. And, and, you know, who knows, maybe we'd collect more data and you'd actually be
able to see that it is above zero. And I think, so yeah, no, I'd agree with that.
So, I mean, this is, this is where it's a little ambiguous. Nate might have said this, but, but
Jay's convinced there's an effect. Nate's not quite sure.
Well, that's the debate, right. So now like you get the P is like a written point five,
zero five, so there's no effect. No, no, it depends. All right. So that you get the posterior
and you interpret the way that you want. And you write about it and you show you actually
posterior, right. Right. And I think that's the, the nice thing, right. Like, yeah, you,
so I could, you could say like this percent of the posterior is above zero and like,
do with that, what you will, like, if I were to make a bet, I would probably bet on the side of
like, probably there is an effect because, you know, most of the probability is saying so.
And, and so that's sort of like, yeah, definitely something interesting to think about. And I
definitely agree with that idea that like the frequentist sort of approach at the beginning,
like, you know, even in the time that I spent on it, it was like, what, five minutes.
And so it takes a lot longer, I think, to think about things in terms of like a full
generative model of how my data could arise. But then like all those potential extensions
at the very end are much more apparent because I did that. And so it's sort of like, you know,
even if I were to find evidence of a null effect, I'm thinking like, well, there are so many other
ways that I could have approached this that might be more consistent with like, what I believe is
actually happening. Like if I, my theory of impulsivity says that there's one normal distribution
and people who aren't really high are just like on the extreme end, then it doesn't really make
sense to estimate different means in normal distributions to see if one group is higher
than the other, because they're not actually groups. Like, maybe if I take that dimensional
approach, looking at correlations for like some individual difference measure, that might
actually make more sense. So that's the type of thing that like doesn't necessarily come to mind
for me when I'm, you know, taking a like frequentist approach or traditional approach that becomes
really apparent because I've just like written out the model. And I'm thinking like that doesn't
actually align with that assumption that I wrote out at the very beginning. And so, you know,
maybe that's a way to kind of improve on things next time.
Someone else like asked me the question.
Do you not worry about identifiability in Bayesian models?
So, I would say in your workflow, you should probably include that, but you also need to
know what identifiability is and explain it. Yeah, definitely. I mean, there's only so much
that I could like reasonably fit in here. Understandably. Yeah. But no, that's true,
because what you can actually do, and that's sort of like a part of the,
so like you can actually just do those prior predictive simulations and then like do one
iteration of that and take what results you get and then fit the model with those and see,
so it's really nice because if you can simulate data from your model, then you can then,
you can simulate and then you can fit that data and then you can see if it's recovering
what you simulated initially. And so there's a really nice loop that you can use to make sure that
we might call that like parameter recovery simulations and cognitive science.
But you can sort of do that to make sure that your model is like performing as you might expect.
And that's definitely an important piece. There's some modern work on that, like they call it,
I think the stand group calls it and Gellman and folks calls it like simulation based calibration,
where you sort of do that repeatedly from your prior simulations and fit it over and over again
and just make sure that like across that, all those different sort of samples you get the sort
of recovery that you would expect. And that can be really important.
Yeah, I think that also helps for detecting bugs as well.
So that kind of last step of detecting bugs, you know, it's either a my model is not identifiable
or if I've done something very wrong. Yeah, I think I always do something wrong when I first
because I mean, if you're not fitting, if you're not just doing like LM and R, you're probably
going to mess something up. And so it's sort of, I do think that's nice is like doing the prior
predictors and all of that, you can catch that a lot easier. Because when I first started doing
there were times where I'd like go like almost fully through a project
with a model that I had like very badly mis-specified in. And I only caught it when
really going through the results and I was like, oh man, back to back to square one. And so that's
why I think that sort of going through the flow up front is really important because I've learned
the lesson the hard way. You want to think that it's easier just to like go through and get the
results, but it's not good. If anyone doesn't have another question, I can shoot another one at you.
Sure, Noah, go ahead. Yeah, so, you know, there's the thing of inform versus diffuse
priors. And you kind of went with something that I think is pretty objectively diffuse,
you know, you can argue that it's diffuse. But what if I wanted to use an inform prior?
So that posterior is still valid, right? And then, you know, I could use multiple informed priors.
So how do you feel about making instead of just a single posterior and publishing that,
maybe software so that way people can put in their own priors and then see what their posterior
look like? Yeah, no, I mean, I mean, I've done, I think you see a lot of applied papers will do
like some sort of like prior sensitivity analysis stuff like that. And so it's definitely something
that you can you can do. Yeah, and I think for me, it's sort of like I think of the prior itself as
like a model that I should be sort of checking and scrutinizing and that sort of thing. And so
so that's, it's, I think it's definitely important to kind of know what the implications are, like
would I make the different, like would I make a different inference if I were to use like a diffuse
versus a really informed prior. But like there are certain areas in cognitive science, especially
where like having informed priors is actually really important and things like when you're fitting
like evidence accumulation models or like kind of like complicated models that we
have studied for a long time, we might, there might be certain parameters like a drift rate or
like a non decision time or like whatever it might be, we might know more or less like what
the distribution should look like. And I think the example that I gave today kind of shows that
like, if I'm just making inference on an individual, the prior is like the group level.
And that's sort of the kind of main idea of hierarchical base. And so like you can
sort of, so we can kind of take that knowledge that we have of like what people look like in
general and sort of put it into this, into these models in the form of priors. And I think
often that's actually more helpful than sort of assuming you know nothing up front and then
