Так, вот еще у нас в программе Кристон. Великие ужасные смыслы Будлин. Он не тот, кем хочет казаться. Его практически невозможно понимать вот так линейно и тем более пересказывать линейно.
Вот, и поэтому у нас уже второй закат на Кристон. Первый был два года назад. Мы тогда несколько раз активно общались, чувствуете новые решения у нас. Он нам выводил кучу в ресературах, так что нам очень благодарны.
Вот, и в частности, он просил обратить внимание на Кристон. Ну, и мы довольно быстро бросили это дело, потому что он нам оказался очень темным, на самом деле такой весь, темный, непонятным, который хочет говорить сложно о простых вещах, вот как-то вот так.
Ну, вот, теперь, когда мы увидели еще и ваше интерес, и, значит, помню, что это все неслучайно, и, опять же, оно, тем понятнее дело, не удовольствует нашим неудовольствиям. Вот.
Ну, и мы начали действительно разбираться, практически проделали двойную работу. С одной стороны, мы пытались его понять для себя, а отдельно мы думали, как вы это рассказали вам.
Вот, то есть, как я сказал, линейно, то есть, вот, пересказывать статью совершенно невозможно, мы подали другим путем.
Спасибо.
То есть, развалить его на части, извлечь основные ключевые слова и сложить заново так, как мы это понимаем. То есть, фактически, это будет не пересказ Кристона, а его интерпретация.
Ну, возможно, вас разочаруем, что делать.
Так, в общем, не стрелять с пениска, мы интерпретируем, как у них.
Значит, основный доклад будет у 2 июня, и потом, вот, по этим двум основным составом, с этим, особо претенциозным.
А вот, а потом, Сережа еще расскажет, то, что мы изучили, вот, есть дыхание статьи.
К сожалению, совсем без формул не удалось это все проделать.
Да, то есть, формул здесь много, но я постараюсь каждую из них объяснить, но, вот, приходилось все время находить компромиссу между точностью и простотой.
Я старалась все время смещаться в сторону простоты, поэтому некоторое определение, может быть, даже не совсем корректно, но зато они, более-менее, вот, выстроились в такой ряд.
Поэтому, ну, если мы все это преодолеем, все остальные статьи Фристона, вот, поверьте нашему опыту, теперь читаются легко, свободно.
Да, так что вот такой план, сначала я попробую общими словами описать принцип, как Фристон его понимает, потом мы несколько углубимся в теорию,
после этого я уже придется разобрать математическую модель и сопутствующие модели, в частности, байсовский вывод и активный вывод.
Вот, ну и есть пятый пункт, он уже такой опциональный, если будет желание, то можно с ними тоже разобраться.
Это те темы, к которым Фристон буквально в каждой статье обращается и апелирует, это вот бытовочная горушка информации,
бессознательный вывод Гильмгольца и теорема Хорошим регулятором.
Они не обязательно для понимания, но в принципе это интересная вещь и остановимся на описание принципа свободной энергии.
Вот как он звучит и формируется, любая самоорганизующая система, находящаяся в равновесии со своим окружением, минимизирует свою свободную энергию.
И с помощью этого принципа объясняется способность биологических систем противостоять конденции к беспорядку,
поддерживать неустойчивое равновесие за счет ограничения множества своих возможных состояний.
И вот такое пока еще не точное определение свободной энергии, свободная энергия — это мера, ограничивающая сверху неожиданность.
Ну вот сопрась, к сожалению, вот такой перевод и он будет нас все время сопровождать — это одно из ключевых понятий в куфристам.
И вот свободная энергия — это то, что ограничивает эту неожиданность сверху.
Такая величина, больше которой у системы сюрприз просто быть не может.
Вот там последняя трочка была данной модели.
Про порождающую модель мы поговорим чуть позже.
То есть все, что выделено синим — это все понятия, которые будут дальше объяснены, я надеюсь.
Ну то есть мы с ними разберемся так, чтобы они уже не вызывали вопросов.
Вот сейчас еще такие пока предварительные слова.
Биологические системы могут извлекать структурные закономерности и сфуктации окружающей среды
и воплощать их в свою внутреннюю динамику.
В сущности, вот это очень важный постулат принципа,
что каждая система сама становится моделью причиной структуры своего локального окружения.
И после этого она предсказывает, что будет дальше как будто изменяться это окружение.
И таким образом старается уменьшить неожиданность от своих сенсорных входов.
То есть с помощью моделирования своей среды системы приобретают способность гомеостазу,
по ограничению числа состояния, в которых они могут находиться.
Так, вот как это выглядит математически.
Математически способность гомеостазу означает, что с высокой вероятностью
система будет находиться в одном из немногих состояний и с низкой вероятностью
в любом из остальных состояний.
Вот неожиданность формально определяется как логорифм от вероятности отрицательной.
То есть чем больше вероятность события, тем меньше его неожиданность.
То есть нам гораздо вероятнее увидеть рыбу в воде, чем рыбу вне воды.
То есть вот тут Фристан об этом так и говорит.
И эмоционально, и математически здесь неожиданность гораздо выше, чем у рыбы в воде.
И еще одно понятие, с которым мы постоянно будем сталкиваться, энтропия.
Если сказать достаточно неформально, то это получается среднее значение неожиданности по всем состояниям.
Да, то есть у нас есть все возможные состояния системы,
и чем меньше энтропия, тем более определенно ведет себя система.
Чем больше энтропия, тем все состояния ее равновозможны и предсказать про ее состояние ничего практически нельзя.
Так, ну вот начинаются уже обозначения, без которых дальше пока мы не продвинемся.
Все состояния системы делятся на внутреннюю и внешнюю.
Икс — это пространство состояния.
И вот это пространство состояние состоит из пары внутренних и внешних состояний.
То есть в каждый момент времени система характеризуется своим внутренним состоянием и своим внешним состоянием.
Буквально на следующем слайде я расскажу, что это такое.
А система — это организм?
Система — это любое организм от клетки до человека.
То есть другая биологическая активная система.
И, значит, еще некоторое подножество внутренних состояний называется действием.
То есть это то, что система может совершить в данный момент времени под воздействием окружающей среды и под воздействием своих внутренних состояний.
Окружающая среда обозначается буквой омега.
Вот омега маленькая — это футуарство окружающей среды.
То есть она постоянно меняется и воздействует на нашу систему.
И, наконец, последние два обозначения.
Без них мы тоже дальше пойти не можем, поэтому я их привожу.
Это так называемые внутренние и внешние отображения.
То есть внутреннее отображение ФИР — это выбор системы своего одного из своих внутренних состояний.
Под воздействием предыдущих внутренних и внешних состояний.
А вот эта букваэра у нас всего означает, что это во времени происходит.
ФИС — это выбор системы внешнего состояния.
Внешнее состояние зависит от окружающей среды.
Тоже оно меняется во времени, зависит от окружающей среды, от действий системы и от внешних состояний.
С они называются, на самом деле, то, что приходит на сенсор.
Поэтому они называются внешними состояниями.
А-а, да, да, это что?
Нет, не a, а-а, а-а.
А вот здесь интерпретация нет.
И вот смотрите, еще здесь появляется запись динамики модели.
Такой реакции, стимулрекции?
Нет, это не совсем реакция.
А внутреннее состояние — это ваше мере реакции.
Значит, смотрите, пожалуйста, два дифференциальных уравнения, система дифференциальных уравнений.
Больше они нигде в презентации появляться не будут, потому что это достаточно в общем виде уравнения,
которая описывает динамику любой системы.
Вот тут написано от модели Хочке на Хаксли до модели лодки Вольтеры, которая описывает динамику лидов.
Это очень принято, это не фристушки.
Нет, нет, это просто самая обыкновенная система уравнений.
Вот первое уравнение описывает динамику внешних состояний.
И вот видите, она является функцией как внешних, так и внутренних состояний.
Плюс на нее воздействует окружающая среда.
Вот что здесь написано.
А динамика внутренних состояний зависит только от внутренних же предыдущих и внешних состояний.
То есть окружающая среда напрямую на внутренние состояния влиять не может.
Значит, два крайних случая.
Открытые системы, которые имеют только внешние состояния и замкнутые системы, которые имеют только внутренние состояния.
Фристон пишет, что такие системы по определению не могут быть активными.
То есть это какие-то неживые системы, которые либо сами в себе замкнутые или имеют входов.
А разве открытая система не предполагает наличие внутреннего и внешнего?
Это вот у него такое определение.
Это считается, что вообще нет ни одного своего состояния.
То есть среда подействовала, система изменилась, и никак братья стоять воздействию среды, она не может.
А мы как раз рассматриваем активные системы, которые имеют и те и другие состояния.
То есть у нас с и р.
И вот как раз здесь он приводит примеры для клетки, внешние состояния.
Это состояние рецептеров, а внутренние – это всё, что у него делается внутри.
Для мозга он говорит, внешние состояния – это сенсорные входы,
а внутренние состояния – это некоторые макроскопические состояния,
такие как интенсивность генерации спайков и всё другое.
Процессы, которые там происходят.
И вот такая схема, по которой динамическая система всё время функционирует.
То есть у неё есть внешние состояния, на которые действуют окружающие среды.
Эти внешние состояния воздействуют на внутренние состояния,
а внутренние состояния в свою очередь снова воздействуют на внешние состояния.
При этом среда изменилась и тоже воздействует.
То есть вот цикл, который всё время вращается внутреннее действует на внешние и наоборот,
и плюс постоянные флуктуации внешней среды.
Вот так вот эта система функционирует за счёт того, что у нас есть эти два отображения –
фейер и фейес, которые постоянно позволяют общаться с внутренним внешним состоянием.
И вот, собственно, эта схема является основополагающей для всех статей фристов.
Ну и пора задаться вопросом, где здесь принцип свободной энергии,
где он в этой схеме помещается.
Чтобы его в эту схему поместить, мы сначала проведём небольшой экскурс
и поймём, откуда он вообще взялся.
То есть где у него теоретические обоснования.
Принцип свободной энергии является разновидностью принципа наименьшего действия.
И я немножко здесь приведу школьной физики,
потому что это пригодится для дальнейшего понимания.
Принцип наименьшего действия – это один из самых фундаментальных принципов физики.
Вот здесь он написан только для механики принцип гамильтона.
На самом деле, ещё раньше его применял на третюю.
Но для механики это принцип гамильтона.
Это способ получения и уравнения движения физической системы
при помощи поиска наименьшего значения специального функционала,
который называется действием.
Вот этот вот интеграл – это и есть действие системы.
Сейчас я про него расскажу чуть подробнее.
Каждая механическая система описывается вот такой вот функцией
от своих координат, от скоростей и от времени.
То есть она движется в пространстве и описывается вот таким вот законом движения.
Тогда пусть в моменты времени T1 она занимает место в пространстве
Q1, а в момент времени T2 она занимает место в пространстве Q2.
Тогда между этими состояниями Q1 и Q2 система движется таким образом,
чтобы вот этот вот интеграл имел наименьшее значение.
Вот эта функция, она не зря называется L,
потому что она является функцией лагранжа или еще ее называют лагранжианом.
А весь этот интеграл называется действием.
Сейчас мы разберем небольшой пример для самой простой системы.
Для механической системы этот лагранжиан равен
просто разности между клиентической и потенциальной энергии.
То есть каждая тело движется в пространстве.
У него в каждый момент времени есть клиентическая энергия и потенциальная энергия.
И интеграл от этой разности должен быть минимальным.
То есть вот что происходит.
Если у нас есть дискобол, который собирается внутрь диска,
и диск летит по заданной траектории,
так вот оказывается вот эта действительная траектория, по которой он летел.
В момент времени T1 он находился вместе к Q1,
в момент времени T2 он находится вместе к Q2.
И вот оказывается, что интеграл вдоль этой траектории будет наименьшим.
То есть у нас гипотетически мы можем построить любую траекторию.
Предположим, вот такую.
Если мы возьмем вдоль нее этот интеграл, он будет больше.
Но это достаточно вычурная траектория, кажется, что это очевидно.
Но даже если мы возьмем вот такую траекторию, то все равно получается,
что именно разность потенциальной теоретической энергии
будет минимально вдоль естественной траектории.
И поэтому как раз все законы движения в механике
вытекают из принципа наименьшего действия.
То есть принцип наименьшего действия – это такой универсальный принцип,
который позволяет нам описать, какое угодно движение.
Но оказывается, классическая механика – это не единственная
область, где этот принцип наименьшего действия работает.
Вот во всех перечисленных областях тоже найдены лагранжианы.
И если минимизировать интеграл от этого лагранжиана,
мы получим принцип наименьшего действия вот в любой из заданных областей
вплоть до квантовой хромодинамики.
Что касается биологических систем, то Христом, я думаю,
что не только он, пытается применить этот принцип
наименьшего действия к биологическим системам.
Здесь он аккуратно дает определение активной системы.
То есть сейчас речь идет об активных системах,
но мы подразумываем, что эти системы – биологические.
Он говорит – динамическая система M.
Вот эта система M – это мы подразумеваем ту систему
дифференциальных уравнений, которая совершенно общего вида
и описывает любую динамическую систему.
Называется активной.
Если она ведет себя таким образом, что ее энтропия минимальна,
вот если мы посмотрим на эти формулы и сейчас попытаемся разобрать,
то это функция Fier.
Это, помните, как раз выбор внутреннего состояния.
И вот она выбирает внутреннее состояние таким образом,
чтобы ее энтропия была минимальна.
Вот энтропия H выбирает состояние такого,
что на каждом такте времени мы выбираем состояние с минимальной энтропией.
А теперь посмотрим на формулу для энтропии.
То есть вот формула для энтропии.
И если мы на нее внимательно посмотрим,
то вот здесь вот в окончательной форме
мы видим ту же букву L.
И это на самом деле тот же самого гранжан,
который мы имели для всех физических систем.
Если его выписать в явном виде,
и если мы вспомним тот слайд с рыбами,
надерями и водерами,
то окажется, что вот эта вот формула,
минус логорифум от вероятности,
это и есть та самая неожиданность,
которую нам нужно минимизировать.
То есть лагранжан в таких активных системах
равен сюрпризам, неожиданности.
И чем меньше неожиданность,
тем меньше энтропия,
и тем больше система
может противостоять воздействием
на нее футболация окружающейся рыбах.
Так, и вот, да, Варя, что?
Меньше энтропия.
Да, вот я помню, что когда я физически читала,
мне вот это было не очень понятно,
потому что кажется, что неожиданность,
она как раз связана с энтропией.
То есть тем меньше неопределенность,
тем меньше вроде бы должен быть сюрприз.
Легче это как бы наоборот.
Есть чисто формальная вещь,
вот у нас стоит с одной стороны энтропия,
а с другой стороны интеграл от неожиданности.
И чем неожиданность потом дыграла меньше,
тем меньше сама энтропия.
Вот, но на самом деле,
чем более предсказуемо следующее состояние,
тем меньше система.
То есть смотрите, у нас здесь...
Это правильно, это как раз да,
и чем меньше сюрприз, да,
тем меньше интеграл.
Тем меньше сюрприз, да,
тем меньше интеграл.
Да, совершенно верно.
Ну что, дальше?
Так, и вот, значит,
мы понимаем, что нам нужно избегать неожиданности.
Если мы переформулируем принцип свободной энергии,
то он может звучать так.
Все биологические агенты
должны избегать неожиданности
чтобы их состояния находились в физиологических границах.
Это вот фраза фристома.
Но как они это делают?
Объяснить, как они это делают?
Достаточно сложно,
но мы сейчас попробуем это сделать
и углубимся еще немножко в математику.
Вот та же самая форма.
Лагранджиана, он же сюрприз.
Сюда входит вероятность P.
На самом деле, это не одна вероятность,
это целое распределение вероятности.
Что же это такое?
Это распределение вероятности,
от какого-то сигнала,
который приходит на сенсорные входы.
То есть, это некоторая условная вероятность
внешнего состояния, вот S, это сенсор,
при условии, что у нас модель M.
А в системе есть еще одно распределение вероятности,
Q, это такое предположение
о виде распределения P,
от X, которая система считает
наиболее правдоподобно.
То есть, вот представьте, мы что-то видим,
а внутри себя мы об этом как-то рассуждаем.
Это некоторая модель, да?
Да, и вот смотрите, вот здесь как раз
приведена разница между всеми
тремя сигналами
или распределениями, скажем так.
Вот это вот Psi, оно здесь появилось впервые,
но у Фриста оно встречается постоянно.
Это так называемые скрытые состояния.
Это реальность, это объективная реальность,
но она
видится нам только опосредованно через
наши сенсоры P. То есть, это скрытые состояния,
о которых мы можем сидеть только,
потому что пришло на наши сенсорные входы.
И мы имеем какое-то внутреннее представление
об этом ку. Вот я нарисовала все эти
три вещи достаточно близкими,
но на самом деле, наверное, они все-таки могут быть
и не так близкие в зависимости от того,
как оно там на самом деле происходит.
Это внутреннее представление имеет любая система,
не только мозга. Любая система.
Например, клетки, когда они делятся, да?
Они знают, как куда им надо стремиться.
Ну вот у них есть какая-то внутренняя
модель внешней среды.
Да, здорово.
Видимо, да. Если он активный, то да.
Если он какой-то пассивной вирус, то нет.
Так, поехали дальше. Вот.
И теперь мы приближаемся уже к кульминациям.
Это формула свободной энергии.
Вот эта вот формула свободной энергии в чистом виде,
она на самом деле выглядит достаточно жидковато,
но если мы к ней приглядимся,
то это те самые скрытые состояния пси.
А по интеграмм всего-навсего две вероятности.
Вероятность ку, представление системы
о том, что происходит.
И вероятность п, это даже вероятность о мегаплохом
называть, это распределение вероятности,
то, что поступает в системе на сенсорные входы.
И если мы проделаем некие преобразования
с этой формулой, то мы получим вот такую формулу,
она хоть и более длинная,
но уже гораздо лучше интерпретиванная.
Во-первых, первая слагаемая, это наш лаграмжан,
который в сюрпрофисе, да.
А вторая слагаемая, это так называемое расстояние кульба калиблера.
И сейчас я вам про него расскажу.
И они суммируются?
Они суммируются.
Поэтому свободная энергия будет минимизироваться,
если оба эта слагаемая уменьшается.
Если мы поймем про расстояние кульба калиблера,
то дальше уже все очень сильно упростится.
А это как раз приведет к внешнему внутреннюю?
Да.
Вот смотрите, это определение буквально из википедии.
Расстояние кульба калиблера или дивергенция.
Это, если мы все это пропустим, несимметричная мера удаленности
друг от друга двух вероятностных распределений.
Я специально назвала точно так же.
У нас есть некоторое истинное распределение П,
оно еще называется пастулируемое апривое распределение.
То есть истинное в кавычках, потому что оно все-таки пришло
к нам на сенсор и мы могли там чего-то упустить, не заметить и так далее.
А внутри нас есть предполагаемое распределение, которое является
приближением первого.
И вот эта мера, она показывает, насколько они близки.
Если они очень близки, то эта мера равна муле.
А если они не близки, ну чем они дальше, тем больше это расстояние.
И поэтому чем лучше наше внутреннее представление внешней среды,
тем меньше это расстояние.
Да, и соответственно тем меньше будет, вот сейчас даже выскочит,
это истинное распределение, это его приближение.
Вот, то есть здесь внешний мир уже роли не играет,
у нас есть сенсоры П и распределение П.
И есть наше представление о нем П.
И вот чем эти две кривые ближе, чем лучше П понимает,
что происходит с П, тем меньше диверденция куба коллеги.
И соответственно второе слагаемое уменьшается.
И вот ответ таков, что, как системы это делают,
они постоянно сближают П и куб, ощущение предсказанного.
Тем самым второе слагаемое минимизируется,
про сюрприз мы поговорим отдельно, и минимизируется свободная энергия,
а неожиданность минимизируется, поскольку свободная энергия
является ее верхней границей, то, что ограничивает ее сверху.
И вот теперь, как они сближают, как системы сближают П и куб.
Не только куб стремится ближе к П, но за счет того,
что система активна, она может производить какие-то действия,
П может тоже меняться.
Получается, что внешние состояния действуют на сенсоры.
Сенсоры действуют на внутреннее состояние, изменяют П.
И если система видит, что что-то изменяет куб,
то есть внутреннее состояние вызывает какой-то дискомфорт,
она выполняет какие-то действия,
чтобы изменить внешние состояния,
то есть такой цикл, который постоянно не только куб приближает куб,
но и наоборот, система ведет себя активно
и заставляет куб тоже приближаться куб.
А при этом могут удаляться от объективной реальности?
Не то, чтобы удаляться от объективной реальности,
но в принципе да, вот к этим экшенам,
может относиться и, скажем, отрицательной червью.
Я что-то вижу, не хочу этого видеть.
Я вот, когда ты на спаках наступает,
эти сенсоры выключаются.
Вот это мне понравилось.
Ну или наоборот, если я что-то не примеряюсь,
я могу свет включить и все.
Ну то есть экшена, наоборот, может относиться все что-то.
Не запрещено пока.
Таким образом, и поэку мы все время сдвигаем и сближаем, пытаемся сближать.
Вот это вот расстояние, есть специальная теорема,
доказано, что оно не может быть отрицательным,
и поэтому минимум будет достигаться, когда оно равномерно.
Вот.
Чем ближе поэку, тем меньше F.
F это свободная энергия.
Но об этом мы уже сказали.
А как приближаются поэку, об этом говорит Байсовский.
И сейчас опять небольшое теоретическое отступление.
Здесь, я думаю, что многие проходили это в процессе обучения.
Это самое начало теории вероятности.
Но, тем не менее, я повторю, здесь используется так называемая Байсовская вероятность.
Байсовская вероятность – это субъективная вероятность для системы,
которая определяется как степень уверенности выставности суждения.
Ну здесь для человека, понятно.
Просто степень уверенности в чем-то, для любой системы.
Для определения степени уверенности в истину суждения
при получении новой информации используется теорема Байса.
И Байсовская вероятность противопоставляется к классической частruкной вероятности.
То есть обычно классическая теория вероятности
используют вот такие, т.к. объективные вероятности.
А Байсовская вероятность – она субъективна.
На ней основан Байсовский вывод.
вывод, но я тут не буду все это зачитывать, я скажу своими словами, если у человека есть какая-то
гипотеза, и у него есть степень уверенности в этой гипотезе, и тут случились какие-то события,
которые подтверждают, либо опровергают эту гипотезу, он начинает пересчитывать свои степени
уверенности. То есть получается, что байсовский вывод, это пересчета приворных вероятностей гипотез,
получение опростровированных вероятностей с учетом факта наступления некоторого события.
Вот здесь такой пример, он взят из интернета, у нас есть некий друг Фред, который хочет взять
печенье, для этого у него есть две вазы, в первой вазе 10 штук шоколадного печенья и 30
кростового печенья, а во второй поран по 20 каждым. Вот Фред на угар, не видя, выбирается
сначала вазу, а потом берет из нее печенье, и у нас уже есть событие, печенье, которое взял Фред,
оказалось простым и шоколадным. Спрашивается, какова вероятность, что он взял его из первой
вазы? До того, как Фред взял печенье, вероятность того, что он выбрал первую вазу и вторую
ваза была одинаковая, потому что он просто отвернулся и выбрал. Но когда он взял простое печенье,
ясно, что там, где его было больше, та ваза сразу начинает нас легировать в наших гипотезах. Вот
теорема Байса, она совершенно формального, для таких простых случаев, говорит, что можно пересчитать
опостоверную вероятность, и вместо 0,5 она уже станет 0,6. А если такой вывод повторяется к
личности, то эти вероятности будут каким-то образом все время изменяться, и мы будем строить
некоторое распределение внутри себя о том, что происходит. Вы оправили, я понимаю, что вот эта
оптимизация по Байсу, она возможно только когда события повторяются циклические, а для единичного
события это не работает. На самом деле те статьи, которые я прочитала, готовись к этому семенарю,
то есть статьи про Байсовские вероятности пишут люди, которые являются оповодетами Байсовской
вероятности. Я понимаю, да. И они пишут, что даже если никакого повторения нет, то тогда просто
оприорные, опостоверные вероятности совпадают, и все равно Байсовская вероятность лучше,
чем статистическая. И вот этот слайд я как раз взяла из одной лекции для студентов,
которую читает тоже, видимо, апологет Байсовской вероятности. А лучше она чем, тем, что лучше
видсказывает. А тем, что вот вот, Варя, вот как раз вот на этом слайде все это написано,
что в частотном подходе, вот смотрите, два подхода, которые друг к другу противопоставляются,
в частотный подход, в нем предполагается, что в мире есть какая-то объективная
неопределенность, хотя, как говорит вот этот лектор, в жизни объективной неопределенности
сейчас практически не встречаются. Чуть не единственным примером может служить радиоактивный
распад. А все остальное каким-то образом предопределено. Вот, в Байсовском подходе
предполагается, что случайность есть мера нашего незнания. А поскольку мера нашего
незнания не может увеличиваться, может только уменьшаться, то чем больше мы знаем, тем ближе
вот эти вот Байсовские вероятности к истинному распределению и к объективному положению вещей.
Кстати, насколько я понимаю, Байсовский подход ничего не говорит о том, как
прамируется приорная вероятность. Да. Вот они откуда-то взялись? Ну тем не менее. Скорее всего,
это как раз их частотный подход людей. Ну, Олег Петрович, дело в том, что об этом говорит теорема о
хорошем регуляторе. Если мы являемся хорошей моделью нашего окружения, то у нас о приорной
вероятности хорошем. А если мы плохая модель нашего окружения, то мы просто не выживем,
поэтому нам никуда не деться, как иметь хорошие Байсовские вероятности. А вот. И, значит, очередной,
так сказать, небольшой топик это авареационные Байсовские методы. Это буквально вчера, мы провели
большую серию внутренних семинаров. Повтористому, да. Вот. И это буквально вчерашние результаты
Сережина. Это не мои результаты. Которые мы все вместе вчера узнали. Значит, вот у нас есть некий
сигнал X. Объективность совершенно это то, что внешний мир нам представляет. Да, он каким-то образом
на нас действует. P – это наши сенсоры, которые принимают эти сигналы. И Q – это модель. Да. Но для
модели, на самом деле, все эти сигналы совершенно не важны. Для модели достаточно... А модель откуда
мы... Наше представление о внешнем мире. То есть, например, я вот всю жизнь наблюдала, как монетки
падают, и решила, что они падают аромой, решкой, приблизительно с равными вероятностями. И мои внутренние
представления как раз я не обязана помнить все монетки, которые в моей жизни упали. Мне достаточно
некие... Да, сформировать некие усредненные характеристики. И поэтому вот эта вот Q,
она не обязана знать весь этот X. Ей нужно поймать только... Вот смотрите, если P... Ну все-таки,
если отказаться от сознающего субъекта, от перейти к какой-нибудь более простой
биологической системе, почему это должна работать? Откуда может взяться представление?
Ну, хороший вопрос. Ну, я так понимаю, что если клетки на сенсоры приходят, какая-то положительная
информация, она знает, что ей будет хорошо. Если отрицательная, то плохо. Но вот в буквальном
смысле это где-то так. Если на нее действовать чем-то негативным, пусть не клеткой, пусть не
активной системой, которая способна как-то подействовать. Можем еще один вопрос. Он чуть-чуть в сторону.
Вот Петрович сказал, что первый подход вашей группы к Христому оставил кислое впечатление. Вот
сейчас после второго подхода у вас осталось кислое или оно стало получше? Вы знаете, я бы назвала
его сдержанно-оптимистичным. Потому что вот из всего того, что я рассказываю, все это не результаты
Христому. Это все вот такой бэкграунд, на котором он пытается строить свои модели. А его модели,
они, собственно, они практически пустые. То есть это все уравнение в общем виде. Туда
никаких реалий пока не засунуть. Вполне возможно, что из этого можно будет что-то извлечь конструктивное,
но пока... Ну то есть теперь, когда мы можем владеем аппаратом, мы можем подсчитать еще и тогда...
Я бы сказала, что его заслуга хотя бы в том, что он привлек внимание на арабиологов к бэкграундам,
которые мы не знали. А это очень хороший бэкграунд. Бэкграунд полезный. Безусловно. Даже вот
байфский вывод. Есть такие, которые ему недавно узнали и может быть имеется между ими заняться,
которые говорят о том, что вот на уровне нейронных популяций, нейронные популяции каким-то
образом моделируют, моделируют распределение и моделируют байфский вывод на нейронном уровне.
То есть надо уже рассматривать не отдельные сигналы и не отдельные нейроны, а популяцию,
которая каким-то образом моделирует. Ну ничего более конкретного пока мы это не можем, но это одно
есть направление, которое нужно посмотреть. И с этим Фристона не вели нас, например, вот на это.
А вот с этим горлышком это не связано с тем, что в этом моделире есть некоторая необходимость как бы
системы, потому что она просто не может... Если мы избавимся от принципа свободной энергии,
ничто не изменится. Вот такое впечатление, что все эти формулы... Фристона, на самом деле,
он не математик, математики так не пишут. Вот физик. Физик, да, вот физики каким-то образом
умеют рассуждать мимо формул. Они формулы видят гораздо больше того, что видят математики. И вот этот
переход, о котором говорила Докладе, вот этот Лагонаджан, одно дело жилище Лагонаджан,
он работает совершенно конкретно неизмеримыми величинами. Массы, скорости и так далее. А здесь этот
Лагонаджан, он метафорический. Он не содержит этот величин, который можно померить, поставить туда
и что-то получить. Вот как ты не... Но в этих терминах это что-то вроде некоторого значения понятия,
когда говоришь. Такое ощущение, что вот это славная свободная энергия, это некая метафора. Это нечто,
что надо не видеть. А вот переход к Балюсу это уже нечто более конкретно. Ну, я тогда еще два слова
скажу. Когда я готовила этот доклад, я решила как раз вот весь скепсис оставить при себе вот с открытым
сердцем, рассказать вам о том, что пишет Фристом. И на самом деле, если изменить внутреннее
восприятие, то и Фристом покажется более-менее ничего. Так что мы сближаем по этому. Таким образом.
И вот, значит, мы хотим из этих сигналов выделить некоторую квинтусенцию, забыть уже про внешнюю,
то есть не то чтобы забыть, но оперировать уже, скажем, здесь у нас мотожедания дисперсия. То есть
наиболее ожидаемые величины и их разброс. А полностью вот этот тряп нас уже интересовать не
будет. Тогда получится, что у нас представление КО, оно как бы базируется только на П и не базируется на
внешнем мире. Но П постоянно изменяется в зависимости от того. Мы все время получаем новые
данные и да. То есть и, соответственно, средние значения тоже постоянно изменяются. И поэтому все
время происходит вот такой обмен и подстройка. Вот это еще один термин из работ Фристомов,
то постоянная самоорганизация и подстройка одного как другой. Вот это хорошая вещь на самом деле.
Она очень хорошая, но она не Фристомовская. И куча из действия может влиять на внешние
состояния, оббирающие те или иные данные для П. То есть не только из П берет вот эти выжимки,
но и наоборот говорит сенсором, на что обратить внимание и что нужно смотреть для того,
чтобы подтвердить или опровернуть некоторые свои представления. То есть вот таким образом
реализуется активный вывод. Так, ну и наконец, я это уже более быстро пробегусь. Отсюда вот прямо
непосредственно из боясовских методов следует бутылочная горошка информации. И вот Фристом,
этому особое значение предает. Он говорит, что вот мы разговаривали в терминах биологических
систем, а здесь мы перешли уже в теорию информации. Оказывается, что здесь работают совершенно
сходные механизмы. И если эти механизмы сходны, то значит мы совершим великий прорыв. Сейчас я здесь
просто вот автор этого метода. Сейчас я расскажу, в чем он состоит. У нас есть некоторый сигнал,
который несет в себе информацию о другом сигнале. Причем один сигнал более такой большой,
а другой более маленький. Вот примеры такие. Информация об именах сфотографированных людей,
которая содержится в фотографиях. То есть в фотографиях у нас очень много информации,
и большинство из нее лишен. Нам нужны только именами. Или, скажем, информация о сказанных
словах, содержащейся в звуках ручи. То есть нам нужна только синавтика, а у нас есть звуковый
ряд, который содержит много чего, что нам не является необходимым. Поэтому нам нужно выделить
только релевантную информацию. То есть понимание сигнала X требует больше усилий, чем просто
извлечение Y. И нам нужно определить, какие же свойства вот этого большого сигнала является
существенной для узнавания Y. И вот здесь как раз используется способ бутылочного горлышка.
Мы пропускаем информацию через бутылочное горлышко и берем только необходимые свойства
X с волной, которая нам позволяет узнать Y, а все остальные отбрасываем. Таким образом,
получается двухступенчатая модель. Сначала мы из большого X выбираем характеристические
свойства X с волной, уже по ним легко определяем Y. Здесь я не стала выписывать модели уравнения,
но вот просто очень похоже, правда, на вот эти самые Q, которые берут квинтосенцию СП. И здесь
практически то же самое. Так, ну вот генгольц и бессознательный вывод. Это к делу почти не
относится, но это из уважения к Христа и к генгольцам. В каждой статье он обязательно на него
ссылается и говорит, что все это основано именно на выводах генгольца. Вот у него есть такой трактат
о физиологической оптике. И на этом основаны все построения Христа. О чем пишет генгольц?
Пситические акты обычного восприятия можно назвать бессознательным умом заключениями,
включая их от обычных, так называемых, сознательных заключений. То есть он говорит,
что любое наше восприятие сходно с заключением по аналогии. То есть мы что там многократно
видели и думаем, что, ну вот тут пригодится пример, скажем, вот поскольку в громадном большинстве
случаев стимуляция височной области сетчатки сходит от внешнего света, падающего со стороны
переносицы. Мы заключаем, что тоже имеет место в каждом новом случае стимулирование по уже
части часитчатки. То есть вот если свет падал оттуда, то мы знаем, что если сейчас как он стимулированно,
то значит свет снова падает оттуда. И что говорит генгольц? Мы не способны противостоять
оптическим иллюзиям, профессионально убеждая себя, что наши глаза играют с нами шутку. То есть
сколько бы раз мы не знали, что вот эти линии по длине равны, но мы видим их неравными, ничего
сознательными можем с этим сделать. Ну и второй пример, который он приводит, это сильный эмоциональный
эффект от театральной постановки, который происходит от неспособностей зрителя у саммитса в
визуальном восприятии с генерированными безсознательными выводами. То есть вот
безсознательный вывод, он всегда сильнее у нас. Ну тут мы сказать довольно грубое аналогия в
терминах почти к нитеку. Мы сказать, что наш сознательный вывод, это некая программа,
которой мы научаемся. Мы можем сознательно поменять способ распреждения. А безсознательный
вывод, это аппаратура, которая вот защита, она работает вот так и никак по другому. Вот именно
поэтому мы знаем, что это иллюзия, а все равно наши глаза видят, все равно они по-другому не умеют.
То есть тут получается, что Ку все-таки недостаточно сильный, чтобы изменить СП,
чтобы изменить состояние. А дело в том, что веку это только внешнее состояние,
состояние сенсора. Нет, Олег Петрович, это внутреннее состояние.
Нет, нет, нет, здесь немножечко не о том речь. Здесь речь идет о том, что у нас П настроена так,
что оно воспринимает только то, что воспринимает. И как бы мы Ку не говорили, воспринимай вот так,
воспринимай вот так. П отказывается от этого дела. То есть Ку не всегда может повлиять на П.
То есть потому, что у Фрисона нет никаких ограничений вроде бы для его модели.
Ну вот дело в том, что об этом как раз и говорит теорема о хорошем регуляторе,
который будет очень позже. Вот, тут все очень хитрозавязано. Но вот здесь вот главное то,
что бессознательное заключение по аналогии не является свободными актами сознательного
мышления. И поэтому они непреодолимы, от них никак нельзя избавиться. И при этом,
если вот это, грубо говоря, на аппаратном уровне врожденное, то дальше будет пример
благоприобретенную уже. То есть он пишет, существует множество примеров того,
какими жесткими непреодолимыми становится связи образованным многократным повторением,
даже если они основываются не на естественных, а на условных сочетаниях, как, например,
связи между написанием слова и произношением и смыслами. Ну и дальше вот я вставила такой
известный эффект струпа, когда цвета написаны в жрихтам не того цвета, которое они обозначают,
и восприятия от этого, соответственно, несколько затрудняются. Так, и вот вторая,
вторая общая особенность, вот она здесь выделена жирно, мы лишь по стольку обращаем внимание на
наше ощущение, поскольку они полезны для познания внешних объектов. На против мы привыкли
отвлекаться от деталей чувственных ощущений, которые не имеют значения для восприятия внешних
объектов. И в результате даже недостаточно просто сконцентрироваться, чтобы эти субъективные
ощущения ощутить. И иногда даже обучение не может заставить человека почувствовать то,
что он на самом деле должен чувствовать. Ну вот здесь был пример со слепой тикном,
не Сергеем проводили, но с собой опыты и находили свое слепое и визуальное тикно. Действительно,
его найти просто так достаточно сложно. Да, но для этого нужно специально сконцентрироваться.
Тем не менее, вы должны специально проделать какие-то действия. То есть когда вы
находитесь во внешнем мире, вы это слепое тикно ощущаете. То есть все время мы абстрагируемся от
многих таких ощущений, которые, да, действительно, если мы направим внимание, то мы можем их
ощутить, а так в принципе они нам где-то на перфере или вообще недоступны. Ну и наконец последнее,
это совсем чуть-чуть теорема о хорошем регуляторе. Это теорема на самом деле сформулирована
двумя кибернетиками достаточно давно, которая относится к регуляторам системы. И звучит она так,
каждый хороший регулятор системы должен быть моделью этой системы. Это относится и к любым
механическим системам, но переносятся точно так же и на биологии. Вот этот текст взят, уже не помню из какой статьи,
что касается мозга, поскольку он является успешным и эффективным регулятором для выживания,
он должен развиваться в процессе обучения за счет формирования модели или модели своего
окружения. Теорема это носит достаточно общий характер и ею очень так свободно пользуется и
фристом в том числе. А может у нас пример просто реальный из жизни? Трансформатор. Трансформатор
регулирует в том, что он там регулирует, электрически напряжение. Если трансформатор хорошо
регулирует напряжение, он является моделью сети, в которой он регулирует его. А в физиологии?
В физиологии вот, мозг является регулятором человека.
И поэтому он что, модель человека? Нет, он модель окружающей среды.
В мозге есть модель окружающей среды, такая, которая позволяет человеку выживать. А за счет обучения
она постоянно изменяется. Любой организм содержит внутри себя модель окружающей среды и как
пишет фристом более так сильно сам становится этой моделью. Да, она модель, да, она модель своей
окружающей среды. Ну простенькая. Ну по крайней мере он ни на кого не ссылается.
Ссылается он кстати как раз на теоремы о хорошем регуляторе. Ну и вот написанная ровно так.
Теорема не объясняет, что нужно сделать системе, чтобы стать хорошим регулятором.
Тем не менее эта теорема в кибернетике противопоставляется классическому
управлению в собрательной связи, то есть считается что это более правильный.
Ну и собственно. Это обратно на связь, это понятно. Здесь все туманно, нет? Нет, ну действительно
туманно, может быть потому что вот в этой формулировке модель все-таки употребляется скорее
метафорически чем точно, то есть что значит, что организм имеет модель окружающей среды. Это
значит, что у него на все возможные ситуации есть свои ответы. Но не на все.
Но тем не менее он как-то будет вести себя в любой среде. Ну когда это будет неожиданно,
значит у него модель не включает, то еще он сталкнулся. Ну придется из-за
каких-то вариантов разнооргенизма. Вот в этом смысле довольно грубо.
Ну я бы еще сказала так, что достаточно широкий класс изменений организм пытается игнорировать.
То есть пока не пройден какой-то рубеж внутренние состояния не меняют. Для этого
у него должна быть очень устойчивая модель окружающей среды, чтобы узнать, ну да,
вот это чуть-чуть изменилось, но на это пока реагировать не будем. Только когда некие пороговые
состояния переедено, то тогда происходит какое-то фазовое.
А требования быть модели окружающей среды, отмечают, что надо иметь ответы на все случаи?
Нет, ну вот известен же феномен, когда попадает опустошелайка,
уступит там, ну, предоточение, как говорят, не может существовать, и первым отключается именно
мост. Ну вот, ну, классический пример на большой высотечной разъематизации в салонной
республике. Отключается мост, и человек ничего не помнит, даже если он как-то живой. Ну, и в таких
примерах очень много. Также из глубинности, в состоянии, когда потеря ориендации, то есть,
как бы, нет модели, в состоянии, в каком-то плане. Ну, и тогда, собственно говоря,
тогда система перестает быть активной системой. Ну да, хотя еще он может существовать и может
воспринимать информацию. Ну, все, это уже... Отсутствие внутренних состояло что-то.
А это картинка книгу? Это картинка вот в качестве «Спасибо за внимание»,
картинка из Фристона, которая очень похожа на картинку из Хоккенса, ну и вообще,
на самом принципе, Хоккенса, когда у нас идет, да, с одной стороны, все время идет восприятие,
и, ну, то есть, идут восходящие, неисходящие потоки информации, которые постоянно друг с
другом взаимосвязаны. То есть, у нас идет восприятие, и от верхних слоев идет информация, которая...
Идут предсказания? Идут предсказания, совершенно верно. Если мы посмотрим на Фристонаску модель,
то это действительно очень похоже. Но, к сожалению, вот видите, даже здесь выписано уравнение,
они вроде бы вот такие сложные, но они не имеют никакого глубинного внутреннего смысла.
То есть, МЮ — это какие-то состояния, Ф — это какие-то абстрактные функции, а это их произведение.
То есть, ну... Это пенсия понятия. Понятия, да. То есть, ну, очень абстрактная, ну, хотя очень интересная.
Поэтому спасибо за мнему.
Всё, да.
Нет, ну ещё и Серёжа.
Теперь у нас Серёжа.
А что там, Дима, нас слышит?
Сейчас мы проверим. Дмитрий, вы здесь?
Я здесь, я слышу.
Отлично. А, всё видно, да?
Отлично.
Тогда мы переходим ко второму докладу.
