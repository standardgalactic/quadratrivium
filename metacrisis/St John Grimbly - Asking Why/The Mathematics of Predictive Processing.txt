Hi everyone, so this presentation is a follow-up on our introductory predictive coding presentation
which outlined first, what predictive coding was, and second, its links to deep reinforcement
learning.
So predictive coding is, at its core, a computational theory of the mind and of perception.
So Syngin and I are going to be talking more formally and more precisely about predictive
coding.
We'll be covering the mathematics of predictive coding and talking about computationally,
how exactly does the theory apply in practice.
Right, so as for what we're going to cover, first, I think we'll do a refresh or recap
on predictive coding. Second, we'll try to illustrate the core Bayesian inference or
calculus that lies at the heart of predictive coding using a simple but fundamental example
of perception. So I'll be talking about the mathematics of the model and how exactly the
example works, and then Syngin will take over and discuss the use and the need for approximation
in the model. He also talked about how this approximation procedure enables a neural
implementation and how all of this links up to heavier learning and free energy.
As for the discussion, I think it'd be great to have a kind of Q&A on the slack after we post
this. Hopefully we can answer all of your questions.
Okay, so let's do firstly a quick recap on what predictive coding is.
So recall that it was a computational theory of the mind and one of its main aims was to
explain how perception, namely how we condense and represent the external world, works in our
brains. So here the core idea is that our brains are Bayesian, that what they really are are these
multi-layer predictive machines. So we have these four key ingredients to the theory. So the first
is that all neural processes really consist of two streams. We have a stream of sense data,
which is sort of bottom up, and then a top-down stream of predictions. And here the stream of
predictions tries to minimize the error between what it predicts and what the sense data is.
And here the goal is to minimize surprise or free energy, more on that later. So ultimately
all of this is done to try to produce a kind of simple but useful or effective model of the
external world. So this last point about biological constraint, it's a kind of constraint on any
theoretical neuroscientific theory. And that's to make sure that ensures some kind of empirical
findings. And here the the constraint is one of locality. So first computations have to be local.
That means that neurons only perform computations on the basis of activity of inputs and associated
weights that are essentially nearby to them. They can't perform computation based on some activity
or information that's far from them in the brain or in the body. It must be things that are adjacent
to it essentially. The second is of local plasticity. So here we mean that the synaptic
plasticity is only based on activity of pre-synaptic and post-synaptic neurons. So by synaptic
plasticity what we mean really is how strong or weak the connection is between neurons.
And we're saying that that can only be a function of neurons that are adjacent to the synapse in
question. Okay so now that we've recapped and predicted coding let's look at an example where
perception occurs and let's see how it's modeled. Okay so imagine that we are some animal or a human.
We want to try and further value of a single variable from a single observation. To make
things more concrete let's assume we're trying to infer food size based on light intensity.
Maybe it's some fruit on some tree that we're interested in eating for example.
So at first perception seems really trivial. You might think that you know we just look at
the food and see the size of the food. But two things often complicate perception.
First is that we offer inferred quantities indirectly such as in this example where we use
light intensity to infer food size. And second it's often that our senses aren't perfectly
reliable. So in this example light intensity itself will be a noisy estimate. So let's just go
through that symbolically. So first we have V being the food size that's the quantity we're
interested in inferring. Then we have G being some nonlinear function relating size to light
intensity. So it might be that GOV is V squared for example. Then we have U being a noisy estimate
of the light intensity. Such that U is some random variable that's normally distributed
with mean GOV and variance sigma U. So here you might think sigma U is some kind of matrix
but it's not. It's a real number. We're dealing with U being just a single variable. So this is
all univariate. Then we also have some prior belief about V the size of the food.
So for instance our prior biology that some fruit is the size of say a rugby field is
essentially zero. So we have some conception of prior beliefs and we're going to be using that later.
Okay so I hope that those of you familiar with Bayesian statistics and thinking are seeing
quite a familiar cell. So how could our animal compute the expected food size explicitly?
Well we need to use Bayesian. So what we're interested in is the posterior distribution
P of V given U which is basically the food size given a particular light intensity signal.
So this posterior is given by Bayesian as the product of our prior distribution P of V and
that's you know before the light intensity evidence reaches us what did we think the size of the food
was and then the second part of the product is the likelihood P of U given V which is how likely
we are to observe a quantity U of light intensity given a size V of food and all of this is over
P of U which is a normalization term which ensures that the posterior probabilities of all sizes
integrate to one. So this is a kind of algorithm to find an exact solution
to this inference problem and our animal could use these calculated expected food sizes based on
the posterior distribution. Okay but there's also some problems associated with finding
exact solutions so what are those problems? So first it might be that the posterior might not
take on a standard form and that's bad in the sense that we can't use basic summary statistics to
describe the distribution you know for instance if we had a normal or exponential distribution
we only really need a handful of parameter values to describe them but when the posterior
distribution is not standard that's not possible anymore so it becomes very complex and difficult
to deal with. The second part is that computing that integral for the normalization term is highly
non-trivial and often very computationally intensive so it's not really a viable strategy most of the
time. So instead of finding exact solutions what we need is the physicist's best friend
and that's finding an approximate solution which Sinchen is going to guide you through just now.
Great so now we have an idea of what the problem scenario is and where the problem
in the scenario comes from. We can now think about the actual solution which we could try and implement
in a neural network or some kind of biological brain for example. So the issue that Jean-Luc
mentioned was that the integral which is implicitly required to calculate the value that we're interested
in can be very hard or impossible to calculate so how can we fix this issue while still retaining
enough information to be useful in the real world. For example how can we get enough information about
what the real food size is without inefficiently using resources to calculate this
integral in a sense. So the idea here is that we can instead consider some value with reader notice
phi which represents the most likely food size that this could be and want this value to maximize
p of v given u. So this value will maximize the probability that given some
light intensity u the value of phi is the most expected value in that case. So the posterior
density is then p of phi given u and using Bayes' rule again we can write this as
the probability of seeing that that size phi in the first place multiplied by the probability
of getting some light intensity given that food size and then dividing through by the probability
of observing that light intensity in the first place. The important step here that's different
from earlier is that the probability of u here does not depend on the expected the food size
phi that we're talking about. It depends on the earlier but the actual value we're considering
here is independent of this value. This is very important because the integral earlier came from
the fact that we're trying to calculate this denominator in Bayes' rule. So the fact that
this is independent of u makes this plausible to maximize this value without considering the
denominator. So we can maximize this value just by considering the numerator in Bayes' rule.
So to find this this phi that maximizes the posterior we can simply consider the log of
the numerator here. So recall I see the log is a strictly increasing function and it has
some nice properties of the log rules so you can just apply those to split up the multiplicative
terms. So we get and we define this as some value f which we chose for reasons we'll get into later
and this f can be split up into two terms the log of p of phi plus the log of p of u given phi
and then apply in fact that we know that or we assumed that the distribution of the probability
was normally distributed with some variance sigma and a mean of the non-linear function g
of v. We can use the fact that this is taking the log of this normal distribution
and get this nice form in the next line. The math there is very simple it looks a bit hard but
yeah it follows quite simply if you'd like to look into it. Of course more details are in the
paper if you're interested. So now we can consider this function we can update our our function
by considering the proportion the derivative of f with respect to phi. So we get this nice form
where we have the derivative of the partial derivative of f with respect to phi gives us these
two terms that are important what we consider later. So now we can consider the neural notation.
The notice of the two terms earlier look very much like some kind of difference between what we
expect to see and what we actually observe. So in the first case we have some the most
expected value we want to see is phi minus what we the value v of the posterior divided
through by this variance or this statistic of the distribution of how the values are distributed
in the function in the posterior distribution. So we can think of this as kind of a difference
that is weighted by how much uncertainty we have in our distribution and that is of course
a prediction error because we are finding a difference and our uncertainty gives us some
kind of weighting and this is essentially the difference between what we expect and what we
observe. The same is true for the next term where we have the most the light intensity u minus the
the light intensity we expect to see given some phi which is g where member g is our nonlinear
function and we're weighting this by the the variance in our observation of the light intensity
and these are important because we're going to be considering the dynamics of the prediction errors
and how we can propagate these in the neural network for example. So let's assume that the
v of p which is our food size of posterior sigma p and sigma u are encoded in the strength of some
synaptic connections in our network and then phi and epsilon p epsilon u and u are all encoded in
the activity of the neurons or the total neural population. So prediction errors can be computed
with dynamics just given by these two equations here which follow quite simply from a couple of
steps of maths but we can observe that just setting the less unsized to zero when epsilon
tends to zero we can see that the values converge sorry the equations converge to
what epsilons are defined as in the first line of this slide. So this is important because then
we have now some kind of update rule of the prediction errors or suspect to the prediction
errors themselves and with our values which are important in and encoded in our network.
So that's the key there that all these values here are encoded in our network.
Great so now I can talk about how this helps us with learning. So recall that we're talking about
what is most expected as in the food size and most expected really means the same thing as what is
least surprising. So we're trying to maximize the probability of u and we said that this was not
feasible so instead we consider some joint distribution of u and phi where phi is our
expected value of the food size and this is obviously defined as the probability of the
marginal distributions of p of phi and p of u given phi. Right so it's even simpler to maximize
the log of the joint distribution p of u and phi. So now we can talk about free energy.
So let's imagine we have this posterior distribution p of v given u and we're trying to
approximate it with some function q of v. So we can measure the difference between these
distributions using the Kaelbar virgins which is just defined as the integral of the approximate
function q times the log of the marginal distributions and divided by the joint distribution of u and v.
So we can simply spread this up into two terms the first being the term in terms of the q function
and the second in terms of only the p function. We can do this because the second term
the integral of q of v dv equates to one so we're left with a term of log of p of u.
So why is this important? Recall that earlier we said that the value p of u is what we're trying
to maximize because this gives us the value of the observations which are least surprising.
So by simply maximizing the Kaelbar virgins we are ensuring that p of u is maximized
because the Kaelbar virgins is non-negative. So how do we do this? How do we actually maximize
this p of u? Well we have to ensure that this integral the first term the integral over q v
times the log of q v divided by the joint distribution we have to make sure that that
is a lower bound itself because if that's a lower bound then we can maximize the
lower bound on log of p of u. So if we define this first integral as negative f then we can call
this the free energy and that's exactly what we want to minimize the negative f to ensure that
the log of p of u is maximized. So maximizing f is the same thing as minimizing negative f
so maximizing f gives the desired result. So hopefully this gives you some kind of understanding
of what free energy does and what it represents in the context of theoretical neuroscience and
maybe you have some ideas about how this can be implemented in deep RL. Of course this is
relevant to modern areas of deep RL in the sense that free energy relates well to predictive processing
concepts and active inference which can happen in models of RL where you are trying to minimize
your let's say free energy in a sense because you're trying to maximize your expected result
which is the same as trying to minimize the surprise you are having when you interact with
the environment. So if you'd like to discuss this further feel free to contact me or Jean-Luc
about the stuff and I'm sure both of us would enjoy chatting about your ideas.
Hopefully this format worked and it wasn't too boring. If it was just let me know about
we can improve it and yeah have a good three weeks of isolation.
