I'd like to introduce our first speaker of the day, Stu O'Connor.
Stu is principal infrastructure engineer at Unison Computing.
He's in charge of developing Unison Cloud with a team of awesome people.
And Unison Cloud lets you run your Unison code on fully managed infrastructure with
just a function call.
Stu is going to tell us about that now.
Take your way, Stu.
All right.
Thank you, O'Connor.
I guess I'd say of all the introductions I've ever had, that is by far the most recent.
But yeah, so here we are.
First talk of the day.
I hope everyone's excited.
You know, we're all very excited for sure.
Here to talk about Unison Cloud, like we said before, project we've been putting a lot
of effort into you recently, and we're super psyched that we're at the point now where
we can start showing it off.
And you know, so my goal today will be to try and get you as excited as we are about
Unison Cloud.
All right.
So why don't we dive right in and start talking about what Unison Cloud is.
All right.
All right.
What is it?
It's a simple approach to distributed computing.
Okay.
But why is it?
Well, because maybe distributed computing's gone wrong.
And what do I mean by that?
I mean, it's become really complicated and really hard.
And I think that maybe we're people in a good position to ask if we should be radically
rethinking some of the ways we're doing distributed computing.
Like, you know, we, as a company, haven't been scared to say, you know, step back and
maybe we're doing this all wrong, or maybe it's okay if we make a language that doesn't
work well with GitHub, or it doesn't exactly work well with LSP, the language server projects,
because we think storing source code and files are just an outdated way of thinking.
And so we want to start off by, you know, talking about how software development used
to be just like a lot simpler, like, you know, you just write a program and run a program.
Let's go back to how things worked on my first computer, a TRS-80 back in, like, I
don't know, 1982 or 1983 or whatever it was.
So here you go.
You would just fire up basic A, write some code, and then you'd hit, look, F2 to run
your code.
Like, how could it be any easier than that?
Or, I don't know, I guess you could hit F6 for Tron first, or that's cool.
But you know, and actually, if you look carefully at this basic or know much about basic, you
probably don't want to run this program.
But I don't know, you know, things were simple, you hit F2 and you run.
But of course, you know, new, this wasn't always going to be this simple, and the world
was changing, right?
Because now your computer's online, but that's cool.
But hey, you know, software development still could be pretty simple.
Here we are running some CGI bins in Perl, and look, people are signing up to my guestbook,
and this is still fun.
You know, software is about to become extreme, and it's understandable.
Because now, well, everyone's online, right?
And everything is online.
And that's going to change the way we write software, and it's going to become, like,
a lot more complicated, right?
And then, like, our data's going to get bigger, and we're going to have to change the way
we think about how we deploy software so we can process all that data.
Not only is all this getting more complicated, but the way we deploy software is getting
more complicated, right?
Like, remember, we used to, like, upload jars with some classes in them, and then we had
web applications, and we were deploying war files with, like, you know, war XML, yay.
And then we decided, like, enterprise Java beans or whatever the heck Java server face
was were, and we had ear files, and this accelerated, and things are very fast, still becoming,
you know, more complicated, right?
I mean, this is, like, an old snapshot of AWS from, you know, years and years ago, and
now there's, like, 10 times as much of this stuff out there, right?
So let's talk about, you know, some other ways people are deploying software now and
how many ways it goes wrong, right?
Maybe this is how you're distributing software today.
You build a Docker container, you push it up to your company's repo, and, I don't know,
you ask Kubernetes, can you run my Docker container for me?
Run my thing in 23, and, like, you know, that's the right version, and, like, no one wrote
over my version, and, like, yeah, fingers crossed, like, we're all rooting for you.
What could possibly go wrong, you know, or maybe you're working with Spark, right?
You make a big fat jar, it has all your dependencies, and then you try to get the Spark cluster
to run your job for you, and I hope you realize that the DevOps team finally got around to
upgrading your Spark cluster, so now you don't have the right version of Spark, and things
are not going to go very well, and, you know, the dependency problems with Spark, they never
go away, right?
So, like, Spark's going to be, like, hey, you got the right version of ACCA on there,
and you're going to be, like, I don't even know what ACCA is, and it's just, like, yeah,
fingers crossed, Godspeed, you know, here's one I'm sure a lot of you would hit, right?
Only SPT is, like, your build is broken now for some mesoteric reason, and you're, like,
oh, my God, there's only one person in our company that understood SPT, and now he's
gone, so you're, like, going to stack overflow when you're trying to figure out how to fix
SPT in the SPT communities, like, no big deal, SPT is just, like, a big applicative builder
under the hood, and once you get that, you're golden, and you're just, like, I don't, I
just want to do machine learning.
Now I'm, like, having to figure out the SPT guy, and the next thing you know, you are
the SPT guy, and you're just, like, oh, my God, I hate my life, it's going to work on
my software, right?
Okay, but we're going to be done with all that now.
My goal is going to be convinced to you that, you know, Unison Cloud is going to make all
that stuff obsolete.
Like, we're going to go back to, or we're going to at least try to go back to that happy
place we're in before.
We just run a program, and run a program, and that's it.
So I think Unison Cloud is going to get to be feeling more like that, so let's talk
about what it might look like to write software for Unison Cloud.
It all starts with this remote stability, and the most fundamental thing you can do
with remote is fork, and what does this do?
It starts a possibly remote computation that will produce some value of some arbitrary
type A at some point in the future, and so that's a lot right now.
So we're looking at this type signature of fork.
Let's take it apart and see what it says.
All right, so we see there's two arrows, so you know it takes two function arguments.
One argument is the thing we want to run, and the first argument is where we want to
run it.
Return type is going to be a value of the type path A. This is like a location where
a result can eventually go.
This is something you might be familiar with from other languages or libraries as a future.
And so we want to get the value of our future.
We can await the task, which should produce an A. It might fail, which is why calling it
requires the remote ability here, because it might need to call remote.fail to fail
the computation.
All right, let's take a closer look at this argument, the computation we want to run.
People that are less familiar with Unison syntax might not be as familiar with what
this type says, so let's break it down.
I mean, especially, you know, we're here first talking for the day, and we're just getting
warmed up, right?
So the first thing we want to look at is that single quote at the beginning.
That's Unison shorthand for a delayed computation.
It's just shorthand for a function that takes a unit as input.
This is totally equivalent syntax.
So like, if the computation is not running yet, it just needs a little nudge of you handing
it that unit to get things going.
The other thing that people might not be used to is the bit inside the braces there, which
is the list of abilities needed when you run the function.
So this says that when you do pass that unit argument to the function in order to start
the computation, it has to be done in an environment that has access to a remote ability, and it
has to be run with an inside and exception handler.
The G parameter is a placeholder for anything else your computation might need when it runs,
and we'll talk more about that later.
But what this signature does tell us is that we know for sure we're going to need at least
remote and an exception handler.
All right.
So let's go back to that location argument from our fork function.
What is that argument?
Location is a logical location of a running node inside your cluster where we could want
to run a computation, but how do you get your hands on a location?
There's a couple ways.
First is you can always get a handle on the current location by calling here.
And then there's also another special value called current region, which refers to any
node in the same region.
And then given those two, we have two other functions that help us discover even more
locations.
We have a near function, which we use to find a location that's logically near another location
that we already have.
And what does near mean?
Well, today near means here, but we hope in the future we can be a little bit smart about
that.
Maybe we could say, I know you want to be running here, but here's a little full or a little
busy right now, so we're going to get somewhere nearby.
And then just like near, we have far, which does a similar thing.
It says, I want you to run somewhere far from here, but how far is far?
What does that mean?
Could it be anywhere?
Well, I want you to notice that these two near and far functions, they take two locations
as input in order to produce one.
So if we look at how they're actually used, it should hopefully make a little more sense.
So here we're getting two new locations, near location, far location.
The first one, remote.near, current region here.
This says find me a location in the current region that's near to here.
The second one similarly says find me a location in the current region, which is far from here.
And like I said, in general, right now, near is going to mean here, and far is going to
mean anywhere but here.
But at least we have this bounding box, as we can say, but it has to be in the same region.
This is something that we think is quite potential too, because we hope that soon we're going
to be up in multiple regions.
Maybe you could say, find me a location that's inside US East 1, or find me a location that's
inside US West 2.
This is something I think that has a lot of exciting potential because cross region computing
is something that in the past has been really difficult, and maybe we're going to find a
better way to make it simpler.
You might notice there's these G parameters on the side of the location, which I've sort
of been ignoring.
So let's talk about those.
So this G refers to abilities that you need to have as part of the location that's returned
to you.
So these are abilities we're going to need in order to execute our code.
So the way this works is when we call near or far, we can specify that we need the location
to come back and have some abilities for us in order to execute our code.
But what are the possibilities?
So we have a few, and we're hoping we're going to bring a lot more.
But what I'll do for now is I'll talk to you about the abilities that we've got to start
with.
And later we'll talk about the potential for more abilities.
So here's one.
We have a channel's ability, and this is one that will be available at every location.
So if you ask for a location with channels, any location will work.
And channels are kind of like message cues.
And what I have here is a simplified version of the channel ability.
The real ability has like a bunch more possible operations.
But what I wanted to do is to pare it down to something simple so you can get an idea
of what channels it is and not overwhelm you.
So this shows you basically, gives you an idea of what you can do with channels.
So you can create channels, you can send messages to channels, and you can receive messages
from channels.
You can choose if you want to receive messages in a synchronous or an asynchronous way.
And you can see this is a lot potential here for things you could build out of this.
You could build something that looks like an actor system.
Maybe if we start rethinking how we're going to do services, this is a way that you could
send messages to a service with instructions on how to get a response back.
But we think of these first few abilities and the remote ability, we really think of
as a low level something that we're going to be able to build a lot of other cool stuff
on top of.
So this channel is a nice little simple interface, but lots of potential.
We also have a scratch ability, and that's again one that's going to be for now available
at every single location.
And again, this is a slightly simplified version for demonstration purposes.
So I just want to give you an idea of what it feels like.
But what is scratch, scratch is non-durable stores.
You can use temporary storage for values that you might need to store for a short amount
of time, maybe because you're going to store value because you have more than one computation
read from the same value, or maybe you're going to catch a value that you might want
to refresh later instead of recomputing.
And again, like channels, this is like a very low level interface that we're excited about
all the things we're going to build out of it.
Another ability we recently added is HTTP.
This is what you'd expect, it's a way to do HTTP gets, it's a way to do HTTP posts.
Of course, we handle deletes and patches and puts and all that stuff on the hood, but I'm
just showing you the basics to get a feel for it.
And it does, you know, acts roughly as you expect.
You give it a URL, you get a response, you give it a URL in the body and you can posts
and get a response.
But this one's a little bit curious if you're paying attention, because you might say,
no, why would you need an HTTP ability?
Like I've already seen you have an HTTP library, why can't I just make HTTP calls?
But you know, think about what the value proposition is here.
The value proposition is that we're going to run a cloud and we're going to let you upload
your program to our cloud and we'll run it for you.
Right?
But we can't just let you, you know, run every code, any code you want.
Right?
We're not going to let you do arbitrary, like for example, we're not going to let you run
around and see what's on our file systems and we're not going to let you maybe make arbitrary
network connections and stuff like that.
And interestingly, because of the way Unison is structured and the way it works, it's actually
easy for us to do.
We can scan your program before we run it and look around and see if we see any of the
hatches that refer to any of the IO buildings and just say, you know, I'm not running that
because I could see for sure you're calling, you know, IO.openfile.
So the HTTP ability is a way for us to say, all right, you can't do any IO, but we'll
do this stuff.
You can make some fetch some web pages and stuff like that.
Right?
All right.
So moving on, let's talk about how we might write suffer for remote.
Right?
So here's a little program.
Calculates a very important answer.
It calculates the number 42.
So let's look at this and see how this works.
Well, so you look at the type signature, you see that what we've got is a delayed computation.
There's a little tick mark there next to remote.
So it's a delayed computation that needs the remote ability to produce a natural number.
Right?
And how does that do that?
Well, it creates two tasks.
It creates a 20 task and a 22 task.
And it decides where it's going to run those by picking random locations using our far
commit.
Right?
So it's far region here.
So that's going to pick any random location that's hopefully not here.
And then we see it's got a very simple little computation.
20 is let, you know, 10 plus 10, and 22 is 10 plus 12.
So we fork both of those tasks.
They're off and running on different nodes.
And then what we can do is call our wait function and say, I'll wait for those results to come
back and add the two results.
All right.
This is, you know, not too bad.
Right?
And it's pretty cool.
When you think about what's going on here, we have three computers all collaborating
on doing one computation.
Right?
And we're moving, we're moving functions across the wire because that's something, you know,
Unison knows how to do.
We don't have to serialize anything.
Right?
And we're doing, so we're doing this network computation, but you don't see us talking
about the network too much.
Right?
And we don't have to talk about serialization.
The values are all going to be magically transported first.
And this is something that's, you know, special to Unison.
Our ability to serialize closures and functions like this is, you know, one of the crazy things
that we decided to do that's going to make a lot of the stuff, you know, possible and
easy.
So there are, of course, a few foot guns there, right?
You can't necessarily send an open file across the network and expect you're going to still
be reading from the file and it gets to the other side.
You couldn't hand an open network connection or, you know, we can't close over an mvar
or a tvar or, you know, these kinds of things.
Other than that, you know, Unison is very helpful as far as getting our values around.
And like I said, you know, we don't have to talk about the networking.
We don't have to talk about the serialization.
What we're going to end up talking about more is our problem domain, the thing that we actually
want to spend our energy on.
All right.
So how are we able to get all this to work?
I guess it's really interesting to, you know, talk about, you know, again, why Unison is
special and how we're able to make this even happen.
So here's a simplified version of our 42 program that's just adding two numbers, right?
But it's important to remember that this is not how Unison stores your code or thinks
about your code, right?
Because we don't store your source code.
We turn everything into AST and we store the AST, all right?
So here's how your PROD program would be represented as a tree.
But the other important thing about Unison is remember that it's content address, right?
So Unison hashes all the parts of the tree.
It generates a hash for the entire tree and refers to the function now on from then on
as your hash.
So here I am showing a two byte hashes just for brevity.
Inside of course, we're using shot 512s.
But you know, what this means is that if you have the hash of a function in Unison code
base, you can go look up the definition of a function.
And so this is what can make it very easy for us to distribute your software for you.
Let's look at how we might do that.
So Unison Cloud, we want to run a function.
We just simply say, hey, Cloud, can you run this computation for me?
And we just have to refer to the computation by its hash.
The cloud maybe hasn't seen this hash before.
And so I can ask us, can you send me the tree for that hash?
And we can respond and say, yeah, sure.
3DF7 is the subtree 9E46 added to the subtree C63E.
And for a simple example, we're going to assume the cloud has heard of 22 before and the cloud
has heard of 20 before.
And so it's got everything it needs now to run our program.
But with a larger program, this might take a couple back and forths where it's going
to say, OK, now I actually need you to tell me what 9E46 is too, if maybe that was a whole
other AST.
But we know that by recursively asking over and over, we can get your whole program up
to the node that you're talking to, the one node you're talking to.
But of course, Unison Cloud is going to be made up of many nodes.
It's going to be a peer-to-peer network that collaboratively works to execute your computation.
So how's that going to look?
So we're going to start by picking any random node in the cloud.
And this is going to largely go as it did before.
We're going to have a conversation with that node, and we're going to make sure that it
knows everything about our application.
I mean, it's going to be ready to go.
And then what is it going to do?
In this case, it's going to go pick two random other nodes to help out in this computation.
And it's going to have a similar conversation with those nodes where it can make sure that
the nodes which are going to execute parts of your program have the parts of your program
it needs.
And this is something I think is super cool.
We don't have to worry about how we get the software to the nodes.
We know we can do it.
And it's all automated.
But it's kind of a shift in the paradigm and the way that we're doing things in our
cloud.
Like, instead of shuffling data all over our cloud and moving it from one microserver to
another, or maybe moving it into Kafka to move back to a microservice, which is going
to put it back into Kafka, which is going to come back, that's another microservice.
And we're shuffling the data all around.
But maybe that's not the smartest way to do things, right?
Here, what we're showing is we can, instead, move the code to the data so that the code
just goes to where the data already are.
And that's because these days, remember, our data is big and the data has gotten a lot
bigger than the code.
So let's stop shuffling the data all around inside of Kafka, outside of Kafka, back into
Hadoop, back out of Hadoop, and let's just see if we can move the code to the data.
So like I said before, a lot of these things that we're building on now, like the forking
tasks, waiting tasks, strategy space, channels, these are all building blocks that we're excited
that we're going to build bigger and better and more interesting distributed libraries
on top of.
So spend a little bit of time talking about one of those that we've already worked on,
which is this seek.
And this is one that, if you go to our blog, Paul wrote a great blog post introducing this
called, I think it was Spark-like data structures and 100 lines of code, but all right, let's
talk about what a seek is.
It's a distributed collection that's built on top of remote, like I said, and it's also
built on top of scratch.
And if you're familiar with Spark, you might think of this as being a lot like an RDD.
It's like a distributed list, maybe, because parts of the list on different nodes around
the cluster, but all in all, it makes up a list of a lot of things, spread across the
cluster.
So I listed here some of the things you can do with a seek.
The first two should be obvious, and we've got map and reduce, right?
And this is the thing that makes it RDD like maybe.
The next two are the exciting ones, memo and memo reduce.
That's where we're using scratch space.
What those memo functions allow us to do is to take a computation that we're working
on and just say, hey, let's do a checkpoint and just want you to save all the data to
some temporary spot, and it won't keep going.
And the idea here is that then if we're ready to go back and repeat this, we've already
got a part of our computation is already stored away, and we can skip that bit.
So if we have to rerun a computation, we might not have to recapitulate everything and think
about this.
This is something that happens to people running Spark all the time.
You've got your Spark job that's been running for hours and hours, and you realize, oops,
I made a mistake in the final reduce.
I'm going to have to redo that, which means reprocessing everything we've done so far.
But here, this is going to save us.
This is going to say, hold on, we already know these values.
And Unison again is very uniquely positioned to be able to do this because we're able to
say things like, what's the hash of this function?
And later, we could say, has anyone ever calculated what this function returns?
Because we're really good at saying, OK, if you know about this hash, I know the answer
is all right.
All right.
So what I want to do now is to show off something that's a little bit more of a real piece of
software, not like calculating 42 or some Fibonacci numbers or something like that.
Because I want to show off that we're starting to do real stuff with Unison.
It's not just a toy anymore.
So me and one of my teammates, Cody, we decided to do some real work on a real data set.
See if we can do some real machine learning or something like that.
So this is the data set we got from Kaggle, which is a great place for learning machine
learning and related fields like that.
They got a bunch of cool problems that you can work through to learn about that kind of stuff.
But they also have a lot of cool data sets that you can just play with.
So this is one that we picked.
It's got 700 or so JSON files about asteroids, which are in orbit around Earth.
And they, with ones that have recordings about when they made close flybys in the past.
So we decided to see if we could crunch this data set and just do something that's more
than just, like I said, you know, making Fibonacci's or adding ones together, counting words.
So here we're going to go through all the asteroid data and see if we can find the closest
ever flyby of one of these asteroids.
So let's look at some Unison codes and see what this looks like.
So we start off, we made some simple data types just to store the data that we're interested
in collecting.
So we got asteroids and their names.
And we've got all these lists of previous times they had close approach to Earth and
how close they got, how fast they were going.
All right.
So now we, what we did is we uploaded all 700 and whatever files of the S3 and into one
of our S3 buckets.
And so, you know, here's the code which uses our HTTP ability we talked about before to
try and download a file from an S3 bucket.
And my goal here is not to try and get everyone to understand every line of this code.
But, you know, it should be pretty easily understandable.
You know, it should be able to see what's going on here.
Like I said before, we're declaring our HTTP ability, we're constructing a URL, we're making
a GET request, we're turning that into a response, and we check to see if it worked.
And if it did return, we interpret the body as UTF-8 text.
Cool?
All right, what are we going to do with this text?
It's JSON.
And we got JSON libraries.
So this is a JSON library.
This is showing how we're using a JSON library that, again, Cody and I wrote.
We modeled it.
It was very much inspired by Cersei or Argonaut from the Scowland, which is the one that we're
most familiar with.
And I didn't show all of the parsing data.
I did show just the asterisks.
I'm not showing the flybys and the dates.
Again, I just want to get you a little taste and show you that we're doing real stuff.
All right.
So we parsed all JSON.
We decided our goal was to go and find the closest asterisk, right?
So we're going to need a little reduced function that can compare two asterisks.
Here you go.
You know, this is going to go through and look at all the flybys, find out which closest
flyby is where I give an asterisk, right?
And then compare the two asterisks just by comparing floats.
That's a simple enough, right?
So at this point, we're kind of ready to put our job together.
So let's put it all together.
So here it is.
This is our program.
This is us using seek to find the closest asterisk flyby, right?
And it's easy to read, right?
What are we doing?
We're taking all the files that we're asking seek to chunk those up in bunches of
hundreds.
So we're going to take batches of a hundred of files and we're spread them all
around the cluster.
Okay.
And then in parallel, we start to get them all going and they're all going to start
fetching that stuff from S3.
And then we parse the JSON and we're going to call our cool memo function.
And then we're ready to reduce.
So here's the really cool part, right?
Let's say we decided, you know what?
Closest is not as interesting as fastest.
So we're going to change our, change your computation that I don't want to
reduce and find the closest asterisk, right?
I'm going to reduce and find the fastest asterisk, right?
And that's simple, right?
I'm just changing our one reduced call from closest to fastest.
Easy, right?
But here's the really cool thing, right?
When we go to run this computation again, it's going to be a lot faster, right?
And there's two reasons for that, right?
So one thing you'll notice, like I said before, you know, this, this program is
very similar to our previous program, right?
So remember when we're doing that conversation with this in cloud and we're
saying, Hey, can you run this hash?
And it's going to say, what is that?
Well, now Unison's already going to know what all file names are.
It's going to already know how to turn asteroids from JSON into asteroids.
So when we go to say, can you run my program, all we're going to need to send
it is just the tree for asteroid that fastest, our new function.
It already knows everything else.
But then, of course, when we go to run the computation again, we aren't going to
have to download all the files again.
We're not going to have to parcel JSON again, because our super, our super
cool speak dot memo function store all that stuff in our scratchbilly.
And so now when we get to start our computation, get to start right from the
reduced start, which again, super cool.
You know, I think this is something that could save a lot of computation time.
All right.
So, you know, nothing, I mean, we never talked about this all time as we never
talked about, like, how do you actually run something and use the cloud, right?
I got two ways here, right?
And you do these right from inside UCM, right?
So I did the thing where you're managing your code, right?
First one, remote.pure.run.
This is a way to run a remote computation, but run it all locally.
And this is a great thing for when you're still developing your software.
When you run with pure.run, everything happens locally.
So like calls to near or calls to far, all just going to return here.
Because that's the only place we know about.
Once your software is good to go, you get to run it.
Once your software is good to go, you get to run it in Unison Cloud.
And you use our second way, right?
Remote.cloud.run.
This one is going to do all the thing we talk about.
It's going to send your computation up to the cloud,
close to a cloud node, to make sure that a cloud node has your whole program.
It's going to start running it and we'll wait for an answer to get back, right?
How simple is that, right?
It's pretty easy, you know?
Is it easy hitting F2?
Maybe not, but we're getting pretty close, right?
This is going to be fun again.
And it's going to be easy.
All right, so where do we go from here?
Like I said before, we're just getting excited and we're super excited.
The things we're producing now are the low-level building blocks that we're going to build on.
You know, so for example, you know, the way we're able to combine the remote and the scratch
in order to make the sneak interface works.
About other things we're going to do like that.
Like we think we can take the channels of interface along with the scratch interface
and make a distributed hash table, right?
And we're not, we're planning on not only having a scratch space,
but we're going to add some persistence storage.
So something that won't necessarily get, you know, pushed out.
Something you can guarantee on the game there.
Our most recent hire, Travis, he comes with a whole ton of Postgres expertise.
And he and Runar are often running and getting us a Postgres library.
That's that, and I think that's going to really enable a lot of people
to do more real work inside Unison.
And another exciting development that's going on with Unison that we're going to talk about later
today is that Dan Dole, our compiler engineer, is working hard on getting us to native compilation,
which is super exciting to me, right?
And it's exciting to me for a couple of reasons.
One is that, first of all, things are going to start going way faster, right?
Like hundreds of times faster, I think.
But more exciting to me is that that's going to open up possibilities for us to be able to do
FFIs by linking to C libraries, right?
And I think that's what's going to get us to being able to have a GPU library.
And that's super interesting.
Think about being able to say, hey, Unison, I want you to launch this computation for me,
but do it on a machine that's got GPUs so I can use CUDA to make really fast vector math, right?
That could be like a game changer.
And so just the way we're rethinking how we're doing this distributed thing,
like I kind of hinted at that before, we want to maybe rethink about how we do services.
Maybe we can think of a better paradigm instead of a jumble of microservices that
might not be agreeing on that version of stuff.
So what else is next?
Soon, we're getting ready to launch the Unison Cloud Free Tier, which I think will be exciting.
The idea here is we want people to be able to start playing with remote and channels
and scratch and seek and all that good stuff as soon as possible, right?
So the idea here is you'll be able to fire up Unison, authenticate through GitHub off and
remote.cloud.run and send us your computations and we'll run them for you.
So hopefully that's going to be coming soon.
The idea will not be that we are going to run all your computations,
but we want to give you, let you have some limited playing time and see if we can
help get you, like I said, as excited as we are about all this stuff.
So with that, that's all I've got.
And like I said at the top, my goal was to try and get you as excited as we are
to rethink how we're doing distributed software and make it more fun and less,
I don't know, docker and spark and stuff.
So, but please, you know, I love to talk about stuff.
So please come find me.
I'm always on our Slack and I'm always ready to geek out.
So don't be shy.
But yeah, thank you.
