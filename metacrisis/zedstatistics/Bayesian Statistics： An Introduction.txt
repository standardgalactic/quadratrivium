Hey team, welcome to this introduction to Bayesian statistics.
Technically, it's the fifth in a series of six videos on statistical inference,
although this one will sit somewhat outside of the series. So it doesn't really matter if you
haven't seen the first couple. You can jump straight in here and get a nice little primer
into Bayesian stats. My name is Justin Zeltser. If you don't know me, I'm from zstatistics.com
where you can see the rest of these videos among a whole heap of other statistical resources.
But let's just begin with a little trace through the topics that we're going to be looking at
in this video. First up, we're going to just see how Bayesian statistics in theory compares to
what's called frequentist statistics. Now, if you've not heard the term frequentist before,
you might be surprised to know that it has been the type of statistics you've been conducting
without even realizing it up until this point. But this whole idea of Bayesian statistics
adds something a little new. Well, in fact, it actually adds a different frame of reference,
as we'll see, to the frequentist paradigm. We're then going to look at Bayes theorem itself,
which you learn when you do sort of what? Year nine, year eight, year nine mathematics, maybe?
That's high school. So it's not a very complicated theorem, but its application to statistics is
quite profound. So after we have those two under our belts, we'll have a look at a really cool
visual example. And this is where it all kind of connected for me when I first saw this particular
example. So hopefully you'll stick with me until then, because I think that's a really good way
of trying to get in your head an intuitive sense of how Bayesian stats works. We'll then apply
Bayesian statistics to a normal mean, a distribution for a normal mean or mean from a normal distribution.
We'll then look at two particular subtopics around Bayesian stats, ones to do with conjugate
priors. And finally, we'll look at credible intervals, which are the Bayesian equivalent
for confidence intervals. So let's start now by having a look at the differences between
Bayesian and frequentist statistics. And we're going to start with frequentist just to kick
us off in a direction we're familiar with. Now, in frequentist statistics, the focus is on the
parameter. And it's assumed to be a fixed constant. Now, if you've done any statistics before,
you'll actually be familiar with this concept. We always treat the parameter as some kind of
godly figure, right? It's unknown, but it's always fixed. And we try to do our best to estimate it.
So when we construct confidence intervals, we read them in terms of repeated sampling.
So technically, the way to interpret a confidence interval would be to say that 95% of similar
sized intervals from repeated samples of size n will contain theta, which is our population
parameter. Now, as I said, that will probably be somewhat familiar to you, considering you've
been conducting frequentist statistics up until this point. But I might just reemphasize the idea
that it is about repeated sampling. We have one fixed parameter, and then we assess things in terms
of a sample that's theoretically repeated. So we want to know what would happen if we took this
sample again and again and again and again, and what would the distribution be about our sample
statistics that we generate? That's a frequentist approach. Now, why is Bayesian any different?
Now, here the focus is on subjective probability, meaning that we're looking at it from the
perspective of the subject, the statistician, if you like. So in other words, we're trying to
assess the statisticians uncertainty. And we also take into account her a priori predictions.
So we're not so interested here in whether the parameter is a fixed constant, or we're really
focusing on is what the statistician can really know. So when we construct these things called
credible intervals, which I intimated before were the equivalent of confidence intervals
when we're dealing with Bayesian stats, we're going to read them in terms of subjective uncertainty.
So the uncertainty of the person doing the stats, right? The subject. So it's all well and good to
say that the parameter might be some fixed constant. But who really cares? It's me that's
doing the statistics. So the parameter for all intents and purposes could be considered a variable
from my perspective. So we can say that when we construct these credible intervals, there's a 95%
chance that theta exists within that interval, which is actually a much more convenient interpretation.
So that's really the allure of Bayesian statistics, you get these really nice interpretations
when you're dealing with uncertainty. And also there's quite a bit of power when in taking
into account my a priori predictions or my predictions about the parameter before we've
even conducted the experiment. Now if that still sounds a little bit airy fairy for you,
bear with me because we'll look at the Bayesian theorem itself and then we're going to do a
little example where it will all hopefully come together for you. So let's do that now. Let's
have a look at Bayes theorem. So what does Bayes theorem actually tell you? This is the formula
that you get given as I said in what year nine mathematics, maybe year 10 or something like that,
where it says the probability of B given A is some function of the probability of A given B.
So it sort of switches that conditionality around. And you might see this often in
biostatistics or statistics relating to health because we'll have some situation where it's
the probability of having a disease given that you've tested positive is some function of the
probability of testing positive given that you have the disease, right? That's the classic example
they'll always give which I'm surprised I haven't done in this particular video but you might see
that elsewhere if you have a look at Bayes theorem applied to health stats. Now instead of A and B,
I'm going to use theta and data here. So it's exactly the same thing but I'm saying here the
probability distribution of our parameter given our sample or the data from our sample
is some function of the probability of getting our particular sample given a particular value
of theta and then times the probability of theta divided by the probability of getting our sample.
Now we need a little bit of lingo here when we deal with Bayes theorem. So this particular part
of the formula is called the prior. So this is essentially our prior intuitions about the
parameter value. It has nothing to do with our sample, right? So we can actually construct this
without having conducted our sample in the first place. Now the next piece of terminology we need
to know is that this is called the posterior. So if that's the prior distribution this is the
posterior distribution which is the distribution of theta taking into account our sample. So once
we've actually fed our sample into our little algorithm here we can update our prior and turn
it into something called a posterior probability or in total you can call it a posterior distribution.
Now this little thing in here is called a likelihood and we've dealt with this in previous
videos of this series. This is just a distribution of possible sample outcomes
for every given possible value of theta. And finally on the denominator here this is some kind
of normalizing constant. The reason being when you multiply your likelihood by your prior distribution
it doesn't necessarily follow that the resulting distribution will sum to one. So this normalizing
constant just makes sure that our posterior distribution will sum to one which is important
because don't forget this is a probability distribution. So without this normalizing constant
it's technically not a probability distribution. Now the cool thing about the normalizing constant
which is the thing on the denominator here is that it's got nothing to do with theta.
So in that sense we can kind of ignore it and say that the posterior distribution is proportional to
the likelihood times the prior distribution. So much like in the previous videos of the series
we don't really care about the scale of our probability distribution here.
Given that our normalizing constant was just a constant all it was really doing was
multiplying everything by a particular value. So if we eradicate that we'll still find the same
solutions to all of our problems. When we try to find the maximum value of theta the maximizing
value of theta we can ignore all the constants in these equations.
So that's why we get this nice simple and very powerful relationship between
this thing called a posterior distribution our likelihood and then our prior distribution.
So just to summarize all that Bayes theorem is simply updating our prior intuitions about
the parameter with information we gather from our sample and then it becomes a question of
how much do we weight our prior intuitions versus how much do we weight the information we get from
our sample and that's what the rest of this video is going to be dealing with.
All right so that's enough theory for the moment let's tuck in to this visual example
and as I promised at the outset this is going to lock in a real good sense of intuition for you
if if you haven't got it already. So here's my example it's a very simple example to do with
the biasedness of an ordinary coin sorry I couldn't be more adventurous with this but I just think
visually it just makes the most sense so you'll have to forgive my lack of originality here.
So let theta equal the proportion of heads that you get from a set of tosses of a coin.
Now if you're trying to assess the biasness of a single coin you might start out by thinking that
that coin is unbiased but what does that actually really look like? This is an example of a prior
distribution where we have some inclination that this coin is going to be unbiased such that point
five so 50 percent of the flips will turn out to be heads but let's just say that we're not completely
convinced that this coin is going to be unbiased so we're going to have a distribution about our
prior intuitions. So what happens now is that we take a sample so let's take the simplest
example you possibly could which is just flipping one coin. Now let's say we've flipped one coin
and it's turned out that it's heads. Now how will this single piece of information we've gathered
affect our prior predictions? Well we can construct now a likelihood distribution for each
respective value of theta so what this means is that say if theta was one so if this coin was so
biased that 100 percent of flips turned out to be heads then of course the probability of getting
a head out of one flip would be 100 percent but if the coin was only so biased that 90 percent of
flips turned out to be heads then of course the probability of getting the data that we did was
90 percent so point nine so you can see this is going to be a one-to-one relationship right?
So in the event that the coin was unbiased you've got a 50 percent chance of getting that data that
we just got. So in this piecemeal way each of these possible values of theta can be assessed
for their likelihood of generating the sample that we got. Now the posterior distribution
is just some manner of product between these two distributions so even though our prior indicated
a very symmetrical distribution once we incorporate the likelihood which is weighted
more heavily up to the higher values of theta you can see that we're going to get a bit of a
skewed posterior distribution and essentially what's happening is that we're multiplying
each pairing of these two bars together so point five we're multiplying the height of this bar up
here for the probability in the prior to this likelihood and the same will happen for 0.6 and
0.7 and 0.8 etc but the posterior due to being a product of prior and likelihood becomes skewed.
Now I mentioned a bit earlier that it's some measure of a product. The reason why it's not
a pure product is again because of this normalizing constant so we're essentially taking each of those
individual probabilities from the likelihood and multiplying it by the prior and then we divide
by this normalizing constant which in this case happens to be 0.5 so if you want to check my
calculations and you divide everything by 0.5 you'll hopefully sort of get the heights of these
bars looking pretty similar but hopefully that's kind of given you a nice little picture of how
Bayesian statistics incorporates prior predictions of our parameter value with a particular sample
that we've got. All right so we're going to look at now Bayesian inference for a normal mean
so it's going to be a little bit more in-depth.
So let's consider a single observation taken from a normal distribution with mean theta and known
standard deviation sigma. Now I've simplified it by allowing the standard deviation to be known
so there's only one parameter of interest here which is theta.
Bayesian statistics can obviously deal with a situation where there's multiple parameters going
at once but let's just do one thing at a time and so I've simplified it by just allowing
theta to be our parameter. Sigma is therefore fixed as our variance or our standard deviation.
Now this is a likelihood you might have seen it before if you've dealt with the pdf the probability
density function that emanates from a normal distribution. So remembering our Bayesian relationship
where we've got the prior times this likelihood equaling our posterior while being proportional
to our posterior we now have to consider what kind of distribution our prior is going to be.
We have complete autonomy over that we can create whatever type of distribution we want for our prior
but keep in mind that it has to eventually multiply with our likelihood to get our posterior
distribution. So here's just that expression again using y instead of data which is the same thing
I'm just using the letter y here to represent our particular sampled value.
Let's consider a prior of the following form. So here we have again it's in the exponential
function a times theta minus b all squared plus c. Now that c is going to come out in the wash
because it's got nothing to do or it's not being multiplied in to any factor of a theta. So again
we can kind of ignore it we can say that the prior here is proportional to just e to the power of a
times theta minus b all squared. Now the significance of us choosing a prior in this form
is that it becomes really easy to multiply in with our likelihood to get a particular posterior
distribution and before we do multiply it through let's just re-parameterize this prior.
So instead of having a and b let's have tau and mu to Greek letters and the reason why I would have
re-parameterized it to look a bit like this as opposed to a simpler a times theta minus b
is that this is now in a form which represents a normal distribution itself. So here is a prior
distribution which is normally distributed with a mean of mu and a variance of tau squared.
Now put subscripts zero here because what we're going to find is that when we multiply this prior
distribution with our likelihood we're going to get a posterior that looks very similar
and that's indeed very handy so let's see how that plays out.
Now before we reveal what the posterior is going to look like keep in mind that we're
going to need a sample right because you need to multiply the prior by a likelihood and what's
a likelihood well it's the likelihood of a particular sample given certain values of your
parameter so we need that sample. So for the moment let's just presume the simplest sample
possible where I have one value or one observation which is why. So the posterior given that it's
a product of this prior and the likelihood that we saw before it's pretty simple because we have
two expressions within an exponential function that are getting multiplied together so the indices
just get added together so we've got this first bit here from the likelihood and this bit over
here comes straight from the prior so our posterior will be proportional to this particular expression
here. Now you can see I've got mu naught and tau naught here what I want to do is I kind of want
to try to express this in a simplified form and I'll do that in this next section here where instead
of mu noughts and tau noughts and y's and sigmas I'm changing it to mu one and tau one
and there's quite a bit of algebra that is required to get from this first one to this second one
but to spare you the agony of that I've just done it behind the scenes but believe you me
the values for mu one and tau one that eventually are given by these two expressions here so
this is showing you the relationship between our new mu one and tau one versus the old mu nought
and tau nought so in other words our posterior here is also going to be a normal distribution
because this is the form of a normal distribution but it will have a mean of mu one and a variance
of tau one squared where those two values are given by these two expressions so let's first
have a look at mu one so don't forget this is the new mean or the posterior's mean given a priors
mean and also the priors standard deviation so these particular formula hold here for where we
have a sample of size one and that's why I've called it mu one and tau one so what's happening here
is that our mean of our posterior distribution is some kind of weighted average of the mean
from the prior distribution and our single data point y you can see that y is kind of scaled by
its variance and the priors mean is scaled by its variance and furthermore you can see that one on
tau one squared so this is the precision when you have one on a variance we can call that the
precision so this is the precision of our posterior distribution it's again some kind of average of
the priors precision and the precision of our sample which just happens to be of size one
for the moment so you can see what's going to happen if our variance of our prior distribution is
quite high then it's going to tilt this weighted average in favor of our sample's value but on
the contrary if the variance of our prior distribution is quite low it's going to take a
while for our posterior to budge away from that mu naught value and the same could be said for the
precision over here so I'll repeat that it's a weighted average of our prior distribution
and the sample that we got now we can generalize this to a scenario where there's more than one
observation in the sample and let's just say there are now n observations in the sample so these
formula will change it's the same posterior distribution up here but if instead of using
mu one we use mu n which is now the mean for a posterior distribution after generating a sample
of size n everything's the same except we now have these factors of n in here with the samples mean
and similarly on the precision side instead of one on sigma squared we now have n on sigma squared
so not only do we have an effect of the respective variances here in terms of in terms of how this
average is being weighted but we also have this factor of size this factor of n so if our sample
size increases our posterior's mean will start gravitating to that sample's average the weighting
becomes more heavily tilted to the side of the sample away from the prior distribution
similarly with our precision if n becomes a lot larger the precision is going to be weighted more
heavily in favor of sigma squared which is the known variance from our sampling as opposed to
our priors variance now i should have maybe updated this formula up here to write mu n and
tau n up there but you can try to visualize that yourselves but hopefully that's given you a nice
little window into how a posterior's mean and precision or mean and variance is just a weighted
average of the prior distribution and the likelihood of generating our specific sample
all right let's move on so let's now have a look at this concept of conjugate priors
now you would have noticed in the previous example when we chose a normal distribution
for our prior distribution it eventuated in a normal distribution in the posterior
and that eventuality was very handy for us remember the way we were comparing mu one
to mu nought and tau one to tau nought and considering how it's like a weighted average of
all that stuff well that only could happen because we were dealing with the same distributions on
both ends for prior and posterior and when that happens we say that the prior is a conjugate
prior because it creates in the posterior the same distribution let's have a look at the
Bayesian equation again i'm not going to go over that for a third time but we often choose convenient
parametric forms for our priors such that the posterior remains manageable and as i said before
where the priors parametric form is retained through to the posterior this is called a conjugate
prior so don't be confused this is not to do with the likelihood but if a particular prior we choose
eventuates in the same distribution for our posterior we say that that prior is a conjugate
prior so what i've got here is just a list of likelihoods with the particular distributions
that would be conjugate for the prior distribution so when the likelihood's normal
choosing a prior that's a normal distribution will result in a posterior normal distribution
now this is actually called being a self conjugate which doesn't often happen for other
distributions you can see that if our likelihood is expressed as a binomial or Bernoulli distribution
we need a beta prior distribution to result in a beta posterior distribution
so the corresponding conjugate prior would be a beta distribution so for exponential and Poisson
it's gamma and for uniform it's Pareto and multinomial it's something called the Diraclete
which i'm sure is a french statistician from a bygone era
okay so it's just in here just editing this video and interrupting momentarily
to let you know that i was full of it when i said the guy was french turns out he's german
definitely from a bygone era but do a little research justin before doing the first run
of the audio that's my lesson and i also like how i pronounced it in a very french way
just to show how culturally aware i am anyway back to the show
if you're keen on diving a little deeper into conjugate priors i'll leave in the notes of
or in the description of the video a link to these lecture notes i found online from
carlos the third university in madrid they're written in english thankfully but they were
quite useful when i was trying to do a little research to figure out you know what these combinations
of likelihoods and conjugate priors were but i'm not going to let you escape that easily
we're going to have a look at the combination of binomial likelihood and choosing a beta
distribution for our prior to allow it to be conjugate so let's do that now let's have a
look at the beta prior so let's consider a binomial likelihood function which you would have seen
in your studies prior to this hopefully where you've got your parameter to the power of y and
then one minus the parameter to n minus y so let's consider a prior of this form which looks almost
exactly the same which is theta to the power of something times one minus theta to the power of
something else now the convenience of choosing something in this form is that like with the
normal distribution when you multiply this with the likelihood you're actually going to get a very
easy calculation so our posterior distribution itself will be of the same form but the only
problem with this is that this doesn't necessarily result in a probability distribution what do i
mean by that well the total probability density across all combinations of alpha and beta will not
sum to one and that's a problem because a prior distribution is a prior probability
distribution so while it's well and good to say that the prior needs to be of this form well
it still needs to be a legitimate probability distribution so to do that we divide by this
thing called the beta function so this itself is called the beta distribution which could get a
little bit confusing so let's try to get our terminology correct from the outset this is
called a beta distribution part of which is this beta function of alpha and beta but again this
just this beta function just acts as some kind of normalizing constant you can you can say the
parts of this prior distribution that deal with theta are on the numerator
so for the nerds out there this beta function is actually just an integral of what's on the top
here between zero and one so essentially it divides by the area under the curve right
remember your calculus from school your geometrical applications of calculus
by dividing by the area under the curve we insure insure that the resulting distribution
will sum to one so let's now actually write the posterior it's the product of the
likelihood function and this new beta distribution now as you can see i've actually ignored the beta
function because as i said before it's essentially a constant it's got nothing to do with theta so
our posterior is proportional to this theta to the power of y plus alpha minus one and then one
minus theta to all this stuff here so you can see our posterior is actually still in the same form
as our prior which is very convenient now the reason why i've chosen this one to to
highlight is that you can see hopefully the relationship between the posterior here and our
original likelihood function the difference is only these alpha minus one and beta minus one
characters so if this likelihood function represents what's happening in our sample
our posterior represents a sort of combination of what's happening in our sample and what we've
presumed from our prior distribution so we've got again two components the y component or the n minus
y component on this side and we have this alpha and beta component which comes from our prior so
in other words the prior distribution is essentially analogous to having alpha minus one additional
individuals with the disease and beta minus one additional individuals without the disease
now i've assumed here that theta is the probability of developing some kind of disease here but you
could just put with the event and without the event if you want to keep it general
so all you need to do is select a value of alpha and beta respectively
and you're essentially adding some individuals into your sample
now what's the effect of sample size if your sample size increases for a given prior distribution
so say alpha and beta is fixed but your sample size is now increasing quite clearly your posterior
is going to lean more heavily towards whatever's happening in your sample so again that nice sort
of waiting happens between sample and prior
all right so the final little piece of the puzzle is looking at credible intervals which is the
Bayesian equivalent of confidence intervals
so i've split this particular slide into looking at point estimation and then interval
estimation itself so i guess before we can calculate a credible interval we want to figure
out what the say the sample mean or the sample the estimation of our point of interest is
so i've answered this somewhat generally here's a particular posterior distribution that happens
to be skewed now if you got this as your output posterior kind of like the one we got when we
were looking at the the coin toss example it looked a little bit like this
what would be the best estimate you had for theta where would you put it
well of course your first instinct might be to say well let's put it at the top of this
distribution which is in fact the distributions mode if you chose to do that you'll be putting it
at the maximum posterior estimate which would make a lot of sense but that's not the only possible
place to put it if the posterior distribution is skewed as it is here you might want to make
it the mean of the distribution or maybe the median of the distribution and there may be decent
reasons for doing that the convention tells us to leave it at the mode but there might be some
decent reasons for making it the mean or the median it's not as necessarily clear as you might have
thought so how then do we construct intervals so say a 95 percent credible interval
so if we wanted to much like for frequentist distributions we can construct a region wherein
lies 95 percent of the distribution so let's just say that yellow region is 95 percent
therefore the distance between these two points the blue lines here becomes our 95 percent credible
interval but here's an interesting point to note I can actually shift these goalposts I can shift
these two blue boundaries and let's just say I don't want to put my credible interval here
I can also create another interval say between these two points where again there lies 95 percent
of the distribution so there's not one unique set of boundaries wherein there lies 95 percent
so we actually need to be a little bit more specific if we want to define one credible interval
so often we call the interval the hdi which is the highest density interval
and it's the particular interval where there's 95 percent between it so if I was to color this yellow
there would be 95 percent in here but also there is no point outside of this interval that has a
higher probability than the points inside the interval so if that condition holds then you
know you have the highest the highest density interval and that would likely accord with your
intuition at any rate now how do you actually generate these intervals well you're going to need
to use some some statistical software so I haven't dealt with in the video here how to calculate
your highest density interval although keep in mind what happens with your posterior distribution
when your sample size gets reasonably large is that it becomes is that it approaches a normal
distribution so even though this is very skewed this would only really happen with a fairly small
sample if your sample size increased much like everything in the world of statistics it will
approach a normal distribution at which point you can quite simply construct a 95 credible
interval much like you'd construct a 95 confidence interval because we know that a normal distribution
is symmetrical so you can start with your sample point estimate and then you can do a 95 confidence
interval just using the same formula you would use when you're constructing your 95 confidence
interval so I haven't quite done that for you here but I'm hoping that gives you an appreciation
for credible intervals but then also for Bayesian statistics as a whole well I hope you've enjoyed
it if you like these videos feel free to have a look at zstatistics.com or the youtube group
zstatistics I'll put all the social stuff up here and as always any comments feel free to throw them
up on the video thanks for watching and I will see you for the final video in the series shortly
you
