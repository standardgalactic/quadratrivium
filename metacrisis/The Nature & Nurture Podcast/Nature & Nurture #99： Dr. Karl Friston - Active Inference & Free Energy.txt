It's a great honor to be joined today by Dr. Carl Friston.
He is a professor of neurology at the University College London and one of the world's most
influential neuroscientists.
He invented statistical parametric mapping, voxel-based morphometry, and dynamic causal
modeling, all of which are statistical techniques foundational for modern neuroimaging, such
as fMRI involved in my own research and those of countless others.
Dr. Friston has authored or co-authored hundreds of scientific publications detailing out these
theoretical and methodological advancements in neuroscience.
He's also the mind behind the theory of active inference, the free energy principle in mind,
brain, and behavior, detailing out a theory of consciousness that you've called physics
of belief.
Dr. Friston, welcome to the Nature and Nurture podcast.
Thank you very much for inviting me.
Thank you also for that lovely introduction.
You've spoken of the brain as a fantastic organ, fantastic not only in the sense of being amazingly
complex, but fantastic that it's constantly generating fantasies.
That's absolutely right, and that speaks, I think, to a long-standing, a long-standing
sort of appreciation of the brain as a constructive organ, constructing hypotheses, explanations,
fantasies that best explain the sensorium.
So that's one of my favorite ideas, which I guess you can trace back to Kant, but certainly
very nicely articulated by people like Helmholtz in the 19th century.
And to my eye, resurgent in the sense that things like predictive processing have become
a really dominant paradigm in cognitive neuroscience.
And I would imagine some of your FMRI studies, the notion that the brain is actively there
as a statistical organ, constructing fantasies that best explain what we see and what we hear
and what we feel.
Absolutely.
And I'm a developmentalist, so I study adolescent brain developments, which I'm hoping we can
get into later once we lay out the groundwork for the active inference theory.
Now, I know that your early career began in psychiatry, and you did a lot of research on
schizophrenia, which you could call a mental illness of the brain being too fantastical.
Does this connect to what later became the groundwork for active inference?
Yeah, in a very deep and pragmatic way.
I mean, much of the theoretical neurobiology that you refer to, things like the free entry
principle and active inference, that came from an approach to understanding psychiatric
disorders as a kind of false inference.
So the idea here was, if psychology is all about predictive processing active inference
sensemaking under uncertainty, then psychopathology must then be a reflection of some kind of
false inference of the kind that as a statistician, you would know as type one errors or type
two errors, inferring things are not there when they are, as in neglect syndromes or
inferring something is there when it's not such as in hallucinations and delusions.
So that foundational training, clinical training really set the direction of travel in terms
of the theoretical developments.
Interestingly, in your introduction, en route in that direction of travel was the application
of the machinery of sensemaking to imaging time series data.
So using exactly the same techniques that we are now supposing is a metaphor or indeed a
description of the way that the brain makes sense of sensory time series or sensory streams.
We were using exactly the same mathematics and modelling approaches to make sense of
fMRI time series and brain imaging time series.
But all that started with schizophrenia research, specifically.
Is this because what the brain is doing when it's making predictions is it's all fundamentally
probabilistic because the world is constantly changing in front of you and there's uncertainty
involved?
Yeah, I think that's a very nice way of expressing it.
You know, sometimes cast in terms of making decisions, whether they're perceptual or sort
of choice behaviour like decisions, they're always under uncertainty.
So as soon as you mention uncertainty, you are implicitly committing to a probabilistic
description of either those decisions or the way that we try to explain our sensations.
But I probably go a bit further than that.
I mean, if you look at all physics, it's all about uncertainty and measurement.
I mean, quantum physics just is a theory of probability distributions.
And if you see those probability distributions being updated or you put measurement into the
mix, you basically have a statement that physics is inference.
And I think that theme follows through in terms of the mechanics behind statistical
thermodynamics, for example, it's all about distributions.
And indeed, most of the things that we measure can, such as temperature, just depend upon
probability distributions.
And from the point of view of active inference, our inference about those probability distributions.
There's a very elegant pairing of something objective and physical, like entropy with
something that we normally think of as subjective and perhaps not able to be quantified like
belief, but then what this active inference theory is doing is that when we're making predictions,
there's going to be a certain degree of error in our predictions and our beliefs.
And you can quantify that with entropy, right?
And what the brain is trying to do is minimize that entropy or uncertainty or free energy.
Are these all interchangeable terms here?
Yes, I think that's absolutely right and, again, well expressed.
Let me sort of slightly nuance it because there are different perspectives
on these different descriptions of probability distributions.
So the free energy principle as an account of self-organization is an explanation
for why certain kinds of systems avoid the natural tendency to disorder and dissipation.
So often articulated in terms of the second law of thermodynamics that there's a progressive
increase in the entropy of closed systems.
So why is it the case that things like you and me do not progressively become more dissipated?
Why don't we just dissolve into the environment?
Why don't we evaporate?
Why don't we decay?
And therefore one has to explain or at least describe things like you and me
in terms of resisting that entropic decay.
But the entropy in this instance is basically the entropy of our sense and physical states.
And the way that you get to that is by basically trying to minimize the surprise
that you would engender by sampling the world in a particular way.
Technically that surprise is known as self-information and the average of the
self-information or surprise is known as entropy.
So you can get from an explanation as to why the entropy of our sense to world
is upper bounded by expressing that as succumbing to or committing to the imperative to minimize
self-information or surprise.
And that surprise is bound or approximated by the free energy.
So at that level all of those things are slowness.
But there's an interesting if you like twist or nuance.
When in doing that we also have to estimate the uncertainty of our predictions that are
necessary to evaluate the surprise sometimes described in terms of prediction error.
And that is a really important aspect of the kind of belief updating that active inference covers.
And I mentioned that specifically because it ties into the schizophrenia issue
that if we are unable to estimate the uncertainty of our beliefs or the confidence
that we should be ascribing to any sensory information then we are very subject to making
joint false conclusions.
So we come back to the delicate game of getting the best explanations for our sensory input.
And that game is rendered very very delicate because of the difficulty in estimating the
degree of uncertainty we have.
So I think from the point of view of a clinical neuroscientist or indeed a psychiatrist
then that special aspect of uncertainty quantification representing our confidence
or uncertainty in our various beliefs whether they're sub-personal beliefs or propositional beliefs
is centre stage and understanding the ways in which we can succumb to false inference.
This anti-intropic process of consuming outside energy to maintain one's form
and avoid decay is a useful definition for what life itself is or is doing.
Yeah yeah and you know one could regard the free energy principle as really addressing that issue
usually framed though in terms of what properties must the system possess in order to exist in that
sort of entropy-defying in that entropy-defying sense.
So the physics part of the free energy principle really starts exactly from that observation
that there are certain things that tend to resist this tendency to increase in their entropy.
And then you ask what aspect of their dynamics can account for this apparent resistance to
being dispersed by random fluctuations or perturbations from the outside world.
And the answer ends up being simply that you've got to counter the dispersive aspects of all these
perturbations and fluctuations with a systematic gathering yourself together
around those states of being that you are characteristic of you that define you in some
unique sense. You speak in your book about two separate roads to active inference. The first
high road I believe is what you were just discussing where you begin with this free energy principle
and if the goal is to minimize entropy then any anti-intropic system will eventually arrive at
active inference of a process of making that happen and then the opposite the low road to that
would be assuming you have an agent who's modeling the world and making predictions and having
some degree of error there and aiming to minimize error that would be more of a bottom-up approach.
Is that right? Yes you've taken my favorite dual approach to it absolutely and I very
started with the high road I'm sorry for that. No you're absolutely right but I think that
it's useful you introduce that distinction. From the point of view of a physicist where
just talking about something which is unremarkable not has no anthropomorphic story it's just that
should certain systems have technically what is called a pullback attractor should they
sort of converge to a certain set of states that defines the kind of thing that they are
then they will be equipped with a description that can be unpacked a story if you like that
can be unpacked from the point of view of the low road which is what would these systems look like
well they would look like they were trying to minimize their surprise where they would look
like their internal dynamics whereas the neuronal dynamics was in the game of minimizing prediction
error and then you'll get the low road story which brings us back to this long edigary or legacy
or you know from Plato via Kant to Helmholtz and all intervening theorists people like Richard
Gregory perception hypothesis testing you know the idea again the brain is a fantastic organ
that it is actively trying to minimize its prediction error and then you get in the 21st
century with predicted processing being the way that people generally think about neuronal dynamics
as trying to minimize precision weighted prediction error. I slip in there precision weighted
because that was a nuance I was talking about before it's not just a question of minimizing
prediction error as a way of understanding this existential imperative to minimize surprise
but you have to also do it in proportion to the precision that you afford certain prediction
error so the precision is the inverse uncertainty so that's what I was saying I tried to imply before
that you know not only are we overall anti-entropic machines but in reading that
the dynamics that render us resistant to entropic forces and we can be read as or it looks as if
we are actually estimating the uncertainty about our own inferences our own predictions
that are I repeat necessary to elaborate a prediction error to quantify the surprise in
this moment and how we're going to use that surprise or prediction error to update
revise our beliefs about states of affairs out there and indeed what we're going to do next.
This is a great place to bring in the hierarchical processing element of active inference because it
sounds like when you're talking about precision there if we think about ordinary human consciousness
there's a whole bunch of things that are going on that you're not really aware of like we lack the
precision to detect you know the dust modes that are floating around in the air but perhaps if you
were a small fly or like a microbe or something as long as there was still some motivation or
some degree of predictive processing at all then suddenly those tiny details become a lot more
relevant so that's that's an example of precision yes. Yeah no absolutely yeah so
yeah there are two themes you've introduced both of which I'm tempted to pursue one which is
exactly the sort of the precision and the coarse graining that we necessarily bring to the to our
aspirations and to our fantasies and we all have the right degree of coarse graining and certainly
if I was a fly I would be operating a much smaller scale and wouldn't sort of coarse grained to the
same extent that that you and I would be much larger creatures so there's a really interesting
and very very important theoretical explanation why there is an optimal degree of coarse graining
and the other issue you brought the other sort of architectural issue about sense making and
coarse graining is a hierarchical aspect so you can actually think of successive levels of say
the cortical hierarchy in our brains as doing a special kind of coarse graining as you get deeper
and deeper into or higher and higher ascending these hierarchies that kind of coarse graining
is usually not just over sort of say space in the sense of receptive field and vision
but crucially also over time and that we're sort of lumping together epochs of time at
successively longer time scales in the way that we generate those fantasies so I don't know how much
you want me to unpack though it was direction where you'd like me to go the imperative to
coarse graining the right and the right kind of way or the importance of of hierarchical
processing here here might be an idea that connects them together so
are the systems of the the agent which is doing the processing themselves are those
something that are are flexible and could be redefined because most of the time when we're
talking about consciousness we're talking about the whole human but you could also from the perspective
of an individual neuron have its predictive processing in response to its immediate chemical
environment and that would be on a much smaller both time scale and physical space scale so that
that seems to be one of these trade-offs of precision where like the smaller the
agent in question is or the system in question is the more precise you get but then these larger
systems can be assembled from collections of smaller subsystems as in the case of the brain
and millions of neurons billions of neurons interacting yeah well that does certainly
link the two themes and brings a very important principle to the four which is
the scale invariant principles that could be applied as you say to a single neuron indeed
even probably at the molecular level so what is conserved over different scales be the
be they sort of the neuronal level at the mesoscopic scale of a neural population or a
macroscopic scale that you might apply to say intrinsic brain networks that have this hierarchical
kind of structure the principles are exactly the same but the scale at which they are operating
is clearly unique to the scale that you want to identify and I think the important insight here
is that all these principles to operate these predictive processing free energy minimizing
principles to be in play at any given scale they have to be afforded the right context by
exactly the same processes at the scale above so for example for my favorite hippocampus
cell to operate properly it has to be in a milieu and have the right connectivity with
the hippocampus as an organ or a system and for the hippocampus to exist it has to be living
in a brain and for the brain to exist it has to be living in a body and the body has to live
right out to the biosphere so at every scale you have this this nesting where the scale above
provides the right kind of context for the scale below to do its sense making its surprise
minimization its its self-evident thing and yet at the same time that lower level scale is now
coupled to and informing and creating the context in which these processes are going on
so that's a really important sort of generic observation about any kind of self-organization
and really starts to bite when you start to think about
say psychological or neurodevelopment in an evolutionary context or indeed an evo-devo
context you know that the same principles are operating at an evolutionary very very slow
time scale that are coupled in both directions to faster time scales say for example developmental
time scales either you know of the kind in very early neurodevelopment that we determined epigenetically
or during during the lifetime that themselves contextualize experience dependent learning
as it unfolds over hours and days and weeks as in procedural learning that itself contextualizes
short-term sensory learning that itself then contextualizes your inference in the moment
sense making perceptual synthesis and of course all of that chain of moving down the scales
can now be rehearsed by moving up and I need to do the right kind of inference in order to
have the right experiences to get the right kind of experience and both that's suited to the right
kind of learning which enables the right kind of structural learning that enables the right kind
of adaptive fitness of my phenotype that enables me or my my phenotype to be represented in the next
generation so your that that observation that scale free aspect and the intimate coupling between
different scales I think is something a fundamental observation about and well certainly biotic
self-organization possibly all kinds of self-organization that transcend those neuroscience
but certainly it's in the face when you start to think about the different kinds of neuroscience
you might want to do or pursue in your career if you began only with the low road version of
the story which seems to take as a first principle the fact that you have some system that's engaging
in prediction and error correction you might be faced with the criticism of how do you know this
isn't just an elegant heuristic model of what's going on as opposed to what's actually going on
but it sounds like as soon as you combine it with these high road properties of scale invariance
and free energy minimization then you have a stronger case that this is actually what's going on
yes I've never heard that express like that before I think that's absolutely right yeah yeah
you know there is a danger though but once you endorse or provide a complementary perspective
on the low road predictive processing story with a high church physics of self-organization in
non-equilibrium steady state and that you can almost trivialize the problem because
the physics the high road explanation is very deflationary in the sense that all it's saying
is that systems that have characteristic states behave as if they are attracted to those characteristics
states that's almost tautological and so you're in if you like providing a construct validity or
an explanation for the low road predictive processing story or predictive coding story
if you're talking about prediction precision wedged prediction errors and you're also treading in
in the dangerous waters of providing a tautological almost tautological account of what's going
of what's going on that could potentially explain everything in a very weak way but I do
I concur with your observation that the fact you've got this deflationary story it could be no other
way and it just looks as if we are carefully constructing in a purposeful way good generative
models good fun fantasies that explain our sense to world and it is actually very compelling you
know and it sort of begs the question you know is it just that this is our way of understanding
ourselves or is there a deeper question or a deeper explanation for why this has to be the
way that we operate and it could be no other way active inference to me seems to describe a
computational process of cognition but doesn't necessarily describe what it is that's doing
the computation and I'm wondering how this connects to integrated information theory
which is another theory of consciousness that essentially argues that all matter carries a
certain degree of information even at the basic physical level and then as it begins to interact
you get shared information across coupled systems and as those systems grow more and more complex
they contain more and more information and at a certain point you get a system which can
reference itself and that is how consciousness is defined and I think if you begin with that
definition of what a conscious system looks like and then apply active inference to it
the whole thing seems to fit together very nicely in my view but I I usually see these
as competing theories as opposed to complementaries I'd like to hear your thoughts on how they
fit together or don't. Right well I'm very happy to share my thoughts and I'm smiling because
I am currently involved in a temple of foundation adversarial research collaboration
to pitch IIT against active inference and predictive processing and so that presupposes
that one or the other is right and I'm smiling because I agree with you I think they're entirely
compatible but I can't say that in public because I have a certain research collaboration to tell
that one is a better explanation for empirical evidence than the other but I think you're absolutely
right I mean you know the free energy principle and IIT are very different things but in their
difference it means that there is I think a very clear way of them shaking each other's hands first
of all the free energy principle is not a theory of consciousness it's a theory of self-organization
that you know ultimately inherits from from the physics of things that persist and then be read
as a physics of sense-making or sentient behavior but sentient is just a description of the kind
of behavior that these systems have been it's a first principles account it's literally a
variational principle of these to action it can also be cast as a constraint of maximum entry
principle it's an example of the principle of unitarity in quantum information theory of quantum
physics and I could go on so it is a first principle account that is predicated on or
starts with the natural laws of physics so it sort of comes along with a naturalization
bait into it IIT is an axiomatic theory you know it is based upon five axioms I think it's five axioms
that make some claims about or assumptions about the nature of consciousness and sort of then works
backwards to see what kind of information theoretic treatment would accommodate or be consistent with
these axiomatic claims and so in virtue the fact that these two approaches start from very very
different ends one with nothing to say about consciousness the other one starting from an
axiomatic specification of what it is to be conscious or what which is a conscious system must
possess and I think it's perfectly possible for them to meet in the middle and you know all the
tenets of integrated information theory I would be extremely surprised would if you couldn't find
all of those predictions and tenets in a and particularly a hierarchical message passing
scheme where the message passing scheme was prescribed by an application of the free energy
principle IIT is sometimes criticized for being panpsychist in the sense that if it defines
consciousness as information in any matter can carry information then it says everything is
conscious to some degree even if your keyboard is only a little bit conscious and life is much
more conscious and active inference we talked about as part of this anti-entropic process
which is also the same process that life works by where again it's consuming excess energy
outside the system to maintain its own form does active inference say anything about
whether this type of predictive processing either requires the system to be alive or
maybe the other way around where if it's alive by definition it is engaging in predictive processing
I think okay that's a great question yeah I mean it would certainly say to be alive is to have
to be describable as being engaged in predictive processing and that you know can come be expressed
in many different flavors Jacob Howie and his colleagues and like to describe this as self
evidencing so mathematically surprise with self-information is a negative log of something
called model evidence the evidence for your fantastic model your internal model your
genetic model your world model so as you minimize the surprise or variation of free energy you are
also equivalently maximizing the evidence for your for your model sense your self-evidencing
so that means to exist is to self-evidence and that just basically means it will look as if
things that exist or certain things that exist will be look as if they are behaving to garner
evidence for their models of the of the lived world but as I said certain things because that comes
back to now a difference between the free energy principle and IIT and you you've drilled down
exactly on on that key difference which is this notion of psychism so from the free energy point
of view and certainly active inference point of view it would you know you would have to specify
what you mean by a certain kind of thing that can be conscious and I think the answer to that was
very much buried in the way that you phrased your question you were talking about maintenance and
self-reference and the like all of these things are active processes so at some point you have to
write into active inference what you mean by something possessing a a a sentence or a consciousness
and what normally transpires is that there has to be an active aspect of it you know an auto
an auto poetic aspect there has to be some notion of self-assembly there has to be some
certainly notion of self-reference but in in a way that actively you know to use your description
before about sort of doing energy sorry doing work expending energy to to minimize the entropy
in a thermodynamic sense that then tells you that you really have to commit to a particular kind of
have to commit to a specification of what you mean by sentient behavior and consciousness
in relation to action and that is not something that IIT has it's not about
things that act upon other things it's just about the statistical dependencies of systems
that would be necessarily possessed by something that is conscious but it doesn't tell you whether
it is conscious because it possesses this particular kind of statistical dependencies
active inference on the other hand gives you an opportunity to say well there are different
kinds of sense making where it's the different kinds of self-organization and some of them
may be equipped with the features that would underwrite sentient behavior and then you have
to ask them what are they and normally the story goes that you really your your your fantasies
your genetic models your internal models have to entail the consequences of your action
so now you become an agent so there are lots of things to which the free energy principle applies
not all of them are agents but there is a certain special kind of system defined technically in
terms of the coupling between various states that constitute the system and its environment
that could be ascribed the the quality of agency and that entails a degree of planning
that itself entails a degree of future modeling it entails a degree of selection selecting amongst
among counterfactual futures so for these kinds of systems then you would you would certainly
well there are people who would certainly say that you have to have this as a minimum
and necessary set of properties in order to qualify as potentially being conscious so notice
there can't be panpsychism under that argument whereas there can be under IIT so there is a
bright line I think when it gets to that level of argument but it does depend very sensitively on
what you would consider to be conscious if existence itself is self-referencing and at the
quantum level particles are fundamentally probabilistic which seems to be a form of
activeness and they can become entangled which seems to have some degree of future prediction
why doesn't that become panpsychist in the sense that any single particle could be an agent
right because just being entangled and just existing does not qualify you as an agent
so to put it very simply if you can plan if your internal dynamics represent the consequences
of your action upon the world then you can be called an agent if you can't then you can't be
called an agent so a particle can't plan the weather can't plan your thermostat can't plan
all of these things are certainly subject to or one can apply the free energy
principle to all of these kinds of things evolution can't plan you can apply the free
energy principle to evolution natural selection becomes Bayesian model selection
but none of these things at each of these scales plan so that's what I was trying to make before
about the importance of being an agent and the importance of having a special kind of
generative model that arises from a particular sparse coupling between the inside and the outside
and in particular the ability or the availability of our active states or the way that we act upon
our world to the internal sense making dynamics and paradoxically if you disconnect the active
states from they say the neuronal states though that now you are not in direct receipt of the
states of say your autonomic reflexes or your actuators your straight or muscle for example
but you can only sense the consequences of your motor acts or autonomic acts
as is the case of things like you and me then your actions now become hidden from you your sense
making in your fantasies and you now have to infer your own actions and that becomes one way of
articulating that is that action now is the consequence of planning as inference and so
unlike most systems where the active states that constitute a separation between the internal
states and the external states are directly influencing the interest say the interest
in the states and there are certain special kind of systems where their own actions now becomes a
hidden cause of their sensorium and it is I think the unique province of these kinds of systems that
would qualify for the kind of self maintenance and self reference that you're talking about
so I'm not using I wasn't reading self reference here as some say recurrent dynamic
I'm reading the literal sense that I have to refer to my own sensations to infer the consequences of
my own actions but I don't have direct access to them and as such then you become a special kind of
particle and potentially the kind of particle that could be conscious and you know implicit
in that construction is the kind of particle that could be equipped with the label agency
you can also argue because the consequences of one's own action when inferred and indeed when
planned are in the future then you have to have a fantasy or a generated model that has a certain
temple depth to it so you're now transcending the present moment so the way that we make sense of our
world unlike a thermostat is in terms of narratives that run deep into the future and I've it is that
sort of if you like special aspect which eludes panpsychism so these kinds of systems are pretty
rare and you may argue that they are definitive certainly of they are definitive of beyond just
biotic self organization so you know you could have an interesting argument about whether
plans plan there is actually some evidence that they they can actually plan and I've enjoyed writing
with colleagues in in plan biology but I certainly know that thermostats don't plan so a thermostat
is you know is a really nice example of a sort of you know a system that controls its own environment
it is anti-entropic in the sense that it keeps the temperature within bounds it does all the
right things you might might want of any machine that is so self-referential in that in a very
simplistic sense but it doesn't have the kind of self-referential aspect that I was speaking about
that is entailed by the capacity to plan and select from various plans and infer what what is the
most likely thing I would do given the sensory evidence at hand at the moment under the belief
that I am actually the agent and the cause of that sensory evidence and it is that aspect of
inference that I think draws a bright line between an agent and an artifact with autonomy
and I would argue that consciousness is sent in behavior really belongs to agents and not just
systems that have autonomy would you say then that the error that is measured when engaging in
active inference is not just what is going on in the actual world versus what is the system
predicting because that could seem to apply to a thermostat you can measure the degree of uncertainty
in the thermostat rating even though it has no vested stake in the outcome of that rating versus
if it's a true agent then it's not only error between actual world but predicted world
but predicted world and something like desired end goal yes yeah absolutely so and that can be
expressed very simply at a number of levels you could actually just write it down in terms of
prediction errors so it's not just the prediction error between my predictions and my sensory data
there's also the prediction error between my beliefs having seen the data and my prior beliefs
before seeing the data and those prior beliefs define the characteristic states that you would
expect yourself to be in so these would be for example sort of very sub-personal and possibly
epigenetic epigenetically specified innate priors underwrite the set points of our home
estasis so the fact that I have a particular characteristic bodily temperature is an expression
that my brain and to the extent it controls my body temperature as is privately that's the kind
of thermoreceptive information I would expect to receive and of course I can get hot and cold
and I sense the hot and coldness by minimizing the prediction error between my thermoreceptive
input and my sub-personal beliefs about the current temperature but in moving away from my
prior beliefs I also would use another kind of prediction error which is the disparity between
my posterior and my prior beliefs and that I'm going to try to keep as small as possible
so the prior beliefs you bring to the table as a phenotype of your fundamental role in essentially
making it look as if you have some sort of purpose simply because they a priori describe
the kind of states that characterize you as this kind of thing so mathematically it's called a
pullback attractor or an attracting set but you have to occupy a set of states you have to occupy
to be you and if you wander away too far then into very uncharacteristic surprising states
you cease to be what you are so I'd be a very very hot me probably a dead me if I had a you know
temperature of 112 and so priors I think are absolutely important they become even more
interesting when you think about how do I choose actions make decisions life choices
and these these can be at any different scale so at the very fine time scale of the cognitive
moment or perhaps the perceptual moment these would be where do I choose to look next you know I
have to make that decision at between 250 milliseconds as I systematically palpate my visual
scene to secure the right kind of visual input through to choosing do I do a phd or not you know
do I go for a postdoc or go into continue Michael Little says or whatever all of these
are instances of sub personal and possibly personal decisions that involve rolling out into
the future and asking in your fantasy in your genetic model what would happen if I did that
and then you score it in terms of effectively like prediction errors but these are prediction errors
about things that haven't yet happened so they're technically they're possibly a predictive
densities and they you know what what transpires is that if you just go back to the physics of
self-organizing systems and work out the most probable actions that these systems take they
actually comply exactly with the two aspects of Bayes optimality one aspect is what we were
just talking about which is effectively acting in a way to minimize the prediction error of the
divergence between what a priori I expect something like me to encounter and what would
happen if I did that and therefore I am more likely to choose those courses of action that
bring me towards what I a priori expect to hear we sometimes call these prior preferences
and these define the kinds of outcomes that I would expect to expect to encounter but just
out of interest there's another very important component to technically what's called an expected
free energy above and beyond that expected value where value is just the log of my prior preferences
and that's the expected information gain so once you start to apply these this mechanics to
things in the future that are consequent upon action you now have another kind of Bayes optimality
which is the that comes from the Bayesian principles of optimal experimental design
and what that just says is I act in a way that resolves the greatest uncertainty and identifies
the best fantasy that is most precisely specified but I've got the right kind of information
and it's that expected information gain that drives eye movements you know so we don't look here or
over there in order to get the ward or to satisfy some utility function we do it because looking
over there means that I'm going to resolve the most uncertainty about what caused those peripheral
visual sensations in other words looking over there making that choice has the greatest
expected information gain so that's a really important aspect of creatures or systems that
can plan or engage or look as if they're doing planning as inference that they are quintessentially
curious or they will look as if they're curious so that would be another thing if you like that
draws a bright line between an active inference formulation of sentient behavior and more
descriptive information theoretic descriptions of what it would look like if you could measure
something that was sentient and that you will have a description of sentient behavior that is
at its heart has to be or have been some kind of curiosity responding to epistemic affordances
and salient purely to look as if it is sating itself with the intrinsic
motivational value of finding out what would happen if I did that or what's behind that door
or what does this person think or what kind of person am I and I move this is this my arm
or is it yours all of these questions have to be actively resolved by sampling the right kind of
information the right kind of sensory evidence which means that you have to you have to choose
the right actions and decisions in order to answer those questions so I would say that was one
hallmark of conscious behavior very simply curiosity that's an integral part of planning
and agency right this prior motivation for maximizing information gain seems to get at a
fundamental paradox of free energy minimization because if you were purely after minimizing
entropy you would think that you would engage in the most narrow existence possible just
pure repetition where nothing new ever happens but at the same time error is relatively low
yep absolutely this is the dark room problem isn't it that's
so that the dark room paradox was was introduced exactly to to sort of
challenge the free energy principle formulation that you know if I really want to minimize my
surprise I just go to a dark room switch off the light slide and close my eyes
and I often think cheekily well yes I do that every night but aside from that aside from that
response I think you've you've exactly highlighted it it is because we are curious creatures the
first thing that we will do in fact we would have to do in order to survive in the long term
when entering a dark room is to switch on the light because this then maximizes the expected
information gain it resolves any ambiguity about my sort of low low illuminance visual
input and so just out of interest for your amusement the formulation in terms of expected
information gain and the full elaboration of the energy principle to accommodate this expected
free energy conditioned upon plans as we project into the future actually came as a response to
critiques based upon the dark room problem dark room problem so this is a nice example of some
adversarial theoretical debate actually pushing the free energy principle to a more formal and
principle stance so that it could exactly address that you know that that particular pushback
where does free will come into viewing our minds as just aiming to mathematically optimize for
lowest uncertainty and highest expected information gain I think most easy to answer
because the answer is again predicated on the notion of entertaining counterfactual plans
that roll out into the future so as soon as you have a set of plausible courses of action
amongst which and you have to infer the consequences of this plausible set of
behaviors you now are in the game of how to select one amongst a set of plausible
courses of action and I think just having the verb select as part of active infants
immediately touches upon the notions of free will there is always going to be in any
annuals influence something that can be read as an active basic model selection and it's you
doing that selection no one else and no one else could even see you doing that selection because
you're hidden behind your your sensory veil your markoff blanket and so only you know what you're
selecting from it's your plans and you're selecting from it so I think free will certainly
one minimal and rather simplistic reading of free will is an emergent property of this kind of
creature that can engage in you know in this kind of kind of planning you know and I love this notion
of basic model selection because it just speaks to sort of free selection in sort of you know
quantum physics for example free will in philosophy natural selection that the evolutionary
timescale I'm not saying that natural selection exerts free will but it certainly has the same
job of selecting the phenotypes with the lowest free energy or the highest adaptive fitness or
highest marginal item or model evidence and but because we're actually doing this in our heads
in the future I think that that would that has I think all the necessary properties that you would
be that you need I think to convince somebody that this thing does actually have free will
indeed we're getting more into philosophy than neuroscience here but do you think there's a
meaningful difference between we actually making those decisions versus something like the brain
is doing this automatically and the experience of these decisions is epiphenomenal that's a very
good question I think um well I wouldn't say it's probably not so important what I think but certainly
um in people or in discussions with people which are actually all going at the moment are literally
you know I'm involved and have just recently been involved in a rents workshop addressing
phenomenology and asking exactly these kinds of questions I've just been revising last night a
paper I'm trying to address this and look at the sort of the minimal assumptions you need to be
you need you need to bear down this but I I think that the answer to the question that's
emerging from these discussion is yes it is epiphenomenal in the sense that the kind of
actions that we are talking about that enable the right kind of sense making and planning
are not in themselves um experienced but we experience the consequences of those mental
actions as it were and and if that is the case then you could certainly say that in recognizing
the fact that I am attending to this or that I have just looked over there it is an epiphenomenal
but there's a twist to that that epiphenomenal actually will cause the next
change in attentional set and the next action so it's both the epiphenomenal is both cause
and consequence just in virtue of this hierarchical structure that we were talking about before
that is a necessary property of most charity models of most sophisticated life forms um
no one if you like um personal or sub personal belief and now we're talking about some things
that are clearly propositional um qualitatively that would be experienced um no one neuronal
representation or encoding or parameterization of these belief structures is epiphenomenal
in the sense that it's both cause and consequence of other belief structures in the brain so I think
yes you're absolutely right our sense of self for example our all valenced um senses of um
states of being of an emotional thought I'm in pain I'm tired I'm in love I'm embarrassed
over to various degrees all of these are fantasies we bring to the table to best explain
the myriad of interceptive ex receptive proprioceptive inputs that constitute the evidence for yes I'm
in this particular state of anxiety or fear or pain um so on that level those experiences are
epiphenomenal in that they are caused by the sensations but remember because you have to
control the precision of your prediction errors in order to select the right sensory evidence
and to attend to the right sense the evidence all of those epiphenomenal representations are
actually the cause of the way that you actively sample in a planned way those particular sensations
so um I think we gotta be we are getting very philosophical now but but also I think um you know
interim realms which are practically very important in terms of the neurobiology of
interception and selfhood um and the you know the pathologies that can attend um various um
you know abnormal inferences or false inferences or fatal inferences about selfhood and understanding
the mechanics of this kind of belief updating entailed by the neural dynamics I think is actually
quite important but in answer to your question yes they are epiphenomenal in relation to the
actual acts or attentional deployments that we that we prosecute um but we can always sense the
consequences of those it makes sense from an evolutionary perspective that we would evolve
to experience lower than average or lower than expected uncertainty as positive and higher than
average or higher than expected uncertainty as negative but at the neurobiological level
how is the uncertainty actually translated into effect?
Well I think the answer to that question is almost identical to um sort of the
general answer I gave you before I think notions like I am anxious is just a hypothesis you know
it's a hypothesis that best explains the fact that I got a mild tachycardia that I have a certain
sort of um proprioceptive um pattern of inputs that suggest I'm either in a fright or flight I've
got like say increased muscle tension um it's a hypothesis um that is fully endorsed by the fact
that I can't see those around this dark corner in this dark alley um all of this you know um
situated um sense making is best explained by the simple hypothesis I am anxious and of course
this circular causality I was referring to well that because I am anxious means that I will expect
to have a certain set of tension in my muscles I will expect to have a particular tachycardia
and expect to have a certain degree of um perspiration up or down and those predictions
are now fulfilled by active inference so I now cause my bodily states that are providing evidence
for the hypothesis that I am anxious and and it's interesting that you you um you sort of draw the
you play the evolution card I don't think you even need to play the evolution card and saying that
being anxious is have negative valence remember that the whole point of the free energy principle
the whole point of existence is to minimize self-information or expected free energy or
expected surprise that just is uncertainty so that is entropy entropy is the average or the
expected surprise or self-information expected prediction error so existing which we um assume
is the realization of our um full back attractor our attractive attractive set um is if you like the
like a temperature it is the measure of goodness and we've just realized that in terms of making sense
of our own speaking um so I think it's a really useful observation that nearly everything that
is negatively balanced in terms of these various hypotheses about states of mind that I could
entertain with a greater or lesser um uh coarse-graining you know I could be inlexithymic or I could
have a very fine grained understanding of my emotional states you know that are um nicely
nuanced so I'm not just left with I feel good and bad but um certainly access of good and bad is
always tied to a degree of uncertainty when they're talking about sort of you know
depressive disorders um or their maintenance you know or obsessive compulsive disorders um uh or
literally um you know anxiety disorders generalize anxiety disorders that they can all be um if you
like easily understood with the simple mnemonic if it increases uncertainty it's going to make you
feel bad or commit to the hypothesis I feel bad in some way I think in some kind of angst
that there has a negatively balanced aspect to it but you see this everywhere you see this in
the economic markets yeah once the bad thing it's not to do with being poor it's just uncertainty
it's just uncertainty in the market the markets have lost confidence that's the only thing that
makes a bad market um you see it in homeostasis you know what's bad from a homeostatic point of
view you know it's just not knowing um well from the perspective of our stasis whether my homeostasis
is going to work um so nearly everything we do um certainly as prescribed by the free energy principle
is just about making the world more predictable and more certain and less uncertain and any failures
to do that or any excursions from the um that trajectory will be experienced as your natural
experience as alarming or negatively balanced and these hypotheses needed to be conscious but they
could be referring to autonomic nervous system behavior oh yeah absolutely I mean I think sort of
you know being able to articulate uh propositionally or in conversation and to have the um
the phenomenal experience of having these emotions I I think they have to be personal
but the mechanism upon which they rest and the systems um that they control in terms of instantiating
various attentional sex or setting the synaptic gain in the right kind of way in your in your
anterior insular right down to the periaxial ductal gray and if I was Mark Soames I'd say that the
you know the um the heart of feeling is just in the executive of setting the neuromodulatory tone
at the level of the brainstem so he refers to this as felt uncertainty he thinks that all
consciousness um certainly has to go through has to have a right of passage through the brainstem
that is basically the the biophysical encoding of the of the uncertainty that they all precision
the inverse uncertainty which is the precision we were talking about before but it was then
broadcast to the rest rest rest rest of the brain but that would be sorry is that neurochemical
signaling analogous to modifying numerical weights in like a black box of an artificial neural network
yeah I know it's exactly the same so I mean as a from the point of view of neurobiology it's just
the particular example I would have had in mind then when referring to
felt uncertainty in um as talked about by Mark Soames where the sounds of origin
or the classical um um modulating neurotransmitter systems you know dopamine and
epinephrine serotonin and the like um more generally really include any synaptic mechanism
that changes excitation in the mission balance or personaptic excitability so
that would usually entail fast synchronous interactions of the kind measured with EEG
so there's lots of biophysics and neurobiology about the physiology of gain control that is the
place that one encodes the precision say a prediction errors on the point of view machine
learning exactly the same mechanisms can be found in certain kinds of machine learning not all of
them but I think the most compelling example would be the the transformer architectures
that are underneath the hood and chat gtp and gtp4 and they actually have something called
attention and this attention basically allows them to pick up things that are predictable on the basis
of stuff that's happened in the past in this instance in the stream of text for example so they are
they are using exactly the same sort of modulatory biosignal gating in this instance in a very simple
way in the context of an autoregression like model over time that is being instantiated by the top
down control of the felt uncertainty that would be the ascending neuromodulated transmitter systems
like dopamine that has different roles depending on where it projects so you know when you talk
about uncertainty generally or precision generally you're talking about attributes of belief structures
basing belief structures that are you know generally sub-personal and therefore you have to
also say precision or what so you know the precision that say it means a cortical as
opposed to it means an olympic dokonergic projection would control would be the precision of something
that's very different depending upon what those beliefs were about. In my own area of adolescent
brain development you see a reorganizational period of brain development around puberty and
especially the dopamine activity you're mentioning now there's some evidence that sex hormones
are involved in restructuring or modulating how dopamine is processed and that leads to
a whole bunch of changes in reward sensitivity and inhibitory control and at a high level in
impulsivity and risk taking I'm wondering how you would interpret all that from a perspective
of active inference because it seems like what's happening when you have a critical period of
brain development whether that's in early childhood or this second critical period of puberty
what's happening is that the meta parameters of this active inference process are being changed
such that plasticity goes up so you could be taking in the same amount of information but
during certain critical windows you're more susceptible to changing your worldview more
the internal model is influenced more by the same amount of information compared to later
in adulthood when the brain is more static. Yeah now I think that's absolutely right
and how would it be explained under active inference in a relatively straightforward and
possibly even easy to simulate way so it sort of comes back to what we're talking about before
about the coupling between different scales of the same kind of process so we're talking about here
about a particular developmental phase and it's a developmental phase that is associated with
all sorts of requirements for new learning either in terms of discovering your adulthood
or preparing to leave home to go to university or to your first job or
from an anatomical perspective a time of great pruning say in the frontal radiations
so it's restructuring of the brain so you ask yourself then well how do I understand
the coupling between the different levels of optimization of my fantastic model my
generative model well we said earlier that sort of you've got to have the right kind of
learning in terms of experience dependent plasticity to enable you to do the structural
learning to prune or remove redundant connections say in the elimination of redundant
white matter connections in the frontal lobe this characteristic of late adolescence for example
you're making the brain more efficient and of course you have to have the right pruning in
order to retain the correct the right synapses to do the experience dependent learning so what
does that mean well it just basically means that if you are going to engage in some structural
learning and remodeling then you need to emphasize the the the epistemic aspects of your
and behavior so you become more information seeking so remember for the the plans that we
commit to including adolescence can be described as if they are in the same instance maximizing
expected information gain and maximizing expected say value where value is the log of your preferred
states of being but those do depend upon the precision of your beliefs about your preferred
outcomes so in the absence of any preferred preferences if you feel very adventurous you
don't care what happens to you which is a certain characterization certain children or
certain points in life and then what you are just left with is the is the is the epistemic gain
so what you predict would be that if there's a scheduled series of developmental stages of
that are all driven by the selection of the good models that have the right degree of course
spreading which usually entails minimizing complexity by removing degrees of freedom
so good models usually as they get older get simpler and smaller and certainly you know there's
an argument to suggest that normal aging you see that decline in say synaptic density and certainly
in in white matter connections but that's particularly accelerated you know during during
adolescence you would argue that you know what you you do indeed want to engage more in sensation
seeking novelty seeking exploratory behavior which is exactly what you would get if you had a
if you increased the expected information gain contribution to what underwrites your
plans and you know provides an overall tone to the kind of behaviors that I that I engage with
and interestingly you know it's interesting you ask that question because if you look at the maths
of the things that control the relative contribution of my epistemic habits relative to those based
upon the expected information gain the quantity that seems to provide the balance looks as or
behaves in exactly the same way that dopamine behaves so it may well be that dopamine has a
very special role say possibly you know the certain projections of the dopamine system
may have exactly this role in setting the the temperature of or the gain of certain
imperatives for plans relative to your habitual plans of the way that I normally do things
and looked at like that it does it you know it does not surprise me to hear that dopamine plays a
particularly important role and anything that acts as a neuromodulator or affects your neopastasty
including things like you know steroids and sex hormones and like plays will play a very
important role in optimizing the degree of epistemic behavior explorative behavior relative to
explosive behavior of the kind that would be driven by your by your prior preferences so
you could you know in principle simulate this and it would be entirely based on it
it does it does require that to specify the you know the scheduling the if you like the curriculum
learning the epigenetic epigenetic programs of neurodevelopment but if you specify those as
context-wise with certain levels of levels at certain degrees of learning and learning to learn
then I'm sure you could simulate that you know you know in a biologically very plausible way
so there's meta-optimization in the sense of not only learning to learn now and not only learning
to learn across time but learning to learn itself and modifying the weights at which you take in new
information and update your beliefs absolutely yeah yeah and just to sort of really join all those
dots you know the precision is almost universally a learning rate so if you if you just think about
the basic mechanics of belief updating whether you're thinking about a predictive coding scheme
accountability if you're an engineer if you do behave or psychology and risk or a Wagner model
you're changing your beliefs whether these are beliefs about sort of connections or beliefs
in perceptual inference you're changing your expectations about the world in proportion to
a prediction error completely reward prediction error in say the scholar Wagner models and
and the rate at which it changes is the precision you afford that prediction error so it's usually
some alpha in a scholar Wagner model that is the precision you're affording the prediction error
so that's why I was emphasizing precision weighted prediction errors before but when you look at the
form of the equation the rate of change in my expectation is equal to precision times prediction
error that is to increase the precision you increase the learning rate literally so I think
it's a really nice sort of juxtaposition the way you ask the question that there is you know an
optimal way to learn and that's just optimizing your rate learning rates that just is getting the
precision right which is just another facet of quantifying your uncertainty so when I'm when I've
got very precise information or there's lots of epistemically rich information that I'm actively
harvesting because I'm going to disclose and do undie jumping and exploring relationships as an
adolescent and then this is very precise information I actively garnered and I'm going to I'm going to
afford it then a greater learning rate simply by precision weighting all of the ensuing prediction
errors that are driving my expensive end of plasticity not only will I be spending another
several years thinking about this but doing so while my brain is in the last few years
of maturation we've covered a lot of material today Dr. Friston thank you very much for your time
my pleasure I've greatly enjoyed talking to you thank you
