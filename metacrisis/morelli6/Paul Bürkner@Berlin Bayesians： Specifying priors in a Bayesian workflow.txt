in a Bayesian workload, but we're going to talk about in a second. Okay, so let me switch to full
screen. So yes, questions in between are allowed. Very welcome. You can like write them in the chat
and then I can, and from time to time, I'm going to ask if there are questions and you can also
speak up and towards the end of the talk, at the end of the talk, there will be more time for
questions. So I want to make this in some way a little bit interactive as far as we can achieve
given the online format. Okay, so I assume nobody complained that I'm not going to,
that I'm not hearable, so I just, I'm just going to start. So first things first, I don't have many
good answers yet. That is like, this this talk comes perhaps, I don't know, two or three years
too early for me to give you good answers for how to specify priors, like for Bayesian models or
specifying price and Bayesian work from what generally, because the problem is, as you probably
all are aware in your own research, excuse me, it's pretty hard, right? So that's why you're all here
and be rest assured that I also don't have good answers yet, but I have some, I have some observations
on prior specifications, some of that will be known to a lot of people here, some of them will
may not be known, or at least interesting, and then I'm going to talk about some directions that I
think are worth exploring in the future in your research, in my research, so that we eventually
become, as a field, better at specifying prior institutions. Okay, so this is a Bayesian meetup,
so I expect everybody to be familiar with Bayesian inference per se, just to a quick summary of
Bayesian inference, so when we're doing Bayesian inference, we're interested in the posterior
distribution, that is the distribution of the model parameters theta, the posterior distribution,
which is simply proportional to the product of likelihood and prior, the likelihood is the
distribution of the data y, given the parameters theta, and the prior is p of theta, so that's
the distribution of the parameters before seeing the data, and this factorization
in likelihood and prior is nothing else than one specification of the joint distribution of all
quantities that we are modeling as in principle uncertain, that is, it's a factorization
of the joint distribution of y and theta, and that should tell us, first of all, one thing
that the prior cannot be understood without using the model as a context, right, so without
knowing what our likelihood model is, the prior is a little bit meaningless, or it's like totally
meaningless in a way, right, so we need, even if we have the same kind of prior information for an
experiment, if we change our likelihood or other parts of the model that are not the prior, we also
need, or potentially need to adjust the prior, even just to convey the same kind of prior information,
so the prior can in generally only understood in the context of the likelihood, I'm just trying to
get rid of the bar at the top, but okay, it's fine, so let's consider the following simple
example, suppose we have a logistic regression case where we have a predictor x, and we have a
binary response variable y, either zero or one, success or no success, or whatever our
binary variable implies, and then perhaps one canonical model would be logistic regression
here, so that's the green curve together with some uncertainty interval in this simulated example,
and the blue curve is simple fitting linear regression to that case, which I mean, I think we
could like have endless Twitter discussions on whether a linear probability model is sensible
at all, so if we can run linear regression for binary data, but the point is both of them are
trying to give us the relation between x and y, so both of them will have a slope parameter,
a regression slope parameter demonstrating the relationship or the effect of x on y,
but the scale on which those, the slope parameter is, is very, very different, right, so for the
linear regression it's, there's no nonlinear transformation, so we can directly interpret
this parameter as if we're changing x by one, then there's kind of a change in probability
of success in y, so it's directly linear interpretation, while for logistic regression
we have this kind of inverse logit transformation that makes, that makes the slope parameter
interpretation very different, and as a result the same kind of prior information that we may be
having between the relationship of x and y will be, will need to be conveyed in, in a prior
distribution in a very different form depending on what of these two models we are considering,
even the same information needs to be written down in a prior in a very different way,
which should tell us the simple example that the prior can only be understood and can only be
specified in the context of the specific model it is part of, which is why, like which is one of
the reasons that prior distribution, prior specification is so hard because there are so
many other things we need to be taken into account to specify our prior reasonably.
Okay, so what are your reasons for using priors? I'm not going to make a survey or something,
but I just give you, I'd say 20 seconds or something to think of what are your reasons
for using priors or why would you like to use priors in your research? I'm not going to ask
you so you can just think by yourself and I'm going to present you my list or my subset of a list
in a second.
So I assume some reasons came to mind, so I'm going to present you a list of
some kind of reasons that some of which are mine, some of which are like more generally
of the field and we're going to talk about some of them and we can, if there's more interest,
we can also talk about them later and if you have questions already know about those,
then we're going to talk about that today or right now. So one reason for using priors that
is quite popular these days is that we make a priori implausible values unlikely, a priori
using priors and which is something we these days, for example, call weekly informative priors.
So consider a model where we're estimating the height of people and then we probably would know
that a person is not 10 meters tall, right? We may not know exactly how tall they are but we
definitely know they are not 10 meters tall or they're also not 100 meters tall, which is an
information we could specify within a priori and thus put into our model without actually
having much specific knowledge about the experiment, without having any specific knowledge about
about our study, but by simply understanding the scales of the data and of the related
parameters that we are working with. So if we have height of people in meters,
then it's not going to be 10 meters. This is something we can use to our advantage
to a priori rule out values that won't be happening in the data anyway, right? So for example, if we
are talking about, if we're talking about default priors and software weekly informative priors
could be one of such a good default priors, but there are some pitfalls that if you're interested
we could talk about later on. Then if we move to what is usually conveyed as the main meaning of
prior distribution and introduction courses, including my own, in a lot of cases is that we
incorporate specific expert information into the model. That is we have we have actual concrete
information about what specifically happens from an expert in the field and we want to put this
information into the model and we do so via the prior distribution. And this reason for
specifying priors differs from the first one in the sense that this expert information may
a priori at least probabilistically rule out values that would be plausible, but the expert
information is more specific and saying there's only like a specific range of the principal
plausible values that could be relevant, at least like with a certain kind of probability.
And so for for a long time the subjective priors was like highly debated and I think that the term
subjective didn't help in that because it conveyed that when we're doing research and we're adding
this expert information into our model, we're doing something that is subjective which kind of
contradicts the idea of research of science being objective, right? So I don't like the word
subjective priors here so I'm putting in quotes, but what we mean is that we have reasonable detailed
expert knowledge outside of our data and this is something we want to incorporate and argue this
if we have such information is something we should be incorporating at least in certain cases,
of course we need to defend it but we should be incorporating that to improve our general
inference we're going to have. And of course one specific expert information could be information
from previous research so if we want to integrate our information from our experiment directly
with previous research we could try to incorporate the global expert information in the field
in the form of a prior to update that global expert information by our prior by our data
but we need to keep in mind that the resulting posterior distribution then not only is based
on our data but on the data and all the previous knowledge we had in the field at least that knowledge
that we have try to translate to a prior distribution. So that's one one direction we could go
but also something that for a long time was was quite common and is nowadays still like very
relevant in practice is that we try to mimic frequentist methods this is something that that
Andrew Gellman on his blog post recently called the Bayesian cringe that is we're having we're
fitting Bayesian models we like that in principle but then for some reason we're just trying to
to specify our Bayesian model that is this case our priors in a way that the same same results
occur as in frequent as in the quote-unquote equivalent frequentist methods which is which is
a little bit weird and to me at least and in this in this context we are sometimes talking about
uninformative price or objective priors although of course like objective is in the same way as
subjective I think a misnomer in this case and what is more it's unclear what actually
uninformative means and I'm going to illustrate that in a second so for those of you who are like
more familiar with specifics of such priors they are so-called reference priors which I think are the
are the closest we we got to some kind of uninformative priors but like for non-trivial
models these are mathematically really complex so I don't know if anybody derives
derives them and uses them for those purpose but they're mathematically nice I haven't seen
them used in practice often and what we have seen in terms of uninformative price quite a lot
in practice or uninformative quote-unquote are uniform priors and I'm going to illustrate that
they are not really uninformative in a second something that a lot of you even if you're using
fragmentist methods do in terms of price specification is that you're using priors to
represent known data structures which is something here I'm calling multi-level priors so for example
if I'm having a repeated measures design so I'm measuring multiple observations per person
in my experiment and those different observations are going to be dependent on each other
and I want to model that with a parameter with one parameter per person say
but I'm also I also have some idea that all the people are coming from some kind of distribution
and a lot of lot of properties in nature are in some way normally distributed so we could specify
multi-level normal distribution over our parameters one per person right so that would
would lead to multi-level models but in essence those hierarchical multi-level priors are nothing
else like multi-level hierarchical distributions are nothing else than prior distributions from a
patient setting conveying a certain kind of information in a classical multi-level model is
exchangeability but of course we could go beyond that so for example if we have spatial units we
have a priori justified idea that two spatial units being adjacent to each other are somehow
related correlated so we could have a multi-level prior for example in conditional auto regressive
prior that tells us that adjacent adjacent parameters or parameters of adjacent spatial
units are correlated with each other so we can have this known data structure of spatial
relation of spatial positioning conveyed in the form of a prior distribution same of course with
temporal data so we would expect some temporal outer correlation in our data if we have
longitudinal data and we could incorporate that in the form of a multi-level prior by for example
saying that each that each parameters prior of time point t is normally distributed say with mean
parameter t minus one so that just directly form a parameter and some some standard deviation also
to be estimated so we can use exchangeability structure spatial structure temporal structure
and I'm sure also other structure that we know is in our data by design of our studies we can use
this information to specify priors to improve in general our inference which is the essence of
multi-level models whether we specify them in a Bayesian what a frequentist setting
there are more reasons for example we could regularize the model to avoid overfitting which
specifically becomes relevant when we have way more parameters than we have observations
you know for example this happens in a lot of cases in genetic association studies where we have
we have we have genetic information on of let's say 100 people but we have 1000 say
potential gene regions that could be related to some kind of behavior of those humans so we have
10 000 say parameters but only 100 observations and without really strong regularization
we don't have any chance and the Bayesian setting of course the way we regularize is
we use priors and in those cases we call would call them shrinkage priors that like really shrink
a lot of parameters close to zero or we could call them sparsifying priors although sparsifying
priors is something that for a lot of people conveys that we are that we are shrinking parameters
exactly to zero if we are in a regression setting or whatever whatever value means that there is no
effect of a certain variable but a lot of cases the the shrinkage priors we're having in practice
these days i'm going to talk about the later on they are not shrinking exactly to zero but something
close to zero so prefer the term shrinkage rather than sparsifying priors there are there are other
purposes for example to enable hypothesis testing via base factors i'm not going to talk about these
in detail but if we're not interested in posterior inference but in um Bayesian model comparison
via base factors opposed to your model probabilities we need priors and other rules may apply then if
we interest in posterior inference um there's specific reasons for example we could also facilitate
conversions sometimes we don't want to convey any prior information we just want our MCMC algorithm
to work and then we specify price accordingly um just to get that to work the canonical example of
that um our conjugate priors that enable Gibbs sampling so conjugate in the conjugate priors
setting our uh the the prior would be of the same family of distribution as the posterior or in this
case for Gibbs sampling the marginal price and the marginal posteriors and and so that would enable
much quicker convergence if we have certain kind of specific priors that facilitate a sampling of
our specific for example MCMC algorithm i'm sure there are more reasons um so far are there any
questions about my my list okay so there are a lot of reasons to be using uh why we should be
using priors and all of them are answered in in some different way there's some overlap but there
are a lot of different prior distributions out out there yet it's still hard to specify priors
so let's let's um make some observations about priors in general that i have made some of them
known to most of you some of which may be new some of which have been new to me um until only
recently so one thing that is um i think generally acknowledged yet often forgotten is
that uniformity is informative right so when we when we would like to have some some
uninformative priors people tend to like uniform priors but then they forget that a uniform prior
becomes non-uniform under any kind of non-linear transformation so consider the case where i'm
in some kind of unconstrained space let's say on the on a logit scale and i'm specifying uniform
prior between minus 10 and 10 so there you see the histogram because i sampled from it but like
in expectation there would be uniform but when i'm looking at what this what this prior um of
so my quantity x um on the logits scale implies on my original probability scale that is after
after applying inverse logit um i see that my uniform prior has become a lot of things but
certainly not uniform in the sense that suddenly most of my probability most of my yeah most of
my priors even at close to zero quite close to one on my inverse inverse logit that is on the
original binary scale that is what i perceived as uninformative is actually in some way quite
informative so that doesn't mean you need to worry that your uniform priors are suddenly
bias your your results unless you like specify them like really badly for example like have
proper uniform priors with ill specified boundaries for example if my true parameter on the
logit scale would be bigger than 10 then of course cutting cutting the prior of a 10 would be
problematic so that doesn't it doesn't mean that you need to be worried that your uniform
priors are going to be super duper bad unless you misspecify the boundaries if you have boundaries
on the prior but it means that the notion of uniform priors being uninformative is in some way
ill informed and a little bit misleading so we need to be careful in communicating to other people
that uniform priors were actually uninformative if we're trying to make that
another point i want to make is that prior tails matter so in lots of cases we can reason
about the bulk of the distribution of the prior distribution let's say the mean
and we can reason about some kind of measure of uncertainty let's say standard deviation right
but the very tails of the distribution are often like barely barely understandable for humans
and they're barely visualizable um so they may be looking a bit different those tails but we
don't really know what that implies and so i want you to consider the following example so what is
the posterior if the prior on my parameter theta is a student t distribution with 40 degrees of
freedom um location parameter five and scale parameter one and if the likelihood alone that is
without specifying any prior on theta would imply a posterior um a normal posterior with mean
minus five and standard deviation one so note that if my student t prior would not have four
degrees of freedom but infinite degrees of freedom or 50 or 30 or whatever would be enough
then the prior the the prior would be the same as in this case in hypothetical case the likelihood
implied posterior right so the only difference between that this this prior and the likelihood
in prior posterior here is that the prior is slightly a slightly fatter tails because we we
we have a student t prior with 40 degrees of freedom as compared to a normal prior so i give you
again a second a few seconds to think about what would you think how would the the posterior look
like if we combined that prior with the likelihood that alone had implied that this kind of normal
posterior so what i would think uh or what i what i thought before actually seeing the results
is that it's somehow in the middle right because one has mean five the other one has mean minus
five they're roughly of the same scale so i would expect the the mean of that the posterior
distribution is roughly at zero because the two the two distribution kind of cancel each other out
in the tails don't really matter unfortunately that's not quite the case so uh on the the here you see
the results so the the pink one is the likelihood implied posterior if we didn't have any prior if
we have improper flat prior the yellow is the the prior we have been seeing and the the green
one is the posterior so we see although both of them the the likely implied posterior and the prior
look super similar except that the prior has slightly fatter tails the posterior is almost
completely equal to what the data would suggest and the the prior seems to have no or almost no
effect even though they look almost the same just because the prior at the prior tail was a little
bit fatter yeah of course the same thing happens the other way around because of symmetry in this
example is if my prior would be normal and my likelihood implied posterior would be a student
with 40 degrees of freedom then of course my posterior would almost be identical to the
prior rather than what the likelihood would imply and this has like relevant consequences
in practice because if we have very few observations let's say five observations in like
six observations simple linear regression setting i'm not saying you should randomly
linear regression based on six observation but you should suppose for some reason you
would be doing something or something equivalent to that then we know that our likelihood implied
posterior is a student t-distribution with some like small degrees of freedom and if i'm then
specifying a prior distribution the data won't have any effect just because my my prior has so much
smaller tails or the likelihood implied posterior so much fatter tails than the prior you know
of course we can now ask the question what happens if likelihood implied posterior and prior
are the same thing so what happens if both of them are normal just one with me minus five the other
one with me five yeah okay that's easy we know if we combine two to normal distribution the result
becomes again a normal distribution that's one reason why the normal distribution is so nice
we reduce our variance in general we see that the the variance of the posterior has been shrinking
as compared to prior and we are ending up exactly in the middle because of symmetry
and we can of course now ask the same question but having the likelihood and the prior both the
student student t-distributions with four degrees of freedom right and of course by symmetry we know
the distribution will be symmetric so let's see how that looks like oh it's completely different
right it's symmetric but it's bimodal so the normal combining two normal distributions leads to
normal distribution in the middle combining two student t-distributions with few degrees of freedom
leads to a bimodal distribution without barely any shrinkage right so what i'm trying to say here is
tail prior tails matter and the tail of the prior as compared to the tail of the likely
implied posterior may have a huge effect on the result even though you and i cannot at least i
let's say it's even though i and most of you probably cannot reasonably reason about the tails
of my prior distribution because they're very hard to understand and that they're very hard to have
subject matter knowledge on any questions so far
okay so um another point i want to make about prior specification is that priors on high
dimension in high dimensional models on high dimensional spaces are weird with high dimension
i mean here if we have a lot of parameters so if we have a model with a lot of parameters
and in most like most practical cases we do have that these days and at least in a lot of fields
then even though we try to specify weekly informative priors the resulting joint prior
becomes really strange and i'm going to illustrate in a second so suppose i have a linear regression
model with y as our outcome variable it's as a like a normal likelihood with mean mu and constant
standard deviation sigma and my mean mu is simple linear regression model of k terms with my regression
coefficients bk and my predictor variables xk suppose that all my xk are standardized have mean
zero and standard deviation one for the sake of the argument here under which case i would say that
normal distribution with mean zero and standard deviation one on my regression coefficients
would be considered weekly informative right so a little bit wider than we would expect but you
know not super wide because we wouldn't expect a relation of let's say one one thousand or something
if y was also standardized which in this case i assume and if y is also standardized then for
example we could specify an exponential prior with with rate one on our residual standard deviation
sigma and that would probably be considered rather weekly informative we are still a little
bit wider than we would expect but like we are not unreasonably wide so the question we can now
be asking is what happens to the a priori percentage of explained variance r square that is how much
variance does the model think it will explain a priori as we increase the number of predictors k
and add more and more and more predicted terms so again i'll give you like 10 seconds or something
while i drink something to think what would you expect how the the explained variance looks
like under this model as we are changing k so what happens is that the prior the the
the model in the prior implied prior on r square becomes weirder and weirder and weirder as we
increase the number of predictors as we increase k so if we consider the setting above and always
have this normal zero one price on the regression coefficients and then have more and more of these
regression coefficients we see that the prior on r square becomes more and more peaked at one
time for example for 15 predictor terms under the setting i explained here most of the mass
of the of the prior on r squared is already above 90 percent of explained variance that
is a priori the model thinks it will explain all of the variance there is no response variable
which in most settings you you may have come across this is not the case so for example in
psychology where some experience with you rarely have more than perhaps 10 or 20 or 30 or so
explained varying sometimes we have more in a lot of cases we have less sometimes we are happy
with five percent variance explained and more there is not to explain because there's so much
noise but definitely there is not something like 99 percent of explained variance okay
so what that tells us is when we are having weekly informative price on each parameter
but each of these priors are independent of all the other parameters that is if we are
specifying marginal price on each parameter then the implied joint distribution for example now
here quantified in terms of the prior on r squared becomes really really weird or put to put it in
other words it's it's for high-dimensional models it's impossible to prevent a priori overfitting
with independent priors because the joint prior implied by the the stack of independent priors
becomes increasingly increasingly absurd and we could for example demonstrate that here on
r squared and this leads to some of the future research directions and I think we should tackle
their community. Another different topic where we desperately need prior distributions whether
we want it or not is simulation based calibration so when we want to understand whether my posterior
approximator for example my MCMC sampler works correctly what I can do is I can combine I can
compare the data average posterior to the prior data average means I am sampling from the prior
I'm sampling from the likelihood I'm fitting my model that is I'm computing the posterior
distribution based on the simulated data I'm doing this repeatedly for simulated priors
simulated data sets and I'm integrating that I'm integrating over those repeated runs and then the
resulting distribution that comes out of this data average posterior should look like the original
prior and if my posterior approximator is perfect that is that is if my posterior inference is
perfect then this equality will hold full stop if you want to read more about that there's paper
about toss and all validating Bayesian inference algorithms with simulation based calibration
and of course since we need to sample from the prior in this case in order to compare prior to
data average posterior otherwise we would never have a data average posterior
we need to think about the priors and it turns out that in a lot of cases our weekly informative
or like default priors or whatever our talk price we have been choosing choosing out of convenience
will be so bad that we can't that we can't reach reasonable simulation based calibration but not
because the algorithm or the software is necessarily bad but because of our prior information was so
absurd that we ran into edge cases that would never happen in real data but happens for the
simulated data and completely break your algorithm without actually being of concern for real data
because the real data leads to much nice more nicely behaved posterior distributions so I'm
going to illustrate that using yet another regression example but this case we're using
gamma regression it is we no longer have a normal likelihood but we have a gamma likelihood
with mean mu and here the additional parameters the shape that I'm calling alpha which kind of
I mean it controls the shape of that distribution around the mean the exact meaning of the shape
doesn't matter here exactly and the gamma distribution is a distribution for positive
only data that is if we just have a linear predictor it won't work we need a link function or
the inverse of a link function we need a response function this case we choose a lock link or
exponential response function so we're taking our linear predictor this case with an intercept
and passing through an exponential transform and using this is our predicted
mean of my gamma likelihood then I can specify some priors for example I could specify normal
mean to standard deviation five prior on my intercept I could have again standard normal
priors on the rest of the regression coefficients and I could specify a gamma prior with shape 0.01
and rate 0.014 alpha the reason I'm using this parameter on I'm using here on the slides this
parameter that is prior on alpha is that this is the the default prior for gamma regression in
b rms please don't ask me why I came up with this default prior for the for the shape I
no think it's a very bad idea I can change it but I have no good idea how I should change it but
let's stick to that prior for the purpose of illustration and then we can ask is our when
we we now run simulation based calibration with stan using hmc using one of the best software with
one of the best algorithms in the world to perform basian inference right so really state of the art
let's see if this is well calibrated for this relatively simple generalized linear model in
this case a gamma regression yeah so what happens um and to to understand um to understand first
what kind of data we are producing we could look at one instance at one instant of the data set
we are going to producing with our priors and then like sample from the priors sample from the
likelihood so this is one example of a data set so we see that a lot of our responses are almost
zero or or de facto zero which is hard to convey using this histogram while some observations are
above 200 000 and this is a data set I've been selecting that is well behaved like if I repeatedly
ran that a lot of those data sets completely break down and I couldn't even plot them properly so
this is a well behaved data set that come comes out of our prior simulations that we were using
for simulation test calibration and so now we have to ask stan to fit the scammer regression
model based on this and even we are the data sets so let's see what happens uh and the um
the the gist of it is it completely breaks down so if my model would be well calibrated
my black curve would be in the blue envelope so under well calibration um we would expect our
sample ecdf in this case the the details of the plot don't matter to be within this blue envelope
I'm not going to talk about the detail of this plot here but the point is what we are seeing here
the black line is so drastically outside of that that blue curve that my inference has
completely broken down this is true for the both regression coefficients b1 and b2 just taken as
example I think in this case I used 16 or 15 predictors it's true for my intercept and it's
true for my shape around that's not to say that gamma regression wouldn't work well in for real
data sets on the contrast then would do really well in general but it says that for this example
for those extreme priors given a complicated no potentially numerical instable likelihood such
as the gamma distribution and a lock link or equivalently an exponential transform things
will be super duper weird so a lot of responses are exactly or very close to zero a lot of responses
are very extreme some data sets are even weirder where all of the observations are zero and this is
something that stan or any other algorithm couldn't work well in a lot of cases and would
completely fail not because the algorithm or the software is bad but because our prior simulations
were so ridiculously bad that any inference based on these prior simulations was doomed to fail
so we need to we need to find reasonable priors in order for us to have a chance to get simulation
based calibration to work reliably especially in high dimensional modes
I'm going to to quickly only talk about margin likelihoods and base factors because that's of
kind of tangential interest here but also since for for base factors we're going to average
over the prior we are analyzing prior predictive performance the prior always matches so even if
we had if we had two posterior if we have two priors and they implied almost the same posterior
the posterior distribution would be same or posterior inference would be same would be the
same but since the base factor investigates prior predictive performance not posterior
predictive performance two priors may lead to completely different margin likelihoods that
is completely different base factors in a certain setting even if they're the implied
posteriors would be almost the same that is base factor related analysis requires always
very carefully specified priors and priors will always match regardless of whether the priors
influence the posterior or not it's in the interest of time I'm going a bit quicker
so you may ask what now okay so what are we going to do now with all of this information
and I don't have a lot of answers I have some directions yeah so the first direction
that I think is really promising are joint priors or specifically hierarchical joint priors so what
do I mean by that in a way it's very similar to what we do with multi-level models already but
generalizing that to all sorts of model always having those kind of hierarchical priors and with
hierarchical priors I mean that we have a vector of original model parameters theta let's say k
theta of these parameters and we're setting we're setting a prior on each of those theta case
and this prior depends on hyperparameters lambda in this case l hyperparameters
and those hyperparameters are now not fixed right in the other example we previously had
those hyperparams are fixed but they are also parameters estimated from model
that is they themselves have priors and those priors then depend on some very few hyper hyper
parameters that are then fixed so some some hyper hyper parameters tau for example m of them
and they are low dimensional and user-choosable and if we do it in just the right way we are
doing two things so first of all we're reducing the complexity of prior specification for the
user because if they already get the hierarchical distribution set appropriately for the model of
interest then they just have to use just have to specify those m hyper hyper parameters tau
and through the hierarchical prior on lambda and then on theta this this information gets
pushed forward to my actual model parameters but I just have to specify a few of them and then if
I'm specifying the priors uh on on lambda and specifically the price on theta correctly for
example as in multi-level models I'm getting those shrinkage effects where I'm preventing
overfitting even if I have a lot of parameters which in essence is the secret of multi-level
models that I have the hierarchical prior that that helps to prevent overfitting even in very
high dimensional models and one suggestion would be to roll out hierarchical joint priors in much
much more general cases than not only in multi-level models two examples of these for for linear
regression models are by Pirouin et al and Sang et al one is the regularized horseshoe prior
built on the original horseshoe prior and the other one is the R2D2 shrinkage prior which actually
works on R squared the quantity I illustrated earlier. Another way we should be able to improve
specifications is by actually making prior elicitation that is translating expert knowledge
into priors feasible in practice so there has been a lot of prior elicitation research but
it didn't really find a way into practice yet and we're going to have an soon we're going to have
an overview paper on prior elicitation led by Pietro Smicola and and by Artu Klami to people
from Helsinki University including a lot of other people from Helsinki area and myself
is in progress it should be an archive soon and it's about a review of prior elicitation
and among others we're discussing there a prior elicitation hyper queue discussing dimensions
in this case seven dimensions how we could characterize prior elicitation method for example
what kind of prior distributions are we eliciting is it a universe or a multivariate prior is it
parametric or non-parametric just an example or what kind of information we're actually
querying from the expert this is D6 that is what kind of elicitation method what kind of
questions are we answering the expert in order to get their prior expert knowledge
or there's like another dimension D4 is how do we actually translate the the queried
expert information and and translate that computationally and what kind of model are we
using to translate the expert prior information to the actual prior distributions and parameters
so this paper will be quite long it will be out soon an archive you can look out for that
it has a lot of details and it has also several potential research direction how we could
make prior elicitation more practically applicable and more practically and widely applied
one one thing I want to highlight is the idea of prior elicitation in observable space
because in a lot of cases the experts don't understand the parameters or their scales
or whatever well they don't like sometimes they do sometimes they don't in any case it
remains difficult to specify in a lot of cases prior distributions on my parameters
for example because they don't have an have an intuitive scale or because we have a lot of them
so it would be very hard to specify a prior individual in each of them another way around
around both of these issues would be if my expert would would give me prior information
on the data distribution because usually they understand the outcome data they understand
the height of people they understand the reproduction rate of bacteria or something
because that's the outcome they've been studying for years so they know how roughly
that should should be distributed so we can elicit a prior distribution p hat of y so the hat means
so we have asked the the expert we have estimated that that prior distribution the marginal prior
distribution on the observable space on the observables y and then we need to translate
the solicited prior distribution from the expert to find a sensible prior p hat of theta on our
parameters such that the the elicit a prior distribution is consistent with the translated
prior distribution of the parameters or vice versa that is if we have if we have the sensible
prior distribution p hat of theta and we're going to we're going to average that we're going to
to to weight the likelihood with that and going to average over the parameter space
we should be getting back our elicited prior distribution of the expert in the observable
space that of course is an inverse problem so we have p hat of y we need to find p hat of theta
that's sensible under some constraints that's a quite complicated topic i have some more ideas on
that that i hope to work on the next couple of years one example for some simple cases we have
published in uai in the uai conference 2020 led by Marcelo Hartmann and by Otto Klein
last but not least we would like to understand the prior sensitivity on the posterior that is
how sensitive is the posterior to my to perturbations changes in the prior distribution
so if we just look at the left hand side of the graph because that's concerns the prior
we can run the posterior inference then we estimate the properties of the posterior with
perturbed priors with slightly changed priors then we check the prior sensitivity that that is we
see how the how the posterior changes if we change the prior bit we diagnose the sensitivity
and if we're happy with how sensitive the the model is with respect to the prior we can use
the model otherwise we could adjust the model adjust the prior in some way or the other i'm not
saying that that our model should be insensitive to the prior it just means that if i want to use
a prior such that the model is insensitive to slight perturbation in the prior i should see that
or the other way around if i want to have a prior that is informative that uses a lot of
information from my expert but then i see that my model is completely insensitive to the prior
that's also not something i know i want so i want to know how sensitive my model my model results
on my posterior is with regard to the prior and of course i would like to know how my model is
also sensitive they are with respect to the like it but that's not the primary concern in this talk
we've recently put a paper an archive that discusses these things and it's led by noah
kalloynen and archiverterie and it's about detecting and diagnosing prior likelihood sensitivity
power scaling you can look it up if you like and so what power scaling does is it just raises
our prior to the power of some quantity that we call alpha so if alpha is one we're raising
something to the power of one that is nothing changes if we raise something to the power of
higher than one we make the prior narrower in this case if if we raise it to something that
smaller than one then we make it wider so we see that for example if we have an exponential
distribution and if we make alpha bigger the the exponential distribution becomes tighter
if we make it wider the exponential distribution becomes wider the same happens with normal
distribution beta distributions gamma distributions so power scaling is a way to continuously
change the the the informativeness if you will of a prior without without causing other problems
such as without leaving the the original definition space so if something was defined between zero
and one if my prior was defined on zero and one then power scaling will leave it on exactly the
same scale so it's a very convenient and general way to scale to scale prior distributions and
that's the way we explore it in this paper and then we can directly visualize how changing or how
power scaling changes the impact of the prior so if we look at the middle plot with with alpha
is one on the left hand side is the the prior this is a bad color for me let's call it green
and then there is the likelihood and in red and there's posterior and something like blue
and we see in this case my posterior is quite similar to the likely implied posterior the
prior doesn't have much effect if i'm going to the right hand side i'm going to the right plot
where alpha is two i'm making through power scaling i'm making my prior narrower and as a
result i mean increasing the informative the information conveyed in the prior and as a result
the posterior will change towards the direction of the prior if i'm making through power scaling
the prior wider for example choose alpha 0.5 i'm making the prior wider less informative
so the likelihood will dominate and so by by slightly perturbing this alpha value and then
using importance sampling to avoid refitting the model i can see how sensitive my posterior is with
respect to perturbations in the prior and i can judge whether this kind of sensitivity is something
i want in my specific case whether i want sensitivity not one and compare it to what i'm
actually seeing in practice okay there was a lot of information a lot of detail i hope you enjoyed
it thank you very much for listening if you want to learn more about me and my research you could
look at my website you could tag me on twitter and you could also write me an email if you have
any research ideas with respect to prior specification that you would be interested
in to pursue and you would like to involve me in that research or have my questions want to
discuss that please reach out to me i'm very interested in those questions so thank you very
much for listening i hope someone else is still there so thank you paul very much for this very
interesting talk and now i would open it for question you can either write it in the in the
chat or you can just raise your hand and we have the first question by sirin hi can you hear me
yes yes thanks for the talk so i would be interested in your take on maximum entropy
so some people are really into choosing maximum entropy priors so what is your opinion on that
i like the approach so so for those of you who who are not familiar with with maximum
entropy priors the the kind of ideas we have some certain restrictions some certain like
boundary information let me let me say like this these are my words now and we want to find the
the prior that is consistent with this with these the certain kind of boundary informations
but under under the the set of all of these consistent priors has the maximum entropy so it
is the widest in a sense and i think this is a really good idea so if we have this kind of
specific information and we want my priors to be consistent with that but at the same time being
maximally uninformative within that space of acceptable priors then that's the maximum entropy
priors i like this approach i would like to use it more but of course in in some cases it's hard
to like specify these specific information that priors needs to be consistent with and then it's
not always trivial to find the prior that has maximum entropy in that space so i would like to
see this more i but i also see that it may be computationally very difficult in a lot of cases
so do we have any other questions
you can raise your hand write it in the chat
then well people think if they have any questions then i have a question
like you were talking about this how it gets very crazy when you have lots of dimensions and
independent priors and that it's practically impossible to for the model to avoid overfitting
and i was wondering maybe i missed something but i was wondering why you think that the
independence is the problem and why the problem is not resolved by just making the prior tighter
so the problem is not resolved by making the prior okay the problem is resolved
by making the prior narrower but we need to make it narrower and narrower and narrower as
the more terms we are adding so any constant prior let's say standard normal in the examples
i was considering will overfit eventually as we add more and more terms the problem is what is
what is locally weekly informative made for a lot of terms be globally very very weird
like very informative in a way the other way around if i have a lot of terms and i have some
kind of like static prior that is non-hierarchical if i if i want to make something like reasonably
informative on this combined scale let's say on r squared for example and i have a lot of terms
then my individual priors on the individual parameters become suddenly very very informative
so suppose i had so many terms that in order to have the global uncertainty well behaved
the global prior well behaved that that my individual price would be normal mean zero
standard deviation 0.01 and i had some standardized ones that would be extremely informative in a
lot of cases and so what i what i need to find are priors that have most of their mouth closed
mouth close to zero but they're allowing some of the coefficients to escape the funnel close to zero
and be in the boundaries there cannot be a lot of big parameters at the same time at least but
in a regression setting it's sometimes called the piranha theory so you cannot have a lot of
big fish in one little pond a lot of them maybe need need to be small on so that's only some of
them can be large but if i'm not having an arachnica prior then either the the i have a strong
informativeness on the local scale on the global scale both of which is not what i want
but i will never have the case that kind of most of which is close to zero but some of
them are allowed to be further away from zero um yeah so so that's that's the
understanding and standing i'm trying to to get across
great thank you so i think i'm in the question right yeah
okay so so david bok asked you talked about likelihood and purpose theory
regardless of the prior how can this be done without a prior so in these examples i've been
using here i've been using um just the simple setting where i'm have a linear regression
with just an intercept so i just i just want to an estimate a mean of a distribution
and without a prior in this case i mean an improper flat prior that is what what my
frequentist result would give me if i didn't if if my prior was improper flat over the whole
real line which is what is consistent and with with frequentist statistics um then this would be
the posterior you are right without the prior as a misnomer yeah um so it's with an improper flat
prior this is what the what the likelihood would imply in terms of posterior
anyone else
well if uh no one else has any further questions then i will stop there
