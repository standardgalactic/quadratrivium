Processing Overview for morelli6
============================
Checking morelli6/Paul Bürkner@Berlin Bayesians： Specifying priors in a Bayesian workflow.txt
1. **Maximum Entropy Priors**: The approach of using maximum entropy priors within a space of acceptable priors is a good way to balance between consistency with known information and maintaining maximal uncertainty where data can inform the most. This approach is computationally challenging but has its merits.

2. **Independence and Overfitting**: The issue with independent priors in high-dimensional spaces is not necessarily the independence itself but the fact that as the number of parameters grows, a fixed prior becomes increasingly informative and can lead to overfitting. To avoid this, one needs to tighten the prior more and more, which can become impractical or computationally intensive.

3. **Local vs Global Priors**: In a regression with many terms, a globally non-informative (e.g., standard normal) prior will result in individual parameter priors that are extremely informative when the dimensionality is high. This can lead to unrealistic results if not managed properly. The challenge is to have priors that allow for some parameters to be somewhat informative while keeping most of them close to zero (the "piranha theory").

4. **Prior Independence**: If one does not use a hierarchical prior, the individual priors on each coefficient must be sufficiently informative to ensure that the global posterior remains well-behaved and not overly influenced by any single parameter.

5. **Frequentist Perspective**: Without a proper prior (and assuming linear regression), using an improper flat prior over the whole real line leads to a posterior distribution that is consistent with likelihood-based frequentist inference. This is because the likelihood itself, in the absence of a prior, dictates the posterior when dealing with improper priors.

In summary, the discussion touched upon the balance between incorporating informative priors and maintaining model robustness, especially in high-dimensional settings. The maximum entropy principle offers a method for determining priors that are as informative as possible given known constraints without overfitting. Additionally, the use of improper flat priors in frequentist frameworks was highlighted as a way to avoid relying on prior distributions when making inferences based solely on likelihood.

