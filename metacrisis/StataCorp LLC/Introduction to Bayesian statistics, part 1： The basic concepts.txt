Hi, Chuck Huber here for the Stata YouTube channel. Today I'd like to give you a relatively
non-technical introduction to Bayesian statistics. The Bayesian approach to statistics has become
increasingly popular and you can fit Bayesian models using the Bayes M.H. command in Stata.
This video will provide a brief introduction to the concepts of Bayesian statistics and
you can refer to other videos for worked examples using Stata.
Many of us were trained using a frequentist approach to statistics where parameters are
treated as fixed but unknown quantities. We can estimate these parameters using samples
from a population, but different samples give us different estimates. The distribution
of these different estimates is called the sampling distribution and it quantifies the
uncertainty of our estimate, but the parameter itself is still considered fixed.
The Bayesian approach is a different way of thinking about statistics. Parameters are
treated as random variables which can be described with a probability distribution.
We don't even need data to describe the distribution of a parameter, probability is simply our
degree of belief. Let's consider a simple example to develop
our intuition. Here I have an American quarter. This side is often called the head side and
the other side is called the tail side. If I toss the coin in the air it must land on
either the head side or the tail side and we could use theta to denote the probability
that the coin lands with the head side facing up.
Since I don't have any data I could use a uniform distribution to express the belief
that the probability of heads could be anywhere between zero and one with equal probability.
Here I've used a beta distribution with parameters one and one which is equivalent to a uniform
distribution on the interval zero to one. But common sense would suggest that the probability
of heads is closer to point five and we could express this belief mathematically by increasing
the parameters of our beta distribution which would concentrate the probability closer
to point five. This mathematical expression of our belief about the parameter is called
the prior distribution. We could also investigate the value of theta by conducting an experiment.
We could flip the coin ten times and calculate the proportion of times that the coin lands
with the head side up. Here we observe four heads out of ten tosses of the coin. We can
quantify the results of our experiment using a likelihood function. In this case the blue
line displays a binomial likelihood function for theta given four heads out of ten coin
tosses.
Bayesian analysis allows us to update our belief about the parameter with the results of our
experiment. In simple cases we can compute a posterior distribution for the parameter
by multiplying the prior distribution and the likelihood function. In this example the
beta distribution is called a conjugate prior for the binomial likelihood function because
the posterior distribution belongs to the same distribution family as the prior distribution.
Both the prior and the posterior have beta distributions. Here I have plotted the posterior
distribution of theta with the prior distribution and the likelihood function. Notice that the
posterior closely resembles the prior distribution. This is because we used an informative prior
and a relatively small sample size. Let's look at the effect of different priors on
the posterior distribution. Here the red line represents a completely uninformative prior
in the form of a beta distribution with parameters one and one. The likelihood function is plotted
in blue but it is masked by the posterior distribution which is plotted in black. This
is an important feature of Bayesian analysis. The posterior distribution will usually be
equivalent to the likelihood function when we use completely uninformative priors. The
animation shows that more informative priors will have greater influence on the posterior
distribution for a given sample size. And this animation shows that larger sample sizes
will give the likelihood function more influence on the posterior distribution for a given
prior distribution. In practice this means that we can obtain precise estimates of the
posterior distribution using smaller sample sizes when we use more informative priors.
But similar precision may require a larger sample size when we use a weaker uninformative
prior. We can use the posterior distribution to calculate various quantities. For example
we can calculate the mean of the posterior distribution or the median or the mode if
we wanted. And we can calculate the probability that theta lies within an interval such as
0.4 to 0.5. And similarly we can calculate a 95% equal tail credible interval. We can
also calculate a highest posterior density credible interval also known as an HPD credible
interval. The calculation of HPD credible intervals is often described as dropping a
horizontal line through the density until the probability within the interval equals 95%
or 0.95. For a symmetric distribution the equal tail and HPD credible intervals will
look very similar but they will look very different for skewed distributions. One nice
feature of the Bayesian approach to statistics is that the posterior distribution from a
previous study can often serve as the prior distribution for subsequent studies. Here
we have used the posterior distribution as the prior for an experiment with a sample
size of 100. The resulting posterior from this experiment has an even narrower distribution
and we can then calculate the posterior mean in 95% credible interval for theta.
So how do we do this in STATA? Well here I've simply typed in the raw data, 0 represents
tails, 1 represents heads and at the bottom I've tabulated the variable heads and I can
see that there are 6 tails and 4 heads. So if I wanted to fit a Bayesian model I can
go to statistics and then scroll down to Bayesian analysis and then select estimation and then
this will open the Bayes-MH dialog box. So the first thing we need to do in the model
tab under syntax is we need to select univariate distributions and then under model we need
to select a dependent variable which in this case is heads. There are no independent variables
in this case. Next we need to specify a likelihood function or distribution as well as our priors.
So for the distribution I'm going to select Bernoulli distribution and then for the success
probability I'm going to click create and I need to specify a parameter name. So I'm
going to call my parameter name theta and there are some other options here but we'll
come back and talk about those in other videos. For now I just need a name for my parameter.
Then the next thing I need to do is specify the priors for my parameter or prior. So let's
click create and then at the top of the prior dialog box I need to select which parameter
which in this case is theta, it's the only one and then I need to choose a prior distribution.
In this case I'm going to use a beta distribution and for shape parameter A I'm going to use
one and shape parameter B I'm going to use one as well. This is the beta version of a
uniform distribution. Then I'm going to click OK and then click submit and in the results
window let's take a look at the command here. The command is Bayes mh followed by the dependent
variable heads and then I can see my likelihood function specified as theta Bernoulli likelihood
with parameter theta and then I have a prior for the parameter theta in which case in this
case the prior is a beta 1 1 distribution. Then if I look further down in my output I
can see that the command echoes the likelihood in the prior. It also shows me some information
about the the MCMC iterations and so forth and we're going to come back and talk about
that in the next video. So let's just skip to the bottom here. We can see that the mean
for theta is 0.4132. The standard deviation is 0.1370 for theta. Then we have our MCSC
our Monte Carlo standard error and we also have a median reported in addition to the
mean. Stata reports the median. Then we also have our equal tailed 95% credible interval.
If we wanted to see the HPD credible interval we can go back to reporting to the reporting tab
and I can check this box that says display HPD credible interval. So let's click that click
submit and now we can see that state is reported in HPD credible interval. So another thing we
might want to do is use a more informative prior. Here we've used a beta 1 1 which is equivalent
to a uniform. Let's go back to the model tab and select our prior one and click edit. And now I
could change the shape parameters. Maybe I want to use a 30 30 shape parameter 30 for a and for
B. Click OK. And let's click OK again. And now Stata will refit the model using a beta a more
informative prior beta 30 30 and notice that our mean has moved closer to 0.5 because again we've
used a much more informative prior that's closer to 0.5. So anyway that's the basics of how you
could do this. So I hope this was helpful. Thanks for stopping by.
