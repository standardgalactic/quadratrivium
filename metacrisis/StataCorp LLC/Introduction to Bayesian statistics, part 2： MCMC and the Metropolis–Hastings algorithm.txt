Hi, Chuck Huber here for the State of YouTube channel. Today I'd like to give you a relatively
non-technical introduction to Markov Chain Monte Carlo, which is often shortened to MCMC.
There are different variations of MCMC, and today I'm going to focus on the Metropolis
Hastings algorithm. In the interest of brevity, I'm going to omit some details, and I strongly
encourage you to read the manual before using MCMC in practice. Let's continue with the
coin toss example from part one. We are interested in the posterior distribution of the parameter
theta, which is the probability that a coin toss results in heads. Our prior distribution
is an uninformative or flat beta distribution with parameters one and one. And we will use
a binomial likelihood function to quantify the data from our experiment, which resulted
in four heads out of ten tosses. We can use MCMC with the Metropolis Hastings algorithm
to generate a sample from the posterior distribution of theta. We can then use this sample to estimate
things such as the mean of the posterior distribution. There are three parts to this technique, the
Monte Carlo part, Markov chains, and the Metropolis Hastings algorithm. The term Monte Carlo refers
to methods that rely on the generation of random numbers. For example, I could generate
a series of random numbers from a normal distribution with a mean of .5 and some arbitrary
variance sigma. In this example, I'm going to refer to the normal distribution as the
proposal distribution. In the graph on the right, I'm going to shift the proposal distribution
to the right each time I draw a value of theta. This graph is called a trace plot, and it
allows me to view the values of theta in the order in which they were drawn. The density
plot on the left is a histogram of the samples we have generated. Here I've rotated the
density plot so that the theta axes for both graphs are parallel. If we were to generate
10,000 numbers, the resulting density looks much like our proposal distribution. Note
that the proposal distribution does not change from one iteration to the next. A Markov chain
is a sequence of numbers where each number is dependent on the previous number in the
sequence. In this example, each value of theta is drawn from a normal proposal distribution
with a mean equal to the previous value of theta. If we generate 10,000 numbers using
Markov chain Monte Carlo, the trace plot of theta appears to wander and is often called
a random walk. Note that the resulting density plot does not resemble the proposal distribution.
The Metropolis Hastings algorithm is used to decide which proposed values of theta to
accept or reject. We begin by calculating the posterior probability using the newly
generated value of theta. We also calculate the posterior probability using the previous
value of theta. Notice that we don't have to know the functional form of the posterior
distribution. We simply multiply the prior distribution in the likelihood function. If
the posterior probability is greater for the new value of theta, the ratio of the probabilities
will be greater than one and we will always accept the new value of theta. If the posterior
probability is greater for the previous value of theta, we will not necessarily discard
the new value of theta. We can treat ratios less than one as an acceptance probability.
Then we can draw a uniform random number and keep the new value of theta if the random
number is less than the acceptance probability. Otherwise, we discard the new value of theta
and keep the previous value. Let's look at an example of how this works. Here the previous
value of theta was 0.517 and we have drawn a new value of theta which equals 0.38 from
our proposal distribution. The ratio of the posterior probabilities for the two values
of theta is equal to 1.307. Since the ratio is greater than one, the acceptance probability
is one and we will keep the new value of theta. The new value of 0.38 becomes the previous
value of theta and we draw a new value of theta from our proposal distribution which
equals 0.286. The ratio of the posterior probabilities 0.747 becomes our acceptance probability and
we draw a random number from a uniform distribution. The random number 0.094 is less than the acceptance
probability of 0.747 so we will keep the new value of theta. So the new value of 0.286
becomes the previous theta and we draw a new theta from the proposal distribution which
equals 0.088. The ratio of the posterior probabilities equals 0.039 which will be used as the acceptance
probability. The random uniform number drawn in step 3 is larger than the acceptance probability
so we reject the new value of theta and keep the old value of 0.286. I have displayed the
new value of theta in red to indicate that it has been rejected. We can repeat this algorithm
ten thousand times and notice that some proposed values of theta are green indicating that they
have been accepted while others are red indicating that they have been rejected. The result of
this process is a sample from the posterior distribution. There are two issues with the
Metropolis Hastings algorithm. First the sample is dependent on the starting value. We can
reduce the influence of the starting value by discarding the first part of the sample.
This part of the sample is called the burn-in period. The burn-in period is the time it
takes for the chain to stabilize so that it isn't drifting up or down over time. By default
theta discards the first 2,500 values from the sample but this default is arbitrary and
you may want to use a different value after inspecting your trace plot. We can also compare
density plots of the first and last half of the sample with the full sample to look for
differences. The second issue is that the values of theta are correlated because they
are generated by a Markov process. This is normal and theta accounts for the autocorrelation
and its estimation algorithms. Excessive autocorrelation may indicate problems with model specification
and should be investigated further. If the model is correctly specified, thinning is
sometimes used to reduce autocorrelation. Thinning refers to the process of increasing
the MCMC sample size and drawing samples at regular intervals. For example we could generate
a sample of 30,000 values and keep every third value. In this example we know that the posterior
distribution is a beta distribution with parameters 5 and 7. This graph compares a histogram
and kernel density estimate of our MCMC sample with the theoretical posterior distribution.
In this case our MCMC sample provides a very good approximation to the theoretical posterior
distribution. So let's continue our coin toss example using Bayes' image. We specified
a beta distribution with parameters 1 and 1 along with a Bernoulli likelihood. Next we
can specify the size of our MCMC sample, the length of the Bernin period and the thinning
interval in the simulation tab of the Bayes' image dialog box. The output displays our
choices including the total number of MCMC iterations, the length of the Bernin period
and the MCMC sample size. It also shows the acceptance rate which is the proportion of
proposed values of theta that were included in our MCMC sample. I'll refer you to the
manual for a definition of efficiency but high efficiency indicates low auto correlation
and low efficiency indicates high auto correlation. We can also view diagnostic plots by selecting
statistics, Bayesian analysis and then graphical summaries. In the main tab we can select multiple
diagnostics in compact form and then we can choose the parameter of interest. Stata will
create a graph that contains a density plot for our MCMC sample, a trace plot, density
plots of the first and last half of the MCMC chain as well as a Corellogram. So that's
the idea behind Markov Chain Monte Carlo using the Metropolis Hastings algorithm and a brief
introduction to it using Stata. I hope this was helpful. Thanks for stopping by.
Thank you.
