I said I would make fewer videos, but honestly making videos is a great way for me to learn
about stuff.
Let's learn about Bayesian neural networks.
A neural network, whether it has linear layers, convolutional layers, recurrent layers, or
all of those layers, a regular old neural network is a deterministic neural network.
After training the neural network, its weights will stay the same forever, so if you give
it the same input again and again, it'll return the same output every time.
I've made plenty of these.
A Bayesian neural network is not deterministic.
Instead of weights, it has probability distributions, usually Gaussian bell curves defined by a
mean and standard deviation.
Every time the neural network needs a weight, it picks one at random from the corresponding
probability distribution, so if you give it the same input twice, its outputs might be
completely different.
Why would we want to do that?
Well, it's cool.
It looks a little organic, doesn't it, or biological maybe, like random mutations in evolution.
It's like the networks got imagination instead of just being a computer playing by the book.
To put it in more technical terms, the Bayesian approach accounts for its own uncertainty leading
to generalizability.
This happens to help in reinforcement learning, which is what I'm always thinking about nowadays.
In reinforcement learning, a policy is a neural network which turns experiences into actions.
An agent can follow the policy and report back all the rewards and punishments it got.
The policy trains on those reports, hopefully eventually maximizing the agent's rewards.
But it's hard to collect good training data.
If the policy doesn't make the agent explore enough, the policy won't learn anything from
the reports it gets.
We can make the agent explore by telling it to act randomly.
That can get some nice training data.
I've also learned to use TN2 to implement the soft-actor-critic method in which the
policy trains to maximize rewards and act as randomly as possible.
Good data galore.
A Bayesian neural network might help in the same way.
Some randomness could get training data with variety, so the policy would be more generalized.
It just so happens there's a Python package called Blitz, which can make Bayesian layers
in PyTorch.
So let's make reinforcement learning for my predator-prey scenario more complicated.
In my predator-prey pie-bullet environment, two spheres start in random positions pointing
at random angles.
The red predator sphere is constantly moving and wants to collide with the blue prey sphere,
who is, for now, just standing still.
At every time step, the predator applies its policy to its observation to adjust the angle
of its velocity.
This predator is moving randomly.
If I was the predator as a living squid, I'd spin around in one place until I spotted
the prey and then move forward toward it.
It just so happens that's what this deterministic policy does when it's well-trained.
Before we add Bayesian layers to this policy, I wanted to describe the structure of this
policy I've been using for a while now.
The predator's observation is a consecutive series of eight pictures from its first-person
perspective.
Each picture is two-dimensional and has four channels, red, green, and blue, and distance.
First, I want to reduce the observation's dimensionality.
It's too big and deep and complicated for a long short-term memory layer.
I feed the series of two-dimensional pictures through three-dimensional convolution, but
with just a two-dimensional kernel.
So all the 2D pictures are convoluted in exactly the same way.
3D max pooling with two-dimensional kernel and two-dimensional stride shrinks the observation
small enough for the long short-term memory layer to handle quickly.
This is just like a collage, but I think it works.
After each epoch of training, the policy is tested 100 times with the predator and prey
starting extra far apart from each other.
After an hour of training and testing, the predator collides with the prey over 90% of
the time.
Well, would you look at that?
Let's see what happens with a Bayesian policy.
Blitz offers Bayesian linear layers, Bayesian convolutional layers, and even Bayesian long
short-term memory layers.
I obviously tried implementing all I could at once, but then the policy took way too
long to run.
I ended up implementing just one Bayesian convolutional layer.
So this part of the neural network is deterministic, but this layer has a curious imagination.
So the rest of the neural network has an element of randomness.
I tried training this a couple times, and testing it a whole bunch.
And although it wasn't always so impressive, sometimes it would only take about 20 minutes
to get 90% predator victory in a test.
Look at it go!
Hot diggity daffodil!
Okay, I think it's working.
Thinking's all I do here at Thinkster.
You'll notice I'm not using Tianshu for this.
I'm confident Tianshu's soft actor critic policy, trained on five environments at once,
would work better if I could get it to work with a Bayesian neural network, or even a
deterministic neural network.
Check this out.
In a few epochs, the test value goes up real fast, but then it crashes, and never goes
back up.
This stuff is hard.
Maybe I should make fewer videos, like once a month, once I get things working.
Bye-bye.
By the way, I've got a Patreon at patreon.com slash thinkster.
I want to thank all these squidlings and elder squids.
Thanks.
