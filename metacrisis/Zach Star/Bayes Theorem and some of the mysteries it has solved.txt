This video was sponsored by CuriosityStream, home to over 2,000 documentaries and non-fiction titles for Curious Minds.
In 2014, a man named John Albridge was working on a lobster boat.
He was 65 kilometers off the tip of Long Island, and it was the middle of the night.
But while working, a handle that he was using for leverage snapped, sending him flying backwards into the ocean.
He yelled back to the two other people on the boat, but they were sleeping and the engine was too loud.
So the boat, which was on autopilot, faded into the night.
It was 3.30 in the morning and John was stranded in the Atlantic Ocean with one goal, to stay alive.
It wasn't until 6 a.m. when the other two men on board woke up and they immediately called the Coast Guard saying they've got a man overboard.
Now, they had no idea if he fell off five minutes ago or eight hours ago.
All they knew was their current location and the last time they saw him.
Based on water temperature and weather, the Coast Guard determined that John had about a 19-hour survival window before
hypothermia took over and his muscles gave out.
So Nat was a race against the clock to figure out where he was and here's what they did.
Based on the boat's current position and its autopilot speed, they could determine the maximum distance John could be from that location.
From this, we can sweep out a circle, which tells us if John stayed put, he has to be somewhere in that circle.
But the boat was on a course away from the shore, so we could really narrow that region down.
Then the Coast Guard input all the given information into something known as the search and rescue optimal planning system,
which operates based on math, statistics, and actual data about the weather, ocean currents, and so on.
The system superimposes a grid over the ocean and assigns each square a probability that John is there based on given information.
Since there was a several hour long period where he could have fallen out, we expect to see a long strip of high probabilities and thus a search began.
Now, if we first search here, let's say, and don't find him, does that mean he's not there and thus the new probability is zero?
The answer is no. He's a small head in a huge ocean. We could just have missed him.
So after we search a grid, we need to update its probability, not just set it to zero.
And this brings us to Bayesian search theory, an application of Bayesian statistics that's used to find lost objects.
The foundation to this is obviously Bayes' theorem, but I'm just going to use conditional probability to keep things simple.
Either way, the goal here is to find the probability of some event A being true, given that another event B is true.
For example, if I roll the die and hid the result from you, then ask, what's the chance it's a five?
You would obviously say one in six.
But if I gave you more info, like I told you the roll came out odd, this is a relevant given that reveals more about the situation.
So the probability of it being a five, given that it's odd, is one-third. New information can really change things.
Now, Bayes' theorem for the probability of A given B is written like this, which can be derived from this formula,
which I'll be using soon. And by the way, this numerator just means the probability that both events are true.
Now, going back, let's say we search this square and don't find him.
We now want a new probability, the probability that he is there, given that we looked and didn't find him.
Well, for that square, he's either there or he's not.
Maybe the square has a five percent chance that he's there, according to the software. If he's in that square, he will either be found or not.
Maybe the skies are clear, so there's a 70% chance that you find him if you look, but still a 30% chance you miss him.
If he's not in the square, then of course, he won't be found with 100% certainty.
From the formula we saw earlier, we are first going to find the probability that he is in the square and also not found, as those are the two events
I listed up here. The probability that he's in the square and not found can be determined by just going down these branches and
multiplying those probabilities together.
Then we need to find the probability that he's not in that square period,
which can be determined by going down these two branches that both end in being not found.
We add the product of those probabilities and we get a value of about 1.55%,
which again is the chance that he is still in that square, given that we couldn't find him there.
If we go back to our grid, that square we just searched is now updated to its new 1.55% probability.
Now since that square went down, the other squares probabilities go up by a small amount.
It's obviously best to continue our search probably with this square,
but after a few hours of searching, we can see it might be more beneficial to search that original square again,
rather than one of these lower probability green squares out here.
Now through various studies a while back,
it was determined that the probability of detecting an object is inversely related to the cube of the distance away.
Or at least it serves as a strong estimate, but no, typically more general detection equations are used.
The constant is usually determined experimentally based on things like depth of the water,
if the object is submerged, or the size of the object that you're looking for.
This is one way that probability value we saw earlier could be estimated for those who are curious.
So if John is here, let's say, and a helicopter flies this straight line path looking for him,
at any given point there's some probability of locating John based on the distance.
This probability changes at each point, and if we integrate or sum the values along the line,
we get a total probability of detection for that line.
But we don't know where John is, so we need to just do sweeps that optimize our odds,
which means we need to apply optimized search patterns.
Now if there's a high probability of John being in one certain location,
like if we knew exactly where he fell off, then an expanding square search might be ideal to cover areas of higher probabilities.
Sometimes a creeping line search is better, but in this case it was best to use a parallel search running parallel to the strip of high probability.
And yes, that software we saw earlier was calculating all of this using math and statistics based on given information.
Now after hours in the water, John managed to find something to keep him afloat,
but he had lost so much energy and was preparing for the real chance of dying.
He decided to soon tie his body to the object, so when he was found,
the body could be delivered to his parents so they'd have something to bury.
But finally at 3pm on one of the final sweeps,
the search team saw John and got him safely to the hospital,
where he was treated for hypothermia and made a full recovery so he could tell this story.
Without John's ability to stay calm at sea and a formula that dates back a few hundred years,
it's very likely this man wouldn't be alive today.
Next up in 1966, a United States Air Force bomber collided with another aircraft
during a mid-air refueling over the Mediterranean Sea near Spain.
On board the bomber were four hydrogen bombs that fell to the earth.
Three were recovered within 24 hours, but no one knew where the fourth one was,
and thus analysts assumed it was resting somewhere at the bottom of the ocean.
To narrow down the search, some experts put Bayesian search theory to work again.
The factors here were different than the last story though,
like the missing object was submerged in the ocean this time.
However, in this case, a local fisherman witnessed something fall into the ocean by parachute on the day of the crash,
which gave the government an idea of where the bomb likely was,
which really helped shape the initial probability map.
Now, two months had gone by with no results,
but remember, failed searches lead to higher probabilities of other squares as things are updated.
After around 80 days, the bomb was found almost 3,000 feet underwater
and within 1.5 kilometers of where the fisherman predicted.
It's assumed that the fisherman's testimony saved the government over a year of work.
Next up on June 1st, 2009, Air France Flight 447 mysteriously disappeared over the Atlantic Ocean
as it was traveling from Brazil to France and no one knew what caused it.
Within a few days, floating debris and a few bodies were found fairly close to the last known location of the plane,
but in order to determine what caused the crash, they needed to find the black box or the flight recorders.
First, they determined the maximum distance the plane could have flown from its last known position.
They expected to find the wreckage somewhere in this area.
Then, based on ocean currents and bodies that were found,
experts performed a backwards drift to produce trajectories that could locate the crash site.
They then found a 95% confidence zone or a rectangle that likely contained the wreck location.
After several unsuccessful searches, a research consultancy was tasked with creating a probability map using available information.
This was the initial distribution, but given failed searches, they were constantly making more updated maps.
And by the way, I really can't do justice for all the analysis that went into this.
But soon after resuming the search using the statistical analysis,
Sonar scans discovered large portions of debris, and a few weeks later, the flight recorders were discovered all right around the areas of high probability.
They found out that the crash was caused by inconsistencies with airspeed measurements, likely due to tubes being obstructed by ice crystals.
The plane ended up stalling and did not recover, and 228 people died on this flight.
Many bodies were recovered and returned to the families to bury, however, 74 were never found.
Two years after this, in 1968, four submarines mysteriously disappeared, all of which occurred within a few months period.
The French submarine has never been found.
The Israeli submarine wasn't found for 30 years, and the Soviet submarine was partially recovered six years later.
But the USS Scorpion of the US Navy was found just a few months after disappeared, and although I'm not going to go into detail on this one,
it was the same Bayesian search theory that helped locate the wreckage.
Now, to be fair, an acoustics expert used Sonar technology to help pinpoint where to look here, which was a huge factor, of course.
So I'm not saying statistics and probability were the only reasons these mysteries were solved, but they did play a big role.
Unfortunately, when the submarine went down, it took 99 people with it, killing all of them, and the cause of what sank the USS Scorpion is still unknown to this day.
Then in 1857, the SS Central America sank during a hurricane, taking 425 of the 578 passengers and 14,000 kilograms of gold to the bottom of the ocean.
The modern day equivalent of that is $292 million, by the way.
The location of the gold remained a mystery for over 100 years, but in 1988, a man named Tommy Gregory Thompson used route information and documents from the time of the disaster
to reconstruct the ship's path.
He then created a probability map of likely places where the gold would be located, and despite skeptics, that hard work paid off when he and his team recovered over $100 million worth of gold.
So for anyone who doesn't see the use of stats, it made this guy a millionaire.
But then he sold millions of dollars of gold before paying his investors and his team.
He was then sued by several of those people.
He went into hiding in 2012 and was arrested three years later, and in November 2018, a jury awarded several million dollars back to investors and team members.
Well, this guy's so stupid.
Hold up.
Can we just acknowledge all the dates I just said here?
Because this gold was lost in 1857 before the American Civil War.
It was discovered in 1988, just a few years before I was born, and that court case I just mentioned happened in November of 2018, which is four months ago as of releasing this video.
This is real recent, and I just find it crazy that one storm that happened over 150 years ago is still having effects till this day.
Okay, now let's get back to the next story.
Now moving on to a new category.
In 1787 and 1788, Alexander Hamilton, James Madison, and John Jay anonymously wrote 85 essays known as the Federalist Papers, which were meant to persuade people to ratify the Constitution.
The specific author of each of these essays was mostly known with Hamilton and Madison writing a majority.
However, 12 were left for debate as Noah knew whether Hamilton or Madison wrote them.
Hamilton was shot by Aaron Burr and died in 1804, taking the secret to his grave.
This remained a mystery for about 175 years until two statisticians decided to take a crack at it.
They took articles in which they knew whether Hamilton or Madison was the author and analyzed certain words that one used more than the other.
Their writing styles were extremely similar, but words like while, wilts, upon, and so on were a few words of interest, since, for example, Madison rarely used the word upon, whereas Hamilton did.
In fact, throughout 49 of Hamilton's papers, they found that the word upon came up 3.24 times per 1000 words on average.
After analyzing all the known papers of both Madison and Hamilton, a comparison could be made showing a big difference.
When faced with a mystery article, the statisticians first assumed 50-50 odds that was written by either author.
They would then analyze the frequency of 30 words of interest and update the probability using Bayesian statistics.
New given information like how often a word would appear allowed for a more updated and accurate probability of who wrote it.
Now, they first ran this test on known papers of Hamilton and Madison, and it predicted the author correctly every single time, and not just by a little.
The most inconclusive result occurred when running the numbers on an article written by Hamilton, and it still said there was a 95% chance that he was the author.
So the test seemed accurate, and when it finally came time to test the 12 mystery articles, the result said every single one was written by Madison.
The weakest case was number 55, which still said there was a 99.6% chance Madison wrote it.
Is this a guarantee? No. Even if the word frequencies were accurate, it's possible the two edited each other's papers or something like that.
But we're definitely more confident than ever that Madison is the true author of those 12 papers.
Then I'm sure many of you recognize this.
This is one scene from the movie The Imitation Game about how mathematicians broke encrypted German messages during World War II.
In the movie you see this electromechanical machine that tests possible wheel arrangements of an enigma, which the Nazis used to encrypt messages.
Due to the extremely large number of possible settings, Alan Turing, one of the inventors of the machine we just saw, first needed to drastically reduce the number of tests this machine had to conduct.
Turing developed a manual method to do this in which he would guess strings of letters in an unencrypted message, which he could do because things like the weather report or phrases like Heil Hitler were repetitive and predictable.
He would then measure the validity of his guesses using Bayesian-like methods, updating the probabilities as more information came in.
In fact, while doing analysis, one of his coworkers asked him, aren't you essentially using Bayes' theorem? To which he replied, I suppose.
So it wasn't exactly Bayes' theorem like we know it, but most people say this analysis involved Bayesian inference.
Now there's of course way more to this, and actually Numberphile has a few great videos on how the code breakers would slide the expected word along a string of encrypted letters, which help reduce possibilities, and I'll link that below.
But as we now know, the analysis developed by Alan Turing and his coworkers is assumed to have ended the war several years earlier and thus saved millions of lives.
Again, probability and statistics weren't the only factors, of course, but without them, who knows how things would have turned out.
And by the way, Alan Turing was not the only person involved in code breaking during World War II.
In fact, a mathematician named Bill Tutt, who you've probably never heard of, pulled off what some described as one of the greatest intellectual feats of World War II, also helping save millions of lives.
I unfortunately don't have time to go over it, but if you want to learn more, then I highly recommend heading on over to CuriosityStream, who I'd like to thank for sponsoring this video.
CuriosityStream is a streaming service that hosts thousands of documentaries and nonfiction titles spanning history, physics in the universe, technology, nature, and plenty more.
If, for example, you really want to change your understanding of the physical world, some of my personal favorite documentaries to watch are those on quantum physics, since pretty much every moment of these goes against everything you assume about physics.
Founded by John Hendricks, aka the founder of the Discovery Channel, CuriosityStream is an extremely affordable streaming service at only $2.99 a month that will satisfy anyone with a strong desire to learn, explore, and understand the world around us.
The service is available on a variety of platforms worldwide, including Roku, Android, iOS, Xbox One, Apple TV, and more.
If you go to curiositystream.com slash majorprep or click the link below and use the promo code majorprep, you'll get your first month's membership completely free, and this gives you unlimited access to top documentaries and nonfiction series that I know many of you will find very interesting.
Again, links are below, and with that, I'm going to end that video there.
