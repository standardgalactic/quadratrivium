Okay, good, so can you guys see the full screen?
Yes.
Okay, good, good.
All right. Well, good to see you all there in Egypt.
It sounds like a great workshop and you've got some big buddies of mine,
Fernando and Pedro there, I think, somewhere in the background, I don't know.
So you've got an interesting great week ahead with lots of exciting speakers.
And I think it's kind of an honor for me to be able to present probably what I would say is one of the most interesting and important attempts at some sort of a general theory of what's going on in the brain from an algorithmic perspective,
the Bayesian brain predictive processing, active inference and the free energy principle.
So, you know, I can see you guys all there.
I wonder if by a show of hands, I could like anyone could put up their hand if they feel like they have a grip on the free energy principle and active inference.
Anyone put up their hand.
Can't quite see how about how about something easier like anyone like familiar with the kind of basic Bayesian brain hypothesis.
Okay, so you know a few so okay well, okay, we'll look, we're going to go at it slowly.
I assume that like most people have a kind of a mathematical background so so that's going to make it a little easier.
So we'll start from philosophy, and you know the basic, the basic place we start is from the nature of perception.
You know, I'm outside, I'm looking at a tree, and I kind of just see a tree and it's just amazing.
But we are thinking, well, somehow I form some sort of a representation of that tree in my mind.
And, of course, what we, what we begin to understand is that actually, if I've always perceiving just the representation of the tree in my mind, then I'm not really perceiving the tree as it is in itself, you know, that's a sort of philosophical issue.
And also, the other thing that I must be doing is I'm also forming a representation of myself. And that is so in our, in our minds I you know we're also forming some sort of representation of ourselves.
And it's a big issue in philosophy how this works.
And, you know, one of the first great philosophers to really start to nail this is Immanuel Kant in the 18th century.
And what he showed is that, look, we only experience the phenomenal world, the things as they appear to us.
We have to, when we use the categories of space and time and causality to arrange our perceptual framework.
And these kind of things like space, time, causality are kind of hyper prized that we bring to bear to organize our sensory appearance, our sensory data.
And what he called it is that he thought that he had created some sort of Copernican revolution in, in the, in the mind sciences in the sense that he said look, it's objects that must conform to our cognition rather than cognition, conforming to objects.
And this was a huge insight and, you know, about 100 years later, Helmholtz, a very famous physicist and psychologist, started to crystallize this as perception as inference.
And this is really where, where, where the, where the free energy principle and predictive processing start with.
When we look out from our eyes, unlike what sort of Descartes, the typical view that Descartes might have helped, we are not just passively experiencing what's coming through our eyes.
Um, perception is kind of hard.
If you look at this video, what you see is roughly speaking, what we might see in accurately if you could actually see what's coming through your eyes, because all that we can proceed through our eyes is a very, very small central area of the macular for there.
And it's very difficult to see what's going on. And by the way, you know, most of it's in black and white only the central partisan color. So if we actually see what I was, what you're actually seeing.
Now you can see much clearer that, you know, you're perceiving this be buzzing around a flower.
Now, it's very difficult to make sense of the dot on the left, but somehow our brain constructs a whole image.
Now, how is that done?
That is done. That is called the Bayesian perception problem. And the basic problem is this, we receive some sort of that, you know, that we're assuming that there is something out there in the world, which are the causes of our perception.
Actually, according to Kant, we can't really say much about this at all. But what we do get is sensory data. So you is some sort of large vector of sensory data.
And the problem for the agent is that we have to form something called the recognition density, which is the probability probability distribution of like, what are the causes of my sensory data given my sensory data, what are the causes of my sensations given my sensory data.
Now that, of course, is solved by using Bayes' theorem. And Bayes' theorem, as you know, this term here what we're trying to solve for is the posterior, the probability of the causes of my sensations given my sensory data.
And to solve that, we have to use two terms as well as a normalizing term underneath here.
We start with our prior probability of what do we, before receiving any data, what do we expect the, what are the background, prior beliefs around the causes of these features in the world, that there are trees in the world, for example, and there are agents.
And then the second term is the likelihood of the data. What is the probability of getting this particular sensory data, given that it comes from my best guess, which there is a tree.
So in other words, how likely am I to receive these particular sensations, given that there's a tree out there, my hypothesis of what's going on. And underneath is the probability of receiving this sensory data in the first place, which is a kind of normalizing term.
Now that normalizing term actually is very difficult to compute. It's an infinite integral. You basically have to go through every single hypothesis of, you know, this sensory data could be caused by a tree, it could be caused by a mountain, it could be caused by, you know, some sort of hallucination, you know, you couldn't possibly do it.
So it's computation explosive. So although Bayes' theorem is the mathematically optimal solution that maximizes the value of all information, it is absolutely the correct mathematical solution.
We cannot solve it. We cannot solve it using machine learning, because it's computationally expressive, and evolution cannot solve it. Our biological beings cannot solve it explicitly. So how do we get around it?
Well, we can draw inspiration from machine learning techniques like artificial neural networks.
So artificial neural networks, you know, train on whole loads of sensory data. So whole loads of data, for example, this particular neuron could train on a whole load of faces.
But as it learns, as we adjust these synaptic weights, it starts to do something called feature extraction. So at low levels of the sensory data, at the low levels of the neural network, it extracts low level features like edges,
sorry, edges and corners and little things like this, no level features. At mid levels of the network, it's extracting mid level features like noses and eyes and ears.
And then at high level areas, it extracts the high level features like whole faces, for example.
But the question is, you know, how do these networks train? These networks train essentially by being provided some ground truth sensory data.
So what we do is go through something called supervised learning. Okay, so the way supervised learning works is that we really know what are true faces.
You're given all these thousands of examples of true faces. And then what happens is that we get a new face, a sensory data, and we take a guess, this network has not yet trained, it takes a guess at what this is.
This could be a cat or a dog or a face. And what we know is whatever this image is, it could be, as I said, anything, a cat or a dog or a face, we actually know the true category.
It's actually a face. And so we can compare the guess of the neural network with the ground truth, the true features. And that sets up a prediction error.
We use that prediction error signal, or generally we square it, because then the derivative is very simple. And we basically, you know, apply multivariate calculus to minimize the prediction error, and adjust all these synaptic weights, so that the error is is minimized and
step by step, the neural, the neurons start learning. That's a very famous algorithm called the back propagation algorithm.
Now the problem with that is that there is no supervisor in the brain. Who is giving us the ground truth data, who is saying, this is a cat, this is a dog, this is a face.
There isn't anyone, even if you're told as a child, you know, this is a cat two or three times, that's simply not enough data to kind of do supervised learning.
So somehow the brain must be doing this in an unsupervised way. So how might we be doing that? Well, there's only one thing we know. Okay, we don't know the ground truth. The only one thing we know is the data itself.
And so if we have to guess, if we only know the data itself and we're trying to guess a feature vector, in other words, what are the features of my experience, dogs, cats, faces, edges, you know, I mean, whatever features there are,
what we can do is a brilliant strategy, which is to try to take a get try to learn this net feature network and then whatever we guess at, we then say, let me then assume that that's the feature in the world of the world.
Those are the causes of my sensory data. And let me generate a network going the other way to construct some predicted, some predicted data, let me simulate what I would be seeing if there was a cat and a dog and a face in this picture.
And that is, and then we can compare the predicted data that I the predict what I'm what I'm guessing are the predictions, and the actual sensory data and that sets up a prediction error.
And that is used to train the network. Now this kind of network is called, in general, an auto encoder. And there are three parts of the network. The first part is the recognition model or the feed forward model, which is which is trying to go from the data to a feature
to the code. This gray bar here is called the code. If any of you have heard of predictive coding. Well, the code that they're referring to and predictive coding is basically this line here, this series of neurons in the middle here.
And then the second part is the generative model. This is generating what I would be seeing. If this was the correct features of the world.
And that second part of the model is called a decoder as well. So the problem is that this kind of network is extremely difficult to train.
Because it's very nonlinear and it's very, you know, there are too many possibilities and data is ambiguous and difficult to decipher.
So these kind of networks, especially a type of network that called the variational auto encoders are trained using something called free energy as an objective function rather than just prediction error.
And free energy takes into account the prediction error that we talk talked about. Plus, it takes into account priors, prior probability around what you would expect this feature vector to be.
All right.
And that gives us a principled way to train the network.
Okay.
So this kind of network that we just showed in machine learning, we can take inspiration from that inspiration last week I was with all week with a whole lot of scientists including Carl Friston who is really one of the founders of the free energy principle.
In fact, the kind of key architect, and he actually warned us about kind of trying to use machine learning analogies. However, he felt that this particular analogy is a very good one.
And so I commend it to you to really think about how the brain is working.
So we're using this model as a kind of idea of how the brain is working, except that in the brain, what happens is that this top network, this green network, this generative model actually folds back onto the feed forward network.
So, I don't know if this animation is coming through clearly, but that was supposed to illustrate the network folding back onto itself.
And the feed forward network, the blue network kind of goes up.
And in fact, what actually travels up, according to the predictive processing framework is not the data itself but simply the prediction errors.
Simply these prediction errors travel up and what's actually happening is that this generative model is taking a guess from the latent layer the code and the feature vector and generating what I would be seeing.
And the only thing that's traveling up is the prediction errors compared to this generative model.
So this is the fundamental idea of what is going on in predictive processing, except that this is probably happening in a hierarchical way.
We have a number of these kind of models that are going on organized in sort of layers.
And then what happens is that these priors are actually arising from higher level models in the brain.
And prediction errors actually go up to these high level models and priors for this higher level model are coming from even higher level models in the brain.
And so they could be maybe, you know, who knows how many layers in this, you know, perhaps half a dozen layers in this kind of hierarchical predictive processing framework.
Okay.
So, let's look at specifically how the variational algorithm is supposed to work now.
As we said, we receive, you know, there are certain features in the world that we're assuming are generating our sensory data, the sensory data, these features in the world are generating our sensory data.
And we cannot solve Bayes' theorem correctly. So what we have to do is we have to form an inference, we have to make a guess at an approximate posterior.
And in functional form, often, you know, in these kind of models, they're often assumed to be Gaussian, for example, you know, multi-dimensional Gaussian with certain parameters.
And that is instantiated in the example that I gave, it's operationalized in the feedforward network that we talked about, the recognition model, the blue network at the bottom of the autoencoder.
And from this approximate posterior, we then have an estimate of the code, this red bar here, which is the features, an estimate of the features in the world, that there's a tree, that there's an agent.
And from there, we then create, we have a generative model, which in the brain, of course, feeds back on itself.
And the output of the generative model is a, is a, the sensory data that I would be, my expectations of what I would be seeing in the world.
In this case, I don't know if you can see this, this sort of little tree that sits on top of the generative model.
And from there, we then are in a position to compare the predicted data, you dash, and the actual data.
And we get a prediction error.
And that is at the heart of trying to train this model.
But as I said before, there's one more factor that goes into the free energy, and that is the deviation from my prior expectations, how different are these features from my prior expectation.
So for example, if it, if just using the prediction, you know, if I thought this was a alien spacecraft that it landed, that would have a lot kind of a low prior probability.
And so that would mean that these causes are kind of far away from my prior. And that would mean that I would be penalized for thinking it's a alien spacecraft.
And using this free energy, which combines both the prediction error and this kale divergence, which is essentially the distance or some sort of inverted commas distance, it's not actually a metric from my prior expectations.
I can then train this, this recognition model and the generator model itself to minimize free energy, and I will hopefully slowly, slowly get a convergence between this approximate posterior and the real posterior, you know, really accurate image of what's going on.
And that in a nutshell is the variational algorithm to solve approximate Bayesian inference. Now, before we move on.
Again, show of hands I just I know you're sort of a mathematical crew but how many people are familiar with relative entropy kale divergence can someone can you have a show of hands please.
I can see maybe 23 okay fine. So this is this is really important and and since I understand you're supposed to be like understanding the stuff in the technical basis so let me let me just go through this because this is kind of important, it's very easy.
I'm not going to. But, but this, this basic idea is that the kale divergence between two probability distributions is essentially measuring something like a distance that you know how far away are these distributions how different are these distributions.
And the particular form that that that this functional form has is is you basically compute.
So let's take this the first probability distribution P log of P over Q. Okay, and so these are the two distributions you trying to compare. But what it really means the take home message that I want you to say is what it really means is how much information, how much information.
I just thought that a treasure was buried on a straight line, let's say a road, going between zero and one. Okay, and I have no information that was my, my kind of probability distribution is Q of X I don't know where the treasure is buried it could be anywhere along this line.
And, and then someone offers me a kind of a more accurate map saying actually the treasures buried between 0.87825 0.825 and one right here P of X. This is a new information that I get a more accurate probability distribution.
How much information have I got how many bits do I get from going from Q of X to P of X what is the informational distance and that informational distance is roughly speaking how many yes no questions.
I have how many bits of information how many yes no questions I have to go from Q of X to P of X and, you know, it's kind of easy. I could say, is it on the left of Q of X or the right of Q of X that's question one.
So it's on the right. Okay, if it's on the right is it on the first half or the second half it's on the second. Okay, that's the second question is it then I say, is it on the first half of the second half of that and it's on the second half so it's P of X so three questions.
So in fact, if I compute this, this sum and let's just do it like an integral because it's the same thing.
What I see is that I can actually compute that it's pretty easy integral is like this this is eight, and then that's log of eight P of X is height of eight because the area under the curve is one and Q of X is one and times one you know the area on the curve.
If I compute that I get exactly three bits because this this this integral limits give gives me one eighth. Okay, so anyway, just wanted to explain that that it's a very simple calculation is roughly speaking the informational distance.
So you have to worry about using the word distance and metric if you want a real metric, you have to use something called the Fisher information distance, you integrate this KL divergence, and then you get, you know, you get into the kind of wonderful informational geometry that I'm
sure Fernando is going to be talking to you about.
So,
as I said this is not a metric because it's not symmetric but let's not worry about that. Now, let's get into the real heart of it because you're here to learn about the free energy principle.
And I'm only going through this, not to kind of do a shock and all but actually to say, in fact how simple this is you will see as you go through your, you know as you go through your careers in this whole area that
are bamboozled by all the mathematics around the free energy principle and actually it's relatively easy to do, but it's couched in very technical answer calculus. Okay, and it's kind of difficult to unravel.
So, I hope you will later on come back and say oh well that was a beautiful explanation that you saw here in Egypt.
So the basic idea is, is to solve the fundamental problem that we talked about what are the causes of my sensory data.
And, and we're going to use basis theorem, but as we just saw we cannot use basis serum directly because it's computational explosive. So we're going to use a variational algorithm.
We're going to approximate the actual posterior that we're trying to learn with, we're going to approximate it with this q of this q distribution which is, you know, for example could be a simple form like a Gaussian with some parameters,
and we are going to try to minimize the distance inverted commas, the technically the KL divergence between q and P. So, we're going to minimize the call back libel divergence between the q distribution and the true posterior p.
By changing the parameters of the underlying district q distribution. Okay, so what does that mean.
Does this come across this this equation, can everyone see it. Yeah. Yeah. Okay, good. So this says, please minimize by changing these parameters of the q distribution, the KL divergence between P and q.
All right, how do we do that. Well, I showed you the functional form of what a KL divergence looks like this is just the definition of it. All right, that sounds straightforward enough and we just sum over all the categories V all the particular features it could be in this.
Now, you'll immediately recognize that when I sum with a probability in front that's basically an expectation value right. So I can basically rewrite that as the expectation of this log of q over P distribution.
So here's the expectation under the q measure.
This is a measure probability measure under the q probability measure it's this distance now of course I just put a minus sign instead of dividing because that's just a property of logs. Okay, you can separate out the logs in this way very simple.
And then we're just going to use basis theorem because what we recognize is look, this is the probability of the features given my data.
And that is here on the side that is just the posterior. Well, if I take logs of this posterior here, then I get a log in front on this side.
And of course, when I multiply things the logs add, and when I divide the logs, you subtract so essentially this here which is the postive the log the log of the posterior simply becomes this which in these, these are just the terms the likelihood of the data.
So the term as this term here, this is the prior the log of the prior, and this is the log of the probability of the data that normalizing term.
Okay, so this is just applying basis theorem.
And then we just rewrite this in very simple terms we simply take the prior and put it in with the first term, and then we separate out the other two terms.
Okay. And what we then have a simple rewriting is we recognize that the first term here is nothing but another KL divergence of this form, except that what we have is a KL divergence between the approximate posterior.
This, this, this, this, this Q distribution and the prior probability of V. Okay. And then these terms here are simply the expected value under the Q measure of the log probability of the sensory data of the likelihood of the data and then this term here.
This term here I can take out of the, by the way, I could take out of the expectation operator because it doesn't depend on Q at all right so there's you know there's nothing stochastic here so I can just take it out.
And then, finally, so we can notice that it's a constant.
Now, these first two terms are the free energy.
Okay. This first term here, the KL divergence and then the second term, the, the expected surprise I'm going to come on to see why that's called surprise. So this term here is roughly speaking, how surprising is the sensory data what is the expected
surprise of this sense of this data, given that I'm assuming that given this feature, given that I think it's a tree, how surprising is the sensory data. So, if you if you look it's a negative log, if you remember logs go from minus infinity to zero so for
this sign, it goes from plus infinity all the way down to, to zero. And if you have, if the, if it's very, very surprising, if it's not very surprising at all. In other words, this is precisely the kind of sensory data that I would expect, given that it's a tree.
It's not very surprising. It's got a, it's got a low, it's got a high probability of being this kind of sensory data, given that it's a tree, then it's not very surprising. If on, on the other hand, it's like, really unsurprising, you know, I'm seeing something that is more like
eggs and crocodiles, then it's very surprising data and so this term would be highly surprising. So the first term in this term in the free energy is surprise, how surprising is my sensory data. In other words, how good a fit is this explanation for the data.
The second term in this, the first leading term here is the KL divergence between the, between the, the, the posterior that I've learned and the prior. So take a look at this furry animal here.
It could be actually, let's suppose that it's equally likely to be a dog, a cute little furry dog with blonde hair or a polar bear. Okay. In other words, let's suppose that the surprise of the sensory data, given that it's a polar bear is equal to the surprise that it's, that it's a dog. In other words, they, in other words, the data fits both the dog model and the polar bear model.
But polar bears, especially walking around in Egypt are very unlikely. The prior probability of seeing a polar bear in Egypt is very low. And so this term would be a very high distance between the posterior that is a polar bear and, and the prior that's better.
So you'd be very much penalized. Remember, we're trying to minimize free energy here. Okay. So these are the two terms of free energy. Now, I'm sure you're probably asking and no one ever explains in, in, in neuroscience, why is this called free energy?
I mean, I, I thought the free energy was something to do with entropy and energy and like stuff like that, you know, jewels, you know, this is information, this is not jewels.
It's called the free energy because you can rewrite this term. I won't explain how this is very easy to rewrite this from, from this line here into a slightly different format and you can check it for yourself that it's possible to do this.
And this happens to be this way of rewriting this free energy term here.
It happens to be equal to exact, I mean, isomorphic with the Helmholtz free energy in thermodynamics. Okay. And the reason for that is because this term here is the average energy.
Remember, you, when you sum up in multiply by probability, this, this thing here, which, which is going to turn out to be the energy of the explanation.
That's the average energy. And this is the entropy minus log P log P some is the entropy. Okay. So here's the end. So energy minus entropy or average energy minus entropy, as you might recognize is the typical form of free energy.
And, and then you might say, well, why on earth is this an energy? This is an energy if you, you know, for those of you, remember, you know, done physics, the Boltzmann distribution in thermodynamics is that the probability is related to the energy by an exponential term.
So if I take log of both sides, I get rid of this exponential terms and then the minus sign comes over and hey presto minus log P. And that is equal to the energy. So that's called the energy of the explanation, the energy of the explanation that this is a tree.
So that is the free energy. So there you have it. And now you can forget all about it because you've seen it once in your life. Once you've seen it in your life has been demystified and you can say it wasn't magic after all.
So now I need to move on a little bit faster. So I'm actually just going to, to, to with through some of these bits, but, but roughly seeing that what this is basically saying is that is that there's a very nice approximation and of this free energy principle.
And the takeaway message is that the posterior belief, the posterior that you're forming is roughly speaking the weighted average of your priors and your sensory evidence, your priors and the sensory evidence coming from the likelihood and the, the weights of that weighted
average, turn out to be the precision of the distribution. In other words, how if your precision is inverse variance. In other words, if your prior is particularly sharply focused, you know, low standard deviation, then it's going to be highly weighted within the posterior.
Or if you get very accurate data, okay, you're going to highly weighted within your posterior. Okay.
Right. So now, so we've got this basic hierarchical model and these precision that we talked about how sharply influenced are going to influence the in the top down ratings top down waiting of the priors compared to the sensory data.
In the brain, we've got a kind of cortical hierarchy, and that is the hierarchy that I'm talking about in the, in the hierarchical predictive processing framework is precisely going to be that it's a homolog of the hierarchy in the cortex.
And this is a very nice machine learning exercise that was done to try to figure out the cortical hierarchy and, you know, very similar, similar things to what one might imagine, you know, low level sensory percepts are low level features, mid level features like motion and rotation and faces.
And then at the right at the top of the cortical hierarchy are language and concepts and reasoning, as you might expect. And of course, this hierarchy is probably best not represented as a, as a pyramid structure, but in fact from a hierarchical from a predictive
processing point of view. It's really seen as these, these different channels of streams of hierarchical predictive processing models, until they start integrating together, perhaps in some sort of a global workspace or something that you might learn about
because no one has put together predictive processing and global workspace theory as yet. People have tried to attempt to do that, but I'm just kind of warming you up and making you salivate the say that I'm you know maybe these things could fit together in some way.
We have these predictive processing models of vision, audition, the motor system actually goes outwards all faction, somatosensory interception, gestation, the whole visceral proprioception, and they kind of come together.
Now, actually, we're probably not going to have time to go through the phenomenal self model but it's super interesting that in fact, the self itself is nothing but an internal model within a predictive processing hierarchy, hierarchical framework.
And so the basic upshot is that our conscious experience, to the extent that we're conscious of the generative model will be the contents of our general model. That is the take home message at what will become a kind of predictive processing theory of consciousness as yet it hasn't been worked out but
if it is worked out, the computational neuro phenomenology assumption is that the contents of our generative model is the contents of our conscious experience to the extent that we're conscious of it.
This is not completely worked out because we've got to figure out what what are we conscious of. But in other words, you know, if I'm looking out at the tree, I'm going to have like a represent, you know, you know, in my generative model this, this this construction of a tree and a, and an avatar, you
know, self avatar and some thoughts and these thoughts could be visual thoughts and and auditory thoughts blah blah blah and visceral sensations and proprioception and smells and sensations of the breath at my nose and my the sense of my heartbeat and sounds and emotions.
Like, you know, I'm constructing my emotions in a sense like as Lisa from embarrasses kind of described in this way.
And all these contents of the generative model aren't just sitting on top of each other until unless you're kind of doing psychedelics or in a meditative state they're organized into a phenomenal self model so what does that look like.
So they are reorganized. I don't know if that that animation came through okay I'll do it again. They are organized into a subject object duality. So everything in this blue area here is within a subject.
There's internal interception. These are extra reception.
There's a certain perspective. There's a subject object relationship. There's a notion of agency there's multi sensory integration.
There's a sense of mindness, a kind of a sense of a Cartesian theater. These are all constructions.
I'm not really aware of the fact that there is this phenomenal self model and that's called phenomenal transparency, according to the philosopher Thomas Merzinger.
Before I finish just to finish up because the real fruit of this whole free energy principle and predictive pressing framework is something absolutely amazing, which is that we can put action into the same framework to minimize free energy.
And that is called active inference and to be honest, I think that that's a real that's though that's why this framework is so amazing because it kind of unifies action and perception in a single framework.
And Carl Friston came up with with this kind of idea in the, maybe around 2008 to nine.
And, and the way it works is that essentially the way you can put it is that the goal states, let's say I'm trying to reach over to grab an apple.
The apple is here in green, if you can see and I'm currently here. Well, if the way it sets up is it puts that getting the apple is in my prior I have a prior expectation to get the apple.
That's kind of my reward goal states are embedded into the price. And then, immediately, there's a prediction error between where my finger is at the moment, my hand is at the moment, and where I wanted to be getting the apple right.
So this prediction error will be resolved, not by changing my perception, but by moving my muscles. In other words, doing action.
And so inference and action are resolved together. And there is a movement trajectory until the minimum prediction error is minimized and I got the apple. Absolutely astounding.
So the way, okay, I won't go through this but this is this is what you're going to finally understand later on. And I just don't want you to be mesmerized by this but this is the kind of final active inference framework with action in it.
And it's typically instantiated in a partially observed Markov decision making process under active inference. So the way this works is that what we saw before is that I'm trying to predict my my sensory data these oh is the outcomes, the outcomes of my kind of the you know the the world shows me my sensory data these
this is this is the sensory data, and I'm trying to model the world and that's these states. These are internal models of the world and these matrices, which, which probability transition matrices don't worry about the details about these, you know how these are formed.
And what I need to do when I bring action in is that I need to have some sort of preferences some sort of goal stage which are called preferred preferred observations this is this, this is a probability distribution over my preferred outcomes.
And that is gives me my my expected free energy and the expected free energy and then I have my prize around my habits, your habitual actions, and a precision waiting over this expected free energy and that gives me my policies of the world, and the whole thing is minimizing expected free energy.
So, that is a, that's a quick whirlwind tour on on this framework.
We can go to questions now.
Okay.
Hi, can you hear us.
Yes, we can.
Right. Okay, good.
Yeah.
Okay, cool. So who's good questions. Okay, I saw you first.
There you go.
Hello.
Thank you for the talk.
The machine learning analogy that you said reflect it like seems to us suggests that you're thinking of kind of enter and learning happening in the brain.
Like, like, some, like other aspects of the
free energy principles.
Maybe yours. Can you hold it?
Can you hear me well.
It goes rather as if you're talking underwater.
I'm sorry a little bit.
Okay.
Okay.
The machine learning analogy you said reflect seem to suggest that you're saying of like enter and learning we happening in the brain.
Like, from like other aspects of like free energy principle that I have read about it's
there's also the
there's also the aspect of hierarchical self organization.
I was wanting to verify if you could come on on if that can be brought together in some sense, or if that's like
or does a contradictory views.
Okay. Yeah, look, I think
I think you have
I think you have put your finger on a very important issue which is that, you know, I don't think we have the grant. Well, first of all, take the machine learning analogy as simply pedagogical, pedagogical, right.
This is really to introduce and try to make sense of the free energy principle.
How it's being instantiating in the brain is something we really don't have good models and data over yet.
So, I think it's a great question and I think you should think about working on that.
Thank you.
Other questions.
I think at this first.
I think you can ask a question.
I should know. Good to see here.
I didn't know you're out there.
Yeah.
Yeah.
Okay, great.
So,
predictive processing is not in itself a theory of consciousness, but clearly it has implications for consciousness.
I noticed that you framed most of the discussion in terms of perception, rather than consciousness.
It seems like free energy principle in general comments a lot more explicitly on that rather than consciousness.
Does Carl or anybody else who is working on the free energy principle explicitly distinguish between perception and consciousness? What is the difference?
Right.
They be treated as the same.
Okay, so, as I mentioned, the whole of last week, I was at a workshop with Carl Friston.
Anil Sirth, Jacob Hawi, Thomas Metzing, you know, the whole leaders of this whole area on computational neuro phenomenology.
And that what you're asking was precisely what we were debating back and forth, you know, and it was very, it was very interesting.
So, I would say that.
So, you know, we are right at the beginning of saying, look, we have our conscious experience, and we know that the, and we have the neural going on.
And we know that the brain must be solving some sort of Bayesian inference problem. So it's a good way of approximating what's going on in the brain is by modeling it as kind of a predictive processing framework.
And given that we are trying to, given that our phenomenal experience is, you know, if you take a functionalist perspective, our phenomenal experience is somehow being worked out in terms of neural going on.
You can then say that the predictive processing model is going to be a homologue of what is going on in the brain but also what is going on in phenomenology so we're precisely trying to draw, you're absolutely right to ask this question.
But we are explicitly say trying to draw an analogy to say yes indeed. This is a way that the project of of computational neuro phenomenology is precisely using active inference frameworks for example or other computational models to model our phenomenal experience.
And, and if you want to drill down further, if you're assuming some sort of active inference framework, then I would even say that that parts of the posterior or the posterior is the homologue of what is our conscious experience and our precision weightings are the homologue of our attention mechanism.
In other words, the sort of, you know, if I, if I really pay attention to this, this bottle of water, then in my posterior, remember the precision weighting is the, is the weight that's used to calculate the weighted average in my posterior.
So, you know, this is strongly weighted into my, into my posterior distribution. In other words, my, my consciousness is deeply in dominated by this bottle. Okay.
So, great question. You're right at the cutting edge. We don't have a theory of consciousness which is based around predictive processing. However, what I'm trying to say to you is that it's starting to look very evocative.
Thank you so much.
Any other questions.
Yep.
So, actually, my question is exactly pretty similar to Ken's question. So, actually, I read from a lot of places like, oh, people always to predict processing active inference is not really a theory of consciousness, it's just a framework.
I'm so confused. So, for example, I see a lot of adversarial adversarial collaboration is now trying to test the prediction of pretty the processing or active inference between these pretty PBT theories and other theories.
So, it's a framework or it's a theory. If it's just a framework, then it does not really review, for example, necessary review for example in the background masking why sometimes I see the stimuli and why the other times I do not.
Yeah, and, and, and if this, if it is a theory, then it is a theory, then it's no more framework, then it should give it explanation to.
So, for example, why backward masking binocular library works.
So, guys, you know, guys and girls what you're seeing here is how science is actually, you know, the, the sausage making.
There is a, you know, there are people.
Anil Seth and Jacob Howie have written a paper to say predictive processing is not a theory of consciousness. And then other people like Adam Safran a couple just a year, you know, go kind of create something like a really interesting theory of consciousness
integrated world model theory, which is essentially integrating aspects of predictive processing theory and combining it with the global workspace theory in a kind of sense that I alluded to about sharing of information at the center of the ring.
And actually also combining it with areas of the back of the brain in it terms, right. And so actually integrating it in and basically what you're seeing is the kind of maneuvering in the field where people think that we're kind of grappling towards a predictive
process of processing theory of consciousness. It's not yet clear. It's not like 20 years old like 23 years old like global workspace theory or it or something.
You know, it's right at the beginnings of this. I predict that we will have a, you know, a theory of consciousness based around predictive processing.
And also predict that people will say that's not a theory. Just like people say other theories in our theory, you know, and there'll be lots of argument and they'll be, you know, and I think that will be a richer process for thinking about consciousness, because actually I'm kind of partial to the view that, you know,
predictive processing theories, IIT theories, global workspace theories, higher order theories, and predictive processing theories are all bringing something important to the table.
And actually at ASSE, which is the main consciousness conference last year, there was a debate in that whole thing between these four theories, and there was supposed to be a fifth theory except that Anil Seth had long COVID so he couldn't come.
And, you know, there were five theories that were put up for debate. So, good question.
But, you know, see the messy process of science in front of you.
I'm just really confused. It's like, for example, even Anil Seth, like at the Wayne Palm paper, he just says one thing. Oh, it's just a framework, not a theory. But at the same time, you also see his reviews in natural reviews, neuroscience lately, like a theory of consciousness is still listed as one of the theory, and he is a participant.
And if you know that it's not a theory, why do you still participate in that adversarial collaboration in testing it as a theory of consciousness? I just don't get it.
It's wonderful. It's wonderful and nuanced. And in that nuance we might get progress.
Unfortunately, at the cutting edge, science is always confusing. You know, literally everyone at the conference last week was confused, and that was a sign of progress.
Thank you.
Anymore?
One more?
Anyone?
And, you know, I think the tutors should feel free to ask questions as well.
Hi. Thank you for your talk.
I was just wondering, because actually the free energy principle also tries to explain more than just the brain, right?
So it's more or less a theory of everything. So how is your take on that? Like what can predictive processing tell us more than about the brain?
Okay, great. Yeah, that's a very good point. And I didn't bring that out, but thank you for asking a question on that.
So, Coffrist often says that there are two ways into the free energy principle. One is the low road, and one is the high road. And I took the low road, which is through inference.
And then there's another road, which is called the high road, which is based around the concept of Markov blankets.
And Markov blankets are essentially, think of them like informational boundaries, like, for example, a cell membrane for a cell, that all information has to go through the cell membrane.
And so if you look at the informational states of the cell membrane, you will get that all the information inside will that has come from outside will have to have been represented on the on the blanket states on the back on the Markov blanket states.
Now, it's essentially saying that if you have a Markov blanket, then you are going to be instantiating a model of the external data.
And this is very powerful. And this, you know, the sort of principled information theoretic arguments that have been proofs that have been trying to be given for this. And this means that everywhere you see a Markov blanket, not just in the brain, but in the body, perhaps in the organs, perhaps in the cells,
perhaps ecosystems, perhaps the planet Earth, you know, I mean, even speech, I mean, even, you know, within in the in the evolution tree and species and things like that, you know, you're going to see this kind of what is called in the in the area, Bayesian mechanics.
And I think it's a wonderful idea. It's a very powerful idea. You know, and it's very evocative. So yeah, just that that's a brief glimpse of how that is just focus on the brain.
Yeah, there is, is there a way to detect a Markov blanket, like when you see something like, okay, this is a Markov blanket, and this is not one. Or is that just like you could basically see where depending on your definition or
But that's, that's a very good question. To be honest, I think most Markov blankets are only approximate and that's an issue in theory, but, but you basically have to see.
A canonical example would be some sort of system that is auto poetic and self organizing and continuing through time. So if a system exists. So this is how the, I'm sketching a little bit how the proof works.
If a system exists, the nature of existence is that it's a system for itself, which means that there is a Markov blanket. So just to call it that there exists a system inside of saying that we can define a system self contained kind of system
that exists, which means that the fact that itself contained in this way means that it's got a Markov blankets is a great question, because, because, you know, this stuff is very powerful. I mean, there was a paper written a couple of years ago which was saying a general
theory of everything, every space thing. And, like our first like the thing was things that exist. I mean, it was really deep stuff. And anything that exists exists for itself has a Markov blanket, because that's what it means that it persists through time.
One of the just just just very quickly because it's kind of a cool thing. If it persists through time as a system. One of the things that it must be doing is also in practice, not being broken down by entropy surviving against the second the threat of the
second law, and this is thermodynamic entropy. And so a system has a model of its external environment in order and acts in such a way as to avoid the second law. So, this is a sort of interesting argument to say that things like homeostasis and action are are
the basis of instantiating active inference. In order to, or as a result, they, it means the system continues to exist which is resist the second law thermodynamics.
Interesting. Thank you.
So now that we have applied free energy to every space thing.
I think we can finish here. Thank you very much for the amazing talk.
And yep.
Thank you so much. Thank you and good luck for the rest of this workshop.
