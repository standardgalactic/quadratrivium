My name is Eram Emesh Yalzajer and I'm a senior research specialist at ETTR for the Division
of Vehicle Driver and Systems Safety here at Virginia Tech. We appreciate you joining the webinar
on the active inference models of perception and control. You have all come into the meeting muted
and with your cameras disabled. If you have questions during the presentation, please use
the chat box at the bottom of your Zoom window. We will address those questions either during
or during the Q&A session at the end. You would rather ask your questions yourself.
You are most welcome to do that. You can either raise your hand and let me know
or you can wait, as I said, for the Q&A session at the end.
Captions will be on for your convenience. At this time, I would like to introduce our
presenter today. Dr. Alfredo Garcia received his Bachelor of Science in Electrical Engineering
from the University of Las Vegas in Colombia in 1991. DEA in Automotive and Informative
Investorial from the University of Los Angeles in two weeks, France in 1992 and PhD in Industrial
and Operations Engineering from the University of Michigan in 1997. From 1998 to 2000, he served
as a commissioner in the Colombian Energy Regulatory Commission and from 2001 to 2017,
he was a member of the faculty at the University of Virginia and the University of Florida.
He currently holds the Mike and Sugar Bonds Endowed Professorship for Industrial and Systems
Engineering Departments at Texas A&M University. During 2016 to 2017, he served as a program
manager for the Multi-Agent Network Control Program with the Army Research Office at Research
Triangle Park. His research interests include game theory and dynamic optimization with applications
of electricity and communication networks. At this time, I'm going to turn it over to him
and we are good. All right. Okay. Thank you, Adam. So thank you for attending this talk. As Adam
indicated, I'm not quite in transportation, but this whole project started out of a collaboration
with Anthony McDonald, who's now at Wisconsin, and he was the PI in this project.
And this is the work of Rand Way, who's a PhD student and recently graduated and is now
with this AI company versus. And we have also collaborated with Gustav Marcula from Leeds
University and Johann Engstrom and Matthew O'Kelly from Waymo. So I like to start from the end,
if you will, because I like to motivate the talk, which is going to be a little bit technical,
but I like to show you what is the end product of a lot of the work that we did.
So I like to call your attention to the two columns on the left. So what we have here,
the first column is the data for tasks following in this data set in the interaction data set.
So we selected a subset of the interaction data set for this task following
maneuver. And what you see here, for instance, is the different visualizations of the data.
So you have a relative distance to the car ahead, I'm sorry, relative velocity
and relative distance. And in the color coding is acceleration. So the the redder
or the higher the acceleration and so on and so forth. This is another part of the data. This is
looming. So the rate of closing the percept the perceptual rate of closing to the vehicle ahead
of you. And this is the inverse of that. So it's one over seconds. And again, versus relative
distance. And again, the color coding has to do with acceleration. And finally, the same
variable in the y axis and in the x axis, we have the relative velocity with respect to the car
ahead. And as I said, this is the the color coding. So negative acceleration slamming on the brakes
is red, hot red. And then slamming the accelerator pedal is deep green, so on and so forth.
So this is the data. And then in the column right next to it is what our model at the end of all
these comments and goings, we built a model. And then now we simulate the model. And this gives
us a sense of how good is the the model at the end of the day. As you can see, it captures some of
the of the trends. It is unable to capture this behavior that is kind of not as well represented
in the data set. But for the most part, this visual sanity check tells us that this model
that we're about to describe does a fairly decent job at fitting the data. So what is the model?
And this is these are the two columns on the right. So the model, this is the main
point of our work is a model that
is structured into two parts. It has a one part that has to do with perception.
And that's this column. As you can see, there is a color coding of the data. So what we have is a
finite set of states that are associated with a collection of observations. So for instance,
these observations here in light blue have to do with values of relative velocity that are high
and values of relative distance that are low. So this is a light blue state compared to a deep
blue state, which is a one in which there are very low negative relative velocity values and
very low relative distance value. So I will explain this in a moment, but what I'm trying to
start out with is giving you a sense of what the structure of the model is. So there is first
a part of the model that is dealing with perception, how the human agent
takes the external sensory input and in a sense constructs a simplified representation
by means of these hidden states. Notice that it has a flavor of categorical perception because
a whole collection of sensory observations that are real numbers are kind of mapped
into a color. So it's in a sense capturing a notion of categorical perception.
And on the right hand side, we have a model of preferences. So why are people doing what they're
doing? In essence, what this tries to explain. And the way to do it is to use a model of preferences.
What is it that people like about when they're doing this task following maneuver?
And so the colors are in the deep purple to extremely this light. So it's going to be in terms
of a preferred distribution of observations. So what we're going to come up after the
estimation exercise is a distribution of what the agents would like to see in the world.
What is the distribution of the preferred observations? And so that's why it's a log
of the preference distribution. And as you can see, these values here,
where the relative velocity is high and the relative distance is low, these are values that
are not like a very low log of the probability because these are dangerous, potentially dangerous
situations. Instead, the values that have the highest preferences are values in which the
relative velocity is close to zero. So hence the light color here. And also a distance that varies
according to a certain range. You can also see that there's kind of an asymmetry between the
preferences for this type of observations as opposed to this type of observations. The color,
the intensity of the color is not the same. So in a sense, what we have built is a model of the
policy that these agents are following. This is a population model by means of constructing a
perception model in a model of preferences. And this is what I would like to describe in a moment.
So this is the outline for the talk. What are the basis of modeling a perception and control?
How do we learn a model once we posit a model? How do we incorporate this notion of active
inference, which is a fairly relatively new but very popular paradigm from cognitive science.
And then the details of the application to the car following test.
Okay, so modeling perception and control. So this is what I was referring to earlier as to
what do I mean by saying when I say that the agent takes a high-dimensional observations
and has a low-dimensional representation of that. So these are the things in green are the things
that we observe in gray. We also observe the maneuvers that they are doing, the accelerations.
And the agent's internal representation is this bubble here. So this bubble here is
essentially composed of there is a hidden state S. And there is also a belief about
what is the distribution of the states? What is it? How likely is it that you are in a given state
at a given point in time? This is this sequence of Bs. So this is the subjective representation
of the world, of the environment by the agent. This is all these arcs have to do with parts of
this model. For instance, this arc here, these arcs, are essentially associated to a generative
model of observations, meaning that the agent expects a certain type of distribution of the
observations when the agent believes the state of the system is a certain value S.T. So this is
these links here. In a similar manner, there are these beliefs about the state. So this is the
belief that the current state, what is the distribution of the current state? And we're
going to use a Bayesian framework. So we're going to use the beliefs as the conditional
probability given the history of what has transpired after up to time T, what is the
conditional probability of the hidden state is S. And this is our belief distribution. And finally,
there is also a model of the state dynamics. So the agent also has a model of how the
state of the world would change when undertaking a certain course of action. And this is
modeled by a transition probability, a Markovian model at the end of the day. And so our model of
how the control actually exercises or has an effect in the state of the world is given by this
Markov transition probabilities. So this is what people refer to, for those of you who may not be
familiar, a partially observable Markov decision process. This is a POMDP, which is a model that
is used in control, artificial intelligence. But in this case, we're using it as a representation
of a human agent's perception and action loop. Okay, so what are the ingredients of this POMDP
model? We have a history of what has transpired in the form of observations and actions. And then
we are positing the following rationale for what the agent is doing. The agent has some
way of determining which are good outcomes or which are good states to be in and which are bad
states. And this is the reward function R of ST over AT. So this is just a representation of the
preferences of the agent in terms of the hidden states and the actual actions
that the agent can undertake. And if we were positing this problem in terms of an unbounded
horizon, that's why this sum is unbounded, we would have to take all these rewards and aggregate
them into a single number. And this is why we use this discounting factor. But as we will
see in a moment, we didn't have to use that in the task following, because we're using a rolling
or receding horizon paradigm. Now, there is this element that may not be familiar with some of
you, which is there is an information processing cost. In other words, these agents have
opportunity costs for how engaged they are in the task. So marshaling resources to
undertake the task, mental resources has an opportunity cost. And so if you are busy doing
something else like you shouldn't be, like texting or talking to somebody else, this opportunity
cost, this C, is a measure of what an information processing cost. So how focused on the task or
how costly it is to fully engage in the task. Okay, so these are the primitives of the model.
And as I said before, we're going to be taking a Bayesian approach to modeling the
way the agent sees the world. And I will need to kind of formalize the elements of the model
to more definitions. One is the specter reward. And notice this expectation is with respect to
subjective beliefs. So this is what I believe my reward would be if I were to implement action AT
when my beliefs about the hidden state of the world is B of T. And then this is also what I
expect the observations will be. So in taking action AT, when I believe the state of the world
distribution is BT, this sigma is a distribution over observation. So it's telling me what is it
that you expect the observations to look like when you do this control action. And so on the basis of
these two things, for those of you who are familiar with the dynamic programming formulation, this
problem can be expressed in a recursive fashion. So we can use this device called the value function.
So it's a function of the current set of beliefs. And I'm going to use this start to
denote that this is the optimal value function. And it happens to have this recursive structure.
So you do what is the value of what you are supposed to do best is the maximum over all the
possible policies that you can consider at this point. And they will give you this immediate
expected reward plus the future reward, which is a function of which observations you're likely to see.
And the resulting beliefs that you are going to construct on the basis of those observations that
you are likely to observe. So this is a very nice mathematical characterization that allows us to
do a lot of computation. Okay, now we are going to work with the following specification for the
information processing cost. So information processing cost is going to turn out to be
something like a distance, right? This KL divergence is not quite a metric, it's not quite a
distance, but it's close to a distance between two distributions. And so what we have is that
the agent has a default policy. So it has learned to do this task after training. So it has a
default policy. But of course, if fully focused on the task, that agent may actually do even better.
So we're going to, by using this specification, what we're saying is that the finer the agent
is in undertaking the task, the costlier it is for the agent. So the agent can do little effort
and just revert to the default way in which the agent does things. Like this is how I learned
by, from memory, and I just will do things in a mechanic way. But that would be when this cost is
high, then maybe you want to revert to the simpler way, the default control policy. But if you want
to really dedicate your resources, your main resources to identifying which is the best course
of action, then you will identify a policy pie that would be different from this default. And
the measure of this distance between these two distributions is a proxy for information processing
cost. So the nice thing about this specification is that it gives us a close form expression
for the optimal policy. It's in the form of a softmax policy. This simply says that
the likelihood that I take action A when the beliefs are B is going to be
this, this is proportional to how big is this value Q, right? And this value Q is the immediate
reward that I would receive by undertaking action A when in state B plus the future reward,
which is the expected value over all possible observations and the likelihood that I will
see this O prime. And then the resulting beliefs that I would be in at that point on, right?
So notice that this is kind of like a bias. This default policy serves in a sense to capture
the default way of doing things is already there in the background. And then depending
upon how high is this cost, you're going to improve upon it by your optimizing behavior.
Okay, so this is a model of both perception and control. This is the actual control policy,
which is determined by basically the two parts of our model are going to be this guy here,
which are the preferences, right? And this guy here, which is the perception, how is the agent
expecting things to change in the environment as a function of the action? So that's a lot of
a very short description of this POMDP model. Now we take the, we change hats and take on the
statistician hat. Well, we have data. And what we would like to see is which of these POMDP models
would be the best fit to our data. So this is the question of learning a model of perception
and control. So now the problem is I have data in the form of sequences of observations and
implement interactions. And I'm going to call that let's say tau. And so when I see, when we have
this interaction data set that is like a video of from above from a high up of the cars engaged in
the highway in a task that is car following, right? We are not dealing here with merging
or with exiting or entering the highway. These are other tasks. We're only dealing here with the
task associated with car following. And if we have this video from above that gives us the
relative velocities, accelerations, distance and all you name it, then what we have is the sequences
observations and implement interactions. And what we like to see is how do we go back
from the data to a model, right? That identifies the primitives of perception and control. And as I
said, the primitives are going to be in the form of the reward, the preferences, how good is an
action when a given set of beliefs is given. And what are the expectations about the way the world
is going to change as a function of the action undertaking, right? So in another word, this is
what I have in the next slide. So I have parameter theta one for my model of perception. So I need
to estimate theta one. And by that, I mean a Markovian transition kernel for the state,
the hidden state and observation probabilities for the actual observations. And then a model of
preferences, which is a simple reward function. So this is the model. And well, I like to choose
these values of theta, theta one and theta two, so that they are close to give me a high likelihood.
So the log likelihood will be essentially the probability that when I specify the
values of theta one and theta two, in this POMDP formulation, I have specified essentially
what is the optimal policy. And so by changing the parameters, I am playing with different
specifications of the optimal policy. And so this is the log likelihood of the data given a choice
of the parameters, which are, as I mentioned a moment ago, the parameters for
preferences, reward, and environment dynamics, the transition probability.
Okay, so this turns out to be, so now if we, let me stop here for a moment and say the following.
This is not an easy problem to solve because you can imagine that when an agent is executing
or planning a task, a control task, there are trade-offs over time that are being
considered. So how do you evaluate trade-offs over time? You have to have both a model of
rewards, but also a model of how is the environment going to change. So if you're trying to find a
model that rationalizes data, it could be that there might be multiple ways, multiple combinations
of reward and models of the world that are going to be consistent with the data. In other words,
this problem is not identifiable. In general, as it is, there might be many models that are
consistent with the same data set. That poses a big problem for us, right, because this is what
we would like to see is to go back from the data to a model. So to alleviate this non-identifiability,
we're going to assume, we're going to make an assumption that it's essentially,
this is the mathematical description, but in words, it says the following. The agent is
sufficiently good at the task. In other words, I'm going to have, remember that I parametrized
with Theta 1 everything that has to do with perception of the environment. And so by saying
that the prior on Theta 1 is increasing in the likelihood of the observations. So this is the
likelihood of observations. Remember that Delta is this way that we parametrized the distribution
of observables as a function of the current beliefs and the actions. And so if an agent is an expert,
this means that this agent has a fairly accurate model of the environment. And this parameter,
this is a hyperparameter, unfortunately, but we have to pay the price. Lambda gives us a way
to control how accurate is the model of the environment that this agent has. And so we'll
start with this assumption and then we can put together now what is the log of the posterior
distribution. So since what interest is in the posterior, given the data, what is the posterior
distribution, then we find that under this assumption, the posterior distribution will be
proportional to the log likelihood of the actions that are in the data set. Plus this log likelihood
of observations which are multiplied by this accuracy hyperparameter that essentially allows us to
describe how accurate we believe the model of the environment that the agent has is.
And so at the end of the day, this is our objective. Our objective will be to try to find the value of
Delta that maximizes this posterior distribution. And this is our, it all boils down, the estimation
problem boils down to solving this bi-level optimization problem. So why is it bi-level?
Because okay, we have an objective which is to find the maximum posteriori value of Delta. So
we now have a sense of what is the posterior distribution of Delta and we would like to find
the maximum value of that posterior distribution, MAP estimate, that's the objective. But we would
like to do so, but all the while keeping in close sight the fact that this is a rational agent.
This is an agent that is behaving according to the tenets of this model which is maximizing
some rewards that are a function of preferences and it has a an information processing cost.
It's paying attention to a level to a certain degree. Okay, so this is the actual technique.
Right. Okay, so now let me move ahead and I'll tell you a little bit about active inference
because this is from up to this point what I have done is I started by positing the PMDP model
which is a general description of acting on their uncertainty on their partial observable states.
Then we consider the statistician problem which is okay, I have data, I want to see
which is the PMDP model that fits best the data. And now we would like to see if the state of the
art in cognitive science allows us to make a more appropriate choice or specification of this model
and this is where active inference comes into play. So let me do a quick overview of what I
think is active inference. So there's a recent textbook and this is I'm doing
market in IT press by Carl Freiston and his colleagues on active inference.
And so the idea of active inference is that unlike other paradigms of cognition which are
essentially posited on the premise that the brain is like a prediction machine. We're constantly
trying to predict what is going to happen next, right? That this inference is not just passive,
it's not just the world sending us, inundating us with a sensory input and trying to figure out
how is the world but also we are active in the world. We act in the world and we act in the world
so as to induce a distribution of observables that is according to our desires. So we want to
match what is perceived versus what is preferred. And this is why the term active because the brain
is not passive, the brain is also actively engaged in the world trying to change the
distribution of observables in a certain direction. So this is a typical plot that they use.
So we have this is the world, right, with observations and we are here with the human brain
but the brain but the human can also act upon the world. So to change the
the states of sensory data. And what is appealing about active inference is that these two things,
both this loop here and this loop can be seen as the two sides of the same coin.
And that coin is essentially free energy. So all the brain is doing is minimizing free energy.
Why do they mean by free energy is a measure of surprise. So for instance here when I'm trying
to build a model of what I think the world is like, I am trying to minimize surprise. I don't want to
believe in a world that then would be totally off with respect to incoming data. I want to minimize
surprise. And lo and behold a Bayesian brain would be one that would do a very good job at
not being surprised by the environment. But there is another side of the coin which is this
active part. So in this active part then the agent is acting so as to minimize surprise in the form of
I want to see observations that are in accordance or close to a certain preferred distribution
over the states of the world. So this is, I mean in the time I have, I have tried to explain
the general underpinnings of this very appealing theory. Now what we did at the end of the day,
it's a simplification, is we simply take a specification for reward in terms of negative
expected free energy. So there are two parts to this. There is a first part which is usually
referred to as pragmatic value. And so in a sense this is where the preferences of the world
are playing a role. What this is saying is I would like to make sure that what I believe about the
state of the world matches what is my preferred distribution about the state of the world. So
this first part can be seen as a pragmatic value as the term they use to refer to the fact that I
like the states of the world to be according to p tilde and I would like to make sure to minimize
this mismatch between what I believe, what my beliefs will be after I take this action
and the distribution of the world. But there is also a very important second component and this
they refer to as epistemic value. And this means in addition to pragmatic value I am also concerned
about being in the states of the world in which I'm likely to be surprised. There's a lot of variation
or a lot of entropy. So this term here is the entropy of the model of the world of the
observations that results after taking action at and landing on a state s t plus one. So in
addition to being close to what I like to be in, I also dislike being states for which I have
a very little certainty as to what are the observations that I should expect. And this second
term is called epistemic value. Sometimes the agent under these preferences will be justifying
taking an action that doesn't seem to be all that rational from the point of view of preferences
respect to states of the world. But that one action might mitigate or minimize the uncertainty
after the distribution on the states of the world. This is the nice balance between
you have a goal but you also have limited knowledge about the way the world works.
And so sometimes you might be better off by just simply staying put with a situation in which
your uncertainty about the state of the world is not as high rather than shooting for a presumably
high pragmatic value but with a lot of uncertainty. Okay, so this is the specification that we did
for the model. And now we go to the actual, I think this is what you guys are interested because
this is a transportation talk. And so we did a very simple, I mean, this is a simple task,
car following. Nonetheless, it has its own set of complexities. Okay, so we use the interaction
data set. And from that data set, we took, by the way, those who are interested in the details can
click here. This is the, we have a report on the archives with the details about this work.
Okay, so we take the interaction data set and we selected a number of trajectories
in terms of position, velocities and headings. And at a sampling frequency of 10 hertz. So these
are like videos of these cars that are following the task. So this is a picture of that. And there
are cars that are going in one direction. And there are cars that are going in another direction.
And these lanes that are going from east to west, west to east may differ in the, say, the traffic,
the average speed and things like that. So this is a good way of testing. I will see later on
of testing how much of a generalization capability has the model because we can train a model in one
sense and then use it in the other sense to see how good does it do. Okay, so let me start with
what are the alternatives. So we used the specification of the intelligent driver model,
which is an engineering model that specifies a certain number of parameters.
So it's a very transparent model because it's purely dictated by a system of equations.
And the dynamic equations are governed by a few parameters. And then all we're doing is playing
with the parameters to see which one fits the best of the data. Then we consider two machine
learning models. And so machine learning models are essentially black boxes. In this case, this is
just a multi-layer perceptron, just a neural network. And so what we do is we don't know what
is going on inside. This is the total opposite from IBM, whereas IBM is completely transparent.
Right? These two guys are completely not transparent because these are black boxes. We
throw data in and we do this supervised training by means of this technique called behavioral
cloning. So in behavioral cloning, all you do is to try to maximize the fit of the data by using
this either a multi-layer perceptron or a recurrent neural network specification.
Okay, so in the first set of evaluation that we conducted, we refer to this as offline because
all we're interested is in how good of a prediction is our model doing. And we refer to the model as
AIDA, so it's an active inference driver agent. And so how good of a prediction in terms of the
acceleration. So in the following, all you're doing is essentially slamming on the pedal or on
the brakes. And we would like to get a sense of, relatively, to respect to these other models,
how good of a job the active inference is in predicting the acceleration and our breaking
patterns in the data. And so it does a very good job. This is the performance. Now notice that
the performance metric, this is the mean absolute error. But we are using the interquartile
measure. So we're dismissing the stream quartiles of the distribution and taking
the absolute error over those remaining values. And again, the comparison is
done with a different lane. So we do the same lane testing and then the different lane testing.
And this we call offline evaluation. So the offline evaluations, things are looking good.
Things are not as good, but continue to be relatively decent in what we call the online
evaluation. So here, the objective is a little bit different. We're not interested in how good
of a fit or a predictor we have for acceleration, but rather how good of a control we have been
able to extract from this data. So what we do is we take the controller, the policy, that we were
able to estimate, and we plug it to a real simulation of the environment. So the controller is
just going to, that's why it's referred to as online, it would be putting that close feedback
loop with the environment. And so as you can see, and the measure of performance is the
deviation. So this is absolute deviation with respect to the actual, the
the original performance. And so we start to see that here it's not so clear anymore that AIDA
has the best performance, even though it's still competitive with the multi-layer perceptron
model. By mind you, we still have, it's still doing much better than the
IDM model. And it's giving us a sense of these primitives of the model that are
highly interpretable as I started the discussion a moment ago. Okay, so another way to see the
performance is if we track a trajectory. So this is relative distance, this is relative
velocity, and this is looming. And see the performance of the agent. This is the true
acceleration, right? On the right hand side, we have the true acceleration. So this is a
very solid deceleration followed by then an acceleration. And we would like to see in this
particular sequence how good the model does. Now, maybe this is not so clear to you guys
here because this is in gray, but remember that these are random policies. And so it doesn't give
us, it's not going to give us A value, but it gives us a distribution over actions.
But what this is saying is that most of the weight is on the actions that you can see here
that are kind of mimicking the actual data and then likewise for acceleration.
Now, as for the hidden states, so this is the belief distribution.
I just want to remind that we have five minutes remaining.
Five minutes, yeah, I'm about to finish, yes. And so the belief distribution,
remember, these states are this construct that the model comes up with that are associated with
a categorical description of the environment, right? And so typically you have states, these
states are maybe dangerous states. And so the belief is that you are in a dangerous
state and then you go back once you accelerate to a relatively normal state.
Okay, so let me finish by pointing out the limitations of this. The limitations are
basically the data, right? And so one thing we observed is that when we do the online
simulation, there might be crashes, right? So we take the model, we plug it into a real
environment and lo and behold, sometimes the controller leads to a crash. This is a situation
of a crash, which is represented here by two vehicles being on the same location at the same
time, right? Now, what we think is happening here is that there is a limit to how much we
can generalize the model that we have built because there are no accidents in the data set.
There's very few observations of very tight or close to collision situations. And so not
surprisingly, the model that we are able to construct, which is a model based on normal
operating conditions, it's unable to control a vehicle that is being subject to a very
extreme condition in which they say the looming decreases so rapidly to this value.
So we do see these limits to the generalization. Okay, so let me conclude with a few thoughts.
We have taken a long journey into using tools from stochastic control,
much statistical learning, and cognitive science to come up with a model that has a structure,
right? What we're doing here is saying we don't take a black box approach to modeling driving.
We are able to identify the primitives on a model that is easier to understand. It's a bit more
transparent than, and that at least in this task, this is not done at the expense of effectiveness
as a model, right? Because typically, these machine learning models are going to outperform
anybody because they are massive and they have billions of parameters and they will do a much
better job, but at the expense of lack of transparency. So at least here, we are always
maintaining, making sure that we have an interpretable model within sight rather than
making a leap in the dark and letting the machine learning identify something that we don't really
understand how it's working. And this is, I think, what the merit of our work is. Now,
for future work, we would like to, and there is evidence in the data, so that there is heterogeneity,
right? So this is an important part that we cannot run out of time. And by that, I mean that not all
drivers, a population model only goes so far because you can see that there are drivers that
are more aggressive and there are drivers that are less aggressive. And it would have been nice to
also build like two different models for the same data set in which one is associated with some
fraction of the data set that is associated with drivers that we believe are more aggressive,
people that have preferences for being closer to the Hakara head or other drivers that are more
conservative and don't do that. But this is a part of the work that we were unable to complete.
And so with that, I close. Thank you. Thank you so much, Dr. Garcia. At this time,
we welcome any questions or anybody has any questions. Please go ahead and use the chat box
to go ahead and read them out. While that is something that people may be mulling over,
I would have a few questions that I'd like to. Okay, sure. Go ahead. How scalable is the estimation
that this result? That's a very good question. Okay, so there is a lot of computation and the
bottleneck probably is in this. Let me go back to the slides.
Okay, so if you remember the head of the estimation is solving this
build by level optimization problem here. Oops, sorry here, right? So this part, the upper level
problem is relatively easier. We're doing stochastic gradient updates. The harder part,
the part where it's not easily scalable is the inner loop, right? Because in the inner loop,
we're solving a stochastic dynamic problem. This is where they're receiving horizon.
I forgot to mention this, but there's so much information that I can put in this in the talk.
But our model of the agent is an agent that is looking ahead eight units of time and deciding
what to do now and then revisiting that decision. This looking ahead eight units, it's solving a
PONDP with the computational version that entails. And so the scalability, maybe the bottleneck may
lie in this inner loop, which for the purpose of our task, following environment was not because
it was a simplified environment. But if we were to take this to a more complex environment,
I'm sure this will be a bottleneck. Thank you. Another one that we have is,
what is different and are unique about the active engines? That's a very good question, yes.
I think it has, it's twofold the answer. In my opinion, right, is that it's a nice
encompassing framework. So as I described, it puts in the same token, this and this loops.
So this is the estimation and this is the control. And typically, there's been,
like in the control literature, there's been literature on the duality between these two tasks.
But active engines does a very nice job of putting these two things as two sides of the same
coin. And in particular, I think what I, many models don't have is this emphasis on the epistemic
value. So whereas you all have heard exploration based on exploitation in sequential dynamic
decision making when the environment is unknown, and then there is a merit in exploring before
you actually exploit. But I don't think there is such a compact and analytically
workable representation as the one that they are given in this sense. Because it essentially,
it's all in the same currency, if you will. It's all in the, you're estimating distributions,
right? You're making actions so as to induce distributions. And in that facilitates looking
at both the backward and the forward problems in the same type of perspective. So I think what is
novel is the fact that it encompasses, in a nice way, the two problems, and that it brings,
it highlights the importance of the epistemic value in human decision making in dynamic tasks.
Thank you.
Oh, I have to say, I have to say also that there is evidence of, this is a topic I'm not familiar
with, but I think there is evidence, like neural footprint of a lot of the theories
that underlie active inference. So there is evidence that our brain works the way,
this way, and the way they conceptualize this active inference framework.
There's a question of do you think the model is region specific or can it be generalized globally?
Yeah, this is a good question that I think is also related to heterogeneity, right? So in a sense,
so I think that the key point here is heterogeneity. If this data, so we ended up only working with
data from China, even though the interaction data has entries from other countries, but it was a means
to try to control, we cannot deal with too much heterogeneity, right? And so in the next step,
what we're planning to do is to try to enlarge a little bit this model by using
effectation maximization to be able to account for heterogeneity. In other words, the answer would
not be, this is your model, but this is a collection of models that can be used to describe this
data set. And this is something that we haven't done yet, but I think it might be worthwhile
looking into. Thank you for the question. Thank you. I'd like to thank everyone again for joining
the webinar. This webinar will be uploaded to YouTube and will be modified. Everyone will email
when it is complete. The registration is open for our next webinar. The next webinar will be on the
real-time risk prediction at the signalized intersection using grapheneal networks presented
by researchers at the Virginia Tech Transportation Institute. This webinar is scheduled for October
24th at 2.30 Eastern Time. You can register now at the link that I just sent over to the chat window.
I will need this up for a little bit, but everyone would like to
register for the next webinar, and thank you all so much again for attending, and thank you
and I think I'll see you again at our presentation tonight. All right, thank you.
