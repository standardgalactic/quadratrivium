So again, welcome everybody.
Today I'm going to talk about Bayesian item response
modeling in R with BRMS.
And Stan, I'm trying to make this conceptually high level
and then go a little bit into the syntactical details
to fit item response model, IRT models in BRMS.
So this talk is based on a paper of mine
which last year has been published in Journal of Statistical
Software after, I think, being accepted for over a year
and still being put on hold because it
was part of a special issue, which was delayed a bit.
So now it's there, it's officially published,
but it was also there before an archive.
So let's talk a little bit about general purpose IRT.
So for item response modeling, we
need a set of parameters, zeta i for item i.
So every item has a bunch of parameters zeta i.
And for every person, we have a set of parameters theta p.
So for those of you who are not familiar with IRT modeling,
we're just wondering what is happening here.
So in item response modeling, what we do
is basically every person that attends some kind of test
that's intelligence or creativity tests,
they are going to answer, in most cases, every item.
That is basically we have, in the end, a fully crossed
matrix such that every person has
provided response on every item.
If that is some kind of ability tests, like intelligence,
creativity or something, that could be part of IRT
or it could be some kind of personality test,
for example, if we were to measure something
like extraversion, emotional stability,
conscientiousness, something like that.
And the goal of IRT would then be
to measure both person and item parameters as closely
as possible.
For example, if we were to do some personal selection based
on some intelligence test, we are really
interested in getting those person parameters representing
the person's ability and general intelligence,
for example, as good and as precisely as possible.
So in IRT, we are really concerned
with measuring and modeling stuff as good as possible
so that we have really valid estimates of what
we later on are interested in, be it item
or be it person parameters or both.
So every item has those item parameters,
every person has the person parameters.
And in usual cases, every person sees every item
and the other.
So a model for the responses YIP,
so I for item P for person, is very generally
and holistically speaking, just like some kind of model
given the item parameters of the item I
and given the person parameters of the item P,
of the person P.
So in order to make sense, so I have to make sure
that this model works at all in some reasonable ways,
we need some kind of restrictions on the item
and person parameters.
And otherwise, we're ending up in situations
that we would in classical statistics call
non-identified, which basically tells us
that we can't really estimate that model.
Now non-identified is not really a term
that really makes sense in a lot of Bayesian statistics
since as long as we're specifying some reasonable
prior distributions, we always get a posterior distribution
but the problem remains that if the model is not identified
in a classical fragmented sense, then chances are
we won't get any sense of results
of Bayesian models either regardless of whether we get
a posterior distribution or not.
So we need some kind of restrictions
and like the specific restrictions don't matter here
right now, we just have to keep in mind
we have these kind of restrictions.
So when we know the Bayesian IIT modeling,
most of which stays the same, we're just replacing
those kind of restrictions, some kind of restrictions
with priors.
So we're going to have some for now
not specifically specified prior on the item parameters
and we have some not for now specifically specified priors
on the person parameters.
And that's really it.
What means is that when we're doing Bayesian
item response modeling as in person to like
just item response modeling, it's modeling wise
having a bunch of priors and then estimation wise
having different estimation techniques, usually not
optimization, but something like that can estimate
distributions most likely MCMC as for example
in BRMS instead.
So there's not really much to newly learn
if you already know IIT and for some reason
want to do Bayesian IIT for example, because the model
and your classical IIT models end up not converging
or there's some other problems.
You want to try Bayesian IIT, there's not much additional
stuff you have to understand to get going there
and BRMS hopefully makes it easy in some ways.
And I hope that this talk can help you with that.
By the way, it's actually this Zoom pane.
Is it kind of in the way of the slides?
Do you still see the full slide?
We still see the full slide.
Perfect, because I don't, but that's good enough.
Okay.
Well, we do.
Okay, that's all I need.
Okay, so let's extend that a little bit,
make it a little bit more interesting
because I want to look at IIT models
in the form of distributional regression.
We will later see more like in terms of distributional
multi-level model season.
But I want to explain here what I mean
by distributional regression.
So first of all, we have to consider the data
to be in long format.
While before we had this, some kind of metrics
of like items as rows of that metrics
and persons on the columns of that metrics
and the responses being in that metrics.
When we're now actually wanting to do that in BRMS
and a lot of other packages that represent IIT models
as some kind of multi-level models,
we need things in long format.
That is, we need one long vector of responses.
And then we have additional columns indicating the item
number and the person number or person index
to which that observation that response belongs.
So we're thinking of the responses to be in long format.
And then we consider the point-wise
that is per response likelihood
in terms of distributional parameters,
psi one to psi k.
What I mean by that?
So consider we had a normal likelihood.
We just assume our response would be normal.
Then we would have two distributional parameters,
mean and standard deviation or mean and variance
depending on how we parameterize it.
What we're usually doing in any kinds of regression models
is, or in most like typical regression models
be them Bayesian or otherwise,
that we only predict the mean or some other kind
of location parameter of all likelihood.
And then all the other parameters, for example,
the residual standard deviation sigma in a normal model
would be assumed constant across observation.
It would still be estimated that assumed constant
across observations that is not predicted.
What we're now doing in distribution regression
is we allow not only the mean to potentially be predicted
by some kind of regression model
or by item or person parameters in IRT,
but also all subset of the other distributional parameters.
For example, the residual standard deviation
in the Gaussian model or for example,
if we had a skew normal model
that has some additional skewness parameter,
we could additionally model the skewness parameter as well
because perhaps items can vary in their skewness.
Or if you're more familiar with, for example,
response time modeling,
and I'm going to talk about that later in more detail,
sometimes in response times modeling,
we would like to model response time
with the exponentially modified Gaussian distribution.
It's kind of a convenient choice,
not necessarily principled, but convenient.
And there we would also have something like mean
standard deviation or some kind of variation measure
and some right skewness measure.
So we could have like multiple distributional parameters
that all of which could be depending
on item and person parameters in IRT.
And that's what I'm trying to convey here
in the second equation.
So each or a subset of those distributional parameters
can be connected to the item and person parameters
by a response function.
FK where K here indicates the distributional parameter.
So distribution parameter K psi K
is equal to some response function FK
of the item and person parameters.
When response function is something that you're unfamiliar
with response functions are basically inverse link functions.
So if we think of a log link,
then the response function would be the inverse of it
that is exponential.
Or if we think of a logit link,
then the inverse of it would be the inverse logit
or like logistic function.
And that's typically what we have in generalized linear models
here with distributional models.
We have this kind of regression model
and the link function for each
or like at least multiple distributional parameters.
Okay, let's take a look at binary,
IRT modes of binary responses,
which is perhaps the most common form of IRT models
where basically binary responses mean
we have zero or one responses, for example,
when we just score an intelligence test
and we score every item,
whether it's correct, assign a one or incorrect,
assign a zero, that's kind of just a typical case.
And those, the situation, these responses,
these binary responses, we would canonically model
with a Bernoulli distribution of a single parameter psi
that basically gives the success probability
or the probability of giving the correct response.
Now, of course, some items may be more difficult
or easier than others,
and some people may be like more able to solve these items
than ought to give the correct response than others.
So quite naturally, we would like to relate this success
or this correct answer probability psi
to our personal item parameters.
And when we consider the rush model,
which is kind of the most basic IRT model, if we will,
then psi is like a function F,
the response function that I'm going to explain
in a second of the sum of the corresponding item
and person parameter,
where here this person parameter theta P
would be the ability, that is the higher theta P,
the greater the chance that this person answers
that item correctly,
and the theta I would be an easiness parameter
that is the higher theta I, the easier the item is,
that is the more likely every kind of person
will answer that item correctly.
So for F, we could have different choices.
Here, like in the canonical IRT rush model,
we are choosing the logistic response function,
i.e. the logit link, which is simply exp of that sum
of theta P and theta P and theta I divided by one plus
that same term.
By doing this kind of response,
we would ensure that our probability of answering
that item correctly for a given person
is always between zero and one.
Okay, I want to pause here quickly and ask
if there are any questions so far.
No, wait, yes, there just came one question in the chat.
Is there a specific reason for summing difficulty
and ability?
What would be the alternative?
I guess that would be the add-on question.
The convention is a subtraction.
Yes, so if we were to do a subtraction
that is basically person minus item parameters,
then the item parameters would be difficulty.
That is the higher the item parameter,
the higher the difficulty.
When we are adding the two together,
then the item parameters are no longer difficulty,
but easiness that is higher the item parameter,
the easier the item.
I know the person who knows that question,
I'm just explaining it for the audience.
So the reason I'm using here addition
is that we are going to use a regression framework
for that where we're just having one term plus another
plus a third term plus a fourth, et cetera.
So by having this easiness plus parameterization,
it fits more easily in regression,
but in any case it's equivalent
because the difficulty is just minus the easiness.
There is one more question here.
Are the indices i and p really required?
What if there are several i equals one, two, three, and p?
For the same response, so I'm being a bit lazy
with the indices here.
So I just want to indicate
that whatever response we're talking about
is for a specific item and for a specific person.
I'm not using the, these indexes consistently
across all the formulas here,
just to try to like ease the understanding
of the general conceptual idea.
But I'm not sure if that answers the question,
perhaps that the question would have to be clarified.
Yeah, that just came in at all question.
So should it be psi i p?
Yes, it should be.
I was just lazy when adding that.
Yeah.
Good to see that other people are lazy as well.
Yeah, no, no.
So here you just, the point is more like
our predictions are always depending
on a certain item and person
and all the quantities that follow from that also
could have this i p index if we wanted to be strict.
So this is just to be understood conceptually.
I think that was it for now.
Okay, perfect.
So let's continue.
So in this case of the rush model,
be it Bayesian, estimated Bayesian or otherwise,
we have one person parameter and we have one item parameter.
So let's make the situation a little bit more complicated
and go to the so-called 2PL or Birnbaum model,
which is basically two parameter logistic model,
PL for parameter logistic model.
And what we're adding there is,
in this case, the discrimination alpha i,
that basically tells us how steep the logistic curve,
so if the discrimination is higher,
the steepness of the logistic curve would be higher as well.
And we would say the item would be discriminating
more strongly, so it would be more informative
if we will about ability differences.
And if alpha i was smaller,
then the item would be less informative.
So for example, if you're aiming to estimate
person parameters quite precisely with fewer items,
then probably highly discriminating items
are the way to go, but also can need to be taken
that there are some spread and difficulty
of these discriminating items.
We can go one step further and go for a 3PL model
where we additionally have some guessing parameters.
So for example, if we're in a false choice experiment
and there are four different options of which one is true,
then even if the person doesn't know the right answer
and they are guessing,
they are still guessing the right answer
with probability 25%.
And this is something we could include in our model
by basically multiplying our 2PL part with one minus
this gamma i and adding gamma i,
where gamma i then would be the guessing probability.
So even if the person had no idea whatsoever
what's going on, they would still have an expectation,
a probability of answering this false choice question
correctly by 25%.
So we could add on like more parameters,
but I think the idea should be clear that
as we are adding more item parameters
or like person parameters, we could also add more.
The problem becomes more and more complicated
and also the estimation becomes more complicated.
And as models become more complicated, more non-linear,
we clearly see the 3PL model is quite non-linear,
Bayesian estimation tends to shine more
because it's more robust in terms of estimation performance
and we can make it additional robust
by specifying prior distributions.
Let's talk about priors.
So we have lots of choices.
I just want to conceptually differentiate
between two options we are generally having.
We can specify non-hierarchical priors
and we can specify hierarchical priors.
And this holds true for both item and person parameters,
I'm just going to explain for the item parameters here.
Non-hierarchical priors, this case means
that every item parameter zeta i
gets its own independent prior
with some kind of fixed hyper parameters.
For example, in this Rache model,
we could say every of the difficulty
or easiness parameters is a prior normally distributed
with mean zero and standard deviation three,
which for the logit scale
would be a rather weakly informative prior
adding just a little bit prior knowledge
about the understanding of the scale
of the logistic model, but like nothing else.
It would regularize the model
to make it a little bit more stable,
but otherwise would add too much information.
But the most important of the non-hierarchical part is
the hyper parameters, the prior, this zero and the three
mean zero, standard deviation three here are fixed.
We just set them and that's it.
Now compare that to the second option,
which is a hierarchical prior.
In which case, for example,
our easiness parameter zeta i
would be normally distributed again around zero,
but now with a parameter sigma zeta
that would not be fixed by the user,
but rather estimated by the model
and gets its own hyper prior.
So for example, we could say zeta,
a sigma zeta is half normally distributed
with the nativation one, normal plus means
it's a truncated normal and it's truncated at zero.
So the normal plus prior with mean zero and standard deviation
one with location parameter one and scale parameter,
location parameter zero, scale parameter one
would basically be a truncated normal
that's just in the positive field line.
The important part here in the hierarchical formulation,
all the easiness parameters of the different items
are related to each other because they're sharing
same, in this case, single hyper parameter sigma,
and by that means we are implying shrinkage
as in all kinds of multineveral models
that is items that are specifically easy
or specifically difficult,
get a little bit shrunken towards the mean.
And I'm highlighting this here in terms of item parameters
because if you're interested in IIT,
you probably are familiar with a convention
that item parameters get non-hierarchical priors,
even if we don't call them non-hierarchical
are they rather don't get any priors at all
while the person parameter gets those hierarchical priors.
And the logic that is usually applied here
is that item parameters is just a fixed set of items,
they're not coming from a population,
we're just having some fixed item parameters
while the people are coming from a much bigger population
and we've just drawn a sample from that population
but the items are just like those items we're having
and there's no population that are coming from.
And then that's used as an explanation
of why we're having non-hierarchical priors
on item parameters and hierarchical priors
and person parameters.
And what I'm trying to convey here
is I don't really like this argumentation.
So for me, whether I'm implying non-hierarchical
or hierarchical priors is simply,
do I want shrinkage or not?
Do I want like additional this partial pooling
like shrinkage depending on the other item
or person parameters or do I not want to do that?
And my answer is usually I want that kind of shrinkage
to make my model a little bit more robust.
So my tendency would be to have both hierarchical priors
on person and on item parameters,
although in BRMS you could do both to both
depending on what you would like.
We could make that also more involved.
So if we had multiple item parameters per item,
for example, in the 3PL model,
we had three item parameters like easiness,
discrimination and guessing parameter
and we could scale them appropriately
with some link functions.
Then we could like model that hierarchically
with a multivariate normal prior
that not only had those standard deviations
belonging to the specific parameters,
but it would also have a correlation matrix here,
omega, zeta that would model the correlation.
For example, we could think of a case
where some items that are more difficult
are also better discriminating
or would have different guessing probabilities
than other parameters.
The exact notation here doesn't really matter the points.
Hierarchical priors can also be applied
if we have multiple parameters per item
or multiple parameters per person.
Okay, are there any questions about that so far?
Yes, there was a question that just came up
and the question is,
to what extent is choice between hierarchical
and non-hierarchical priors determined
by the amount of data we have?
If we have a lot of data, is shrinkage as critical?
That's a really good question.
So there are two different amounts of data.
One is, for example,
if we're thinking of item parameters here,
how many items are we having?
And the second amount of data is,
how many observations per item are we having?
In this case, it would just be how many persons are we having
if every person sees every item?
So what we need to be careful with is
if we are applying hierarchical priors,
if we have just very few items such that, for example,
we have just two or three items,
then having a hierarchical prior would be a little bit difficult
because then that standard deviation parameter
would just be estimated based on two or three items.
Imagine estimating a standard deviation
based on three observations.
That's not going to end up being so well.
In this case, perhaps we better go with a non-hierarchical version
to avoid this additional hierarchical parameter.
So that's one kind of amount of data.
The other one would be the amount of persons.
And yes, that's true.
If we have a lot of persons,
then probably the shrinkage won't matter too much
unless we are in cases where the responses on some items
are really awkward, for example, very extreme.
Almost everybody answers them correctly
or almost everybody answers them incorrectly.
Then things may become a little weird
and their hierarchical priors can actually help stabilizing that.
So it's not only the amount of data,
also the amount of person, for example, in this case,
but it's also the specific response pattern.
So unless you have only very few items,
probably hierarchical priors, at least I would prefer them
and in the quote-unquote worst case,
they would probably not be any different
than the non-hierarchical ones.
Yeah, thank you.
And there was another question.
It's, do shrinkage and regularization refer to the same concept?
Yes.
So simple answer.
Thank you.
There was all the questions.
Good.
OK, so after this kind of just general introduction,
I want to show you a little bit how we can apply
general purpose IRT models with BRMS and Stan.
For those of you who don't know,
Stan is a probabilistic programming language
to fit open-ended Bayesian models.
It's internally uses C++, so it's really, really efficient,
really fast.
And the Stan modeling language is accessible through R,
through Python, Julia, Command, Line,
all the kinds of interfaces.
It has its own language, which is a little bit something
between C++ and R, but much easier to use than C++
and for those of you familiar with R, it's not too difficult.
However, of course, setting up really complicated models
involves a lot of work, a lot of expertise
to set that up correctly, making it numerical, stable,
like having efficient parametrizations and so on and so
forth.
So there are lots of packages that use Stan internally,
but externally, for example, use an R interface.
And one of these is my R package, BRMS,
which focuses on all kinds of regression model.
Internally, it writes Stan code that is human-readable,
mostly, yet fast.
But on the outside, all you have to do
is basically specify a bunch of R code, mostly R formulas,
and then you're good to go.
So from now on, I'm going to use BRMS,
but know that Stan handles things internally,
and BRMS wouldn't be what it would be if Stan wouldn't
be that great of a probabilistic programming language.
OK, so let's specify our IIT models in BRMS.
We're going to start with a so-called family,
while family basically means what kind of likelihood
are we assuming.
So we can use all sorts of standard families
that are available in R, such as Gaussian, Poisson,
binomial, but we can also use that a little bit more
general BRMS family interface, where
we can have access to a lot of more families.
So in the BRMS family interface, we
would specify the family, quote unquote,
that is the likelihood, Gaussian,
binomial, Poisson, whatever you like,
with the corresponding link function
that applies to the main location, i.e. mean parameter.
And if we had more distribution parameters
that we wanted to predict, we could also
specify more link functions that apply
to these additional distribution parameters.
For example, if we're looking at a binary model with a Bernoulli
family, for example, to set up a rush model or something
like that, or we had to do, say, a BRMS family, Bernoulli,
with a link, log it.
If we had set up a distributional model that
had, for example, both predictions of the mean
and of the residual standard deviation sigma,
then, for example, we would specify family Gaussian link
as identity, because we don't have any relevant link
function on the mean.
We don't need to.
We could, but we don't need to.
And we would specify the lock link on sigma,
because standard deviation can only
be positive by applying a lock link or equivalently
exponential response function, which
ensuring that our predicted sigma is always positive.
If you don't specify the link functions,
defaults are used.
Here you see the actual defaults just explicitly
being spelled out.
The formula, so here we're going to specify
how exactly our responses are related to item and person
parameters.
If we just specify one formula, then it's
just going to be applied to the mean parameter.
So in the first formula, items have independent priors.
Persons have hierarchical priors.
And our response would be y, and our item index
would be item, and our person index would be person.
Then we would say y tilde 0 plus item plus this kind
of random effect notation, one pipe person.
What does that mean?
So first of all, everything before the tilde
is the response in this case called y.
Then the 0 plus item ensures that if item is a factor,
we're going to remove the intercept
and instead have cell mean coding for the items.
That is, for every item, we are going
to have one fixed effect, like one coefficient per item.
If we didn't use this 0 plus item,
then we would have some kind of dummy coding, for example.
This is general formula syntax style,
where just one item would be the reference,
and all the other item effects would be in reference
to that reference item, which is a little bit awkward.
So we're removing the intercept and showing cell mean
coding for the items.
Every item gets a coefficient, and everybody's happy.
So we are saying 0 plus item.
And then we're using this LME4 random effects notation,
where things before the pipe, for this pipe symbol,
is other coefficients that are supposed
to vary across the grouping factor behind after that pipe.
So one here stands for intercept,
and the person is the grouping variable.
So we're having a specific intercept parameter
for each person, which just means every person gets
their own, for example, ability parameter.
If we want to have both item and person parameters
hierarchically, we're going to add the overall intercept 1
back again, and we're going to have this kind of random effects
formulation, one pipe item, that is,
every item gets their own item parameter,
but we're having internally this hierarchical prior
the same with person.
If we wanted to add a covariate, for example,
the age of the participants, if there
was a test that would be where the ability of the persons
may depend on their age, we could add that covariate just
as we could in any kind of other regression framework
to have some kind of explanatory IRT models.
It's just like multilevel regression
with some additional predictors that could also
vary in effect across item and person.
We could make this now a little bit more complicated.
So suppose we are having multiple distribution
of parameters, for example, one mean parameter, which
is represented in the first formula always,
and then we had two additional distribution of parameters,
part two and part three, each of which
would be related to item and person parameters,
to their own item and person parameters,
then we would specify three formulas, one
for the main parameter, and then one for part two,
one for part four, part three.
And we would combine that with the BF function, where BF stands
for BRMS formula.
It doesn't stand for base factors,
so be aware of that.
If you want to make it explicit, you
could also write BRMS formula, which is just a little bit long.
But BRMS supports also the second kind of interface
for multiple parameters, which is so-called nonlinear
formula syntax.
Remember, for example, in the binary IRT models,
we didn't really have multiple distribution parameters.
We just had the success probability in some way.
But we were relating that to multiple additional parameters,
for example, discrimination and guessing.
So for example, if our response y is some nonlinear function
of, let's say, some covariate x and two nonlinear parameters,
let's say, discrimination and easiness or something,
then we could specify some nonlinear function fun
of that covariate x plus the distribution parameters.
And then we would specify formulas, linear formulas,
for the two nonlinear parameters.
And to make sure, BRMS knows that this first formula,
with a y tilde fun, is not treated as a linear function
of parsed as usually R parses formulas,
but literally treated literally as a nonlinear function
with setting nL is equal to 2 such that BRMS knows
this is going to happen.
Are there any?
I'm going to add one more slide, and I'm going to ask for questions.
So if we look at this again, then we
have different item parameters per nonlinear distribution
parameters, same with different parsed parameters,
but they all are going to be estimated as independent.
But as I said initially, it is very plausible that,
for example, the easiness of items
relates to their discrimination, or, for example,
the ability of persons in a certain aspect, for example,
mean response times could very well relate,
i.e., is correlated with the person's ability in terms
of a person's property in terms of the variation
of response times.
There's a long literature on that showing, basically,
that people who tend to have longer response times
also have more variation in response times.
So it may make sense to assume those different random effects
of a person of items to be modeled as correlated.
And in BRMS, this is realized across formulas
by having two of these pipe symbols
and then having a symbol in between them, for example,
i for items and p for persons.
It doesn't really matter what you put in there,
as long as you put the same symbol in those terms that
are supposed to be correlated.
So if you want to correlate all person parameters with each other,
two pipes, and then a p in between,
and all of them are going to be modeled as correlated,
even across formulas.
Same with items.
OK.
So before we're going to look at one application,
are there any further questions?
Right.
Now there are no additional questions.
Good.
OK.
Oh, there is a question, I think.
Well, a comment.
Pretty cool stuff.
Yes.
And now it's also a question coming.
Are interactions expressed in the typical R formula way?
Yes.
There are.
Yeah, that was it.
That was it.
OK.
Good.
OK.
So I know this was a little bit abstract in lots of ways.
Lace, let's make this a little bit more concrete by looking
at one case study, which is the verbal aggression data set,
which is, I'm not sure if it's actually a real data set
or if it's a simulated data set.
Let's just pretend it's real.
I don't know if it is, to be honest.
And in this data set, we're going
to analyze different kinds of verbal aggression
and how they relate, for example, to the gender of people
or to different kind of aspects of the items.
So first of all, people were answering certain items
that asked if, in a certain situation, they would curse,
for example, or they would do some other actions that
relate to verbal aggression.
And there would be different modes and the variable mode,
for example, do they want to show that verbal aggressive
behavior or would they actually show it in that case?
So one thing is, of course, like one step further away
from actually doing it and doing it.
And there could be different situations, different kinds
of verbal aggression.
And the responses actually has three levels, which is, no,
I wouldn't show that verbal aggression behavior
or didn't want to do that, perhaps something between and yes.
So the rest is this three outcomes
and the R2 column at the very end is just no or yes,
whereas perhaps it's just coded as yes, which is just anything
else than no would be coded as yes.
And then we could be looking at, for example,
how does gender relate to this verbal aggressive behavior
or wish to do that?
Or how does the general trait anger here
listed in the anger column relate to these features?
So there are lots of things in terms
of item and person covariates going on that we could look at.
So now I have to actually move this bar that you don't see
so that I see what I'm actually writing.
So let's focus on the binary response case R2 in BRMS
by our ordinal models.
We could also model the three responses,
but we just focus on the two responses case here.
And if we're just fitting a 1PL that is a rush model
with hierarchical price and item and person parameters,
where item is indicated by this item column and ID
is the person column, then we would specify our rush model
with hierarchical price and item and ID.
And by the syntax, we automatically
have the hierarchical prior.
We wouldn't need to specify explicitly the prior for that.
But what we could specify if we wanted to
is the specific prior for the standard deviation
of those random effects of ID and of person and of item.
And here, for example, we would specify that standard deviation
parameters as being half normally distributed 0, 3.
This half is not specified here or this truncation is not
specified here, but it happens implicitly behind the scenes.
So BRMS makes sure, rather, that we're having always
the right boundaries.
Then the deviation can only be positive.
BRMS makes sure that's the case.
And standard translate is normal 0, 3 prior to truncated,
normal 0, 3 prior truncated at 0.
And then we would put everything together using the BRM call.
So we would specify our formula or a data set, our prior,
and our family, which in this case
would be Bernoulli with our logit link.
Then we could, for example, run the plot method.
We could see the marginal density of the parameters.
The intercept is the overall intercept here.
And SDID intercept is the standard deviation
across the person parameters of the intercept here.
We just have intercept.
So it's just the person parameters.
And SDITEM intercept is the standard deviation across items.
And if we look at the right-hand side,
we see that we have fitted four MCMC chains.
So we're using Bayesian estimation
to fit the posterior distribution.
I'm not going to talk about that in detail
in the interest of time here.
But we see things have been mixing quite well.
So our Bayesian model has been able to obtain,
at least for those three parameters,
a quite trustworthy posterior distribution
that we can use for further inference.
Now we could, for example, look at the item parameters.
So on the x-axis, we see the estimate of the item parameter.
On the y-axis, we see the item number.
We have the easiness parameterization.
So we see that kind of items with a lower number
that were initially, at the start of the test,
are rather easy.
While to the end, we're getting more difficult items
that, in this case, difficult would mean fewer people
would agree to that item, which is basically the reason
I think the first items are more like,
do I want to show verbal aggressive behavior
while the later items are more like,
would I actually do that?
And this apparently is something that fewer people do actually
show verbal aggressive behavior than people
who want to show that, which I think all of us can relate to,
I guess.
So now let's get a little bit crazy and fit a 2PL model.
So 2PL model, we have the item easiness,
we have the item discrimination,
and we have still the personability.
So we are separating that model in two parts.
First of all, this is what I'm calling ETA,
which simply has the item easiness.
And the person, ability or person parameter.
And then we're having the discrimination.
And the discrimination is something that usually
can only be positive.
So we want to ensure that it's positive.
And the way we do that is we are specifying our item
discriminations on the log scale.
And then after exponentiating it,
they're going to be only be positive.
That is, we're having our linear formula on some kind of scale
where we consume things to be linear.
And then we're transforming them, for example,
here with an exponential transform,
such that they adhere to the correct restrictions
we want to apply, for example, discriminations
can only be positive.
So that's using the nonlinear formula syntax.
So R2 is nonlinear related to exp of the nonlinear parameter
log alpha times the nonlinear parameter eta.
And then we're plugging in our linear formulas for eta
and log alpha.
Prior specification becomes a bit more involved.
We didn't have to specify any priors.
BRMS would take care of them in terms of default priors.
But yeah, I mean, that doesn't mean they are always sensible.
So I would always recommend that you specify your own priors
and not necessarily rely on the default priors.
And I want to bring your attention
to the third line of the prior specification, which
is this constant 1 of the SD class of the standard deviation
of group ID, which basically means here
we are fixing the standard deviation of our person
parameters to 1, that is this constant 1.
And the reason why we're doing that
is if we didn't do so, we would have a non-identification
problem because the average discrimination
and the standard deviation of the person parameters
are not jointly identified.
We could get away with them without specifying
those constants just by having proper priors
and both average discrimination and the standard deviation
of the person parameters.
But still, the scale wouldn't tell us much.
And even if we would get results,
the scale would just be fully determined
based on our priors, which is not necessarily what we want.
So what we can do is we can directly solve or resolve
our identification problem by fixing the standard deviation
to 1 of the person parameters, which is something that is also
commonly done generally in IRT models.
OK, so if we were, for example, to compare those two models, one
with the discrimination, the 2PL model and one
without discrimination, the 1PL model,
and we would do model comparison, for example,
with approximately one across validation,
we would see that a 2PL model fits a little bit better,
although not so convincingly so.
So very quickly, when we are running this comparison
by a leave on a cross validation,
the model that is shown in the first row is the best model.
All models are going to be compared to the best models,
so the best model against the best has zero difference.
And then we have an ordering of all following models.
In this case, we are just two.
And one, the 1PL model, is a little bit worse,
but the uncertainty in that difference is also quite great,
so we don't really have good argumentation
for using one or the other models.
So let's stick to the 1PL model for now
and do some sort of explanatory IRT
by adding a lot of predictors.
For example, the B type, which I forgot what it actually meant,
is basically whether we are going to curse or scald
so different kinds of verbal aggression
we would like to do.
The situation that is described in the item mode
is whether people actually want to show
that verbal aggressive behavior or do it.
Anger is some kind of trait anger,
how angry people are in general
and gender is the self-assigned gender of those people.
Now we could predict all of that in an explanatory IRT model
and we could even say,
we assume that items have differential item functioning,
for example, across genders,
so some items may be easier or more difficult
for the different genders.
Or we could say that people are differing
in how they react to the different mode
that is want or actually do the,
showing the verbal aggressive behavior.
And we can, for example, do that like that,
so zero plus gender would mean we would have
one item parameter,
and we would have like two item parameters per item,
one for assigned male and one for like self-assigned female,
and for mode do or actually,
want or actually do show the verbal aggressive behavior
across persons.
And again, we can run that model.
And when we're looking at the conditional effects
or conditional predictions
of the interaction of mode and gender,
we would actually see that both genders,
and at least in this example,
there were apparently no non-binary people present,
so we just have two genders here,
show the same or roughly the same kind of probability
of wanting to show verbal aggressive behavior,
but then when it comes to actually doing
those verbal aggressive behavior,
females would rate their probability of doing that
much less, roughly around 70%
while males are way above 80%.
So we're seeing that in terms of actually showing,
doing the verbal aggressive behavior,
at least in this data set,
males have higher probability,
according to their self-reports on these items.
Okay, are there any more questions?
Currently, there are no,
well, I shouldn't start talking too early,
now a question came.
So the question is, why is the error bar larger for female,
even if more female were present in the data set?
So I just trust you
whether they are more female than that data set,
which I don't know out of my mind.
So suppose that's the case,
the reason is that we're having an upper boundary at one, right?
So there are two reasons.
First of all, as you see at the variance
of the Bernoulli distribution,
the variance is the success probability
or like probability of saying yes,
times one minus that probability,
which is highest when we are having probability of 0.5
and lower as we go to the more extreme.
So that could contribute here to that uncertainty.
And the other could simply be
that there's less variation across males
than there's across females.
And as a result, the average effect that we're seeing here,
like average, quote unquote,
let's treat it like that for a second.
For the average across females and average across males,
there's the average across females is more certain
because there's less variation across female participants
than there is across males.
Both mechanisms could apply here.
I suspect it's probably the second.
So what we have to do,
we would have to check whether the variation
in these effects is greater females than it is in males.
And perhaps that could explain that behavior.
Yeah, thank you.
And that was it for me.
Okay.
So there's another example where we're actually analyzing
response times in the interest of time.
I'm going to only go quickly over that
just to give you an idea what we could do.
So for example, here in this diff and this rotation,
dataset we're interested in
modeling the ability for people to mentally rotate an object.
So we see two objects.
One is mentally, one is rotated against the other on the screen
and we have to mentally rotate it back in order to determine
whether those two objects we see on the screen
is in fact the same or not.
And this task is more difficult.
The stronger the rotation is of one object against another.
So we have three rotation degrees, 150, 150 degrees.
And what we are seeing here is the time and seconds
that people take in order to respond.
And RASP here is coded one if they're responding correctly.
That is, if they're correctly saying it's the same object
just rotated if they're correctly saying it's not the same object.
And so RASP one would mean people have batteries
ability in doing that task.
And if they do it in with lower response times
that of course the button.
And we could analyze the dataset for example
with Wiener drift diffusion models,
which is kind of cognitive models that are quite involved,
but it could also be used in IRT.
So here we would have four distribution parameters.
The most important is here denoted as V.
That's the drift rate.
That is whether we are drifting,
in the positive direction, which in this case in our example
would be towards the correct response
or drifting towards a negative response
by where V would be positive,
would be drifting towards the correct response.
We would have the boundary factor A,
which would basically tell us how far kind of the mental boundary
is of people until they are threshold is until they decide
to make it a same object.
So they're accumulating evidence.
They're mentally rotating, mentally rotating.
At some point say, okay, I think it's the same object or not.
And then at some threshold they're saying
they're making the decision
and this threshold may vary across people.
Some people may be more cautious
where their boundary separation would be bigger
than other people.
And then the third parameter that is here of interest
is the so-called non-decision time
that is related to all the time involved
that is not about the mental process that we care about
but about other things such as actually the visual process
of seeing the object or the motor response
of initiating the response,
which is not our cognitive process that we are interested here.
So all of them would go into non-decision time
and what we interested here
that would actually have a drift would be the mental process
that decides what we are choosing.
And so whether we're choosing,
it's the same way it's the other object or here,
whether it's then our correct choice or the incorrect choice.
So very quickly, we would model the drift rate,
which is the main parameter in that model
by both person item parameters.
And we could also model the boundary separation,
BS, and the non-decision time
as varying across persons and items
because people may be more or less cautious
having different boundaries.
So we could also model the boundary separation,
because people may be more or less cautious
having different boundaries separation
or it could be more or less quick
in non-decision relevant task,
which would affect the non-decision time
and similar items would also induce
like different drift rates, different boundary separations
and different non-decision times,
depending on how the items look exactly.
So we can model that with this distribution of framework
and then we could look at the results
and we can see that Mu here is the drift rate.
So we see that for rotational degrees of 50 degrees
as compared to 100 or 150,
people have higher drift rate
that is they are drifting quicker
to the response,
which is one which in this case indicates a correct response,
which basically indicates
that rotation of 50 degrees is easier
than 100 or 150 degrees
doesn't really make a difference.
In terms of boundary separation,
we don't really see that there's any effect
of the degree of rotation.
So it doesn't really like matter in that case,
but we see the non-decision time
is actually way quicker for the lower rotation.
So apparently there are aspects of that task
that are according to the model,
not related to the kind of the decision process itself
that make those rotation by 50 degrees in some sense easier.
Whether that's actually like cognitive modeling-wise,
dull or not,
or rather an artifact of the data I can't fully tell.
Okay, so to summarize,
and then you can also ask questions about the last example.
So when you're interested in learning more about BRMS and STAN,
you can check out the initial help page of BRMS.
You could look at the set of vignettes that BRMS has.
When you've fitted models
and want to learn what kind of post-processing you want to do,
you can use the methods function with a class BRMS FIDGE.
BRMS has a website that you can use.
STAN has a website too.
What I should have added here, we have a forum,
which is the STAN discourse forum.
So if you're Googling STAN discourse forum,
there you can ask BRMS and STAN related questions.
Please don't ask BRMS related questions as BRMS issues
because they don't reach such a wide audience as in the STAN forums
and also there are way fewer people to answer your questions.
So please ask questions at the STAN discourse forums.
If you like, you can also contact me via email or tap me on Twitter.
If you have specific BRMS related questions, again,
I would like to ask you to ask them in the STAN discourse forums.
There are a lot of people active and a lot of people very willing
and able to answer your questions.
Thank you so much for being here and I'm now happy to
and now answer all the further questions you may be having.
