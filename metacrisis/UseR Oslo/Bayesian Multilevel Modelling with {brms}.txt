Okay. Thank you very much for inviting me and thank you to all participants for being here.
It's a pleasure to be here and I'm very much looking forward to talking to you a little
bit about Bayesian statistics and about BRMS. So as said earlier, I'm happy to take questions
kind of in between. I'm not going to monitor the chat, but like I will be notified and I will
kind of give at the right moments kind of the possibility so that these questions
can be answered. In my talk today, I will kind of mix it up between some ideas of Bayesian statistics
and then how to realize that in BRMS so that the R code is just not out of context but kind of
embedded in a context of where it hopefully makes some sense. Okay. So let's start and
I'm not really sure what it is with Bayesian statisticians that whatever they do that kind
of have the feeling that they need to introduce the Bayes theorem. So in that tradition I'm going
to do that as well. And so basically the goal of Bayesian inference which is part of like Bayesian
statistics is that we estimate the posterior distribution that is the distribution of the
parameters which are called theta given our data which I call y here. So it's basically the distribution
of the unknowns, our parameters given what we know or have observed our data.
And that kind of in this ideal sense summarizes all the information we have about the parameters.
And they are like in classical Bayesian statistics two primary sources of where that
information about our parameters can come from. One is our data itself. So that enters
the posterior through the likelihood which is this distribution p of y given theta on the right
hand side. So it's basically the distribution of the data given our parameters and it's nothing
else than what we optimize a maximum likelihood estimation. So it's kind of if we were just doing
maximum likelihood estimation we'd just use the likelihood. But in Bayesian statistics kind of
we multiply that likelihood with another term which we call prior so p of theta which is basically
the distribution of the parameters before having seen the data. So there we can enter all sorts of
prior information from prior experience, from expert knowledge, from whatever that doesn't come
from our data directly. Then we combine those two sources via multiplication and to make sure
that the resulting function is like actually a distribution that is our posterior distribution
we normalize using the margin likelihood p of y. The marginal likelihood kind of
gets us into trouble in a lot of cases but we don't need to talk about that here today.
So I would like to kind of slightly rethink the Bayes' theorem and first of all drop the margin
likelihood so that the posterior is just proportional to the product of likelihood and prior.
And this product of likelihood and prior is nothing else than the joint distribution of
y and theta that is the joint distribution of all quantities we have in the model.
So what we basically do when we're doing Bayesian modeling and we're building a Bayesian model
we're writing a joint model of all quantities we have or like think are interesting in our
specific use case some of which are known or observed data some of which are unknown parameters
but kind of the difference in the sense of what's known or what's unknown is kind of fluent. So for
example if we have missing data some of what was originally supposed to be observed suddenly
isn't observed anymore so becomes parameters. So in Bayesian statistics in this thinking of joint
priors what is parameters what is data what is known as unknown kind of becomes more loose
and we can think of it more generally as this joint model of everything we have or everything
we care about and then just hope we have enough information in our model so that we get some
reasonable information out of that for our unknowns that is for our parameters.
And so I want to briefly start with some like personal list of advantages and disadvantages
of Bayesian statistics there may be more there may be less so that's kind of just my my personal
list. So so I think we have like quite a few advantages one is that we have natural approach
to expressing uncertainty and we express uncertainty using probability that is basically all uncertainty
there is in Bayesian statistics we can ideally express using probability which I think is quite
intuitive. We have the ability to incorporate prior information if we have prior information
and if we want to incorporate that prior information using the prior distribution.
It's not necessarily easy to kind of take expert knowledge and put that into a prior there's a
whole field called prior licitation that goes into that detail but in principle this is possible
and if we are able to do it it's really helpful. We have an increased modeling flexibility that
is we are able to specify and fit much more complicated models that is models with more
parameters than would be usually possible with other approaches. We get the full posterior
distribution of parameters that is we get the full distribution of them instead of
the point estimate and some approximate measure of uncertainty such as the standard error in
classical statistics so we get more information out for our parameters and I think the last
point is perhaps one of the most nice ones is that we have a natural propagation of uncertainty
that is if we are interested in a quantity that can be computed based on our parameters.
We can take the posterior distribution of the parameters to obtain the posterior distribution
of whatever quantity we're interested in so suppose we have two parameters a and b and we
are not interested in a and b but rather in a plus b or a times b or a to the power of b whatever
we can basically take the machinery of Bayesian statistics to propagate the uncertainty from a
and b to our new quantity let's say a plus b no matter the transformation we do so that's a quite
quite powerful property that I think like one starts only to fully realize how nice it is once
actually working with it and I think there's one disadvantage it's better it's a major one which
is a slow speed of model estimations so instead of like taking fractions of a second or a second
or a few seconds things may things may take seconds or minutes or hours or days if we have
lots of data in very complicated models so Bayesian estimation is much much slower at least the
current ones we are mostly using and this is the main drawback which means if we have some
let's say real-time application probably we wouldn't use kind of the full machinery of
Bayesian statistics or if we have rather simple models but really big data let's say like linear
regression with millions of billions of observations right then we probably wouldn't use Bayesian
statistics because it just takes too long a few millions we can handle a few more we may if we
have like lots of computing power but beyond that it probably becomes too much in if we if we are
using kind of more exact estimation algorithms rather than approximation okay so before I start
with a software part are there any questions so far yes there was a question coming in just now
um these advantages you talked about advantages compared to what
yes um advantages compared to like um
freq like what what I call frequent statistics so basically usually represented using some sort
of maximum likelihood estimation and what are people called classical statistics yes
another question was that came in was uh if even MCMC is slow yes yeah from my own experience I can
confirm this uh and the last question was could you briefly comment on variational methods to
speed stuff up yes um okay so so variational methods are a kind of way to approximate the
posterior using some fixed family of distribution so suppose we could be trying to approximate our
posterior using a multivariate normal right um so that's that's kind of one standard approach
to that but there are others um so I don't like them much because I care about the posterior
distribution and the the the dependent structure of the parameters um and usually kind of the
for reasonable complex models where we want to use Bayesian statistics our posterior distribution
is quite far away from the family of distribution where we are usually using in variation inference
that is usually some sort of multivariate normal distributions and as a result our the the the
variation of the approximate posterior we are obtaining using variation inference is usually
quite bad for the models I care about so so but but that's there's my personal my current
personal opinion on that it may change in the future and it may change depending on the model
for instance if we were to fit Bayesian neural networks which is like a completely different
topic not not part of today then we may have no other choice because MCMC doesn't work on those
this amount of data and and model complexity or it doesn't doesn't work with with the years of fitting
time and so there we may want to use variational um inference but then we only care about predictions
and it may be that like in a lot of cases variation inference of of predictions is well
calibrated and works well but the variation inference for the posterior doesn't work well
in neural networks I wouldn't care because I don't care about the distribution of the neural
network weights but in Bayesian statistics where I'm going to interpret the posterior and its
parameters because the parameters usually have meaning I care about accuracy in the posterior
distribution and that's to my current knowledge usually not sufficiently um realized using
variational methods um what are more questions just a sec yes there was a thank you to the answer
just now and will we learn about how to approach a speed up of the estimation or where we can read
up on it that's a really I mean that's an open-ended topic
so it's not part of today because um but I'll I'll see if I can address it kind of
in the end if you remind me of that because like that that's like a quite big question and I don't
want to spend too much time on it now um but I'll talk about that in the end if you remind me of it
yeah I'll make a note thank you thanks good okay so let's let's start a little bit with the
software part and there's lots of Bayesian software and I'm just going to focus on two
one is of course BRMS kind of the the name and the the title of this this talk and the other one
Stan so Stan is a probabilistic programming language that is a programming language justice
are justice this python or whatever but focusing on writing down Bayesian models
and kind of there are multiple probabilistic programming languages existing but Stan is like
the one I'm using I'm part of the Stan development team and it's has kind of kind of currently I think
the best implementation of the of Hamiltonian and Monte Carlo sample as we currently have in
the world to my knowledge but like things are in flux so they make that might change
and the fact I'm talking about Stan is because BRMS uses Stan behind the scenes so BRMS is basically
an R interface to the probabilistic programming language Stan so if you have ever used BRMS I
will use in the future you will use Stan behind the scenes even if you may not necessarily notice
that you do so I don't want to talk about Stan in detail but I just want to show you real quick
how Stan the Stan language looks like on a simple example of linear regression
so in Stan the Stan program consists of several blocks there are more than three but the three
main ones are data that is where your data comes in that you kind of pass to Stan parameters that
is the the variables you're going to estimate and model where you basically put data and parameters
together so that the data informs the parameters through your model if we are to like specify
linear regression we need a couple of data for example we need the total number of observation
which I'm calling n here and it's an integer with lower value of one because we would need at least
one observation to do anything sensible we need a response variable that is the
variable being predicted I call that y it's a vector of length n we need a number of regression
coefficients I call that k and we need a predictor design matrix which basically
contains our predictive values as columns and kind of in the columns are the predictive
areas in the rows are the observations so that's one way to realize linear regression
as parameters we of course have the regression coefficients I call them b and we have some
residual standard deviation which I call sigma it's the real value with a lower boundary of zero
because standard deviation needs to be positive so what you see here is then a statically typed so
you have to define which type each variable is before using it which is kind of which you're
probably not used for mar because in r you can just assign everything to everything and it usually
works instead you don't but the advantage is you can really read through the stand program
without having seen the data because everything is statically typed in the model block we put
stuff together so first we are creating our predictor term which I call mu simply by x
matrix multiplication b so that's our predictor term and we specify a prior for example on sigma
so this is our prime formation of sigma with an exponential prior and rate 0.1 I just chose that
for an example and then in the end the last statement is basically our likelihood that is
we call in linear regression y is normally distributed around our linear predictor mu
with a standard deviation sigma and s r stands vectorized because like y is a vector and mu is
a vector and sigma is a real value so by y not distributed norm and mu sigma we're basically
telling stan to do that for every single observation and you will see that like in
lots of aspects stan is actually also among others inspired by r okay but of course like
topic of today is not to fit models in stan so we're going to use one high level interface to
stan which is brms so it stands for bayesian regression models using stan the idea is that
we specify bayesian models via an extended r formula syntax so whenever when you are familiar with
our formula syntax you can use that in in in brms as well and I've extended it in various
ways so that it's more powerful for the stuff that brms can do we internally write the stan code that
is readable yet fast that is you can just use brms to create stan code and look at the stan
code and go from there um but of course brms then also fits the model so you don't need to look at
the stan code if you don't care about it so whenever I I need to kind of um have a balance
between readability and speed of the of the sampling I'm focusing on speed that is there are
parts of the stan code of brms that is less readable because it's it's more efficient
to sample for me or to fit the model basically um brms provides an easy interface to defining
priors perhaps I would say it provides an interface for defining priors and whether it's easy or not
certainly depends on the perspective um and one other important part is that facilitates
post-processing that is it's not only about estimation of the models but to create summaries
to create predictions to create nice graphics to create model comparisons um whatever you are
interested in in the end we have some like nice functionality for post-processing
so that you don't have to leave brms to do all sorts of of things you may want to do
after having fitted your models um and so just to give you an idea of what happens behind the
scenes with regard to stan code suppose we have some more complex like not linear regression a
little bit more complex model um then this is the kind of stan code would be that that
brms generates I'm not going through the details I'm just showing you there's a lot of stuff going
on so that's page one data and some transform data then parameters and some transform parameters
and then like lots of models and some some other stuff so um this kind of if you were writing this
kind of model here it's a simple multi-level model yourself and stan it wouldn't look exactly like
this but like brms writes it that way because it's it's easier to write it machine kind of wire
machine that way um and it's more efficient but I want to show with that is kind of when you fit
more complicated models and stan and want to make them efficient it may get a little complicated
but brms does all all kinds of things for you if you're staying within the framework of models
that brms supports okay so before I continue with the next topic um are there more questions
there were no questions great okay so so now I want to like go through some
some ideas that I follow when I'm analyzing um when I'm analyzing data and I'm going to
to use brms to kind of showcase the situation so first of all and this is highly related to
multi-level models is we should think about the data structure that is we should think about
how our data is structured and how this structure should be reflected in our models that is for
example if I have multiple observation from the same class of students that we have observed
multiple students from the same class those students will be dependent on each other because
they're coming from the same class they're sharing the same class from the same teacher
and so on so I want to reflect that structure in my models um and of course we could go further
if we had observations from multiple classes within the same school right those classes were
dependent on each other because they're from the same school and so on so basically in most
observational and in or in nearly all observational and in most experimental data we have some
advanced data structure some hierarchy some complicated dependent structure in our data
that we want to reflect in our models and one way to do that is to use multi-level models so I'm
going to illustrate that using a simple example um so this is an example I like it's the sleep
study data set from the lme4 package in r and it's basically about the effects of sleep deprivation
on reaction times and here we see kind of a selection of 18 subjects and we see how the
average reaction time um changes over 10 consecutive days from 0 to 9 where we had
nine days of consecutive sleep deprivation where the participants slept only three hours or less
per night and if you ever like have have slept that that few hours per night over multiple nights
you know you're getting very tired and for most most of us um reaction time becomes slower for
instance subject three and eight the one on the top left it kind of it starts at 250 milliseconds
an average of the first day and it kind of ends at about 500 at the very end so it they became quite
quite slow through those nine days of sleep deprivation but there are huge difference differences
between subjects um sample subject three and a nine is quite quick at the start and doesn't seem to
be affected by sleep deprivation at least when measured using a simple reaction time task and
they are kind of all sorts of in between subjects and of course the structure of the data there is
multiple observations per person is something we want to reflect in our models to accurately
account for the data structure to accurately account for the uncertainty to have some nice
and well calibrated estimates no matter if we do that in Bayesian or or Frequentist or whatever way
so let's ignore the structure for now and just fit a linear regression model for example we could
like fit that linear regression model on each of them and would get like a linear regression for
each of them but of course we could also say I don't care about the individual subjects all I
care about is kind of the kind of the average reaction time increase through the the days of
sleep deprivation so I could in other words just fit a linear regression model ignoring
that we have multiple observations per person and basically just say we have like in this case
180 uh conditionally independent data points without recurring to the fact that some of those
data points are from the same person so let's start to fit a simple linear regression in BRMS
so after having loaded BRMS in R we use the BRM function as the general interface to to BRMS to
Bayesian multi-level models so whatever model you specify and we want to fit it use the BRM function
and we are as said using our formula syntax that is our response variable is on the left
hand side of the tilt in our predictor variables on the right so in this case reaction is our
response variable and we're going to predict it using the days of sleep deprivation and as data
said we're using the sleep study data and of course this completely ignores the multi-level
structure for now just to get started and then after having done that we can do a lot of stuff
and one the method I want to highlight here is the conditional effects methods
it basically provides a graphical summary of the relation of predictor and response variables
and what we see here is that kind of on average kind of we start with like about 250 milliseconds
at day zero and then go to roughly 350 or whatever at the the ninth day of the tenth day rather
but we of course completely ignore the multi-level structure there's the the person dependence
in our dataset so let's not make that a mistake and correctly account for this dependency instruction
in other words we're going to fit a multi-level model with one grouping factor which is the subject
the data observation belongs to so I want to like specify now a very intercept varying slope model
that is I want to specify that the intercept that is the the y-intercept of the regression
varies across subjects and the slope with regard to days varies across subjects so we had seen
that this makes sense in our in this model in this graphical representation here that people
started differently at day zero and they also reacted differently to sleep deprivation over
those consecutive days of sleep deprivation so we want to model both as varying across subjects
and the the syntax BRMS uses is in this simple case identical to the syntax in the lme4 package
or for that matter in a lot of other multi-level packages in r that is if you want to add those
varying coefficients across a grouping factor you're writing stuff in brackets and you're
you're separating in those brackets the predictors one plus days so one is the intercept days is the
kind of slope of days by the grouping factor which is subject by this pipe vertical dash thingy
so the whole formulas basically we are predicting reaction time using a global intercept one plus
a slope of days also global plus some subject specific intercept and subject specific slope
of days we could if you're familiar with our formula syntax we could also just ignore this
one plus and then if we would still have an intercept implicitly I just wrote it down here
to make it very explicit that there are intercepts being estimated so once we have specified the
formula we can again pass that to BRMS fit the model stand you stand as used behind the seeds
and then we're going to get kind of this plot which again still tells us the average effect
of steep deprivation across over the days on the on the reaction time but this time we have
accounted for the fact that each 10 observations are belonging to one subject so we have accurately
accounted for the data structure and that's our uncertainty estimates which are represented here
as 95% credible intervals in the gray lines are more uncertain because we accurately
that took the uncertainty structure in the of the data into account
okay question questions yeah can you exclude the global intercept and the global slope
I mean yes you can but usually I wouldn't recommend it
so so the the random or the random or varying intercepts and slopes are generally the varying
effects are generally sense most sensible if there's a global intercept and slope present
but syntactically you can exclude them yes and another question came up
is the data object given to BRM structured similar to the list given to our stand
no it's a data frame so it's a data frame where your your columns are your variables
for instance reaction and days here and subject and the rows are your observations so BRMS expects
data frames or will transform everything you pass there to this kind of data frame structure
then another question was what sorry what are the distributional assumptions for the residuals
global and you know here now that we didn't specify anything we assume that there is a kind of
the likelihood is normally distributed so basically we have like normally distributed
errors but of course we're going to change that I'm going to illustrate that in a second
so BRMS supports a lot of different different likelihood options or prior and prior options
but default is is normal likely and the final question was is there a function to provide
the raw stand program executed yes scenes yes so if you're fit the model you use the
stand code method one word and to extract the stand code and if you just want to get the
stand code without having to fit the model you're using make underscore stand code
wonderful that's it great okay so now that we have thought a little bit about data structure
let's think let's think a little bit about distributions and so let's think about the
distribution of the data first so we should think about the likelihood in other words
so I'm using now another example which is the kidney data set it's basically the kidney data set
is in part of BRMS and it basically the models that all contains data about the
recurrence time of kidney infection in like a couple of patients so time is our recurrence
time until this kind of infection occurs again and we have multiple predictors two of which are age
and sex the details don't matter too much here but what matters of course is that our response
variable time kind of can by nature only be positive because it needs to be the time that
the recurrence of the disease needs to occur after the disease occurred first so time must
be positive and but let's suppose we ignore that fact and predict time using age and sex
using family Gaussian that is using a Gaussian or normal likelihood we then fit the model and
then evaluate the fit of the model to the data and one way we could do that is to use
so-called posterior predictive checks or short pp checks what that basically what they basically do
is they compare the distribution of the response variable y of the observed data to the to the
predictions of the model so so in other words what the model would predicts in terms of response
variable after it has been fitted and as a kind of rule of thumb if a model is even remotely reasonable
the the marginal distribution of the observed responses y should be similar to the predictive
responses y rep of the model and if that's not the case there's probably a big problem in the model
because it doesn't even predict the response variable marginally right and so suppose we were
ignoring the fact that time is positive but still using a Gaussian likelihood and then we're kind of
using a posterior predictive check what we see here is that the the the model doesn't fit at all
and in particular we see that the model actually makes negative predictions so y rep is the
are the the responses predicted by the model and a lot of which are actually negative so we have like
we have like 10 repetitions of that of that ecdf of the community of distribution function line
to reflect uncertainty in those predictions and we see a lot of those predictions are actually
negative which of course cannot be because time only needs to be positive and as a result also
because that model predicts so badly we also shouldn't trust its inference so we shouldn't
trust its coefficients on age and sex so we shouldn't trust inference on other things
because the predictions are so off so if we had a model that like fit the data well it
it didn't mean this model would be great but at least it wouldn't be as awful as as our Gaussian
model on that time data so we should think about what likelihood we should be applying and let's
use something else for instance we could choose family gamma lock so we we could use a gamma
distribution for our time data with a lock linked to make sure predictions are positive
there are lots of different choices in brms and you can check the documentation but that would be
kind of a rather simple example and if we do that and fit the model again and look at our posterior
predictive check we see that now the predictive responses overlay the observed responses quite
nicely that doesn't mean this model is great but it doesn't mean it's not totally awful so we should
think about what kind of distributions we are applying to our data okay that was a quick question
about pp check of the function with pp check can we measure how good our predictions are
what what does it mean how good our predictions are they are like a lot of different metrics
that's not specified okay so then i'll continue but once it's specified i'm happy to answer it
because that's that's like a very big question it's a good question but it's like right now for me to
take turns i'll make a note good um of course we should be thinking about the likelihood that is
the distribution of our data but we should also be thinking about the distribution of our parameters
before having seen the data in other words we should be thinking about the prior so let's assume
we are kind of generalizing the model we just had but now specifying a prior distribution on our
regression coefficients so i'm using that doing that with a with a prior argument in brms
and by default if i'm just specifying a prior i'm going to put that on regression coefficients
and here i chose as a prior for the coefficients of age and sex a normal distribution with mean zero
and standard deviation 0.5 right um so intuitively i would say that looks quite
quite narrow so so we're kind of very narrow in what we expect about these coefficients
but when we now go ahead and actually check what kind of data the prior implies things get really
awful so in other words we're doing a prior predictive checks that is we're not looking what
the model predicts after having seen the data so after having obtained the posterior but we're
doing prior predictive checks that is we're just looking what the model would predict based on the
prior without having seen the data or before having seen the data and um here's like one example of
how that's plotted so on the x-axis is the is the log observed response log y and on the the y-axis
is the log prior predicted response y-wrap so remember this is on the log scale and what we
see is basically that um that y-wrap is around like four orders of magnitude bigger than the
observed y like so it's it's completely off it's we are predicting way too big things although we
actually thought that we were using kind of only very narrow priors and the reason is basically that
we are working on the log scale because we were using a gamma log model that is after
exponentiating things explode quickly and what is more h is specified in years so coefficients
of 0.5 um with an h range of say let's say 10 to to 80 years suddenly makes a really big difference
um and so the point I want to make here is that we should be thinking about prior distributions
when we when we hope when we know or hope that the data contains enough information
to get reasonable estimates then we can go with the default price on brms and we don't care for that
but if we have very complex models and we have a feeling that there isn't enough information
in our data to inform the posterior distribution or if we explicitly have prior knowledge we want to
incorporate into our into our model we need to think about the prior distributions and in in
models that use anything else than kind of the linear structure specifying prior distributions
that are reasonable is really really hard so that's like an active the active area of research
then I'm also participating in on on like how to how to specify prior distributions appropriately
and most importantly intuitively so that people without like having being expert statisticians
being able to specify reasonable price okay um question more questions so far yes um there was
a follow-up to the pp check question um more precisely is it a tool to check model predictions
roughly and and it's noted that the person shouldn't have used the word measure uh
interested in goodness of fit measurements yeah um so it's so it's it's a rough graphical
um graphical visualization and you can use different types of the posterior predictive checks
most posterior predictive checks are marginal that is they they are comparing the marginal
distributions note that the marginal distributions may be quite similar although kind of in on an
individual level predictions are super off so it's just like first rough check if things go
roughly well and when we want to move forward we need to actually compare predictions on an
individual observation level or on groups of observations or we need to like on that basis
compare different models and see which of them predicts better and then there are also other
aspects such as on what what data we are predicting for example by default posterior predictive checks
do in sample fits that are basically checking predictions of fit to data we have used to fit
the model on and it's easy to like get perfect in sample predictions if we include kind of as many
predictors as we have observations in the model so if we really care about like predictions we
should be caring about out of sample predictions that is let model predict data it has not seen
before and all of that things are possible in brms and we could tweak the pp check method to also do
that by specifying new data but i am using pp check as the first rough check and everything else
then is used by means of other methods it's done by means of other methods thank you there was one
more question does brms use simulation like mcmc or numerical approximation yes i should have said
that earlier i'm sorry for that yes it uses mcmc so stan has multiple algorithms but most most
importantly and like used 99. something percent mcmc specifically hamiltonian like some adaptive
version of hamiltonian and montecarlo and brms also supports the variational inference algorithms
supported in stan but as things currently stand i wouldn't wouldn't use them because they are
to my understanding right now not reliably enough for complicated enough models so it's always using
mcmc sampling that is is using samples to represent the posterior distribution and for example if we
look at this posterior predictive checks or rather in that one we see here we have those 10
light blue lines of of kind of predicted responses and each of those light blue lines
corresponds to one posterior sample here we have plotted 10 10 posterior samples but by
default we're going to have 4 000 of course the user can change that but we're not going to plot
4 000 lines in that plot so we are restricting ourselves to 10 here
more questions just a small clarification question just came in in general
why as in the letter y is the data and theta the parameter right yes
okay thank you i'm very happy to get so many questions usually i have to like
force people to ask questions but not today that's great um so next thing i want to talk
about is some extensions of standard multi-level r formula syntax that i incorporated in brms to
make it more powerful so in brms we can what i call add a kind of some addition terms to the left
hand side of the of the formula that is to the left hand side of the tilde um so for example
if we have censored data that is kind of censored some observations are censored if we have just
observed them to a certain point and like we but we didn't know when actually kind of some
a certain event occurred so for an example that was barely explained so let me explain again so in the
in the kidney example some participants kind of initially had the kidney disease but then they
didn't have a recurrence of the kidney disease so during the whole time of the study they didn't
have any recurrence um they may still have that recurrence later on in time but we had to stop
the study so we only know the recurrence didn't happen in the time interval of the study but it
may have happened later on in other words our some of our observations are censored because we only
know the event in this case the recurrence time will happen in the future but we don't know when
exactly so this is additional information on our response variable they are different kinds of
additional informations for instance censoring here waiting truncation certain other kinds of
things we may want to use they that all complement the information in the response variable itself
and those kind of pieces of information you can pass in brms using this what i call addition
term syntax so after the response variable you're using this pipe thingy this this vertical dash
um then you're specifying the function that you want to use here for censored data we're using
the sense function and insert in inside the censored sense function we're specifying the
variable that contains the relevant information in this case about censoring and in the kidney
example this variable is called censored um so for example um for for our purposes relevant
here is that some observations are right censored that is we know the time the the the recurrence
of the kidney disease didn't occur until a certain time but it may occur later on in time and so
there are different kinds of these addition terms but all of them can be specified on the
left hand side of the formula following an initial pipe after the response variable
okay uh are there any immediate questions
great with regard to that topic already no questions at the moment good okay um
then i want to dive kind of into a different area of of brms which is the modeling of unknown
non-linear functions so what do i mean by that um so sometimes we have if we have like modeling
non-linear functions we have a quite clear idea of how the functional form looks like
and then we could also specify that in brms but i'm not going to talk about that today because
that goes into too much detail but in a lot of cases we just assume that there is some non-linear
relationship between variables but we don't know the exact functional form instead we want the model
to find out itself how this not how this non-linear functional form looks like
so we can think of that in terms of this following simple equation so y our response
variable is equal to f of x plus epsilon where epsilon is some error term and f is some non-linear
function and x is our predictor and and if we we model unknown non-linear function we don't
know the functional form of f but instead we have to model have to let the model figure it out itself
how f exactly looks like and there are various approaches to that two very general ones that
are supported in brms are splines and Gaussian processes so in splines very briefly we represent
our non-linear function f of x using a sum of basis functions bj which could be like a lot of
different potential functions each time multiplied by a regression coefficient and if we just add up
enough of these functions this case uppercase j of these then we make the spline flexible enough
so that it can fit kind of a lot of different smooth functional forms of course it's quite
prone to overfitting because it is potentially so flexible so we need to make sure that those
coefficients beta j that correspond to our basis functions kind of are not arbitrary but are in
some sort of sort connected and regularized so that they kind of don't blow up so what we do or how
we can represent splines that's one way to represent is that we say in a Bayesian setting
those those coefficients beta j come from a joint distribution with some hyperparameters lambda
so all of them are coming from the same distribution and those hyperparameters lambda
also estimated from the model so in a sense we can represent splines actually as as multi-level
terms um this is the internal representation used by brms and those regularized splines
using this kind of multi-level structure make make sure at least help that we don't overfit
too much using splines so splines are really complicated and I wouldn't claim that I understand
all of the details of splines but they're really good packages in in our most notably the mgcv
package that handles splines quite well and brms uses mgcv behind the scenes to set up all the
mathematical stuff with regard to splines but all what the user sees is if you want to fit
kind of this non-linear function of x using a spline all you do is wrap x in s so basically
y till the sx plus additional terms means let y be predicted by a spline of x there are lots of
different options of splines like the default ones um is just s of x um the the second kind of
very general approach to non-linear functions are Gaussian processes and it's it's kind of an
abstract thing that we we could say that a Gaussian process is a prior across functions
whatever that means so we're basically saying that kind of our our function values come from a
multivariate like a very big multivariate normal distribution with a mean zero and some
a function that defines our our covariance matrix of those function values and there are
like lots of details of Gaussian processes not the topic of today but if you want to fit
Gaussian processes or alternatively also approximate Gaussian processes that have some other advantages
and we can just write gpx and then we're going to fit a Gaussian process of x to predict y again
different options you can specify there and you can look in the documentation of the gp
functions to see some of these options paul that was a quick question uh about the splines the s
splines uh is the number of knots in the splines defined using the mgcv cross validation um it's
so it is it is defined using mgcv so whatever mgcv uses as a default
spline as default number of knots and as default knots for the given spline basis function this
is what brms will use thank you by default you can change it just another question does brms
support a smoothing spline model with random smooths across groups
yes absolutely so um yes ish i'm going to elaborate so i understand the question
in the following way so if we go back to our to our linear multi-level model here so we had like
a kind of varying linear effects for each subject but of course we could also say
each subject should have its individual their individual spline right so and that kind of we
are kind of using a multi-level spline model i understand that way if that's if i understand
incorrectly please correct me afterwards um yes that's supported there are there is a certain
spline basis function mgcv that does that but right now it's super super duper slow in brms
because mgcv creates really sparse matrices and sparse matrix algebra and stan is not yet very
fast in fact i'm using dense matrix multiplication still here but um kind of help is underway uh
in some of the newest updates of stan we will have much more efficient efficient sparse matrix
algebra and then i hope at least that also those multi-level spline models will be much faster in
brms there was just one more question here would a multi-level Gaussian process model
require partial pooling of the covariance matrix parameters or are you describing all
Gaussian process models as multi-level okay i didn't intend yeah okay so i didn't intend to
describe Gaussian processes as multi-level models here of course they can be seen that way in a
certain kind of kind of way but um so so with regard to the former part of the question
a brms doesn't support kind of multi-level Gaussian processes in the sense that we have
partial pooling over the covariance matrices i have thought of that but i didn't get a quite
nice specification of that yet i think there's an issue on brms GitHub for that so that's not
yet supported there we have this kind of partial pooling over multiple Gaussian processes because
i'm i'm not yet sure about the the right notion of such kind of model not sure if that answers
the question but i'll try to do for now uh one final question here what is the default basis set
for smoothing splines in brms cubic splines or thin plate splines thin plate splines mgcd
uses thin plate splines by default and that's what brms uses as well
uh poll this is a good time to take a 10 minute break we've been at it for about an hour now
that that's good so um yeah so welcome back we just had a quick break if you're watching this on
youtube uh feel free to continue enjoying the talk
polio muted
thanks okay let's try again now can you hear me yes yes good um paul that was a quick question if we
can throw that in um dust brms currently supports survey weights for example pizza weights if support
weights so if you have weights from somewhere you can weight it um but i mean it goes into a lot
of detail about survey literature um i personally prefer instead of weight i prefer using a post
stratification um which is basically realized through multi-level modeling um and we have
like to illustrate how to do that in brms in a complicated setting we have like a paper that's
actually um like not specifically about beijing methods but you can see all the details there it's
um you can see it on my my web page it's from marta kuricinska and it's about um trust in
political institutions in europe and if you read the paper and look at the the appendices of that
there's like a lots of stuff actually where we used uh post stratification um and in specific
parts also survey weights in brms or weights in general thank you good okay so now i want to apply
um splines like in in one in one simple example or like one actually more complicated example
which is about modeling housing rents in munich um so so basically what we want to do we want to
predict the rent per square meter of apartments in munich um as a function of the area of that
apartment and the construction year year c for construction year um and of course like we have
like at least i don't have any idea how kind of the area of an apartment or the construction year
should be related to rent per square meter especially not since we would expect some non-linear
effects of these two predictors on rent per square meter um and we also know idea how they
interact so we're going to fit a two-dimensional spline that is s area comma year c so we have a
two-dimensional spline of area and construction year and in addition in this model like our our
apartments come from multiple districts of munich and of course like um i mean munich is like a
it's very expensive regardless probably but they are some districts which are even more expensive
than others so we want to take that into account so we're adding a varying intercept to the model
using one pipe district and then we can fit that in brms um this model kind of is is already kind
of nice but i want to go one step further and not only predict um kind of the mean rent per square
meter by area and construction year but i also want to predict the variation of the rent per
square meter by area and construction year district because it's very very all possible
that for some areas like of the apartment or construction years there's just much more variation
in rent per square meter than for others so we want to predict both the mean and the variance or
the standard deviation of our rent per square meter um and i call that distributional regression
because we're not only predicting the mean or some other measure of central tenancy
of our response variables distribution but we're going to predict multiple aspects of that for
instance the mean which i call mu and the residual standard deviation which i call sigma in normal
models and in this housing rents the munix example we're going to predict both the mean
and the residual standard deviation sigma by a two-dimensional spline of area and construction
year and by each like each time a varying intercept of district so we have two varying
intercepts of district one for the mean and one for the residual standard deviation
um and if we want to make them to be correlated we have to use what i call id syntax so if we
didn't specify something just like those varying intercepts like in the fit red one model those
two intercepts would be modeled as uncorrelated but of course they may very well be correlated
and the way we realize that um is that we are not using not one but two pipes and then
putting a symbol between the pipes for instance d here but it's literally it literally doesn't
matter what we put between there we could say i don't like Bayesian statistics or i don't like
waiting for my Markov chains to converge and it would work as long as the symbol in between
is the same in this case this lowercase d is the same and when we do that that way basically those
two varying intercepts of district will be modeled as correlated even though they are appearing in
multiple formulas that is like BRMS specifies allows very complex models with multiple formulas
if we have multiple distribution parameters we're going to predict um i'm finishing that example
i'm sure there are questions and i'm going to answer them in a second um so we also fit that
model and then we are going to illustrate our splines and kind of one way to illustrate the
splines themselves is using conditional smooths um so let's look at the results of the spline on
the prediction of mu that is on the prediction of the mean rent per square meter so here um brighter
values indicate higher rent per square meter um darker values indicate lower rent per square meter
i obscured the axis because it doesn't matter here um so what we see is that there's like
particularly high rent per square meter for like um for very small apartments like below 40
square meter and therefore like very new or quite newish buildings with construction years
in 1982-2000 so that was new back then when the dataset was created but there's lots of space
especially for bigger apartments and older apartments that is like a slower lower construction
years or older construction years where the rent per square meter is comparably small
but of course we did not only model the mean rent per square meter but also the variation
or the residual variation of that and that's the spline for this so we see there's particularly
high variation in the rent per square meter for very small apartments with with of very low
buildings so on the bottom left or in the bottom right while there's comparably little variation
if we have kind of average areas with some construction years around 1960s so we had a
lot of additional information by not only modeling the mean but also by modeling um the residual
standard deviation sigma okay um yes are there questions about that
there was one question in the chat a few minutes ago and the question was is sigma square the
group level variance no um it's the residual variance of the likelihood that is sigma is the
residual standard deviation of the likelihood and sigma square is the residual variance of the
likelihood the group level variance would be would would be what i call kind of the the variance of
standard deviation in multi-level terms so in that model we would have group level standard
deviations of variances of the districts uh through those varying intercept terms but there
would be something else in the residual standard deviation sigma which is a parameter of the
likelihood so a normal likelihood not only has a mean parameter but also standard deviation
parameter and sigma is that standard deviation parameter of the normal likelihood
thank you and uh another question uh that just came what package is used for those visualizations
the ggplot or ggplot 2 rather yeah yeah that's it for the moment good um great so um
i think that's yeah so um i'm this going to be the kind of the last specific topic i'm going
to talk about and then we can decide on whether i'll just take more questions or i walk through
some more like one more smaller case study we'll see about that so one thing i um i talked about
when initially when answering question is um is kind of how do we actually compare models or
earlier how do we actually evaluate model fit or predictions and of course we can evaluate model fit
or predictions using in-sample predictions that is predicting data um that the model has seen like
that the model was fitted on or out of sample predictions that is um predicting data the model
has not seen the problem of course is how do we estimate predictions for new data that is out of
sample predictions without actually new data because we have used all the data sensibly in our model
and one kind of the the natural way to to answer that is using cross validation and in cross
validation we are splitting our data set in two parts one we call the training data that is the
data we fit the model on and the other is the uh is the test data that is the the data that kind of
we evaluate model fit on notably the test data uh was not seen by the model during fitting so it's
basically really new data for the purpose of predictions and of course um if we just do the
splitting into training and test data once our out of sample predictions may be quite biased
just because we happen to kind of um unluckily select a certain kind of training and a certain
kind of test data set so in cross validation we not only do one data split but we do a lot of data
splits um so and the first formula on that on this page basically this p y s given y minus s means
y s is our test data so the new data and y minus s is all the data not in our test data that is the
training data so when we're doing cross validation we're interested in the posterior predictive
distribution of the new data y s given all the the old or training data y minus s and to make
sure we are not just being depending on that specific s and minus s data split we're going
to do that for a lot of different data data splits um and then we sum over the results
so if we take this posterior predictive distribution of y s given y minus s take the
logarithm of that and then sum over a lot of these different training and test data splits
we are getting to some measure which which we call expected lock predictive density or short
ELPD so kind of a detailed description of kind of out of sample predictive performance and cross
validation is kind of out of scope of this talk but the ELPD is one central measure to measure
out of sample predictive performance and cross validation other measures could be kind of
mean absolute error or root mean squared error RMSE um we prefer the ELPD because it kind of
better takes the whole posterior predictive distribution into account rather than just
primarily measures of central tendency um so this kind of cross validation in specific
ways is um is automated in BRMS um the problem we have usually with cross validation is we have
to refit the model all of time so each time we have a new data split we're going to fit the model
on the new training data which doesn't matter if we have like each model like the model fits like
in one second or so but if our model takes say 12 hours to fit and we have 100 data splits that's
not going to work at any reasonable time so in other words we would like to do approximate
cross validation that is um kind of do some sort of cross validation but without actually
refitting the model and just using the one single global model on the full data to approximate
the specific kind of cross validation procedures that doesn't work well generally but it works
for leave one out cross validation leave one across validation each time we leave out only a
single observation then fit the data on a lot of observation and predict that single left out
observation and we do that for each observations and this can be approximated quite efficiently
because the gap between the full model with all observations and the the model with without one
single observation is quite small so we can bridge that gap efficiently using um important sampling
to basically have some out of sample approximately one across validation without actually having to
refit the model and this is conveniently implemented um in the loop package with the
loop function i know it's an unfortunate name i didn't came up with it um to evaluate this kind of
out of sample predictive performance and obtain kind of an EOPD estimate of out of sample predictive
performance based on leave one out cross validation so we can do that for our second model fit rent
two which which kind of gives us that output i'm not going to into the detail yet like one aspect
i want to highlight is that the second matrix we have in the output about Pareto k diagnostic
values tells us something of how good the approximation worked and if it didn't work well
we're going to get some warnings and so as a rule of thumb i'm happy with using approximations as long
as the approximation method tells me when its results shouldn't be trusted and the kind of
approximate leave one across validation we are having implemented in lieu is in the way that it
tells us very well when it shouldn't be trusted well the quick question here uh regarding uh
loop functionality yes um i believe the loop package requires you to include a log likelihood in
your stand model do all the rms models include this they don't include in the stand code but
all brms models have kind of in r inside brms have log likelihood functions that can be used
so as a user of brms you don't need to worry about it a quick follow-up from me does that also
cover the command stand r back end yes because it the back end just relates to how the model
in stand is fitted but the log likelihood functions are not defined in stand they're defined in r
so they're independent of the the standard interface then there was another question here
is it possible to use other scoring rules in brms than the log score to measure the predictive
performance um so right now loot loot for for most of the for for a long time loot did only
support elpd but there has recently been a pull request to support other um other kind of measures
for instance rms e um i'm not sure how i think this has been merged but i'm not sure if it's on
cron yet i would have to check but in any case soon it should be possible perhaps on the github
version of loot it's already be possible i'm not sure if the current interface of brms to loot
does allow that already i think not so as soon as that stuff is on cron uh and if brms doesn't
yet support it feel free to open an issue to remind me to to make this possible in brms
perfect that's it for now yeah so just one thing to add using the loot package and some functions
of that manually it's possible to have those those other measures in place already but the interface
would be kind of a lot of manual hacking um so so that should be over so that should be changed
quite soon good um okay so so individually our elpd values don't tell us much so we're going to
compare them um and so we're going to compare them using the loot compare function so we're
computing loot for fit rent one and for fit rent two we're throwing that into the loot compare
function and then we get this nice output metrics with the best fitting model on top so basically
the models are ordered by how well they fit and we see that so basically all models are
compared with the best model so the best model has difference to itself of zero so the first row is
zero and we see the best fitting model is fit rent two and fit rent one is kind of has a elpd
difference of minus 50 that alone wouldn't tell us much but we see that the standard error of the
difference is roughly 10 um so it's in this case it's roughly five times a standard error so we have
quite some evidence that the the second model including also as two-dimensional spline and
varying intercepts for the residual standard deviation sigma is the better predicting model
according to elpd and the one across from the chat again yes how to interpret good and bad
perito k diagnostics yes so everything below 0.7 is fine everything above that is dangerous
so you should aim to have no perito case above 0.7 and if you have some they are three options
first of all if they're just a few and the differences of your model are big enough you're
probably okay to ignore them please please don't say you you heard that from me you know so so that's
it's it's kind of unsafe to do that because you're ignoring warnings which may mess up your results
but of course like that's possibility if they're just a few observations compared to like a lot of
other observations but it was fine the other two more reliable options are first either do some
manually refitting for the specific observations where the perito k was high that is do exactly
for non cross validation for those vrms supports that using the relu function or the relu option in
lube and we have recently implemented another way to get around kind of doing exactly one
across validation because that's obviously time intensive and we call that moment matching so
that tries to kind of try even harder for important sampling to bridge the gap and you can activate
that in lieu using the moment on the score match argument and this is also like implemented in
recent versions of the rms um yes i have seen a lot of people when when kind of lube failed
to use other information criteria and so so lube can be seen as an information criterion just as
w a i c or a i c but please be aware if the lube i c throws hyperator case that means the assumptions
that are implicit in a i c or w a i c are also not met but those like a i c and w a c or like some
other information criteria usually won't have a diagnostic to tell you but there will still be
ill post if lube throws an option so just changing to w a i c or some other information
criterion is no solution they just don't have the diagnostic to tell you they are not reliable
but if lube i c tells you it's not reliable neither will other information criteria be
that all approximately one across validation such as a i c or w a i c
okay so um in the interest of time i would like to skip this kind of case study where i'm going
through some like iterative model building you can i will like you will have it on youtube i will
share the the slides so you can go through that um so i will kind of uh um wrap up so that we have
like more time for like additional questions if they are um so if you want to learn more about
myself you can check out my github page you can write me an email if you have brms questions
perhaps don't write me an email but but ask like in the forums i'm going to
link to in a second and if you want to check me on twitter i'm happy to be tagged as well
if you want to learn about brms brms is on github of course also on cram but it's hosted on
github primarily um since brms is part of stan itself um it's also we have the forums on the
discord stan forums um you can check out the help pages and if you don't know where to start
that the our help pages brms has i don't know seven eight vignettes and growing and you can access
kind of links to them using the vignette package is equal to brms function in r if you want to learn
more about stan stan has its own website mcstan.org and of course you can also ask
stan questions in the stan forums at this course so um that's it for me thank you very much for
your attention and um yes i'm i'm happy you participated and asked so many questions thank you
