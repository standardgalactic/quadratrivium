Hey everybody, it's Eric Torrenberg, co-founder, partner of Village Global, a network-driven venture firm.
And this is Venture Stories, a podcast covering topics relating to tech and business with world-leading experts.
Hey everybody, welcome to another episode of Venture Stories by Village Global.
I'm here today with a very special guest, Daniel Schmockdenberger.
Daniel is a civilization designer interested in social architecture.
And if you listen to the episodes of Jordan Greenhall and Jim Rutt, you will love this episode.
Daniel, welcome to the podcast.
Thanks for having me. Good to be here.
I want you to sort of give people a bit more of who you are and what you spend your time thinking about.
So I want to ask a couple of variations of a similar question and you can answer whichever one you feel most inspired to jump off from.
The first variation of it is, when you define the work that you do, what sort of threads it together?
What sort of the underlying thread that you just keep pulling?
And another variation that's looking back and another variation of the question is looking forward.
What are the current questions that you are most wrestling with?
You are spending your most time on trying to figure out and hope that many years from now,
when you were retrospectively looking back at the work that you've done,
hope you've taken a stab at or made a dent in terms of your contribution?
Sure.
I think one way of talking about the central thread is how do we best help improve humanity's wisdom,
the way that we individually and as various groups and all the way up to collectively make sense of what is important and meaningful,
what's going on, what a good designer strategy to serve what's meaningful without causing other problems is,
and how do we make choices and coordinate choices effectively towards a better world than a shittier world.
And so basically all of the sources of unnecessary suffering and particularly human induced unnecessary suffering
kind of all are intolerable to me, have always been, and there's a lot of them, right?
Everything from the animal rights species extinction factory farm stuff to environmental devastation
to wars that could have been solved for other reasons that have economic incentives
and whatever to the escalation of those with increasing population and technology and globalization and fragility
that looks like catastrophic and existential risks.
And given that there are so many that they're really unsolvable at the level of each particular instance,
I have been interested in not just how are they interconnected,
but what is underneath and driving why humans are behaving in ways that generate those problems and that don't solve them.
And so, you know, you use the term civilization design,
which is if we look at say species extinction, whether we're talking about the decrease in whale populations
or cutting down of old growth forests or whatever, and whether we killed the thing for the direct commoditization of it
or we killed the habitat land, it was because nature was worth more dead than alive.
So we had a perverse incentive situation.
We can see a lot of situations where war is great for GDP and not war would be bad for GDP.
Where sick people are better for GDP, where addiction is better for GDP,
and GDP as a whole in aggregate or particular groups.
And so we start to look at, oh, underneath lots of problems in the world is perverse incentive,
economic, but also political, et cetera, so issues associated with differential power
and the ability for some agent to get ahead at the expense of other agents or the commons.
And then everybody playing those types of games, upratcheting their capacity to get ahead at the cost of more expense being externalized.
And you're not just talking about short-term versus long-term GDP.
You're talking about this fundamental misalignment at times between what's best for long-term GDP
and what's best for long-term civilization.
Well, interestingly, what is best for long-term consideration and what's best for others oftentimes correspond?
Because I might have a situation where what seems like the rational best choice for me in the short-term
that ends up damaging the commons or others also damages my possibility space in the long-term,
because now I live on an ecologically devastated planet.
Or now I've engendered more enmity from the other groups that I won the battle but have not won the war.
And I deployed the asymmetric technology, now they reverse engineered it and are going to deploy it back.
So we have these situations where every agent doing the thing that seems like the rational best choice in the short-term
just focusing on them creates the worst possibility space for the whole over the long-term.
So this is an example of the coordination failures that are underneath most of the problems in the world.
I don't want to cut all of the trees down and I don't need to.
But if I don't, the other guy is going to, so me not doing it doesn't leave a forest.
And then he's going to use his increased economic power to beat me in some other important ways.
So now we actually race to cut it down as fast as possible.
Or so we see tragedy of the commons type scenarios or we have to race to build the AI weapons before the other guy.
So arms races, tragedy of the commons are examples of multipolar traps where no one is maliciously doing the bad thing
and yet everyone's racing to do the destructive things.
I think Slate Star Codex called this malloc.
Yes, the meditations on malloc is required reading.
Yeah. And so, you know, you asked me before we started talking how I discovered your work.
And I mentioned I went to the crypto rabbit hole and let me do the game you had a hole.
And I discovered, you know, a few years ago, sort of this literature around progress and Tyler Cowan
and his book stubborn attachments and how, you know, improving economic growth and GDP
and technological progress was the, you know, highly correlated with so many things that we care about,
you know, informatality, health care, so many great things about the world.
And, you know, I read things like, you know, Jeffrey West scale where it says that economic growth is inherently unsustainable.
You try to innovate your way through it, but, you know, you'll reach some limits.
But then I read things like Andrew McCaffey's more from less where it says, hey, you know,
we've now decoupled resource consumption or resource usage from consumption.
And I don't know who to believe or I know to believe, but who's right?
Which do we have to innovate through it? Is the only way out is through?
Or do you have to, you know, have this radical reformation of how we how we run our economy more Charles Eisenstein stuff?
And so that's my question to you, Dan. How do you think about that?
Yeah, so let's talk about limits of growth and upper bounds and that kind of question.
So obviously the growth of the materials economy in the domain of atoms is different than in the domain of bits.
And we would be silly to not acknowledge that fundamental difference.
And there's a third difference in terms of the physics of the materials economy.
The domain of atoms, the domain of energy and the domain of bits all have fundamentally different scare cities and different
thermodynamics associated different conservation laws.
So we have to have different accounting for them.
We have a finite amount of atoms of various types.
And of course, via better mining technologies and whatever and turning the atoms into molecules to better chemistry type technologies.
We can modulate that to some degree, but we also have upper bounds on that.
It's also important to realize that it's not just getting the atoms.
It's what do we do with them after the fact, which is the waste management.
So for the most part, so far, humanity's had a linear materials economy where we take resources out of the earth in a way that are not
renewing themselves, which is ultimately causing net depletion.
And then we use them for some finite period of time and then we put them back into the earth in a way that they can't actually be reabsorbed
properly, so toxicity, net accumulation.
The depletion and accumulation in the ecosystem where the ecosystem before humans doing a materials economy had closed loop processes.
The animals, the plants, the funguses, the everything in an ecosystem are made of the same atomic ingredients and can be fully recycled.
And so anything dies and it becomes food for new things.
And the food for new things doesn't try to figure out how to mine and get savings accounts in very effective ways because they don't have tool innovation.
So you get these kind of closed loop processes that is a huge part of the anti fragility of how ecologies work.
So obviously we have to move from a linear materials economy to a closed loop economy.
That's pretty straightforward where we take responsibility for closed loop on the atomic cycles.
So we aren't producing accumulation on one side or depletion on the other.
Now we can keep cycling them so we don't have a situation that once we've used it, it's gone, right?
But we do have a situation where we start moving from increased growth of the number of atoms, measuring growth or progress in terms of the number of atoms,
to that the atoms are being put into continuously more interesting forms.
The forms are defined by design that is stored in a pattern of bits.
And obviously the bits don't have a similar type of fundamental limit to them than the atoms do.
So atoms we have to think about closed loop accounting on.
Now to be able to process the old stuff and to new stuff atomically takes energy.
Energy has a different kind of conservation to it, which is that we get a finite amount of energy bandwidth every day.
We can use it more efficiently, solar and other types of things.
But we don't have an indefinite amount of energy.
So we have to be cycling the atoms within a limited energetic bandwidth.
And we have basically unlimited amount of bit pattern limited only by the energy and atom needs of the computational substrate.
But then the bits are only useful in so far as they can actually be put into form and or there is limited human attention to be able to engage them as types of software.
And so human attention becomes another fundamental scarcity that is part of this equation.
And so when people look at things like the fundamental and non-rival risk nature of software, you can't abstract that to the domain of energy or atoms, right?
Like you really have to think about these and the accounting of them differently.
The moment you think about attention as a scarce resource to compete over monetizing, everything gets evil and fucked and hurt.
As we see in the world with all of the problems associated with social media type dynamics and marketing, because monetizing other people's attention, which is obviously at the heart of the idea of marketing, right?
But not just marketing propaganda in a government way and every the desire to control other people's behavior for personal benefit, the benefit of the corporation or the political party or the whatever it is.
But if I am if I am a political party or I'm Google or Facebook or I'm an advertiser on there and I'm a multi-billion dollar kind of organization, employing tools of radical asymmetric advantage to be able to use the most effective
normal stimuli to cause addiction and entrapment to, you know, this is all the stuff Tristan Harris and those folks talk about to people.
There is a kind of asymmetric information and narrative warfare for people's attention.
And you notice that there's no definition of consciousness you can give that's meaningful that doesn't require defining attention or a definition of choice.
So attention is very fundamental to what it means to be human and what it means to have any kind of sovereignty.
So using asymmetrically powerful tools to capture and direct and split test optimize how effectively you were able to capture and direct someone else's attention is trying to put any degree of free will or choice they have under your control for a purpose that benefits you and not them.
And again, as an asymmetric type warfare.
So we have to stop thinking about the attention economy that way.
And now we can start to look at why we think of it that way and what would it take to change the macroeconomics underneath that.
So are there fundamental limits of growth?
Yes.
Can we create more efficiency to where those limits are extending but still bounded?
Yes.
And so you will get to a place of diminishing returns on the increased efficiency where we don't have increased resource per capita multiplied by increased population on a linear or an exponential curve forever will get diminishing return.
And so we do have to think about planetary boundaries in a intelligent way.
And do we do we have to think about that in a way that's sort of top debt like do we steer the market a little bit or do we have to get involved or will the market sort of figure that out on its own or solve problems on its own agency.
We don't have a market.
So every Adam Smith type arguments about the self correcting intelligence of the market even though it wouldn't have been true then is super not true.
Once you have bank bailouts and you subsidize oil companies and you have monopolistic kind of investments in military manufacturers and things like that.
So there's no self correcting nature of a market there.
Also, when you think about fundamental like theory of why a market would self correct you have this idea that humans are going to be sense making what they actually want that provides a good quality of life and that that expresses itself as demand.
And the demand is kind of the environmental niche.
If we try and reify this with evolutionary theory, we reify theory of markets.
The demand causes an evolutionary niche in which different providers of products and services that there's a demand for emerge.
And so the variations and how they make their product or service are kind of like mutation.
And then some of the companies will succeed into that survival selection and some won't.
And then a couple companies that have different IP or a different approach or a different market segment might merge.
And so that's like mate dynamics where you get combinatorial dynamics.
So the idea is that markets are like evolution.
And so there is this kind of correcting increasing complexity type dynamic.
And of course that people competing against each other to provide the best products and services at the best value to rational agents will make the rationally best choice is what actually increases the quality of life for everyone.
This is a cute idea.
That's obviously silly because there was a meme going around on Facebook that said the smartest people go to the best colleges to go work at the biggest tech companies to figure out how to engineer addiction for children more effectively.
And we call this meeting meaningful demand and the self correcting nature of the market.
The moment that you can manufacture demand, the intelligence of the whole thing is broken.
And it's important to understand that supply and demand will have a cemetery between them on aggregate, meaning the company that's supplying and the collective body of all of the demand.
But there is no coordination or intelligence of the collective body of all the customers.
But there is a coordinated intelligence on the supply side.
So it's really the supply supply side against a single customer.
That is an asymmetric dynamic that doesn't have the same good for both of them.
There's a lot of times where the person doesn't need to buy the new shit that now they feel like they need to buy for status or addiction or whatever type thing it is.
It won't actually increase their quality of life in a meaningful way.
But we can use the supply side can use asymmetric manipulation tools to get people to want to buy it and also confuse them about what the best product for services because people aren't actually seeing what the best product for services are seeing what the best marketed one is.
We can also then use our market dominant position to squish smaller competitors that have a better product for service so it doesn't have a choice of actually emerging on and on.
So the self correcting nature of a market is not a real thing anymore to the degree that it ever was a real thing.
And there's actually a deeper argument as to why it never really was.
But even in this situation the asymmetric ability to manufacture demand that actually breaks the intelligence of demand as the basis of selection is actually a fundamental broken kind of situation.
Not to mention then that governments the whole idea of a nation state or government is to bind or regulate the most predatory aspects of where there would be per perverse financial incentive.
So it would be very profitable to do crime extortion.
So we're going to make a law not to do it would be profitable cut down all the trees so we make laws to say no we're protecting state parks.
But as soon as the regulator is supposed to regulate economic interests but that individual regulator is nowhere near as motivated to be effective at that personally as the economic interest is motivated to break their ability to be effective at it.
This is the libertarian public choice critique of this kind of liberal democracy structure representative liberal democracy.
Then you end up getting a situation where market forces capture the regulating forces through campaign financing through paying all the lobbyists through getting compromising information on representatives through whatever right.
They capture them and then have them actually serve their interest and now you don't even have the checking forces of market competition because you actually have the regulator doing the opposite of regulation.
Because obviously the regulator will always be something that we want to try and capture the market and it's important to see all the places where.
The idea of the thing right the idea of what a healthy market would be and what a healthy constitutional republic would be and how they would work together.
You'll see like in this nation the founding fathers said again and again things like.
This system only works with a comprehensively educated public that understands the history of.
What governance systems failed and which one succeeded and why so that they don't get compelled into those things that is educated enough to not be susceptible to propaganda.
That if you don't actively engage yourself in civics you're consenting de facto to be ruled on and on.
There's quotes founding father quotes I think this one is Jefferson if I could either have perfect news and a route trash government or I could have.
Perfect government and trash news I'd take perfect news right because if people were comprehensively educated they could reform government not the other way around.
So we don't we don't have that and so what you can see is that internal vested interest forces cause predation.
To the structures that were designed to be the best structures they could be.
And so we've had radical institutional decay to where markets incentivize predation and extraction more than they incentivize meaningful production of the things that most matter.
And representatives don't represent the people they represent the financial interests that get them in place and governments don't govern for the good of the whole they pay attention to what will get them reelected on for your.
Term cycles and how they figure out to make a lot of money on the very small salary and blah blah blah.
So I don't know if I actually answered your question I said that the underlying myth that there is this thing called a market and that it has self correcting properties.
Is really importantly gibberish and we have to acknowledge that if we want to think sanely about things.
So I'm curious to take a stab at alternatives or even characteristics alternatives that could emerge in maybe the medium term.
And so you mentioned the attention economy and the problems with that like what's an alternative to that.
And you've talked about in previous talks about how when you talk about regulating a market people get concerned start thinking about communism.
And that's not what you're talking about.
But you've also talked about how it's not democracy is all sorts of failures with it.
Like all the systems that we currently have aren't up for the task that don't meet the problems that we currently face.
So what does or what could.
Yeah so fundamental principle that we have to factor is that any game can be gained the moment you define what the success criteria is.
And what the laws and the whatever there will be places where you can violate the spirit of the law while meeting the letter of the law.
There will be places where you can figure out how to externalize some cost to something that wasn't accounted for in the model while being able to benefit at the thing that was accounted.
There will be some place where you can figure out how to have plausible deniability or high culpability to be able to get ahead.
And so it's an important thing to understand there is no such thing as a perfect social system and there never will be.
And meaning when we think about a perfect social system when we try to imagine what that would be what we're usually thinking about is OK how do we remove all the perverse incentives from the system.
Such that no one ever has an incentive to do something that is fucked for anybody.
And how do we make sure that the system of incentives predisposes everyone to make the most on the considerate positive choices for them in the hole in the near term and longer.
And what we're actually asking is how do we remove choice and make a system that predisposes everyone's choice in a particular way.
And as long as there is the reality of choice is the reality of the possibility of people making choices that are intentionally.
Benefiting the whole but wrong mistake mistake theory or not intentionally benefiting the whole or even intentionally damaging this is an important concept.
I'll come back to the more tangible in a moment but I think you'll find that if you try to define meaningfulness like the meaningfulness of existence of life.
You won't be able to find any definition that doesn't bind the concept of meaningfulness to the concept of choice.
And so in so far as we are we would seek to make a system that actually compels people's choice to be a particular way.
So their choice is no longer actually choice it's the result of the causation of social engineering.
We're actually trying to make a system that debases the basis of meaningfulness.
So and I can also say that there will be no matter if I have a system that tries to look at a bunch of metrics not just GDP but I'm going to look at a bunch of things that are metrics that are uncommentable.
So I'm going to look at CO2 and nitrogen effluents and microbiome of the soil and whatever all things I can and factor them into some complex system optimization waiting function.
There will always be metrics that matter that aren't included that we don't even know exist yet.
The unknown unknown set is never factorable into the thing.
And so there isn't a perfect system.
This is Asimov science is the ever writing of wrong.
There is only progressively hopefully better systems where the failures in the current system get acknowledged and improved.
But that takes humans that have a kind of general intelligence beyond the system dynamics to notice what's wrong with the system and make choices that the system is not predisposing to make good choices independent of system predisposition and to then also work to improve the system.
So we want to make the best systems that we can.
We also want to develop humans that make choices not just influenced by but beyond the influence of the current types of social coordination systems.
So the reason I say this is because what that means is that there is no answer to a civilization that doesn't totally suck that doesn't involve the comprehensive education and development of everyone to be good stewards of human choice.
And any attempt to do something that doesn't involve that will always end up being some kind of propaganda for a system of control.
And so that's fundamental.
It's also important to understand that no matter what proposition I offer for anything or anyone offers, I can also say how it could be gained.
So again, this would take some people being earnest and smart enough to notice where that was occurring to be able to work with it and bind it.
So that's a fundamental statement.
So that's not me answering your question.
It's saying why that question is unanswerable in a perfected form.
So I can give some examples of what progressively better systems and the writing of wrong are, and we can still go into where they would be gameable and then what the next progressive step would have to be.
Perfect.
I'm also curious if you can go deeper on how to make what a good steward of human choice looks like out of make those what it doesn't look like and how we can.
And I don't know if it's a segue into Forest Landry stuff, but how we can how we can do that.
Which way do you want to go first propose transitional systems or choice.
Let's do choice and then we'll do transitional systems.
Let's say we're looking at economics as a system that is predisposing patterns of human choice.
And we think of it that way it's a system of extrinsic motive incentive to create coordination it's not just resource allocation it's also extrinsic incentive right so that we can get people to do the labor jobs and those types of things.
But it's not just get people to do the labor jobs which is a how do we extrinsically override people's intrinsic motives.
It ends up being kind of a system of incentive writ large and so a dollar or whatever kind of currency is a fungible singular maximally reduced unit of value for all types of value.
That's the idea so that we can take value whether it's a service or a product or whatever something made out of bits or attention or atoms or energy or whatever it is.
And we can decide how much it's worth so that we can create exchange between them so we have this kind of abstracted value metric.
Well, there are kinds of value that actually matter that are not measurable they're not quantifiable.
There are kinds of value that are not extractible and exchangeable.
And so in so far as value that is measurable and extractable and exchangeable will confer game theoretic advantage to me over.
The kinds of value that are not then I can't make decisions that optimize sunsets or rainbows or songbirds or.
Future generations of children getting to fall in love or whatever I can't optimize for that because in so far as I put resource time attention energy or physical resources into those things.
I'm actually losing maximizing my own power and advantage relative to those who are doing purely measurable extractive exchangeable things.
This is a this is a multipolar trap issue and it's based on the fact that the metric of vacation right metrization of value is actually a fundamental problem.
We have to make choices in service of values that can't be put into fungible metrics.
So now when we're talking about how do we condition good human choice we can see that very fundamental to something just like the idea of currency.
Is conditioning choice in a way that is not what we would call being a steward of good choice that if we look at wisdom from any tradition any kind of good thinking on what wisdom is it would have people make choices that are not fully based on a type of metric accounting.
So if someone has a fiduciary responsibility to utilize their resources and they have now quantified the value of other people's lives as human resources in terms of their hours and what they get paid and whatever we've quantified all of those types of resources.
And they have to maximize shareholder returns as part of fiduciary responsibility they will be obligated to destroy real value in service of the types of value that confer game theoretic advantage and that are on the fiduciary relationship.
So this is just one of a lot of examples of why you can't leave choice to a system and yet why it's also very hard to develop humans that would make choice outside of what MOLOC or the paperclip maximizer of the system or ants.
Yeah there's the version of the Silicon Valley which is you know measure what matters. If it matters you'll measure it. If you don't measure it you will pay attention to it. Of course problem is that the things that truly matter in our lives we can't really measure it.
And so there's this conundrum there. Do you try to find better ways to measure the things that are unmeasurable hoping that you'll then prioritize it and not corrupt it in the process.
Do you just acknowledge that there's certain things that you'll never be able to measure and you need to find ways to prioritize it or do you get rid of all metrics all together and yet that seems in fact these all seem like imperfect.
Of course you don't get rid of all metrics but you have a choice making process that is informed by but not determined by the metrics.
Because there will be a general intelligence capacity to look at all the metrics and say there are things that matter that are not being captured here adequately.
And I can understand qualified things that aren't quantified I can understand some quantified things that I just paid attention to that aren't on my dashboard of metrics to look at.
I can understand that my simplified algorithm is probably actually really wrong that's paying attention to how I did an economic model or whatever it is.
And so the ability to hold responsibility for choice and this is one of the other things is that people actually get out of ethical responsibility.
Even to their their own self by creating a decision making system that they have to follow.
When SS troops were being tried in the Hague after World War two and they were being asked about war crimes they were all asked did you believe in everything that you did in 90% of them said no.
We actually like we started to and then we didn't believe in it and then they were asked did you try and stop and all of them said no we didn't try and stop and then.
When asked why they quoted a German phrase that said something like officers orders we didn't have a choice so the moment that you say fiduciary responsibility or company policy or.
This is what I got to do to you know compete with the market you can be a Nazi right you can actually remove all ethical culpability by saying you don't have a choice.
So the question was how do we develop good stewards of choice and we can see that it's not only the sense making the calculate stuff well it's also things like courage.
It's also things like depth of congruency and knowing ultimately what your life is in service to.
And what is meaningful to you and what matters beyond what any dashboard might be able to represent.
Knowing and here's the difference between virtue and virtue signaling.
Virtue is where there's something that matters to me that is not expressed in some type of game theoretic advantage it's not expressed in dollars or likes or tweets or status or election.
That I am willing to sacrifice those game theoretic advantage and service of.
Virtue signaling is using the performance of a virtue that people will care about to get more game theoretic advantage.
And when it comes down to the two masters issue of will I actually sacrifice some personal game theoretic advantage and service of this if the answer is no with some justification.
And so this then becomes a core thing in choice is where am I willing to do something that doesn't maximize profits.
It doesn't mean that it makes me unprofitable or crashes my company right but where am I willing to do something that doesn't maximize profits or slows the speed of growth or doesn't maximize market capture whatever because there's something that I care about that I'm actually committed to that I have devotion to deeper than that thing.
And if people don't know the answer to that then I don't trust them as stewards of choice at all.
And is it also being good steward of choice is understanding the power of but also the limitations of the scientific worldview or scientism or that is the main sort of guide for how you make decisions or determine what's what's meaningful or makes sense.
Yeah so just like you were saying do we get rid of metrics or not there's a and the answer is we're informed by but not determined by we're informed by scientific process and methodology but that doesn't actually determine the choice.
And so one has to ask what is what is the methodology.
That is relevant to influencing choice that includes but is not limited to the methods of science because you know we have this classic is odd.
Distinction in science that science can say what is it can't say what odd because science specifically the way you know there are some extensions but the way we think about science in a pop area and sense is it is the epistemology to study things in the domain of the measurable and the repeatable.
Something has to be both measurable and repeatable to be able to be subject.
Well there are things that are real that are measurable but not repeatable.
There are things that are real that are not measurable measurable is in the domain of third person and obviously first person itself is not measurable you can measure third person neural correlates that and then over norm how well you think they're correlated.
And use a physicalist assumption to say that physical was primary and first third person was primary and first person must have been at the phenomenal none of which is grounded in anything other than a self referential assumption.
So the real is not exclusively measurable and repeatable so science is a necessary but not sufficient epistemology.
So when I'm asking questions about choice making I'm asking odd questions not just is not have to be informed by is otherwise they're on ground.
Right so one of the problems here is that the philosophy of science gave us so much power over religions and other things to be able to understand the physical world and then apply that as technology and engineering to have more power over the physical world.
So anyone who paid attention to Newton's calculus could make cannonballs hit the target way better than the pendulum dowsers could and could do a lot of things.
So those who kind of paid attention to it just started to win via war economic war or military war or whatever it was.
But then what happens is we have a dominant system dominant kind of philosophic system that says we can't say what a good choice is.
There is no basis for that.
There is no reification of the concept of good or the concept of choice.
Both of those are concepts that we don't actually understand.
So we assume choices summed up causation from particle physics in the brain or genes or whatever domain we happen to study.
And good is purely a nonsense social construct.
The best we can do is look at evolutionary biology reify it with game theory and say good is that which doesn't lose an arrival risk context.
Maybe try and reify it down to thermodynamics and say who can garner the more the most scarce thermodynamic energy.
But that's not actually a definition of good in any meaningful way.
And so good choice is both of the words are outside of the domain of the philosophy of science.
And yet it's going to create all the power that empowers the choices that we're making through whatever methodology to be more powerful and thus win.
And so then we say OK well if science gets applied as technology which increases our ability for warfare and economic extraction and production and narrative control and information control and all those things.
Then what is guiding how that technology is implemented.
Well the economics that funds it.
And so then the economics is basically saying a good choice is the game theoretic choice that wins.
So when you have technology and getting into the phase of exponential technology all of the philosophy of science empowered by game theory as its ethical system.
That system ends up in my assessment self terminating because unlike animals that can't increase their game theoretic capacity radically within a life that only increases very slowly over over natural selection time periods in relationship to the
collective pressures that all of the other animals in the ecosystem also upregulate predator only gets a little bit better right through a mutation.
And then at best they start to eat a little bit more of the slower prey which inbreeds the faster praise genes which make them get a little bit faster.
So you don't get radical asymmetries of power which is key to how the metastability of nature works.
But the moment we had stone tools we were able to increase our predative capacity faster than the Amazon than the mammoths could deal with it faster than the whatever and we started extincting species and over hunting environments.
And then rather than having our population come into steady state with the environment like every other predator we could just move to another environment become the apex predator everywhere then start farming environments then start mining resource and moving to a.
You know fundamentally net unsustainable type dynamic is the toolmaking gate which was early philosophy of science right before formalized philosophy of science it was our abstraction capacity mediated.
Ability to understand principles right the principle of sharpness that could make a sharper knife not just use a sharp rock.
And then to test kind of empirically does this thing actually work better.
Our ability to do that broke the cemetery of us with regard to nature and also of the most powerful humans with regard to the other humans and that kind of cemetery is key to what allows the metastability of evolution.
So we're still using an evolutionary motive i.e. be the apex predator but without evolutionary capacities with capacities that aren't defined by evolution because our capacities are evolutionarily unprecedented.
No other animal could kill the whole world we can't they can't ruin whole ecosystems they can't genetically engineer new species they can't.
Because our capacities evolutionarily unprecedented the power of our choices the basis of our choice also has to be evolutionarily unprecedented.
Because we have the power to influence the whole so much we have to have consideration for the whole.
In our choice making because our capacity can influence the long term so much we have to have consideration of the long term in our choice making.
If we keep making choices to optimize our near term apex predator status but with capacities that break the entire.
Metastability then we self terminate.
Yep.
And evolution is is as a term as concept of the field is used to justify or explain.
You do lots of different things about how society currently works do you do you have and how people work and why they do things do you have sort of any different views from the mainstream evolutionary.
In terms of how we should interpret Darwin or other sort of important sort of diverges that have different implications for what's quote unquote natural or appropriate or irrelevant for us.
It's a big conversation there's lots of things in here and the idea that the mainstream thinking about Darwin that was just individual survival of the fittest without considering symbiosis
or ecosystem dynamics like that critique is now I would say fairly well known by anyone that's paying attention.
And that if you look at an ecosystem and you look at the relationships of gas exchange between plants and animals and all of the food cycle dynamics and aerobic and anaerobic bacteria in the soil and you see that most of the things are actually.
Very codependent interdependent you know symbiotic.
It happens to be that the that the competitive things just stand out in an interesting way right when when bucks are you know budding antlers it just obviously stands out and it ends up being that of course really focusing on that whoever really focuses on that and wins wars those
memes get selected for and so in the same way that you know genes that have certain kind of dominance get selected for so do memes even if they aren't actually holistic or true or you know in that sense.
So there's a there's a lot we could talk about of not just looking at individual animal selection or individual species selection but selection at the level of whole ecosystems that's really critical because obviously if an individual.
Species got so successful relative to the other species that it depends upon that it debased the ecosystem that it depends upon it its own short term success would kill its long term success.
And this happens right like we've seen invasive species come in devastating ecosystem and then they're gone.
And so the only things that make it through long term are the things that lead to self stabilizing ecologies.
But when we don't pay attention to that part of evolutionary theory when we are justifying being the apex predator in the market.
So that's very important there's a lot of things like that that are very important.
But I think the main thing I would give is that the moment we understood how to create tools via abstraction we were no longer evolutionarily precedented.
And so we can't apply evolutionary theory to us in the same way.
And all you've got to do is just like look at a gorilla attacking and then watch a nuclear bomb go off and look at a beaver cutting trees down and then watch slash and burn.
And look at a great white eating tuna and then look at a mile long drift net pulling up you know hundreds of tons of fish and you're like oh we're not fucking apex predators right.
Look at a bee dam as a beaver dam as the most radical environmental engineering of any animal then look at LA as you're flying into it.
And so OK we are not evolutionarily precedented creatures and religions talked about us being between the animals and the gods and needing to ascend into our god like nature.
And really what we can see with exponentially increasing technology is the power of God's right.
Speaking metaphorically but but something that is less like the power of animals and more like the power of what we would describe to who can create and maintain and destroy the whole.
And so if you have the power of God's and you don't have the right choice making to hold that power i.e. the love and wisdom and prudence of God's then that's a temporary story.
What should serve the role that we that mainstream thinks evolution serves.
What's a different theory or school or is it is it created.
It should be like if we are beyond evolution how do we.
So evolution is a process by which new things come to be that were not before.
Toolmaking or technology design is a process by which new things come to be that weren't before.
The unconscious relationship between those two self terminates at a certain point.
We can think about in a very general sense and we could go deeper into this but in a general sense we can think about evolution as an unconscious process there's not like a central top down mediated here we want creatures like this there's.
Mutation is an unconscious process and then survival selection everybody wants to survive and mate selection right so.
But what happens is that the fact that mutation is distributed across the whole system and the selective pressures are co selective.
Creates a radically decentralized radically parallel type of process that ends up being very slow most things fail but what makes it through are highly interoperable increasing in order to complexity holes.
And toolmaking so we can think of we can think of evolution as a kind of decentralized unconscious.
Parallel process but that is actually.
Tending to the metastability of the whole toolmaking is a conscious process can happen in a single individual can happen in or small groups.
Can happen in cereal not parallel someone can take insights and you know recursively work on those insights.
And it creates parts right to create it up regulates one input variable relative to others to be able to up regulate some output variables relative to others through the conscious ability to understand that.
So there is a new third creative process so if we say that evolution is unconscious tending of holes that obviously I'm simplifying radically to make a construction here and toolmaking is conscious creation of parts and there has to be a new process that is the conscious
creation and tending of holes and because if all of the agents are acting like parts and up regulating their capacity but breaking the power cemetery.
That is where the metastability comes and not thinking of the long term of the whole and when anyone deploys more asymmetric power power everyone then ends up reverse engineering it also deploying it you get this exponential curve of power deployment all of which is driving
and driving externalization that ends so as soon as we can affect the whole that significantly there has to be conscious governance of how that occurs which is where there has to be large scale sense making.
And large scale values generation meaning thoughts about what is good beyond just what wins and arrivals context and then large scale coordination capacity.
And so and this is where we start to talk about well how do we develop humans that can do that how do we develop relationships and communication processes and protocols that work better at shared sense making and shared values generation to inform better shared choice
making where we actually have better capacity to coordinate than getting stuck in things like perverse incentive and multipolar traps.
This may be a couple of things one is does it make sense to think of humans as a means to an end in terms of some other being that has more powers than just like we know we were some other being before us was it was means to us and does this resonate with sort of the idea of
like global brain or the new sphere or like and how do you see that manifesting or playing out.
Do I think that we are part of the hydro hydrocarbon bootloader process to make the advanced silica based AIs that then deplet replace all of the hydrocarbons and are better or that humans give rise to the trans humans and some sense in terms of how we
can modify ourselves are physiological substrate.
Both of those sound pretty dystopic to me.
And I would very much like to make sure we are not heading down those trajectories.
I think that we it's kind of like you see a plant that has some traditional recognized medicinal value and then we try to use scientific method to identify the active ingredient extract that active ingredient potentize it and then give it as a drug.
And we're paying attention to one thing that that herb supposedly did and we're measuring that one thing that one metric and we say look at this is a more powerful.
But there's often also a process where now that more powerful thing might also cause side effects that the plant didn't have there's externalities because there was a synergy of the way the molecules work together.
But it might also be that there were a bunch of other things that were not the primary objective that were positive things that were happening that are now lost.
So oftentimes something makes it through whether it's genetic or mimetic makes it through an evolutionary process that factors everything in a closed loop way.
And then we try to understand it by reducing it to some number of active metrics that we can then measure and optimize.
And the unknown unknown set the set of things that are relevant that we don't even know that we don't know that aren't in our model is more important than we account for most of the time.
And so yeah I think that there is a fundamental information theoretic problem in the way we think about AI the way we think about CRISPR and synthetic biology, which is we will define what we're trying to achieve as good with one or two or five known metrics.
And we will seek to make a technology that has the first order effect of increasing those metrics. Right the first order effect being a direct effect.
And the calculation to be able to see a first order effect on a known small set of metrics is relatively easy.
But the ability to calculate for second and third and fourth order effects on a very large unknown set of metrics is very hard.
There is a you know P not equals NP issue here.
And so, but if I'm being driven by kind of first to market or first to publish kind of dynamics I have not I have the opposite of an incentive to do deep and protracted safety analysis I have all of the incentive to say well we learn from mistakes and you know let's go.
But the level of mistakes when we're making things that replicate.
Like it's a really big deal to get that pattern replication tech is different it's different even the nukes because nukes don't make more nukes and detonate themselves, but biotech does.
And, you know, and possibly auto poetic AI could right so the moment we start getting into technology that is self replicating or just too powerful when we don't have safe to fail probes.
And we don't even recognize how shitty we are at safety analysis on second third fourth order effects over longer periods of time on metrics we don't know to look at.
This is a this is an information theory issue that we really have to pay more attention to with the level of technological capacity we're getting.
So I'm saying this to say that I think a lot of the we understand the genome well enough to start editing it.
Is is stupid hubris and that doesn't mean that I'm a lead that says there is no way to move forward it means what is what is the actual complexity of the situation information theoretically and then what's the complexity of my analysis in terms of orders of magnitude and are they close.
And if my analysis is factored 10 to the five bits and the reality of the situation is maybe 10 to the 20 bits of information I probably shouldn't do the thing yet and I should think about safe to fail.
Now this is an information theory thing but I think mostly it's actually not an information theory thing it's a motive thing because I'm not even motivated to try to do good safety analysis and motivated to do the bare minimum safety analysis as a source of plausible deniability.
To say that I did safety analysis to actually move forward as fucking fast as I can on getting whatever that one or two metrics that gives me game theoretic advantage that I'm trying to optimize for.
I want to go back to a choice making and then to transitionary systems a couple questions one is you mentioned in a different episode you talked about how sort of we have these false paradoxes between sort of a false you know a false
Economies mean mind body or heart brain or are there others I'm curious to be unpack that a little bit where it comes from or why it's false and then mentioned you if we think about science is one tool in the toolkit.
Is there what it what's another important tool or how do you think about that that took up broadly in terms of you could do choice making and then maybe this related to and I know this is a huge topic so it's a little bit of injustice to just shrink it to a minute.
But if you could sort of summarize the crux of what you find really interesting about forest lander's work or what you think his contribution is to accept that that's related.
Yeah.
In terms of the need to have a ethical system that is commensurable that is able to have logical congruence with the philosophy of science.
So that the philosophy that gives rise to our ability to make more powerful choices is also connected to the ability to make better choices.
Working towards that in a meaningful way is one of the areas that forests work has been that I think is most meaningful of a contribution.
There's lots of things I think are very meaningful contribution.
And so you know again we said the philosophy of science looks at the domain of the measurable and repeatable or we can say the domain of measurable repeatable specifically which is always going to be third person.
And so there are some philosophical assumptions about what third person means what first person means and what the relationship between first and third person are or reifying what we mean by subjective what we mean by objected and what the relationship between
that forest work starts at that level and the reification gives the basis to then be able to ground the philosophy of science and something deeper than it.
Like obviously if we're looking at the domain of measurable and repeatable we have to start with measurable and the measurement always involves a measure measuring some measured.
There's a triplicate relationship between the observed the process of observation and the observer.
And yet then what we try to do is take for the most part the measurement process and especially the measure for granted and create an ontology of measures.
Even though the fundamental basis of the concept of measurement itself isn't even semantically frameable in a logical way without those three concepts being bound.
So it's very interesting if you look at kind of Vedantic idealism or Buddhist idealism or solipsism in general on one side and you look at physicalism on the other side I would say that they make the exact same reductive mistake.
So physicalism says that objective or physical is fundamentally real and subjective is either not real like it's just an illusion a lot you know radical limitism or it's an epiphenomena it's you know secondary and they have a very compelling argument for that right.
The compelling argument is it requires assuming a bunch of things as more epistemically grounded than I think they are.
But so big bang just a bunch of plasma and then particles and whatever it doesn't seem like there's anything we can call consciousness in there.
Eventually planets at a certain point hydrological biological process proto cells whatever.
It's very hard to think about consciousness before that so it seems like there was physical stuff before there was consciousness stuff so then consciousness must have emerged out of there and.
If people can describe their subjective experience well hooked up to an e.g. or an f.m. or I and we can look at their brain and we can see some type of statistical correlation between patterns of brain and their subjective experience and we can say.
There's a correlation between subjective and objective but we think that the objective is controlled by the objective I.E. that physics is causally closed.
Therefore your brain state this moment is the result of your brain state a moment ago and the particle physics that governs you know I on differentials of.
What's going to actually happen in the brain in relationship with the body in the environment so if there is a one for one correlation between.
Describe subjective states and observed objective states and the objective states are the result of scientific causation and this closure over that then subjectivity must be a causal epiphenomena right like.
There's various versions of that argument and as you can see you know argument between Sam Harris and then then it who both would.
Typically think of themselves as being mostly informed by the philosophy of science come to you know then then it says that consciousness is not real but free will is and Sam says free will is not real but consciousness is and.
At the end of their argument Sam said something like it appears we have fundamentally different intuitions on this which is actually very important which meant that the symbol grounding.
Was actually the key thing and this is where you get Tarski's theorem you know coming out of girdle's theorem which is that.
Any formal logical system involves ideas that cannot be reified in that system and that means that there is no logical system that is consistent it's also complete.
And I now this comes back to why I said there is no perfect social system and it comes back to why the first thing Lao Tzu said when he wrote his book is that the Dow that is nameable is the eternal Dow.
Is the formal system that is complete is is doesn't exist right.
Okay so we can see how basically.
Philosophy of sciences and obviously I'm radically oversimplifying to go quickly and we can do the nuance thing at some point.
So the question what is real philosophy of science as well the stuff that is measurable and repeatable seems to be pretty clearly real because we might think that sound is faster than light but we can all measure that it's not and it's the same every time and so the.
The congruency of objectivity and the repeatability of it and it's independence to our beliefs about it seems like a good basis for real and then the fact that we can make predictions with it and make shit out of it that works and whatever.
And so then of course you have an epistemology which is measuring and repeating and then testing and you know like that which is you have an epistemology of third person.
That is the epistemology that makes sense starting with the ontological assumption that what is real is the stuff that you can assess me about epistemology.
And so then you do a bunch of third person assessment and all you find is causation and third person and so of course you end up then coming to what is real is third person stuff because your whole process has been measurements on third person stuff.
So there's an ontological assumption that creates an inappropriate epistemology that then proves the ontological assumption.
So then they don't or Buddhism did the exact same thing on the opposite side.
They say that subjectivity is fundamental and objectivity is either an illusion or Maya etc. or an epiphenomena the structures arising within consciousness and they argue it in a very similar kind of way on the opposite they say well I don't really I can't prove that I'm not dreaming.
Or in a hallucination or anything else so my measurements might all be part of a dream and you and the repeatability of it is all part of my dream.
So I have less epistemic ground on the object of stuff out here than I have on the basis that there is an experience arising that I'm actually experiencing something.
Descartes said something obviously very similar to this.
So what I have the most epistemic grounding on is that experience is occurring but my perception of myself as experienced or might be an illusion.
There is a self here but it might be different than I think it is and there's a world but it might be different than I think it is.
So there is some self experiencing a world experience or experiencing an experience that they the experienced and the experiencer might have delusion but the experience is prima facia occurring.
So what I can what is real is that which can be experienced as opposed to measured and repeated.
Therefore I have an epistemology of inquiry and noticing the nature of experience and running experiments in there of moving attention in certain ways and seeing what happens with experience and so I get a phenomenological epistemology.
And then that ends up confirming see everything that I notice is experience and everything that is objective is a right I can only say even exists because I'm experiencing it if I couldn't actually experience the measurement the measurement wouldn't have occurred.
This is the if the tree falls in the forest and there's no one there to hear it.
So I would say that they both basically made an ontologic assumption that then created an epistemology that then confirm the ontologic assumption in East is East and West is West and ever the twain shall meet.
And then of course Silicon Valley and whatever we do this very funny thing like take principles of Buddhism to do a mindfulness practice to crush the competition more effectively.
Which is a very funny strange hybrid of kind of the game theoretic versions of both.
But underneath the experience so OK.
The Vedantist will walk you through a process and say notice the tree now notice that you want the tree you're the one noticing the tree OK so notice some other objects notice that you aren't those objects are the one noticing the objects now notice your thoughts those thoughts are objects.
Objects of attention so notice that you you can hold an image of the tree in your mind so you aren't the thoughts you are the witness of the thoughts.
And you aren't the emotions the sensations because you're witnessing them on and on.
And so all of those are not ultimately what you are they're changing what is unchanging and all of them is the witness therefore.
You know the Upanishads say again and again only the self is real only the witness is right.
But they started with an observer observing and observed.
Right I had to start with the tree.
I had to start with a bunch of objects to even point at the subject because the subject is not even definable outside of its relationship with an object whether the object is a perception or the perceived right.
And so I took for granted that I was only able to even notice subjectivity or consciousness or reference it by starting with subjective objective in the relationship between them which is the same thing as I start with in science taking a measurement.
And there is clearly a conscious experience of registering a measurement through a measurement process.
Then I'm going to take for granted the conscious registering of that and the measurement process to make an ontology of the measured.
So there is a deeper process that says that to even define observation or experience or measurement well there are these binding of the concepts that are necessary and sufficient and there's a triplicate kind of binding.
If we can define a formal philosophy there a formal metaphysics there then the on the epistemology of the objective which will be a reified philosophy of science is now.
Founded in something that also can give us a epistemology of the subjective and an epistemology of the relationship between the subjective and the objective.
And ethics actually lives in the relationship between the subjective and the objective which is why you have to factor objective measurement metrics utilitarianism and why that is necessary but not sufficient.
So Forrest's work you know if somebody Googles eminent metaphysics you'll see a PDF come up and then he has a few other writings on a website uvsm.com Forrest's work.
So we know from from girdle's theorem we know from Tarski's theorem that we cannot have a system that is both perfectly complete and perfectly congruent.
So one of the things that Forrest ends up doing in the metaphysics is actually defining an upper bound on the noble.
And in any domain where the upper bound on the noble is that's something called the incommensuration theorem.
And then being able to define what the appropriate epistemic process within the domain of the noble is and then also how to do inter domain transfers across the epistemics of different systems.
And so basically as a metaphysics it is not complete, but it defines the upper bound of what level of completeness could happen and then it, it offers the upper bound of completeness that's at least the claim and it was interesting for me.
When I first looked at it and I had, you know, decent background in both science and the philosophy of science and philosophic systems including, you know, purse and whitehead and other people that had done things that were maybe closest in that direction.
It really just looked like gibberish.
When you read the axioms and modalities, it is hard for it to make any sense and it also seemed kind of like impenetrable gibberish.
So, if I only got the book I would have definitely not done much with it after the first half an hour and then gave up.
It was only because I got to spend a decent bit of time with Forrest and he was able to actually show me the application of it to solving problems that in domains that I knew well.
That I'm like, oh, that is actually fucking novel insight in the entire space that I actually know well and that that happened across enough spaces that I gave the benefit of the doubt to say, oh, there's actually an epistemic there.
That I can see the expressions of in areas that I have more domain knowledge.
So then the long haul of working to understand it came in and I will say that it's been extremely helpful in being able to take other epistemologies like evolutionary theory or complexity science or mathematics or whatever and understand them in terms of a meta epistemology that
both gives more clarity and insight and more capacity for inner domain transfers formal inner domain transfers.
And I was having this conversation the other day because so much of the time that is a epistemic system that we use when discussing other things were like fuck how do we it's so hard to have conversations where people don't have this now because it is such a powerful set of tools.
So we're working on helping Forrest develop ways of having people be able to learn the relevant parts easier because right now it's not very easy.
Sam Harrison Jordan Pearson had this sort of famous debate. I don't know exactly what the contents were my understanding is that Jordan Pearson was pro religion and Sam Harris was sort of anti religion.
And then Brett Weinstein also has his sort of own view on game theory and the importance of and you sometimes you talk about rising above game theory, but he has a strong view of no no we need to harness game theory and use it in the right way to understand trade
offs and you talk about you know not needing or trying to rise above trade offs. I'm curious if there's anything there that's interesting that's worth commenting on.
So when I say things like having the power of God requires love and wisdom of God's. I think a lot of people especially in the you know Jordan Peterson kind of world think that I'm going to try and do Jesus smuggling and and I'm not I'm.
I'm using that phrase in a in a symbolic way not a literal way and because it I think metaphorically or symbolically has insight that it offers so I'm going to say a few more things that are taking a religious idea and I'm not going to do Jesus smuggling.
I'm simply saying that there was actually philosophic insight expressed through the religious tradition that has value.
So the no false idols concept in the commandments.
The way that I think of that there's a few different ways but the way I think of that is that a model of reality is not reality.
And this is back to Isimov's ever writing of wrong right and this is back to that no consistent system will be complete.
And when you even look at the definition of a complex system in complexity science is a system that when you model it meaning model it means a finite set of variables and a finite set of transforms or algorithms right which will predict some percentage less than 100% of its behavior.
So when you create a complicated model of a complex system it still has some behavior not explained by the complicated system that we would describe as an emergent properties.
And if you add more variables you might get closer to you go from 80% accuracy to 85% accuracy or whatever but you don't get 100% accuracy.
So of course the Leibnizian idea is that 0.99999 indefinitely you run the limit of it equals one.
I think there's a fundamental philosophic problem here this is numerically very useful that it is numerically valuable to consider it as one versus is one is actually a really important distinction.
So similarly there's a we see some correlation we assume some causation we see that as we add more variables it gets closer therefore we assume it is purely deterministic on a finite variable set that we have not understood all the variables of completely but we're converging on perfect understanding.
No, we're converging on statistically better understanding that will always continue to have unknown unknown and gaps in it.
And so given that biology and ecology and psychology and sociology and all the things that we really care about are because I would say we only care about tools and infrastructure in so far as they affect those things.
The things that we really care about are all complex systems which the model of the complex system will never be the thing right.
It doesn't mean it's not useful it can be totally useful but also if I make a model of a thing that doesn't describe its behavior perfectly there's some emergent properties not described by the model then if I optimize for the model.
Where the model doesn't map to the realities where I fuck up the reality.
And we see that all the time a very simplified economic models of rational human actors and you know Malthusian insights and whatever and reality doesn't follow the model perfectly so it has some benefit and then some fucked up things right so then we try to make a slightly more complicated model.
So the no false idols is that the model of reality is never reality so don't ever get to attach to it.
Don't ever get emotionally religiously attached to capitalism or to communism or to a particular scientific understanding of a thing.
Hold it as useful and maybe the best we currently have and as fundamentally and inexorably inadequate.
So there is a reverence for reality that is beyond current and ever description.
And that reverence for reality has a seek to want to keep making better models while not being attached to our models just saying they are relatively better than other models.
And inadequate and we're pretty sure a thousand years from now will have much much better models and then a thousand years from then again right.
So religion.
I've been reading the founding fathers of this country letters to each other a lot lately as I've been thinking about public education and the need for just an almost mind blowing an increase in what we would call public education.
And one of the things they spoke to again and again we can think this is an antiquated idea of a bunch of like old white dudes in a time where science hadn't advanced enough.
And I really think that that is a mistaken assessment.
They talked about the need for a religious education as being as critical as a education in the sciences and technology and trades and etc.
And some of them said things like I actually don't care if they're studying Confucius or Rumi or you know whatever it's there needs to be a transcendental operator.
There needs to be that whatever our current model is we need to be able to have at least some frames for an intuitive connection to something that is beyond the limit of those frames.
And so this is where I would say that you know utilitarian ethics is necessary but not sufficient virtue ethics also serves a role because in the utilitarian ethics.
First if I take negative utilitarianism I want to minimize suffering then obviously the best way to do that is to just kill the universe.
And I'll have the least area under the curve of suffering into the future.
We can see that every one of the utilitarian theories if hypotheses if I do a reducto ad absurdiom I can see that it doesn't actually further.
And this is again one of these partial considerations of something more complex than the model can hold.
And so in utilitarian calculus I have to assume higher degree of epistemic certainty than I usually have when actually making choice.
Right because oftentimes I'm making a choice in not only the complex domain but the anthro complex domain the domain of humans interacting with lots of humans interacting with biological systems interacting with technological systems.
And so what I'll end up doing is say OK the things that matter are these things and the effect of this action is going to be this and so I'm willing to have the ends justify the means and I'll sacrifice some harm here for some greater good here.
But so often I cannot actually say that that will be the greater good that in terms of the effect of the action that the harm will be limited in that way that the benefits will be that because the unknown unknown set is so large.
Now I'm not doing a God of the gaps argument I'm saying simply we can't predict the weather 10 days out.
And so one of the places where people start to look at virtue ethics is OK be where I don't have the epistemic certainty to just run the utilitarian calculus on a finite set of metrics.
Is there something that creates more integrity in the moment where then the next moment unfolds from greater integrity.
And we can say that the way that people think about both utilitarian and virtue ethics are like they're not even using deep enough cognitive tools.
And so we can this is where I do think that the metaphysics is a deepening of the cognitive tools that are being used.
It's very interesting.
So I think you know religions were obviously like they were philosophers who were wise people who were having deep experiences and noticing humanity and paying attention to earlier philosophers that had genetically identical brains to what we have today
and who were sharing wise things.
And then there was the capture of that by political forces that distorted it for fucked up purposes and it's both.
And there's heaps of fucked up dummy relevant stuff.
And there's actually heaps of really deep insightful things.
And I would neither want to have to keep the bathwater nor throw the baby out nor be bound to the previous babies as the best baby we can get.
Right.
So I think that when the founding fathers are saying there's a need for something like a religious process is when people think about heaven and hell and some almighty God judging or they think about reincarnation or something like that.
If they're not yet at the kind of level of moral development that can transcend narcissism and care about the subjective experience of others for no rationalized benefit to themselves.
Then thinking about reward and punishment for themselves in an afterlife is a nice way to smuggle in infinity.
Right.
It's a nice way to at least get them to for their own self benefit.
Think about things that in the rational calculus right now they can't see how serves their own self benefit.
But there's something in a transcendental operator of what is more meaningful to me than my own life.
And do I have a sense of life with a capital L like all of life all of experience that is more than just my experience.
And where beyond choice making calculus I have a yeah I have some unfuck with the bull virtues and ethical senses.
Of course the religions are not adequate and of course they have heaps of nonsense but I think the need for a transcendental operator is clear.
So then the question of what do transcendental operators that are not irrational gibberish that is also bound to bad concepts what does that look like moving forward.
And I think we could look at this in a way that would both clarify and reify previous religions into healthier versions of themselves as well as offer a post religion post science philosophy that includes and transcends the meaningful parts of both pre modern and modern.
Bert Weinstein had a quote he said something along the lines of you know I think of religions is not literally true metaphorically true in the sense of if I believe them.
My life will be better basically that they did not literally happen but there's some truth that is indescribable.
Is that a with that crude not crude but very simplified statement do you resonate with or is you see it's like different or is that not the right way to think about.
I think we have to be very careful here because you know we can make the Bertrand Russell argument very quickly against them that.
Believing nonsense because it seems to make you happier or have some pragmatic or utilitarian purpose is not a reasonable thing to do because if it's not true it'll end up actually.
Fucking reality up in some ways as you're operating on a not true map of the real territory but affecting the territory in the process.
So I don't think it's that we want to indulge things that are untrue because it seems comforting.
I think it's that we recognize that the philosophy of science and utilitarianism do not give us access to all of the true.
And so then we have to say what is an expanded epistemology for the domain of the real not just the domain of the repeatable and the objective.
So in the same way that and you know then people very quickly say well intuition is just pretty obviously corrupted by desires and fears and blah blah.
Rationalities obviously corrupted.
You actually have to train rationality.
This is actually what formal logic and mathematics and critical thinking and you know all of the domains of learning how to fucking think properly are because we've all heard people make rational arguments that are just utterly gibberish because of lots of rational mistakes logical fallacies whatever.
You have to train intuition as well.
And if you look at what narcissists and sociopaths will do in gas lighting that will end up fucking other people up.
Gas lighting is usually making a rational argument that someone feels compelled to believe that gets them to override their intuition.
And so someone's like man something fucking feels wrong with this person feels like they're not being honest with me or whatever and then the person makes the argument well enough and they're like OK well I want to give the benefit of the doubt I don't want whatever it is and so.
And then they realize that they were actually being played by someone that was better at playing the game than they were.
There is a way that we can use a rationalist orientation to gaslight ourselves to override.
Other valid methods of knowing because we don't know how to do the derivation and we want to be able to do the derivation we want intellectual congruency.
Now the argument I'm making can then easily be weaponized to smuggle in gibberish.
But then recognizing that someone can smuggle in gibberish can also have them reject the truth to the argument.
And so there is no way forward other than a truly earnest one because you can weaponize the arguments on either sides.
I want to return back to or delve into transitionary systems and some of my sort of these are all related but so many questions there are.
How do we internalize externalities. What does it look like to eliminate trade offs.
How do we you know what technology can help us build you know real time balance sheet of the commons.
Are there examples of game B sort of institutional you know examples that exist today or even directions that are directionally game B.
Okay I was talking about weaponization a moment ago and also virtue signaling earlier.
There's a lot of stuff that calls itself conscious capitalism or impact investment or sustainable business or whatever that.
You know basically advertising something game B like that is never going to violate a fiduciary responsibility to do profit maximization or actually have any real consequentiality on its own advantage.
And it's just basically taking a part of its marketing or talent retention budget to you know appeal to millennials or to regulatory bodies or whatever it is.
So are there are there things that companies can do are the things within market dynamics that would be good for the world totally.
But are there a lot of things that are advertising that they are that that aren't actually that also totally.
So the discernment of anyone who's thinking about getting involved either as an investor or an employee or whatever like there has to be fairly close thoughtful discernment.
I was just talking to a guy the other day who had been at one of the top position at one of the major banks and is now in one of the South American countries that has part of the Amazon and is working on a approach to protect the Amazon and use market based solutions
to protect the Amazon worth more alive than dead so that you know market dynamics will keep it alive.
And while that would be a nice thing to do the particular strategy was like terrible everywhere because you know involve things like well the indigenous people who depend upon the environment there and protected if we can support their rights in some way that will end up supporting the Amazon.
So we're we're making this fair trade artisan craft business thing that the indigenous people will produce their stuff and will sell the shit out of it and be able to provide you know economic well benefit to those acres associate.
I'm like OK.
So right now the indigenous people do like a thousand things and making just the amount of craft that they need to live is one of the things that they do.
Now as soon as you start providing economic advantage and survival to doing that one thing and not all the other things and you just convert the indigenous people to a Chinese sweatshop where now everyone just does the artisan craft all of the time and they try and bring in more people to do that thing.
And like if you want to know how to destroy an indigenous culture figure out how to monetize one part of it heavily independent of any kind of value on the other things.
And this is where we talked about values that are extractible and exchangeable compared to things that are valuable but aren't extractible and exchangeable.
So it's very important to really think through those strategies and say is this going to be the right way to do it.
And when you start to monetize one of the parts that is extractible and exchangeable and not all the other ones and you can for a lot of advantage in doing that then obviously you start to bias the entire culture towards doing that particular thing and you lose key other parts.
So it's not just that someone has to say will this be net positive and in a false dichotomy that says well if we don't do that they're all burned and slaughtered but you have to say are there better strategies.
So this is actually an important topic the rationalist community has this you know ongoing debate in certain areas on mistake theory versus conflict theory.
Michael Vassar turned me on to this thinking and there's this question of do we do the dumb things because of mistake theory meaning there was just no way we could have predicted that externality.
Which is too hard or conflict theory which was we didn't really try to predict the externality or the problem or we even wanted to cause it.
And while both are true one of the things we have to factor is that mistake theory ends up being a source of plausible deniability for conflict theory.
So there are heaps of places where we are actually doing something that we know is going to cause harm.
And whether it's PNG not fixing their things or the deep shore thing you know not fixing its oil rigging or the whatever like we know something's going to cause a problem.
Or we don't know it is but we didn't really try to do good safety analysis either because that would have been expensive and then after the fact we say there's no way we could have known.
So we'll end up using complexity and mistake theory as a source of plausible deniability for being assholes.
And so we have to be very careful not to do that so oftentimes we'll look at a strategy and make a very false comparison and say well either we do this or don't do this and if we don't do it great tragedy.
So we straw man all the other possibilities to then support why ours is utilitarian the best as opposed to really try to think are there better strategies.
And probably there are much better strategies but maybe they take more thinking and don't provide as much immediate capitalization for me.
So again there is no answer other than really deep earnestness of what you actually care about.
Okay so the reason I brought that up is to say are there things that are advertised as transitional or game be or vectoring in the right direction that really should not be considered that.
Yes so one has to really do the thoughtful sense making if they actually care about the direction things are really oriented.
You know attention economy what could be done there I haven't done a thorough enough analysis to be able to make.
Full suggestions and say that when we run the run all of the trying as best we can second third order effects that this would be the right thing to do but we can at least propose in the direction of.
I think jaren laniers argument around that one of the problems of the Facebook YouTube kind of attention capture world one of the sources of the kind of Tristan Harris problems is.
That the internet was developing in a way where we both wanted information to be free and we wanted.
Entrepreneurs to be able to make a lot of money into the answer was the advertising model and the advertising model fucks up the fiduciary relationship orientation where.
Facebook the user is not the customer the advertisers the customer so their actual orientation is to optimize the utility for their customer and that's their fiduciary responsibility but the.
Person that is using it that is sharing all of their personal information in their life and their the degree of personal information they share with everybody over messenger and whatever is then all being weaponized against them for.
You know the advertiser whereas if there was a pay for service model it would be closer to a market like in terms of the Adam Smith theory make a product or service and deliver it to a customer.
At a value that they want to pay for and deliver a better product to them as soon as you're intermediating it through deliver it to somebody else's customer where it's not it's not clear.
You know how the market dynamics work I think you've actually broken what should be the intelligence of the market and you've made something that's fundamentally has to be evil.
And so if people had to pay a monthly fee to access YouTube or to access Facebook and there was no incentive for them to maximize time on site there was only an incentive.
For them to actually have the customers feel more fulfilled with their experience so rather than drive addiction through hyper normal stimuli they were trying to drive the actual quality of engagement.
I think that would make a huge difference now exactly how do we do that how do we transition the existing ones how to make the new ones.
I don't know a lot of like we would have to do deep kind of design work and to do the design work we'd have to identify the problem space well enough to do the design work.
But just an example of kind of thinking through changes that could be made that still allow businesses to operate.
But that don't allow all of the business models because some of the business models are really fucked enough they should just be regulated against.
If you had asked me in a message earlier what do I think the role of BC's could be.
I want to see financial institutions that are created that invest only in technologies that are needed to solve the fundamental problems of the world including the R&D that's needed in the domain of atoms and energy not just bits long term and large scale.
I think that money on money dynamics so I've got money I'm going to loan it to you I want to 10x my money because a bunch of them are going to fail or 100x or 1000x so I want the fastest path to liquidity events.
I think that is one of the things that has most fucked the entire intelligence of the market.
I think that when you look at a kind of classical market financial services were and should be less than 5% of the total market and now they're 35-40% I think that is completely predated and extractive and not creative.
And so you know you look at 1971 Nixon getting us off the gold standard and off of the Bretton Woods convention and.
The beginning of all the financial deregulation that occurred and you see that the government and the market stopped innovating in the way that they had before we stopped having shit like Apollo projects in Manhattan projects.
We started having stupidly bloated f-35s that cost 1.5 trillion dollars that we didn't need in the first place.
And so I think that like China right now because and you can see why China has an emperor that doesn't have term limits and that basically got rid of the multipolar traps and created a kind of top down control system so they can actually coordinate.
Well there's all the fucked up things of dictatorship but there's also if you a democracy or republic is only better than a dictatorship if you have a really educated population.
That can overcome the cemetery the asymmetry of supply side orientation which we don't have in which case this thing ends up having so many coordination failures and broken sense making that really you have the simulation of a democracy.
That is an oligarchy controlling representatives on both sides so you can't even have accountability to who the fuck the dictator is.
And the short term term limits where nobody will invest in long term stuff so China's getting to build high speed rail all over the fucking country right and not just their country all over.
Africa and all over you know other parts of Southeast Asian and Europe and getting to do like fundamental R&D and infrastructure development we haven't done good new infrastructure in forever.
And or the kind of R&D so like where the fuck is the thorium research right like the government should have a it's also important to understand when you look at post World War two.
A lot of the things that we attribute to market we're actually because of government spending in the market wouldn't have done it otherwise a lot of like the early contracts to HP and IBM or government contracts that put the money in to be able to do deep R&D that created stuff that then the private sector benefited for.
So governments actually investing a percentage of federal budget in R&D and then in infrastructure is important.
And so neither is the public sector is the government doing that nor does the private sector have 25 year return horizons to be able to do stuff like thorium and trains and whatever.
So short term money on money basically broke everything right in the financial sector short term money on money both the directors.
And officers of a corporation being held on quarterly timelines so of course they're not going to like do something that will give pay off at the long time in the future.
And the representatives having four year time term lines like Notre Dame took 200 years to build thorium is going to take a while to research trains take a while to build.
So I would like to see venture funds and growth equity funds that say we do not need another fucking Snapchat right we do not need more kind of like addictive technology even though Metcalfe's law is awesome and we'll like make a lot of money doing it like not only do we
not need it it's net bad for the world so we're not going to fund that shit.
We are only going to fund the things that the world really needs that are not currently on track to happen that solve major problems and we're going to do stuff not just in the domain of bits because of course bits.
I have no cost of goods in the domain of bits so I can have exponential returns that I don't because of all margin that I don't have in the domain of atoms were actually have cost of goods but.
The virtual is based upon the value of the physical and so if the virtual starts debase the physical because it's easier for capital to flow there.
Now again any system that emerges out of a foundational system and then debases its own substrate is self terminating.
The virtual needs to enrich the physical not debase it so a healthy social media would be one that made people's friendships and physical life better.
But if I'm with my friends and I'm checking fucking Facebook or I'm so socially awkward because I grew up on it that I don't have real friends then the virtual is debasing the physical that is net bad for the world.
If financially the virtual debases investment in the physical that is net bad for the world so I want to see.
People who are investing in long term r&d of real stuff that needs to happen specifically in the domain of atoms and energy.
And who have a longer term you know event horizon associated with that and it's still market.
Right I'm still not a non-profit I'm still talking about being able to make returns and so start to think about dynasties again in your family name and being able to you know.
Optimize for that because otherwise the time horizons are too short so I do think that.
Wanting the figure like waiting to see because building shit is hard and so VCs have this position of let other people build shit see the stuff that looks like it could succeed and then try to capture as much.
Ownership of it as possible the board seats etc to be able to create a artificial distorted bubble on its valuation to exit.
That's just fucking evil right like it's bad for everybody other than them.
In terms of bad for the world.
I would like to see people have capital saying how do we deploy this capital which is concentrated choice making ability to actually make good choices right when we're talking about.
Choice making power needs choice making basis money is concentrated choice making so what are we doing it in service to and for the most part anyone who has asymmetric power.
Is incented to and capable to use their asymmetric power to maintain and increase the asymmetry of their power.
That will always that's that's evil right that's the fucked up thing we need to say how do I use my symmetric power to actually.
Increase the baseline of quality of life for everyone to actually use my symmetric power to debase the asymmetry of it by rising everybody up not by.
Lowering not by tall poppy syndrome communism but but nobody's oriented to do that but.
It would be totally possible to be a mention of entry capitalist who said.
I am looking to use market dynamics and also use philanthropic dynamics where that makes sense or government type dynamics that makes sense to actually.
Do things that will be net positive to the world not where I can tell the story that it was net positive as a type of marketing and possible deniability for something that was clearly just net positive for me.
So yes I would like to see financial services totally fucking restructured and I would like to see anyone with integrity actually work on their own internal congruence to say.
There are deals.
I'm willing to turn down there are places where I'm not going to try and push for a liquidity event or fucking take the company away from the founders if the founders actually care about something.
There are places where there's a good idea but the founders are evil and I don't want to support that thing like what what things am I willing to say no to and what things are worth saying yes to and then actually find and invest and support in those things.
And with the time horizons that are needed to do the type of stuff that's needed.
Yeah.
And it means a couple of other things related to this one is you've written about or talked about how you want to see us move beyond private property or things like private property because sort of projecting sort of implies that we are all sort of discreet individuals who are not connected or don't have intersecting
you know interweaving value creation or in ways that would align us better.
I'm curious if you can unpack that.
I'm curious if you think Wikipedia or crypto currencies are game B institutions and then maybe in closing as it relates to that is how do we transition from you know from game A to game B or some of these things we've talked about.
So when I've said that there are intrinsic perverse incentives that are inseparable from private property while that is true that doesn't mean that I think we're anywhere close to being able to approach what it would take to change it.
In the same way that I was mentioning earlier that there's problems with having a single fungible currency single value metric that orient towards.
Measurable extractable exchangeable wealth of other things and that orient towards optionality over real value and whatever.
So there's some very deep restructuring that is beyond the way we currently think of currency to mediate exchange and the ownership of the things in the first place private property.
This is not something that I see happening in this generation or the next generation.
So we could have a conversation to talk about what those post capitalist systems look like but they don't like way anything I've ever seen proposed so far.
That's an important thing to know.
So for the next few generations while those things are being developed it's actually making liberal democracy better which largely looks like reversing the predatory decay that has happened and.
Having markets more like markets and having representative democracy more like it was intended to be that is what we should be focused on.
So when I was talking about longer term event horizons and more earnest assessment in financial services.
That's still game A in the way you know it is not a fundamentally axiomatically restructured system but it's the healthiest expression and it's the version of game A that is the story that we're sold right.
The story that we're sold is that markets drive innovation and that governments govern well and that representatives represent the people and that courts create justice and that.
Universities teach people how to think well and that news teaches you what's going on in the state of the world and the doctors make you healthy and the police keep you safe like that.
That's the game A store is just fucking not true.
It doesn't mean it's not true anywhere of course it has elements of truth but we can also see how inside of those institutions people figured out how to.
Degrade the institutions in ways that were beneficial to themselves fuck the thing for the whole so long as they could hide that right and this is you know when you look at.
Baudrillard's model of simulacrid decay which Michael Besser also turned me on to it kind of explains how this phenomenon happens.
So we need to reverse that kind of institutional decay and reboot something like better versions of these institutions to be directionally.
Right while we are working on fundamentally new things from an axiom level.
And in the previous episode during I'm curious what you think about the game be movement more broadly during Hall said when it came he becomes a movement it will have failed.
I think it's being somewhat somewhat provocative there but it was sort of how do you sort of think about these concepts that you've been talking about.
You all talk about years start to gain a lot of momentum and community around them and what's your sort of advice for that community with you know failure modes to watch out for.
How do you think about sort of this burgeoning game be community and movement.
I think that most dominant narratives politically right politically left whatever a political I think they're they were also influenced by the narrative warfare landscape.
Of what do I need to do to get people to believe this to advance our agenda including where it's not fully true or where it's more provocative or where it straw man's the other side or where it appeals to people's.
Unmet needs and emotions in ways we know aren't fully true or whatever.
I don't think that there are any dominant narratives on anything that are not mostly narrative warfare.
Either side of climate change either side of capitalism vaccines whatever like I think most of the topics end up having.
People in institutes that have something to gain from lots of people believing a certain thing figuring out how to tell compelling versions of that they get people to defect on their own sense making to be part of an in group.
A raid against and held together against an outgroup and so the kind of good nuanced thinking that would be needed to figure out complex things I see almost nowhere and I think what Jordan was referencing is.
Game be would could very easily be another version of that so who it used to be the pope and then it was you know university professor whatever who's the authority that knows what's true and has the.
Monopoly on legitimate authority who can tell everyone else is less smart and informed than them what true and right is.
To be able to control them basically the moment that legitimate authority seems to exist there's an orientation of anyone who wants power to capture it.
And so then of course the sides that want whoever captures it wants to continue to double down on it being legitimate authority and whoever wants to fight who captured it wants to debase that it's fake news or it's a false narrative or whatever right.
I think that the idea of most people not doing in depth sense making themselves and looking to see.
Which of the talking heads they align with and which of the groups they can be part of the in group on and defect on their own personal responsibility and sense making to be part of group think is.
The failure mode of game a.
And so it doesn't matter what narrative that's not the thing we want to do.
There isn't a plant the flag and get everyone to rally has a war around the new flag there's a how do we get people to actually make sense of the world better in a more thoughtful earnest nuanced way how do we get them to check their own biases better how do we get them to not be susceptible to group think better.
How do we get them to have more courage to express what they actually feel and believe in the presence of a group that is trying to group think everyone into it.
And then how do we get higher qualities of civil conversation where the people are oriented to dialectic rather than war and defection.
I think what we're seeking is how do we have collective intelligence that is really collective intelligence rather than kind of dumb mobs captured by leaders that want to be leaders.
I want to be sensitive to your time so I want to close on on a last sort of couple questions that like previous ones are going to seem very different.
Maybe they're related.
Feel free to take whichever ones appeal to you.
So one is I just I read this book the sovereign individual recently.
Have you read the sovereign individual.
The basic thesis is that we're going to transform through the 90s.
And it says we're going to transform from citizens of governments.
You know we exist to serve governments to customers of governments where governments exist to serve us.
There are any markets for governments.
The world is going to look you know technology up until now is as increasingly central.
You know become more and more centralized.
It's now going to become more and more decentralized.
Might look the world might look like the decentralized city states you know and people self segmenting.
I'm curious what you add you have a sort of if anything comes to mind for you when you think about that future.
Everything you've said in a different episode is you are more excited about sort of the enlightenment of the feminine and a femininity.
I'm curious if you can talk a little bit about that.
And then lastly this is enclosing you said you've been reading a lot about you know education or thinking a lot about what the future patients and maybe
enclosing any any thoughts thoughts about that as it relates to what we've been discussing that comes to mind.
I yeah I mean I'll just say as a general statement.
I think central versus decentralized is another one of these kind of oversimplified narrative warfare things where of course there are things.
Are better when they're centralized and there are things that are better when they're decentralized and the best systems involve the relationship between those things and in a nuance and contextual way.
Because I can show failure modes of the central systems like basically why China is doing better infrastructure than the U.S. right now actually has to do with more centralized control that doesn't create multiple or traps.
But then you also see more corruption and control and you know less of certain kinds of innovation incentives with centralization so.
I think the conversation needs to go deeper than it usually does there.
Similarly if you apply gibberish market theory to governments and say we'll become customers and the company serves the customer.
But we know that the company can actually manufacture demand and capture the customer when it has asymmetric capacities like so basically I would say that in the same thing with masculine feminine I think.
Most of like we can see the left having a feminism that is moved to kind of LGBTQ trans ideology and interracial ideology that is kind of represented in intersectionality.
And then we can see a very strong backlash to that and alt right kind of thinking and so there's like massively charged stuff here and I think the general the underlying.
Ideas of what is true and what is good or we need to go much deeper on.
I think the con.
I think the conversation is usually framed in a way where I'm rejecting the frames and would want to reconstruct the frames.
So that's why these are longer conversations.
Totally perfect.
That's a perfect note to close on.
Thank you so much for coming up podcast for listeners who want to go deeper in Daniel's work.
Check out is it civilization emerging dot com.
Yeah.
Anything else you want to point them to.
There is more on the media page in terms of podcasts.
So that's probably the place to start.
Yeah.
And go down the YouTube rabbit hole if you found this interesting you won't be disappointed.
Daniel I'll let you go.
Thank you so much for coming up podcast and talk to you soon.
Thanks for having me.
It was good.
If you're an early stage entrepreneur, we'd love to hear from you.
Please hit us up at villageglobal.vc slash network catalyst.
