So, you've spent some time learning about Bayesian statistics, and you've convinced
yourself that this is the appropriate way to view a world full of uncertainty.
So you've taken the time to learn how to use software that uses the Markov Chain Monte-Carlo
approach, which seems to work great in theory, but when you try to apply these concepts in
the real world, you discover that the MCMC approach is too slow.
In addition, the MCMC approach requires too much computing power, so you keep draining
your batteries.
So while the MCMC approach may be great in theory, in many cases the MCMC approach is
impractical.
So what can you do?
Is there a more practical Bayesian approach?
My friend, let me introduce you to Rxinfer.jl.
What's Rxinfer.jl?
Well, let's find out.
Welcome to Darko.jl, where I explore the vast Julia wilderness and turn my discoveries into
wholesome Julia tutorials.
According to its GitHub page, Rxinfer.jl is a Julia package or automatic Bayesian inference
on a factor graph with reactive message passing.
When I first read this, I had no idea what this means.
No!
I still don't really, but the main takeaway for me is that Rxinfer.jl does something similar
to Turing.jl.
But it does it in a completely different way.
The Rxinfer.jl package is a product of BiasLab.
The Bias in BiasLab stands for Bayesian Intelligent Autonomous Systems.
BiasLab is a research lab in Eindhoven University of Technology located in Eindhoven, the Netherlands.
I'm going to apologize in advance for all of my pronunciations in this section.
According to the BiasLab website, their research focuses on nature-inspired, quote-unquote,
natural artificial intelligence technology for engineering applications.
They are inspired by recent results in theoretical neuroscience, the free energy principle, on
how the brain perceives, learns, and makes decisions through unsupervised real-time interactions
with its environment.
In summary, this lab is looking for applied solutions that minimize the so-called free
energy.
Their website lists examples like real-time decision-making or robots and drones.
So this research group has a need for a practical Bayesian approach, one that's efficient enough
to handle real-time streaming data.
This is not feasible to do by using Markov Chain Monte Carlo.
Back in Juliakan 2018, they introduced a probabilistic programming package called Fornelab.jl.
In fact, Juliakan 2018 was also when Turing.jl was introduced.
The lead developers for Fornelab.jl were Marco Cox and Thais Vonderloch, who are both
at Eindhoven University of Technology at the time.
Thais is currently an assistant professor at Eindhoven University of Technology and is
a current member of BiasLab.
Fornelab.jl has evolved into a new package called ReactiveMP.jl.
BiasLab has also developed packages called GraphPpl.jl and Rocket.jl.
This stack of three packages, ReactiveMP.jl, GraphPpl.jl, and Rocket.jl, have been bundled
into a new package called RxInfer.jl.
The Rx in RxInfer.jl is short for ReactiveX, which is a reactive framework originally created
by Microsoft.
The lead developer for all of these packages, RxInfer.jl, ReactiveMP, GraphPpl, and Rocket
is Dmitry Bogaev, who is currently a PhD candidate at Eindhoven University of Technology
and is a current member of BiasLab.
Dmitry introduced Rocket.jl at JuliKon2020 and he introduced ReactiveMP.jl at JuliKon2021
and he introduced GraphPpl.jl at JuliKon2022.
In addition, he published a paper in Scientific Programming earlier this year introducing
the RxInfer.jl package in an article titled Reactive Message Passing for Scalable Bayesian
Inference.
The best way to get to know the RxInfer.jl package is to use it, so let's take a look.
In the interest of time, I've prepared some Julia code in advance.
You can find a copy of my code in my GitHub repository.
There's a link to it in the description below.
If you want to follow along, you can save a copy of this file in a tutorial folder on
your computer.
After you save the file, in Vias code, launch the Julia REPL by using ALT-J then ALT-O.
Change the present working directory to your tutorial directory.
Enter the package manager by using the closing square brackets.
Save the tutorial directory.
Add the following packages, RxInfer and Statsplots.
Exit the package manager by hitting Backspace.
In the editor, execute the code to load the packages.
For this tutorial, I'm using a random seed.
Okay, now that we have our package is loaded, let's generate some random data.
For this tutorial, we're going to simulate some random coin tosses.
This simulation is like the Hello World of probabilistic programming.
In this simulation, we don't know if this coin is a fair coin or a biased coin, so we're
going to toss this coin 10 times and record our observations.
The true success rate for this coin is 75%, meaning that as the number of coin tosses
approaches infinity, the coin will land on heads 75% of the time.
But we as the observer don't know this true success rate.
So we're going to assume that the distribution for the success rate for these coin tosses
is likely a Bernoulli distribution.
And then we're going to try to find that distribution for the success rate by using
RxInfer.jl.
In our simulation, 1 means heads and 0 means tails.
So after tossing the coin 10 times, the coin landed on heads seven times and landed on
tails three times.
Now that we have our observations, let's build a Bayesian model using RxInfer.jl.
I'm going to type in the code for the model first, and then we'll talk about it after
I'm done typing.
So at first glance, the code for this model definition looks similar to the code that
we used when defining models in Turing.jl.
Like in Turing, we use the at model macro immediately before the Julia function definition.
Like in Turing, we assign distributions by using the tilde notation.
And like in Turing, we use a for loop to fill a vector with the likelihood distribution.
But unlike in Turing, the observation data is handled differently.
In Turing, we could simply pass the observation data as an argument in the function definition.
But when using RxInfer.jl, we need to be very specific in defining how our observation
data is formatted.
We do this in RxInfer by using the dataVare function.
DataVare is short for data variables.
The dataVare function creates a mechanism to pass data values to the model.
The first argument is the data type of our data.
This first argument is required, but all other arguments are optional.
In this example, we're using the dataVare function to let our model know to expect Boolean
data types and to expect 10 values.
So other than the way we need to handle the observation data, the syntax that we use to
define a Bayesian model in RxInfer.jl is very similar to the syntax that we used to define
a Bayesian model in Turing.jl.
Now let's move on to the step for finding the posterior distribution to see if that
step is similar as well.
Assigning the model to a variable named model is the same when using Turing.
But like we saw in the model definition section, the way that we pass data to the model is
significantly different when using RxInfer.
RxInfer requires the observation data to be in the form of a Julia named tuple or a Julia
dictionary.
In Julia, we need to include that comma for named tuples with a single member, otherwise
we'll get an error.
Now to find the posterior distribution, RxInfer uses a function named inference.
But RxInfer doesn't use MCMCChains, so it doesn't require a sampler since it won't
be drawing any random samples.
When using the inference function, all of the arguments are keyword arguments, so we
can't simply pass the variables named model and data.
We need to include model equals and data equals as keyword arguments in the inference function.
When we execute the inference function, we don't get an MCMCChain, nor do we get any
diagnostic summary tables.
Instead, the inference function returns this output in the REPL that states that there's
a posterior distribution available under the name success underscore rates.
We can access this distribution by using dot posteriors and including the name of the
available distribution as a symbol within square brackets.
In this example, RxInfer has found that the posterior distribution is a beta distribution
with an alpha value of 8 and a beta value of 4.
We can view the mean and the standard deviation of this distribution like we can with any
other distribution.
So, after making 10 observations and after using RxInfer, our updated belief is that
the distribution for the success rate for this coin has a mean value of approximately
67%, with a standard deviation of roughly 13%.
Now that we have our results, let's create some visualizations.
The first plot is our prior belief, which is the beta distribution with an alpha value
of 1 and a beta value of 1.
This is just a uniform distribution that we used as a weekly informative prior.
The second plot is the posterior distribution, or our updated belief that we found using
RxInfer.
We can plot the posterior distribution just like we can with any other distribution when
using the stats plots package.
These next few plotting recipes are optional.
I'm going to add some vertical lines to the posterior distribution plot, just for reference.
And then combine the two plots together into a single plot.
Any observations, we had no idea if our coin was a fair coin or if our coin was biased
to land on heads or biased to land on tails.
After tossing the coin 10 times and observing that the coin landed on heads 7 times and
landed on tails 3 times, we were able to update our belief.
Although we still don't know the true value of the success rate for this coin, we now
believe that the true value lies somewhere within this posterior probability distribution.
We're not supposed to know the true value, but we can see in this plot that the true
value is within one standard deviation of the mean in the posterior distribution after
just 10 observations.
So what just happened?
On a superficial level, it appears that the RxInfer.jl package and the Turing.jl package
are similar.
But under the hood, these packages are very different.
Let's take a few minutes to explore some of those differences.
I'll be the first to admit that I don't fully understand how RxInfer.jl works, but I'm
going to take a step at explaining what just happened.
If you're more familiar with the inner workings of RxInfer.jl, please feel free to correct
me in the comments below.
That way, we can all learn together.
So when we used the at-model macro, graphppl.jl generated a factor graph to represent our
model.
A factor graph is a collection of nodes and edges that is a very efficient and a very
scalable way to build a computer model that can ingest new data and update the posterior
in real time.
Specifically, the type of factor graph generated by graphppl.jl is known as a Forney-style
factor graph or FFG.
In an FFG, the edges represent variables, and nodes capture probabilistic relationships
between variables.
Once the model has been constructed, a way that it communicates internally is by using
something called message passing.
Message passing is performed by reactivemp.jl.
I'm not sure how it handles static data, like our example, but in theory, this model
takes in the first observation and updates the posterior right away.
And then the posterior immediately becomes the new prior.
Then the model takes in the second observation and then updates the posterior again.
And then this posterior immediately becomes the new prior.
Rx and Fur will repeat this process until it goes through all of the data.
So this process is perfect for ingesting an infinite stream of data in real time.
I believe that Rx and Fur is ingesting data and updating the posterior by using racket.jl.
Now in this example, it doesn't appear that Rx and Fur is actually inferring anything,
despite using a function named inference.
It seems to be calculating the posterior directly, meaning that we started with a prior distribution
of beta-1-1.
If we add the seven heads to the alpha parameter and add three tails to the beta parameter,
then we'll end up with a beta distribution of beta-8-4, which is what the posterior distribution
ended up being.
That's because when the prior distribution is a beta distribution and the likelihood
distribution is a Bernoulli distribution, then the posterior distribution is known to
be a beta distribution.
In Bayesian probability terminology, when the posterior and the prior are part of the
same probability distribution family, the prior and the posterior are called a conjugate
distribution.
And the prior is called a conjugate prior, or the likelihood function.
There's a whole collection of these conjugate distributions, where the analytic solution
to the posterior is known.
There's a table of this collection in the Wikipedia article titled Conjugate Prior.
Rx and Fur.jl takes advantage of this knowledge to find the posterior directly without needing
Markov chain Monte Carlo, and without drawing any random samples.
The Rx and Fur.jl documentation states that Rx and Fur.jl is capable of inference or non-conjugate
distributions, but it's unclear to me how their inference process works.
Also in their documentation, it states that Rx and Fur is capable of running various Bayesian
inference algorithms in different parts of the factor graph of a single probabilistic
model.
Again, it's unclear to me how this is done.
What is clear is that I'm in over my head.
So let's stop here for today.
Even though I don't fully understand everything yet, I understand enough to recognize that
the Rx and Fur.jl package is a very clever approach to tackling a very complicated problem.
We really only scratch the surface of using this package, so I'm going to make another
Rx and Fur.jl tutorial so we can go through a more complex example.
In the meantime, if you're impressed with the Rx and Fur.jl package and you want to
show your support to the developer, then please consider going to the GitHub page, and please
consider leaving a star.
If you made it this far, congratulations!
If you enjoyed this video and you feel like you learned something new, then please consider
liking, commenting, sharing, and subscribing.
If you'd like to support the educational work that I'm doing, then please consider using
the Super Thanks button.
For ongoing support, please consider joining and becoming a channel member.
Channel members get ad-free, early access to all of my new videos.
Thanks for watching, and good luck on your Julia journey!
