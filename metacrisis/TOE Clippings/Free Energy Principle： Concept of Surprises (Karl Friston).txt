Why did you call it the free energy principle? When I first heard, well, I heard about it
quite a few times, then I kept dismissing it because I thought it was referring to perpetual
motion or the extraction of energy from vacuum fluctuations, right? I understand that it
has something to do with free energy in the physics term, but what led you to calling
it the free energy principle? And have you have you heard of other people? Have you heard
of the case of mistaken identity from other people? Is it just me?
No, no, I've certainly come across that, but not so nicely articulated or gently articulated.
So yeah, for me in my world, the free energy was the most natural and obvious thing to
call it because the kind of free energy that we're dealing with, or when I say we, people
in either Bayesian statistics or laterally what we know now as machine learning would
always refer to this quantity, this variational free energy as a computable objective function
for any inference or estimation problem. So whether you're doing sort of classification
of machine learning and you're using restrictive belts of a machine and you're using, let's
make it more current. So you're doing, you're using high end deep learning using a variational
autoencoder to try and recognize some sequence of images. Then the weights that you are optimizing
in that deep learning setting in that particular variational autoencoder setting are optimized
with respect to a variational free energy. So this variational free energy plays a central
role and has done for more than half a century now in optimization problems. Its origins
actually really, really quite interesting. Historically again, this is not really my
field, but from what I gather, there are two ways that the free energy sort of came into
play. There's the American route, and this starts with Richard Feynman as with most things.
So he had the problem of basically wanting to, you know, express it simply, evaluate the
probability distribution over all the paths an electron say could take. And he realized
that that was an intractable integration problem. He couldn't work out the sort of the normalization
constant to make sure that probability distribution summed to one. So he was faced with an intractable,
effectively integration problem in quantum electrodynamics. So he solved it by converting
that integration problem into an optimization problem. And the way he did it was to introduce
a variational free energy that was always greater than the quantity that he wanted to minimize
or was less than the quantity that he wanted to maximize, which in this instance is just
the marginal likelihood or the likelihood distribution of various paths, say a small
particle might take. So that's where I learned about free energy, essentially a Feynman-esque
free energy that was provided a bound approximation to something that you actually wanted to maximize
or optimize. And in my world, that that thing you want to maximize is the model evidence
or the marginal likelihood, because back to self evidencing, but also the marginal likelihood,
sort of indicating that this is a central notion in physics, in Bayesian statistics as well.
And that quantity, that variational free energy has been in play now for, you know, not quite a
century, but certainly many, many decades, currently known as an elbow, ELBO, an evidence
lower bound. So that evidence lower bound underwrites, you know, all of high end machine learning.
And, you know, certain simply reduced cases of it, I would actually suggest probably all of
machine learning at some point refer to or can be seen as a special case of this. The other route
inherits more from the Russian side. And this is, you know, notions of algorithmic complexity,
underlying universal computation via Komolgroff complexity and sort of induction. So the big
drive there is, again, formulating universal computation as basically communicating or
articulating or encoding some structure in the simplest way possible by introducing a bound on
the thing that you want to, you want to optimize or minimize or extremize. So you can also find
the free energy in the context of sort of a more Shannon nest like take on communication and
optimization. And David McCabe sort of originally brought that perspective to the table perspective
that was was well understood by people like Wallace and Melbourne, who themselves had taken it
from from the Russian literature and so on. I think he was he was an American but also taking
this algorithmic complexity. So it's a very long answer to actually say that the free energy
principle or the principle of minimizing free energy has underwritten universal computation,
practical approaches to quantum electrical dynamics and machine learning since since the 1950s. So
that's what the free energy means. It's not that the energy is costless. It's just borrows from
the formalism of the thermodynamic free energy are the Gibbs or Helmholtz free energy.
