As of today, we are in a war that has moved the atomic clock closer to midnight than it has ever been.
We're dealing with nukes and AI and things like that.
We could easily have the last chapter in that book if we are not more careful about confident wrong ideas.
This is a different sort of podcast, and not only because it's Daniel Schmattenberger,
one of the most requested guests, who by the way I'll give an introduction to shortly,
but also because today marks season three of the theories of everything podcast.
Each episode will be far more in depth, more challenging, more engaging, have more energy,
more effort, and more thought placed into it than any single one of the previous episodes.
Welcome to the season premiere of season three of the theories of everything podcast with myself,
Kurt Geimangel.
This will be a journey of a podcast with several moments of pause, of tutelage,
of reflection, of surprise appearances, even personal confessions.
This is meant for you to be able to watch and rewatch or listen and re-listen.
As with every Toe podcast, there are timestamps in the description,
as you could just scroll through to see the different headings, the chapter marks.
I say this phrase frequently in the theories of everything podcast.
This phrase just get wet, which comes from Wheeler, and it's about how there are these
abstruse concepts in mathematics, and you're mainly supposed to get used to them,
rather than attempt to bang your head against the wall to understand it the first time through.
It's generally in the rewatching that much of the lessons are acquired and absorbed and understood.
While you may be listening to this, so either you're walking around and it's on YouTube,
or you're listening on Spotify or iTunes, by the way, if you're watching on YouTube,
this is on Spotify and iTunes, links in the description.
I recommend that you at least watch it once on YouTube, or you periodically check in,
because occasionally there are equations being referenced, visuals.
I don't know about you, but much or most, in fact, of the podcast that I watch,
I walk away with this feeling like I've learned something, but I actually haven't,
and the next day if you ask me to recall, I wouldn't be able to recall much of it.
That means that they're great for being entertaining and feeling like I'm learning something.
That is the feelings of productivity, but if I actually want to deep dive into a subject matter,
it seems to fail at that, at least for myself.
Therefore, I'm attempting to solve that by working with the interviewee,
for instance, we worked with Daniel, to making this episode, and any episode that comes out
from season three onward, from this point onward, to make it not only a fantastic podcast,
but perhaps in this small, humble way to evolve what a podcast could be.
You may not know this, but in addition to math and physics, my background is in filmmaking,
so I know how powerful certain techniques can be with regard to elucidation,
how the difference between making a cut here or making a cut here can be the difference between
you absorbing a lesson or it being forgotten. By the way, my name is Kurt Jaimungal,
and this is a podcast called Theories of Everything, dedicated to investigating the
versicolored terrain of theories of everything, primarily from a theoretical physics perspective,
but also venturing beyond that to hopefully understand what the heck is fundamental reality,
get closer to it, can you do so? Is there a fundamental reality?
Is it fundamental? Because even the word fundamental has certain presumptions in it.
I'm going to use almost everything from my filmmaking background and my mathematical
background to make Toe the deepest dive, not only with the guest, but we'd like it to be
the deepest dive on the subject matter that the guest is speaking about. It's so supplementary
that it's best to call it complimentary, as the aim is to achieve so much that there's no fat,
there's just meat. It's all substantive, that's the goal. Now there's some necessary infrastructure
of concepts to be explicated prior in order to gain the most from this conversation with Daniel,
so I'll attempt to outline when needed. Again, timestamps are in the description,
so you can go at your own pace, you can revisit sections. There will also be announcements
throughout, and especially at the end of this video, so stay tuned. Now Daniel Schmattenberger's
a systems thinker, which is different than reductionism, primarily in its focus. So systems
thinkers think about the interactions, the N2 or greater interactions, the second order or third
order. And Daniel in this conversation is constantly referring to the interconnectivity of systems
and the potential for unintended consequences. We also talk about the risks associated with AI.
We also talk about their boons, because that's often overlooked. Plenty of alarmist talk is on
this subject. When talking about the risks, we're mainly talking about its alignment or misalignment
with human values. We also talk about why each route, even if it's aligned, isn't exactly salutary.
About a third of the way through, Daniel begins to advocate for a cooperative orientation in AI
development, where the focus is on ensuring that AI systems are designed to benefit and that there
are safeguards placed in, much like any other technology. You can think about this in terms
of a tweet, a recent tweet by Rob Miles, which says, it's not that hard to go to the moon,
but in worlds that manage it, saying that these astronauts will probably die, is responded with
a detailed technical plan showing all the fail safes, testings, and procedures that are in place.
They're not met with, hey, wow, what an extraordinarily speculative claim. Now,
this cooperative orientation resonates with the concept of Nash equilibrium.
A Nash equilibrium occurs when all players choose their optimal strategy, given their beliefs
about other people's strategies, such that no one player can benefit from altering their strategy.
Now, that was fairly abstract. So let me give an instance. There's rock, paper, scissors, and you
may think, hey, how the heck can you choose an optimal strategy in this random game? Well, that's
the answer. It's actually to be random. So a one third chance of being rock or paper or scissors.
And you can see this because if you were to choose, let's say, one half chance of being rock,
well, then a player can beat you one half of the time by choosing their strategy to be paper.
And then that means that you can improve your strategy by choosing something else.
In game theory, a move is something that you do at a particular point in the game,
or it's a decision that you make. For instance, in this game, you can reveal a card, you can draw a
card, you can relocate a chip from one place to another. Moves are the building blocks of games,
and each player makes a move individually in response to what you do or what you don't do,
or in response to something that they're thinking, a strategy, for instance. A strategy is a complete
plan of action that you employ throughout the game. A strategy is your response to all possible
situations, all situations that can be thrown your way. And by the way, that's what this upside
down funny looking symbol is. This means for all in math and in logic. It's a comprehensive guide
that dictates the actions you take in response to the players you cooperate with, and also the
players that you don't. A common misconception about Nash Equilibria is that they result in the
best possible outcome for all players. Actually, most often they're suboptimal for each player.
They also have social inefficiencies. For instance, the infamous prisoner's dilemma.
Now, this relates to AI systems. And as Daniel talks about, this has significant implications
for AI risks. Do we know if AI systems will adopt cooperative or uncooperative strategies?
How desirable or undesirable will those outcomes be? What about the nation states that possess
them? Will it be ordered in positive or will it be chaotic and a tactic like the intersection
behind me? Although it's fairly ordered right now. It's usually not like this. A stability of a
Nash equilibrium refers to its robustness in face of small changes, perturbations in payoffs
or strategies. An unstable Nash equilibrium can collapse under slight perturbations,
leading to shifts in player strategies and then consequently a new Nash equilibrium.
In the case of AI risk, an unstable Nash equilibrium could result in rapid and extreme
harmful oscillations in AI behavior as they compete for dominance. And by the way, this
isn't including that an AI itself may be fractionated in the way that we are as people
with several selves inside us vying for control in a Jungian manner.
Generalizations also have a huge role in understanding complex systems. So what occurs
is you take some concept and then you list out some conditions and then you relax some of those
conditions. You abstract away through the recognition of certain recurring patterns.
We can construct frameworks. We can hypothesize such that hopefully it captures not only this
phenomenon, but a diverse array of phenomenon. The themes of theories of everything on this
channel is what is fundamental reality. And like I mentioned, we generally explore that from a
theoretical physics perspective, but we also abstract out and think, well, what is consciousness?
Does that arise from material? Does it have a relationship to what's fundamental reality?
What about philosophy? What does that have to say metaphysics? So that is generalizations
empower prognostication, the discerning of patterns, and they streamline our examination
of the environment that we seem to be embedded in. Now, in the realm of quantum mechanics,
generalizations take on a specific significance. Now, given that we talk about probability and
uncertainty, both in these videos, which you're seeing on screen now, and in this conversation
with Daniel, thus it's fruitful to explore one powerful generalization of probabilities
that bridges classical mechanics with quantum theory called quasi-probability distributions.
Born in the early days of quantum mechanics, a quasi-probability distribution, also known as a
QPD, bridges between classical and quantum theories. There's this guy named Eugene Wigner,
who around 1932 published his paper on the quantum corrections of thermodynamic
equilibriums, which introduces the Wigner function. What's notable here is that both position and
momentum appear in this analog to the wave function when ordinarily you choose to work in the so-called
momentum space or position space, but not both. To better grasp the concept, think of quasi-probability
distributions as maps that encode quantum features into classical-like probability distributions.
Whenever you hear the suffix like, you should immediately be skeptical as space-like isn't
space and time-like isn't the same thing as time. In this instance, classical-like isn't classical.
There's something called the Kalimogorov axioms of probability, and some of them are relaxed in
these quasi-probability distributions. For instance, you're allowed negative probabilities. They also
don't have to sum up to one, and doing so with the Wigner function reveals some of the more peculiar
aspects of quantum theory like superposition and entanglement. The development of QPDs expanded
with the Glauber-Sedarshin p-representation, introduced by Sedarshin in 1963 and refined by
Glauber and Hussamese Q-representation in 1940. QPDs play a crucial role in quantum tomography,
which allow us to reconstruct and characterize unknown quantum states. They also maintain their
invariance under symplectic transformations, preserving the structure of phase space dynamics.
You can think of this as preserving the areas of parallelograms formed by vectors in phase space.
Nowadays, QPDs have ventured beyond the quantum realm, inspiring advancements in machine learning
and artificial intelligence. This is called quantum machine learning, and while it's in its infancy,
it may be that the next breakthrough in lowering compute lies with these kernel methods and
quantum variational encoders. By leveraging QPDs in place of density matrices, researchers
gain the ability to study quantum processes with reduced computational complexity. For instance,
QPDs have been employed to create quantum-inspired optimization algorithms like the quantum-inspired
genetic algorithm, QGA, which incorporates quantum superposition to enhance search and
optimization processes. Quantum variational auto-encoders can be used for tasks such as
quantum states compression and quantum generative models, also quantum error mitigation.
The whole point of this is that there are new techniques being developed
daily, and unlike the incremental change of the past, there's a probability, a low one, but it's
non-zero, that one of these will remarkably and irrevocably change the landscape of technology.
So generalizations are important. For instance, spin and gr. So general
relativity is known to be the only theory that's consistent with being Lorentz invariant,
having an interaction and being spin two, something called spin two. This means if you have a field
and it's spin two and it's not free, so there's interactions, and it's Lorentz invariant, then
general relativity pops out, meaning you get it as a result. Now this interacting aspect is important
because if you have a scalar, so if you have a spin zero field, then what happens is it couples to
the trace of the energy momentum tensor, because there's nothing else for it to couple two. And
it turns out that does reproduce Newton's law of gravity. However, as soon as you add an interacting
relativistic matter, then you don't get that light bends. So then you think, well, let's generalize
it to spin one, and then there are some problems there, and you think, well, let's generalize it
to spin three and above, and there's some no-goal theorems by Weinberg there. By the way, the
problem with spin one is that masses will repel for the same reason that in electromagnetism,
that if you have same charges, they repel. Okay, other than just a handful of papers,
it seems like we've covered all the necessary ground, and when there's more room to be covered,
I'll cover it spasmodically throughout the podcast. There will be links to the papers and to the
other concepts that are explored in the description. Most of the prep work for this conversation seems
to be out of the way. So now let's introduce Daniel Schmottenberger. Welcome, valued listeners and
watchers. Today, we're honored to introduce this remarkable guest, an extraordinary, extraordinary
thinker who transcends conventional boundaries, Daniel Schmottenberger. I'm sure what are the
underlying causes that everything from nuclear war to environmental degradation to animal rights
issues to class issues, what do these things have in common? As a multidisciplinary aficionado,
Daniel's expertise spans complex systems theory, evolutionary dynamics, and existential risk,
topics that challenge the forefront of academic exploration, seamlessly melding different fields
such as philosophy, neuroscience, and sustainability. He offers a comprehensive understanding of our
world's most pressing challenges. Really, the thing we have to shift is the economy,
because perverse economic incentive is under the whole thing. There's no way that as long as you
have a for-profit military-industrial complex as the largest block of the global economy that
you could ever have peace. There's an anti-incentive on it as long as there's so much money to be made
with mining, etc., like we have to fix the nature of economic incentives. In 2018, Daniel co-founded
the Consilience Project, a groundbreaking initiative that aims to foster societal-wide
transformation via the synthesis of disparate domains promoting collaboration, innovation,
as well as something we used to call wisdom. Today's conversation delves into AI,
consciousness, and morality aligning with the themes of the TOE podcast. It may challenge your
beliefs. It'll present alternative perspectives to the AI-risk scenarios by also outlining the
positive cases which are often overlooked. Ultimately, Daniel offers a fresh outlook on
the interconnectedness of reality. Say, let's get the decentralized collective intelligence of the
world having the best frameworks for understanding the most fundamental problems as the center of
the innovative focus of the creativity of the world. So, Utah Watchers, you, my name is Kurt
Jaimungal. Prepare for a captivating journey as we explore the peerless and thralling world
of Daniel Schmottenberger. Enjoy!
I do not know with what weapons World War III will be fought, but World War IV will be fought with
sticks and stones. All right, Daniel, what have you been up to in the past few years?
Um, past few years. Trying to understand the unfolding global situation and the
trajectories towards existential and global catastrophic risk in particular,
the solutions to those that involve control mechanisms that create trajectories towards
dystopias, and the consideration of what a world that is neither in the attractor basin of catastrophe
or dystopia looks like a kind of third attractor. What would it take to have a civilization that
could steward the power of exponential technology much better than we have stewarded all of our
previous technological power? What would that mean in terms of culture and in terms of political
economies and governance and things like that? So, thinking about those things and acting on
specific cases of near-term catastrophic risks that we were hoping to ameliorate and helping
with various projects on how to transition institutions to be more intelligent and things
like that. What are some of these near-term catastrophic risks?
Well, as of today, we are in a war that has moved the atomic clock closer to midnight than it has
ever been, and that's a pretty obvious one. If we were to write a book about the folly of the
history of human hubris, we would get very concerned about where we are confident about
where we're right, where we might actually be wrong, and the consequences of it. As we're
dealing with nukes and AI and things like that, we could easily have the last chapter in that book
if we are not more careful about confident wrong ideas. So, what are all the assumptions
in the way we're navigating that particular conflict that might not be right? What are the
ways we are modeling the various sides and what would an end state that is viable for the world,
and that just at minimum doesn't go to a global catastrophic risk? That's an example.
If we look at the domain of synthetic biology as an exponential, as a different kind of
advanced technology, exponential tech, and we look at that the cost of things like gene sequencing
and then the ability to synthesize genomes, gene printing are dropping faster than Moore's law
in cost. Well, open science means that the most virulent viruses possible studied in
contexts that have ethical review boards getting open published, then that's a situation where
that knowledge combined with near term decentralized gene printers is decentralized catastrophe
weapons on purpose or even accidentally. There are heaps of examples in the environmental space.
If we look at our planetary boundaries, climate change is the one people have the most
awareness of publicly, but if you look at the other planetary boundaries like
mining pollution or chemical pollution or nitrogen dead zones and oceans or biodiversity
loss or species extinction, we've already passed certain tipping points. The question is
how runaway are those effects? There was an article published a few months ago on PFOS and
PFAS, the fluorinated surfactants forever chemicals, as they're popularly called,
that found higher than EPA allowable standards of them in rainwater all around the world,
including in snowfall and Antarctica, because they actually evaporate and we're not slowing down
on the production of those and they're endocrine disruptors and carcinogens and that doesn't
just affect humans, but affects things like the entirety of ecology and soil microorganisms.
This kind of humongous effect. So those are all examples. And I would say right now,
I know the topic of our conversation today is AI. AI is both a novel example of a possible
catastrophic risk through certain types of utilization. It is also an accelerant to every
category of catastrophic risk potentially. So that one has a lot of attention at the moment.
So that makes AI different than the rest that you've mentioned?
Definitely. And are you focused primarily on avoiding disaster or moving towards something
that's much more heavenly or positive, like a Shangri-La?
So we have an assessment called the metacrisis. The it's a more popular term out there right
now, the polycrisis. We've been calling this the metacrisis since before coming across that term.
Polycrisis is the idea that the global catastrophic risk that we all need to focus on and coordinate
on is not just climate change and is not just wealth inequality and is not just kind of the
breakdown of Pax Americana and the possibility of war or these species extinction issues,
but it's lots of things. There's lots of different global catastrophic risks.
And that they interact with each other and they're complicated and there can even be
cascades between them, right? We don't have to have climate change produce total
venusification of the earth to produce a global catastrophic risk. It just has to increase the
likelihood of extreme weather events in an area. And we've already seen that happening,
statistics on that seem quite clear. And it's not just total climate change, deforestation,
affecting local transpiration and heat in an area can have an effect on and total amount of
pavement laden whatever can have an effect on extreme weather events. But extreme weather events,
I mean, we saw what happened to Australia a couple of years ago when a significant percentage
of a whole continent burned in a way that we don't have near term historical precedent for.
We saw the way that droughts affected the migration that led to the whole Syrian conflict that got
very close to a much larger scale conflict. The Australia situation happened to hit a low
population density area, but there are plenty of high population density areas that are getting
very near the temperatures that create total crop failures, whether we're talking about India,
Pakistan, Bangladesh, Nigeria, Iran. And so if you have massive human migration, the UN currently
predicts hundreds of millions of climate-mediated migrants in the next decade and a half,
then it's pretty easy under those situations to have resource wars.
And those can hit existing political fault lines and then technological amplification. And so
in the past, we obviously had a lot less people, right? We only had half a billion people for the
entirety of the history of the world till the Industrial Revolution. And then with the Green
Revolution and nitrogen fertilizer and oil and like that, we went from half a billion people to
eight billion people overnight in historical timelines. And we went from those people mostly
living on local subsistence to almost all living on dependent upon very complicated supply chains
now that are six-continent mediated supply chains. So that means that there's radically more fragility
in the life support systems so that local catastrophes can turn to breakdowns of supply
chains, economic effects, et cetera, that affect people very widely. So polycrisis, kind of looking
at all that, metacrisis adds looking at the underlying drivers of all of them. Why do we
have all of these issues? And what would it take to solve them not just on a point-by-point basis,
but to solve the underlying basis? So we can see that all of these have to do with coordination
failures. We can see that underneath all of them, there are things like perverse economic interests.
If the cost of the environmental pollution to clean it up was something where in the process of
the corporation selling the PFOS as a surfactant for waterproofing clothes or whatever, it also
had to pay for the cost to clean up its effect in the environment or the oil cost had to clean up
the effect on the environment. So you didn't have the perverse incentive to externalize costs onto
nature's balance sheet, which nobody enforces. Obviously, we'd have none of those environmental
issues. That would be a totally different situation. So can we address perverse incentive,
writ large? That would require fundamental changes in what we think of as economy and how we enact
that so political economy. So we think about those things. So I would say with the metacrisis
assessment, we would say that we're in a very novel position with regard to catastrophic risk,
global catastrophic risk, because until World War II, there was no technology big enough to
cause a global catastrophic risk as a result of dumb human choices or human failure quickly.
And then with the bomb, there was the beginning. And that's a moment ago in evolutionary time.
And if we reverse back a little bit before the bomb, until the industrial revolution,
we didn't have any technology that could have caused global catastrophic risk even cumulatively.
The industrial technology, extracting stuff from nature and turning it into human stuff
for a little while before turning it into pollution and trash, where we're extracting
stuff from nature in ways that destroy the environment faster than nature can replenish
and then turning it into trash and pollution faster than it can be processed, and doing
exponentially more of that because it's coupled to a economy that requires exponential growth to
keep up with interest. That creates an existential risk. It creates a catastrophic risk within about
a few centuries of cumulative effects. And we're basically at that few century point.
And so that's very new. All of our historical systems for that, our historical systems
for thinking about governance in the world didn't have to deal with those effects. We
could just kind of think about the world as inexhaustible. And then of course, when we got
the bomb, we're like, all right, this is the first technology that rather than racing to implement,
we have to ensure that no one ever uses. In all previous technologies, there was a race to implement
it. It was a very different situation. But since that time, a lot more catastrophic technologies
have emerged. Catastrophic technologies in terms of applications of AI and synthetic biology and
cyber and various things that are way easier to build the nukes and way harder to control.
And when you have many actors that have access to many different types of catastrophic technology
that can't be monitored, you don't get mutually assured destruction and those types of safeties.
So we'd say that we're in a situation where the catastrophic risk landscape is novel, nothing in
history has been anything like it. And the current trajectory doesn't look awesome for making it
through what it would take to make it through actually requires change to those underlying
coordinate coordination structures of humanity very deeply. So I don't see a model where we do
make it through those that doesn't also become a whole lot more awesome. And that's what we say
and that's what we say. The only other example is to control for catastrophes,
you can try to put very strong control provisions. Okay, so now unlike in the past,
people could or pretty soon have gene drives where they could build pandemic weapons in their
basement or drone weapons where they could take out infrastructure targets or now AI weapons even
easier. We can't let that happen. So we need to liquid a surveillance to know what everybody's
doing in their basement because if we don't in the world is unacceptably fragile. So we
can see catastrophes or dystopias, right? Because most versions of ubiquitous surveillance are
pretty terrible. And so if you can control decentralized action, if you don't control
decentralized action, the current decentralized action is moving towards planetary boundaries
and conflict and etc. If you control it, then who what are the checks and balances on that control?
Sorry, what do you mean control decentralized actions?
So when we look at what is what causes catastrophe. So when we're talking about
environmental issues, there's not one group that is taking all the fish out of the ocean
or causing species extinction or doing all the pollution. There's a decentralized incentive
that lots of companies share to do those things. So nobody's intentionally trying to remove all
the fish from the ocean. They're trying to meet an economic incentive that they have that's associated
with fishing, but the cumulative effect of that is overfishing the ocean, right? So if you try to,
if there's a decentralized set of activity where the lack of coordination of everybody doing that,
everybody pursuing their own near term optimum creates the shitty term global minimum for everybody,
right? A long term bad outcome for everybody. If you try to create some centralized control
against that, that's a lot of centralized power and where the checks and balances on that power.
Otherwise, how do you create decentralized coordination? And similarly, if you're looking at
things like in an age where terrorism can get exponential technologies and you don't want
exponentially empowered terrorism with catastrophe weapons for everyone,
to be able to see what's being developed ahead of time, does that look like a degree of surveillance
that nobody wants to be able to control those things not happening, right? So I mean,
how to prevent the catastrophes, if the catastrophes are currently the result of the
human motivational landscape in a decentralized way, if the solution is a centralized method
powerful enough to do it, where the checks and balances on that power. So a future that is neither
cascading catastrophes nor control dystopias is the one that we're interested in. And so yes,
I would say the whole focus is that this is now AI comes back into the topic because a lot of people
see possibilities for a very pro-topian future with AI where it can help solve coordination issues
and solve lots of resource allocation issues. It also, and it can, it can also make lots of things.
The catastrophes worse and dystopias worse. It's actually kind of unique in being able to make
both of those things more powerful. Can you explain what you mean when you say that the
negative externalities are coupled to an economy that depends on exponential growth?
Yeah. So
if you think about it just in the first principle way, the idea is supposed to be something like
there are real goods and services that people want that improve their life that we care about.
And so the services might not be physical goods directly. There might be things humans are doing,
but they still depend upon lots of goods, right? If you are going to provide a consultation over a
Zoom meeting, you have to have laptops and satellites and power lines and mining and all
those things. So you can't separate the service industry from the goods industry. So there's
physical stuff that we want. And to mediate the access to that and the exchange of it,
we think about it through a currency. So it's supposed to be that there's this physical stuff
and the currency is a way of being able to mediate the incentives and exchange of it.
But the currency starts to gain its own physics, right? So we make a currency that has no intrinsic
value that is just representative of any kind of value we could want. But the moment we do something
like interest, where we're now exponentiating the monetary supply, independent of an actual
automatic growth of goods or services, to not debase the value of the currency,
you have to also exponentiate the total amount of goods and services. And everybody's seen
how compounding interest works, right? Because you have a particular amount of interest and then
you have interest on that amount of interest. So you do get an exponential curve. Obviously,
that's just the beginning financial services as a whole. And all of the dynamics where you have
money making on money, mean that you expand the monetary supply on an exponential curve,
which was based on the idea that there is a natural exponential curve of population.
Anyways, and there is a natural growth of goods and services correlated. But that was true at
an early part of a curve that was supposed to be an S curve, right? You have an exponential
curve that in flex goes into an X curve, but we don't have the S curve part of the financial
system planned. The financial system has to keep doing exponential growth or it breaks.
And not only is that key to the financial system, because what does it mean that the financial
system without interest, say it's a very deeply different system, that formalizing that was also
key to our solution to not have World War III, right? The history of the world in terms of
war does not look great that the major empires and major nations don't stay out of violent conflict
with each other very long. And World War I was supposed to be the war that ended all wars,
but it wasn't. We had World War II. Now this one really has to be the war that ends all major
superpower wars because of the bomb. We can't do that again. And the primary basis of wars,
one of the primary bases had been resources, which was a particular empire wanted to grow and get
more stuff. And that meant taking it from somebody else. And so the idea of if we could
exponentially grow global GDP, everybody could have more without taking each other's stuff.
It's so highly positive sound that we don't have to go zero someone war. So the whole post-World
War II banking system, the Bretton Woods monetary system, et cetera, was part of the how do we not
have World War, along with mutually assured destruction, the UN and other international
intergovernmental organizations. But that let's exponentially grow the monetary system also meant
that if you have a whole bunch more dollars and you don't have more goods and services, the dollars
become more or less and it's just inflation and debasing the currency. So now you have an artificial
incentive to keep growing the physical economy, which also means that the materials economy
has to have an exponential amount of nature getting turned from nature into stuff into trash and
pollution in the linear materials economy. And you don't get to exponentially do that on the finite
biosphere forever. So the economy is tied to interest and that's at the root of this of what
you just explained, not at the root of every catastrophe. Interest is the beginning of what
all of the financial services do, but there is an embedded growth obligation of which interest
is the first thing you can see on the economic system. The embedded growth obligation that creates
exponentiation of it tied to the physical world where exponential curves don't get a run forever
is one of the problems. There are a handful. This is where we're thinking about a crisis. What are
the underlying issues? This is one of the underlying issues. There's quite a few other ones that we
can look at to say, if we really want to address the issues, we have to address it at this level.
What's the issue with transitioning from something that's exponential to sub exponential when it
comes to the economy? What's the issue with it? Well, I mean,
there's a bunch of ways we could go. There is a old refrain from the hippie days and it seems
very obvious. I think as soon as anyone thinks about it, which is that you can't run an exponential
growth system on a finite planet forever. That seems kind of obvious and intuitive.
Because it's so obvious and intuitive, there's a lot of counters to it.
One counter is we're not going to run it on a finite planet forever. We're going to become an
interplanetary species, mine asteroids, ship our ways to the sun, blah, blah, blah.
I don't think that we are anywhere near close, independent of the ethical or aesthetic argument
on if us obliterating our planet's carrying capacity and then exporting that to the rest of
the universe is a good or lovely idea or not. Independent of that argument, the timelines
by which that could actually meet the humanity super organism growing needs relative to the
timelines where this thing starts failing don't work. That's not an answer. That said,
the attempt to even try to get there quicker is a utilization of resources here that is speeding
up the breakdown here faster than it is providing alternatives. The other answer people have to
why there could be an exponential growth forever is because digital. That more and more money
is a result of software being created, a result of digital entertainment being created,
and that there's a lot less physical impact of that. We can keep growing digital goods
because it doesn't affect the physical plan and physical supply chain so we can keep the
exponential growth up forever. That's very much the Silicon Valley take on it.
Of course, that has an effect. It does not solve the problem. It's pretty straightforward to see
why which is for let's go ahead and say software in particular. Does software have to run on hardware
where the computer systems and server banks and satellites and etc. require massive mining which
also requires a financial system in police and courts to maintain the entire cybernetic system
that runs all that? Yes, it does. Does a lot more compute require more of that? More
atoms, adjacent services, energy? Yes. Also, for us to consider software valuable,
it's either because we're engaging with what it's doing directly. That's the case in entertainment
or education or something, but then it is interfacing with the finite resource called human
attention, of which there is a finite amount. Or because we're not necessarily being entertained
or educated or engaging with it, but it's doing something, for us to again to consider valuable,
it is doing something to the physical world. The software is engaging, say supply chain
optimization or new modeling for how to make better transistors or something like that.
But then it's still moving atoms around using energy and physical space, which is a finite
resource. If it is not either affecting the physical world or affecting our attention,
why would we value it? We don't. So it still bottoms out on finite resources. So I can't
just keep producing an infinite amount of software where you get more and more content that nobody
has time to watch and more and more designs for physical things that we don't have physical
atoms for, energy for. You get a diminishing return on the value of it if it's not coupled to things
that are finite. The value of it is in modulating things that are also finite. So there's a
coupling coefficient there. You still don't get an exponential curve. So what we just did is say
the old hippie refrain, you can't run an exponential economy on a finite planet forever.
The alt, the counters to it don't hold. What about mind uploading or some computer brain
interface to allow us to have more attention exponentially?
Yeah, so it's another, it's kind of, that's almost like the hybrid of the other two, right,
which is get beyond this planet and do it more digitally. So get beyond this brain
and become digital gods in the singularity universe.
I, again, I think there are pretty interesting arguments we can have ethically, aesthetically,
and epistemically about why that is neither possible nor desirable.
But independent of those, I don't think it's anywhere close. And same like the
multi-planetary species, it is nowhere near close enough to address any of the timelines we have
by which economy has to change because the growth imperative on the economy as is,
moving us towards catastrophic tipping points. So if it were close, would that change your
assessment or you still have other issues? If it were close, then we would have to say
first that is implying that we have a good reason to think that it's possible, right,
that it's possible to, and that means all the axioms that consciousness is substrate independent
that the consciousness is purely a function of compute, strong computationalism holds
that we could map the states of the brain and or if we believe in embodied cognition,
the physiology adequately to represent that informational system on some other substrate
that that could operate with an amount of energy that is and substrate that's possible, blah,
blah, blah. So first we have to believe that's possible. I would question literally every one
of the axioms or assumptions that you said. We're going to get to that. We would say,
is it desirable? And how do we know how ahead of time? And now you get something very much like
how do I know that the AI is sentient, which the for the most part on all AI risk topics,
whether it's sentient or not as irrelevant, whether it does stuff, it's all that matters.
But how do you tell if it's sentient and all of the
Chalmers, Pesambi questions or whatever are actually really hard? Because what we're asking is how
can we use third person observation to infer something about the nature of first person,
given the ontological difference between them? So how would we know that that future is desirable?
What would the are there safe to fail tests? And what would we have to test to know it to
start making that conversion? But I don't think we have to answer any of those questions because
I don't think that anybody that is working on whole brain emulation thinks that we are close
enough that it would address the timeline of the economy issues that you're addressing.
Let's attempt to address one of the questions about substrate independence.
What are your views? Is consciousness something that our biological brains do or that requires
development from an embryonic stage like whatever it is that produced us, there's something special
about us or animals, or is it something that can be transferred or started up booted up from scratch
into what's not us, like decidedly not us a computer? Okay, so this is now much more a proper
theory of everything conversation than the topic that we intended for the day, which is about AI
risk. So what I will do is say briefly the conclusion of my thoughts on this without actually going
into it in depth, but I would be happy to explore that at some point. I think that how I come to
my position on it to try to do a kind of proper construction takes a while. So briefly I'll say
I'm not a strong computationist, meaning don't believe that mind, universe, sentience,
qualia is purely a function of computation. I am not an emergent physicalist that believes
that consciousness is an epiphenomena of non-conscious physics that in the same way that we have
weak emergence, more of a particular property through certain kind of combinatorics or strong
emergence, new properties emerging out of some type of interaction where that hasn't occurred before,
like a cell respirating or none of the molecules that make it up respirate. I believe in weak
emergence that happens all the time. You get more of certain qualities that happens in metallurgy
when you combine metals where the combined tensile strength or shearing strength or whatever is more
than you would expect as a result of the nature of how the molecular lattice is formed. You get more
of a thing of the same type. I believe in strong emergence, which is you get new types of things
you didn't have before, like respiration and replication out of parts, none of which do that.
But those are all still in the domain of third person, accessible things. The idea of radical
emergence, that you get the emergence of first person out of third person or third person out of
first person, which is idealism on one side and physicalism on the other, I don't buy either of.
I think that idealism and physicalism are similar types of reductionism,
where they both take certain ontological assumptions to bootload their epistemology and then get
self-referential dynamics. So I don't think that if a computational system gets advanced enough,
automatically consciousness pops out of it. That's one. Two, I do think that the process of a system
self-organizing is fundamentally connected to the nature of experience of selfness and things that
are being designed and are not self-organizing, where the boundary between the system and its
environment that exchanges energy and information and matter across the boundary is a
auto-poetic process. I do believe that's fundamental to the nature of things that have
self-other recognition. And on substrate independence, I do believe that carbon and
silicon are different in pretty fundamental ways that don't orient to the same types of
possibilities. And I think that that's actually pretty important to the AI risk argument.
But so I'll just go ahead and say those things. I also don't think I believe that embodied cognition
in the Demasio sense is important and that a scan of purely brain states isn't sufficient.
I also don't think that a scan of brain states is possible even in theory.
And sorry to interrupt. I know you said you don't believe it's possible. What if it is? And
you're able to scan your brain state and body state. So we take into account the embodied cognition.
Sure. So
I think that, okay, it's not simply a matter of scanning the brain state. We need to scan the
rest of the central nervous system. No, we also have to get the peripheral nervous system. No,
we have to get the endocrine system. No, all of the cells have the production and reception of
neuroendocrine type things. We have to scan the whole thing. Does that then extend to
the microbiome, viral, etc.? I would argue yes. Does it then extend to the environment?
I would argue yes. Where does that stop its extension is actually a very important question.
So I would take the embodied cognition a step further. The other thing is Stuart Kaufman's
arguments about quantum amplification to the mesoscopic level, that quantum mechanical events
don't just fully cancel themselves out at the subatomic level and at the level of brains,
everything that is happening is straight forwardly classical, but that there is
quantum mechanical, some fundamental kind of indeterminism built in phenomena,
they end up affecting what happens at the level of molecules. Now then one can say, well,
is that just mean we have to add a certain amount of a random function in or is there something
else? This is a big rabbit hole, I would say for another time, because then you get into
quantum entanglement and coherence. So you get something that is neither
perfectly random meaning without pattern. You get important distribution even on a single one,
but it's also not deterministic or with hidden variables. So do I think that what's happening
in the brain body system is not purely deterministic? And also as a result of that,
means you could not measure or scan it even in principle in a kind of Heisenberg sense.
Yes, I think that. Have you heard of David Walpart and his limits on inference systems,
inference machines, sorry? I have not studied his work. Okay. Well, anyway, he echoes something
similar, which says that you can't have Laplace's demon even in a classical world, you can't have
Laplace's demon. So let me talk about the economy, which only on your podcast would happen.
Why is it that if somehow this exponential curve starts to get to where the S is the top of the S
that the halting or the slowing down of the economy is something that's so catastrophic and
calamitous, rather than something that would mutate. And if we need to just at that point,
as it starts to slow down, we make minor changes here and there, is this something that's
entirely new, like will it all come crashing down? Okay, so let me make the question clear.
It sounds like, look, the economy is tied to exponential growth, we can't grow exponentially,
virtually no one believes that. So at some point, and let's just imagine it's three decades,
just to give some numbers. So at some point, three decades from now, this exponential curve for all
of the economy will start to show its legs and start to weaken and we'll see that it's nearing
the S part. So what does that mean that there's been fire in the streets, that the buildings
don't work, that the water doesn't run anymore, like what will happen? Okay.
So people often make jokes about physicists in particular, starting to look at biology and
language and society and modeling in particularly funny reductionist ways,
because they try to map the entire economy through the second law of thermodynamics or
something like that. And because what we're really talking about is the maximally complex
and anthropocomplex thing and embedded complexity we can, because we're talking about all of human
motives and how do humans respond to the idea that there is fundamentally limits on the growth
possible to them, or there's less stuff possible for them, or that there, whether it's issues
that are associated with environmental extraction. So here's one of the classic challenges,
is that the problems, the catastrophic risks, many of them in the environmental category,
are the result of cumulative action long term where the upsides are the result of individual
action short term and the asymmetry between those is particularly problematic. It's why you get this
collective choice making challenge, meaning if I cut down a tree for timber, I don't
obviously perceive the change to the atmosphere or to the climate or to watersheds or to anything,
but my bank account goes up through being able to sell that lumber immediately.
And the same is true if I fish or if I do anything like that. But when you run the
Kantian categorical imperative across it and you have the movement from half a billion people doing
it to a pre-industrial revolution to eight billion, and you have something like in the
industrial world, 100x resource per capita consumption, just calorically measured today
then at the beginning of the industrial revolution, then you start realizing, okay, well the cumulative
effects of that don't work. They break the planet and they start creating tipping points
that auto propagate in the wrong direction. But no individual person or even local area doing the
thing recognizes their action is driving that downside. And how do you get global enforcement
of the thing? And if you don't get global enforcement, why should anyone let themselves
be curtailed and other people aren't being curtailed and that'll give them game theoretic advantage?
So this is actually, there's a handful of asymmetries that are important to understand
with regard to risk. All right, we've covered plenty so far. And so it's fruitful to have a
brief summary. We've talked about the faulty foundation of our monetary system. Daniel argues
that post-World War II, especially our economic system is not only encouraged, but been dependent
on exponential monetary growth. And this can't continually occur. We've also talked about the
digital escape plan and how this is an illusion, at least in Daniel's eye. He believes that digital
growth has physical costs because their hardware, their human attention limits their finite resources,
linear resources, as he calls them, though I have my issues with the term linear resource,
because technically anything is linear when measured against itself. We've also talked about
how moving to Mars won't save us, us being civilization. Daniel believes that the idea
of becoming an interplanetary species to escape resource limitations is unrealistic,
perhaps even ethically questionable. We've also talked about how mind uploading is not
what it's cracked up to be. It may not occur. And even if it does, it's not the answer because
it's either unfeasible, but even if it's feasible, Daniel believes it to be undesirable. Another
resource as we expand our digital footprint is the privacy of our digital resources. You can see
this being recognized even by OpenAI as they recently announced an incognito mode. And this is
where our sponsor comes in. Do you ever get the feeling that your internet provider knows more
about you than your own mother? It's like they're in your head, they can predict your next move.
When I'm researching complicated physics topics or checking the latest news or just in general
what I want privacy on, I don't want to have to go and research which VPN is best. I don't want
to be bothered by that. Well, I and you can put those fears to rest with private internet access.
The VPN provider that's got your back with over 30 million downloads, they're the real deal when
it comes to keeping your online activity private. And they've got apps for every operating system
you can protect 10 of your devices at once, even if you're unfortunate enough like me to love Windows.
And if you're worried about strange items popping up in your search history, don't worry,
I'm not judging. Private internet access comes in here as they encrypt your connection. They hide
your IP address so your ISP doesn't have access to those strange items in your history. They make
you a ghost online. It's like Batman's Cave before your browsing history. With private internet access
you can keep your odd internet searches, let's say, on the down low. It's like having your own
personal confessional booth, except you never need to talk to a priest. So why wait? Head over to
piavpn.com slash toe to and get yourself an 82 and 82% discount. That's less than the price of a
coffee per month. And let's face it, your online privacy is worth way more than a latte. That's
piavpn.com slash to now and get the protection you deserve. Brilliant is a place where there are
bite sized interactive learning experiences for science, engineering and mathematics. Artificial
intelligence in its current form uses machine learning which uses neural nets, often at least,
and there are several courses on Brilliant's website teaching you the concepts underlying neural
nets and computation in an extremely intuitive manner that's interactive, which is unlike almost
any of the tutorials out there. They quiz you. I personally took the course on random variable
distributions and knowledge and uncertainty because I wanted to learn more about entropy,
especially as there may be a video coming out on entropy, as well as you can learn group theory on
their website, which underlies physics that is SU3 cross SU2 cross U1 is the standard model gauge
group. Visit brilliant.org slash toe to get 20% off your annual premium subscription. As usual,
I recommend you don't stop before four lessons. You have to just get wet. You have to try it out.
I think you'll be greatly surprised at the ease at which you can now comprehend subjects you
previously had a difficult time grokking. The bad is the material from which the good may learn.
So this is actually, there's a handful of asymmetries that are important to understand
with regard to risk. One is this one that I'm saying, which is you have risks that are the
results of long term cumulative action, but that you actually have to change individual action
because of that. But the upside, the benefit, the individual making that action realizes the
benefit directly. And so this is a classic tragedy of the commons type, I should write,
but tragedy of the commons at a, not just local scales, but at global scales.
Some of the other asymmetries are particularly important is
people who focus on the upside, who focus on opportunity do better game theoretically for
the most part than people who focus on risk when it comes to new technologies and advancement and
progress in general. Because if someone says, Hey, we said, we thought Vioxx or DDT or any
number of things were good ideas, they ended up, or leaded gasoline, they ended up being really bad.
Later, we want to do really good long term safety testing regarding first, second, third order
effects of this. They're going to spend a lot of money and not get first to market and then
probably decide the whole thing wasn't a good idea at all. Or if they do decide how to do a
safe version, it takes them a very long time. The person says, no, the risks aren't that bad.
Let me show you does a bullshit job of risk analysis as a box checking process. And then really
emphasizes the upsides is going to get first mover advantage, make all the money, they will
privatize the gains, socialize the losses, then when the problems get revealed a long time later
and are unfixable, that will have already happened. So these are just examples of some
of the kind of choice making asymmetries that are significant to understand the situation.
I only partly answered your question. Sure. Are you having in mind a particular corporation currently?
Totally. Okay. Not a particular corporation, but a particularly important consideration in
the entire topic. One view is that Google is not coming out with something that's competitive,
like Bart is not competitive. I think even Google would admit that. And so one view is that, well,
they're highly testing. Another one, I've spoken to some people behind the scenes and they say,
Google doesn't have anything. They don't have anything like chat GPT. It's BS when they say so.
Even open AI doesn't know why chat GPT works like GPT for works as well as it does. They just
threw so much data at it and it was a surprise to them and in some ways they got lucky. So do you
see what's happening right now between Microsoft and Google as Google is actually the more cautious
one and Microsoft is the more brazen one and perhaps should be a bit more circumspect?
I have heard a lot of things about the choices that both companies made to not release stuff
and safety studies that they did and then what influenced the choices to release stuff inside
of Microsoft and open AI and how Google is handling it. I don't know that these stories are
the totality of information on it that's relevant. Do I think that economic
forcing functions have played a role in something that affected the safety analysis totally?
Do I think that that is an unacceptably dumb thing on a topic that has this level of
safety risk associated totally? So now getting into what is unique about AI risk,
right? What is unique about it relative to all other risks? Do we people are saying things like
we need an FDA for AI right now? Which I would argue is both true and a profoundly inadequate
analogy because a single new chemical that comes out is not an agent. It is not a dynamic thing
that continues to respond differently to huge numbers of new unpredictable stimuli. So how
you do the assessment of the phase phase of possible things is totally different.
It would probably be good to dive into what is the risk space of AI? Why is it unique and how
given all of the differences of concern, how to framework and think about that properly?
What else is unique about it and why can't we have an FDA or a UN version of an FDA for
AI? And when I say UN, sorry, what I mean is global.
Yeah. Well, obviously you bring up UN and say global because you have to have global
regulation on something like that, right? In the same way that when people talk about climate
regulation, if we were, if any country, if any group of countries was to try to price
carbon properly, meaning what does it take to renewably produce those hydrocarbons?
And what does it take to in real time fix all of the effects both sequester the CO2,
clean up the oil spills, whatever it is? The price of oil would become high enough
with those costs internalized that oil then as an input to industries, literally every industry
would be non-profitable. And so even if any country was to try to make some steps in the
direction of internalizing cost and other ones didn't, then the other ones who continued externalize
their costs get so much further ahead in terms of GDP that can be applied to militaries and surplus
of many different kinds and advancing exponential tech that insofar as those are also competing
entities for world resources and control, that's not a viable thing. This is true for AI as well.
And this then starts to hit this other issue, which is if you can't regulate something on a
purely national level, because it's not just how does it affect the people in the nation,
but how does it affect the nation's capability to interact with other nations?
Now you get to the, and so the creation of the UN was kind of the recognition and the existence
in the emergence of World War II that nation state governance alone was not adequate to prevent
World War. Obviously that's why the League of Nations came after World War I and it was not
strong enough to prevent World War II. Now you get to the topic of why so many people are super
concerned about global government and don't want global government and they'll say things like
the risks are being exaggerated and blown out of proportion to be able to drive control paradigms
and the people who want to have a one-world government or a powerful nation government
exaggerate the risks so that they can drive control paradigms where they will be the one
in the control side. This can be excessive paranoia, but it's also a really realistic and
founded consideration, which is are there any radical asymmetries of power where the side that
had all the power used it really well historically? It doesn't look that good, right? And so we see
a reason to be concerned about something like a one-world government that has no possible checks
and balances, but there's also a concern about not having anything where you get some type of
global governance. If not government, meaning some unified establishment that has monopoly
of violence, at least governance, meaning some coordination where everyone is not left in the
multipolar trap saying we can't bind our behavior because they won't and if they won't then we have
to race ahead, right? We can't stop overfishing because the fish will all get killed because
they're doing the thing anyway, so not only will we not stop, we will actually race to do it faster
than them so they don't get more resource relative to us, those types of issues. So obviously with
regard to the environment, we call it a tragedy of the commons with regard to the development of
possible military technology, we call them an arms race. Both of them are examples of social
traps or multipolar traps. Briefly, why do you call it a multipolar trap?
Social trap is a term used in the social sciences quite a lot to indicate a coordination failure
of this type where each agent pursuing their own near-term rational interest creates a situation
that moves the entire global situation long-term to a suboptimal equilibrium for everybody, right?
And there's a lot of work in various fields of social science on social traps.
The first time I'm aware of the term multipolar trap entering the conversation was in a great
article called Meditations on Molok by Scott Alexander where he, I believe he's the one who
coined the term multipolar trap there and it's pretty close to a social trap. If I was going to
define a distinction, it might be something like
in a classic tragedy of the commons scenario where everyone is utilizing a common wealth
resource like say fishing or cutting down trees in a forest or whatever.
You're not necessarily in the situation where everyone is racing to do it faster than the
other person to destroy it, just them simply not curtailing their own behavior and yet you have
a resource per capita consumption growth and a total population growth sets that the environment
can't deal with it. You still end up getting environmental devastation, but as soon as you
kind of move over into, hey, even if I don't cut down the trees or I don't fish, the other side is
going to, so I literally don't have the ability to protect the forest, but I do have the ability to
cut down some of it, benefit myself or our people or tribe or nation or whatever it is.
And if I don't, the other guys will break it down anyways, but they'll also use the economic
advantage of that against us and whatever the next rivalrous conflict is. So not only do I have
to keep doing it, but I have to race to do it faster than they do. I actually have to apply
innovation now. And so this is where you get an accelerating dynamic. And if you don't just have
two actors doing this, but you have many actors doing this, where it's very hard to be able to
bind it because how do you ensure that all the actors are keeping the agreement, right? You
have to make some non-proliferation agreement. You have to have some way of ensuring that they're
all keeping it, and you have to have some enforceable deterrent if anyone violates it.
Those happen, but it's not trivial. It's not trivial to enact those. And it's particularly,
so let's say we've achieved that when it comes to nukes in some ways, though at the beginning of
the current post-World War II system, there was only two superpowers with nukes, and now
there's roughly nine countries with them. There are not 100 countries with them. There aren't
even 30 because we've done a really intense job of ensuring that Iran and many countries that want
nukes don't get them, right? And why? Because there are not uranium mines everywhere, kind of,
can see where they are. Uranium enrichment takes massive capability that you can literally see
from space, right? This radioactive activity associated. So it's somewhat easy to monitor
that that's happening. This is not true at all with the newer technologies that provide more
catastrophic capabilities. So obviously with AI right now and the regulation of it, there are
conversations about like, we need to monitor all large GPU clusters or something like that,
which to some degree can be done. But in terms of applications, it takes a very large GPU cluster
to develop an LLM. It takes a very small one to run that LLM afterwards, right? And then can you run
it for destructive purposes? And it takes a very large capability to advance something like a
CRISPR or a new type of synthetic bio knowledge. It doesn't take that much to be able to reverse
engineer it after it's been developed. So this brings up this very important point of
when the technology is built, there's this general refrain that all technology dual use,
right? Meaning that if it wasn't, sometimes it's developed for military purpose first and then
becomes used for civilian normal market purposes. But if it's being developed for some non-military
purpose, it's probably a militarized application. That's what's meant with dual use is military
versus non-military. This is a double-edged sword. It's positive and negative. It's not the same as
that. Yeah, it is. What it's saying is you're developing this for some purpose, but it has
other purposes too, right? And it has purposes that can be used for violence or conflict or
destruction or something. And well, that is historically mostly used with the concept of
has a military application can be used to advance war and killing and things like that,
whether by a state actor or a non-state actor. So you'd call it terrorist activity.
Sorry, when I was thinking of military, I was also thinking in terms of pure defense,
not just defense that also can be something that can attack.
Yeah. Yeah, the pure defense only military,
it starts becoming part of most military doctrines that
viable defense requires things that look like escalation. But that's another topic as well.
So it's not just that all technologies are dual use. It's that they have many uses. You develop
a technology and I think a good way to think about. So now this is a little bit of theory of tech.
Did we close multipolar trap? Well, you mentioned that it first came up in Scott
Erickson's or Alexander. Yeah. And so basically the concept is you have many different agents who
all of them pursuing their own rational interest and maybe they can't even avoid it because it
would be so irrational. It would be so bad for them game theoretically that the effect of each
of the agents pursuing their own rational interest produces a global effect that is somewhere between
catastrophic or at least far from the global optimum if they could coordinate better. So this is
basically a particular type of multi-agent coordination failure. And we see this all over
in the tragedy of the commons as an example, a market race to the bottom like happens in marketing
and attention currently. It's an example and then arms race is another example. Those would all be
examples of a kind of multipolar trap coordination failure. And this is why if you have say one
nation is advancing by weapons or advancing AI weapons, either AI applied to cyber or applied
to drones or applied to autonomous weapons of various kinds. If any country is doing that,
it is such an obvious strategic advantage that every other country has to be developing the same
types of things plus the whole suite of counters and defenses to those types of things. And so you
could just say, well, the world in which everybody has autonomous weapons is such a worse world than
this world that we should just all agree not to do it. Except how do I know the other guy is actually
keeping the agreement? Well, with the nukes we can tell because we can see if they're mining
uranium and enriching it because it takes massive facilities and they're radioactive and stuff like
that. But if we're talking about things like working with AI systems or synthetic biosystems that
don't require a bunch of exotic materials, exotic mining that don't produce radioactive
tracers, etc., and they can be done in a deep underground military base, how do we know if
they're doing it or not? So if we don't know if the other side is doing it or not, then the game
theory is you have to assume they are because if you assume they are, you're going to develop it as
well. And then if they do have it and use it, you aren't totally screwed. Whereas the risk on the
other assumption that they aren't, if you were wrong, you're totally screwed. So under not having
full knowledge, the game theory orients to worst case scenario and being prepared against the worst
case. But what that means is all sides assume that of each other. We don't know that the other guys
are keeping the agreement. Therefore, we have to race ahead with this thing. And so this is why
you're saying when it comes to things like AI, do we need something that is not just an FDA thing,
but a UN thing? This is the kind of thing that would require international agreement. And obviously,
when there was the question of creating a pause on a six month pause or whatever, one of the first
things people brought up is won't that let China race ahead? And isn't this a US-China
competitiveness issue? And we can see with the CHIPS Act in trying to ban ASML downstream type
GPUs to China. And we can see with the pressures over Taiwan and TSMC that there is actually a lot
of US-China great power play competition related to computation and AI in specific.
And so it's a classic situation that if you can't put certain types of control mechanisms
in internationally, you will probably fail at being able to get them nationally as well.
So about this competition where the tragedy of the commons such that like, well, the competitiveness
plus tragedy of the commons accelerates the tragedy of the commons, why is it not much
more simple, religiously simple, ethically simple, where we go back and we say, hey,
what I'm going to do is outputting something negative. I don't care that if you do it,
you're going to get ahead. I don't care if you're going to eliminate me. I would rather die for
your sins rather than contribute my own sins. So the selflessness, why isn't that sort of ethic,
like we say we don't want to be Luddites, but why isn't that a solution?
I mean, you're bringing up a great point, which is, can there be a long range thinking about
the kind of world we want to live in and a recognition of the kind of beings we have to be,
the behaviors we would have to do and not do for that world to come about where we bind ourselves,
right, where we have some kind of, whether the ethics reduces to law, meaning there's a monopoly
of violence that backs up the thing or not, can we at least self-police in some way towards it?
And the answer is, the long-term answer must involve that, I would argue.
Past examples have involved it, but let's talk about where it's limited.
One could argue that the Sabbath and the punishments for violating the Sabbath is an example
of binding a multipolar trap. So you're not going to work on the Sabbath, and if you do,
there's 29 different reasons laid out, why you can be killed for working on the Sabbath. It seems
to secular people not thinking about the Chesterton fence deeply, it seems like
a ridiculous, wacky religious idea, not grounded in anything within a ridiculous
amount of consequence. Now, your theory of justice is, is it only a personal or is it
a collective theory of justice, right? Some theories of justice are your punishment is not
based on just what was right for that one person, but creating an adequate deterrent for the entire
population, because if you don't, what happens? So a classic example is Singapore's drug policy
is pretty harsh, right? Life in prison for just possession of drugs. Well, that was following
the devastating effect that the British had on the Chinese with the opium wars,
and recognizing how as a kind of population-centric warfare, the British were able to influence
like catastrophic damage on China. They're like, we don't want that here, and we know that there
are external forces that will push to do that kind of thing, and it's not just personal choice,
once there are asymmetric forces trying to affect the most vulnerable people in the most
vulnerable ways. So we're going to make it to where the deterrent on drug use is so bad,
nobody will do it. So if you, if you say that, you actually have to lock somebody up forever
for smoking pot, which feels very unfair to them. But you probably only have to do that
like a few times before nobody ever will fucking touch it because the deterrent is so bad, and
they believe it'll be enforced, we're hands off. And if the net effect on the society as a whole
is that you don't have black markets associated with drugs and gangs and the violence that's
associated, and you don't have ODs and you don't have the susceptibility to population-centered
warfare and whatever, they might argue a utilitarian ethical calculus that the harsh
punishment was radically less harm to the total situation than not having that, right? So you
have a strong deterrent. So that's just, I'm not saying that I think that is an adequate theory
of justice, but it is a theory of justice, right? So let's say that the Sabbath was something like
this, and I'm not saying that the rabbis that were creating it at the time thought this, so many
people suggest that that's probably what they thought. Some very competitive people wanting
to get ahead will work every day. If they'll work seven days a week, and as a result, they will
they'll be able to get a little bit more grain farming, whatever, than other people get more
surplus, start turning that into compounding benefits. And if anyone does, it'll create a
competitive pressure where everyone has to. So nobody spends any time with their family, nobody
spends any time connecting to what binds the culture together, the religious idea, etc. So
we're going to make a Sabbath where no one is even allowed to work, and there's such a harsh
punishment against it that we're binding the multipolar trap, right? Because even though it
would make sense, and they're in that person's rational interest to work that extra day a few
times to get ahead, the net effect on the society cumulatively is actually a shittier world. So
we're going to bind it because people having that time off to be with their family, each other,
and studying ethics is a good idea. I would argue that religion has heaps of examples like this
of how do we bind our own behavior to be aligned with some ethic? But I would also argue,
because that was the question you were asking, right? Is there some kind of religious bind to
the multipolar trap? And I think that the Sabbath is a good example.
I think we can also show how well that didn't work for Tibet when China invaded,
right? Which is we want to be nonviolent, we have a religion that's oriented towards nonviolence.
And we can see that there were, if you think about it, the time of Genghis Khan or Alexander the
Great or whatever, where you have a set of worldviews that doesn't constrain itself in that way.
And it's going to go initiate conflict with the people who didn't do anything to initiate it
and don't want it. But the worldview that orients itself that way also develops military capability
and maximum extraction for the surplus to do that thing. The other worldviews don't make it
through, they get wiped out because, so there are indigenous cultures and matriarchal cultures
and whatever that we just don't even have anymore, don't even have the ideas of remnants because
it just got wiped out by warring cultures. And so does that, does that produce the long-term world
we want? No, it doesn't either. And so there has been this kind of multipolar trap on
that the natural selection, if you want to call it that, of worldviews that make it through
are selected by their ability to grow their population, have outsized influence on other
population and win wars and basically things that don't necessarily map to a good world long-term.
But the things that might map to a good world long-term might not ever get to the long-term
because they get wiped out in the short term. Yeah, I don't buy that. So I'm not saying this
as someone who's religious or from a religious perspective. Well, this is a religious perspective,
sorry, but I'm not saying this as someone who's advocating for a certain religion. The most
dominant religion in the world is Christianity. And that's the story of someone who had the
government against him and he said, no, I'm not going to fight back. In fact, if you want to
persecute me, go ahead. I will come to you. And one of the most striking stories, literally
striking in the Bible to me is the story of Jesus, the captain, and Peter, his friend cut off the
captor's ear. The guy was going to take Jesus to kill Jesus and Jesus said, no, no, no, don't do that
and took the ear and healed his captor. So think about this though. Yes, Jesus is the guy who said,
let he who has no sins cast the first stone and they brought Mary Magdalene and all those things.
But we somehow did the Crusades in his name and the Inquisition in his name and the Dark Ages in
his name, right? That's some weird ass mental gymnastics. But the scenes, the versions that
were going to stay peaceful and not do Crusades, how many do you see around and how much power did
they get? So what happens is you have a bunch of different interpretations, the interpretations
that orient themselves to power and to propagation propagate and make it through the interaction
between the memes. So memes engage in a kind of competitive selection like genes do, but not
individual memes, meme complexes. So if we have a religion that says, be humble, be quiet, listen
to people and don't push your ideas on anybody. And then you have another one that says, go out and
proselytize, convert everyone to your religion and kill the infidels, which one gets more people
involved, right? And so the ones that have propagation and that have conflict ideas built
right in. So of course, then the meme sets evolve over time, right? The religious interpretations
don't stay the same. And the meme sets that end up winning through how they reduce themselves to
the behaviors that affect war and population growth and governance, et cetera, are all part of it.
So the fact that the dude who said, let he who has no sins among you cast the first stone got to be
the religion that became dominant through the Crusades and through violent expansionism and then
through radical torturous oppression is fascinating, right? And it shows you that you have like a real
philosophy and then you have politics of power and you have fusions of those things that you have to
understand both of when you're studying religion. To me, and I don't mean to harp on this point,
but it doesn't have to be a choice between, hey, let me do good and let me not push my
views on anyone and proselytizing slash killing, because you can also proselytize and say your
ideas and hopefully people will hopefully maybe there is something in us, maybe there's something
cosmically in us. I don't know that says, Hey, you know what? I like that. I don't like that killing.
I don't like where that will lead. It resonates with me that the sins get passed down or that the
violence gets passed down and amplified. But I need to be told that. So I do need to hear that
because I can't come up with that on my own. So that's why I'm saying the proselytizing is a part
of it, whether proselytizing is explicit or it's lived and you just see how someone lives and then
you inquire, Hey, what are your views? And why are you so happy when you have nothing and I'm so
miserable and I have everything? I just don't see it as a choice between you do good locally and
don't tell anyone about it or you can tell people about your ethical system, but also oppress them.
Well, to make the thought experiment, we picked both extremes.
So we can see that the Mormons proselytize, but they don't kill everyone who disagrees
in the crusading kind of way. They have not expanded as much as the crusades. They've not
got as much global dominance or total population as a result. But they have not got nowhere, right?
We can see that the ones that say, Hey, if someone is interested, we'll share, but we're not going
to proselytize because we have a certain humility of how much we don't know and a respect for everyone's
choice. The mystery schools stay pretty small. And again, when we were talking about asymmetries,
those who are more focused on the opportunity and downplay the risk move ahead, get the investment
capital, etc. And those who are focused on the risk heavily don't. There's a there's a similar
thing here, which is like there's an asymmetry and the ideas that hit evolutionary drivers,
even if perverse forms, right? Like in the evolutionary environment, it was where there
was actual food scarcity, we evolved dopamine, allergic dopamine, opioid responses to salt,
fat and sugar, which were hard to get and useful. As soon as we got the point where we could produce
lots and lots of salt, fat and sugar, and there was no scarcity on those things, our genetics
didn't change. And so the fact that it felt really good when you ate that and incentivize you to get
more of it where that little bit of surplus might mean you make it through the famine versus not,
it was it was an adaptive response, right? Then we create a Anthropocene where we have
hostess and McDonald's giving amounts of salt, fat, sugar that are and the combinations of them
with the kind of optimized palatability where it is not only is it not evolutionarily useful to
get it anymore, it is actually the primary cause of disease in the environments where that's available.
It doesn't mean that the dopaminergic signal changed, right? So we're able to kind of take an
evolutionary signal and hijack it. And this is obviously what fast food does to the evolutionary
programs around food. It's what social media does to the impulses for social connectivity.
It's what porn does for the impulses to sexual connection associated with intimacy and procreation
and all like that is to extract the hypernormal stimuli from the rest of what makes it actually
evolutionarily fit. Same thing can happen in religion, right? You can offer people an artificial
sense of certainty and offer them an artificial sense of belonging and security and you know,
various things like that. And without much actual deep philosophic consideration or necessarily
even deep numinous experience. And that similarly has the ability to scale more quickly than something
where you want people to actually understand deeply, discover things themselves, have integrated
experiences, not just do the right action, but for the right intrinsically emerging reasons.
Which is why, you know, your podcast doesn't have one of their happens as many views as
the most trending TikTok videos that require less work and are shorter and more oriented to
hypernormal stimuli. So I'm not saying we can't work with these things. I'm saying these are the
things we have to work with. So we're in a situation where the, let's say that we're in group,
in groups and out groups, we both cooperate and compete at different times based on
what game theory seemed to make most sense. And they would typically cooperate while reserving
the right to compete and to even fully defect if they need to, right? Resource scarcity or something.
Or just a sociopath coming into leadership, which totally happens.
So the combination of the worldviews, everybody needs to believe our religion. If they don't,
they are bad. And so we're going to convert them or whatever, right? Or everyone needs to be,
have a democracy because that's good and all the forms of governance are bad or whatever it is,
that there's ideology that orients itself. There's a tech stack that is a part of the capacity to
do that. There are coordination mechanisms that are about to do that. So the full stack of the
superstructure, the worldviews, the social structure and the infrastructure, what are engaged in in
group out group competitions and that are up regulating largely shaped by those competitions.
It just happens to be that the version that makes it through that shaping process is also
orienting us towards a whole suite of global catastrophic risks. It is basically self terminating.
And so it has been the case that you have to win the local arms race because otherwise you lose,
but the arms races that are externalizing harm, but on an exponential curve that have cumulative
effects, you don't actually get to keep externalizing on an exponential curve or running arms races on
an exponential curve in a finite space forever. So we're at this interesting space where you can't
try to build an alternate world that just loses, but you also can't keep trying to win in the same
definition of when. This is the interesting point we're at, which is we have to actually
build a version of when that is not for an in group in relationship to an out group,
but is something that actually allows some kind of omni-win that gets us out of those
multipolar traps. And this was all coming from the topic of you starting with why you brought
up the UN and that you have to deal with these things with some kind of sense of
how are other people dealing with them and how does that affect the choice making process?
Some people would say, look, we're group selected, and then we can make our group to be
the tribe versus another tribe. And one of the solutions is if there was aliens, and then we
could bind together as humans and fight something external. It doesn't have to be aliens. The point
is that there needs to be something extra. So he's saying there's another option and that that
option to bind together in order to fight some other out group, whether the group is something
physical or could be more abstract, that that's not something that should be pursued. And there's
another option. I didn't say that, but it's an interesting conversation.
If we are not binding in groups to fight out groups, so this is kind of like
Machiavelli's enemy hypothesis that people are kind of evolutionarily tribal,
and that to unify a lot of people at a much larger than tribal scale, given that they naturally
will find their own differences and conflicts and reasons to otherize somebody because they
have more influence over their own small group or whatever, that to unify them works best if you
have a shared enemy that forces them to unify. And so then you eventually, of course, this makes
small tribes unified to deal with a larger tribe, and then you get kingdoms and nation states and
global economic trading blocks, and eventually you get great superpower conflicts.
And that if the only way to unify, that if groups opposing each other in that way ends up being
catastrophic for the world, so we want to get everybody unified in some way, do we need a
shared enemy? Obviously, this has been talked about a gazillion times can climate change or
environmental harm be the shared enemy. Not really. Even if everyone believed in it, which they
don't, it doesn't hit people's agency bias in the same way and whatever. Could we stage a false
flag alien invasion that makes us unify? Of course, this has actually been an explored topic,
both in sci fi and reality. And it's a how deeply explored is a question, but
yes, it's a very natural topic to explore that something like a attack from the outside would
allow that kind of unification. Because of that, there are people who are very skeptical and concerned
of anything that looks like a presented shared threat that should create some unified response.
Because then they're like, well, what is the government that regulate that will navigate that
shared threat and who has any checks and balances on that if that thing becomes captured or corrupt?
And so this this is again the catastrophes or dystopias. If you don't have some coordination,
you get these problems of coordination failure. If your coordination is imposed, you end up getting
oppression dynamics. So how do you get coordination that is global, but that is emergent, that has
that keeps local power from doing things that drive multipolar traps, but that also ensures
that you don't get centralized power that can be captured or corrupted, right? A system of
coordination has to address both of those things. And as we move into in more people with more
resource consumption per capita and more and the cumulative tipping points on the biosphere
being hit, but even more than that exponentially more power available to exponentially more actors.
Obviously, if we look at the history of how humans have used power and you put an exponential curve
on that, it doesn't go well. So yeah, that's one way of thinking about the coordination issue current.
When we were thinking about the UN or whatever is this global agency potentially, the phrase,
they have no checks and balances comes up. Is there a way of organizing something that is global
and influential that has its own internal checks and balances? I don't understand how the US political
system works. It's my understanding that it's tripartite and antagonistic. I don't understand
the details of it. I'm apolitical, at least consciously. I haven't looked into it, but the
point is I can that's interesting. I don't know how that works. I wonder how much that doesn't
work, how much that can be accelerated, amplified? Well, one point we bring up is that any proposed
system of coordination, governance, whatever, is not going to work the same way after it's been
running for a long time as when it was initially developed, because all of the systems have a
certain kind of institutional decay or entropy built in that has to be considered, because
every vested interest that is being bound has a vested interest in figuring out how to break
the control system or capture or corrupt it or something. And so it's not just how do we build
a system that does that, but it's also how do we build a system that continues to upregulate
itself to deal with an increasingly complex different world than the one it was originally
designed for and that continues to deal with the fact that wherever there is an incentive to gain
the system is going to happen. So you have to not only figure out a system that makes sense
currently, but a system that has an adaptive intelligence that is adequate for the changing
landscape. So when you look at the US, because leaving corrupt monarchy was key to the founding
here, and so we were going to try to do this democracy non-monarchy thing. It was also the
result of a change in tech. It was a result of the printing press where rather than before a printing
press and everyone could not have textbooks and couldn't have newspapers and to have access to
information, someone had to copy a book by hand, which meant that there were very few of them or
copy the information by hand, so only the wealthy could have it. The idea of a wealthy
nobility class that got educated enough to make good choices for everyone else, where if they
were too corrupt, the people would overthrow them, so there was a certain kind of checks and balance
that kind of maybe made sense, right? With a no-bless-o-pleasure built into the obligation of the
nobility class to roll wealth. I'm not saying it did, but that's at least the story. But as soon
as the printing press comes and now everybody could have textbooks and get educated, and everybody
could have a newspaper and know what's going on, it kind of debases the idea that you need a nobility
class to make all the choices because everyone else doesn't know what's really going on. You say,
well, maybe we could all get educated enough to understand how to process information and we
could all get news to be able to understand what's going on and all have a say. Obviously,
democracy emerged following that change in information tech. I'm saying this because of
course, AI is a radical change in information tech that will also obliterate our existing
political economies and coordination systems and make new ones and changes to culture as well.
The difference in the AI case just briefly is that I don't see the AI as democratizing
more so than exacerbating the inequality in terms of like, so if you're extremely bright,
the amount of information you can process is going to be far outpacing someone who either is not so
bright or gets access to that AI three weeks later. Thinking through in the same way that the
printing press had an effect on central religion through everybody can have a Bible and read it
and learn on their own and kind of Lutheran revolution and it had an effect on central
government in the form of feudalism, we can then look at kind of McLuhan's insights of how
information tech changes the nature of the collective intelligence and motivation and
type of mind that everyone is operating with and as a result, the emergent type of society,
we can look at the way that the internet and digital have already done that,
looking at the way social media has affected media for instance, which affects our democratic
systems is a pretty obvious one. But then we can look at AI and not just AI but different types
of AI, different ways it could develop. LM is very different than other kinds of AI, right?
So we'll come to that in a moment, but let's come back to the other question because you were
asking the checks and balances one. So the idea in the US system was the British system following
the Magna Carta and the Treaty of Forest and whatever was supposed to be the most
ideal, noble thing around and ended up being in their experience a totally corrupt thing. So
the idea that no matter how you develop a system, it can be corrupted, that was built in. So how do
we make sure that no part gets too much power and that we have checks and balances throughout was
kind of key? So before you even get into the three branches of government, you already have the
separation of the state and the church, which was already a key part. And you have the separation
of the market and the state, right? Which is the, you have a liberal democracy that is proposed,
so you don't have a pure market function, but you also don't have that the state is running the
entire economy. And so the separation of the market, the state, the church is a few other ways
of thinking about separation was already a part of it. And then with regard to the state's function,
the separation of the legislative, the judicial and the executive were critical. And then within
each of those within the legislative, a bicameral breakdown was really important. And that then
the Tint Amendment was to push as much power, the subsidiary principle to the states as possible
and as little to the federal. So there were many, many steps of checks and balances on concentrated
power that were built into the system. But of course, everyone who is smart, who is also
agentic, who wants more power, looks for loopholes and or figures out how to write laws and to get
them passed, right, doing legislation and lobbying. And of course, corporations can pay for a lot more
lawyers than an average citizen can or then a nonprofit group that doesn't have a revenue
stream associated. So the group that is trying to turn commons into commodities versus one that's
trying to protect the commons will inherently have a bigger revenue stream to employ media to
change everyone's mind or to employ campaign budgets or to employ lobbyists or whatever. So you
end up seeing that there is a progressive kind of loophole finding corruption, because the underlying
incentive systems invested interests are still there, right?
Baudrillard Simulation and Simulacra that discusses the steps of the degradation from a new system to
how it eventually devolves into mostly a simulation of what it originally was is a good analysis on
this we could discuss. But so that's a little bit on kind of the history of checks and balances on
power, but I don't think anybody looks at our current US system and says it's doing a great job of
that. And there's a bunch of reasons. In addition to the one that I said about how there is a
natural process of figuring out how to influence this, like everyone who, okay, there's one other
part that's actually worth saying. So you have a state, you have a market, and you have the people
as members of a democratic government, meaning their function in state, not their function in
market. So a government of form by the people, the people might not all be representatives, but
they can all speak to their representative, decide how it votes, those types of things, right? So
you have a market, and you have the people as members of a democratic government, meaning their
types of things, right? So there's supposed to be a check and balance between these three,
that the main reason that there is law is to prevent some people or groups of people from
doing things that they have an incentive to do that would suck for everybody else.
Obviously, whether it's individual stealing or murder or whatever, or it's a corporation and
cutting down the national forest or polluting the waterways too much, there is somebody has
an incentive to do something. And in a democracy where the idea is supposed to be that we all want
and value different things, but the collective will of the people as determined through some voting
process gets instantiated into law, where a monopoly of violence can back that up,
that's kind of core to the idea of a liberal democracy, right? I'm not arguing that it is a
good system, but I'm arguing for the core logic of it. And it's because the recognition that if
we just had a pure market system, the reason why there wasn't just a pure kind of laissez-faire
system, even though the people building this understood, or at least their expressed reason,
is an impure type market dynamic. As you were mentioning with AI, some people are way better
at it than other people. And as a result, we'll just end up getting a lot more money that they
can convert to more land, resources, employees, etc. And you end up getting a power law distribution
on wealth, which is a power law distribution on everything, and these people's interests
end up determining the whole society, and these people's interests are pretty determined for them.
And so if you want to create protections for these people at all, and that was basically the King
George situation and the inspiration for the Declaration of Independence and leaving, which
was there was too much concentrated power and it was kind of fucked. So how do we make that not
happen? Well, since we know that the market is going to kind of naturally do that, let's create a
state that is more powerful than any market actor. And let's make sure that the state reflects the
values of all the people. So the little guys get to unify themselves through a vote, right?
And then you get to have a representative that represents everybody. It's the only one given
a monopoly of violence, and it gets to make sure that any more powerful actors are checked. That's
kind of the idea. Yeah, so so far, this is an account of how it's been like a history lesson,
but you aren't saying this is how it should continue to be, nor this is how it's operating
in its ideal sense currently. Are you just saying that this was the reasoning behind it?
A one key part of how it broke down. So the idea is that the market, people will have incentives
to do things that are good for them that might suck for the environment or others. And so others
have the ability to agree upon laws that will bind those actors to not do that thing, right?
So the state is supposed to check the market, let the market do its thing, do resource distribution
productivity, let it do that because it's good, but check the particularly fucked applications.
And in order for the state to check the market, the people are supposed to check the state,
and ensure that the state is actually doing the thing that it's supposed to do and that the
representatives aren't corrupt and taking back in deals and all those kinds of things, right?
And then there's a way in which the market kind of checks the people, meaning that the people can't,
there's the accounting checks them. They can't vote themselves more rights than they're willing
to take responsibility for. They can't make the economics of the whole situation not work,
right? They can't vote themselves a bunch. If the people all say, yes, we should all get no taxes,
but lots of social services, then the accounting is what actually checks the people, right?
So that's the idea of how you have this kind of self-stabilizing thing.
But of course, the people stopped checking the market. Once we were out of kind of
the sense of an eminent need for revolution, then the people have a lot of shit to do other
than really pay attention to government in detail. And there's a bunch of other reasons
beyond the scope of this conversation, why the people stopped checking the government,
in which case the market is continuously trying to influence the government through lobbying and
legislation and campaign finance and all those other things. And so then you end up getting
regulatory capture rather than regulatory effectiveness, right? So when you put those checks
and balances, it's going to change. When everyone's scared of concentrated power following a revolution,
it's different than four generations later where nobody actually feels that fear anymore
and is busy doing other shit, right? So it's not just how do you build your system,
it's how do you build a system where the initial people that went through the difficult thing to
build it when they die, you didn't just pass on the system, but the generator function of the kinds
of insights needed to keep updating and evolving the system under an evolving context.
So when you ask the question about could such a thing be built at an international level,
where there are checks and balances, the answer is it's super hard, but yes. But it's not just
can you design it properly up front, it's also can you factor how that system then, even if well
intended at first, it's kind of like all technologies do use. So you build the gene editing for
immuno oncology, but then it can be used for bio weapons. You have to not just think about what
you're building it for, but all the things that will happen, having created that thing. Same thing
with government, you have to think about not just who you're building it for right now, but as the
landscape changes, culture changes, can this thing be corrupted? Can it be captured in future
different contexts and how do you build in immune systems to that? And that sort of thinking seems
to be missing with the development of AI. And it reminds me, I've said this several times,
like the development of the bomb where Feynman and Oppenheimer, mainly Feynman and his peers said
they didn't think about what they were creating, they were thinking we're having fun speaking
about these topics. It's even more fun to do research on these topics. Einstein said like,
I burned my hands had I known that this was what was going to be developed. I wasn't thinking about
that. I wasn't thinking about the consequences and Feynman said something similar. We're consumed
with the achieving of a goal and we're not thinking about what would occur as a consequence once we
attain it. And you hear this constantly in the AI scene, channels like two minute papers that say,
what a time to be alive. That's like his catchphrase, what a time to be alive, like encouraging and
amazed constantly thinking, what is going to be like two papers down the line said enthusiastically.
I see little caution expressed. Yes. Geez, Louise, like what the heck are we building and should we,
just because we could, should we? When the people who express caution, this now relates to this
asymmetry, we said, if people are like, Hey, this is extremely risky technology, we need to
understand the risk space very deeply. First, we need to ensure the development of the technology
and then it's future use by everybody is safe enough to be worth built.
Those people end up running nonprofits because there's no upside to that. There's no immediate
capital upside to that. So they have already done getting the capital to get really good
researchers or big enough computers and data sets to try to run stuff on for trials.
And the people that are like, Oh, there's a market application to this, have a much easier
time getting a massive GPU cluster and a lot of talent and a lot of data. And so we can see this,
if you name the names that are out there and their views and then map them to the types of
organizations they run and the type of motivational cognitive bias, it's somewhat maps, right?
So what is this is what we intended to talk about all of this has been interesting preface.
What is the actual risk space for the AI? What do we know? What do we not know? How should we
think about it? How should we proceed, especially given that AI is a lot of different things?
Shall we dive in there now?
Sure. I just wanted to point out that although this seems like a disagreement between you and I
on the surface, there's an agreement. So again, I'm not a Mormon, but I don't see the Mormons
failure because they go when they say, Hey, you should whatever they say, act right, be humble,
be kind, don't over consume and so on. But then their religion doesn't grow. I don't see that as
a failure of them because maybe their religion is larger than just being Mormon. It's something
about the values that are spread and then they send out these values and they filter through the
community in the same way that these nonprofits, just because they're not the largest, doesn't
mean that the values that they send out don't influence you and I and influence the people
who are listening who then act differently because of these values. We have no idea how much the
pacifism of Tolstoy has influenced you hugging your father and your brother and the positive
sentiment we have generally speaking in society toward decentralization. And it also reminds
me of the Cassandras for people who don't know what a Cassandra is. It's someone who makes a
prediction of it's a doomsayer. It's akin to a doomsayer. They're the opposite of a self fulfilling
belief. So self fulfilling belief is one where you state it and you create the conditions such
that it becomes true. Whereas in the best case for the people who say the world is going to end,
well, their success depends on them being self sacrificing depends on us then being able to
repudiate them, let the world hasn't ended. We have no idea how much the doomsayers or the fear
mongerers said something that made us straighten up and act right just enough that it pushed us
off of the brink and influenced us to make society live in. So we then have this archetype
of the cautious and contemplatious false cravens of the past. It's just not clear that because
they have died doesn't mean that they're unsuccessful. That's what I mean. Yeah, so there's a
this important point. The idea that even though Greece didn't
continue its empire relative to Rome, that its memes ended up influencing the whole Roman empire,
and so in some way it won or similar with Judaic ideas or whatever. One of the greatest
examples of that that people talk about right now is Tibetan Buddhism, which is okay. So
from the point of view of Tibet as a nation, the Tibetan people, the integrity of that wisdom
tradition, it was radically destroyed. But in the process of the world seeing that and having
some empathy and gendered for it, even though it didn't protect Tibet, did that actually disseminate
Buddhist ideals through the whole world radically faster? There's a similar conversation as
so many people become interested in ayahuasca and plant medicines from indigenous cultures that
the economic pressure of that is making what is remnant of those cultures get turned into tourism
and ayahuasca production, or shipibo production or whatever it is, that on one hand it actually looks
like a destructive act on those cultures and the other way it's are the memetics of those now
becoming dispersed throughout the dominant systems. That is a part of the consideration set that has
to be considered. And now do we see, for instance, that particularly nonviolent groups like say the
Jains as an example of maximum nonviolence, that those memes do become decentralized and affect
everybody? It's not a simple yes or no, right? There's a whole bunch of contextual applications.
So when we look at is, are there memes from Buddhism that have influenced the world? Yes.
But are they the ones that are compatible with the motivation set of the world they influence?
So you've got this like, Buddhist techniques of mindfulness for capitalists in Silicon Valley
to crush it a capitalism is a very weird version of the subset of the Buddhist stack, right?
And I remember when I first started seeing kind of the popularization of mindfulness techniques
in business so that people could focus better and crush it a capitalism, how
fucking hilarious that thing is, right? Because in some ways, you are distributing
good ideas in other ways, you're actually extracting from a whole cultural set the part that ends up
being a service ingredient to another set. So the topic of what makes it through, I mean,
it's a complex topic. What I will say is the idea that a civilization, which is its
superstructure, its worldview values, what is true, good, beautiful, what it's oriented to,
what's the good life, its social structure, meaning its political economy and institutions,
its formal incentives and deterrents and how it organizes collective agreement and its infrastructure,
the physical tech stack that it mediates all this on together in a civilizational stack.
One of those competing with other ones for scarce resource and dominance,
and all of them engaged in that particular competitive thing, I would argue no one actually
gets to win that. That process of the competition of those relative to each other does actually
create an overall global civilizational topology that self terminates.
But also no one trying to create a good long term future can just lose at that game in the
short term. So you can neither create the world you want by just losing at that game,
nor by trying to win at it. It's something else. It's actually having to abandon that game to try
to change the game dynamics themselves. Ah, okay. So I was just watching Arrested Development,
this that you can't play the game, you can't frame it in the same way. So Buster from Arrested
Development, I don't know if you've seen it. He's this mama's boy. He wants to go out with this
other girl who happens to have the same name as his mom. But anyway, she's like, I don't want to
go out with you because you're just in love with your mom that he's like, no, and he just left
his mom's home. He's like 40 years old or 35. And he's like, no, no, no, it's the opposite.
I'm leaving my mom for you. You're replacing my mom. And then she's like, no. And it's because
he's still framing it in the same way. Anyway, we have to abandon the frame. So please,
what does that look like and integrate AI into this answer?
So we have not just tried to give the frame on AI risk yet. And to incorporate that in the
what is the long term solution for civilization as a whole look like, let's actually just kind
of do the AI risk part first, and then we can bring it back together.
But let's try to frame how to think about AI risks, AI opportunities and potentials,
including how AI can help solve other risks, which has to be factored.
I will add as preface that I am not an AI developer. I don't have a background in that.
I'm not even a AI risk expert or specialist. I know you're in conversation might have Yudkowski
and other people who really are Stuart Russell, Bostrom, those guys would be great.
Because of some maybe novel perspectives about thinking about risk and governance approaches
to risk writ large metacrisis, that's the perspective that I'm taking into the AI topic.
So what is unique about AI risk relative to other risks? We were talking earlier about
environmental risks and risks associated with large scale war and breakdown of human systems
and synthetic bio and other things. If we look at other technologies that have the potential
to do some catastrophic things like nuclear, it's very easy to see that nuclear weapons
don't make better bio weapons. They don't make better cyber weapons.
They don't even make better nuclear weapons directly. And the same is true for biotechnology.
It doesn't automatically make those other things. AI is pretty unique in that you can use AI to
evolve the state of the art and nuclear technology and delivery technology and intelligence
technology and bio and cyber and literally all of them. So it is unique in its omnipurpose potential
in that way. Because of course all those other technologies were developed by human intelligence,
human intelligence, agency, creativity, some unique faculties of human cognitive process.
And so where all of the other technologies are kind of the result of that human process,
building a technology that is doing that human process possibly much faster than much more scale
is obviously a unique kind of case, right? And so there's thinking about what type of risk
does an AI system create on its own. But then there's thinking about how does, how do AI systems
affect all other categories of risk, right? We have to think about both of those.
And then in addition to the fact that the nukes don't automatically make better bio weapons,
the nukes don't even automatically make more nukes, right? They're not pattern replicating,
but to the degree that we actually get AI systems that not only can make all the other things better,
but they can make better AI systems and to the degree that there starts to be something like
autonomy in that process, then the self upgrading and omnipotential of all the other things.
It's also true that there's an exponential curve in the development of hardware that AI runs on,
right? Better GPUs and all of different kinds of computational capabilities. There's an
exponential curve in IoT systems for capturing more data to train them on, exponentially more
people and money going into the field because of the way that shared knowledge systems work,
kind of exponential development in the software and cognitive architectures. So we're looking at
the intersection of multiple exponential curves, not just a single one. That is also kind of important
and unique to understand about the space. So thinking about the case of AI turning into AGI,
an autonomous artificial intelligence system that we can no longer pull the plug on, that has goals,
has objective functions, whatever they happen to be, that is something that guys like Bostrom
and Yudkowski have done a very good job of describing why that's a very risky thing. I think
everybody at this point probably has a decent sense of it, but just make it very quick.
When we say a narrow AI system, we mean something that is trained to be good at a very specific
domain, like beating people at chess or beating them at Go, or being able to summarize a large
body of text. When we say general intelligence, we mean something that could maybe do all of those
things and can figure out how to be better than humans at new domains that has not been trained
on through some kind of abstraction or lateral application of what it already knows. So if you
put us into an environment where we have to figure out what is even the adaptive thing to do,
we will do it. There's a certain kind of general intelligence that we have. So we talk about a
generally intelligent, artificial intelligence. The idea, and then of course, because we can
develop AI systems, one of the things it could do is develop AI systems. So if it has more
cognitive capability than us in some ways, it can develop a better AI system and that one could
recursively develop a better one. And you get this kind of thought about recursive takeoff
in the power of an AI system. And there are conversations about whether that would be slow
or fast. Is there an upper boundary on how intelligent a system could be in humans or near
the top of that? Or are we barely scratching at the beginning of it and we could have something
millions of times smarter than us? So that's all kind of part of that conversation. But the idea
that we could create a artificial intelligence that could basically beat us at all games,
right? Which it could think about economy and affecting public opinion and military as games.
And it has faster feedback loops, faster to the loops to get better than we do. So if we're like
trying to deal with it, it's going to win at newly defined games. And if that thing we can't pull
the plug on and it can anticipate our movements and beat us at all games, if it has goals that are
directly antithetical to ours, or not even directly antithetical, but the way in which it
fulfills its goals might involve externalizing harm to things that are part of our goal set,
that's bad for us, right? So the idea of don't let that thing happen prevents getting to an
unaligned AGI. That's that particular category of risk. And so there are arguments around,
could an AGI like that, is it even possible? That's one question. If it is possible to have such a
thing, is it possible to align it with human interests? What would that take? If it is possible
to align it, is it possible to know ahead of time that the system you have will be aligned and will
stay aligned, right? Like those are all some of the questions in the space. And then do our current
trajectories of AI research like transformer tech or just neural networks or deep learning in
general, do these converge on general intelligence? And if so, in what time period? Those are all some
of the questions regarding the AGI risk space. Now, I want to talk about that risk, but I want to
talk about other risks using that as an example in the space. Any questions or thoughts on that
one to begin with? Sure. Number one is that we may already have artificial intelligence in a baby
form like we have hugging face, I don't know if you know what that is. And then there's the paper
of sparks of artificial general intelligence. And that's distinguished from something that updates
itself. I just want to make that clear. So there's a lot of questions regarding,
does it have to be better than humans at everything to be an existential risk?
We could imagine a von Neumann machine that was self replicating and self evolving that was not
better at everything, but better at turning what was around it into more of itself and
evolving its ability to do so, and just having way faster feedback loops. And we could imagine
that becoming an existential risk with a speed of a particular type of intelligence that does
not mean better than us at everything. Yeah, that's a great point. Like an asteroid is not
better than us at almost anything, but it can destroy us. And it yeah, it's not doing it through
some kind of process that involves learning or navigating competitions at all. It's just
kinetic impact. This would be a kind of intelligence, but it could be a one that's a lot more like a
very bad pandemic, right? And the intelligence of a pathogen, then the intelligence of a God.
So talking about if the system is generally like what type of intelligence it would need,
is it generally intelligent? Is it autonomous? Is it agentic? Is it self upgrading? They're
related concepts, but they're not identical concepts. So what I so let's go ahead and put the
category of AGI risk as one topic in the AI risk space. Let's come to a much nearer term set of
things, which is AI empowering bad actors. And we can talk about what of that is possible with
the existing technology? What of that is possible with impending technology that we're for sure
going to get on the current course versus things where we don't know how long it's going to take
or even if we'll get there. So with regard to AI empowering bad actors, we could say
how undefined the bad actor is obviously because one person's freedom fighter is another person's
terrorist. So we can imagine someone who is terrified about environmental collapse deciding
to become an eco terrorist being a maximally good actor in their worldview, but saying that the only
answer is to start taking out massive chunks of the civilizational system that's destroying the
environment. So I'm simply saying that I'm not being simplistic about what we mean by bad actor,
but oriented to from whatever motivational type, whether it was pure satism, whether it's nihilistic
burn it all down or whether it's well motivated, but maybe misguided considerations. But AI for
some destructive purpose. So now this is something we have to address first.
One thing I have found when people think about how significant AI risk will be
and how significant AI upside will be. First on AI upside, it's just important because if we talk
about risk and we don't talk about upside, it will be easy for a lot of people to say, oh, this is a
techno pessimist Luddite perspective and kind of dismiss it at that. So I would like to say there is a
the upsides of AI, the best case examples that everyone is interested in, everyone is interested
in, they're awesome. Can we, all the things that we care about that we use intelligence to figure
out where intelligence is rate limiting, figure them out, rate limiting to figure out the rest
of the problems, could we use it to solve those problems? So could AI make major breakthroughs
in cancer and immuno oncology? And does anyone who's talking about slowing down AI, are they
factoring all the kids that are dying of cancer right now? And if we could speed that thing up,
could we affect that in our like, that's a very personal, very real thing, right? So AI applied
to curing all kinds of diseases and AI applied to psychiatric diseases and scientific breakthroughs
and maybe resource optimization issues that help the environment and maybe the ability to help with
coordination challenges have applied in certain ways. The positive applications that the kind of
customized AI tutoring that could provide Marcus Aurelius level education were the best tutors
of all of Rome were personally tutoring him in every topic, could provide something better than
that to every human, right? Could democratize aristocratic tutoring. There was Eric Holes
art essays on aristocratic tutoring are really good. You should bring them on here. But basically,
it was something many people have come to, which is that the great polymaths and super geniuses,
the highest statistical correlator that pops out is that they all had something like aristocratic
tutoring when they were young, were the vast majority of them, that even von Neumann and
Einstein had mathematicians as governesses before they went to school. And Terry Tau had Paul Erdos,
there's this famous image of, I don't know who had an Edwin though.
So that is a, many of the people simply had parents that were very actively involved,
scientists, philosophers, thinkers, you know, but if you think about why Marcus Aurelius dedicated
the whole first chapter of meditations to his tutors, and if you think about how the Dalai Lama
is conditioned, where you find this three old boy and have the top
llamas in all of Tibet to do them on everything that is the whole canon of knowledge.
Of course, if that was applied to everybody, we would have a very different world, right? This
is very interesting insight, because it says that the upper boundaries on that a lot of what we call
human nature, because it's ubiquitous, it's not nature, it's nature through ubiquitous conditioning,
that the edge cases on human behavior show conditioning in common. And that if you could
make that kind of conditioning, ubiquitous, you would actually change the human condition pretty
profoundly. But as we move from feudalism to democracy and wanted to kind of get rid of all
the dreadful unequal aspects of feudalism, looking at the fact that like you can't learn to be a
world-class mathematician by a person who's not a world-class mathematician the same way you can
by one who is. And you can't, you don't get a bunch of world-class mathematicians becoming third
grade or eighth grade high school teachers, you know, or school teachers. So how would you do
that? So it's kind of repugnant from a privileged lack of democratized capability point of view,
right? And yet could you have, could I make LLM-trained AIs in better than LLM ones where I can
have Von Neumann and Einstein and Gertl all in a conversation with me about formal logic,
where they are not only representing their ideas, but maybe even now have access to all
the ideas since them and are pedagogically regulating themselves to my learning style?
That's kind of amazing, right? Like, and could they maybe be doing that based on also
psychological development theories? A colleague of mine, Zach Stein, has been working on this a lot
of how to be evolving, not just their cognitive capacity, but their psychosocial, moral, aesthetic,
ethical, etc., a full suite of human capacities. So I'm simply saying AI applied rightly. There's
a lot of things to be excited and optimistic about in. So that's a given. And we could do a
whole long conversation on more of those examples. When I look at how people orient to the topic of
AI risk, one of the things that seems to be a common kind of where their knee-jerk reactions
before understanding all the arguments pro and con well comes is how much they have a
bias towards a kind of techno-optimism or techno-pessimism. Kind of a where
Pinker, Hans Rosling, there are still problems, but they're getting better. The world is getting
progressively better. And it's a result of things like capitalism and technology and science and
progress. And so more of that will just keep equaling better. And yes, there will be problems,
but they're worth it. Versus, I would call that techno-capital optimism. But the naive version
that doesn't look at the cost of that thing, we would call a naive progress dialectic.
In the dialectic of progress is good, progress is not good. Or we're really making progress
versus we're actually losing critical things or causing harm, whatever they say in that dialectic.
That's on the progress side, but a naive version of it. And so most of the, and just to address that
briefly, so the naive progress story looks at all the things that have gotten better. And you can
see this in lots of good books and Pinker's books, Diamandis' books, Rosling's talks, on and on.
And then the extension of that into the future Diamandis start to do, and we could say Kurzweil
is kind of an extension of that far out. Why naive is if it doesn't look at what is lost in that
process and what is harmed in that process, as well as the increase in the types of risk that are
happening. And so I would argue that most of those things in every kind of Rosling presentation
is cherry picking its data out of a humongous set to make a cherry picked argument.
This is one of the reasons that fact checking is not enough is because you can cherry pick your
facts, you can frame them in a particular way and create a conclusion that the totality of
knowledge wouldn't support it all because of that process, right? I would say there is a naive
techno pessimism or Luddite direction that looks at the real harms tech causes culturally,
socially, environmentally, or other things and wants back to the land of movement and organic,
natural, traditional, whatever various types of, and if it is not paying attention to the types
of benefit that are legitimate, that's naive. But also if it's not paying attention to the fact
that that worldview will simply, as we talked about before, not forward itself because the one
that advances more tech will develop more power and end up becoming the dominant world system.
That also means that it's not actually having a worldview that can't orient towards shaping the
world. So we have to, so putting those together, I would say all of the things that the techno
optimists say tech has made better and all of us like a world we're going to the dentist involves
novocaine versus not novocaine and where we have painkillers and where we have antibiotics under
infection and stuff like that, all of the things that tech has made better have not come for free.
There have been externalized costs and the cumulative effect of all of those costs is
really, really significant. And so if you look at the progress narrative, the indigenous people
that were genocided don't see it as a progress narrative. The fact that there are more, there's
more biomass of animals in factory farms than there is in the wild today does not see that as a sign
of progress. The animals that live in factory farms or all the species that are extinct don't
see it as progress. The fact that we have many, many different possibilities of destroying the
life support capacity of the planet relative to any previous time or that almost no teen girl growing
up in the industrialized world doesn't have body dysmorphia where that was not an ubiquitous thing.
There's a lot of things where you can say, damn, those technologies upregulated some things and
externalized costs somewhere else. If you factor the totality of that, then you can say, okay,
there are a lot of positive examples any new type of tech can have. But there's also a lot of
externalities and harms it can have. And we want to see how to get more of the upsides with less
of the downsides. And that can't be a rush forward process, right? That actually requires a lot of
thinking about how to do that. So I'm actually, I actually am a techno optimist in a way,
meaning I do see a future that is high nature stewardship, high touch, and is also high tech.
High touch? Yeah, meaning
that the tech does not move us into being disembodied heads, mediating exclusively through a digital
world. So I would argue that your online relationships don't do everything that offline
relationships do. They do some additional things like distance and network dynamics, whatever.
But if they debase your embodied relationships, they're causing a harm. If they, if the online
relationships improve your embodied relationships, not just the create online relationships and
debase them, then that's a different thing, right? So that's what I mean by high touch.
So I want to say that naive techno optimism is if there's, you know, if we look at the history
of corporations that are developing technology, market technology advancement focused on the
upside and not terribly focused on the downside, we look at four out of five doctors choose camel
cigarettes. We look at better living through chemistry, providing DDT and parathion and
malathion. We look at adding lead to gasoline in a way that it talks the chemical that was bound
in ore underneath the biosphere and sprayed it into the atmosphere ubiquitously. It's dropped
about a billion IQ points off the planet, made everybody more violent in terms of its neurotoxicology
effects. Trusting the groups that are making the upside on moving the thing to figure out the
risks historically is not a very good idea. And I'm mentioning that in terms of now trusting
the AI groups to do their own risk assessment. And if you think about the totality of risks
well, then you want to say, how do we move the positive applications of this technology forward
in a way that mitigates the really negative applications of it, that if one wants to be
a techno optimist responsibly, they have to be thinking about that well.
So what about the AI companies that say we do third party testing for safety? Are you still
consider that somehow internal because they're the ones going out? Depends. If so,
when I early in the process of getting into risk assessment, I had times where corporations
asked me to do risk assessment on the technology or process. And then when I did an honest risk
assessment, they were not happy because what they wanted me to do was some kind of box checking
exercise that wouldn't cost them very much and wouldn't limit what they were going to do. So
they had plausible deniability to say they had done the thing and move forward quickly. Because
what they didn't want was me to say, actually, there is no way for you to pursue the market
viability of this that does not cause excessive harms. Or were you dealing with those harms,
messes up your possibility for margins? Without breaking any NDA of yours that you may have signed,
are you able to go into what a company did that you disapproved of and what was the result of it?
Just as an example to make this more concrete for people who are listening.
Yeah, totally. I have seen this in the example of something like a mining technology or
a new type of packaging technology that is wanting to say why it's doing something
that addresses some of the environmental concerns. It addresses the environmental concerns that
we identify a bunch of other ones that it doesn't address well, that it moves some of the harm from
this area to the other one. That's an example of where some of the problem would come. But I find
this is just as bad in the nonprofit space or in the government space as well, not just in the
for-profit space. Because they also have, even if it's not a profit motive, they have an institution
mandate. And their institutional mandate is narrow. They can advance that narrow thing.
This is now the same thing as an AI objective. If the AI has an objective function to optimize
X, whatever X is, or optimize a weighted function of XYZ and metrics, everything that falls outside
of that set, harm can be externalized to that and achieve its objective function. So I remember
talking to groups, UN-associated groups, they were working on World Hunger.
And their particular solutions involved bringing conventional agriculture to areas in the world
that didn't have it, which meant all of the pesticides, herbicides, and nitrogen fertilizers.
And it was a huge increase in nitrogen fertilizer by a bunch of river deltas,
where it currently wasn't, that would increase dead zones and oceans from that nitrogen effluent.
That would affect the fisheries in those areas and everything else and the total biodiversity.
And when I brought it up to them, they're like, oh, I guess that's true, but those are not the
metrics we're tasked with. We're tasked with how many people get fed this year, not how much the
environment is ruined in the process. And so the reduction of the totality of an interconnected
world to a finite set of metrics we're going to optimize for, whether it's one metric called
net profit or GDP, or it's the metric of whatever the institution is tasked with,
or getting elected, or something like that, it is entirely possible to advance that metric
at the cost of other ones. And then it's entirely possible that other groups who see that
create counter responses to that, who do the same thing in opposite directions.
And the totality of human behavior optimizing narrow metrics, while both driving arms races
and externalizing metrics in wide areas, is at the heart of the coordination failures we face.
And so it happens to be that this is already something that we see with humans outside of AI,
but giving an AI an objective function is the same type of issue. So I was mentioning
examples in the nonprofit space. I think there are examples of how to do AI safety that can also
be dangerous. And so it's important. Sure. So somebody proposes an idea like here's a type of AI
that could be good, and we should build it. Or on the other side, here's a AI safety protocol
that would be good, and we should instant instantiate it in regulation or whatever.
We want to red team those ideas, meaning see how they break or fail, and violate team that
meaning see how they externalize harm somewhere else that they didn't intend before implementing
that just means think through the causal set beyond the obvious set you're intending it for.
Right. So there was this call for a six month pause on
training large language models bigger than GPT for. I'm not saying that pauses a bad idea. I'm
saying as instantiated, it's not implementable. And it's not obviously good. So you saw the push
back as people are like, all right, so that means that whatever actors are not included in this,
which might mean bad actors, rush ahead relative. That's a real consideration. And one has to say,
okay, so are we stopping the accumulation of larger GPU clusters during that time? Are we stopping
the development of larger access to larger data sets during that time that will be able to quickly
configure them? Are we also stopping, you know, there are plenty of other types of AI that are
not LLMs being deployed to the public, but that are very powerful. Black Rocks Aladdin played
some role in the fact that it has more assets under management than the GDP of the United States.
And there are military application is in development. And so
can you in so what is the actual risk space? And are we talking about slowing the whole thing?
Or are we talking about slowing some parts relative to other parts where these kind of game
theoretic questions emerge? How would we ensure that the whole space was slowing? How would we
enforce that? Those are all things that have to be considered. You mentioned that GDP is not a
great indicator and GDP goes up with war and more military manufacturing. It goes up with
increased consumerism and the cost of the environment. It goes up with addiction. Addiction
is great for lifetime value of a customer. So there's something called Goodheart's law. So I'm
sure you're familiar with Goodheart. Is this at the core of what you're saying? It's like, hey,
explain it. As soon as you have a metric that you try to optimize for ceases to become a good
metric. For instance, I think this is from the Simpsons, but it may be real that there was a
town overrun by snakes or rats. I think those rats. And then you say, Hey, give me rat tails,
because it implies that you killed a rat. I'll give you a dollar every time you bring me a rat
tail and we'll reduce the amount of rats. Maybe it initially did so. But then people realized,
I can farm rats and then just kill them and give you tails. And thus I have more total rats. This
is a much more general phenomenon. So perhaps if you think Twitter followers are great, okay, yeah.
If you incentivize any metric, there are perverse forms of fulfilling that metric, meaning there's
a way to fulfill that metric that either no longer provides the good or it provides the good while
also affecting some other bats, right? Which basically means you probably thought of that metric
in a specific context, right? Like there's a bunch of wild rats and the only way to get a rat tail
is to kill a wild rat, not in the context of farm rats. And so it kind of relates to the topic
we were mentioning earlier about government that it's not just instantiating a government that
makes sense on the current landscape, but recognizing the landscape is going to keep
changing and it'll change in a way that has an incentive to figure out how to control the
regulatory systems and how to game the metric systems. So with regard to the topic of AI alignment,
right? Because if we tell the AI maximize the number of rat tails, then it could like
Bostrom's paperclip maximizer. Before we continue, it's imperative that we have a brief overview of
Bostrom's thought experiment called the paperclip maximizer. The paperclip maximizer scenario initially
conceived by philosopher Nick Bostrom in 2003 illustrates the potential dangers associated
with misaligned goals of artificial general intelligence that is AGI agents. In this hypothetical
scenario, an AGI is tasked with the seemingly innocuous goal of maximizing the number of paperclips
it produces. However, rather than competence and focus serving as a salutary quality, it's in fact
due to its extreme competence and single-minded focus that it proceeds to transform the entire
planet and eventually the universe into paperclips, annihilating humanity, and all of life in the
process. The core ideas to understand from this scenario are the importance of value alignment,
the orthogonality thesis, and instrumental convergence. Value alignment is the process
of ensuring that an AGI shares our values and goals in order to prevent cataclysmic outcomes,
such as the aforementioned paperclip maximizer. The orthogonality thesis states that intelligence
and goals can be independent, implying that a highly intelligent AGI can have arbitrary goals.
You hear this by the way when people say that we've become more knowledgeable with time,
yet our ancestors were wiser. Instrumental convergence refers to the phenomenon where
diverse goals lead to similar instrumental behaviors like resource acquisition and
self-preservation. For instance, as Marvin Minsky points out, both goals of prove the
Riemann hypothesis and make paperclips may result in all of the Earth's resources being dismantled,
disintegrated in an effort to accomplish these goals.
Thus, despite the ultimate goal being different, for instance, the Riemann hypothesis and make
paperclips are not the same, there's a convergence along the way. What's often overlooked in
AGI development is something called the value loading problem. This refers to the difficulty
of encoding our moral and ethical principles into a machine. That is, how do you load the values?
Keep in mind that AGI needs to be courageable and robust to distributional shifts. AGI, or even
baby AGI, needs to maintain its alignment even when encountering situations deviating from its
training data. Additionally, something that we want is that the AGI should be able to recognize
ambiguity in its objectives and seek clarification rather than optimizing based on flawed interpretations.
Of course, we as people have ambiguity and flawed interpretations. The difference is that AGI could
decidedly exacerbate our own existing known and unknown flawed nature. Another difference is that
we can't replicate on a second to second or millisecond to second basis, at least not yet.
One promising approach to this AGI alignment scenario or misalignment scenario is something
called reward modeling. This involves estimating a reward function based upon observing our
preferences rather than us providing predefined objectives and some of the more hilarious examples
I found of the predefined sort are as follows. The aircraft landing problem was explicated in 1998
when Feldt attempted to evolve an algorithm for landing aircraft using genetic programming.
The evolved algorithm exploited some overflow errors in the physics simulator,
creating extreme forces that were estimated to be zero because of the error,
resulting in a perfect score without actually solving the problem that it was intended to solve.
Another example is the case of the Roomba. In a tweet, Custard Smigley described connecting
a neural network to this Roomba to navigate without bumping into objects. The reward scheme
encouraged speed and discouraged hitting the bumper sensors. Okay, so think about it,
what could happen? Well, the Roomba learned to drive backward. There are no sensors in the back,
so just went about bumping frequently and merrily. In a more recent example, a reinforcement learning
agent was trained to play the video game Road Runner. It was penalized for losing in level two,
so did it just become fantastic at the game? Not quite. The agent discovered that it could kill
itself at the end of level one to avoid losing in level two, thus exploiting the reward system
without actually improving its performance. What would happen if it was tasked to keep us
from hitting some tipping point? By the way, is living more valuable than not living?
What's the rational answer to this? This is perhaps the most important fundamental question.
With regard to the topic of AI alignment, right, because if we tell the AI maximize the number
of rat tails, then it could, like Bostrom's paperclip maximizer,
start clear-cutting forests to grow massive factory farms of rats. You can do the reducto
ad absurdum of a very powerful system. Then the question is, you say, okay, well,
do the rat tails or the GDP or the whatever it is while also affecting this other metric, okay?
Well, you can do those two metrics and there's still something armed. What about these three?
The question is, is there a finite, describable set of things that is adequate for something that can do
optimization that powerfully, right? That is a way of thinking, and so it's, is there a
finitely describable definition of good? Is another way of thinking about it, right? Or
in terms of optimization theory? Yeah, that's something I think about. The misalignment problem.
Is it, in principle, impossible to make the explicit what's implicit? When we state a goal,
it carries with it manifold, unstated assumptions. For instance, I say, bring me coffee or bring me
uber food. We imply indirectly, don't run over a pedestrian to bring me the uber food.
Don't take it from the kitchen prior to it being packaged. Don't break through my door to give it
to me. And we cloak all of that and say, that's just common sense. Common sense is extremely
difficult to make explicit. Even object recognition is extremely difficult. And then as soon as we
can get a robot to do something that is human-like, then it becomes more and more black box-like.
And then you have this huge problem of interpretability of AI. So it's an extremely difficult
problem. And I wonder how much of the misalignment problem is just that? Is it just the fact that
we can't make explicit what's implicit? And we overvalue how much the explicit matters and
implicit is far more complex? I don't know. This is just something that I'm putting out there and
asking. In other words, to relate this to what you were saying is, is it finite? And even if it's
finite, is it like a tractable amount of finiteness that either we can handle it or we can design an
AI that we feel like we have a handle over that can understand it? Yeah. And if you try to say,
okay, can I mine myself, my brain for all the implicit assumptions and put them all?
I think every version of the thought experiment you realize you can't. But even if you do,
that's only the ones that are associated with the kinds of context you've been exposed to so far.
But there are a heap of things that nobody has ever done that maybe an AI could do
that now also have to be factored in there that you didn't think to say because it was never
something that happened previously where there is evolved knowledge to say, don't do those things.
There's also something that because humans all co-evolved and have similarish nervous systems
and all kind of need to breathe oxygen and want to universe a world that has similar physics and
whatever, there's some stuff where the implicit processing is kind of baked into the evolutionary
process that brought us that is not true for a silica-based system. It has totally different,
that is not subject to the same physical constraints, right? Optimize itself in a very
different physical environment. And so even the thing that we would call just kind of
an intuitive thing is very different for a very different type of system.
So the one, I would say when it comes to the topic of AGI alignment,
there are different positions on alignment. I would say the strongest position is AGI alignment is
not, well first we actually have to discuss what we even mean by alignment, right? Because
initially the topic of alignment means can we ensure that the AI is aligned with human values
and human intentions so that when you say, bring me a cup of coffee, that you're all those implicit
intentions that you have are not damaged in the process. But if we look at the, all of the animals
and factory farms and the extinct species and the disruption to the environment and the conflict
between humans and other humans and class subjugation and all those things, you can say
human intent is not unproblematic. And exponentiating human intent as is
is not actually an awesome solution. And so do you want it to be aligned with human intention?
Well it currently looks like human intention has created a social sphere and a techno sphere that
is fundamentally misaligned with the biosphere they depend upon and it is the techno sphere,
social sphere complex is kind of auto poetically scaling while debasing the substrate it depends
upon. In other words, it's on a self-termination path. And that represents something like the
collective intent of humans currently in this context. So if you ensure that the AI is aligned
with intent in the narrow obvious definition, that is also not a good definition of alignment.
So insofar as the humans are not aligned, their intent is not aligned with the biosphere they
depend upon and is not aligned with the well-being of other humans who will produce counter responses.
And most of the time isn't even aligned with their own future good, as is the case with all
addictive behavior, right? Sorry to interrupt, I'm so sorry. Is this a place where you disagree
with Yodkowski or has he also expressed points that are in alignment with your point about alignment?
I don't know if he has. There's nothing that I know of that I disagree with. I think when he's
I'm sure he's thought about this, I just haven't read that of him. When he's talking about alignment,
he's talking about this more basic issue of as he tries to give the example, if you have a very
powerful AI and you ask it to do something that would be very hard for us to do but should be a
tractable task for it, like replicate the strawberry at a cellular level, that can you
make an AI that could do that that doesn't destroy the world in the process?
Even that level not being clear how to do at all is the thing he's generally focused on. I'm
sure he has deeper arguments beyond it that if we got that thing down, what else would we have to get?
So,
one could say, like if we look at all of the social media issues that the social dilemma
addressed, where you can say, right, Facebook can say we're giving people what they want,
or TikTok, or the YouTube algorithm, or Instagram, or whatever, because we're not forcing people to
use it, except it's saying we're giving people what we want in the same way that the drug dealer
gives drugs to kids is saying that, right, which is we can create addiction, we can kind of pray on
the lower angels of people's nature, and if they're individual people who don't even know
they're in such a competition, and we're talking about a major fraction of a trillion dollar
organization employing supercomputers and AI in an asymmetric warfare against them,
to say we are giving them what they want while engineering what they want, that's,
it's a tricky proposition, but we can see how the algorithm that optimizes for, whether it's
time on site or engagement, both have happened, that's a perverse metric, right, because you
can get it through driving addiction and driving tribalism and driving fear and limbic hijacks
and all those things. What's important to acknowledge, that's already an AI, right,
it's already a type of artificial intelligence that is taking personal, collecting personal data
about me, and then looking at the totality of content that it can draw from, and being able to
create a news feed for me that continues to learn based on what is stickiest for me, right,
what I engage with the most. Now, in this case, the AI isn't creating the content, it's just
choosing which content gets put in front of people. In doing so, it is now incentivizing
all content creators to create the content that does best on those algorithms. So it's actually,
in a way, farming all human content creators, because it's incentivizing them to do whatever
it is that is within the algorithm's bidding. Now, as soon as we have synthetic media, which is
rapidly emerging, where we can not just have humans creating whatever the TikTok video is,
but we can have deep peg versions of them that are being created very rapidly. And now you have a
curation AI, where that first one's AI was just to curate the stickiest stuff, personalized to
people, and creation AI's that can be creating multiple things to split tests relative to each
other. And the feedback loop between those, you can just see that the problem that has been there
just hypertrophies, let alone the breakdown of the epistemic comment and the ability to tell
what is real and not real and all those types of issues. I want to come back to where we were.
So, obviously, the curation algorithm is saying that it's aligned with human intent, but not
really, right? It's aligned with human intent because it's giving stuff that they empirically
like because they're engaging with it. But most people then actually end up having regret of
how much time they spend on those platforms and wish that they did less of it. And they don't
plan in the day, I want to spend this much time doom-scrolling. And so, is it really aligned
with their intent? And in general, is aligning with intent that includes the lowest angels of
people's nature type intent? Is that a good thing? Is that a good type of alignment when you factor
the totality of effects it has? So, we could say that the solution to the algorithm issue
should be that because the social media platform is gathering personal data about me,
and it's gathering based on its ability to model my psyche based on all of who my friends are and
what I like and what I don't like and all those things in my mouse hover patterns,
it has an amount of data about me that can model my future behavior better than a lawyer or a
psychotherapist or anybody else could. So, there are provisions in law of privileged information.
If you have privileged information, what are you allowed to do with it? And there are provisions
in law about undue influence. So, we could argue that the platforms are gathering privileged
information, that they have undue influence. As a result, there should be a fiduciary responsibility.
This is one of the things that we do when there's a radical asymmetry of power,
because if there's a symmetry of power, we say caveat and tour, buyer beware, it's kind of on
you to make sure that you don't get sold a shitty thing or engage with. But if there's a radical
asymmetry of power, can you tell the kid buyer beware about an adult that is playing them?
No, you can't, right? And so, in that way, can the person who isn't a doctor know that they really
don't need a kidney transplant if the doctor tells them that they do because the doctor gets paid
when they give kidney transplants? Well, that's so bad, we don't want that to happen. We make law
saying doctors can't do that. There's a Hippocratic oath to act not just in their own economic interest,
but they are an agent on behalf of the principal because the principal cannot buyer beware, right?
And so, then there is a board of other doctors who are also at that upper asymmetry who can verify
did the person do malpractice or not. Same with a lawyer. If the lawyer wanted to just
bill by the 15-minute sections maximally to drain as much money from me,
they could because there's no way I can know that what they're telling me about law is wrong
because they have so much asymmetric knowledge relative to me that we have to make that illegal.
We have to make sure that the lawyer is an agent on behalf of me as the principal. So,
with lawyers and doctors and therapists and financial advisors, we have this fiduciary
principal agent binding thing, right? And it's because there's such an asymmetry
that there cannot be self-protection, right? If I'm engaging with them and giving them this
privileged information and they wanted to fuck me, they could, right? And so, but for my own well-being,
I have to engage with them and give them this information. So, we have to have some legal
way of binding that. But of course, in the case to bind it where the lawyers all have some
practice law that they can be bound by, they can be shown they did malpractice. And same with
doctors, that requires a legal body of lawyers or a body of doctors that can assess if what
that doctor or lawyer did was wrong. So, somebody else that has even higher asymmetry, right?
Group of the top things. This becomes very hard when it comes to AI.
So, let's start by saying the, rather than the AI being a rival risk relationship with me
when I'm on social media. And it is actually gathering the information about me not to
optimize my well-being, but to optimize ad sales for other, for the corporation that is the platform
and the corporations that are its actual customers, right? In which case, it has the incentive to
pray on the lowest angels of my nature and then be able to say it was my intent and I had free
choice. So, we could say that should be a violation of the principal agent issue. And because there's
undue influence, we can show there's undue influence. Concellions project, we wrote some articles on
this. There's one on undue influence that makes its argument more deeply. And because you can show
it's gathering privileged information, it should be in fiduciary relationship where it has to pay
attention to my goals and optimize aligned with my goals rather than I'm the product that's optimizing
with the goals of the corporation or its customers, right? In order to do that, that would change its
business model. It couldn't have an ad model anymore. I would either have to pay a monthly fee for it
or the state or some comments would have to pay for it and everybody had access to it or some other
thing. That seems like a very good step in the right direction. And that is an alignment issue
thing, right? The principal agent issue is a way of trying to solve the alignment, which is to say
that this more powerful AI system here, the curatorial AI, social media, would be aligned with
my interest in bound in some way. And maybe we would extend that to all types of AI. Well,
of course, in the AGI case where it becomes fully autonomous and becomes more powerful than in the
other systems, what other system can check it to see if what it is doing is actually aligned or not?
There isn't a group of lawyers that can check that lawyer, right? So that becomes a big issue. And
if it really becomes autonomous as opposed to empowering a corporation, which is run by humans,
it's different. And so this is one part on the topic of alignment and alignment with our
intention or well-being. You can do superficial alignment with our intention, which the social
media thing already does, but it's not aligned with our actual well-being because an asymmetric
agent is capable of exploiting your sense of intentionality. And similarly, when you say
there's a common sense that says, don't break the door down when you're bringing me coffee,
there should be a common sense that says, don't overfish the entire ocean and cut all the damn
trees down and turn them into forests in the process of growing GDP. And there is clearly not.
Right? And so we can see that the current, without AI, human world system already actually doesn't
have that kind of check and balance in it, in all the areas that it should, just so long as
the harms are externalized somewhere far enough that we don't instantly notice them and change them.
So the question of what do we, if we have radically more powerful optimizer than we already have,
what do we align its goal with? If we just say align it with our intention, but it can change our
intention because it can behavior mod me, and we can't possibly deal with that because of the
asymmetry, that's no good, as in the Facebook case. If we try to align it with the interest of a
nation state that can drive arms races with other ones and other nation states in war or drive it,
align it with the current economy that's misaligned with the biosphere, that's not good.
So the topic of alignment is actually an incredibly deep topic.
And this now gets to what you've probably addressed on your show in other places. It gets
to a very philosophic issue, which is the kind of is-ought issue, which is science can say what
is, it can't say what ought, right in that kind of distinction by Mel and others historically,
and that the applied side of science is technology and engineering can change what is,
but what ought to be, what is the ethics that is somehow compatible with science is a challenge.
The best answer we have had, arguably, that came from the mind that created both a lot of our nuclear
technology and our foundations of AI, von Neumann was game theory, right, that the idea that is
good is the idea that doesn't lose. And we can arguably say that that thing, instantiated by
markets and national security protocols, has actually been the dominant definition of ought
that ends up driving the power of technology, because if science says we can't say what ought,
we can't. We can only say what is, but we're really fucking powerful at saying what is in a way that
reduces to technology that changes what is, where we can optimize some metrics and say it's good,
even if we externalize a lot of harm to other metrics or optimize in groups at the expense of
outgroups or whatever it is, right? But we say that not only do we not have an ought,
but that any system of ought is not the philosophy of science. So is, insofar as that's concerned,
out of scope or gibberish, well, then what ends up guiding the power of technology,
markets do, and to some extent national security does. In other words, rival risk game
theoretic kind of interests. And so what gets researched, the thing that has the most market
potential. And so then, again, it is what is actually developing the technology, because as
Einstein said, like, I was developing science, not knowing it was going to do that application,
didn't want that application, wanted science for social responsibility. But what ends up,
for the most part, the research that gets funded is R&D towards something that ends up either
advancing the interest of a nation state or the interest of a corporation or whatever the
metric set, the game theoretic metric set of the group of people that is doing the thing, right?
And so what I would say is that as we get to more and more powerful is more and more powerful
science that creates more and more powerful tech and exponentially powerful tech, especially as
we're already hitting fragility of the planetary systems. And when we say more powerful, we mean
like exponentially more powerful, not iteratively more powerful. You have to have a system of
powerful enough to guide, bind, and direct it. Because if you don't, it is powerful enough to,
in whatever it is optimizing for, destroy enough that what it optimizes for doesn't matter anymore.
Now, this is a fundamentally deep metaphysical philosophical issue. And of course, when we
talk about regulation, law, the basis of law is jurisprudence, right? And is
odd questions, right? Applied ethics that get institutionalized for exactly this reason.
And so when we say if we have tech that is powerful enough to do pretty much fucking anything,
what should be guiding that and what should be binding it? And if we don't answer those well,
what is the default of what we'll be guiding as and binding it currently? And what does that
world look like? So this is super cheerful conversation. What is the call to action?
Okay, we're quite a far ways away from that. Let me try to expedite a couple other parts. When
we were mentioning the AI risk, we said AI empowering bad actors. So you can think about
whether a bad actor is a domestic terrorist, who the best thing they can do right now is get
an AR-15 and shoot up a transformer station to take down the power lines. AR-15 is a kind of tech
that has some capability. As you have more people getting a sense of
being disenfranchised by the current system and being motivated to utilize what is at their
resources to do something about it. And the barrier of entry of the more powerful tech is
getting lowered. You can put those things together. But whether you're talking about domestic
terrorism like that, or you're talking about international terrorism from larger groups,
or you're talking about full military applications. But let's just go ahead and say,
and can we make deep fakes that make the worst kinds of confusion, conspiracy theory,
in-group, out-group thinking, propaganda, of course, right? That is an emerging technology
that's imminent. Can we use people's voices and what looks like their video and text for
ransom and fucked up stuff? So you can think of all the bad actor applications and then you can
pretty much apply it to, this is a piece of theory I wanted to say, every technology has
certain affordances. When you build it, it can do things. Where without that technology, you
couldn't do those things. A tractor allows me to do things that I couldn't do without a tractor,
just a shell, in terms of volume and types of work and various things.
Every technology is also combinatorial with other tech. Because what a hammer can do, if I
don't have a saw to cut the timber first, is very different than what it can do if you have that.
And obviously, it requires the blacksmithing to make the hammer, right? So you don't just have
individual tech, you have tech ecosystems. And the combinatorial potential of these pieces of tech
together have different affordances, right? But then what do we use it for is based on the
motivational landscape. I can use something like a hammer to be Jimmy Carter and build houses for
the homeless with Habitat for Humanity, or I can use it as a weapon and kill people with it.
And so the tech has the affordances to do both of those. So the tech will be developed and utilized
based on motivational landscapes. Sure. And just briefly, and going back to earlier, it's not just
dual, because that would be double-edged, it's multipolar. Omni. Yes. Okay. So what we can say
is the tech will end up getting utilized by all, potentially getting utilized by all agents for
all motives that that tech offers affordances relevant to their motives. Right? And so when
we're building a piece of tech, we don't want to think about what is our motive to use it. We
want to think about, are we making a new capacity that didn't exist before, lowering the barrier of
entry to a particular kind of capacity, where now what are all the motives that all agents have who
have access to that technology and what is the world that results from everybody utilizing it that way?
That's factoring second, third, fourth order thinking into the development of something,
a new capacity that changes the landscape of the world. I would say that every scientist who is
working on synthetic bio for curing cancer or AI for solving some awesome problem,
every scientist and engineer and et cetera has an entrepreneur, an ethical responsibility to think
about the new capability they're bringing into the world that didn't exist, not just how they want
to use it, but the totality of use that will happen by them having brought it into the world
that wouldn't have they not. There is no current, when I say there's an ethical
responsibility, there is no legal responsibility. There is no fiduciary responsibility where you
are liable for the harms that get produced by a thing that you bring about that someone else
reverse engineers uses a different way. But there is financial incentive and Nobel prizes for
developing the thing for your purpose. And then again, socializing the losses of whatever anybody
else does with it. So this is one of those cases where the personal near term narrow motive,
this is us being fucking narrow AI's in an ethical sense, is to do the thing even if the
net result of the thing ends up being catastrophically harmful. So the incentive deterrent,
the motivational landscape is messed up. So every tech, now I want to make a couple more
philosophy of tech arguments, tech is not values neutral, meaning that the hammer is not good or
bad. It's just a hammer and whether you use it to build a house for the homeless or
unbeat someone's head is up to you that the motivational landscape and the tech have nothing
to do with each other. This is not true. If a technology gives the capacity to do something
that provides advantage, relative advantage in a competitive environment, whether it's one nation
state competing with another nation state or one corporation or one tribe with another one.
If it provides significant competitive advantage, if you use it a particular way,
then anyone using it that way creates a multipolar trap that obligates the others to use it that way
or a related way. And so we end up getting a couple things, right? This is the classic example
I've used a lot as if we think about, and it's because there's been so much analysis on this
example, if you think about the plow as a technology that was one of the key technologies
that moved us from sub done bar number, hunter gatherer, maybe horticultural subsistence cultures
to large agricultural civilizations. The plow is not a neutral technology where you can choose to
use it or not choose to use it. The populations that used it made it through famines and grew
their populations way faster than the ones who didn't, and they used their much, much larger
populations to win at wars, right? So the meme set that goes along with using it ends up making it
through evolution, the meme set that doesn't make it through evolution. And correspondingly,
there are, in order to implement the tech, it has ethical consequences. I had to clear cut
in order to do the kind of row cropping that the plow really makes possible.
I have to get an open piece of land that is now being used for just human food production and
not any other purpose. So I'm going to clear cut the forest or a meadow or something to be able to
do that. I'm going to, so I'm already starting the Anthropocene with that, right? Changing natural
environments from whatever habitat and home they were for all the life that was there,
to now serving the purpose of optimizing human productivity. And I have to yoke an ox, and I
probably have to castrate it and do a bunch of other things to be able to make that work and
probably beat the ox all day to keep pulling the plow. In order to do that, I have to move from the
animism of, I respect the great spirit of the buffalo, and we kill this one with reverence,
knowing that as it nourishes our bodies, our bodies will be put in the dirt and make grass
that its ancestors will eat in a part of the great circle of life. And whatever kind of
idea like that too, it's just a dumb animal. It's put here for human purposes, man's dominion over,
it doesn't have feelings like us, that kind of thing, which then spills over too. It's just,
it's not like us. So we remove our empathy from it, and we apply that to other races, other classes,
other species, other whatever, right? So something like the plow is not values neutral.
To be able to utilize it, I have to rationalize its use, realizing it creates certain externalities.
And if I see those externalities, I have to be, I have to have a value system that goes along with
that. Wait, sorry, to be particular with the word choice, it's not that the plow is not value
neutral, it's the use of the plow. Exactly. Exactly. And that the plow doesn't use itself,
right? And so the use of the plow is not values neutral. Now, a life where I am hunting and
gathering versus a life where I'm plowing are also totally different lives. So it codes a
completely different behavior set. In doing that, it makes completely new mythos, which is why the
hunter gather mythos and the agrarian mythos are completely different, right? And they have different
views towards men and women and towards sky gods versus animism and towards all kinds of things.
And so, but the other thing is that it provides so much game theoretic advantage of those who use
it relative to those who don't that when they hit competitive situations, that's why there are not
many hunter gatherers left and why the whole society went agricultural. So it's not just
that the tech, so the tech codes, the tech requires people using it, which changes the
patterns of human behavior, changing the patterns of human behavior automatically changes the patterns
of perception and human psyche, metaphors, cultures, etc. And the externalities that it creates and the
benefits that it causes become implicit to the value system because the value system can't be
totally incommensurate with the power system, right? And so the dominant narrative ends up becoming
support for, one could argue, apologism for the dominant power system. And because we can't feel
totally bad about how we meet our needs, so we have to have a value system that deals with the
problems of how we do so. But then it's also that the tech that does this becomes obligate,
because when anyone is using it, everyone else has to or they kind of lose by default.
So when you recognize that tech affects the technology when utilized
affects patterns of human behavior, humans now do the thing they do with the tech, so people do this
thing, and they didn't used to do this thing, right? On the cell phone or whatever. To get the
benefits of the tech, you have a totally different pattern of human behavior. As a result, you have
different nature of mind, you have different value systems that emerge, and it becomes obligate or
some version of a compensatory tech becomes obligate because the omniscient aren't really
shaping the world. They're no longer engaged in the great power competition.
I have a bone to pick there. I had watched a few months ago, and I don't know anything about the
omniscient, or I didn't know anything about the omniscient. I'm just someone who grew up in this
city, and so I dismissed them as Luddites, like we've used that term several times, and they're
backward, they don't know what they're talking about. And then I watched a video, the omniscient
aren't idiots, they're not asinine. There's a reason why they do what they do, and they either
explicitly or intuitively understand that the technology changes the social dynamics in the
way that they view the world and has ethical considerations. But that influenced me. That
influenced perhaps millions of people because it's a video I think it has a few million hits.
Even if they're local, just them saying, you know what? I don't care. I'm going to continue to act
right. It can still influence outward. Anyway, and we're talking about it now. Maybe this will
influence people, and hopefully to something positive, and I hopefully to myself something
positive. Yeah. Okay, so it's not that you come to this a few times, which is even if you have a
memeplex that is not, that doesn't become part of the dominant system, can it infect or influence
the memeplex in a way that steers it? Yes. But one does not want to be naive about how much
influence that's going to have. They want to be thoughtful about exactly how that'll work and
what kinds of influences. We mentioned not all of Buddhism got picked up everywhere, right? Like
the parts that had to do with why people should take vows of poverty and live on very little,
that didn't really get picked up. The parts on how to reduce stress got picked up. The parts on
what a healthy motive is didn't get picked up as much as the parts on how to empower your motive
through a more functional mind. So it's important to get that the memes might live in a complex,
in a context when they influence, parts of them are going to interact with another memeplex and
the technoplex and everything else. And so you are right to say that it's not that they have no
influence, but obviously the Amish, and not speaking to that they're dumb and backwards, but
that in their don't want to engage tech for these reasons argument, they don't have a
significant say in whether we engage a particular nuclear war or not. Or they were not the ones
that overfished the ocean caused species extinction, but they also couldn't stop it.
Right? They are not the ones that are creating synthetic biology that can make totally new
species. And this is why I say there is a naive techno-optimism that focuses on the upsides and
doesn't focus on all the nth order effects and downsides. And as we were just mentioning,
the externalities of tech are not just physical, right? You do this mining to get the thing you
want, but there's a lot of mining pollution or the herbicide does make farming easier in this way,
but it harms the environment and human health in this other way. That's physical externalities.
But you also get these psychosocial externalities. You use Facebook for this purpose and it ends up
eroding democracy and doubling down on bias and increasing addiction and body dysmorphia and
things like that, right? So the tech affects not, it doesn't, it has effects that are not the ones
you intended, some of which might be positive. You can have a positive externality, but it might
have a lot of negative externalities. And those negative externalities can cascade second, third,
fourth order effects. So there's a naive techno-optimism that doesn't pay enough attention to that.
There's a naive techno-pessimism that says, well, I'm aware of those negative externalities. I don't
want those for our people. We think we can isolate our people from everybody else and say, we're
going to not do that. But we're going to have decreased influence over what everyone who is
doing that has, right? Which is what then some of the AI labs argue is there's, in arms race,
we can't stop the arms race on it. And so only being at the front of the arms race can we steer it.
I would argue that that is a naive version of that particular thing, but nonetheless.
So if we want to, in one way of reading,
one of the problematic lessons of the elves in Tolkien is, and I'm just making this as like
a toy model, is in some ways they figured out how to have a nicer life than the humans and
dwarves and whatever else they were able to do, radical life extension and figure out great
GDP per capita where the poorest people were doing well. And they were so kind of,
but they became insular because they were so disenchanted with the world of men and elves
and whatever that they're like, fuck it, we're just going to kind of isolate and do our own
thing our own way. But it ends up being that you're still all sharing Middle Earth and the
problem somewhere else can cascade into catastrophic problems that end up messing up your world too.
So you can't be isolationist forever in an interconnected world. So they actually had to,
they were kind of obligated if we rewrote the story to take whatever they had learned and
try to help everybody else have it or have enough of it that you didn't get work dominance and stuff
like that. So basically arguing that a isolationist, we see a problem, we're going to avoid that for
ourselves, doesn't work with planetary problems. And so I'm not interested in the naive techno
negative versions that say because we see a problem with tech, we're not going to do it.
But we're also going to kind of load a seat in the process and not engage with the fact that we
actually care about what happens for the world overall, and we have to engage with how the
world as a whole is doing that thing. Makes sense what I mean by the naive techno pessimism. And
is that you do not get to do effective isolationism on an interconnected planet that is hitting
planetary boundaries with exponential tech? Yeah, I guess what I'm trying to express is that
even the Amish with what they're doing, it's not as simple as the memeplex that's exported by the
Amish is the Amish memeplex. There's something else that even influenced them, even yourself.
You may be in a position that saves earth, at least for now, from a hugely catastrophic event,
same with Yodkowsky and same with some others. But what influenced you? There's some good in you,
hopefully, that was influenced by something else by something else that's good, which also
influenced the Amish. Each person is corrupt in some way. So I'm saying that there's something
that's like the unity of virtues that influences us. And that as long as we go back and we think
or constantly we're assessing ourselves and saying is what I'm doing good, then these other
memeplexes that are being thrown to us, yes, it's in a different context, somehow it can orient and
pick up the good. We're completely on the same page, which is that that happens, does not always
happen, and that that is a important thing to have happen. But if that happened adequately, or at the
yeah, I will say adequately, then we wouldn't have extinct all the species that we have.
We would not have turned as many old growth forests into deserts. We would not have had as many
genocides and unnecessary wars and etc. So seeing the failure and where either someone's
definition of good is too narrow, get our God to win and fuck everybody else or grow GDP and
they'll take care of everything, we can well intendedly pursue a definition of good that's too
narrow and externalize harm unintentionally. We can pursue a definition of good that we really
believe in that other people don't believe in and our answer is to win over them, but it creates an
arms race where now we're in competition over the thing. Or where there are people who are really
not pursuing the good of all, even they're not even trying to, right? There's a whether it's
sociopathy from a head injury or genes or trauma or whatever it is, they are pursuing
a different thing, right? But they're good at acquiring power. And this is actually a very
important thing is that the psychologies that are that want power and are good at getting it.
And the psychologies that would be the best stewards of power for the well-being of all are not the
same set of psychological attributes. They're pretty close to inversely correlated. So those types of
things have to be calculated in this. So what you're saying right now, which is great, and you're
saying it is that there is some odd impulse that is not only an impulse, right? That you're calling
something and you're saying that some people feel very called by that and that that's important.
I agree completely. Now, why is that historically and currently not a strong enough binding is the
important question? Why has that not been a strong enough binding for all the species that are extinct
and all the animals and factory farms and all the disruption, etc. And then what would it take
for it to become a strong enough binding or the nature of the question here, right? That's actually
at the heart of the Metacrisis question is to have like what is a system of odd that is actually
commensurable with the system of is and what is a way of having that actually sufficiently
influenced behavior such that the catastrophic behaviors don't occur and that the nature of
the influence is not so top down that it becomes dystopic. And that's something like is there either
a so one way of thinking about this is I've mentioned the term a couple of times superstructure,
social structure, infrastructure that comes from Marvin Harris's work on cultural materialism,
basically saying every civilization you can think of in those ways. What is its kind of
memeplex? What is its social coordination strategies and what is the physical tooling
set upon which it depends and different social theorists will say which of these they think is
most fundamental, that the value systems are ultimately what steer behavior and determine
the types of tech we build and the types of societies. Religious thinkers think there,
enlightenment thinkers think there. The social system actually whatever you incentivize financially
is what's going to win because whether it's good or not the people who do that get the money can
incentivize more people, create the law, etc. So ultimately the most powerful thing is the social
coordination systems. And then the other schools of thought say no actually the thing that changes
in time the most is the tech and the tech influences the patterns of human behavior values
everything else. And that's actually what Marvin Harris roughly was saying was that
the change in the techplex is end up being the most influential thing to the change in
because it does affect worldviews and it does affect social systems. In the way we already
mentioned that the change of the techplex of the printing press affected both worldviews and
social systems so did the plow, so did the internet, so it was about to be AI.
I would argue that these three are interacting with each other in complex ways.
They all inter inform each other and what we have to think about is
what changes in all three of them simultaneously factoring all the feedback loops produce a
virtuous cycle that orients in a direction that isn't catastrophes or dystopias is the
right way of thinking about it. And ultimately the direction actually has to be the superstructure
informing the social structure informing or guide by and direct the infrastructure.
Sorry can you repeat that once more?
Yeah the right now especially post industrial revolution physical technology infrastructure
had way faster feedback loops on it than the others did. And because of that it started
breaking the previous industrial era tech at massive scales with those externalities
whatever can't be managed by agrarian era or hunter-gatherer era worldviews and
political systems. So we ended up getting a whole new set of political systems both
nation democratic liberal democracy and capitalism and social communism emerging as
writing the industrial revolution and what should be the political economy that
governs that thing. But the feedback loops from tech and specifically whether it's a nation state
caught on multipolar traps that's building the tech in a central government communist type place
or a market building it but that has perverse incentives built into what its incentive structure
is that has influence on our social structures and our cultures superstructures that we could say
the dominance of that direction is one way of thinking about the driver of the metacrisis.
Now the other direction if we are to say no these examples of the tech won't be built or
we're not going to use the tech in these ways right we're not going yes you can use a tech that
extracts some parts of rocks from other parts of rocks that gives us metal we want but also gives
a lot of waste no you can't put all that waste in the waterway right or you can't put that pollution
there or you can't cut all the trees down in that area because we're calling it a national park
right that law or regulation that is not just the tech thing that's the socials layer so that
layer has to bind the tech and guide and direct it say these applications and not these ones yeah
right yeah but if the social system is not an instantiation if the social structure is not an
instantiation of the superstructure i.e. it's not an instantiation of the will of the people then
it will be oppressive right which is why the idea of democracy emerged out of the idea of the
enlightenment which was this was a kind of governance that only worked for a comprehensively
educated and you read all the founding documents that the comprehensive education had to be is
and ought right it said you must have a moral education as well as a scientific education
and you must be schooled in the science of governance and only a people like that going
back to what we said earlier could check the government right could both know the jurisprudence
to instantiate what is good law could engage in dialectics to listen to other people's point
of view to come up with democratic answers so it was the idea that there was a kind of superstructure
possibility which was some kind of enlightenment or a era values that could make a type of
social structure that could both utilize the tech and guide it but also bind its destructive
applications so when you're saying isn't there some superstructures and there's some sense of good
that will make us bind our capacities i would argue that if the sense of good doesn't emerge
from the collective understanding and will of the people but is instantiated in government
because we the technocrats know the right answer or we the enlightened know the right answer
that will be oppressive and people are right to be concerned by it but if the collective
understanding is i want what i want i don't care what the effects are or fuck those guys over there
or i'm not paying attention to externalities or whatever then the collective will of the people
is too dumb to govern and misguided to govern exponential tech and will self terminate so you
cannot have a a uneducated unevolved set of values in a libertarian way guide exponential tech well
right it has to be more considerate has to think through nth order effects
but you also can't have a government that says we are the enlightened months and we figured it out
and we're going to impose it on everyone else without it being oppressive and tyrannical which
means nothing less than a kind of cultural enlightenment is required long term so that the
collective will of the people now this gets back to the alignment topic is the will of the people
aligned with itself actually right is what i want factoring the effects of what i want the
nth order effects which means how other people will respond to that and all that comes from it
is what i want actually even aligned with a viable future right and so that is when we're
talking about getting in line alignment right alignment with my intention where my intention
is that my country wins at all costs where then china is like well fuck we're going to do the
same thing russia etc so you get arms races that intent or my intent is i want more stuff and
keep up with the jones's and i'm not paying attention to planetary boundaries those intents
are not aligned with their own fulfillment because the world self terminates for too long in that
process and so with the power of exponential tech and the cumulative effects of industrial tech
we do have to actually get odd that can bind the power of that is and it can't be imposed it
does have to be emergent which does mean something like that sense that you're saying has to become
very universal and nurtured right and then has to also instantiate itself in reformation of
systems of governance i love what you said let's see if i can recapitulate it there's tech at the
bottom there's a social structure here and then there's culture okay so these are people up here
there's people in all three there's people's values up here values so values are up here
and then there's the social structure over here and then there's tech over here okay so currently
the tech influences the way our society is structured which also influences our values and
a part of the meta crisis is saying that that's upside down but it shouldn't just be whatever
values that just get imposed onto the social structure onto the values have to somehow come
from someplace else and then the mystics have their other they have to be coherent with reality
sure so the spiritual people may call this something akin to god and the enlightenment
people may say i don't know maybe there's some evolutionary will that comes out and if we just
close our eyes and hope for the best somehow that emerges whatever it's called it's not entirely us
it's not entirely our conscious selves the conscious self would be the more humanistic
the enlightenment way of thinking about it is that we can impose our own values
as Nietzsche had something similar to this so i like this i'm in agreement with it i think
we've just been using different terminology you and i both know that um when you in many ways
i'm gonna say this
in evolution of cultural worldview and values adequate to steward the power of exponential
technology in non catastrophic or non dystopic ways is happening in some areas but a regress
is also happening in some areas right there is increasing left right polarization i thought
you were gonna say there's a regress happening like in demand so for instance we generally
think like it has to be malthusian and the more that we use the more the demand for that increases
and that's obviously removing some of the more scarce objects like art and gold which their value
comes from scarcity but there is like the largest health trend right now is fasting it's like we've
gotten so much food that we're like let's just not have any food and then there's also recycling
like just imagine that we think about recycling at all so there is some recognition that hey look
we're consuming too much let's cut back and so it's not purely just an exponential function
it is we take into account the rate of production well so what we can see is that
the percentage of the total let's go ahead and say us but we could look at
ua or nigeria or whatever various places the percent of the us population that um is regularly
doing fasting is still a relatively small percentage and in the same way that like it is
true that when there's a market race to the bottom that we saw in food which hostess and mcdonald's
you know etc kind of want which is how do we make the food uh more and more combinations of salt
fat and sugar and texture and palatability that maximize kind of stickiness and addiction which
of course if i have a fiduciary responsibility to shareholder maximization and the key to that
is to optimize the lifetime value of a customer addiction is awesome right um it's actually
not awesome it's legally obligate um because of maximize shareholder returns um so that created
a race to the bottom where rather than starvation being the leading cause of death obesity was the
leading cause of health related death in the west okay well that bottom of the race to the bottom
also creates a race to the top for a different niche so then whole foods becomes the fastest growing
supermarket and biohacking and etc so that's true did that affect the overall population
demographics regarding obesity very significantly not really um did it stop the race to the bottom
no it just added another little niche race which also then separated which created more class
system separation so um it's not that those effect don't happen are they happening at the
scale and speed necessary when we look at catastrophic risks so of course i can also
pay more for a post-consumer recycled thing and there is more recycling happening but there's
also more net extraction of raw resources and more net waste and pollution per year than there was
the previous year because the whole system is growing exponentially so even if the recycling
is growing it's actually not growing enough to even keep up with demand right so um what i'm saying is
now let's come bring that back to the memetic space which is where i was there are both evolution of
values where people are wanting to think through catastrophic risk existential risk planetary
wellbeing of everybody long term so that's good but there is also um cultural kind of regress
where people are getting narrower value systems with more antipathy towards other people they
share the planet with that have narrower value systems on the other side and left right polarization
in the us is one classic example and um so the point is cultural enlightenment is not impossible
but it's also not a given right the kind of the kind of memetic shift and this is obviously
i think a big part of why you do the public education memetic work that you do is because
of having a sensibility about um is is it possible to support the development and evolution of
worldviews and people in ways that can propagate and create good um well you're saying it much more
benevolently honestly it's just selfish that i'm just super super super curious about all of these
topics and by luck some other people care to listen and follow along and i just get to elucidate
myself it's so fun it bangs on every cylinder and some other people seem to like it i hope that what
i'm doing is something positive i hope that it's not producing more harm i'm not even considering
this is central for the internet and it's using up energy and okay what you just what you just said
takes somewhere that i i wanted to go this very interesting so we're talking about alignment and
is uh is a particular alignment is a particular say human intention aligned with the collective
well-being of everybody or even their own long-term future at the base of the alignment problems is
that we are not aligned with the other parts of our own self right so from my kind of Jungian
parts conflict point of view um motivations complex because there's different parts of us
that have different motivations and one way of thinking about psychological health the parts
integration view is the degree to which those different parts are in good communication with
each other and see synergistic strategies to meet their needs that don't require that part of self's
motivation harming another part of self but they're actually doing synergistic stuff so
the whole of self pulls in the same direction if you think of like the parts of self as sled dogs
they can be pulling in opposite directions you get nowhere they're all towards themselves
so we can see psychological health and ill health is how conflicted are the parts of
ourselves versus how synergistic are they synergistic does not mean homogenous doesn't
mean we just have one motive it means that the various motives find synergistic alignment rather
than yeah like our bodies are synergistic our heart is not the same as a liver exactly now
in your heart is not going to optimize itself it delivers long-term harm even though on its own
you could say it has a different incentive it is part of a interconnected system where that
actually doesn't really make sense but a cancer cell will optimize itself or the the it's both
itself how much sugar it consumes and its reproduction cycles at the expense of things
around it and in doing so it actually is on a self-terminating curve because it ends up killing
the host and then killing itself and so the cancer that does not want to bind its consumption
and regulation aligned with the pattern of the whole ends up actually doing better in the short
term meaning consuming more and producing more and then there's a maximum number of cancer
cells right before the body dies and they're all dead so there is a if something is inextricably
interconnected with the rest of reality like the heart and the liver or the various cells
but it forgets that or doesn't understand that and optimizes itself at the expense of the other
things it can be on what looks like a short-term winning path but to self-terminate to be an
evolutionary cul-de-sac and I would argue that the collective action failures of humanity as a
whole are pursuing an evolutionary cul-de-sac and so one way of thinking about this when we say we
aren't even that aligned with ourselves we think of motive it's in we like to think of leaders what
is Putin doing or what is Biden or she or doing in a particular thing what is their motive
or what is the founder of an AI lab motive motive will always be that each of the parts of the
self has a different motive right so typically like some unconscious part of me still wants to
get the amount of parental approval that I didn't get and then projecting that on the world through
some idea of success or to prove that it's enough or whatever and some part of me is just directly
motivated by money some evolutionary part is motivated by opera maximizing mate selection
opportunities some part of me genuinely wants to do good some part of me wants intellectual
congruence right and so it's there can absolutely be a burn it all down part right and this is why
shadow works important right which is look at and talk to all of these parts and see how to get
them to move forward together this is basically governance at the level of the self so I don't
know if you ever watched and this might be because we're long even though there's so much left to
discuss a decent place for us to wrap up on alignment I would say one of the better a number of the
theorists who you have referenced on the show would be good references for what I would consider the
deepest drivers of the metacrisis and also what the alignment considerations if you think of like
Ian McGill crests work with the master in the emissary the right hemisphere is the master
and the left hemisphere is the master's emissary
in his 2009 opus the master and his emissary Ian McGill chris discusses the distinct functions of
the brain's left and right hemispheres generally there's plenty of pop science who around this
concept but then you can dispel by going even further to find the correctness about it the
left hemisphere focus on processes such as formal logic symbol manipulation and linear analysis
while the right hemisphere is concerned with context awareness the appreciation of unique
instances and topological understanding hey maybe there's some stone duality between them but I
haven't thought much about this John vervekey's work by the way explores the primacy of cognitive
processes like relevance realization relevance realization aiming to bridge the gap between
analytic and intuitive thinking both McGill chris and vervekey emphasize the importance of
integrating the strengths of each hemisphere or modes of cognition when attempting to tackle
intricate problems such as the risks of ever more powerful ai's the argument is that ai systems
primarily operate using left hemisphere capabilities like pattern recognition logical reasoning and
general optimization problems however they fail to adequately consider the subtleties of human values
and ethical implications which thus leads to unintended consequences to mitigate ai risks and
prevent an arms race incorporating insights from both hemispheres and embracing context awareness
is crucial this requires interdisciplinary collaboration between mathematicians computer
scientists physicists philosophers neuroscientists and by the way it's something that we're
attempting in our humble manner on the theories of everything channel by exploring concepts in
complex systems theory and how it applies to our current unprecedented situation we at least hope
to understand the interconnectedness of the factors that play in ai development for instance
addressing ai risks can involve analyzing multi-agent systems considering network effects
and potential feedback loops which Ian McGill chris would argue greatly benefits from your
right hemisphere's contextual understanding we do not think ourselves into a new way of living
we live ourselves into a new way of thinking you could say and i talked to ian about this and
i said so would you say that the metacrisis as i formulate it is the result of getting the master
in the emissary wrong which is kind of getting the principal and agent between those two different
aspects of human and he goes yes exactly that's kind of the whole key that there is a function
that he's calling the master that perceives the kind of unmediated field of interconnected
wholeness multimodally perceives and experiences it and then there is this other set of networks
capacities or dispositions that perceive parts relative to parts name them do symbol grounding
and orient more in the domain of symbol and can do relevance realization what part is relevant
to a particular goal i have and salient realization what things should be relevant to some goal and
i should be tracking and information compression which are largely things that we think of as like
human intelligence which of course ai is the taking that emissary part and turning it into a
external tool rather than that's the thing that makes tools in us now take that thing and make
that as a tool but also unbound by the master function way who he would call that it's a very
interesting way of thinking about ai alignment and whatever um and that the master function that is
perceiving the unmediated ground directly not mediated by symbol field of interconnected wholeness
that the other function that can do relevance realization parts realization uh salient realization
for compression basically utility function stuff that that has to be in service of the
field of interconnected wholeness if not will up regulate some parts at the expense of other
ones in the cumulative effect of that on an exponential curve revenge should bring us to
the metacrisis and self-terminate i would say what miguel crest was saying was expanding on
what bone said about the implicate order and wholeness bone theory of wholeness and the implicate
order states that there is something like life and mind unfolded in everything a tremendous number
of ways in which uh one can see unfold in the mind one can see the thoughts and fold feelings
and fold thoughts because given my feeling will give rise to a thought thoughts and fold feelings
the thought that the snake is dangerous will unfold the feeling of danger which will then unfold
when you see a snake right bone was looking at um the orientation of a mind that mostly
thinks in words of western mind you know in particular uh to break reality into parts
and make sure that our word the symbol that would correspond with the ground there
corresponded with the things that it was supposed to and not the other things so try to draw
rigorous boundaries to you know divide everything up led to us fundamentally relating to everything
as parts first so how then when now we have this human mind that's you know paleolithic and it's
now put in a world where we have a different technology that is relying on reward circuits
that maybe are not as virtuous as we would like is that where we are now in this conversation
and when bone and christina murty did their dialogues which i don't know if you've watched
those are some of my favorite dialogues in history um bone was basically answering what is the cause
of all the problems what's the cause of the metacrisis he didn't call it that at the time
and he basically said a kind of fragmented or fractured consciousness that sees everything as
parts where you can up regulate some parts relative to other ones without thinking about
the effect of that on the whole right and that obviously comes from einstein being one of his
teachers where einstein said it's an optical delusion of consciousness to believe there are
separate things there is in reality one thing we call universe regarding the theme of consciousness
it's prudent to give an explication here as often at least i found that mysteries arise
because we're calling different phenomenon by the same term and this applies to consciousness
which doesn't refer to just one aspect but rather several that can be delineated to further
differentiate i spoke to professor greg henryks on this very topic i'm attempting to delineate
a few concepts that is adjectival consciousness adverbial consciousness and phenomenal consciousness
which i believe is the same as p consciousness but if that's not the same then that's four
different concepts so what are they can you give the audience and myself an explanation as to when
are some satisfied but not others so that we can delineate totally yep yeah and and actually
adjectival adverbial are gonna when we use p when john and i certainly use p consciousness
phenomenological consciousness is reflecting on adjectival adverbial consciousness and john
refers to john verveky john verveky yeah because we we then did a whole series untangling the world
not to make sure that our systems were in line both in terms of our definitional systems and
our causal explanatory framework so we did a 10 part series on just the two of us on untangling
the world not of consciousness and then we did one on the self then we did one on well-being and we
also did one on development and transformation was ax sign so all of we our systems i think are now
completely synced up at least in relation to the language structures that we have and and so i can
tell you what we would mean by adjectival adverbial consciousness which then sort of is what most
people mean by phenomenological consciousness okay so if i understand correctly one has to do with
degrees mm-hmm and then another has to do with a hearingness and a nowness yeah exactly so actually
there's i like to so i would encourage us to say there let's define conscious there are three
different kinds of definitions of consciousness okay that i think the first definition of
consciousness is functional awareness and responsibility okay this is something that
shows awareness and can respond with control in that the broadest definition then even things
like bacteria can show a kind of functional awareness and responsibility okay that's the
behavioral responsiveness and when you say hey is that guy conscious what you mean is he's not
responding at all okay he's not showing any functional awareness and responsibility he's
either knocked out or blacked out or asleep okay so when you say consciousness in that way
that's functional awareness and responsibility which you can see from the outside and you see
in the way in which the agent's operating on the arena because they're showing functional
awareness and responsibility okay the second meaning of consciousness is subjective conscious
experience of being in the world the first person experience of being and this is where the hard
problem of consciousness comes online and that's what most people mean by p-consciousness or
phenomenological consciousness it's a subjective experience of being which is only available
from the inside out okay so that's and then the final one is a self-conscious access okay so that
now i can be know that i have had an experience retrieve it and then in its highest form report
on it so self-consciousness then is the capacity to recursively access one's phenomenological thing
and an explicit self-consciousness which is what humans have and other animals generally don't
is this capacity say kurt i am thinking about your question i'm experiencing your face and this is
my narrative in relation so that's explicit self-conscious awareness uh-huh just a moment
you said access is that the same as access consciousness or is that's different no that's
net blocks access consciousness which basically there's the do you have the experience and then
is there a memory access loop that stores it and then can use it so so if i can gain access to it
that's a sort of access consciousness as relates to that i want to make sure that i understand this
there's a door behind me if i go and i access is what i'm accessing qualia and is it the
action of accessing that's called access consciousness like the manipulation of data or is
right it's the well it's basically so you have awareness and then you have memory of
the awareness that you know that that some aspects of it knows that you were aware
so it's like so you can imagine awareness without really like one way of differentiated it would be
sort of we have with a sensory perceptual awareness that lasts say three tenths of a second to three
seconds it's like a flash okay then you have working memory which extends it across time and
puts it on a loop that loop is what allows you to then gain access to it and manipulate it
so working memory is a center can be thought of then as a part as a as the um uh the white board
that allows continuous access to the flash so there's a flash and then there's the extension
and manipulation of the flash which you then need access to okay uh the basic layers of this
what john and i argue is that out of the body comes what we call valence qualia valence qualia
basically orients and gives value to and can be thought of as in like pleasure and pain in the body
okay and it yokes a sensory state with an affective state and points you in a direction
or yoke means tie together like to yoke stuff together so this is the sort of the earliest
form of consciousness is probably of kind of valence consciousness okay that basically pulls
you you know oh it feels good feels bad kind of deal gets me active gets me passive but it's
just sort of like this kind of felt sense of the body okay that's the argument from john and i's
position is that that probably is the base of your subjective conscious experience or the
base of your phenomenological experience okay then what happens and that would be maybe present
you know in say reptiles fish maybe down into insects at some level okay uh then the argument
would be in birds and mammals and maybe lower we don't really know but there's good reason to believe
in birds and mammals you get a global workspace the the global workspace is when it extends from
these sensory flashes into a workspace where you have access and recursive looping on it and and
it's the framing of that is is the um adverbial consciousness is the framing and extension of
that the hereness nowness and togetherness that indexes the thing pulls it together that's what
john calls adverbial consciousness okay okay and then it's what's on the screen of that
adverbial consciousness is what john calls adjectival consciousness so it's like it's the screen of
attention that orients and indexes that's adverbial okay and then what is actually the properties that
you experience that's adjectival first i came in with three questions and i'll have so many so many
more okay this valence is it purely pain and pleasure or is there something else are the third
fourth elements um there's certainly pleasure pain active passive to orient and like and want
basically so but but that gets you know so but basically you have that what's called the circumplex
model of affect which basically is the core energizing structure of your motivational emotional
in its broadest frame is two poles one is active passive okay it's like sort of spend energy or
conserve energy and the other pleasure that is either something that you want or something you
like or pain that's something that you don't want or don't like that it's basic so that's the so the
valence is what we sort of focused on in relationship to just the ground of it but there are definitely
at least these two poles of active passive and pleasure pain at a minimum can you make an analogy
with this computer screen right now so the computer screen is somehow the workspace and then the
pixels and the fact that they're bound together is adjectival or the the intensity is adverbial
or the other way around like can you just spell out an absolutely right so the screening the
framing of the screen which brend basically says okay this is the frame in the relevance and the
hearingness nowness of what is going to be brought that is adverbial that's what john called adverbial
consciousness and he has a whole argument as to why especially through what's called the pure
consciousness event that's achieved in medication and several other things that there's a differentiation
but what he calls the indexing function of consciousness which basically is the framing you
bring you index you say that thing without specifying what the thing is okay it's the that
thing that brings it and then you then discriminate on the properties that's the diff that's the
different pixel shapes that give rise to a form that give rise to an experience quality and that's
the adjectival quality so and and these are both of these are john's terms but i've incorporated them
in my work them i use them okay another analogy now to abandon the screen it'd be like if looking is
one aspect and then what you're looking at is another what you're looking at is akin to the
qualia in a pure consciousness event the at may not be there but you're looking exactly it's the
framing exactly index framing that's why john may takes off his glasses okay the glasses are much
more like the adverbial framing they pull and organize okay and it's a looking okay the pointing
the indexing in fact he actually he uses work in cognitive science okay where you can track like
if i give you like four different things to track on a screen okay and they're changing colors and
changing shapes four different things you can track five six seven you stop losing ability to
track however what you what you lose first is the ability to track the specifics you can tell
where something is but you can't tell what it is actually so in other words it's sort of like
you're trying to track everything but it changes like from red to blue to green you're much better
like i think it's over there it indexes but i can't tell you whether it's an a of b or red or a green
i can't tell you the specificity so in other words i'm tracking the entity okay that's the index
and that's different than the specifying the nature of the form and indeed we have lots of
different systems that track like what what is the thing versus how is it moving the how is it
moving it's more of an index structure but if we think of this kind of bomean wholeness we could
say that the metacrisis is a function of missing bomean wholeness and doing optimization on parts
and so i can optimize self at the expense of other but of course that then leads to others
figuring out how to do that and needing to for protection and now arm's races of everybody
doing that the whole externality said i can optimize self at expense of other i can optimize
in group at the expense of out group i can optimize one metric at the expense of other
metrics i can optimize one species at the expense of other species i can optimize my current at the
expense of our future all the way down to one part of self relative to the other parts of self
so the wholeness of all the parts of self in synergy and all of the people species etc and
how to consider the whole how to consider the effects on the whole maybe that was something
that other animals did not have to do maybe it was something that even earlier humans didn't
have to do because they couldn't affect the whole all that much when we have the ability to affect
the whole this much this quickly because of tech right because and particularly because of exponentially
powerful tech whatever ways we are either consciously saying this is a part of the whole
i don't care about or i'm happy to destroy conflict theory or this is a part of the whole
i'm just not even factoring maybe i don't even know the factor that's in the unknown unknown set
but i'm still going to affect it by the thing i do so what is outside of my care or my consideration
right conflict theory and mistake theory with exponential tech gets harmed produces its own
counter responses and cascade effects the net effect of that is termination with this much power
what does it take to steward the power adequately is to think about the total cascading effect of
the choices and all agents doing that and say how do we coordinate all agents doing that in a way
that has the integrity of the whole up regulated rather than down regulated and so i would say
bomean wholeness is a good framework for alignment not alignment of an ai with human intent
but aligned with the interconnected complexity of reality may i inquire how did you attain such
a vast array of knowledge what's your educational background what does your routine look like for
studying is it a daily one where you read a certain type of book and you vary the field
week by week what is the regimen how did you get the way that you are
i think my learning process probably in some ways similar to yours you said just very fascinated
and curious and i mean you did it you did something better than i did which is you
pick the topics you're most interested in found the top experts in the world got them to basically
tutor you for free in terms of like in aristocratic tutoring system you did a pretty awesome thing
there there were a few cases where i was fortunate enough to be able to do that other times i just
had to work with the output of their work but i think for me it was a combo of just innate
curiosity independent of any use application and just i think it's natural when you love
something to want to understand it more and so for me the impulse to understand the world
is kind of a sacred impulse but then also the desire to serve the world requires understanding
it well enough to know how the fuck to maybe do that so there is both a very practical and
very not practical impulse on learning that happened to fortunately converge
and how is it that you're able to articulate the views that you have how do you develop them do
you start writing do you do it in conversation with people do you say some sentiment you realized
you know what that was actually pretty great i didn't even realize i thought that until i had said
it now let me write it down so i can remember it you know i have hypotheses about how people
develop the ability to communicate well but my hypotheses about that and my own process are
probably different um i think my own process is i was homeschooled and i was homeschooled in a way
that's maybe a little bit like what people call unschooling now but my i had no curriculum at all
but my parents just had the kind of they they had never studied educational theory they they
hadn't studied constructivism and thought that montessori and dewey's thoughts on constructivism
were right they just kind of had a sense that um if kids innate interest is facilitated uh
there there's a kind of inborn interest function that will guide them to be who they are supposed to
be um so there were some downsides to that which is because i had no curriculum i didn't have like
writing a letter a bunch of times to get fine motor skills down so i have illegible handwriting
i know what the shapes look like but my i have illegible handwriting i spelled phonetically
till i became an adult and spell checker taught me how to spell interesting um and so like i missed
some significant things um but i also got a lot earlier deeper exposure to the things i was really
interested in which were philosophies spiritual studies across lots of areas activism across all
the areas and sciences um and poetry uh but uh uh my education was largely talking with my parents
and some of their friends and it was largely talking right i actually didn't it wasn't until later that
i did a lot of reading and writing so i think it just was very conversation was very native more
than in a lot of people's developmental environment i think that's the answer for me i could say that
for other people i have seen when they start writing and trying to say what is the most concise
and precise way of writing this um that really helps also when they start communicating with
people and getting feedback on their verbal communication when they watch other's communicators
that they really inspired by and watch the patterns but i think it was just that was pretty native for
me all right that was quite a slew of information and it's advantageous to go through and let's
go over a summary as to what's been discussed so far you'll get a final word from daniel in a few
minutes for now you've watched three hours plus of this let's get our bearings we've talked about
how the emergence of ai poses a unique risk that can't be regulated by national agency like the fda
for ai but instead they require some global regulation again this is all argued by daniel
these aren't my positions i'm just summarizing what's occurred so far the risks associated with ai
are not those that are comparable to single chemical as ai's are dynamic agents they respond
differently and unpredictably to stimuli we've also talked about the multipolar trap which is
regarding self-policing and a collective theory of justice such as singapore's drug policy that was
outlined and how this line of thinking can be applied to prevent global catastrophic events
caused by coordination failures of self-interested agents you can go back to that bit of national
equilibrium to understand a bit about that as well as the multipolar trap section timestamps in
the description we also referenced a false flag alien invasion and can that unify humanity a theme
throughout has also been how ai has the potential to revolutionize all fields but it also poses risks
such as empowering bad actors and the development of unaligned general artificial intelligence
okay so this happened about one week ago or so i debated whether or not i should just record an
extra piece now or if i should wait till some next video but given the pace of this and how much
content has already been in this single video i thought hey i'll just record it and give you
all some more content maybe some people aren't aware of this and i think they should be the god
father of ai leaves google this is jeffrey hinton ai could manipulate or possibly figure out a way to
kill humans how could it kill humans if it gets to be much smarter than us it'll be very good at
manipulation because it will have learned that from us and a very few examples of a more intelligent
thing being controlled by a less intelligent thing and it knows how to program so it'll figure out
ways of getting round um restrictions we've put on it it'll figure out ways of manipulating people
to do what it wants it's not clear to me that we can solve this problem jeffrey hinton is someone
who resigned from google approximately one week ago because he believed that ai bots were quite
scary right now they're not more intelligent than us as far as he can tell but he thinks they
soon may be he also said here in some of these quotes that i have that it's hard to see how you
can prevent bad actors from using large language models or the upcoming artificial intelligence
models for bad things dr hinton said after the san francisco startup open ai released a new version
of chat gpt in march as companies improve their artificial intelligence systems hinton believes
that they become increasingly dangerous look how it was five years ago and how it is now
he said of ai technology take the difference and propagate it forward that's scary his immediate
concern is that the internet will be flooded with false videos and text and the average person
will not be able to know what's true any longer now he says and i quote the idea that this stuff
could actually get smarter than people a few people believe that he said but most people thought it
was way off and i thought it was way off in fact i thought it was 30 to 50 years or even longer
away obviously i no longer think that also there's this ted talk that's recently been published as
well just a few days ago it seems like less than two weeks ago yed jin choy who's a computer
scientist said this extreme scale ai models are so expensive to train and only a few tech companies
can afford to do so so we already see the concentration of power but what's worse for ai
safety we're now at the mercy of those few tech companies because researchers in the larger community
do not have the means to truly inspect and dissect these models then chris anderson comes on and asks
about hey look if what we need is some huge change why are you advocating for it because
there's a huge change a large change it's not like a foot at a time every time these ai's are
released this is what her response is acceleration are you are you sure that given the pace at which
those things are going chris even says that it feels like wisdom and knowledge there's a quality
of learning that is still not quite there we don't yet know whether we can fully get there or not
just by scaling things up and then even if we could do we like this idea of having very very
extreme scale ai models that only a few can create an own and lastly there's this video by
sabine hostenfelder that was released just a few days ago many people are concerned about the
sudden rise of ai's and it's not just fear mongering no one knows just how close we are to
human like artificial intelligence current concerns have focused on privacy and biases
and that's fair enough but what i'm more worried about is the impact on society mental well-being
politics and economics a just released report from goldman sachs says that the currently
existing ai systems can replace 300 million jobs worldwide and about one in four work tasks in the
us and europe according to goldman sachs the biggest impacts will be felt in developed economies
are currently unaligned general intelligence is an issue adding artificial in there is like
another kind of worms man the alignment problem isn't just about aligning human intentions with
a collective well-being but also about aligning the different paths of ourselves to work synergistically
toward a common goal this requires a cultural alignment enlightenment i think he used the
word though i'm not entirely sure we also talked about meme complexes that survive past their hosts
and how this is intimately tied up with the notion of the good and just so you know my
feelings are that memes are an emphatically mechanical way of looking at a complex phenomenon
such as a society an extremely complex phenomenon such as a religion of a society across time and
across other societies interacting i don't believe my point was adequately conveyed and if
you're interested in hearing more then let me know in the comments and i'll consider expanding on
my thoughts in a future podcast we also talked about naive techno optimism and how it often
overlooks externalized costs of progress a responsible techno optimism requires thinking
about how to get more upsides with less downsides which can't be achieved good heart's law then
applies to any metric that's incentivized it leads to perverse forms of fulfilling said metric
all right as you can see so much energy went into this episode so much thought so much editing so
much script revision so much interaction with the interviewee and double checking if this was
accurately representing what was said and we plan on continuing that for season three more work
went into this episode than any any of the other episodes of the whole history of theories of
everything if you'd like to support this podcast and continue to see more then go to patreon.com
slash kerch i mungle the link is on the screen right now as well as in the description there's
also theories of everything dot org if you're uncomfortable giving to patreon there's also a
direct paypal link if that's what you're interested in you should also know that as of right now there's
launched merch we've just launched the next merch this is the second time that merch has ever been
on the toe channel the first one is completely gone you can't find any of those any longer but now
you can see it on screen these are references to different toe episodes like just get wet
and disto sure thumbs up if you recognize that and you have toe and it's babbling all the way down
that's from Carl Friston by the way don't thrust your toe trust your toe hey don't talk to me or
i'll bring up hagel many of these are references like i mentioned i agree i agree with how you're
agreeing with me this is what verveki said to ian McGillchrist you have to be a significant fan to
understand this reference and then also there's verveki who's known for saying there's the being
mode and then there's the having mode got abhijnasis i say phase space inorganically in everyday
conversation i have a toe fetish i'm just a gym rat for toes that's me that's what i say
frequently there's also a purse and a toe hat some toe socks i think that was one of the most
popular of the first round so the toe socks are making a comeback if you want to support the channel
and flaunt whatever it is that you feel like you're flaunting then feel free and visit the merch
link in the description or you can visit tinyurl.com slash toe merch toe merch m e r c h just so you
know everything every single thing that you're seeing this editing these effects speaking with
interviewee all of this is done out of pocket i pay for the subscription fees i pay for zoom i
pay for adobe i pay for the editor i pay personally for travel costs if there are any i pay for so
much there's so much that goes into this sponsors help but also your support helps a tremendous
tremendous amount i wouldn't be able to do this without you so thank you so much thank you for
watching for this long holy moly again if you want to support then you can get some merch if you
like and if you want to give directly on a monthly basis to see episodes like this with such hopefully
quality hopefully something that's educating that's elucidating to you that's illuminating to you
then visit patreon.com slash curt jimungle or like i mentioned there's a paypal link in the
description there's also a crypto link in the description now i'm also interested in hearing
what the other side the other side the people who are pro ai unfettered ai who say hey there's
nothing to see here you are all being hyperbolically hysterical i'd like to see someone respond to
what daniel has said about ai but also civilizational risks in general and how ai exacerbates those
so if you think of any guests who would serve as great counterpoints especially those who are
researchers in machine learning then please suggest them in the comment section if you're a professor
and you're watching and you'd like to have a friendly feel locution that means a harmonious
incongruity a good natured debate where the goal isn't to debate but to understand one another's
point of view if you're watching this and you think hey i would like to come on to the theories of
everything channel as a professor along with my other professor friend who believes something that's
antithetical to what i believe about ai risk then please message me you can find my email address
i'm sure you can also leave a comment yeah and who knows when the next episode of toe is coming out
by the way the next one is going to be john greenwald should be in about a week or a week and a
half all right let's get back to this with daniel schmottenberger well this is a great place to end
daniel you're now speaking directly to the people well you have been this whole time but even more
so now to the people who have been watching and listening what's something you want to leave them
with what should they do they're here they've heard all these issues they hear bow me and
they're like okay that sounds cool that's motivating it's a bit abstract but it is motivating okay
what should i do daniel i want the earth to be here in decades from now centuries from now
what should i do so i'm going to answer this in a way that i think factors who your audience
probably is i just i don't know we even shared demographics with me but um based on the attractor
i can guess um if i was answering just to a series of technologists or investors or bureaucrats i might
say something different um and realizing that amongst that audience there are people who are
going to have radically different skills and capacities and parts of it that they feel the
most motivated and oriented to so i'm obviously not going to say one thing everybody should do
um okay what i'll say is
whether it's hearing a conversation like this where the planetary boundaries and really thinking
about how that there's more biomass of animals and factory farms and there are in the wild left
of the total amount of species extinction or the what the risks associated with your rapid development
of decentralized synthetic biology and ai are you hear these things like fuck and it connects you to
what is most important beyond your own narrow life or even the politics that is coming into your
stream or whether it's when you have a deep meditation or a medicine journey or whatever
it is and connect to what is most meaningful design your life in a way where that experience
happens regularly so what you are paying attention to and optimizing for on a daily
basis is connected to the deepest values you have because on a daily basis the people around you and
your job and your newsfeed are probably sharing other things so try to configure it that the
the deepest true good and beautiful that you're aware of is continuously in your awareness so your
daily choices of how you spend your time and money is continuously at least informed by that
that's the first thing i would say i would say a couple of the things aligned with that is
look at things that are happening in the world online to have a sense of things that you can't
see in front of you but then also get offline and connect with both the trees in front of you
and how without any modeling or value system just how innately beautiful they are and also
the mirror on experience when you're with a homeless person right like so both have a sense
of what's happening at scale but then also grounded embodied sense your own care for the real world
that is not just on a computer there's a real world here and then realize like deep in
shit actually matters like independent of whether i can formalize a particular
meaning or purpose of the universe argument or formalize a response to solipsistic arguments
or nihilistic arguments like prima facie reality is meaningful and i actually do care i wouldn't
get sad or upset or inspired or moved if i didn't care about anything i actually do care and so
life matters and i make choices and i can make choices that affect the world so my own choices
matter so what choices am i making every moment and what is the basis that i want to guide them by
right to just deepen the sense of the meaningfulness of life in your own choice
and this and the seriousness with which you take how you design your life
particularly factoring factoring the timeliness and eminence of the issues that we face currently
and then the last thing i would say is as you could like really work to get more educated about
the issues that you care about and are concerned about really work to get more educated about them
get more connected to the people working on them and really study the views that are counter to
the views that naturally appeal to you so you bias correct so that your own well motivated biases
don't mess up your action and in doing that don't let yourself become unagentic don't let yourself
become so overwhelmed don't let yourself fall into easy certainties but also don't let yourself be
overwhelmed by the total uncertainty that you can't act realizing that if you don't act there are
ethical consequences to that too because we're on a moving train thank you daniel i appreciate
you spending almost four hours now with me likewise we covered a bunch of areas that i did
not expect but they're all good areas i'm curious how the thing ends up getting edited and makes it
through and i'm also curious who you're particularly philosophically interested in insightful audience
what questions and thoughts emerge in this and maybe we'll get to address some of them someday
yeah there's definitely going to be a part two cool much more philosophical part two if this one
wasn't already the podcast is now concluded thank you for watching if you haven't subscribed or
clicked on that like button now would be a great time to do so as each subscribe and like helps
youtube push this content to more people you should also know that there's a remarkably active discord
and subreddit for theories of everything where people explicate toes disagree respectfully
about theories and build as a community our own toes links to both are in the description
also i recently found out that external links count plenty toward the algorithm which means
that when you share on twitter on facebook on reddit etc it shows youtube that people are
talking about this outside of youtube which in turn greatly aids the distribution on youtube as well
if you'd like to support more conversations like this then do consider visiting theories of everything
again it's support from the sponsors and you that allow me to work on toe full time
you get early access to ad-free audio episodes there as well every dollar helps far more than
you may think either way your viewership is generosity enough thank you
you
