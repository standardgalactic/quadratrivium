Hi, and it's great to be back, even if only in this kind of funny format for the Royal
Institution. I'm going to share my screen. So the title of this talk changed a bit only
because talking about absolutely everything is a bit much for an hour. And in fact, let
me show you a picture just to start with. So, you know, I have this new book out called
Shape, and that's the subtitle, The Hidden Geometry of Absolutely Everything. Just to
keep straight, what this book was about was not so easy. And so what you're looking at
is sort of my picture, which appears as the frontispiece of the book of not even really
all but just some of the things that the book is about and which ones are connected to which
other things by relations of circumstance or intellectual affinity, or whatever it may be.
It's meant to be a kind of map of the book. I call it like a Tolkien map, because, you know,
that if you sort of like read Lord of the Rings or something, you know, it's kind of complicated
where everything is, and you need to know like where do the elves live, where do the dwarves live,
how do you get from the dwarves to the orcs, etc., etc. It's kind of like that. So I'm just putting
this up to sort of apologize for not talking about absolutely everything. And to talk like this,
I'm going to sort of follow one strand of the geometry, better to say geometries, that this
book is about. But as your eyes gaze over this, if there is other stuff on here that I totally
don't touch in this talk, and you want to talk about it, just ask me. We have a nice long question
period, and I'm happy to talk about anything that's on this page, or anything else. And I'll put
this slide up again at the end of my talk and leave it up as I ask for questions for those who
want to hear it. So let me start tonight's talk with a puzzle, which is just what is this? What
is this list? Kendi, Jean, Abby, Flurry, Meira, Jean, yes, Jean again, but spelled differently,
Starlo, Caming, and Battilia. Okay, I'm going to let you think on that. It's not supposed to be
obvious. But to start to tell the story of what this thing is and answer the puzzle, which I
promised to do by the end, we got to go back to 1904. And the person you're looking at here is
Ronald Ross, not such a household name now, but in 1904, one of the most famous doctors in the
world, he had just won the Nobel Prize in medicine in 1902. And he won the Nobel Prize because he
was the person who discovered that malaria was transmitted by the Anopheles mosquito, obviously
a discovery of huge import for public health around the world. So this headline you see, and I
apologize for it being so blurry, but this is what happens when you do your best to reproduce a
very tiny piece of microfiche on a large slide. The headline says mosquito man coming. Ronald
Ross in 1904, what was he doing? He was on his way to St. Louis, in the middle of the United
States, for the Louisiana Purchase Exposition, which commemorated, which was there to commemorate
the 100th anniversary of the Louisiana Purchase that nearly doubled the size of the United States.
Now, if you really know your US history, you know that that took place in 1803. So it was actually
101 years later, but it was a big event. It's hard to get these things planned completely on time.
More than 20 million people came. It was held at the same time in places, the Olympics, which is
also in St. Louis, and the National Convention of the Democratic Party. Helen Keller was there
giving a talk. Geronimo was there giving a talk. Some people say the ice cream cone was invented
there. It was a very, very big deal for the early United States as it sort of introduced itself
onto the world stage. And in September, there was what was called the International Congress
of Arts and Sciences, almost at the very end, where the convention invited really the most
notable scientists from all over the world to gather in the middle of the United States to talk
about their research and what was going on. And Ronald Ross was among them, as were several other
people we'll talk about today. Ross was giving a lecture called The Logical Basis of the Sanitary
Policy of Mosquito Reduction. So I have to admit that does not sound like that much of a barn burner
of a talk, right? And it definitely doesn't sound like that talk would be the beginning of an entire
new field of mathematics. But it was, and that's kind of the story I want to tell you about,
starting with Ronald Ross in 1904. So what I'm going to show you first is a picture,
a picture from Ronald Ross's talk. By the way, you know what I was doing while Martin was
introducing me? I know I'm supposed to be hanging on every word, but I just, as Martin was saying
that the Royal Institution has been active for more than 200 years, or maybe he said almost 200
years. I was like, gosh, like Ronald Ross like probably spoke at the Royal Institution and I
looked it up. And yes, he did on March 2, 1900. I found just like quickly Googled and found so
I guess 120 years later, here I am reminding you of what Ross was working on. I think probably
none of you were there for that last talk in 1900. So here's the question Ross was interested in,
in this moment. You see, once you know that malaria is spread by mosquitoes,
it's kind of natural to know, okay, what do you do if you want to get rid of malaria? You get rid
of mosquitoes. But you can't get rid of all mosquitoes. And then as now, and maybe I'll say
a bit in a moment about sort of what Ross has to say about contemporary issues in pandemic control,
which is, which of course are relevant. But then as now, epidemic control is not about eradication.
It's not about perfectionism. Making an organism extinct is really, really hard. And you probably
can't do it. So what does that mean? Does that mean your efforts are doomed? No, definitely not.
Whether we're talking about 2021 or whether we're talking about 1904, he said, let's suppose you
can clear out the mosquitoes from some region, make some small region of land mosquito free.
Let's say a circle, like the one I've drawn here. Well, there's still going to be mosquitoes
elsewhere. And those mosquitoes are eventually going to find their way back in to this area that
you've cleared. And the question is, and here's where it's kind of a math question, how fast?
How much time have you bought yourself if you clear an area of a certain known size
of all mosquitoes? So that becomes a question of, well, how fast do mosquitoes go? Okay,
maybe you know something about mosquitoes, maybe you know how far a mosquito can fly in a given
day. I don't particularly know the answer to that question, but that's something a mosquito
biologist would know. And so what you can see on this picture is, well, maybe a mosquito can fly
a mile a day. Let's just posit that doesn't matter what the exact number is. So then you might say,
like, well, in five days, the mosquito can fly one, two, three, four, five miles. So we can get
pretty far. But here's the thing mosquitoes are not goal oriented organism, not dislike them.
That's just how they are. They don't make a plan and stick with it day after day. They're wanderers,
let's say as a sort of simple model, let's say that each day they wake up and they decide to fly
a certain direction, maybe the first day they fly off this way, and the second day, they fly off
some other way, and the third day some different way, and the fourth day some different way,
and the fifth day some different way. So even though the mosquito is doing a lot of flying,
this particular mosquito choosing its directions at random does not end up very far from its starting
point. And from the point of view of disease control, that's good, right? That means that the
mosquito is not going to sort of jet off in some direction and move a very long distance in a week
or a month or two months. It means that in its kind of random floating around, it's going to
sort of typically not wander very far from where it starts. But the question is, how to quantify that?
How to estimate how far the mosquito is likely to go, given that it's very unlikely to go the
maximal distance it could go if it sort of determinedly and doggedly went straight off
in one direction? So this was fundamentally a math question. And maybe because we have a little bit
of extra time together tonight, what an interesting character Ross is. You know, as I said, one of
the most decorated doctors in his time. But he's kind of a bitter guy. He's kind of always unsatisfied,
always feels underappreciated. He was constantly lobbying parliament to give him more and more
monetary prices, to name more things after him. And part of it was that when he writes in his
memoirs, it's kind of shocking that he never really felt that strongly about medicine. He never
really felt like that was the thing to be doing. There was two things that he valued much more.
One of them was poetry. He was super into poetry. He actually published a lot of poems and sort of
clearly thought of that as a higher calling in medicine. He wrote a lot of poems about how
underappreciated he was for his mosquito discovery, by the way. No, I'm not going to do it. I could
read from his poetry, but I won't. The other thing he really liked was mathematics. The other
thing he really held in very high esteem, higher esteem than the stuff he actually did for a living
was mathematics. So he rather proudly said of this talk, he said they invited me to talk about medicine
and I came and gave a talk about mathematics to a group of doctors who were not expecting it.
It had no idea what I was talking about. So I think it says thing about Ross's personality,
but this was something he said not embarrassed that he screwed up his talk, but sort of proudly
that this was what he brought. In fact, I'm going to go off my slides here just to say
Ross later develops the ambition
to develop what he called a theory of happenings. A mathematical theory that would like Newtonian
physics basically cover every kind of human activity, especially anything that could spread
through a human population like a disease. And while we're talking books, let me recommend
Adam Kucharski is a wonderful book. Is it just called Contagion? I don't have the title quite in
my mind, but it's where I learned about this later theory of Ronald Ross. But he did something
pretty smart. At this point, he understood he did not have the math on his own to set up such a
theory. He brought in a professional mathematician, Hilda Hudson, who was an algebraic geometer.
And they together developed the Ross-Hudson model of Contagion, which underlies basically every
model you've seen of COVID spread in the last year. And I know you guys have all seen a lot.
They all descend from this work that Ross and Hudson did in around like 1915, 1916.
And that's a story that's told in the book. It's not a story I'm going to tell tonight,
but while we're talking about Ross, I just felt I had to mention it.
So what does Ross do in this moment? Ross has presented with this problem that he does not
know how to solve. How far should we expect this wandering mosquito to go that chooses its direction
randomly every day? Once again, he knew he didn't quite have the mathematical chops to solve this
problem himself, but he knew who to ask. He asked Carl Pearson. That's who you're seeing
on the left-hand side of this slide. He said, Pearson, do you know how to answer?
Pearson was probably the most notable statistician of his era in the UK. Also,
by the way, like a phenomenal popularizer, he was the Gresham lecturer in geometry and gave a lot
of talks just sort of like this one in London about the geometry of statistics, a subject
that he in some sense invented. Pearson said, I don't know either, but I know what to do.
I'm going to write a letter to nature. This is what you did in those days, right? There was,
you couldn't like post your question on Twitter. If you wanted scientists to read something and
think about some problem that you were interested in, you wrote into the letters column of nature
and say, can anybody help me with this? And so what you see is a short letter. This is the letter
that Pearson wrote in 1905, the problem of the random walk. And this name is what this subject
has been called in mathematics ever since, the theory of random walks given to it by Pearson.
You might notice that there's nothing in here about mosquitoes and this was on purpose. Pearson
told Ross, this is a little bit of mathematical sociology. I'm not going to say this is a problem
about biology because the mathematicians won't work on it. So we took out all mention of biology.
He also, much to Ross's annoyance, took out all mention of Ronald Ross. You may know he presents
it as if it's his own problem. Well, what happens from here? First of all, this particular problem
is quickly solved. In fact, it solved so quickly that it was solved 20 years earlier by Lord Raleigh
who sort of writes in to say, I already did this, Pearson. But that's not really the...
What's kind of cool about this is that this idea of the random walk, which I've told you about
coming up in the context of disease control and coming out of mathematical biology,
is not coming up only there. Somehow, in this time period between 1900 and 1905,
this same question is coming up again and again around the world. And it's an example that we
see in the history of mathematics a lot, that sometimes the world is just ready for some particular
idea. And when that happens, it pops up everywhere at once. Let me give some examples. So what's
happening in Paris at the same time? In Paris, there's a guy called Louis Bachelier. He's a grad
student at the Sorbonne. But he is not a typical student. He doesn't come from the upper classes.
He doesn't have the usually say education. He was an orphan. He had to work before he went to school.
And he worked in the Bourse. He worked in the big bond market in the center of Paris. And what he
wanted to study was the fluctuations of bonds and stock prices. He wanted to use the tools of
mathematics to study that. Much as Ross wanted to use the tools of mathematics to study disease
spread. And I can tell you that in the very high-flown world of Parisian mathematics at that
time, this was very, let me use a French word, it was very déclassé. It was not the kind of
thing the mathematicians were supposed to think about. It was the mathematicians were supposed
to think about if they were even working on applied stuff at all, it was supposed to be
the three-body problem, like celestial mechanics, things floating around in space.
Not like people buying and selling stuff, which is a little low.
But Bachelier's advisor was Henri Poincaré, the great geometer of early 20th century France,
who was a very open-minded guy. And by the way, he was also a speaker at the 1904 St. Louis
Conference. So a story I won't tell tonight, but we can talk about what is also told in the book,
is that Poincaré, speaking two nights after Ross in the same stage, talking about the crisis
in physics that was about to be resolved by Einstein in this sort of astonishing geometrical
feat one year later in 1905. But we're not talking about that, we're talking about Bachelier.
And Bachelier said, maybe nothing is driving these prices. Maybe it's just chance. Maybe
a good way to think about what's happening with the price of a bond is that every day it wakes up
and it decides to go up or down, essentially at random. And he asked, is this a good model? Is
this actually described the way we actually see prices in the market work? And of course,
to answer that question, you have to know, well, what does happen if a bond chooses which direction
to go at random every day and does that? And as you can see, this is essentially the same question
that Ross was asking. A little bit easier because in Bachelier's work, it's only going up and down,
there's only one possible direction of motion. But the same idea, treating a stock as a random
walk. And what Bachelier found is that actually this model does do a pretty good job of reproducing
the way we actually see these prices behave. In other words, you can look and look and look
for some governing principle that tells you what the stock market is going to do, but quite possibly
there is none. Quite possibly it's pure noise, pure randomness. So he writes this famous slogan,
and I like the Vistice, so I screenshot this for you directly from his thesis, where he writes
l'espérance mathématique du spéculateur et null. The value of, the mathematical value of
speculation is zero. That's the conclusion he comes to, which he calls a fundamental principle.
At the same time, and I don't have a slide for this because you guys all know what Einstein
looks like. But Einstein is in Switzerland, work on relativity, but this is something else again.
A big mystery of physics was what was going on with Brownian motion. Okay, so what's Brownian
motion? It's, if you have like a little chunk of something, some small particle suspended in a
liquid, and you look at it in the microscope, what do you see? You see this particle kind of jittering
around. It's moved as if alive. This was discovered by the botanist Robert Brown.
Hundreds of years earlier, I didn't put the near my notes. He at first, he'd first discovered it
with a piece of pollen. So his first thought was like, wow, this is crazy. This piece of pollen
is still maintains like some kind of vitality, even after it's been separated from the plant. But
being a good scientist, he tried it with a bunch of other organic things, but then he tried it with
a lot of non-living things too. He tried it with like shavings from his window pane. He tried it
with like various pieces of minerals lying around. Actually, my favorite of all, he like lists the
things that he tried, putting it in a little drop of liquid and looking at it in his microscope.
One of the things was a fragment of the Sphinx. That was just a normal thing that you'd have
in your lab. I don't know why he doesn't explain why he had a fragment of the Sphinx or how he
had to come by it. But like everything else he put in his drop of water, it jittered and moved
as if alive when he looked at it under his microscope. So we quickly realized this has
nothing to do with biology. It has nothing to do with some kind of life principle. Just things do
that, and nobody knew why. But there was a popular theory most associated with the physicist
Ludwig Boltzmann, who was also in St. Louis, by the way. He and Ross had dinner together.
Boltzmann was one of the most fervent advocates of the idea that matter was made of tiny little
molecules that we couldn't see. It was controversial at the time. But if you believe that, that gives
a possible explanation for why Brownian motion happens. Because if in the drop of water there's
a particle, which we can see in the microscope, and millions and millions of water molecules,
which are much smaller, which we can see in the microscope, but are moving around at high speed
and constantly knocking into the particle, pushing it one way or another, that might explain the
mystery motion. But in order to test that, you have to say, well, so the model would be that
molecules are constantly millions of times a second running into this particle and pushing it
and essentially coming out from a random direction and pushing the particle in a random direction
at some speed. And in order to test whether that would produce the behavior we actually see,
we have to say, well, what would a particle look like if it were millions of times a second
picking a random direction and moving a little bit that way? Guess what? Same problem, right?
Exact same problem. And Einstein solved it too and saw that, oh, yeah, indeed, that exactly
tracks every measurement we can make of the way that particles actually behave. So this at the time
was the most definitive blow yet in favor of the molecular theory. Okay, so I've told you about
what was happening in mathematical biology and I've told you about what was happening in mathematical
finance, although the financial people didn't actually really catch on to what Bachelet was
doing for like 50 more years after that, but that's okay. And I told you what was happening in physics,
but I didn't actually tell you about, to me, the craziest place that this came up. And so let me
do that now, because we got to get to the puzzle. So this is Andre Markov. And what I want to tell
you about is, surprisingly, an argument about theology. Where is this going to come from? Okay,
let me tell you the story. There's a famous theorem called the Law of Large Numbers,
student Jacob Bernoulli. I'm not going to state the theorem formally, it's not that kind of talk
where we state theorems, but here's basically what it says. And it's a fact that you guys are
probably all familiar with it. If you flip a coin a lot of times, the proportion of heads is very
likely to converge towards 50%. If you flip 10 coins and six of them come up heads, 60%,
that is not surprising. If you flip 100 coins and 60 of them come out heads, that's pretty
surprising. If you flip a thousand coins and 600 of them come out heads, that's really surprising.
The more coins you flip, the more likely it is that that proportion is going to be
within some very narrow bound around 50%. Actually, Carl Pearson, whose picture we saw a
minute ago used to illustrate this by literally flinging open a bag of 10,000 pennies on the
floor of the classroom and making the students count the heads. I still haven't decided whether
I think this is great pedagogy or awful pedagogy, but it was definitely something he was well known
for doing as a statistics teacher. Once people knew this, they started to see over the years,
especially as quantitative social science got started, that statistics of human behavior tended
to settle down to averages in just the same way that coin flips did. If you look at proportions
of male to female births or date of first marriage or number of children or whatever,
I mean, these things might vary some with time, but in any given short time span,
you look at what people do and they seem to converge to averages to some kind of general
social average of how many children each couple has or how many crimes are committed per 100,000
people or such and such and such. Why this was was a topic of hot controversy in the scientific
world. It also created a lot of anxiety because people felt, does this mean that we're just like
coins? We're just automata that are operating under the same meaningless laws of randomness
as coin flips do. That was the way a lot of people thought about what these statistical
regularities of human behavior meant. But as you can imagine, a lot of people didn't like that idea
and one of the people who didn't like it was Markov's enemy, Pavel Alexeyevich Nakhrosov.
They led two rival schools of Russian mathematics. Nakhrosov was in Moscow,
Markov was in St. Petersburg, but it was more than just city rivalry. Nakhrosov was an ultra
conservative, ultra devout Russian orthodox theologian. He was, in every respect, the man of the Tsarist
establishment. We're here, we're pre-revolutionary. We're still in this 1900s, 90s, no five period.
Markov, on the other hand, is a fervent atheist, an enemy of the orthodox church. He actually,
after Tolstoy was excommunicated, he was very jealous and he wrote to the church
to demand that he also be excommunicated. He's like, I'm just as much of a heretic as Tolstoy.
You should excommunicate me too, which they did. He was pleased about that. He was actually widely
known as Neštovi Andrei, like a furious Andrei because of all the sort of angry open letters he
would write to the newspaper. So these guys start out enemies just kind of ideologically. And then
it gets worse. Then it gets worse. Why? Because Nakhrosov is one of those who cannot accept
that human beings are just random automata. If the theorem seems to say that, he said,
the theorem must be wrong, but the theorem is not wrong. So what do you do? He thought about
this a little more. And then he made the following claim. He said, but here, a key input to the law
of large numbers is the independence of the coins. The different coin flips are independent from each
other. If that's not true, the theorem might not work. Like if the coins are constrained to
always fall the same, then it won't settle down to 50%. It'll be either 100% heads or 100% tails.
So Nakhrosov said, aha, you thought that Bernoulli's theorem meant that humans were mindless
automatons like coins. But what it actually means is that we're independent from each other,
that we have free will. Well, Markov was not having it. I think from Markov's point of view,
it's okay that we have these different religious views. What's not okay is for Nakhrosov to say
it's mathematics. Now it's a noun, Markov's view. Now something really important is involved, the
honor of mathematics. And so if they have this big fight, and what Markov does is he draws the
following kind of picture. So I'm going to write this sort of in terms of mosquitoes, because
that's what we're talking about. Markov says, consider the following scenario. You have a mosquito,
and let's suppose for simplicity, it only has two places that can live, two bogs. I guess that's
where mosquitoes live, bog zero and bog one. And at any given day, the mosquito makes a random choice.
Mosquitoes, maybe the mosquito is pretty comfortable in both bogs. So if it's in bog zero,
90% chance it's going to stay in bog zero. And maybe 10% chance it'll jump over to bog one.
If it's in bog one, maybe bog one likes a little bit less, there's actually a 20% chance it'll jump
over to bog zero. But still most of the time it'll stay in bog one. And what Markov showed is that
if you let the mosquito live a long time, it too will settle down in a very statistically regular
fashion to spend about two thirds of its life in bog zero and one third of its life in bog one.
Just like a sequence of coins will converge to be 50% heads, 50% tails. Even though
certainly where the mosquito is one day is not at all independent from where the mosquito is
likely to be the next day. In fact, they're very highly correlated. It's very likely to be in the
same place day today. So what Markov was explaining to Krasov is like, yes, if you have independence,
you get Bernoulli's theorem, you get this kind of statistical regularity. But that doesn't mean
that if you see the statistical regularity, it must have been because of independence. There's
lots of non-independent things that give you this regularity too. So he was refuting the Krasov's
claim that he had somehow proved the existence of free will by mathematics. And I got to say,
I'm just, you know, speaking for myself, I think there's like a very long multi-century history
of people trying to prove philosophical claims about the nature of humanity, the nature of
existence, the nature of the deity by mathematical means. I'm kind of a skeptic of all such attempts.
I think math is wonderful and does a lot of incredible things, some of which we're going
to talk about tonight. But it doesn't do that. That's not what it's for. And that's certainly
what Markov thought. And at least on the mathematical point of issue in this argument,
he was clearly the winner. And even today, another name for a random walk or at least a certain
kind of random walk is a Markov chain. Because Markov was the person who, I told you about a
lot of examples that individual people came up with. Markov was the one who really sort of made
this into a theory and understood like the theory of all kinds of processes of this type.
What's funny is that, so I'm going to take this in kind of a weird direction because
Markov was actually not interested in mathematical biology at all. He was tended to
turn up his nose in actual applications. But there was one that he was willing to talk about,
because there was one thing that both atheists and religious Russians definitely valued and
really wanted to think about a lot. And that is the poetry of Alexander Pushkin. And that was
where Markov took this a few years later. He took Pushkin's famous first poem, Eugene O'Nagan,
and just wrote it as a sequence of consonants and vowels. He wrote out like 10,000 letters
of this book, just marking whether they were a consonant or a vowel. And then said, okay,
if one letter is a consonant, what's the next letter likely to be? And he found that it was
pretty likely to be a vowel, like two consonants in a row is pretty rare, about two thirds of the
time to be precise, 66.3% of the next letter after a consonant was a vowel, whereas then 33.7% of the
time it was another consonant. Whereas if you were a vowel, double vowels were even rarer,
only 12.8% of the time would a vowel be followed by a vowel. And so this picture,
you see this is formally very much like the mosquito moving between two bogs. He was modeling
Pushkin as a kind of poetical mosquito kind of moving back and forth between consonant and vowel.
And I think you'll agree, this is like a pretty impoverished and limited way of thinking about
poetry. It throws out a lot of what's special about Pushkin, of course. But what's cool is that
he was able to show that you actually get different Markov chains from different writers. He sort of
did the same thing with a novel by another Russian writer. Now, at least to my knowledge,
kind of forgotten, Aksakov, and found that you just got different values from these numbers.
The patterns of consonants and vowels were different. So Markov chains now, and there's
so much of the story I could tell, and I'm only going to tell a little bit of it tonight, are
fundamental to, I mean, still to mathematical physics, certainly to computer science, like all
to the process of legislative district thing, which I write about a lot in the book and I won't
touch tonight because it's a sort of US-centric political question. Maybe I'll give, okay, I
got to give one example of how this is used. So how does Google work? I think I can tell you
in one picture. I'm going to try. This is intentionally left blank, by the way, because I'm
about to write on it. I don't know how old everybody is in the room, but if you're about my age,
that's about the boundary for can you remember the difference between the internet before Google
and the internet after Google? And if you are younger than me and don't know that, let me tell
you, it was really different. It was an instant humongous change in your ability to search. And
why is that? Because Google developed something called PageRank, which enabled it to say, you know,
I searched for some word like frog and a lot of pages come up that have the word frog in them.
And Google was able to make a very good assessment of which ones were the most important, using the
structure of the web itself. How do you do that? I'll write PageRank so we can all see what it is
or talking about if I can spell it. So a natural thing you might think is, I guess a link, a page
is important if it has a lot of links to it. That's reasonable, right? And that's sort of important
web page that you might want to see. Probably a lot of people would link to it. So one thing you
could do is say, I'm going to rank the pages by the number of incoming links. And that does not
work. And I'll explain to you why it doesn't work. Because if that were the mechanism that a search
engine used, here's what I would do. I would sort of have my frog fan page that I made.
And then I would also have, let's say I have frog fan too. I make two web pages. And then I make like
a lot of links from each one to the other. And I can only draw like five or six, but let's pretend I
drew like 10 billion links each way. So then by your proposed mechanism of rating importance,
my page would be incredibly important. It would come up at the top of the search race. And then,
of course, what there would be would be some sort of ridiculous arms race where people were just like
putting more and more and more links on their page. Okay, this is not what you want to do. You want
something more important like, I don't know, American Frog Society is their one is probably
like the herpetological society or something. But we're just making stuff up. So let's say this.
This would likely have links, also a lot of links, probably not 10 billion of them. And,
but more importantly, those links wouldn't just be from my own pages, they would be from other
important pages. So, you know, maybe there would be a link from the Royal Institute.
You probably have talks about frogs sometimes, right? So there might be like a link to sort of
some official document of the American Frog Society, or the British Frog Society or whatever
it may be, there might even be links from, you know, the Times or other news organs or stuff
like that. And say, what's important is not how many links it has, we should say a site is important
if it has links from other important sites. But wait, now there's a problem because that's
completely circular, right? If we don't know how to define important, how do we know whether the
links it has are from important sites? And the answer, as Larry Page and Sir Guy Brin figured
out and then just became like ungodly rich as a result, is to use the Markov chain.
So just like here, I'll pop back a little bit. Just like the mosquito,
flitting back and forth between bog zero and bog one, bog zero is somehow like a little bit
better, right? And it ends up spending more time in bog zero. You can imagine something like the
mosquito that just is like a little robot that surfs the internet all day long, choosing a random
link from every page and wandering the net in this way. And the theory of Markov chain guarantees
that this will settle down to something statistically regular, and there is a proportion of its life
that the robot will spend at each place. And you know what? That robot is very rarely going
to be over here because how would it get there? If it didn't start there, it would only happen
to end up there if somebody else linked to my dumb frog fan pages, which why would they? Whereas it
probably would, many, many paths would take it to the Royal Institution or would take it to the
Times, which would in turn take it to the American Frog Society. So it would probably end up here
more. So this process of the Markov chain actually does an incredibly good job of sort of showing
you which points in a network are important. The ones that are sort of random walker through
that network would find itself at more. And it's an idea that in some sense uses, I would say it
uses about like college, sophomore level math, and it absolutely transformed the web and created
this corporate behemoth that we have today. So I'm not going to talk too much about the internet.
That's all I'm going to say about it. I actually want to sort of follow this idea of studying language,
this idea that sort of Markov kind of fancifully did when he was thinking about the poetry of
Pushkin, okay? So because why on earth, you may ask, would Markov reduce this beautiful poem to a
sequence of consonants and vowels? Well, it was because like that's what he could do, all right?
He didn't have a computer. He had to limit the story to just two things, a consonant or a vowel.
Well, nowadays we don't have to do that anymore, right? We could keep track of all the letters.
Okay, I don't know how many letters there are in Russian. In English there's 26. So we could keep
doing this with English text. We could keep track of all 26 letters. We could even do more
because this is another one I'm going to draw for you. So we could also talk about
bigrams. And a bigram is just a fancy term for a two-letter sequence, like
a, a, a, b, a, c, a, d, b, a, b, b, b, b, c. I guess we could go all the way down to z, z, this big square
of, I guess it would be 26 by 26. So there would be 26 squared. Who's computing the center ahead?
676 bigrams in all. And every piece of English text, just as Eugenia and Egan can be thought of
as a sequence of consonants and vowels, every English text can be thought of as a sequence of
bigrams. And the following way, if my, if let's say I was doing a royal institution,
then I guess the first bigram would be r, o. And the second bigram would be o, y. And the third
one would be y, a. And then a, l. And then, you know, maybe we should actually count a space as
one of our characters. You get l space and so on and so on and so on. And so you can do the same
kind of analysis, take an English text and say, if you're at one bigram, what is the next bigram
likely to be? So for instance, if I'm at a, a, that's pretty rare in English, right? That's not
going to happen very much. But if you were there, where might you be? You might, maybe you'd be
talking about somebody whose name was Aaron, and then you would go from a, a to a, r. That would
also work if you were talking about an aardvark or an aardwolf. Okay, I'm having a little trouble
thinking of other good words with an a, a bigram in them, but that would be something that could
happen. But you can imagine other things happening too, but probably with a lower probability.
If you were at a, b, but if you're at a, a, the next bigram has to start with a. If you're at
a, b, the next bigram has to start with b. And again, a, b, b, a would probably be more common
than a, b, b, c, than a, b, b, c, because that would mean a word that had a, b, c in succession.
I can't think of one off the top of my head. That probably does exist, but it's not so easy to, uh,
crab cake. Okay, I knew I would have one in mind. So if you were talking about a crab cake, you could
get this, but it would probably be pretty rare. So every one of these arrows, just like the arrows
in our diagram before would have some little probability attached to it. And you could think
of the, some body of English text as being described by what probability, these so-called
transition probabilities there are from one bigram to the next. Now, okay, ready? We're going forward
to the 1940s and Claude Shannon. And Shannon's, what Shannon realized is that this is something
that you could use not just to analyze an existing text, the way that Markov had done with Eugene
O'Nagan, but to create text. Once you have these probabilities, you could just say start with a
bi-gram like a, b, and then like the random walker walking the internet, pick another bi-gram at random
according to whatever the probabilities are that you've computed from your body of English text.
So maybe, maybe you would find a ba was next. So you'd be at,
at ba. And then from ba, you're going to go to, there's lots of places you could go, right? Anything
starting with a, ba-d, ba-e. So maybe the next one is, you know, if you had ab going to ba, going to
at, and your random walk, then you would be like abat. And by this process,
you can produce artificial text. By the way, when Shannon actually did this, it's kind of fun,
you can even do this yourself. Computers and computer times are still pretty limited at this
time. So he did a version of this where he would just say, okay, I'm going to pick a bi-gram like
ab, and then I'm going to pick a book off my shelf and flip through it until I see an a followed by a
b. And then I'm going to see what the next letter is in that word. Maybe he was looking at a book
about an abacus and the next one was a, and we'd write great, aba. Now I'm at ba. Now I pick up
another random book off my shelf, flip through it until I find the letters ba in succession.
What's the next letter after that? Maybe it's t. Okay, then my next bi-gram is at. So sort of
using his library as a sort of way of generating random numbers to let him walk through this
Markov chain and using this to generate random texts. And here's an example of what he got.
In Nois Latwe, Cratecht, Froar, Beers, Grossed, Pondinom, of Demonsters, of the Reptigin,
is Rago Actiona of Cree. I love looking at this, man. This is from his famous paper,
A Theory of Information, in 1948. And I love the way it is clearly capturing something
about the English language and the way it works without actually being English, right? This is
definitely not English. And yet, there's, it's capturing something of the way we put words
together. Nowadays, of course, we don't have to walk around the library the way that Shannon did
in order to carry out a process like this. We can use a computer, right? We can use a computer
with like tons and tons and tons of English text already stored. And depending on the properties
of English text that you start with, you get different results. So I think I'm ready to talk
about the puzzle. By the way, again, with modern computation, I wrote down bi-grams of which there
were 676. You can use longer letter chains too, like trigrams, which are three letters, or even
pentagrams, which are five letters. There's a lot more of those. I can't write them all on a slide,
but a computer is perfectly capable of thinking of an English text as a sequence of
five letter strings and producing artificial English text thereby. And now I'm going to show
you what happens. If you do that with a data set, I love to play with the data set of baby names.
And ideally, guys, I would have a British version of this, but I don't know if the British
government makes this data public. I have not been able to find it. But in the United States,
you can get a list of every name that was given to five or more babies in the United States. And
every year, since 1913, you can even have it broken down by state. It's a great, great, great
data set that the Social Security Administration puts out that I've actually used in papers for
various data analysis things. But what you were looking at here is if I take the list of every
name given to a baby in 1971, which happens to be the year of my birth, that's why I like it.
And then you do this by-gram thing, I told you. You do this Markov chain and randomly walk along
it and make artificial baby names. Here's what you get, Tandola and Berylan, Marahadria, Kassanian,
Quill and Abinelit. And you can do this with three letter sequences. And then you get the thing
that we started with, the puzzle. The answer to the puzzle is those are artificial 1971 U.S. baby
names produced by a random walk on trigrams where you pick one at random, K-E-N. And then
you go around in your database and you look until you see something that starts with,
that has a K-E-N in it. And apparently in that database, the next thing was a D. So the next
trigram would be E-N-D. I'm trying to think if I can guess what name has a K-E-N-D. I can't quite
think of it. Once you start to get the longer the sequence is, somehow the more powerfully
you're capturing the properties of the data set that you trained on, the original text,
but so much so that you actually just start repeating actual things you found in the data set.
So when you start using five letter sequences, you actually see it make, you know, picking out
actual names that were used like Adam and Susan and Kelsey. Again, what's so cool about this to me,
so trigrams for me for baby names are kind of the sweet spot where you get really realistic
looking names that are not just copies of actually existing names. And what's cool to me is that you
can look at, do the same thing with baby names given in 2017 and 1917. And I think you'll agree
that in each case, a lot of those are like pretty realistic names and they somehow from this very
mindless random process capture something of what you might call the style of naming practices
in 2017 and a century earlier in 1917. Those like seem more old fashioned.
We're talking about geometry. So let me
say a little something about geometry that you see here. Okay, this sort of seems like,
you know, who said this seems reasonable. The most common geometric object is the square.
A square is a perfect right angle that has four sides. Okay, okay.
So nobody said this. This is text produced by a sort of of the moment neural net called GPT-3,
which in some sense is the modern air to what Shannon was doing. It's a machine that produces
artificial text, but it's really no different in principle from what I just showed you this kind
of Claude Shannon walking through his library, flipping through the books to find the right
by-gram. Of course, the units for something like GPT-3 are much longer. It's trying to predict the
next word or the sequence of words, not just the next letter, but it's working in fundamentally the
same way. There's some underlying Markov chain that it's learned through some kind of like neural
net optimization process, whatever that means. And it is using that to given some sequence of text,
try to predict what's the next word, put that word down. Having done that, now it has a longer
sequence, trying to try to guess the next word from there. And it is pretty cool and it is pretty
good. And if you train it on your own writing, which is what I did here, it's a little bit spooky
because it does capture something of the style of your own writing, which is pretty weird to read.
At the same time, just like what, just like the sort of artificial words that Shannon made
are not real words, the artificial text that the GPT-3 produces is not real text, even as it kind
of eerily captures some of the features of real text. So one thing when I generated this, I was
really struck by it because especially this last sentence, look at that, but squares aren't just
shapes. They're also numbers. Boy, as a number theorist like me, I love that because, of course,
I'm like, wow, like it really sort of is picking out some actual fact that a square might mean
like a shape like this, but also I would say a square, a perfect square is a kind of number,
like four or nine or 25, a number that is a smaller number multiplied by itself, like five times
five is 25, it's a perfect square, seven times seven is 49, a perfect square. I was like, wow,
that's cool. But let me show you what happens when you continue this trial a little bit.
It starts to say stuff which syntactically makes sense and it looks like English and it even looks
like something stylistically, I suppose that I might write, but it makes no sense at all.
Right. When we say a square is a number, it's not the number you get by adding the length of
the four sides, because that would be called the perimeter. And then it kind of doubles down and
it's like, that's right, one plus four equals two and two plus four equals five and so on.
So it's sort of somehow correctly understanding the kind of thing that it should say without
actually having any grasp. I know I'm anthropomorphizing rather badly, but
of what it is, so to speak, supposed to mean. And this is super fun to play with and I highly
recommend sort of like looking at instances of this thing because it really, to me, gives you a
very good picture of kind of where artificial intelligence, so-called, is now. It can do amazing
things that are also like incredibly limited in ways that I think is really, it's a little bit
hard to grasp unless you sort of play with it a bit yourself. So maybe I'll close because we're
just almost at the end of this part of our time together. In the book, I do talk quite a bit
about artificial intelligence and machine learning and what's going on under the hood because the
geometry of that is actually not as complicated as you might think. And I think it's very useful
to demystify what's going on with a lot of these truly exciting new developments.
And what I want to say, so this is like a sort of cartoon, I think what a lot of people are
concerned about is, well, if computers are better than us at, I don't know, playing chess or carrying
out algebraic computations or translating from one language to another, maybe that's because
they're just better than us. So maybe they'll just supplant us in every way. So to use yet another
geometric metaphor, which my book is full of, that is a one-dimensional model of difficulty,
which I've depicted here, as if difficulty is kind of proceeds along one dimension from easiest
to hardest, and that computers are either ahead of us or behind us. So I think that's pretty badly
wrong. I think when you sort of think about what kind of problems machines right now can be trained
to solve, something I write a lot about in the book in the context of games, especially checkers,
or I guess in this context, I would say drafts. It's not one-dimensional. There are many dimensions
of difficulty. And I think one of the most interesting parts of the new mathematics of
machine learning, yet another new field of mathematics we're developing under our feet,
is to understand which kinds of problems machines are good at and which kinds of problems we're
good at. Machines are great at playing chess, way better than us, all of us. But a robot can't
fold a shirt. And I don't think we really know why that's true. I don't think we really understand yet
what you might call the geometric space of problems, how many dimensions it has, what those
dimensions look like, and where the zone of human superiority is and where the zone of machine
superiority is. So with that, I'm going to put this slide up again. We talked about some of
these things, and I hope the questions have been piling up in the chat. I'm just going to sort of
circle it. I always like to see how far I got and what I talked about. So we talked about Google,
we talked about Carl Pearson, we talked about Hilda Hudson and Ronald Ross. And you guys just
started asking questions, and I think Martin is going to feed them to me, the St. Louis exposition,
which featured Ross and Poincare. But we did a lot today, Bachelier, his students. So Poincare and
Ross are both at the St. Louis exposition. Boltzmann in the molecular theory. I'm just
circling stuff, Martin, so whenever you want to start throwing me questions.
Well, thank you so much, John. That's been absolutely fascinating.
