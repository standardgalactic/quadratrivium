Today on The Standing on the Shoulders of Giants podcast, we're going to do things just a little bit different in that I will be both the host and the guest.
My name is David McKay, or David McKay, if you want to pronounce it the American way.
I have about 25 years in technology, software in particular, and that feeds right into what we're going to talk about today, which is algorithms, artificial intelligence and machine learning.
We're going to learn about their importance in today's world.
We're going to understand that some of these concepts are actually thousands of years old, and we're going to talk through some various examples of these things and how they impact us in our daily lives.
A little about myself is, at 19 years old, after doing some contracting for Hotmail, if you remember that, I worked at a little known search engine in 1999, again at 19 years old, and that was Google, fairly well known now.
If anybody had imposter syndrome, it was me.
If anybody deserved to have imposter syndrome, it was me.
The vast majority of the Google staff had master's degrees at the time, and a tremendous amount of them also had PhDs.
The few with bachelor's degrees were actually few and far and between, especially on the technical side of things.
And the people with zero degrees where no one, I believe I was the only person actually without a bachelor's degree.
Although I eventually finished up university at the end, in my 30s, actually, I definitely had imposter syndrome pretty heavily.
Google was a very different company back then.
In particular, I worked on a team speeding up Google.
They left the relevance of the search engine, which was what made it so popular.
They left that to the really smart guys, and I was just working on how to get the results back to you in a very timely fashion.
My desk at the time was a door, a full-sized door with a handle on the right-hand side, placed on two steel saw horses.
And so Google was a bit of a different company than the paradise you hear it talked about today, or for that matter, when I went back there approximately 11 years later.
My point in telling the story is the concepts that we're going to talk about today are difficult concepts in practice, meaning when you're actually putting them into place.
But they're not difficult concepts to grasp.
Once you've ended up grasping them, they're actually not that difficult concepts in practice either, although some of them are depending on what you're doing.
But they do shape our world.
When I was growing up, GI Joe was one of my favorite cartoons, and one of the axioms on GI Joe was knowing is half the battle.
Call me a Gen X or 80s kid, whatever it may be.
But I believe that knowing is half the battle and understanding these things, even if you don't want to be a software engineer, which is more than fine.
But knowing these things and understanding how they impact in today's world is extraordinarily important.
So with that in mind, why don't we jump right in and talk about algorithms?
An algorithm is a step by step procedure to solve a given problem or to complete a given task.
It is a precise set of instructions that can be followed to achieve a specific goal.
There you go.
Like I said, these concepts are not that hard.
There are some very complex algorithms.
We don't need to go through them.
But that's all it is.
It's a set of rules that are followed to achieve a given outcome.
Now, this might sound familiar to you, and that's because it should be.
An algorithm is a recipe.
A recipe is an algorithm.
The difference being that algorithms are traditionally run by computers, although they do exist in other fields.
And recipes are traditionally completed by human beings.
That's it.
It's pretty straightforward.
We're really just talking about the difference between jargon.
In cooking, you've got jargon.
You've got all sorts of jargon in cooking, just like you've got jargon in computer science.
And the jargon in cooking for how to complete something is a recipe.
And the jargon in computer science for a computer to complete a task is algorithm.
So there we go.
But algorithms have a pretty rich and interesting history.
They were placed back to ancient civilizations where they were actually used to solve mathematical problems and perform various calculations.
Some of those existed in ancient Babylonian society, ancient Egyptian society, and ancient Greek society,
so literally to the absolute beginning of human civilization.
The modern concept of an algorithm, as we think of them on computers,
can be attributed to the work of mathematicians and logiticians in the 19th and 20th centuries,
such as Alan Turing, John von Neumann, and probably my favorite Lady Ada Lovelace.
Let's talk about them. Let's talk about how their algorithms changed the world,
and what did that really look like?
To start, let's talk about Alan Turing.
Alan Turing was a mathematician at my alma mater, Cambridge University.
He was recruited by MI6 to work at a fake radio factory called Bletchley Radio Factory.
The purpose of the fake radio factory was not to manufacture radios, but it was to break the German enigma code.
Now, to be clear, the German enigma code was considered at the time to be absolutely unbreakable.
It was cryptography used to pass messages back and forth, unbreakable as unbreakable gets.
And what I mean by that is, the British actually were in possession of multiple enigma machines,
and yet they still couldn't break the actual code.
That's how well-designed these machines were. They were incredibly well-designed.
So Alan Turing's challenge, along with a team of mathematicians, logiticians, and actually game theorists,
was to design a machine that could effectively beat this enigma device.
What they ended up making was something called an electromechanical device.
It wasn't quite a computer, but it was a computer.
It wouldn't be a computer in the way that we think about computers today,
but it was a purpose-built machine to exact a very specific algorithm.
The algorithm was actually baked into the mechanical components of the machine itself to break enigma.
Now, it was called the BAM, or B-O-M-B-E machine.
It was based on a Polish code breaker called the BAMBA, or B-O-M-B-A machine.
This machine, upon completion, provided crucial intelligence to the European side of World War II.
Now, to put some numbers around this, when I say crucial intelligence,
this thing actually shortened the war on the European front by over two years,
and is credited with saving 14 million lives.
That's a pretty amazing thing for, again, a very purpose-built algorithm.
Now, Alan Turing in particular, later in his life he was found guilty of something called gross indecency laws
that were specifically targeted at homosexuals at the time, which was extraordinarily unfortunate.
Instead of going to prison, he accepted chemical castration, which had some pretty nasty side effects, as one would imagine.
He reportedly died of suicide at 41 years old after ingestion of cyanide,
although there are some conspiracy theories that's not necessarily the case, regardless that is the official record.
However, this story does have a bit of a happy ending, although not in Alan Turing's lifetime, unfortunately.
In 2009, a petition was started to apologize to Alan Turing posthumously,
and it's made its way through the British government at varying levels, having success almost the entire way through.
Now, some might wonder, why exactly did this start in 2009?
And the answer is, we didn't know that enigma was broken until I believe the 90s.
It was literally kept a secret for that long, because when you break an unbreakable code,
you don't tell anybody that you've broken that unbreakable code, you make sure that people still think it's unbreakable.
Regardless, in the 90s it was declassified, I believe it was 90s,
declassified that Alan Turing had actually done this, and in 2009, a petition was started.
This petition culminated in 2013, when a royal pardon was issued by the Queen posthumously for Alan Turing,
and being convicted under those ridiculous laws.
So kudos to the Brits for actually recognizing a flaw of the past and doing something to rectify it.
Now, only four of those royal pardons have gotten through since World War II and the Queen is signed,
so it was a pretty amazing thing.
We owe quite a bit to Alan Turing and the work that he did with that bomb machine,
and again, to that algorithm that was breaking enigma.
Another example is John von Neumann.
By the way, looks like von Neumann, but it's pronounced von Neumann.
He developed something called the Monte Carlo method.
Now, the Monte Carlo method was developed during his work on the Manhattan Project,
the Manhattan Project being the American project to develop the atomic bomb during World War II.
Now, the algorithm, the Monte Carlo method, involves generating random numbers to simulate a very large number of trials
without actually doing those trials and then calculate the probabilities statistically.
This was specifically used to calculate the probability of fission, although he had many other contributions.
Fission was probably the biggest one as fission is splitting of the atom.
In other words, the actual core component of what makes the atomic bomb.
Now, to test this method, von Neumann enlisted the help of various colleagues and put them in a room
to sort of simulate these random numbers.
We don't have to go through the whole experiment, but regardless, it proved to be a resounding success
with both his colleagues and on the project itself.
And it proved to be an incredibly powerful tool for solving complex problems.
Today, the Monte Carlo method is used in various fields such as finance, engineering, and computer science
and is used almost on a daily basis.
And finally, we're going to come to Lady Ada Lovelace, who is my favorite in this group for the sort of pioneers of algorithms.
Lady Ada Lovelace is the daughter, child of Lord Byron.
Now, on a bit of a side note here, if you've never heard of Lord Byron or you've never read any of his poetry,
Lord Byron was incredibly brilliant, as was his wife, who was also, I believe, a professor.
Now, it's no wonder that they produced Lady Ada Lovelace, who was, in my opinion, even more brilliant than they were.
Lady Ada Lovelace produced a handwritten algorithm on a piece of paper in 1842
prior to any machine existing that could even run algorithms.
It was based on Charles Babbage's difference engine, which later, I believe, turned into the analytical engine.
Although neither one of those was actually built during their lifetime.
This was programming based on a spec of a machine that purportedly would exist in the future.
Now, this is something we do in computer science almost on a daily basis,
as we program on things that don't necessarily exist, or for that matter that we're not sure are going to exist,
but we have the specification, and therefore we create the program based upon it.
Lady Ada Lovelace did this in 1842, but not only that,
but she outshined the vast majority of Charles Babbage's other students and colleagues in the sense of
she didn't see this as a difference engine, nor did she see it as a mathematical calculator.
What she saw in particular was she saw a computer.
She speculated on branching conditionals, subroutines, also called functions, and loops,
all three of which to this very day are used in every single solitary computer programming language on the planet,
with very few exceptions.
So in other words, in 1842, she effectively predicted modern computer science without any machines existing.
Lady Ada Lovelace is considered the first computer programmer ever,
and in my opinion, she is the absolute godmother of all of computer science across the board.
So kudos to you, Lady Ada Lovelace.
Algorithms today are also used in a wide range of fields, computer science, mathematics, engineering, and more.
But let's go through an example that you probably use on a day-to-day basis, which is the social media feed algorithm.
They have these on Instagram, TikTok, Facebook, among others.
Basically, this is the algorithm that determines what to show you as a user,
and it determines this based on your interests and your behavior.
Now, when I say behavior, what I mean is your online behavior.
For example, mouse clicks, scrolling, things that you've interacted with before.
But there are certain things you may not know that we're also able to see quite a bit of, especially inside of a web browser.
And that is, we're able to track your mouse.
We know exactly where your mouse is almost at all times.
We're able to see what you're lingering on.
We're able to see what you type even when you don't click submit.
We're able to see literally everything.
Now, this shouldn't really scare you because the truth of the matter is most of these social, in fact, I'll say this differently.
All of these social media companies have zero incentive to ever sell your data to anybody.
In fact, that's the exact opposite of what they want to do.
They want to keep your data for themselves because that's how they make their money.
When people say, I think Facebook's giving away my data and they know so much about me, yes, they do, but they want to keep that to themselves.
Trust me, that is the exact opposite of what social media companies want to do, meaning sell your data.
They don't want to do that at all.
Anyway, going back to the algorithms that they use is what they use is something called a look-alike audience algorithm.
Now, this is based on hundreds and hundreds of factors, which of course we don't have time to go through today.
But some of the things I can tell you is this.
You don't feed, well, maybe some of you do, but most people do not feed enough data to a social media platform that they know every single thing about you.
They can't predict your behavior in the ways that they would like to do.
And by predict your behavior, I mean they can't predict what posts you would like to see.
They can't predict what posts you would like to interact with.
They can't predict what ads would be the most relevant for what you're looking for.
Now, they try to, but they can't do it on an individual basis.
So what they can do is create a look-alike audience, which is exactly what it sounds like.
Audiences that look like one another.
Everybody in that audience looks vaguely similar to other people.
And when I say look like, of course, I don't mean your physical appearance.
What I mean is your activity online.
What does that look like?
If you clicked on Thing X, whatever that may be, it will measure how many people clicked on Thing X and then what was the most common outcome after clicking on Thing X?
Did they click Thing Y, Z, or A or B or C?
And they'll basically stack rank those.
I'm simplifying this quite a bit, but regardless, they'll stack rank what happens after you performed a particular action.
And the vast majority of people in that group will actually do another action that's similar to one another, hence why they look like each other.
Because their behavior is similar online to other people.
Now, ultimately, these social media networks would like to create something that's just specific for you, tailored for you, such that they can understand you very well.
And they want to do this because they want you to have a phenomenal and fantastic experience on their site.
But for now, what we do is we do look-alike audiences.
In other words, when you're wondering why you're seeing that video, you've never clicked on anything like that.
It's almost assuredly because of a look-alike audience that you've been placed into.
Whether or not you're super similar to that audience or not, that's a difficult problem.
You might not be super similar, but you might be similar in ways that you don't necessarily understand.
So that's how a lot of the discovery happens on these types of engines.
But that leads us right into look-alike audiences are also powered by artificial intelligence.
Let's talk about artificial intelligence since that's one of our other topics today.
Artificial intelligence in the field of computer science is something that focuses on the development of intelligent machines that can perform tasks that normally require human level of intelligence.
Things like visual perception, things like speech recognition, things like decision-making, and things like language translation.
The history of these can be traced back to the 1950s when researchers first began exploring the possibility that you could create these machines that could think like human beings.
But before we dive in there, it's important to call out that there are two separate fields of AI at present.
One of them effectively doesn't exist, or it's in its absolute infancy if it does.
And the other one, I wouldn't call it mature, but definitely you've interacted with it on a daily basis.
The two different fields are narrow AI and general AI.
Narrow AI is thinking like a human being in a very narrow way.
So being able to recognize a human being, so something like facial recognition, that is an extraordinarily narrow band.
If all a human could do was recognize people, we would consider that human to be underdeveloped, wouldn't we?
In this particular case, that's what narrow AI is.
You end up finding very particular things that machines are good at and assigning those things to that machine,
and training the machine such that it has close to human-like intelligence,
being able to recognize a cat versus a dog in the instance of visual perception,
being able to recognize people's speech patterns in that case of speech recognition, things of this nature.
General AI is where most of us used to think that AI was,
and that is something like the Hal 9000 computer from the movie Space Odyssey 2001 or the Star Trek computer.
In other words, a singularity, something where we've created what we believe to be consciousness or true artificial intelligence inside of a machine.
The optimists will tell you we are decades away from this.
The pessimists will tell you we will never ever get there.
I like to consider myself a realist and somewhere in between the two and that we are 50 to 100 years away from even coming close to that.
Regardless, general AI is in its infancy and narrow AI is where we are right this second, and that's what we'll talk about.
But these concepts have actually been around for quite some time.
Like I said, the history of AI can be traced back to the 50s, but that's the modern history.
In ancient Greek mythology, there is something called Talos.
Talos is a bronze, a made of bronze automaton, charged with guarding the island of Crete.
Talos was invented and crafted by Hephaestus, who was a Greek builder god of sorts.
Talos was a bronze automaton in every sense of that word.
It effectively was powered by artificial intelligence.
It definitely was not alive and they made that very clear in the myths, and yet it could be controlled and it had specific intelligence for how to guard an island,
who to let past and who to deny access to.
So even the ancient Greeks had the concept of artificial intelligence.
Another example would be the Turing test.
Now the Turing test is actually to test artificial intelligence, and it comes from Alan Turing, who we already discussed.
A Turing test at its core is designed to determine whether you are interacting with a machine or a human being.
Why is that difficult? That seems pretty simple.
The Turing test was originally set up with a teletype and a screen.
In other words, a keyboard and a screen in one room, and a keyboard and a screen in another room, or a machine in the other room.
But you didn't know what was in the other room, and you would converse back and forth with this particular machine or human being and try to decide,
is this a machine I'm talking to, or is this a human being I'm talking to?
The closer you get to basically not understanding what it is you're talking to, the closer you get to passing the Turing test.
There was a chatbot in the 60s specifically designed around this called Eliza,
and actually if you ask Siri or Alexa about that, you'll get some interesting answers.
That was their specific predecessor, although in modern computer science we don't really like to talk about that bot,
because it doesn't really use the concepts that we would consider to be artificial intelligence and machine learning to this day.
Regardless, artificial intelligence has had some pretty big leaps and bounds in the past couple of decades.
In 1997, IBM's Deep Blue beat Gary Kasparov in chess.
This achievement marked the first time a computer was able to beat a world champion in a game of chess.
It was a pretty incredible achievement as well.
In 2005, a Stanford robot drove autonomously for 131 miles along a trail.
Rivian and Tesla, I'm sure they're giving kudos to Stanford just about every day for that one.
In 2013, a Google division called DeepMind created something called AlphaGo.
AlphaGo became the first computer program to defeat a world champion in the ancient Chinese game of Go.
Now, this particular achievement was made possible by the use of reinforcement learning, which is effectively learning via reinforcement,
meaning you teach a computer to make decisions and then you either reward the computer by literally adding a counter or subtracting a counter,
and teaching the computer that adding the counter is a good thing and that subtracting the counter is a bad thing.
But we'll get to that when we talk about machine learning.
In 2018, an AI-generated artwork sold at Christie's for $432,000.
This was the first time an AI-generated artwork had sold at any major auction house ever, although it certainly won't be the last.
A subset of artificial intelligence is called machine learning, which we've been brushing up against so far.
This effectively involves machines learning from data and improving over time based on, again, their learning.
Now, it's important to note that these machines, when we're talking about them learning, they are trained, not taught.
Human beings are taught, not trained, at least in the classic sense.
When machines are set out to perform a particular task with machine learning,
you effectively give them this task in very few directions, if any.
And then you tell the machine afterwards, after it's completed the task,
or, for that matter, hasn't completed the task or done it the wrong way, which is what happens often times.
You tell the machine, that is incorrect. Here's where you messed up.
And then you have the machine do it again.
In this way, machine learning is very much an art and not a science, but we'll get into that later.
The history of machine learning can be traced back to the 1940s and 50s,
when researchers first began exploring the possibility that you could create machines that would learn from their own data.
But, of course, we didn't have the hardware, or, for that matter, the software,
to be able to have the speed such that these machines could do anything usable in that time frame,
although this did speed up pretty heavily in the 90s.
Today, machine learning is used in a wide variety of applications, from image recognition to fraud detection.
Arthur Samuel actually coined the term machine learning.
He taught a machine to play checkers, and he ended up teaching the machine to play checkers against itself.
The idea was to eventually have the machine play every possible game of checkers,
store those outputs in its memory, and thus have an effectively unbeatable machine at checkers,
although that didn't end up happening because we didn't have the hardware at the time when Arthur Samuel was doing these.
Another story about machine learning is it saves lives.
A few years back, a man had a heart attack while he was running.
His smartwatch detected it and called emergency services.
Now, let's pause here for a second.
The reason his smartwatch was able to detect it is it had seen enough electrocardiogram, or EKG,
waveforms to understand when someone was having a heart attack and that he was very far outside the norm.
On top of that, the watch itself was specifically trained to detect these heart attacks after it received this massive amount of health data.
Original iWatches and various other devices were not able to perform this because they didn't have the data to be able to predict when this would occur.
But when you get a large enough data set, you can end up specifically creating predictions and understanding what was occurring with a given person.
But back to the story, the ambulance itself was 20 minutes away.
Now, during this time, his health data was also uploaded to a second app.
The second app was something for people who were prone to heart attacks that he had on his watch.
This app had a machine learning algorithm directly on the watch that specifically looked at his EKG waveforms and his heart rate and realized he was going to have, or at least was very likely to have, a second heart attack.
What it ended up doing was it ended up directly contacting the hospital, who sent out a helicopter to airlift him directly from the spot where he was as the GPS coordinates were also specifically uploaded.
Now, as this occurred, they ended up saving his life via, I believe it was the paddles, the shock paddles twice.
On top of that, they brought him in and he immediately got heart surgery and the surgeon did save his life.
The surgeon afterwards was specifically quoted as saying that time is muscle in terms of heart attacks.
And the longer that you are without treatment for a heart attack, the higher chance you have of dying and that this machine learning algorithm explicitly saved his life.
As if the ambulance was 20 minutes away and took another 20 minutes to get him to the hospital, he would have almost assuredly died.
So there's an example of how machine learning can not only save your life, but it's changing the lives every single day.
But what's a rudimentary example of machine learning?
Imagine a spreadsheet, an Excel sheet.
It's got rows and it's got columns.
Each one of the columns represents a different dimension of data.
And that's what we call it in machine learning dimensions, a different facet of the data.
And each row represents a different record or record of the data.
Now, what we would end up doing, and this is machine learning 101, is if we had, let's say 100,000 rows in this spreadsheet, we would feed 70,000 rows into a machine learning model,
such that it would end up creating a model for prediction.
It would look at these things and there's a tremendous amount of math that goes on in the back end for these various machine learning algorithms.
The math is mostly lambda calculus, some matrix calculus, and a whole ton of linear algebra.
Regardless, we feed it 70,000 lines of this spreadsheet and we hold back 30,000 lines from it.
We end up having the math do its thing and the algorithm do its thing.
And we say, I want you to generate what the next 30,000 lines of data should be.
And so it does that.
Then we feed the original 30,000 lines that we held back from it into the model.
And the model adjusts itself and says, okay, I got it.
Here's where I was off.
And you can end up tweaking your model continuously and getting a better and better model.
More dimensions of data does not necessarily mean a better model, nor does that matter less dimensions of data.
What it does mean is that, and by the way, that's called overfitting and underfitting when you have too many dimensions or too few dimensions.
The model itself is the output from the training initially, which is unique.
And it's unique because what you've ended up doing is you've trained a model against your data and then the output was a model to then predict the data coming inbound.
That's the opposite of how we normally program algorithms in the sense of we normally program explicit rules that a computer is supposed to follow when handling data.
In this way, it's here is the data already, predict what the data should be, tweak yourself to understand what the data that I've held back was and how far off you were.
And the output, therefore, is a model that will hopefully end up predicting data going forward.
And the model itself is just an extraordinarily complex algorithm specifically trained on the data that we fed it.
And there's machine learning in a nutshell.
So to recap, we've learned today about algorithms and we've learned some pretty amazing historical facts about them.
We've learned about artificial intelligence.
We've learned about machine learning and the fact that it impacts end users and even saves lives.
One of the most important things I think we can remember is that technology is here to support us, humans, not the other way around.
We are not slaves to the algorithm as many people often say today.
Quite the opposite, we are its master.
Our behavior determines its output.
So, any thoughts, comments, questions, concerns?
Hit me up on the social platforms at David McKay V or David McKay V if you're wanting the American pronunciation.
And that's it for today.
We have been standing on the shoulders of giants.
Thank you.
