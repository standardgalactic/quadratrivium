One idea which I think makes sense to a lot of people is that the brain is
constantly forming hypotheses about what's going on in the outside world.
But in order to take this idea of how the brain forms hypotheses and make that
concrete, really requires bringing together computational scientists
together with neuroscientists.
Music
The eye is kind of like a camera in that it uses many of the same optical principles,
but it's very unlike a camera in that the eye was designed for a very different purpose.
So for example, a camera is designed to produce beautiful photographs
that you can look at later, but the eye is not trying to do that.
It evolved for a much different purpose, and that is to acquire information
dynamically from the environment.
So there's a vast amount of data that's streamed out of the eye to the brain.
Most of this data is largely forgotten.
The minute you look away, all that essentially disappears.
And so the brain needs to figure out on the fly how to assimilate this information,
which parts to throw away, which parts to add to your model of the world.
And what it's trying to do with this data is not produce some kind of a beautiful
rendition of the scene or store it.
There's no hard disk drive in your brain kind of trying to store this information.
It needs to try to build a model of the world to guide actions.
That's what it's trying to do, not produce pretty pictures.
We can actually learn a lot about our own visual system
by looking at other visual systems throughout the animal kingdom.
One example is the jumping spider, which has eight eyes to explore its visual world.
It has two very high resolution eyes in the front that it uses to do pattern recognition.
It has eyes on the sides of its head, which have low resolution,
but it uses to detect things in the environment to look at.
The box jellyfish has 24 eyes that it uses to navigate through its aquatic environment.
And it's almost as though when nature is trying to build these different visual system,
it's just saying, let's throw some sensors on here and just gather lots of data about the world.
And brains have a way of figuring this out.
Just taking all the state of the streaming and figuring out how to assimilate it into a model
that can be used to guide action.
So I think the important lesson here is that there are some foundational principles
that we can really learn about studying visual systems that are much older than our own,
that are much simpler than our own,
before we even sort of ask questions about how do humans and other mammals see.
Probably one of the largest misconceptions that we have about visual perception
is that we're just experiencing the image on the back of our eye.
But that's definitely not the case because the data that comes in,
it's noisy, it's incomplete, it's unstable, it's non-uniformly sampled and so forth.
So everything we experience about the world largely has to be generated internally by the brain.
It's the brain having to take its expectations, its models of what it thinks the world is like,
combining that with data in order to generate these perceptions that we experience.
So in some sense, perception is really a process of controlled hallucination.
It's a story that we're making up.
It's largely correct hallucination.
It works most of the time.
And this is how we're going about building our perceptions of the world.
So in order to understand the process of image formation and sampling the eye,
we first have to look at the structure of the eye itself.
So what I'm going to do here is draw a picture of the orbit of the eye
with the lens in front and the retina behind.
The retina is basically a very thin hemisphere of tissue
which contains the rods and cones,
which are the photoreceptive elements which actually detect light at the back of the eye.
I'm just going to draw them as little sticks.
So when we look at an object in the world,
I'll just draw the object as an arrow out here,
the object then, the image is projected onto the back of the eye.
The shape of the object is then sensed by the photoreceptors,
but the brain does not actually get to see the output of the photoreceptors itself.
It does not get to see the raw retinal image.
Rather, there's a lot of neural circuitry within the retina,
and the output of all that retinal circuitry and computations
is fed to a set of cells called retinal ganglion cells.
Each of these retinal ganglion cells has a wire called an axon, which comes out of it,
and the collection of all these wires together then exit the back of the eye.
And it's this bundle of fibers coming out of the retinal ganglion cells,
which is a result of a highly processed image within the retina,
which is actually fed to the brain.
The retinal ganglion cells do not sample the image uniformly as in a camera.
Rather, they're packed very densely in the center and much more sparsely
as we move towards the periphery.
So the region of visual space for which you have high acuity is actually very small.
This means that in order to see the world in high detail,
we must constantly be moving our eyes.
For example, when looking at this woman's face,
one moves your eyes about the scene to specific locations like the eyes, the nose, and the mouth.
What's very interesting is that even though these movements are purposeful,
they're not just random scanning movements, you're largely unaware of them.
When you read a book, your eyes are also constantly in motion.
Of course, your eyes have to move from one word to the next across the text through the page.
And so you maybe hold your eyes briefly on one part of the text
and then you move to the next word and the next word and so forth,
in a kind of maybe jerky manner through the text.
Even when we think we're holding our eyes still,
such as when you're fixating an individual letter on the page of text,
the eye is still in motion.
These motions consist of two things.
One is a slow drift motion across the retina
and also corrective microcecades with your small eye movements,
which try to keep the object in the center of gaze.
We think these small eye movements are actually helpful to vision
and that they help to build a higher acuity representation of the object within the cortex.
As you walk through the world, your eyes are also constantly in motion.
You're looking at the ground plane, you're trying to figure out where to place your feet,
where to go, you're looking at maybe objects of interest and so forth,
but your eyes are constantly in motion.
And interestingly, your body is also in motion, of course,
and your head is bobbing up and down,
and so there's machinery, neural machinery,
and muscles that are trying to keep your eyes stabilized,
essentially trying to stabilize the retinal image
as you're going through all this motion as you move through the world.
Music
Now let's think about how this visual information is represented in the brain.
The information from the retina is essentially streamed here into the visual cortex,
but in the back of the brain.
This is the front of the brain and the eyes sort of lie right under here.
So what's interesting about this arrangement when the information comes into the cortex
is that it's essentially split in two.
You have two brains inside of you, a left hemisphere or left brain
and a right hemisphere or right brain.
The left brain is getting projections or information from the left half of each of the retina,
which in turn get information from the right visual field.
So information from the right half of visual space is going into your left brain.
The right brain gets information or projections from the right half of each retina,
which are in turn getting information from the left visual field.
So the left visual field is essentially getting projected into your right brain.
So it's kind of astonishing about this arrangement
as its entire visual field is split in two with one half going to one brain
and the other half going to the other brain.
But note that, as we mentioned before,
because of the highly non-uniform sampling of the retinal ganglion cells in the retina,
these images that are projected into the visual cortex are highly distorted images of the world.
Highly expanded here in the center of vision and highly compressed as you go into the periphery.
It's amazing that our brain can produce such a stable and geometrically correct percept of the world,
even though the image data coming into it is so highly unstable and highly distorted.
This may just look like a movie in the back of the brain,
but of course much more is going on.
Within the area V1, the primary visual cortex, there's computations that are happening,
which try to extract features of the visual scene,
and then they send on this information, the next visual area, so-called area V2.
V2 does some processing on that, sends it on to the next visual area,
and eventually this information percolates to these much higher level areas.
So there's basically a hierarchy of information processing
where low level areas in this hierarchy are processing information about the image, about image properties,
and neurons in higher level areas are trying to extract more abstract properties of the world
about objects and motion and so forth happening.
Music
One of the exciting ideas that's really emerging right now
is this idea of trying to develop computational theories based on the framework of Bayesian inference.
More than 200 years ago, the Reverend Thomas Bayes formulated the foundations of probabilistic inference,
what we today call Bayes' Rule.
What Bayes' Rule essentially tells us is how to update our belief in a certain hypothesis after we've seen some data.
So we can write that out as follows.
We write the probability of a certain hypothesis given some data, D.
The hypothesis H, in perception, is basically our belief about a state of the world,
whether we think a certain object of a certain reflectance, a certain surface, is out there in the world.
The data is basically the sensory information which are measurements about light, about sound intensity, and so forth
that we get from the sensors about what's going on in the world.
But the data does not directly tell us what's actually happening in the world.
This is an inference.
So the problem our brain has to solve is to figure out what's the most probable explanation for what's going on in the world.
What Bayes' Rule tells us is how to calculate these probabilities.
And what it says is you have to combine two different things.
One is your model for how the data would have been generated given a certain state of the world.
So given that you think there's a certain object out there in the world of some color or reflectance,
these are the activities you would expect to see on your sensory receptors.
And the second thing is your prior beliefs about the state of the world.
Some explanations are more probable than others and this comes from your vast experience with the world
or it may be something that you're just innately born with.
These two terms are also divided by another term called the probability of the data or the evidence.
Usually this just plays the role of a normalization constant so we can ignore it
because what we're trying to judge is the relative probability of different hypotheses given the data.
What people in computational neuroscience are trying to do is to understand how these kinds of computations
are implemented in the neural circuitry of the cortex.
In this example, the 3D scene in the shadows suggests that the square in the middle of the front face of the cube is orange
while the middle top square is brown.
If we mask the picture so that the context information is lost, then we see that they are exactly the same color.
So we can understand this percept in terms of the framework of Bayesian inference.
Let us consider two different hypotheses that the brain might consider for the scene.
Hypothesis one is that we're looking at a patch of orange that's viewed in the shadow.
Or low light condition.
And hypothesis two is that the patch is actually brown and viewed in the light.
So what does this term tell us about these two different hypotheses?
Well, both are actually equally consistent with the measurements.
These are the pixel intensities arriving at your retina.
And both of these different explanations are equally probable because an orange in the shadow or brown in the light
would give rise to the same intensities arriving at your eye.
So this term by itself does not really adjudicate between these two different hypotheses.
They're both equally likely and both highly probable explanations for the data.
But now let's consider the prior.
When looking at this scene, we have a three-dimensional context which suggests that the frontal face of the cube is immersed in a shadow.
And so if we go with the idea that there's a shadow there, the only way that that patch could be in the light
is if there's a specific light source just shining on that patch alone and nowhere else in the scene.
We judge that situation to be highly improbable.
And therefore, this idea of the generic lighting assumption that it would be bathed in uniform illumination due to a shadow
is judged to be much more probable.
And so therefore, when we multiply all these things together, this hypothesis H1 comes out to be much more probable.
Optical illusions give us a beautiful way of demonstrating this hallucination machinery of the brain.
We call them illusions or optical illusions, but that's just in the laboratory.
We can create these situations that fool you, but really this is something that's happening all the time.
All of perception is essentially a kind of illusion about the world that the brain forms, but a mostly correct one.
For example, consider the scene here.
When one initially looks at it, when you just see a collection of black and white sort of splotches in the scene,
because you don't really understand what's going on, it's created on purpose to be somewhat ambiguous.
But with a little bit of hint, look for a cow, immense amounts of detail begin to fill in.
If you can't quite see it, we'll provide you some more detail.
What's amazing about this process is that beyond just recognizing the figure in the scene or what the content it is,
there's immense amounts of three-dimensional detail and structure that's being filled in.
The shape of the cow's head, the texture of the cow.
You can separate the graininess as part of the photographic process as opposed to being part of the cow per se.
All of this is being created by the brain.
The initial ambiguity you had at the beginning when you just saw it as a bunch of black and white splotches and boundaries in the scene,
that's just the information being provided by the retina.
That's just the data that's actually being provided about the scene.
Everything else has to be filled in and created by the brain.
Wouldn't it be nice if we could look inside the brain and see what's happening when you see the cow in the scene?
So here's one example of a scene we've created where we can actually do that.
When one initially looks at this scene, one sees the lines which appear to be moving up and down.
But when we make a manipulation of the scene, we can see that something else is going on.
It's actually a diamond that's moving left and right, back and forth.
The reason why we had an incorrect percept initially is because the bars that were in front were hidden to be the same color as the background.
So what's interesting about this particular stimulus is that once you know it's a diamond,
we can go back to the case where the bars are hidden, and it turns out to be a bistable percept,
meaning that in one case you see it as a diamond moving left and right,
and in another case you see it as bars moving up and down.
And it tends to flip spontaneously on you about every 10 seconds or so.
So this makes an ideal stimulus for a functional magnetic resonance imaging experiment
where we can put the subject inside a scanner and have them press one button when they see the diamond,
and press another button when they see it as lines moving up and down.
When we look inside the brain, what we see is that initially when you see this stimulus as a bunch of lines moving up and down,
there's initially a lot of activity in the lower level areas.
This activity percolates up towards the higher level areas, but there's not much activity there in the higher level areas
because evidently there's not much being made of this particular stimulus.
But when the percept of a diamond kicks in, when you finally see it as a whole object moving back and forth,
you see a lot of activity now in the higher level areas, and interestingly much less activity in the lower level areas.
This suggests potentially that the feedback pathways are carrying the predictions of these higher level areas to lower level areas
and that's essentially explaining away the activity down there.
They're explaining away all this ambiguity that was potentially there before.
We're only just still scratching the surface and that the actual function of the visual system is something extremely mysterious.
Even something as simple as a spider or a bee is something that you just look at in awe.
Even given all these powerful mathematics and computers and everything that we have, we're still at a loss.
We're trying the best we can, but we need new tools, we need new theories, and that's where the Simon's Institute comes in,
like new theories of computation.
It's going to be some smart kid 20 years from now who has a new idea, then they'll figure it out and they're going to take us to the next level.
