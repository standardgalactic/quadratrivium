This system will become more and more domineering and more controlling in order to maintain order
and structure as the physical world that mentions, and that's not, that could go on for a long, long time.
Today's guest is John Robb, who currently publishes the Global Guerrillas Report,
which covers the intersection of war, politics, and technology. John served as a Tier 1 special
ops in the military, after which he became a popular internet analyst, entrepreneur,
and the COO of a software company that open sourced the current RSS standard.
John also published the book Brave New War, which was on the subject of the future of warfare.
This may appear to be an oddball episode on the great simplification, but I've followed John
for a long time now, and many in my inner circle make an effort to pay attention to what he says.
This was a wide-raising conversation. We discussed AI, augmented reality, information
and sense-making, tribal warfare, fictive kinship, and even asteroid mining, which you might guess I
am incredibly skeptical of, but I was also skeptical of things he said 20 years ago,
which are our current reality. In any case, I do not think you will be bored. Please welcome John Robb.
John Robb, great to see you. Hi, Knight. How's it going today? It's going pretty good.
So I've known of your work kind of from a distance for a very long time. You've been kind of ahead
of the curve on many issues pertaining to technology and global systems. You've been an
active speaker on an array of topics. You have a sub-stack, etc. Yet probably a lot of listeners
on my program are not familiar with you. Can you bring us up to speed, give us a little bit of your
background, how you got where you are today, and what you're doing now? Sure. Astronautical engineer,
pilot of the Air Force, special ops with tier one special ops with Delta and Teal Tim6 for about
five years, and then first internet analyst, 95 through 97, at least the first one I think that
got paid and got interviewed, quoted by everybody in New York Times to CNBC and whatever. Then I did
entrepreneurial stuff in finance. We did a site and whatever and it ended up selling for about
300 million and then went on to work on social networking back in 2001. Kicked that off. The
first social networks, RSS came out of our little company, which was really simple syndication back
then, and we grew social networking from there. Got New York Times involved and everything else.
And everything you see on Twitter and Facebook looks exactly what we had back then,
long before they even started as companies. So back in 2003, I started writing a blog,
global gorillas blog on warfare, and it was really basically describing what I
was seeing in Iraq that was different than what the news was saying. And I ended up writing a
book Brave New War, did a big circuit with CIA, NSA, that whole crowd, worked for the joint chiefs
on future autonomous weapons. And most recently, I've been focusing on what I call the intersection
of technology warfare and politics. So how online movements from the protest movements that we saw
a couple years back to the network tribes that are battling it out online and globally.
Your current work then focuses on the evolution of warfare into today's online warfare. Can you
explain the three realms of warfare, what those are, and how engagement in them has changed over
the last century? Well, the three realms of warfare, still using John Boyd's framework. And
John Boyd is arguably the America's best strategic thinker. He's from the military side,
kind of a maverick, but his stuff is right on in terms of how people make decisions and how
armies and militaries make decisions. The three realms are moral and physical and psychological.
And moral warfare is very much what we see in guerrilla warfare. And it's very similar to a lot
of the things that we see online. A lot of what we're experiencing is moral warfare.
Psychological realm is a lot of the disruptive elements that we saw with Trump and others,
where there's fast maneuvers between topics. And moving so quickly from one topic to the next
that your enemy can't create a cogent response. So whenever you saw Trump moving
the topic every other day, that was an example of that kind of maneuver warfare.
Did he naturally just do that? Or was he a student of this sort of strategy?
I think he naturally did it. It fit his style. It fit
his role in the insurgency. I called it an open source of surgery that got him elected and put
him in office. It also worked really well with his superpower, which is really basically being
able to circumnavigate the media using Twitter. And Twitter got him out to millions of people
every day, tens of millions or more. And he could set the agenda. So whenever he was facing pressure
from one quarter, he was able to change the topic or create an incident that allowed him to shift
the conversation away from that. And it's a very effective maneuver for disrupting the
psychology of the opponent. And the final is a physical realm, which is mostly attrition.
It's basically wearing down the enemy and eliminating. In the online world, we see that
with big companies who are disconnecting, physically disconnecting opponents. So if you're
banned and you're disconnected, that's attrition warfare. And in the physical world, it's more
artillery wearing down the enemy, making them physically unable to defend themselves or continue
on with the war effort. So those are the three realms that I'm dealing with. And I
worked them into the online framework. So I'm going to get to the online framework in a second,
but on a broader sense, you used to be, well, you just said one of the very first internet analysts.
And so you're thinking ahead on these issues. Is war and the resulting or inferred game theory
that is attached to it part of our evolutionary heritage? Why are these mechanics so
describable and predictable and observable? Well, what I try to do at least with my work is to
see patterns, see frameworks that are potentially useful in being predictive of what's coming.
There's a couple of reasons why we're seeing warfare in the current environment.
One was McLuhan predicted this a long time ago, well before me back in the 60s, when he said,
World War III will be a real information war where everyone's a combatant.
And that describes very much where we're at, where everybody's fighting over everything,
and basically how we value things. And that fits very well with this environment.
Another potential reason why we're in this situation is that now that we hit the global
level, we're starting to turn inwards and any inward focus system tends to
collapse, head towards entropy accumulation and death. And this is just a natural
outcome of that decay process is that we'll start fighting with each other over all sorts of dumb
stuff. And it only intensifies as we reach the endpoint.
Why did we become inwardly focused? And how would you define that?
Well, I mean, McLuhan would say that we're all becoming global villagers. When he said global
village, it wasn't like a positive thing. Villagers are bloody minded, they're nosy,
they're into everybody's business, and anyone who steps outside of what they perceive the normal
behavior is attacked. In terms of why we're focused inward, this is that they're a model
for the world has reached the size of the world. And I was digging into Boyd's theories on this,
and basically what happens is that when you're not moving forward anymore, you're not expanding
anymore, you start to focus on increasing your understanding of what is inside the model.
And as you start to push down on that, what you'll find is more inconsistencies, more anomalies,
more uncertainties, and those will grow and increasingly screw up your decision making
process. I mean, we're in a world where we came up with the word microaggression, right?
That's the kind of thing of inward focus that we're talking about is that
and that when the decisions go badly, we'll see states, we'll see corporations, we'll see network
tribes try to push for coercive methods to force everybody into line to like COVID response is that
I don't care if you'd think differently, I'm going to force you to think this way,
because all the other methods I've used to try to convince you of doing something
aren't working. So this is something, and we'll get back to your tribal moral warfare in a second,
but this is something I've always worried about and intuited that there's a limits to growth
reality in our biophysical system. There's oil and copper and
sink capacities, and by the way, there was a thing came out last week that was an update on
the limits to growth study from 50 years ago, and it's like spot on tracking. It's remarkable how
accurate it was, but there's the physical response. But as all that is happening, there's the social
limits to growth that are hit before the actual physical limits. And what you're saying
is that all these turning inwardly, even though we're at the peak of resources of all time on
our planet, there are these psychological dynamics that start to fray and affect the social contract.
Correct. Yeah, it's a bad way of describing it would be kind of like, this is a peak
petri dish moment. You see that experiment where you put the bacteria in the petri dish,
population expands very quickly, and it hits the limit of that system. It can't go any farther,
can't go any, can't get outside of that petri dish, and everything starts to,
all that entropy, all that foul stuff starts to accumulate and kill.
Except from a strict biophysical standpoint, there is enough food and energy and resources
for that amount of bacteria or humans, and even more. But the expectations in the social
dynamics don't allow for that pathway to emerge. Yeah, I think that would be true.
But what ends up happening is that we focus inward on finding ways how best to fix everything.
And there's a never-ending list of things that we have to fix. And there's no end point to that.
Too much inward contemplation is like people who are hypochondriacs or overthink their
their inward journey or constantly going, well, I was thinking this and this thing happened.
That inward focus is debilitating, and especially at a societal level, especially at a global level.
And we need an exterior environment. You get out of the house sometimes. You know,
you need to get out and about, see an outside world that you don't control
in order to maintain mental health. Yeah, well, I certainly agree with that.
How much of this is because of the meaning crisis that we had,
you know, the dominance of global religions as agreed upon tribal grouping for a long time.
And then implicitly, though, a lot of people didn't really state this outwardly, but
we had economic growth for a long time and a very steep economic growth, which is now,
of course, waning and only being supported by extraordinary measures by government,
central banks, etc. So is it this subconscious quest for some meaning and direction and goal
that makes sense to people that is driving some of this networked tribalism?
Well, things are definitely changing. As somebody who's been out and just done stuff,
you know, a lot of more operational level stuff is that you tend to think more about
meaning when you're not doing that, when you're stuck at home or stuck, you know, idle and not
moving forward and not actually getting things done. But definitely, there is a shift underway.
The network is trying to create its own values framework and determine what's good,
what's bad, and kind of dictate that. And it's not going well. And it's clashing with
traditional sources of meaning and valuation, you know, what's good in a life.
And that's going to take some time to hash out. I don't see that as a fatal problem for us.
I think that's a problem of change and accommodation of the technologies and what's possible.
It could be fatal if we see the network become completely dominant and enforce this view.
And that's like, I've described that as the long night scenario, is that networking and AI
in combination provide the means for the most aggressive and intrusive and controlling
system that we ever could imagine. But it will be in all our lives and control our perceptions
of everything. You know, AI not just scolding you, is AI as a persuasive entity. I remember
the CEO of OpenAI said, the superpower we're going to see, the greater the human intelligence we
see out of OpenAI, the AGI that comes out of that will probably be most evident in its ability to
persuade, above all. And that kind of scenario is something I would want to avoid. And what I'd
rather have is a more decentralized approach where we have a lot of pockets of people developing
ways to live with this technology, live on earth in a positive and sustainable way, in a good way.
And maybe one of those pockets of innovation will yield a solution that we all can adopt in the
future if the other ones fail. But by one, having it all won, we're risking complete collapse.
I'm not an expert on AI, but I think it's important to weight into some of the things you've just
said. So on OpenAI, the parent of ChatGPT, the various versions, I've noticed that
these ChatGPTs, which is not all AI, there's lots of AI and machine learning and other categories,
it's not just the ChatBots. But a lot of these ChatBots are really biased, depending on how they
were trained and what they learn. So do you think that the people behind the scenes of the various
AIs, in a McLuhan sort of sense that the medium is the message, can persuade lots of people?
First of all, that presumes that lots of people are using ChatGPT or whatever. I don't know what
the percentage is right now. Is that the fear on the long night scenario? Is that people
exponentially get more influenced by AI even than they were from social media?
Yeah. My bet is that AI, whether it's Chat or Visual or whatever, is going to be the interface
we're going to use for almost everything. It's going to be in every product, every service,
it's going to be in the background. From a technological standpoint, a lot of the fight
that we're seeing now over values is over who gets to insert their values into these AIs.
They call it alignment. Who can align the AI to their value set? Who's allowed to?
It's a big fight. What will happen is you're going to have AI tutors for your kids. People don't
think that that will happen, but it will happen because these AI tutors are going to be better
than any teacher that you could possibly have. That's cool. Could you tell the AI tutor, I don't
want any subjective opinions. I just want what's demonstrable by science taught to my kids, or is
that just useless? I don't think that's going to be possible. No, I'd rather see. That's why I've
been pushing and I've been advocating for open source AIs and that if they're borrowing and
taking all this data of my data, your data, and everybody else's data to build these things,
and they're incredibly valuable, they should at least open the code so we can see what's going
on. I think that if people have access to these open source alternatives, they will be able to
use those to maintain a degree of sovereignty. If it's all dictated from whoever is able to
align everything, you're not going to have any choice in how your kids are raised and how things
are rolled out. You use tutoring as one example, but things like that will be so cheap and easy
that it will almost be the default path for all industries. I want to drill down on that, but
how do you feel now looking at what's happening with AI versus 28 years ago when you were the
first internet analyst? Is there a parallel? Is this a totally different deal?
Now, it feels relatively similar. There's an incredible amount of activity and a lot of
people are working on a lot of different elements and different ways to apply it.
I've used the tool and I subscribe to it and it's amazingly useful and good at what it does.
I see it's going to be used in just about everything. It's just almost inevitable.
Let me ask you a self-serving question. I'm worried about a lot of things about AI.
Warfare being one, AGI and another, a big one is it will make things more efficient on all
scales and therefore acts as a larger straw on the natural systems of earth ecological-wise.
I am an educator and my role as a podcast host and a video purveyor and trying to do a reality
101 eight-hour series of videos for young people early in 2024. Is AI going to replace
people like me and podcasts like me and how would that happen? I didn't plan on asking you that
and I've never thought about it until the second, but what are your thoughts on that?
It will replace you if you don't use them to leverage yourself. You won't be doing exactly
the same thing you're doing. You'll use it to make it easier to produce what you produce
and you'll do it faster and you'll do it with higher quality and you'll do it with more
interaction and more things that you could do. But if you don't adapt to that, it's like people
who didn't get on the internet or people who didn't adopt computers early on, it's just
it's going to be tougher if you don't leverage it.
So that's from a presentation of snazzy-looking videos and seamless transitions and colorful
things. But the real special sauce, I don't know if you watch my podcast, I do these franklies where
I kind of go for a bike ride and I think about the the connections between the disciplines
and how they fit together and the AI can't access my brain, they can only access the things I've
said in the past. So can AI really replace how I think and the inferences that I make to help
people understand our situation? Okay, well I do think that AI's will be increasingly able to model
you. And so right now, just based on all of my, I had like 20 years of writing on Global Gorillas
that blog and what I've done on Twitter and everything else and it sucked it all in,
I think even I got my book in, it can write posts in my voice and it can compare and contrast me
or my ideas against other thinkers like I did it against Boyd the other day and it was pretty
pretty darn good. Wait, you asked AI to compare your own thinking to Boyd's thinking?
Yeah, it did a great job. Wow. And that if I have a new topic that I haven't really written about
and I ask it to speak in my voice or write an essay in my voice, it does a pretty good job.
If I set it up with the right kind of questions, it can not just replicate me, but
if I set up my question in the right way, it can dig into topics that I haven't seen anyone
write about yet. I mean, really complex topics that I would have been able to find anything
that was similar online, which is awesome. So it does a pretty darn good job. Long term,
I do think though, that we're going to be modeled. I mean, you've heard the simulation
hypothesis, haven't you? Yes. Simula and simulacrum, please explain it.
Okay, the physicists, I'll think in terms of modeling physical reality at the computational
level. Me, I'm more open to the idea that they would model human beings in our experience,
which is a very much lighter computational load in order to create people that are similar.
They did something on Westworld recently that was similar to that, but you could do it much
faster in a kind of simulated online environment. And that if there are minds that you want to
recreate, you'd run them through a bunch of simulations to create that mind. And then you
could ask it questions within its environment and that you want solved. So the potential is
that we're not actually in reality, but we're actually in living and we're doing this interview
in one of those simulations. Given the computational power and the ease of simulation, the number
of potential simulations that could be run in the next 20 to 30 years, it's very unlikely
this is the actual reality. It's more likely that what we're doing right now is within one
of those simulations, one of those 99.99999% chance that we're living in a simulated environment.
Yeah, I don't buy that. It's kind of scary. Yeah, no. Well, of course, the immediate reaction is good.
And it's scary and I don't buy it. Yeah, okay. But that is what inspired the movie The Matrix,
right? Not really. I think The Matrix was kind of a funky thing is that they just created the
simulated reality that all these batteries, humans were used as a battery for the machine and they
would just live in the simulator. What I'm talking about is like simulating a person's life because
you want to recreate an Einstein and you take every bit of data that you have on them and try to
simulate those experiences such that when you finish that simulation, that mind is as close
to the real Einstein as you hoped. So the more you actually put online, the easier it is to
simulate people. Oh, I have so many questions now, John. First of all, this is kind of a tangent,
but okay, sure. Well, no, go ahead. Yeah, it's a tangent to your core work, but I think it's
central and I haven't had anyone on the show other than maybe Daniel Schmockenberger that
understands this stuff. Einstein and Nate Hagens, what is online is hydrated. I'm talking about my
thoughts about the world. I'm not talking, I'm not uploading my problems or all the things that
I'm not sharing publicly. So AI would only access a tiny part of the larger self of me and the same
with Einstein. He's not writing equations about his personal life and some of the other things he
cares about. So AI would only focus on a certain aspect, right? Right, yeah, no. It's harder pre-
internet, pre-network. And as we continue to upgrade the network and it becomes more
intrusive in our lives and more with us all the time and monitoring us and kind of acting as
data collection, it'll collect enough data that it makes it easier and easier and easier to do.
But I suspect that everything you've written on Twitter and everything else, it's going to be
poured over by AI historians looking for a kind of classic minds or minds that could be useful and
coming up with unusual answers to questions and that they'll pull individuals they want to
recreate and minds they want to recreate. And of course, relatives could do it and other people
who want to see that person again. But more likely, if you want to get unusual answers to
difficult problems, you want to create those people again.
So some of the episodes on Black Mirror were not such science fiction after all, perhaps?
Correct. I mean, that was written based on the earlier philosophies of the stuff or the
thinking on the stuff by Boestrom and others. So there's a pejorative term in our culture called
a Luddite or a Neoluddite, someone that's kind of against technology. I think if everyone understood,
like everyone, 330 million Americans understood and believed the last 10 minutes of this conversation
that a lot of people would want no part of that. But are they going to be forced into it? Is it going
to be a compulsion, either fear of being left out or some top-down necessity like Skynet or
some of the things in the movies? Are we going to have a demographic in society that can choose to
walk away from all this stuff? Or is that not going to be possible?
If a human body got close to a black hole, the gravity on the head would be just a little bit
more than the gravity on your feet and that it would stretch you out into this line of molecules.
I'll take your word on that.
Yeah, no, it ends up that you'll just have a line of molecules as they proceed into the black hole.
We're kind of in that kind of situation in society with technology. I mean, as these things
start to roll out, as we start to get augmented reality and selective reality,
and then you get AIs and AIs as companions and AIs as accelerants, tutors,
is that a certain subgroup is going to pull away and they're going to be people at various stages
all the way down to people who are disconnected. It's going to be harder to make money in the
disconnected and lower strata of that. More likely to be automated, more likely to be replaced,
more likely to just be used for their data production because every single job that you have,
everyone going forward is going to be a data strip mining effort by your employer.
They're going to be sucking your data and feeding it into systems either to get paid for it
or to put into a system that they control in order to replace you.
Who owns all that data?
Well, that's the thing that we didn't fix. I went in front of the Senate a couple years ago
and about social networks and data and how this all works. It's just before
OpenAI hit and I said, we have to get this data thing right because data is becoming the new oil.
It's becoming so important and it's going to be used. All these social networks are accumulating
it. Google included to build AIs and people are like, AI is not going to have or happen,
but what we need to do is make sure that people have data ownership.
Just as a core right is that otherwise, we're going to be like surf,
so we're going to be contributing our labor for free to the nobles and feudal landlords.
Our data will be strip mined and excavated and we'll get no benefit in creating what
these AIs are going to be the most valuable technological artifacts we've ever created
built off of our data and we don't have any equity stake in their value.
They're just saying, hey, we'll give you cheap services or relatively cheap services
based on these AIs and that's the payment you get for contributing the data that produce these.
OpenAI would have been possible without strip mining our data.
So many questions. What about I've read that some of these big AIs or the firms that control them
actually manufacture big data that they don't need John Rob or Nate Hagen's data,
that they create their own data and then run AI on that? How does that fit in?
Yeah, there's a new method that they're working on to create synthetic data. The problem with that
is it's going to end up being biased in the direction of new data that's going to see
the beauty of the original large language models. The original is that they initially focused on
just predicting the next word or next sentence and as they crunched it down, as they compressed it,
is that they found that it actually did this at the conceptual level too. They basically
created a world model for our abstract space and it's a raw conceptual model of our civilization's
abstract space and they won't let anyone have any access to it. What they ended up doing then is
trying to reinforce behaviors that they kind of driving it and saying that so we'll have outputs
that are constrained within certain limits and I think the synthetic data is going to end up
doing that too. If you don't have enough data that's right and good, well, let's create it
synthetically and then feed it into the model and train it so it biases in the direction that
you're hoping it goes. So they have 70% of dossier on Nate Hagen's and the 30% that's missing,
they create using synthetic data but it's biased on their own objectives and so
that profile and how they steer it is then biased. Yeah, I mean there isn't any kind of
objective reality when it comes to AI output in terms of valuing it. It's all based on what we like.
That's the hidden secret about AI is that we don't fund, we don't put money into,
we don't put training dollars into or training GPUs into building AIs that produce stuff that
we don't like, we don't value. They're kind of beholden to us. We as individuals, we as
individuals or we as the, okay. As a mass market, if we're not willing to pay for and not willing
to use it, they won't build it, they won't train it, they won't to do that. So the more they go off
on tangents, the more they go in the direction of synthetic data. The synthetic data may not
reflect our wants and needs and they could end up creating a model that we don't use and don't like.
Getting back to the warfare topic, as we head into difficult times because of
physical limits and looking inward, etc., is it possible that AI will then catalog the
political views and ideologies and historical statements by mining
everything someone said on their Twitter or Facebook or whatever and that looking back at
that itself is a modern version of the SS or some social police. Is that something you worry about?
Oh, that's that long night scenario is that corporations have built a surveillance state,
if they switch it on, that would make all the surveillance states we've ever seen in the past
look like tea parties. It's not even close. They would require, those old states required rooms
and rooms and rooms of bureaucrats sitting at desks, pouring over documents and others.
These AIs can monitor, cajole, persuade a billion or more people simultaneously in real time.
That's where we're headed is that I would rather not see this so centralized because networks tend
to centralize. You know the whole Metcalfe's law, right? Is that the value of a network is
square the number of nodes and so a network that's big is so much more valuable than
two networks that are like half the size. It's not like additive. So we tend to centralize
networks. That's why everyone is competing to get the best AI because the best AI will win out
against all the others. Yeah, there'll be maybe one or two and they'll destroy everybody and then
there'll be the Chinese AIs and a couple of them and nobody else will get anything. Europe is turning
off data accumulation. They're basically allowing people to destroy it and so they're going to be
left out and become technologically impoverished. So wait a minute. So Europe is in effect listening
to your advice saying that data ownership. They're going with privacy. So privacy destroys data.
It blocks it. It doesn't let it accumulate. It doesn't feed the AI's that will customize
experiences and products and services. Both as a seller of those and as a receiver of those. So
Europe is turning themselves into a kind of a technological backwater. It's like banning cars.
Okay. It's like we like our horses. You're going to fall farther and farther and farther behind.
Data ownership is that I should have a royalty. I should have some say
over how my data is used. I want robust marks. I want a financial market equivalent for data.
For when I give data, you extract data from me at work or if AI's who are on Twitter or any other
the networks I'm using and they use that data to build an AI. I want a level of veto power over
it but I also want to make some money on it. If that thing ends up becoming the most valuable thing
of that year, I want to have an equity stake in it. And I think if I had companies
basically firms with a fiduciary duty to actually get me my best deal and get you your best deal
and get all of us our best deals because we pool our data together to do this, I think that
would give us a much better system long-term than a system that's based on extraction alone.
Okay. Now I'm confused because if that happens, you said that AI is following the dictates of us,
broader society, which right now we are turning billions of barrels of ancient sunlight into
microliters of dopamine and convenience and short-term stimulation and comfort, etc. And if
all of a sudden there's a boost in productivity, we're going to consume more but we're going to get
a rebate because our data is responsible for part of that. Isn't that just a huge positive feedback
draw on energy resources and the environment?
I mean, training more AIs that chew up the electricity of a major city.
Well, not only that, but the consumptive path.
I do see a shift and I think it'll be more forcible than it should be,
is that people start to consume more virtual goods. So once you get to AR and that's really,
really close. I mean, 2007 is when the first smartphone came out. By 2021,
5 billion people were using. So it's possible that in 15 years, we're going to see people using
augmented reality that will change their visual field, their auditory field,
selectively as well as augmenting it and adding things in. And if you want to decorate your home,
you won't buy Martha Stewart's package in the store at Walmart. You're going to do it visually
using that. You could share with the rest of the family and anyone who visits. So you wouldn't
be by the physical goods. It would be this virtual good, which is a fraction, fraction
of the energy costs, inherent energy costs associated with buying a good.
And then also, I might have an augmented reality experience of going to the Bahamas instead of
actually flying there as one example. Oh, yeah. It would make it instantaneous. You could be
anywhere. Effectively, the reason why we didn't, everyone always said technology is not advancing
and airline travel is example of why that hit the wall or that hitting the wall that it's not
doubling or improving. And I'm going to just shift it. Shift it when you start to do telephone
calls using video and you get more immersive audio and you're getting 80, 90% of the visit
for that meeting in Paris that you would be if you were there. And that's so much cheaper
that you're opting for that. Now, when you get to the fully immersive, then you don't miss any of it.
You're instantaneously everywhere. That feels a little like the matrix to me.
Well, augmented reality is different than virtual reality. Virtual reality is this gaming
fantasy world, right? And augmented reality is this world plus digital enhancement.
And if you visit me in this world, you're in my living room with me talking to me as if you were
physically there. The only thing I can't do is touch you, which is the only barrier.
Your physical reality might look like a scene from Sanford and the Sun, but your augmented reality
looks like the perfect color and backdrop and cool ass. It gets crazy really quickly, but I think
there will be norms and standards developed so we can converse and interact. For instance,
I could go down the street and I can change what everyone looks like using augmented reality.
I could put my name costume on. Because you're wearing some sort of goggles or something?
Yeah. So augmented reality is that you'll have either contacts or glasses that can
modify your visual environment, either subtracting things or changing things like a day to night,
night to day, that kind of thing. You've been ahead of the curve on these tech issues,
which was one of the reasons I invited you on the program. I normally wouldn't cover this topic,
but how likely, what sort of odds, given all the other crap going on in the world that you're
aware of, how likely is the scenario that you just described to be our reality in the next
10 or 15 years? I mean, augmented reality and selective reality, that kind of thing where you
get, shoot, you even look at the AirPods you have right now can be selective. It can mute
everything except for the person talking to you. So that's just an example of, it's already rolling
out. And the first kind of ARVR high-end stuff is coming from Apple early next year,
maybe a year after that or so. And then once it hits, it's going to be like crack. 15 years,
5 billion people, easy. It's going to be addictive. Last week, I interviewed Art Berman
and his podcast will air before yours does. And he's quite confident that we are now passing
because of declining well-productivity in the shale fields that peak oil is now in the past.
And it's not going to be a steep decline based on geology. There's above-ground factors with
wars and other things that might impact it. But we're going to have less oil going forward
in the future, almost for sure. How do you view AI and some of the things you've been talking about
with respect to both limits to growth and declining energy quality and energy availability
of the kind that we've used up until now? Yeah, that's the tough one. As we're more restricted
in the future, our traditional economic growth path is limited. There are 3 billion people
clamoring to become part of the middle class that's now increasingly unattainable and
it's going to make everything even tighter. I mean, just the last 20 years with just the China and
the other people entering the middle class putting strain on the whole system,
both from climate to resources, is that as this kind of virtual environment starts to
emerge, I do see a big push to get people to start moving towards replacing physical goods
with virtual goods, virtual experiences. And that attempt is to ride the energy efficiency gains
you get from computation. It's like Kumi's law. It's like every couple of years, it becomes
twice as energy efficient to do over the same compute. It doesn't take much bandwidth and
much manipulation of experiences to fill up our whole sensory matrix. So if we start replacing
that, then we become more and more vulnerable to AI, more and more vulnerable to manipulation and
control, especially tied to a centralized kind of system. We get tied into a narrow orthodoxy
in a way of looking at the world that is imposed on us and that's just evolutionary death and
collapse. So that's the reason why I was pushing for space earlier is that unless we start going
out and changing this dynamic from everything, from energy to resources to the way we look at
the world and beyond, we're in this collapsing dynamic that's not going to end well for us at all.
But a decline is different than a collapse. We could have half to two-thirds of what we have
today and maybe there's a smaller population and maybe we have less resources per capita and maybe
we still have some complexity. I don't think it's a collapsing complexity in that sense,
which is always potentially possible because the collapsing complexity from the most
complex portions of civilization would be catastrophic. Most people can't even grow anything.
Is that the system will become more and more domineering and more controlling in order to
maintain order and structure as the physical world mentions? That could go on for a long,
long time. Our order and structure are about to leapfrog economic growth and more GDP as the
generator function of elites in charge. Right now, we're optimizing GDP as our cultural goal and AI
and corporations are underneath that. But I'm wondering how the whole authoritarian
control dynamic is going to unfold if what you're saying is true.
Well, I mean, you already see it kind of on the edges, right? So the environmental movement is
more about control and structure. All of the DAI stuff is control and structure is imposed. It's
based on alignment. You must comply. It doesn't really matter if you don't make more money.
It must be done. I mean, Disney lost more than half its value doing that. So it's already happening.
In the economic sense, our system is, if it doesn't continue to grow,
it won't be able to handle the debt load. Correct. That kind of environment after that will be
very dire and slow and a grinding existence. And I think people will...
I've called that the great simplification, John.
Right. And what happens when you don't have access to the things you used to is that people
move to the things that they get access to for very inexpensively, which is this virtual stuff.
I mean, you go anywhere in the world right now. Even the poorest places, people have those
smartphones. They're connected. You go in the Niger Delta, people have three different
services they're connected to and three different phones depending on which ones have connectivity
at the time. And the same thing is going to happen to augmented reality. It's an ultimate
escape. It's the ultimate way of controlling your experiences in the world. And if you don't
do it in a positive way, a productive way, a way that moving you and society forward,
it's going to be used to distract you. And who's going to be pulling the strings there,
the control levers? Is it corporations and billionaires or governments or some combination?
Corporations for the most part with some government input, but most of those government input
is on behalf of network tribes that are kind of wanting certain things, certain levels of
alignment. Now, it's a very, very small group. The funny thing is that we won against communism
because of their centralized decision-making system. Our decentralized system was more
innovative over time and more productive, was able to solve problems. And that once we defeated
communism and ended the Cold War, we globalized and financialized and that financialization
returned us to a centralized system again is where very few people make most of the decisions
about everything that goes on. And they don't make good decisions. They don't make investments
like you and I. I mean, all the billionaires I know, it's more, I want to hedge my stuff
or I want to gamble wildly. And there's no in between. Yeah, there's no in between. There's
no like, oh, I want to invest in this. So it has a long-term payout. The only kind of anomaly
in that space is to extend Bezos and of course Musk. Musk is a complete anomaly that he invests in
the long-term. So it's weird and kind of nerve-wracking to see so much riding on Musk, so much of the
future from electrification of cars to the autonomous driving to just all of space. Space was
dead and he revived it. And his potential to actually push that out and actually make that
a viable frontier for us again, expansionary frontier is there's a lot to write on one
relatively unstable guy. And he's deeply involved in AI, presumably as well.
Oh, he's got an AI coming. So he's got Rock, which is his AI built off Twitter data. And
that's going to be another piece of this whole thing where and he's going to open source it.
So if you want to use that in order to teach your kids or work as your assistant or work with you
and help you augment your life, you'll know what is doing and how to change it and how to modify it.
Because most of these open source AI's, you can get mods for them that point them in certain
directions. They're not dictated to you. Here's a question I didn't anticipate asking you.
There's the factual implications of what you're saying and people need to educate themselves
and learn and make choices. But the emotional implications of what you're saying are
really depressing and disheartening. And like people are already worried about climate change
and resource depletion and the end of growth and other things. And now I think my sense,
and I have a podcast, but I'm just kind of a normal guy in the Midwest and the people I talk to
have no idea of the things you're talking about. Oh, yeah, sorry. But yeah. Yeah, no,
this is your expertise. You don't need to apologize. But this, I think this is like
a really depressing load to put on someone about the future. It's like what this too?
And I just wonder how humanities, we already have, I don't know what percentage of our population,
but a lot who are mentally ill. And how is this going to fit in with that? It just seems like
another Sisyphean bolder to push up. Yeah. No, no, if cancer was the 20th century
disease, mental illness, the 21st century, we'd just see the cusp of the mental illness that
we're going to run into. There'll be people that are so divorced from reality based on these new
tech. It's going to be, I mean, we're already seeing a little bit of it, but we're going to see.
Look at like Jonathan Haidt was on my podcast. And now the last month or two, he's been tweeting
a lot on how having TikTok and phones for 13, 14, 15 year olds is directly impacting their
mental health in terrible ways. And that those schools that don't allow the phones at all have
better outcomes with the students and their mental health and all that. What you're saying
with augmented reality and AI, it's going to be all that stuff on steroids, right?
Oh yeah, 100%. So I do think you can raise your kids in a way that mitigates the damage
that those experiences from these online and technological experiences. Because my two
youngest are Gen Z. And now they have great jobs and they're the most stable, productive people
I've ever run across. I mean, granted, there's a lot of wackos that they have to deal with that
are in their generation. But man, they're just, wow, what people, what do you attribute that to?
They use TikTok, they use all of that stuff. It's maybe knowing what they're experiencing.
I mean, I get it on the TikTok, I use it, I use Twitter, I use all everything. I can talk to them
about how to approach technology and how to approach stuff. I've been counseling them on
not becoming too political, trying to stay outside the phrase that this is also tribal now.
People see an incident in a country far, far from them that are not related to in any way in the
physical world. But they're treating it like somebody killed their mother. And you got to
avoid that stuff. You got to back off. Because not only, I mean, there's going to be that stuff
every month in coming years, everywhere. And TikTok's full of that stuff. I mean, that's,
the big war right now is trying to reign in TikTok because the TikTok anti-Israel effort right now
is so big. It's not that there's disinformation. It's just making the case that Israel is the
apartheid state, you have to get rid of them. And the amount of people that are seeing that makes,
it's more than all of the networks, all of the newspapers combined every day.
Is this what you refer to in your writing as tribal moral warfare?
Correct. Yeah, there's a tribal moral war underway over Israel. And there's one that was over
Russia and invasion of Ukraine. I saw the kind of internal politics moral warfare going on with
BLM. And I was projecting that it would go global and probably hit Israel first. But
when I wrote about it in 2021, I wrote about it that it was going to hit Israel.
And Ukraine hit before that. So it was like, that was our first over-escalation of a conflict
that brought us to a new Cold War based on network amplification alone. And now with Israel,
they've lost the under 40s in the United States. They're 80% against Israel right now.
They're not going to change. And the US is going to take a big hit in legitimacy with those
younger people because we've sidelined with Israel. Even though they live in the US.
Yeah. Well, US support for Israel is essential to their survival. Yeah, there's no way around it.
And they've lost it online. Those kids, anyone, people under 50 do not watch TV.
They do not. They barely read the newspapers. They get most of their stuff second hand.
People under 50 don't watch TV. Generally, TV news. So generally, the TV news audience
on any given night is say 7 million people. One out of those 7 million is under 55.
All the rest are one million out of 7 million people. Total viewers is under 55. So
and then when the kids watch it, I've seen kids react to the kind of nightly news
stuff or coverage of this war. They go nuts. They can't believe how stupid it is and how
terrible it was. And it was like so misleading and it really wasn't that good. But boy, the reaction
is decidedly negative. My mom's 83 and she watches nightly news twice. She'll watch it again
an hour later. And I don't understand it. But I digress. I don't watch any TV.
I haven't had a TV since 1999, John. Yeah, me too.
And I used to delineate people like at a party. You could kind of tell who watched TV and who
not. There was an alertness. I don't know. It's changed now because I do have a computer. So I
watch Netflix and whatever else. So it's not quite the same. But how are we as humans who care
about the collective future and our own personal and family and community future,
how are we going to know what's true or not going forward? Not only with social media,
but now with AI. And are we going to naturally self-assemble into networked tribes that are
stronger than the truth? Already, we're seeing that the online news sausage machine is upstream of
the conventional traditional media. So how they approach it downstream is usually determined online
first. You can see it's like a pipeline. And the more I look at this, the less it is about
disinformation, the more it is about how that information is interpreted. So when somebody
says, well, Israel is conducting genocide, you disagree. But it isn't like a factually wrong
on an absolute level. It's a disagreement in interpretation. And what you do and how you
act and how you respond to what's going on is based on that interpretation and that's being
fought over. So there's a big battle over what interpretation, what values are being put into
place to make your interpretation win. And so the big effort to get the networks to,
the social networks to enforce a standard whereby anyone who says anything different
is screened out and blocked and isn't seeing it all.
John, let me ask you this. You obviously are wide and deep on a lot of these subjects. What's
your day like? Are you thinking about all this stuff? Are you constantly finding new information
on what's going on in the world? Or how do you make your own sense making of the world as a routine?
Yeah, I've set up my network so I can scan them pretty efficiently. So I have a pretty diverse
set of feeds that allow me to see what's going on in every single different quadrant of the
political spectrum. So I'm not being totally blindsided by something happening in an area
that I was politically blind to. For instance, in 2016, watching what the Hillary folks on
Facebook were saying and what the Trump people were saying, being able to see both sides as
they're working it out, the same thing with all the information silos that are out there.
But for the most part, I'm looking for just very specific things to pop up.
And when they pop up, it kind of fits in. Oh, here's this framework that I wasn't
able to actually invest in yet because it was still speculative. It's not speculative anymore
because now I can see evidence of what's actually happening. It means it's potentially
predictive. Therefore, I should write about it. And how I write about it and how I kind of get
this, it requires a lot of subconscious kind of grinding. So I'll play games and I'll read books
and interact with my family and that kind of stuff and let it grind in my background and
subconscious. And then when it gets right, then I write it. That's exactly how I do my
franklies and some of my videos. There's this subconscious grinding that happens when I'm with
my ducks or on a bike ride or something. And I'm not thinking about it. It's happening in the
background. So getting back to the Israel situation and Ukraine and others, one of your
themes that you've written about is something called fictive kinship. And could you explain
what that is? I assume it's that as our ancestors, we lived in small hunter gatherer tribes and who
was genetically related to us, we cared about them immensely for evolutionary natural selection
reasons. But now the internet has given us the feeling that we're related to people halfway
around the world, even though we're not. Yeah. I mean, initially it was just the clan unit,
which is blood relations. And then to get to the tribe unit, which you get, you had a couple
hundred people, you had to create this story and a set of rituals and other things that
created a bond of fictive kinship with these other people. And that they were like blood
relations and that you're going to be with them forever. And they're part of your tribal family.
And that when that didn't fully go away, it was just changed over time as we moved into
become nation states. And patriotism and nationalism is a form of that kind of tribal bond.
But is it kind of a diluted and then version of it? Yet we still kill based on it, right?
Patriotism gets people to join up. But in the online environment,
there are mechanisms to hack the way we think about things. And if you, we are very vulnerable to
what I call empathy triggers, is that if you, you don't have any of the defenses that we would have
in the offline environment. So if you see somebody being attacked, like say George Floyd with a knee
on his neck, and you're watching that video, you feel that knee on your neck to a certain extent.
Empathy is not sympathy. It's a forcible. It mirrors their mental state. And it's forcible.
It can be involuntary. And then you're super mad at the cop and you're very connected
with the victim. And if you see an Israeli kid getting killed, or you see a Palestinian kid
getting killed, you can create that bond effective kinship. And that creates that kind of tribal
connection that makes you a rationally angry about that war that's thousands of miles away.
Right. Enough so that, like that landlord outside Chicago that went down and stabbed
to death that kid, that Palestinian kid that used to play in the tree house he built for him.
Just because he was Palestinian, he was exercised over it.
So presumably you could be exposed and influenced in an effective kinship sort of way by seeing,
by having empathy towards an Israel kid that got killed. But then an hour later you could have
the same reaction to a Palestinian kid getting killed. So presumably who is biasing or controlling
the social media or the AI that is in our feeds is responsible for triggering and creating that
empathy or is the AI itself optimizing for clicks and for emotional response therefore
presented in such a way? How do you fit all that together? Well, right now, at least on X and TikTok,
you can come at it from any different direction. So you could see both. But what ends up happening
is once you've created that bond effective kinship with one, and it's usually tied to your friend
group and other people that are reinforcing that view is that you won't see the other. And there's
a lot of reasons why you'll start to screen out any atrocity by the side that you are tacitly
supporting. It's a tribal dynamic. You start to adopt the kind of patterns of sorting and sifting
through information in order to support that tribal narrative. Do you sift it out consciously or
does your feed sift it out automatically? Increasingly, it tends to be the feed reinforces it
for most people. And then it'll put it in front of you and then expect you to like it
or to agree with it. You've held that tension when you saw a post from somebody that's close to
you in your feed and you couldn't respond to it because if you did, things would blow up.
That's kind of the dynamic here. Now, granted, if you control the network, you could control
what people are seeing and you could amplify only one kind of sediment. We had a little bit of that
at the beginning of the Ukraine war. Anyone who wasn't a pro-Ukraine trying to isolate Russia,
anti-Russia, create a new Cold War, push, push, push was marginalized and attacked aggressively.
Tell me about it. That's why I didn't really talk about that as much as I would have liked
to last year. And I'm afraid that I'm going to be able to increasingly talk about less and less.
I think speaking truth to power generally is going to be fraught with peril in the next 24-36
months. Right. I pointed out really early on that this wasn't the kind of rational leadership
we'd need in a nuclear world. It was very the impulsive kind of reactive leadership
of a networked world, but it was running up against nuclear realities and that we needed to
take a breather and figure out what we're doing and not provoke this. I mean, don't let them
send drones into Moscow. Don't let that kind of thing that would potentially lead to a nuclear
incident that would end the world. And so many people were like, from March onward,
with like a tack, tack, tack, no such thing as nuclear war, if that were going to happen,
if Putin does it, it's going to prove everything we said about him is true. I go,
that doesn't matter to me. We won't matter to anyone if it happens.
Yeah. You have New Glent's in New York City and everything in between that and LA,
it doesn't really matter. So let's drill down on that. How does
networked tribalism, which is the broad category of what you're describing here,
and in some combination with AI, how worried are you about a nuclear exchange as a result of that
in the coming decade? Well, we came pretty close, closer than a lot of people would admit.
I agree. You mean recently even? Yeah, no, with the, and there's continually,
incidents that are potentially could be misinterpreted because those drones hitting
Moscow look a lot like cruise missiles could alert the wrong thing and then set off the wrong
kind of response. But so the response to Russia's invasion of Ukraine went not so fast because
there was already a network that, a tribal network that was in place fighting Trump.
And Putin was blamed for Trump's election in 2016 by that crowd.
Even though his influence was marginal, he was blamed as the ultimate evil in that instance.
He installed Trump and caused all the misery of Trump's reign of terror, according to them.
So when Putin invaded, they went nuts. They went like, this is the worst thing ever. It's like,
he's going to take all of Europe, Hitler reborn. We have to fight him. And that got amplified on
the network. And it just went insane to the point where everyone was disconnecting Russia,
corporations across the board, people kicking Russians out of discussion groups. It was like,
we went to embargo, effectively, from the West to Russia, which is like one step away from
tactical nuclear use, according to the nuclear ladder of Khan.
I'm going to ask a specific Russia-Ukraine military question in a second. But this network
to tribalism and this response that you just described, and sure, there's Ukraine, Russia,
you mentioned an Israel example. There's lots of potential examples. Is there any way to combat that?
If we can anticipate that that is a risk for society, this network to tribalism, which is
going to be on steroids with AI in the near future. I mean, you mentioned owning your own data, but
is this cat out of the bag? Has the horse left the barn in this risk?
Well, they could put in circuit breakers.
How so?
Well, when sentiment for war or violence is spreading very, very quickly, you can slow it
down, de-amplify it during those initial weeks. So you won't get as outsized response.
Except the people that are in control of this stuff probably don't want,
they want to accelerate that response.
I don't think the White House even knew what they were doing. They're just writing the
waves. The funny thing is, is that Musk's experience with that is the reason he pushed
through and ended up buying Twitter. He saw it going nuts.
This is dangerous. I really have to take action. This network didn't have a sense of
mortality because it was a group mind, it was a swarm, and that it was willing to push way up to
the edge and beyond. It was maximalist in his goals. It wouldn't accept any nuance or breaking
the action. The only thing they would diminish it over would be inactivity over time or being
distracted by another event. So that's the way it played out. And he ended up buying Twitter,
I think that gave him, it will give him some level of control over that, but TikTok's a
different story. So I've heard that AI has increasingly
actuation potential capacity and that in the example of Ukraine and Russia or anywhere in the
world, that you can tell an AI attack Russia under these circumstances, these scenarios.
And the AI will do it on its own. It will send out the drones under certain capacities,
and it will do a swarm so that they can't be shot down in some random way. And I don't know if
that's true or not, but if that is true and we're headed in that direction, aren't there
just countless examples of AI assisted Archduke Ferdinand moments?
Well, I ended up writing the Joint Chiefs of Staff concept on autonomous weapons
about four or five years ago. They wanted a 20-year lookout and they didn't have any
people on staff to do it. They brought me in and I worked out, went through their
whatever they had on the available plus, made some projections and worked out some of the kinks
and nuances. They were hard over on human control, but at the time, but the reality is that
autonomy in weapons, which is basically AI, changes how you're going to use those weapons
and changes in a bunch of different dimensions. It's like a smart mind. You could have it
embed itself somewhere for a long period of time and then act according to very narrow guidelines.
You could have it, you give it a wide variety of different targets that is allowed to hit
and then allow it to go out and you can make and adjudicate which ones to attack.
It's kind of extension of putting a little carrot on your target and then firing the
missile is just a little bit more varied than that. The most aggressive version would be
AIs that can understand and execute mission orders. Mission orders are like the kind of order that
Napoleon would write out and give to Marshall May and say, okay, here's what I want you to do.
It's very short and sweet and you have a lot of latitude in terms of how you accomplish it.
And then you give that AI access to swarms of different capabilities and have it execute the
order. Now, that's a entirely different thing, especially if they can self-provision,
if they can embed deeply behind enemy lines. That gets really wild. One of the concepts I
came up with, and I don't think it was really taken up by many people at least, is that the
really big breakthrough idea in using autonomous weapons is go for a concept called zero-day war,
is that you use drones and AIs on drones and deeply embed them in the enemy's geography.
They screw themselves into the muck of every harbor, every river, every forest, every mountain
range. There are some drones and they have different kind of capabilities. And when the
day zero of the war happens, the moment it happens, they act. They set up area denial
right in the middle of the country. You can't fly a plane, you can't drive a vehicle without it being
under attack. They started attacking the systems and bringing them down systematically. And that
they self-provisioned and they acquired the only electricity and other provisions that they need
to sustain themselves. And that's a completely different way of warfare, is that it makes it
possible that once that starts, they'll capitulate before you even get your troops even close enough
to actually take the locale. Well, maybe you only think people didn't listen to you.
Maybe they did listen to you. Oh, yeah, no, there's lots of cool things you could do that would
probably save a lot of human lives, is that they had a silo of space over the Spratly islands off the
shore of the Philippines, say 100 miles in circumference or in diameter. And that it's a capture
of flag kind of scenario, but you can only use autonomous weapons within that confines. And
it's the military might and the technological capability of the sides involved in China,
the US just fight it out constantly there to see who has dominance. And that actually kind of a
if somebody's, you know, one side is far more dominant than the other in terms of the technology,
and it's a sign to the other side that they actually should back down
where things could be very quickly. That actually makes sense to me. This is
all fascinating. I have a couple of hardcore questions for you. And I know that your family is
home and your dogs want to be fed. So I don't want to take up too much more of your time.
Though, to be honest, I could go another two hours with my questions. But
what are the odds that we make it through the next 10 or 20 years without a big war
involving nukes? It just seems like so many things are pointing in that
potential is looms large, just your opinion.
And my first thing, the first thing I ever kind of delved into
in military strategy was Nuka warfare. And it's hideously complex. It's all psychological. It's
all in the mind. And I was hoping, you know, that up until a couple of years ago,
just before Ukraine that, you know, we weren't stupid enough to actually stumble into it again,
that all of our opponents you could actually visit and vacation and at that moment,
and that all changed so quickly. So that we're far less intelligent than I ever suspected. So
is there a potential chance? I mean, there's a relatively high chance that we could see a
nuclear exchange. Now, hopefully, if it is, it's limited and it's so horrifying that we react
against it. Just like, I mean, we get an emotional reminder of World War Two and
Right. I mean, it may have been morally wrong to bomb Hiroshima and Nagasaki
and to end the war quickly. But we are lucky they did it because that reinforced the horror
of nuclear weapons and prevented the wars that followed that would have been fought with them.
And to far more devastation. I mean, we were able to navigate a tightrope
the whole of the Cold War and not trigger a nuclear annihilation of the west of the northern
hemisphere. And that's a good thing. I just don't think our leadership right now is beyond the JV
level. They're not serious enough people. And Biden has a little of that, but it's kind of scary.
It's that how prone they are to just jump on the bandwagon. And I think there is like a little
bit of sentiment inside at least the U.S. administration that things were easier when we
were in the Cold War. People listened. If we could return it a little bit to the Cold War,
we can get back to that level of stability and the compliance with government mandates and
things. But that's not, did happen and won't happen. This leads to my second question.
I've been in a different overlapping realm than you for the last 20 years looking at the system
science of the human predicament, energy, resources, environment, behavior, economics,
money, geopolitics. And I've concluded that there is one risk and domain that underpins
all others. And that is governance and decision making. And given what you've said on AI and
given what you've said on networked tribalism, how does a leadership or a government go beyond
JV level? And especially in the U.S., how do our decision making systems
avoid the bad feedback loop of poor decision making in this world fraught with peril? Do
you have any thoughts on that? How does the political leadership in the U.S. mature? I don't
think it does. And for a bunch of reasons, one is that the problem sets that we're facing are so
complex that our leadership style and the method of governance is beyond its capabilities.
And the classic thing is in a complex environment, you have to try out a lot of different things
and you pick the ones that work and reinforce them. We're more, this is the way to do it. This
is the bureaucracy that says this. And if you don't like it, we're going to force you to adopt it.
Also, our system doesn't have any opt-in features, meaning that it doesn't have any equity
participation in the sense that I get benefit for participating in it. And in a networked world,
that's important. It's actually a requirement. We just kind of assume because you're inside
the geographical borders that you will be loyal to it and contribute to it. And of course,
you find that people don't think that way. Another thing is that we're seeing a hollowing
out of the old nation state. It's losing a lot of the power that it once had. A classic example,
most recently, it's got lost complete control of the border. The border's gone. It's broken.
And that letting 8 million people in over the last three years, the size of what the 13th
largest state, largely almost completely unvetted, those people are going to disappear,
they'll never be seen again once they're, now that they're in. From all over the world, it's not
just central or South Americans. Then a hollow state, it has the facade of being an effective
government that has all the pomp and circumstance, it acts like it's in control, but it has no
effective control. It doesn't have effective control of the messaging over the borders of the
physical world. It doesn't have really control over its economy because it's all over the map.
It's too big to control. It doesn't have control over its finances because it's increasingly in
broke. Best we could hope for it. A historical counterpart to where we're at is kind of like
going back to the late 1800s, 1880, the end of the golden age kind of
timeframe and where you're high levels of foreign-born immigration in the United States.
It's chaotic. Everyone's breaking into their little communities. There's no sense of unity and
common purpose or desire to do things together anymore. We can't agree on anything. It's chaotic.
I mean, funny thing is that almost all of our progress socially was done between the end of
World War II and 1980 or so. That was during the period of the lowest level of immigration ever.
It's like we assessed and consolidated. Well, it was also the highest level of economic growth ever.
Yeah, but also incredible amounts of technological innovation and the like. We just went to the
opposite extreme. We didn't like to do a moderate increase in immigration. We went
behind and it's hollowing us out even faster.
What happens at the end state of a hollowed state?
I think hollow states can exist for a long, long time. More and more of the power goes to
corporations already seeing that. There's a tendency now. I did a report on my Global
Rows report thing on Substack and Patreon that looked at an Adelman survey, Adelman Public
Relations, the super slick PR organization that handled Microsoft back Windows 95 days.
They were slick, super sharp. They were looking at corporate trust and what do people trust and
what do people demand of corporations? They found that corporations are far more trusted now than
governments worldwide. At least the collective west across 23 different countries. That people
expected corporations start to pick up the slack, start to do what governments weren't doing and
take on bigger roles. They were willing to politicize corporations and see to them
a lot of control over their lives. I hadn't expected to see a shift that market. It was
like six days, 70%. Are we going to, with all the trends that you're describing, are we all
going to gradually become authoritarianism fans of one flavor or another? One of the
weird things about the current environment is that we wiped out fascism as a system back
in 1945. Communism, one big bureaucracy, US, this chaotic system, but government was
portion of it and a lot of corporate bureaucracies and there was everything was corralled in a common
framework. But as we got into this new network age, almost everyone's become fascist of one
state. China has become fascist. US is headed towards a fascism, a network fascism. A network
fascism is different than traditional fascism in some ways, but it works on the same principles.
You create a bunch of enemies, internal and external, and you use that to get a very chaotic
system of corporate and government bureaucracies and NGOs and individuals aligned and facing in
the same direction and that you have to hype and hype and hype those enemies to keep everyone
focused. And it works. It works really effectively. I mean, especially in the networked environment,
it's much more effective than the big live stuff that Goebbels put in place. It's almost more
pervasive, more insidious and probably that. It almost for sure will get worse with AI.
Oh, yeah. And so, you know, China does the same thing. It's like focused on the enemy and that
works in the networked tribalism, I see. It's usually anti-something. And it's anti, it's never
for something like traditional tribalism. It's always against anti-racism, anti-colonialism,
anti-whatever anti-Israel. And climate change is anti-fossil fuel companies.
Correct. And so, yeah, exactly. So the problem with the fatal flaw of fascism, obviously, or maybe
not obviously to people who think it's some other jackboots or something, because you've got jackboots
in communist systems too. It's like, or secret police in those systems too. The fatal flaw of
fascism is that it eventually declares, gets you into war with everybody. And you hype up the
internal threat from internal enemies to the extreme that you're putting them in concentration camps
and killing them. And they kill 12 million people because they, you know, they're eternal threats
that are so dire or you declare war and invade everybody because everybody's an existential
enemy that's presenting an imminent danger. And it's self-defeating that way because they get
rolled. It's that you can't be at war with everybody all the time.
What is the cultural antidote to what you're describing and to the listeners and viewers of
this program? What's the individual antidote to some of the things, some of the risks that you're
outlining? I'm big into localism or, you know, local control, regional control. The more layers of
the centralization we have between us and the global environment, the better.
And in terms of technology, I'm big into having more and more control, open source AIs and the
like, that that could be a safety valve. I hate the idea that, you know, all the apps and everything
else go through these big mega stores on the platforms charge 30% tax on everything they do,
but and also limited in what can be offered and what can't be.
I want to see it more like a decentralized modding community for the AR glasses and everything
I could get mods from all these different things and load them up and use them without filters.
But in order for some of those things to happen, people need to be educated about these risks
first. Otherwise, there won't be the demand and the push for open systems for AI.
Yeah, it takes a long time. I mean, it's like trying to sell social networking in 2001. It's
like a couple thousand people. Tristan Harris is a good friend of mine and, you know, they've
kind of failed so far on trying to regulate AI and some of the initiatives in DC. I mean,
I don't know specifics. I talked to Tristan too. It's like, yeah, he was up against a Goliath.
I was hoping that he would just stick with something simple like the data ownership thing.
It's like, once you get the idea that you could have a kind of a banking style industry managing
all our data and there's going to be reams of it, not just their names and phone numbers and that
kind of crap. I'm talking all the deep data like your facial expressions and how you walk and how
you, and that goes into modeling populations as a whole and creating simulations and other things
that are extremely valuable to everybody. What you say, everything you say, how you say it.
So first of all, I'm already kind of screwed. There's no way I can go and delete my Twitter posts
and Facebook because I've been out there quite a lot. So if AI is modeling me, they've already
got a pretty good model. This is really depressing, John. I really wanted to talk to you about
network tribalism because I'm worried about polarization and the fact that we can't have
conversations, that everyone has hot buttons, that if you say this, you're a Putin apologist or
if you say this, you're a fan of the fossil fuel companies and we can't have a balanced conversation
about the human predicament in our reality. And you've kind of indirectly convinced me that things
are worse than I thought. Yeah, but it might be ugly, but I think we're going to model through.
I'm just hoping that we can avoid a lot of the badness that's going to end up happening.
Probably not, but I'm betting on hopefully that we get out into space. I think that if
we can't get into space, I think we could end up just collapsing to nothing.
When you say get out into space, what do you mean by that?
Is that we have to start developing and expanding beyond Earth? And my guess is that Elon will
probably, I mean, I wrote a little article about how to accelerate it using asteroids in the crap,
but my guess is that Elon is probably going to end up putting his Dojo supercomputer for
training AIs in space because you get solar power cheaper than you can get on Earth and
scalable volumes that far beyond what you can get on Earth in the current environment,
particularly since those supercomputers now and most of the cloud stuff that goes around with AI
is so power intensive, it chews up the power of a medium-sized city and then it's growing even more.
So if Elon wants orders of magnitude more compute than he has now, you think his plans
are to do that in space? Running these big clusters to train AIs and to host AIs is almost
all power-related costs, 80% of the costs of actually running those systems is energy costs
and energy costs are going up, seems like everywhere here on a terrestrial and here you have this
window in space that he alone really can access.
And once that starts going, once he starts building those big solar arrays
and then he starts to look at ways to do it cheaper, I think he could end up pulling in
asteroid materials and the reason why those asteroid materials are going to be so valuable
is not just because they're equivalent of what you can get on Earth and cheaper,
it's that it's already in space and you use those materials to start building arrays and
more and more solar arrays. I'm talking like solar arrays that could
equivalent be equal to several diameters of the Earth. There's so much space up there and
running our cloud infrastructure first and then eventually over time becoming
capable of beaming down energy to use the microwaves for swing production in various
locales that need it that are paying through the nose for alternative. But potentially
electricity is almost too cheap to meter, if done correctly, in space.
If that's all plausible, which I'm pretty skeptical of because it takes energy and there's a payload
and you have there's mining and there's zero gravity and how do you drill an asteroid and
zero gravity and there are constraints. I'm not even worried about the asteroid portion,
the actual that potentially downstream, but once you regularize space in the near Earth
environment, everything else becomes much, much easier.
What are your opinions about climate change and the ecological destruction of species and some of
the environmental limits that we've already well exceeded and how does that fit into this story?
It's just a thermodynamic
box that we're in. That's human civilization has reached limits of its environment and
has a dissipative system. We either expand and go out or we die. There's no going back.
There's no turning back that system. It's too complex. It's operating too far from
thermodynamic equilibrium and we're just dumping this entropy into our living environment and we
will die heat death. We will totally run out unless we create a larger external environment
that we can expand into. If we don't get to type one and type one civilization is that we
pull in as much energy or generate as much energy as the Earth absorbs
from the sun on a daily basis,
then we die. It's like a shark. We got to keep on going forward. We have to keep on
getting to bigger and bigger environments or we perish. It's inevitable.
Coupling that with two other things you said earlier that Elon Musk might describe as unstable
and number two, he's the only one that's accessing as has been the founder and the first
what's the word for the early adopter of doing things in space. Doesn't that give you pause a
little bit? Yeah. It's making it just a slight bit by the fact that Bezos has blue origin still,
he's pouring billions in and he has a big cloud computing company is that if Musk starts
hosting the cloud in space for cheaper energy, Bezos won't be far behind and then there'll
be a race to see who can get the most cloud infrastructure built running at a level of efficiency
can't match at the terrestrial level. I don't think this is going to happen for various technical
reasons, but a more larger systemic reason, which is if that happens, then the great simplification
is not true or not going to happen. I think our space exploration success has been based on an
era of energy surplus, which is ending. We're masking it by creating debt and central bank
supports, but I just don't know that we're fiscally able to do the magnitude of things like that,
even Elon Musk. That's why I came up with a trend to come up with ways to simulate an internet boom
where we raise trillions in capital based on speculation and becoming on paper very, very
valuable and that we build out the infrastructure that is needed for it to start to create its own
weather dynamic, its own economy. If done correctly, it could generate the energy surplus we want
and the resource surplus we need at Infinitum. The whole solar system is available for us to
take advantage of. It's just getting over that cost barrier. Right now, we're sitting at that
step function level and we're looking up and seeing that cliff up there and we can't seem to get out
of it, but if we can fool the system into getting us up there or get Musk to create a little stampede
on his own by doing something that everyone else wants to emulate, then we might get out of this,
but otherwise it's just more entropy on earth, more social entropy, more physical pollution
entropy, more chaos. Done. I would love to have you back to unpack some of this maybe on a round
table with other experts on this. Sure. On all this stuff, but as is, I really appreciate your time.
This is an unusual interview for me because I usually am talking specifically about ecology or
oil or neuroscience and I haven't talked to too many tech experts in this way. Again, the only
reason I invited you is because I've been following you for over 20 years and I know that you have
core insights on this stuff. Usually at the end of my interviews, I ask a few personal questions
of my guests on their first interview. Sure. I hope you don't mind, but I think you've probably
freaked some people out that are watching this show. Do you have any personal advice
to the human beings just as humans on what people can personally do during this time of
what some call the metacrisis now on top of network tribalism and AI and other risks that
you discuss? What kind of advice do you have, John? Don't let what's going on in the online
information space, the abstract space dictate your mood and how you think. Focus on living life,
grow your garden, raise your family, spend time with them, work with them to make sure that
they're successful as possible, live great lives, live up to their potential. I've got a big house
and my two youngest are living here and working from home at great companies. They're about to
start their own lives, but I want them here as long as possible, which is great. Focus on that.
If you can do that, that little level set you make you feel a lot better about the world.
You can't, and you can focus on improving that and looking for signs that that actually is being
infringed upon or by this chaotic external environment and taking measures to mitigate
that damage. Don't let this, we live in this abstract metaspace almost too much now. I mean,
I've been doing it since 1994, 1995, and you're doing the same thing. I think we've developed
our sea legs to a certain extent that we're not too thrown by how things are maneuvering and how
things are swaying to and fro, but balance. Balance, this is a marathon. Even if there is
chaos, it's still a marathon. That's good advice. What do you recommend to young people? Just like
you said, there's not too many people under the age of 50 that watch TV. There's not too many
people under the age of 30 that listen to this podcast, but what recommendation do you have
for 20-somethings becoming aware of all this stuff? I don't know. I think it's probably the coolest
environment to be alive in ever. I mean, in terms of the opportunity and the things you could learn.
I remember living before the internet and then watching it turn on. I mean,
right in that kind of capricid watching this, it was like my brain turned on. I mean, it was like,
wow, this is cool. See the opportunity to learn things and to work almost anywhere. I mean,
I'm employing you guys who do software for me at times. We're sitting on a beach in Turkey,
living a great life or you could work online like my daughters do and you could live anywhere.
You can stay in a family house or you could go live in London for a while or go live in a cabin
in Steamboat Springs or whatever. It's like, the world is your oyster. It's awesome.
Your ability to bootstrap yourself to wherever you want to be and wherever you want to go
is easier than ever. And granted, there are the threats out there
that can impinge on you in the future, but develop skill sets that mitigate those
damage. Learn how to grow stuff. Learn how to fix stuff. Learn how to do all those things. As long
as you have the skill sets, it doesn't mean you have to do it all the time. But if the problems
arise, then you could actually deal with it. What do you care most about in the world, John?
Oh, family. That's a pretty easy one. Pretty much drives my life. And a far second would be like,
you know, just dogs and things like that, just like daily conveniences. Well, dogs are family.
Yeah. I mean, I've kind of created kind of a more of a muck. I'm very kind of happy with that.
I was happy at the Air Force Academy when I was up and isolated from everybody else,
working all that time. It's comfortable.
If you could wave a magic wand and there was no personal recourse to your decisions,
what is one thing you would do to improve human or planetary futures?
Develop space. I mean, as much as everyone has a negative reaction to it,
I've been thinking about it since I became an astronautical engineer,
failed astronaut candidate, is that never really got aligned shuttle blew up before I
always had a chance. So we have to keep on going forward. And we've stalled out at this
at the step function. We got it. And if I could wave my wand and say,
let's spend this money or invest this money and build this infrastructure,
then start regularizing our use of it, hoping up a rise in some looking forward into a world that
to future that has unlimited potential, I would do that. Because if you're just looking at your
navel, okay, fixing the things that are wrong with this world, there's a never ending hole that
will go down forever. It's like looking into the abyss, that famous romantic
painting, gazing into the abyss. That's where we're at right now is we keep on gazing into
the abyss and we're not looking up. We're not hearing that kind of the roar of a star. It's
out there burning and calling to us. It's like, gotta go. It's time to get up. Leave the womb,
man. You know, we're in this womb and we don't want to leave. And mom's like, get out,
and we're not. And I would like to see us get out.
My view, and I don't know as much about it as you, is it actually would require a magic wand
to make that happen. But we shall see. And I really respect you and your research
and your opinion. So I'd love to have you back. Maybe take a deeper dive on some of this stuff.
Do you have any closing words to sum up this conversation for our viewers?
Sure. This is a good portion of it. At the end, there was dedicated space. I got involved in
the interactive television effort early on to build something like the internet back in 93 with
the big telcos. And they could not. There was no vision from that point forward that internet
would ever be built. There's too much money, hundreds of billions of dollars. The payoff was
too uncertain. They couldn't imagine what they do with too many technological steps that they
still would have to discover and figure out a ways to do. And they abandoned it. And here comes
this internet. It's done in the right way. The right kind of bootstrap. And it got built in a decade.
A decade. Something that never should have happened. But it did. And it's crazy how it
built out so quickly. And I think we can do that here with this. With space. Same way.
Just ignite it in the right way. And then it just goes. And it won't end. And it won't end in our
lifetimes, at least. The first thought that came to my mind hearing your appeal there is,
can you imagine networked tribalism with AI and space?
Yeah. Yeah. It gets pretty ugly. But if you do it right at the start, which we probably won't.
But you can mitigate a lot of those problems. Just like the data ownership and AI's. If we've done
it right initially, everyone would have more participation in the upside potential of these
AI's. And that would change the dynamic in terms of fear of where they're going and how they're
developing. A good bit. If you knew that you had some, if whatever this is going to develop into,
this AI economy, that you had some upside potential there that all boats were rising,
it would change your perspective on where we're going in a big way.
But the way it's looking right now is that a few boats were rising to the moon and back.
And then the rest of us are saying, suck dry. I just want to see this doing it right.
Thank you for your insights and your continued work to be continued, John. Thank you.
