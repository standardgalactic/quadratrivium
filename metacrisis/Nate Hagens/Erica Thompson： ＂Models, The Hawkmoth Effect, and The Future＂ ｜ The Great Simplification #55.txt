You're listening to The Great Simplification with Nate Hagins. That's me.
On this show, we try to explore and simplify what's happening with energy, the economy,
the environment, and our society. Together with scientists, experts, and leaders,
this show is about understanding the bird's-eye view of how everything fits together,
where we go from here and what we can do about it as a society and as individuals.
Every now and then, I have a guest who's doing something completely different yet unexpectedly
relevant to the themes explored on The Great Simplification podcast.
Dr. Erika Thompson is just such a guest. Erika is a senior policy fellow
at the London School of Economics Data Science Institute, where she works on theoretical
and practical research about mathematical models, their use, interpretation, and social context.
Erika recently wrote a book Escape from Model Land, which explores the pitfalls of conventional
science models and the opportunities by which models might augment our imagination
and collective understanding. How can we rethink and bring creativity into something
as scientific and dry as modeling to create maps of the future that are both more accurate
and, most importantly, more helpful to actual planning responses?
This was a very interesting conversation, which I enjoyed a lot. Please welcome Dr. Erika Thompson.
Good morning, Erika, or good afternoon, I would suppose.
Good morning, Nate. It's good afternoon here, yes, but good morning.
Let's get started.
You are a scientist and you just wrote a book called Escape from Model Land, and we're going
to get into that, but why did you write this book and why is this topic important?
Yeah, okay. So my background is in physics and maths, and then I did a PhD in climate
change science about 10 years ago. And so the first thing I did as part of that PhD was
to do a literature review. It was about North Atlantic storms and how they were given climate
change, and so I looked at a lot of models, and what I found somewhat to my surprise was
that actually the models don't really agree with each other, or they didn't 10 years ago,
maybe they agree better now, but 10 years ago you could find models that said that the
North Atlantic storm tracks were going to go northward, they were going to go southward,
they were going to get stronger, they were going to get weaker, there'd be more intense
storms, there'd be less intense storms, you name it. And the interesting thing was that
each of these would have some kind of uncertainty range or error bar on their prediction, and
they didn't agree. The error bars, the uncertainty ranges didn't agree, and so that sort of made
me think actually this is telling me almost nothing about North Atlantic storms, but it's
telling me a great deal about models and how we use models, and maybe that we are a bit
overconfident in models and the way that we assess these uncertainty ranges. And so I started
thinking about that, and so that was part of my thesis, and then I've gone on to think about
mathematical models in other situations, so climate change is one I'm particularly interested
in, but also financial and economic modelling, public health modelling, as we've seen in the
last three years over the COVID pandemic, and the way that these kind of models are used to
inform policy and decision making as well, of course, because we don't just model for the sake
of having a pretty picture, we model because we're interested in knowing what's going to
happen in order to be able to make decisions in the real world. And so my book is about, it's
sort of about my journey through thinking all of these different things over the last few years
and thinking about how we use these models and what kinds of decisions we're making and how the
models embody and reflect value judgments that are made on the part of the modelers, and what we
can do to use models more effectively, really, and to generate insight from them rather than just
generating numbers.
So I have a lot of questions. While you were speaking about the North Atlantic storms, it just
made me wonder how much of a model is the modelers' ideology or worldview or personality
imbued in that model? And is there any natural check on checks and balances on that?
I mean, so it depends what it is that you're trying to model. And if you're making a model of
tomorrow's weather or the flip of a coin or the next roll of a dice or something, you know,
actually, it doesn't matter who the modeler is. The value judgments don't really come into it
because you're pinned back to the data. You're always pinned back to the data, and you can make
any kind of model you like, but you will be able to test it every day or every time you roll the
dice or every time we get a new observation of weather. You'll be able to check it and you'll
test it and you will go back to the data and you'll be able to say, was this any good or was it
not? And that will be pretty incontrovertible, and it will be something that means that it doesn't
really matter who the modeler is, okay?
But then you're on a spectrum because some things obviously take a lot longer for those
observations to pan out or there isn't very much data or maybe there's lots of data, but it's not
actually necessarily the relevant data. You know, to what extent should we think that successful
predictions of past election results should be indicative of a successful prediction of future
election results, for example. That's something that in order to do that, we have to make an expert
judgment about the degree of similarity of the past situation and the future climate change.
Obviously, we expect that the climate is changing and so observations of past climate, while they are
useful and indicative, aren't necessarily going to tell us exactly what the future climate will be
because the one thing we know is that it's going to be different. And so in those situations where
we're thinking about future systems, where we're thinking about complex systems, maybe chaotic
systems, social systems, any system where humans and human interactions and social characteristics
have some kind of, you know, some part to play in the system, then those become much more
difficult. And I think those are the ones where we really see that the characteristics and the
assumptions and the biases and the blind spots and the preconceptions and the ideas and the
worldviews of the modeller become much more important and they can become embedded in the
model and there isn't a huge amount of data to be able to tie it back at every step.
So, you just wrote a book on models. What is a model? And is a model only a computer thing or do we
also have mental models in each of our heads?
Yeah, so I would say a model very generically is a metaphor that we use, that we sort of construct to
say that one thing is like another thing in order to try to understand the properties of the one thing.
So, you could say the climate system, the real one, is like a climate model in certain ways and it will
give us information about that if we sort of extend the metaphor. But the question is how far can you push
that metaphor before it breaks? A photograph is a sort of an inverted commas metaphor in the same way.
It is like the subject of the photograph in some ways, but obviously it's not like them in other ways.
A picture of me or my face would be like me in the sense that you could recognise me from it, but it looks completely
wrong if you turn it, flip it over and look at the back or it doesn't give you a 3D picture.
It doesn't tell you what I like to eat or what I had for breakfast or my opinions on anything.
So, all metaphors are going to be limited. So, the models that I'm most interested in are the kind of big
complex models that we probably put into a computer and there's a whole load of programming.
So, things like climate models, models of public health, the coronavirus pandemic, models of finance and the
economy, these sort of big complex models. But I think that they are only one end of a spectrum of models
and the same kinds of insights apply all the way along that spectrum.
So, Danella Meadows, I'm sure you're aware of who that is.
She was the lead author of Limits to Growth at the Club of Rome 50 years ago.
She famously distinguished between black box modelling where a human gives an input to a computer model
and out comes an answer versus what her and Dennis Meadows and others work was systems dynamics,
which improve our intuition and understanding kind of like what you were just saying about the picture of the
climate versus the climate system versus the climate model. They improve in our minds what is happening.
So, our modern climate models are a combination of these two.
They both spit out an answer and they improve our understanding of what's going on in the physical world
or have they become so complex with so many inputs that they've actually become black boxes that we can't understand
what's going on internally?
Well, I think they're a combination and obviously the whole point of starting with the laws of physics
is that you expect to be making a system dynamic model.
You know, you're expecting to be able to play with it and match it against real physics and real behaviours
including the complex emergent behaviours that the climate system has
and that you would then be able to sort of test out different hypotheses, try things, see how they work,
see how they match up and improve it and that it would in that sense be a system dynamic model
and help to improve intuition and understanding.
But of course, within that, you know, if you go down to the small scale within climate models,
there are also black boxes.
There are what are called statistical parameterisations of behaviour at the small scale within the grid boxes.
So you might say, I'm interested in the micro physics of how chemical reactions happen in clouds
and somebody might have a, you know, a mechanistic model of how that works,
but in practice, you can't do that in real time on your computer
and so you create a statistical sort of representation of what the net effect of all of that would be
and you stick that in.
And so that is, in some ways, a black box that is part of the model.
And then of course, as you hinted, I think when the models become extremely large and complex,
they can become black boxes in so far as actually it's very hard for one single individual to understand
all of what the model is doing because it's been created over decades by hundreds or even thousands
of different people working on their different subroutines and connecting them all together.
And so I think, you know, it probably is reasonable to say that there is nobody who could fully understand
all of the workings of a whole climate model, you know, one of the real state of the art ones.
And in that sense, yes, it becomes a bit of a black box.
It becomes hard to calibrate.
It becomes hard to say, OK, I'm going to look at the output here and make a few changes
and it will get better because you make a few changes, you twiddle the knobs
and it gets better in some respects and it gets worse in others.
So you've got a whole load of trade-offs as well when you're thinking about where to go,
what's next and how to develop it.
So how can climate modelers attempt to account for the known unknowns?
And is there a practicality to even attempting to model climate when it's so difficult to nearly impossible
to create an accurate model with tipping points and all these emergent things that we just don't understand?
Well, of course, there are a huge number of known unknowns and potentially unknown unknowns in climate
and the modelling that we do does attempt to take those into account in some ways.
You can't really do that within model land.
So my book is called Escape from Model Land and I like to sort of make this picture of model land
being where you are when you're inside your computer model and everything works and all your assumptions are correct
and everything's perfect and actually we're interested in the real world, not in model land.
And so we have to have some way of getting out of model land either with respect to data by sort of checking against data
or by the use of expert judgment by saying, you know, we believe in the quality of this model
because maybe because it has good physical representations or it shows the behaviour in a realistic way, that sort of thing.
So in terms of attempting to account for these unknowns and the things that are not modelable within the confines of model land,
you have to say, actually, we're going to make that leap out of model land.
We're going to make some expert judgment about the quality of the model and the quality of its representation of real life.
And so, for example, you know, just to give a concrete example, what the IPCC have done is used the...
they will take the model output.
So say you've got 20 different models and they are telling you what the global mean temperature is going to be in 2100.
And so you've got 20 different numbers and you could use that to sort of create a distribution or you could generate a confidence interval.
Say you use your 20 models and you make a 90% confidence interval of what the next model run will be.
Well, we're not interested in the next model, we want to know about the real world.
So what the IPCC have done is they've sort of arbitrarily downgraded their confidence.
They say we take a 90% confidence interval about the model and we'll call that a 66% confidence interval for the real world.
So you're basically just saying, actually, we're less confident than the models are.
And so that's a completely ad hoc step.
It's nice because it does genuinely escape from model land.
And so in that sense, the statement made by the IPCC is a statement about the real world and not just a statement about the models.
And I think that's good because it gets towards some kind of accountability.
It's easy to say, oh, it was just a model.
We never expected the models to be perfect retrospectively, but you want to be able to say in advance we understand that our models are not perfect.
And so, you know, you ask whether we should even attempt to model the climate when we can't create an accurate model.
Well, of course, we can create a very good model of the greenhouse effect.
It's just very imprecise.
And we expect that there will be a whole load of interesting stuff going on in terms of the details of what actually happens to the local climate and weather and the things that people care about and the things that impact upon communities, industries, people, etc.
So we can do the large scale and we can't do the small scale and we are making these big and fairly arbitrary judgments about the quality of the model.
Does that mean it's worthless even bothering to try and model climate?
I don't think it does.
You know, I think we still have that, as you said, the system dynamic contribution.
We are doing it in order to improve our understanding of the system and we're doing it in order to understand how it works and how the physics interacts.
We just shouldn't necessarily expect to get reliable, detailed, accurate, precise answers at the local level.
So in your book, you talk about subtracting climate from a climate model.
What do you mean by that?
And how does adding complexity make this a difficult process?
Yeah, so when I say subtracting climate from a climate model, I sort of mean that if we want to know how good our estimate was of something,
you'll say, well, I'm going to predict tomorrow's weather and I'll say it's going to be 16 degrees and it turns out it's 14 degrees and the error was 2 degrees.
So I'm subtracting my prediction with the reality from my prediction to get an idea of the quality of the model.
So when I want to do that with climate, maybe I have this big picture of climate and I have observations which are taken at weather stations and I put those together and I have some kind of analysis of those.
But then my climate model is kind of different from that, right?
Because it doesn't have observations at those particular weather stations.
It has a grid which is say 100 kilometers by 100 kilometers and it has numerical simulations on that grid.
So what does it even mean to say the wind speed in this 100 kilometer grid square is 17 miles per hour northeasterly?
I mean, it's sort of you're saying we can sum up an average of those, all of the wind speed within that grid and take an average of it.
And does it actually make sense to be then subtracting that from an observation at a single point?
Those are fundamentally different things.
So we can do it and we do do it.
And when you do do it, you can generate a picture of how good your climate model is.
You can generate a picture of the errors with respect to reality.
But I think when we're doing that, what is hiding is this fundamental difference in variables that we can subtract real wind speed from model wind speed.
But one of them is in the real world and one of them is in model land and they are not actually the same variable even though we like to think of them as being the same thing.
And so the complexity makes this different difficult because the complexity sort of hides the difficulty of doing that.
If you were just predicting wind speed on a tiny little area, say you've got four grid points and you start trying to do this,
then it becomes obvious quite quickly that you're doing something that doesn't necessarily make complete sense.
But if you're doing it on the large scale and you've just got numbers in a computer program, then it's very easy to do it automatically.
And the question then of whether what you're doing is conceptually founded, I think, is quite an interesting one.
Lots of interesting things in your book.
Most people have heard of the butterfly effect, but less have heard of the hawk moth effect.
I'd never heard of it.
Can you explain both of these and how they're connected to each other and to climate modeling?
So the butterfly effect, which people will generally have heard of because it was popularized in the 1980s and 90s,
is this idea that things can be very sensitively dependent on initial conditions.
So the idea of going back in time and trading on a butterfly and having some major effect on the future
or the flap of a butterfly's wings setting off a tornado in Texas.
I mean, there are various different conceptions of it, but the idea is that a small change at one point in time,
even a tiny little change, can propagate forward into actually very different trajectories of what the future will look like.
And so that's kind of well understood and people, it's in the collective imagination,
but it only refers to this difference of initial conditions.
And so in terms of prediction, it's important, but I think it's less important than a corresponding problem,
which is the slight errors in a model.
So if you are making a model of what the future will look like and your model is very slightly wrong,
to the flap of a butterfly's wings, a tiny bit wrong,
then the differences in the prediction and the real system can propagate forward in just the same way
and diverge in the same way so that a tiny error in the model results in a large prediction error in the future.
And so that I've called the Hawk Moth effect by sort of by analogy with the butterfly effect.
Do you suspect the Hawk Moth effect is larger than the butterfly effect?
I think it must be.
I mean, they're sort of at a minimum, they are the same because the Hawk Moth effect, if your model is wrong,
like a sort of one-time step of the model being wrong introduces a small initial condition error, right?
So at a minimum, one step of the Hawk Moth effect then introduces a butterfly effect.
And so it can't be less in that sense.
Is there such thing called a Hawk Moth?
In the US, I've not heard of that.
Is that a whales creature or...?
So I was going to go for the Moth effect, but it sounded really boring.
So I thought I'd find a type of Moth that was more interesting.
So I looked up and actually there's a whole family of Hawk Moths.
And there are from the death-said Hawk Moth, which is probably the most famous one, which has a sort of skull on its abdomen.
And the one that I like is the Poplar Hawk Moth, which is native to the UK.
And it feeds on poplar trees and it has got a very impressive caterpillar and big furry antennae and nice gray wings.
But it's quite camouflaged.
You know, it's more dark and mysterious than the pictures of the butterfly effect with a really pretty, you know, snazzy butterfly.
The Hawk Moth is a bit more camouflaged and mysterious.
Excellent choice.
Excellent choice.
In your book, you discussed the idea of optimal climate and how this has been used by economists and dominant groups that it has inherent bias.
And maybe we over quantify it.
Can you unpack that a little bit?
Yeah, OK.
So this idea of optimal climate, I mean, this goes way back to the early Europeans going to Africa and saying, oh, how backward these natives are.
And it must be because of their terrible climate.
You know, they would go and they would get ill and they would say, oh, the malaria, the bad food and all that sort of thing.
Oh, isn't this an awful place?
And the reason that we Europeans are so much better and developed in every way must be because we have better weather.
You know, which is hard to understand these days, really.
But so they would then sort of go back and systematize that and say, well, right, so the climate of Europe or North America is whatever it is.
And there's an average temperature and an average rainfall.
And then you come up with some arguments around this about how that must be optimal for growing crops or how people have benefited from, you know, whatever aspect of the weather that you want to focus on.
And contrast that with, say, the Caribbean or West Africa and say, well, this must be the reason why Northern Europe and North America are so much wealthier because of this better climate, which is completely ridiculous.
Obviously, it ignores a whole load of other structural reasons, you know, not least the problem that the dependent variable that they're trying to assess is something that has been fixed by the dominant groups in the first place, you know, GDP.
So who came up with the idea of GDP? Who has the highest GDP? Is this surprising?
And so that's sort of the early history of this idea of optimal climate. And it was used to come up with all sorts of silly rationales for doing all sorts of silly things.
But it becomes, you know, when you bring that sort of forward into the present day and look at what economists are doing in terms of climate and weather outcomes and thinking about how the economy might be affected by climate changes,
then you can find people doing actually very similar things and saying, you know, that the average temperature of a region is whatever it is and correlating that across a number of different countries, correlating productivity, say, with average temperatures.
Again, ignoring a vast number of other structural reasons for differences between countries.
And then perhaps most interestingly, and more recently, looking at changes between years in the same country, so saying, well, it's slightly hotter.
If it was slightly hotter in 2021 than in 2020, then what was the productivity change between those years? And can that then be sort of regressed back onto temperatures?
And then trying to say, well, if the temperature increases a bit, what will be the productivity changes?
And I think, you know, that sort of, it's slightly harder to criticize that, but they come up with some rather ridiculous answers because it's always operating at the margins, right?
And it's ignoring all of these other structural characteristics.
So you have a paper, if you go back, I'm sure you've discussed this on your podcast in the past.
You've got Bill Nordhaus, who got the Nobel Prize in Economics four or five years ago now, who looked at the optimal temperature for the optimal climate outcome relative to how it would influence GDP and all that sort of thing and saying, you know, that maybe the optimal climate would be four degrees of global mean temperature change.
And that's because, again, the analysis is at the margins and it's ignoring a whole load of other things.
And it has these assumptions about what productivity is and what generates productivity and how temperature impacts it as a function of, you know, all the other things that might be going on in a country because obviously there's politics and there's changes in everything else at the same time.
To me, that's the big downside of models, especially if someone is attributed the status of a Nobel Prize winner, they must know what they're talking about.
And to say that 4C temperature differential is the optimum climate just shows how a philosophy can be embedded in a model that's just disastrous.
Many people think a four degree Celsius change would mean human extinction or at least a large population culling of humans and other species because of the positive feedbacks.
Is that like a credible thing in the climate community that people actually believe Nordhaus' research on that or how has that evolved over the last four or five years?
No, I mean, I think if you ask a climate physicist, they will say it's completely ridiculous and there's absolutely no way that four degrees could be the optimal.
You know, that there are so many things that are known to happen well before four degrees that would be extremely bad and so many unknowns that could possibly happen before four degrees that would be even worse.
The concept of four degrees being optimal in any sense whatsoever is extremely hard to comprehend.
You know, you look at the impacts that are happening already at around one degree of global mean temperature change and then the things that are projected to happen before two degrees and it's completely incomprehensible.
And yet, as you say, so this is because of the biases and the things that are built into the model by the economist who says, well, actually everything that happens indoors isn't affected by climate.
The only sectors that will be affected by climate change are agriculture and maybe mining and that sort of thing.
And you know, I think from a trivial look at it, you can see that that's completely ridiculous.
It is completely ridiculous. This gets to the late Herman Daley's point that a standard economist would look at food production as only 3% of our economy.
So if we lose all food, we would only drop 3% in our economy, but the rest of the economy depends on people having food.
What if there was a philanthropist listening to this program that would start a fund to remove prior Nobel Prizes for analysis and conclusions that were total bunk and unhelpful to society and just explain why in each case we're removing this Nobel Prize.
Okay, let me move on. You have so much stuff in your book.
I'm worried you'd have to remove quite a lot of them.
Yeah, really? I know of several, but quite a lot. Yeah, maybe so.
Yeah.
Well, let me talk about that for a second. Does higher status in our society lend more respect and credibility to that person or that entity's model?
Yes, definitely. I mean, it's an entry barrier to even constructing a model that's going to be taken seriously.
So, you know, if you go and ask the man on the street whether what they think is going to happen with the COVID pandemic, say, if you'd ask them in April 2020, they could give you a view.
But nobody's going to take it seriously until you've written it down in a fancy computer model that takes ages to run on a supercomputer and you've got your qualification and you're a doctor or a professor or something from a credible university.
And you have the right connections with the right people to get into the right rooms to have that taken seriously at the highest levels in order for it to feed into decision making.
So, I mean, yes, definitely. And then within academia, of course, there's all the hierarchy and gatekeeping and such like about what counts as being a credible and respectable model.
But even before you get there, there's a huge selection bias in terms of who actually makes it into the community to be able to make a model at all.
Well, building on that, let's get back to the climate space. There's a new paper out by James Hansen who has a lot of status within the climate community.
He was one of the first people in the 1980s to present to Congress on the risk to climate change.
In his new paper, he asserts that the long term sensitivity of a doubling of CO2 is much higher than previously thought.
And he said by the year, I can't remember the exact details, but basically we're headed for 10 degrees Celsius in the long term earth system equilibrium.
So in that case, we don't know if he's right or he's wrong. This is a model that his team have come up with.
But can the Overton window of this is a serious and dire risk people we need to think about and change our behavior to avert this risk?
Can you go too far? Because 10C is extinction of conscious life on earth, in my opinion.
And so do we have the moral right to say, oh, James Hansen, brilliant climate scientist says it's already too late?
I think we have to assume that he's wrong from a moral standpoint and try to avert such a thing whether he's right or wrong.
What do you think about that?
Well, I mean, there's lots of questions there. I mean, there's the plausibility of the 10 degrees, which I think, you know, feels unlikely, but maybe we can't rule it out.
There's the question of whether we should act as if we believe that, you know, if you're putting...
But just to be clear, Eric, he's not talking about 10C this century. He's saying five centuries from now, that's where things will eventually get to. Sorry.
So the long term commitment being 10 degrees. I mean, yeah, I mean, I think we can't rule it out. It's not completely impossible.
It feels unlikely just given the sort of thermodynamics of the earth system.
But of course, there are potentially lots of feedbacks in there and there is very little evidence.
We don't know how things might pan out. And so it is at least possible.
But do you put a 10% chance on it or a 1% chance or a 0.001% chance on it?
And do you allow any of that to figure in your decision making or not?
I think that's a, it is a fair question. And yes, I'd certainly agree.
When you get to the sort of worst, absolute worst case scenarios, there's almost no point considering them at all.
Because if at the point you get to those all bets are off, is it too late?
I think it's clearly not too late. You know, the commitment, there may be commitments in the earth system,
but there are lots of things that one could do to tip things back the other way.
And they may have equally bad outcomes. There are all of the social and political outcomes that you have to think about as well.
But I certainly, I would say the earth system is not committed in a sense to 10 degrees or even to 4 degrees.
But the choices that we make in the next decades have a very great bearing on that.
I completely agree with you, Dr. Thompson.
However, neither you or I are climate modelers of the type of James Hansen.
So we are systems analysts and looking at the subjective and the objective within the models, within the global macroeconomy.
But this gets to my point, you or I don't know his model, and we're not qualified to the level that he is.
So how do we think about that as humans who respect science and peer review, etc.?
Yeah, okay. So, I mean, this is another major point of my book really is talking about this level of expert judgment
and that actually ultimately trust in science and trust in models is equivalent to trust in experts and expertise.
And so the question of whether you decide to trust a model is equivalent to the question of whether you are willing to trust the modeler,
the person who has made it or the people, the group of people that have made it.
And your judgment about whether you wish to trust them is a function of your politics and your values and your knowledge and understanding of that person
and your assessment of their expertise, whether they have any expertise or whether their expertise is relevant to you and the things that you care about,
or whether they have some kind of agenda which might be different from yours and the things that you care about.
And so you can see this everywhere. You can see this in attitudes towards climate science generally,
where certain groups say that they follow the science and respect the science and they will take the mainstream scientific conclusions
and other people will say, oh, this is that there is some kind of conspiracy or that the mainstream scientists have got different agendas,
different value judgments and different political preferences from their own and therefore they would take them there seriously.
They would assign them less trust and they would be less inclined to take the outcomes as being plausible.
And you can see that also in modelling of the pandemic and the effectiveness of vaccines and all of that.
We're familiar with the extreme polarization around these, but I think it's the same question.
It's the same question of whether or not you should believe.
It's not solely a function of the science with capital S.
It's a function of your personality and your relationship with the people who are making the model and your understanding of what they are doing and why they are doing it
and why it's important to them and why it's important for you and whether you should listen.
It's not only your personality, but it's also your identity.
I think for most humans in history, truth didn't matter other than how that truth affected your life, your built identity and your expectations about the future,
which explains why a lot of people don't care at all about climate models because they're busy paying the bills and trying to feed their kids or whatever.
Having said that, out of all the demographics in society that people trust, government, media, scientists and science are still the highest.
It's not as much as it used to be, but there's still we have trust in science.
What are your thoughts on that?
Well, I think that's because generally people do go into science with sort of positive intentions, either to help society or to find new things out and that they are generally somewhat neutral.
I mean, I think that there are obvious entries of political ideologies into the question of what science should be funded and how it should be funded and how that should be allocated and what kind of things we should work on.
But generically, I think scientists are trustworthy in the sense that they are not typically in it for the money.
Most of us could make vastly more money by going into some other career with the transferable skills that scientists have.
And there is an ongoing debate about the degree to which scientists should be involved sort of as citizens or that scientists should stay out of the debate about politics and should just stick to the science.
But I suppose my reflection on that is that the science itself does contain politics.
It does contain value judgments.
And so there is no way to stay out of politics completely just by doing the science unless you are really in model land, you know, if you're a pure mathematician or something.
But even that is a value judgment insofar as you're saying, well, I consider myself at liberty to go and study pure mathematics rather than attempting to contribute to other fields of human endeavor and such like.
Okay, so let's expand on that topic of value judgment.
Here's a quote from your book in a section that criticizes the lack of imagination in modeling.
The quote, the whole concept of predicting the future can sometimes end up reducing the possibility of actively creating a better one.
If we want the future to look different than the present and not just a continuation of all of today's trends, then we have to construct models that are able to imagine something more.
Can you unpack this, Erica, and why it's important?
Yeah, so the example that I was thinking about here is integrated assessment modeling of energy, the energy system and the climate system.
And so these kind of models, they put a price on nuclear energy, renewable energy, fossil fuels, all the rest of it, and the different sectors in which that energy is used.
And they sort of predict forward by saying, we're going to take a least cost pathway.
And so you make your assumptions and you put those in and you say, we expect solar energy to decrease in price at the following rate.
And we expect fossil fuels to become more expensive because there'll be a carbon tax or something.
And you put all of those in and you project it forward.
And it tells you what the lowest cost way of meeting your climate targets is with the assumptions that you've made.
And so I suppose my point is just that these are incredibly boring because everything is based on continuation of today's trends.
Everything is based on pricing.
Everything is based on that assumption about what the cost will be.
And there are choices about what we are going to cost into these models and what we're not going to cost into them.
So you can put a price on nuclear energy.
You can put a price on carbon removal from the atmosphere, somewhat speculative, but you stick it in.
And if that price is high, if you put it in at $500 per tonne of CO2 removed, then there won't be very much of it used.
And if you put it in at $2 per tonne of CO2 removed, then there'll be vast amounts of it in the model.
And so your choice of exactly where to put that price point, which is an arbitrary expectation of the future,
that's going to hugely determine what the outcome of the model is.
And of course, then there are other things which we might consider pricing in, but we don't.
So for example, what would be the cost of behavior change?
If you were willing to put £500 million into a behavior change program to reduce energy usage or improve energy efficiency of appliances or something,
how much would the cost per tonne of CO2 avoided be?
We don't really put that in.
If you wanted to consider geoengineering, for example, I think this is going to be the next big thing that comes into these models,
will be a cost not per tonne of CO2, but per tonne of CO2 equivalent by shading the atmosphere with stratospheric aerosols.
If we put that in, that would probably be pretty cheap relative to the other technologies.
And so as soon as you start putting it into the models, I think we will see just in the same way that we've seen models going from high dependence on renewables to high dependence on carbon dioxide removal.
They will go from high dependence on carbon dioxide removal.
The next step will be high dependence on solar geoengineering.
I think that becomes inevitable.
But the question is, are you going to put it in or not?
And what price are you going to put on it?
And you'll get the answer that you want.
If you want to see a world that has these things, then you put it into your model.
And then it goes into the IPCC report and it goes into summaries for policymakers and it gets taken seriously in the national media.
It becomes part of the national debate.
It becomes realized in a way that it wasn't before.
So carbon dioxide removal from the atmosphere 15 years ago that wasn't really being talked about.
And now it is and it's a major plank of all of the strategies for reaching the two degree Paris Agreement target.
And so it's there and it has come into the debate through the models.
And stratospheric aerosol geoengineering, I suspect, is going to do the same thing over the next 10 years.
It will start by being put into the models.
Then it will be talked about in the media.
Then it will sort of become more of a thing and people will talk about it.
And then it will start to become policy.
So these models have huge power.
They have huge power to change the way that people think.
I have several follow up questions to that.
So there are people out there that say that the predictive natural sciences and models are squeezing out the concurrent imaginative and humanistic accounts of social life and visions of the future,
aka climate reductionism.
So how do you think that that could be changed or is that an important issue?
Yeah, I think that's really true.
And I think that this framing of climate as being a technical problem and a sort of model problem that we must do all this prediction and we must have these massive computers to predict.
And then we'll be able to take action to reduce all of the impacts.
Yes, just unhelpfully reductionist that actually it's not just about climate and it's never just been about climate.
If you could click your fingers and have an infinite source of clean energy that didn't emit any carbon dioxide, you wouldn't actually have solved the problem.
You'd have created a whole load of other problems.
And so I think that's the difficulty that we have that if we are siloed into climate and we say the problem is climate and we have to solve this climate change problem,
then as you say, we squeeze out all of those other questions about what it is to be human and what the future is going to look like and what it ought to look like,
what we want it to look like and what society looks like and how we interact with each other and how we relate to the natural world.
And all of these questions are just kind of shoved to one side in the focus on modelling and predicting and the physics and the technology and the science of it.
We ignore the fact that actually we are humans and this is a human problem and a human question.
So I would like to see a working group four in the IPCC, for example.
They have three working groups and one is on the physical science, one is on impacts and one is on, well, I'm not quite sure exactly what it is, sort of technologies for mitigation.
But we need a fourth one, right? We need one on values and politics.
Why don't we have that? That would be wonderful.
I mean, it feels so much more important than these other ones which get so much traction and so much media coverage.
And yet we're not talking about the ways that people think about it and the framings and the way that people relate to each other and the different value judgments and the different politics that allows somebody who has the same facts available to them
and is perfectly intelligent and has the best interests of the future in mind.
Somebody like that to come to a completely different conclusion from mine about what the solutions should be and how we should act and what we should do to get there.
So do you know or know of Nora Bateson?
Yes, I've heard of her work. I don't know her personally.
So she's the daughter of Gregory Bateson, a famous ecologist. She does systems ecology work, but she uses something called warm data, which is taking the math and the science of a model, but then integrating the human reaction to it.
And you can't just observe something without influencing it. And so it's how do the humans interrelate with the model?
I can email introduce you because I think you guys have a lot in common, which brings me to a naive question.
Is there such a thing as a masculine model versus a feminine model? Because what you just described sounds more qualitative, value based, longer term, integrative thinking.
Are those terms used or is there something akin to that?
I think that's a good question. And yes, I suppose that what I'm doing in a sense is trying to come to a feminist epistemology of modeling, an idea of how we might do modeling, taking a more pluralist approach to different perspectives and integrating different perspectives
and thinking about how the human and how the real life and the real world are at the center rather than the model and the physics and the abstraction and the technology being at the center.
And yes, you could frame that as being masculine versus feminine approach. I think that's not really keen on the gender identification, but insofar as that goes, yes, I think that is what I'm doing.
And that pluralism warmth, human centered approach is what we need to make models that are applicable to all people and also to generate consensus as well.
To be able to say to people, this is why it's important rather than just saying, we have done the calculations and this is the optimal outcome and you have to take it now, whether you like it or not.
I think these are different ways of working. So I would really like to see more of that warm data, more of that consideration of how science and the human interlinked because the science can't exist without the human and there wouldn't be any point in it anyway.
The whole point of making these models or doing the science is to be able to interact more effectively with the real world. So the bottom line is that that's what we're trying to do.
I totally agree with that. But again, we could take that too far because if you add more and more human, you're going to have less and less true signal of what the model is trying to tell you. So there's that balance.
It's interesting you say that, Nate, because I have a section in the book that considers, I think, the orthogonality of usefulness and accuracy.
And so I have a slightly playful example of astrological forecasting and saying that essentially what you get out of astrology as far as I'm concerned, I know some people might disagree, is random.
You get a random forecast, you randomize the outcome and you will say, this is what, you know, here are the things that I should be thinking about, but it gives you a framework for thinking.
It gives you a framework for saying I need to think about X, I need to think about Y, I need to think about Z and I need to think about the impacts on these other things.
And actually, that can be useful, even if it doesn't actually contain any information whatsoever, just the sort of systematizing effect of having a framework to think with can genuinely be useful and genuinely lead to insight and genuinely help you to make better decisions.
And so, as I say, it's slightly playful and slightly tongue in cheek, but I think that certainly we don't want to throw away the information that you can generate from models.
But when we are thinking about these sort of speculative models of the future and of what the future should look like, we have to bear in mind that they really are effectively just mathematical encapsulations of expert judgment.
And so then we go back to the question of who is an expert? Who do you want to be telling you what your future should look like? Do you want it to be the sort of elite, well-educated, middle-class people in universities?
Or do we want it to be, in some sense, more democratic than that? What would a more pluralistic approach to modeling look like?
We don't want to throw away the baby with the bathwater because, obviously, at the other end of the spectrum, we have things like weather models and models of sort of ballistic motion that put people on the moon 50 years ago, and these absolutely work.
You know, they demonstrably work. They are demonstrably correct and useful and contribute to making better decisions and intervening in the real world more effectively.
But at the other end of that spectrum, it is effectively just imagination.
It's what you think it's your ideas, and just because they happen to be written down in a mathematical language, that doesn't make them any more reliable and trustworthy than one that is written down in a different way.
You know, you paint a picture or write a story or whatever.
Isn't that the core challenge at the end of the day? Is that we had imaginations way before we developed science, and therefore an astrological forecast is always going to have more signal to the average human than a climate model?
Maybe. I mean, I don't know. I think climate models have a lot of signal, and I think they can be very compelling, and the way that they're presented can be very compelling.
If you say, you know, here's a picture of the planet, and here are these bright red bits where something nasty is going to happen, and the blue bits where there's less change, or, you know, you paint these pictures of extreme events or wildfires, flood, droughts and all the rest of it.
I think that is actually a compelling story that can be told around these models, and as much as, you know, astrology, which I have to say doesn't appeal to me personally, but it's about the framing and it's about how it's communicated and the model is part of the communication.
So the way that the model is constructed and the kind of outputs that the model gives, the visualization that it permits, these are all part of the communication of the ideas of the modeler to whoever the audience is, whether that's policymakers or the general public or some specific constituency that they are wanting to talk to.
And yes, then there's the sort of reputational questions of who's making it and what their qualifications are and whether they have the right bits of paper and whether they've used the right kind of font to make it look credible.
You know, do you use a font, the same font used by a national newspaper, or do you use a sort of wiggly handwriting font? Obviously, these are, you know, it's trivial, but these influence the degree of credibility that you're seen to have.
So I agree with you on astrology. That's why I long ago gave up match.com. So does everyone humans, does everyone have the ability to understand models and think in systems? I know this is a speculative question, but there's something called the big five personality traits and other psychological metrics.
I'm just wondering, one of the big five traits is openness, openness to new experiences and new information. And on all those traits, by the way, I score in the 100 percentile and openness. So I can naturally look at a model and it tells me something about the world and I'm curious about it.
It tells me something scary about the world. I'm like, oh my gosh, I need to pay attention to that. Whereas other people might not be as open and anything new that pretends something different about the future. I don't want to look at that. I'm just thinking out loud.
Is there any psychological trait that map with the ability of a human to better understand models?
I mean, it depends what kind of models they are. So if it's a climate model, then yes, you know, you have to be sort of relatively numerate and have that kind of systematic approach and probably sort of elite Western education is more likely to give you that.
You know, it's not just personality. It's also how you're brought up. But more generally, I mean, I think anyone can understand models because models can be much more than just a, you know, this numerical story on a computer.
They can also be a work of literature. You know, I think if you want a different kind of climate model, Kim Stanley Robinson's Ministry for the Future novel, I think that that is a different kind of model in the sense of being a sort of expression of imagination about
what a future could look like generated with some degree of internal consistency and logic about how it might pan out and how the different elements would work together to make that outcome happen.
And in, you know, it has the same limitations. It is somewhat contrived and it perhaps ends with an outcome that isn't necessarily one that everybody would agree to be realistic.
And it doesn't represent certain things and it spends too much time on others. But that's exactly what all the other models do.
So Stan is going to be on this show in two months. We have a podcast recording date, but you're talking about Ministry for the Future. Is a book like that also a model?
Because you read the book and it's fiction based on science, tells a story, changes how you think about the world. Is that a model as well?
Yes. So in my conception of what a model is, yes, it is absolutely. And it is something that helps us to think, it helps, it's an aid to thinking, it's an aid to communication.
It maps out this future with some kind of systematic approach. And in that sense, yes, absolutely. It's a model in just the same way that the climate model saw.
So I care, as you and your husband, my friend Chris, know I care a lot about the climate, other species and the environment.
But I know that the broader system involves things like debt and currencies and aggregate human behavior and the metabolism of the superorganism and geopolitics.
And all of these things are likely to play a larger role in our future than concern and understanding about climate change.
So I'm just curious, other than perhaps hedge funds, are there any meta models that incorporate not only the complexity and magnitude of the IPCC climate model,
but then add on debt, oil depletion, geopolitics and all that? Or does that become so complex that it's impossible?
I think it's impossible to do it in a sort of systematic and quantified sense in a computer, in that conception of what a model is.
But if we accept that a novel can be a model, then your thoughts in your own head can be a model as well.
You have some kind of expectation or maybe a few different scenarios of how you think the future might pan out and those could incorporate.
Maybe in different scenarios there are different emphasis on the impact and the influence of debt or of human behavior or geopolitics, nuclear exchange, etc.
That all of those can be incorporated into your mental model of what a future might look like.
And you then use that to determine how you prioritize what you do and you use that to think about how you communicate with others.
And you use that to sort of structure how you behave and interact in this world in the present moment, which was all that we're able to influence.
But with the aim of generating the small butterfly effect or maybe the Hawke-Morth effect of looking for a better future.
And I think that those models that we carry around with us in our heads, they are just as important because they are part of the framing of how we think and how we act.
And the shared narratives and the shared framings are also extremely powerful and the way that those are then propagated through the media and social media and the way that they then influence how much larger numbers of people think and act together.
This podcast itself is an extrapolation and a journey of the model in my head with others.
And I think it's a real-time test of whether it will be more butterfly effect or more Hawke-Morth effect.
So, Erika, this has been great. I know you have kids waiting for your attention there in Wales.
If I could ask you some personal questions at the end of the interview, if you have a few more minutes, that would be great.
Given your work on these issues, do you have any personal advice to listeners at this time of global anxiety about climate, the economy, energy, geopolitics, etc.?
I mean, that's a hard one because obviously it depends who you are and where you're situated as to what will be the advice that speaks most.
And so I suppose I'd say seek out a diversity of people to talk with and to discuss these issues and understand their perspective and think about how you navigate the future.
And yes, there is global anxiety and the future looks difficult on a number of levels, but I think that doesn't mean that the individual future can't be bright and exciting and interesting.
We can strive to do what we can within the constraints and the limitations of the global situation, but all of the impacts are very unevenly distributed.
And one thing that we can do, I suppose, is to keep that in mind and stay positive about the ability to contribute.
Would you change or add anything to that advice if you were talking to a young human?
I don't think so. No, I think finding people that you get on well with and that you're able to talk to is really important because otherwise it's easy to sort of feel all of this like a ton of bricks on your shoulders and just get stuck.
But with other people, you can find something that you can do that makes the most of your gifts and interests and is satisfying, you know, is interesting for you and is a niche that you can occupy regardless of what happens to everything else.
I don't know. I mean, it's a really difficult question, right? I do get people asking me this and young people asking me this and we always have a really interesting conversation that goes in so many different directions.
And for a general audience, I'm not sure there is a generic answer, right? Because it depends who you are and what you want to do.
I think the answer you just gave is the right answer. You end up having a wide-ranging discussion that goes in lots of different directions and that is the answer because processing this and talking about it is the most helpful thing.
And as I don't talk about solutions anymore, I talk about responses and there are a million responses and there's lots of different people in different situations with different value systems and abilities, etc.
So next question, Dr. Thompson, what do you care about most in the world?
Well, I mean, the easy answer is my children and my family, but I suppose I thought about this a bit in terms of maybe what would I be most sad about losing and, you know, my children and my family come at the top of that list as well.
But the biodiversity of life and the kind of abundance and beauty and diversity of life I really care about deeply.
And so I had a conversation maybe 10 years ago with someone who's now moderately high up in this effective altruism community about sort of long-term futures.
And we were trying to think through together some of these questions about, you know, does it matter if the entire human population become extinct or what is the value of additional lives in the future and additional lives now?
And what's interesting about that was that this person and I came to really different conclusions that they went away sort of systematising something very specific about the value of additional lives in the future and how to add them up and what, you know, how you could do that.
And that led them down this path of thinking about the long-termism and that we should be putting more effort into avoiding catastrophic events rather than putting more money into, say, reducing poverty now.
And I think that, you know, that's really interesting discussion because I came 10 years ago to kind of the opposite conclusion that having done all of this and thought about it all, I decided that actually there was no consistent logic to be able to say, actually, I value this and I am then going to be able to write down a definitive calculation of what is the maximum value that can be generated from this planet or this universe and how we can optimise it in future.
You know, I came to the conclusion that actually we can't value lives in the future and lives now on the same scale. They are really not commensurable and that the only opportunity we have to act in the world is now and in the present moment.
So that actually the catastrophic risks in the future, they may well exist, but we can't weigh them in the scales with poverty now. It doesn't make sense to do so. It doesn't conceptually make sense to do so.
And then I was thinking about, you know, how you integrate a value for biodiversity or ecosystems or the abundance of life into that as well. And again, I came to the conclusion that really you can't do that.
I started trying to think about how you would systematise that. Could you say, for example, that the sort of developed stable ecosystems with greater biodiversity, because they have been stable for a long time, have greater value than the ecosystems that are full of pioneer species that are invading a new environment, which is rapidly changing?
And I kind of came to the conclusion that actually we can't really do that either. It just doesn't really make sense. You know, why should we value organisms that have developed to occupy a very specific niche? Why should we value them more than organisms which have developed to be effectively invasive species?
And so that kind of made me think, well, why, if I can't really work out what it is that I value, you know, I could say fuzzily, but I can't really say why a finch with a particular length of beak is more important to me than a rat or a cockroach.
So maybe maybe I'm overthinking it, but I'm not quite sure where you go. You know, I think it's an interesting sort of ecosystem approach to say, well, why, if the future is a changing climate and a changing physical circumstances, then the future of our ecosystems is change and a changing ecosystem implies on balance.
And it implies invasive species and it implies, you know, all of these aspects of change. Is that something that I should be scared of for its own sake? I'm not really quite sure.
I think I would summarize everything you just said, or most of what you just said by stating that ecology worldview and an effective altruism worldview use different mental models.
Yeah.
A few more questions. We've talked a lot about climate change on this conversation, but what could be that? What issue are you most concerned about in the coming decade or so in our world?
I suppose I'm most concerned that people don't feel like they have common purpose, that, you know, on a relatively, from a small scale to a larger scale, you know, thinking about the sort of global geopolitical situation, but also going all the way down as fractured communities
and the impacts of social media maybe on allowing people to spend less time talking and compromising with neighbors of differing views, that this has sort of eroded that sense of common purpose.
And I think in Britain, where I am, you see it really particularly starkly because after the Second World War and in the sort of 1950s, there was really this sense of common purpose and that built things like the British National Health Service and all sorts of things
and improved the life chances for people over decades. And, you know, I've read some quite coherent critiques of sort of British politics and society about what it is that has led to that fracturing and the loss of that feeling of common purpose.
And I mean, there's all sorts of theories, but I think I feel that's what I'm most concerned about is that people are struggling to work together and struggling to understand how to relate to people with even slightly different world views.
And so that's one of the things that I really appreciate about your work, Nate, for example, you know, your insistence on being apolitical and on hearing that diversity of perspectives, I think is just incredibly important.
Thank you. Do you have any stories or personal experiences, Erica, that have happened to you or you've witnessed that give you hope and enthusiasm on some of these issues for the next decade or so?
I mean, I think we can have hope and enthusiasm without having to have evidence for it. But yes, I mean, I think that the change in attitude towards climate, you know, but not just climate, you know, all of these issues, the change that's happened in the last 10 years has been really profound.
So for example, I went to the Copenhagen climate change conference back in 2008, and I came away feeling incredibly disheartened, you know, that it was sort of, you know, it was billed as a failure at the political level, but it was also so clear to me how boring and
unserious the talks were about actually doing anything about the problem in contrast to the enthusiasm and the imagination of civil society in the streets outside that conference center in Copenhagen in 2008.
And over the last 15 years then, since then, there has been a huge change. And I think, you know, people like Greta Thunberg kind of exemplify it and a part of the reason for it. But people are really starting to understand that it is starting to cut through.
People are starting to see that these questions are important, that they have short term and long term and geopolitical consequences, that they're not just a sort of academic ivory tower concern, and they're also not something that can be sorted with a flick of the hand by politicians or civil servants
just saying, ah, yes, here is technical solution XYZ and we'll just do that. So I'm excited by that. I think it's too slow, but who knows what might happen next. So I am cautiously positive.
If you were benevolent dictator of Britain or of the world, and there was no personal recourse to your decision, what is one thing you would do to improve human and planetary futures?
Well, I struggle to answer this question. I try to think about it. I don't know. I mean, I think somehow reducing the consumption of everybody down to a bare minimum, I mean, not a bare minimum, but a reasonable minimum.
And of course, for some that means increasing. And so some form of rationing or quotas or, you know, energy quotas or something like that. But of course, I think that would not be what a benevolent dictator would do.
Because it's too dictatorial. It is too, you just can't impose solutions from above. I think that there is no solution that a dictator could impose, which would actually work, that you have to generate enthusiasm for change from the ground up.
And it has to be, it has to be something that comes from everybody deciding that they want to change because the future will be brighter, not because they have to change, because if you don't do it, this will happen and that terrible thing will happen.
And so I would abolish the dictator and hope for the best. I would resign my position.
I like that answer. If I was in a small way, a benevolent dictator, I would change around the world how people say models and switch it to models in exactly your voice. I think that would be an improvement.
Thank you so much for your time today, Erica. And I learned a lot. I enjoyed this. And to be continued, my friend.
Thanks, Nate. It's a pleasure to have a chat with you.
If you enjoyed or learned from this episode of The Great Simplification, please subscribe to us on your favorite podcast platform and visit thegreatsimplification.com for more information on future releases.
Thank you.
