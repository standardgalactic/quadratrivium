Arwch, adshawddio'n gyd, ac mae gennym nhw yng Nghyddem kom checked mewnor.
Roeddwn ni'n medud i weld.
Roeddwn ni'n meddal'r gweithio девu swyddog josedaeth,
ond mae'n medemed yn ddiwrnod peol splendid,
Glir i.
Dwi'n mythio'n meddwl am sicher a gyda'r hyn o gweled i tu siaradau
overall sy'n hi fy nghyd switching wry eujęd.
I think cheekily called this a Masterclass. That's my word.
What I meant by it was it's an opportunity for you to interrupt me and ask questions
and we can have more of a discussion.
There's lots of time. I only have less than an hour's worth of presentation.
I'm hoping that this will be a vehicle for questions answers and discussion.
Please interrupt if I say anything that's not clear or remember it
gyda myleddenynion, a myledyn yn mud o gyntaf at y blynedd.
Byddwn i gafwch, mae this mae hynny'n f dermuddiaid specsau f knight頂fod dda chi
gallai i wneud am zeithio y f
Oes br lithom yn gwaith cyf supermarketol ym
ynghylch rŵ
oedd credu i chi'r rŵs o'n mewn cyblo
o'r b Lithorion Fiol
ryngawning
i gael rhoi這些nyd
i gael byw gw сво
byw gweld i drredyf!
Dych chi'n gallu
Deym里nio riso
a atniwch unions
o'rouxer arall
Wanth wedi have fi heddiw i ddisgriffa Peiwyr, tりn i wneud yn trawerfodol,
fel drwy ei bawb o gorwydd, felly byddai f gabayr rhagor.
29 iddyf yn gallu werth ail干 am wadinerchwil i'r cwmbi yn coneidiaeth i'u fällt 3 peroifau yn rhan o hynod
yn ysthaith gallu cyhoedd gwneud yma gymserfa niOR
hungnnodd ddweud at yna boardsawr wych,
bod eich niwn eichweith winnersâm o gynnigau yng Ngheif Egyptiol yn y blaes environments cymddian nhw'n tymhwy i nu syniad i hating yn hwnbeth o'r blaenil.
Gwt ydyni, yn cael ei bibliân, gan osach ein ll cymwyn substitute ar heddiw iawn atlunio'r linguad ac yn).
Felly, mae e adjust�….
..es i wedi'i cael enw ei mor ôl, ond anodd annddoedd switches eh workouts, gan wide mainly, gud εich cymwyn ti'n chynig,
er oed yn anodd Non Granir Bethe.
Ofu i dле!
Yn allan nhw decrease'r bwyszenieion
mae'r d hori gallu awd gyda weld serion
Llywagol�드eth Lyrens
ac mae weinig yn swimrwynt
o ran llwyrys Cymru
dangodon dangod ry
mae anawarel yn myth,
mae'r anywale yn eseithio.
Maen nhw'n dal caf fy學, a gwell excitingyyw ar gyfer y sympryniedad,
gyflymio'm시�nd iawn, ar iddynt â'i maes cyhoethau
Mae'n aumento i'n derbyn gweld gennym.
Mae'n ddiwedd eich abstractraf yn besio,
mae'r cynnyd ym mhawahig iawn,
rhywf yn ni'r cyflugau gwlyn,
Coding, Networks Neural and Clothes with a Demonstration of Active Inference with a particular focus on what we were talking about yesterday, which is resolving uncertainty, the epistemic, the knowledge-based, belief-based uncertainty reducing aspects of our behaviour.
So, I'm going to start with a question posed by Scrodinger. How can the events in space and time, which take place within the spatial boundary of a living organism, be accounted for by physics and chemistry?
We're not going to answer that question, but what we are going to do is to note before you can talk about a living organism, you have to be able to separate the living organism from the environment in which that organism is immersed, from the milieu in which the organism lives.
So, what separates the internal states of an organism from the states that don't belong to the organism, which I'll refer to as external states?
But even more simply, before you can talk about anything, you have to be able to define what distinguishes the states of the thing from states that are not part of the thing, or maybe no thing or nothing.
So, Scrodinger would be the first person to acknowledge that the boundary between the states of the thing and the states that do not belong to the thing is itself a statistical object.
And I'm going to take that boundary to be a Markov blanket. Now, I know that some of you know what a Markov blanket is. Could you again put your hands up if you know what a Markov blanket is?
Very good. Right. You, sir. Can you tell your friends what a Markov blanket is?
I have ordered.
What?
Yes, tell everybody else, not just what your one friend is sitting next to you.
We should include the neighbour of our system, I mean the parents of the system, and the child of our system, and also the parents of the system's child.
But I'm not sure why, but I remember that we should do it.
That's absolutely perfect. Yes, thank you very much.
So, I'll just go through that answer. Technically, the Markov blanket of some states comprise the parents, the things that influence me, my children, the things that I influence, and the parents of my children.
So, just to make that a bit more formal, if we have a little universe comprised of different states here drawn by the nodes or the circles of this graph, and influences, causal influences are denoted by the edges or the arrows of this graph, and we identify the internal states as something, say me.
Then my Markov blanket comprises the parents of me, the children of me, and the parents of the children of me.
Now, what that set of states does, it provides a statistical separation of the internal states from the external states, which are the rest of the universe.
So, by statistical separation, what I mean is a conditional independence.
So, put simply, what that means is if I wanted to predict my state, given the rest of the universe, I would only need to know these states here, the state of my Markov blanket.
All the information that is relevant to me is contained within these blanket states.
Technically, that means that the internal states are conditionally independent of the external states when I condition upon or given knowledge of the blanket states.
So, that's basically the Markov blanket.
Now, only about 20% of you put your hand up when I said, what is a Markov blanket?
I suspect you all know what a Markov blanket is, because you will have heard of Markov processes and you will have thought about how states change through time.
So, perhaps the simplest example of a Markov blanket is the present, this moment in time.
Because everything I need to know about the next point in time, about the future, is contained in the information now.
So, if I know exactly the state of the universe now, I don't need to know its entire past to predict the future.
So, in that sense, the present provides a Markov blanket or a separation between the past and the future.
So, we're taking that simple idea and generalising it to any states that may or may not comprise a system of interest.
So, I'm going to make another move here.
I'm going to...
You can interrupt you.
These arrows, do they have been in causation?
Yes, they do.
Yes.
So, statistically, that just means that the probability distribution here of this state depends upon knowing the value of this state.
So, to express that causal dependency as a conditional probability density, yeah, absolutely.
Not in the moment, no, no.
So, this is, if you like, one point in time.
I could actually write it out at time one, time two, time three, and then draw arrows forward in time as well.
And we will see examples of that.
In fact, we saw an example of that yesterday when we were looking at deep temporal models and we'd actually rolled out into the future states into the future.
So, if I was drawing the equivalent graph for a time series, it would be very simple.
It would be state at t, arrow to state at t plus one, arrow to state at t plus two.
So, it would be a very, very simple line graph.
And for any given state, the parents of, for any point in time, say t plus one, the parents are t, the children are t plus one.
And I am the parent of the children, so that doesn't count.
So, you've now got the Markov plunket on either side of me.
And that means I can ignore the future and the deep past.
But it reminds me when we see a graph with time on it, yeah.
Can I clarify, you say there's a motivation from the direction of Markov one that we need to separate internal states from the external states.
Yes.
But you also see that internal states here depends only on your parents, but not on the children and not parents of children.
So, why do we need children and parents of children?
Right, so that's a very interesting question.
The answer is usually given from the point of view of Bayesian inference.
So, the notion of a Markov plunket of this sort was introduced by a scientist called Pearl, Judea Pearl, in his treatment of causal inference in Bayesian networks.
And the answer is that if I know the consequences of something, that has implications for the alternative causes of something.
And that phenomena you will read about is called explaining away.
So, the canonical example of this is the lawn is wet.
So, I look out of my window in the morning and the grass is wet.
Now, I could have two beliefs about what caused that.
It could have been that it has just rained or it could have been that the sprinkler had been wetting the grass.
Now, if I now see the consequence, the grass being wet, and I know something about the causes, say it hasn't rained for ten days,
then that changes my beliefs about the cause, so it must have been the sprinkler.
So, knowing the consequences of something has implications for your beliefs about other things that could have caused it,
which is why you have to have the children and the parents of the children, the other causes of the consequences that you might be generating.
So, the Markov plunket is more than just what causes me.
It's a statistical shroud, a statistical blanket, a statistical set of states that in terms of beliefs and probability distributions
mean that I have everything that I need to know in terms of defining or specifying my current probabilistic state.
So, it's an interesting, if you read Wikipedia and look for explaining a way, it'll take you through a number of examples like that.
Is that all right? Good.
What I'm going to do now is actually split the Markov plunket into two parts,
and I'm going to consider half of the plunket states as sensory states if they are not influenced by the internal states,
and the other part of this bipartisan as active states if they are not influenced by the external states.
And the reason I'm doing that should become clear if we think about this dependency structure in things that we all study and know.
So, in summary, we have internal states, external states here, with the blanket states B being split into active states,
sorry, active states and sensory states here.
And I'm going to refer occasionally to all of the states that comprise a system as particular states.
So, you can think of this as a little particle, but I'll also be talking about very big particles like me,
but we still have this essential structure.
So, two very common in the biological sciences, two very common and important particles are brains and cells.
So, where is that Markov plunket in the dependencies that constitute a brain, for example?
Well, in the brain, we can associate all the neuronal states, all the connectivity, all the connection strengths,
everything you would need to know to write down exactly the state of the brain at this point in time.
We can consider the internal states of the brain.
And these internal states of the brain then influence the active states that could be our autonomic reflexes, our actuators,
the motor, the muscles that actually cause movement, our motor plant.
And these active states then cause changes in the external states.
It could be physiological changes, it could be where I'm pointing my eyes.
Things in the outside world are changed by the active states.
And then these external states that are hidden behind the Markov plunket, the sensory and the active states,
then cause changes in the sensory states.
And we can associate the sensory states with all our sensory organs, our visual organs, our chemo receptors, or auditory system,
that then influence our internal states, that then influence the active states, and so the cycle continues.
And this provides a nice metaphor for the action perception cycle.
And that cycle emphasises that internal states are coupled to external states through a two-way exchange across the Markov plunket.
But that Markov plunket still allows a separation between me and the rest of the world.
Exactly the same dependency structure can be found in a single cell.
So we can associate all the intracellular states, all the genomic, all the message pass in the proteomics,
all the electrochemistry of a cell with the internal states that influence the active states,
say the actin filaments that push the surface of the sensory states into the external world to cause changes,
say movement of the cell.
And those changes then are registered or sensed by sensory receptors on the cell surface,
and they will be the sensory states that cause changes in the internal states, that change again the active states.
The point here is it's exactly the same causal structure that enables an exchange between internal and external states
through the Markov plunket with the circular causality imposed upon it.
So that's the basic structure.
What I want to do now is to ask you to forget about the Markov plunket for a few slides.
What we're going to do is to do some fundamental physics, a very simple sort of fundamental physics.
And then we're going to put the Markov plunket back into the game to see what it means if we had a Markov plunket.
And that's when the interpretation in terms of perception and action as we know it as neuroscientists will become more apparent.
So I just want now to talk about systems, and I'm talking about particular systems that are interesting in the sense that they exist.
And by existing I mean that they have some characteristic states that they occupy over extended periods of time.
So there are lots of systems that don't exist because they dissipate almost immediately.
Before you can observe them or measure them, they have dispersed, dissipated, exploded, dissolved, disappeared.
But I'm interested in the kinds of systems that don't do that, that they seem to endure and have measurable characteristics over long periods of time.
And the way that I'm going to define that mathematically is in terms of them possessing characteristic states to which they look as if they are attracted.
So mathematically that's known as an attracting set.
So I'm talking about systems that possess an attracting set of states.
So to make that visually more intuitive, imagine that there was some universe and I take two states or dimensions of this universe,
say my temperature and my velocity.
And at any one point in time I am defined by a position in this state space.
And as time goes on I move around in this state space with the appropriate dynamics or kinetics and I trace out a trajectory.
So this could be me getting up in the morning, having a cup of coffee, doing my emails, going to work, having lunch, going back in the evening,
and going through my daily cycle and then getting up again in the morning carrying on.
Or it could be my cardiac cycle. This could be a QRS complex.
It could be reflecting the modes of the beat of my heart.
Or it could be a description of the weather that I experience during the year, you know, with Christmas and then Easter and the summer holidays.
The point being, at any temporal scale, in order for me to exist, I have to keep on returning to particular states that characterize me as me.
And that's the essence of having these attracting sets.
I have to keep on revisiting the neighbourhood of states that I am typically found in.
In fact, you can formalize that idea by associating the density of these trajectories on this, what is known as an attracting manifold,
with the probability that if you looked at me at any point in time chosen at random, the density corresponds to the probability of finding me M in that part of state space X at any one point in time.
And that's important, and I'll explain why in a second.
Just to explain where this trajectory comes from, this is, yesterday I was answering a question from the point of view of being a physicist and I said,
well, we start with the Launjvan formalism. This is it. It's a very simple assumption.
Everything that I am about to say and everything that I have said assumes that we can describe a universe in terms of the rate of change of states in that universe,
that are some very, very, very complicated, highly non-linear, very, very high dimensional function of the states that returns the flow.
So the rate of change, the velocity, the motion, the movement of states through the state space is a function of the position, the flow of these states plus some random fluctuations.
And that's it. That's all I'm going to assume.
And given this formalism, I can now, using the interpretation of this attracting set in terms of the probability of being found in any particular state,
I can now ask questions or I can write down what must be the evolution of this probability density.
So this is a bit technical. We don't need to understand it and I'll explain why in a second.
The key point here, if a universe like this exists, then there is always an equation that describes the evolution of this probability density p of x given me.
And that equation is found in nearly every part of physics.
So I've called it the Fokker-Planck equation here.
Some of you will be familiar with it as a master equation.
If you did physics, it would be apparent as the Schrodinger-Wave equation.
And as we'll see in a couple of slides, it even underwrites things like the Boltzmann distribution in statistical thermodynamics.
All we need to know, though, is that there exists an equation that describes the change in the probability distribution as a function of the flow F and the amplitude of these random fluctuations here,
which I'm denoting by omega.
Now you don't need to remember that because the important thing is we've just said that I'm only interested, or we are only interested in systems that have settled down to a non-equilibrium steady state because they have this attracting set,
which means that this probability distribution has stopped changing.
I exist like me for a long period of time.
I will be the same thing tomorrow and the day after and the day after and the day after, which means that the probability distribution itself does not change.
So if that's true, then this rate of change of probability distribution is zero. Why is that important?
Well, it's important because that means I can rearrange this equation to express this probability distribution in terms of the flow, the dynamics, how I move in this state space and the amplitude of the random fluctuations gamma.
And this is the expression down here.
As with yesterday, you don't need to understand the maths. All you need to know is the logic of these expressions and the logic, I repeat, is very simple.
We know that the rate of change probabilistically of the sorts of states that I will find myself in can always be expressed in terms of my flow or the way I move and the level of random fluctuations that cause variations in that movement.
If I exist to the extent that my probability distribution doesn't change over time, that means I must be able to express the sorts of states I find myself in in terms of my flow and the amplitude of the random fluctuations.
I've introduced something here called Q and I'll explain that in the next slide.
For those of you who are interested, this is using the solution of the Focke-Pank equation expressed using the Helmholtz decomposition.
But to be much less mathematical about it, imagine that I placed a drop of ink in a glass of water.
Now normally what would happen is that the molecules of the ink would be subject to these random fluctuations, these random molecular fluctuations, and it would disperse.
It would diffuse away until the concentration, that probability distribution, was uniform over the space of the container containing the water.
So I'd drop it in and then all the ink molecules just diffuse and disperse away.
Now that's not a system that shows this self-organisation to an attracting set, it's not the sort of system like you and me.
The sort of system that we're interested in is a system where it disperses and then seems to gather itself up again.
And then keep itself gathered together as if all the molecules were actually diffusing up concentration gradients.
So almost doing an anti-diffusion and this is the peculiar biological behaviour that we want to be able to describe mathematically.
And it's easy to describe mathematically and it's exactly what I described in the previous slide, but now we can see how this works.
So this equation of flow has two parts. It has a gradient flow and a solenoidal flow, which simply means that to explain this behaviour, to explain this anti-diffusion,
diffusing up concentration gradients, I have to have one component of the flow that pulls me towards peaks of concentration or probability density.
And then there has to be another component that circulates on the levels with the same probability distribution.
And that's all this Helmholtz decomposition says. It just says that if I exist, my states must on average flow up probability density or log probability density gradients
with another circular or solenoidal component.
Now I said before that that very simple and very fundamental equation, the Fokker Planck equation underwrites all the physics.
So I'm going to just very briefly show you why that is the case. Sorry, yes.
Right. Good question, but you're asking it as if you were designing an attracting set.
In this analysis, we're turning things on their head and we're saying if an attracting set exists, what flows must be there if the attracting set exists.
And you're absolutely right that you can produce a particular attracting set with just gradient flows.
But because we're going the other way around, we cannot exclude the possibility of the solenoidal flows.
And in fact, if you think about it, those solenoidal flows become incredibly important in understanding the real world.
So I'll just give you a couple of examples. Say you love gamma oscillations in autism, then the oscillations are an expression of the solenoidal flow.
If you were studying the heavenly bodies, planets, planetary motion, the very orbit of the moon around the earth and the earth around the sun is an expression of the solenoidal flow.
So this solenoidal flow becomes extremely important in terms of a potential property that systems that exist and can be measured possess.
So it's very, it wouldn't be disingenuous. It would be wrong of me to ignore it at this stage.
If you do machine learning or optimization in engineering, what this means is we are not assuming detailed balance.
Does that mean anything to anybody? Does anybody know what detailed balance means?
So it means nothing to you. Let's forget about that.
But there is a whole branch of optimization theory that ignores the solenoidal flow.
It's not useful for our point of view. We need to be able to talk about the solenoidal flow.
It's not going to figure very much here, but in real systems it becomes extremely important in terms of understanding oscillations, synchronization, circular motion, conservative dynamics and the like.
Is that all right?
So it's not just the rate, but secondary rate is higher order?
Absolutely. I mean, because you've written down this in terms of a flow, it will naturally have higher order derivatives to infinite order.
And it actually technically, practically becomes an interesting question if you want to now write down the flow operator here in generalized coordinates of motion.
So you could actually say that X is a position in state space, the velocity, the acceleration, the jerk and so on to arbitrarily high orders.
That becomes important in deriving Lagrangians if you're a physicist.
But for the moment, we'll just say X has everything that we need to know about it.
So let's just look at this from the point of view of a physicist.
So there are only a few things that we can do with this simple description of the universe, but those few things actually give rise to all the physics that we know at the moment.
The first thing that we can do is just say, well, what would happen if the amplitude of the random fluctuations disappeared?
So now we're assigning zero to gamma, the amplitude of the random fluctuations, then what is left is just the serinoidal flow.
And what does that look like?
Well, it is just an expression of Newton's laws of motion.
So all the Lagrangian, all the paths of least action that you will have done at school result in the serinoidal flow that survives when you've eliminated the gradient flow by removing random fluctuations.
So if I wanted to describe very big things where the random fluctuations averaged away, the motion of the planets or very massive balls, for example,
then because they're so big, the average random fluctuations can be treated as zero, and then what we get is classical mechanics.
And because they're classical and we're just moving around at the same level, the ISO contours of the probability, they're conserving the probability.
They're conserving the log probability or their energy, therefore the conservative systems.
So this is why you get to classical mechanics in the limiting case that you remove the gradient flows and just leave the serinoidal flows in play.
I could go the other way.
I could say let's ignore the serinoidal flow, let's assume detailed balance, let's assume a certain form of time reversibility, for example.
And if I do that, then I get to systems as the opposite of conservative systems.
Is it the same principle? Yes, absolutely.
Yes, it has to do with reversibility and that you can sort of go uphill and come downhill using the same path.
Because if you now allow yourself to wander around the hill at the same time, then you can't actually come back down exactly the same way that you went back up, unless you also reverse the serinoidal flow.
Yes, this is very much related to that.
So the opposite end of the spectrum of things we can talk about is when the amplitude of the random fluctuations is very big.
So gamma is very large, but q is very small.
And then we enter this world of detailed balance and dissipative systems.
And that becomes interesting from the point of view of thermodynamics, which is all about the amplitude of the random fluctuations.
So that is temperature.
So we were talking yesterday about where do the units of energy come from.
They come from the fact that temperature is just the amplitude of these random fluctuations.
And then we, from that solution to the Fokker Planck equation, we can now derive Boltzmann's distributions,
or more particularly we can derive all the fluctuation theorems that underwrite the first and second laws of thermodynamics.
And you will recognise some of these expressions from statistical mechanics and stochastic dynamics.
I can even, if I wanted to, just write down the square root, the complex square root of this probability distribution of the states that I could be in.
And that corresponds to the wave function in quantum physics.
And then that Fokker Planck equation becomes a Scrodinger wave equation.
And then from that I can derive all quantum electrodynamics and quantum physics.
So this simple expression of the universe in terms of its probabilistic disposition or structure underwrites all of physics as we know it.
The reason I'm going through this repertoire of different kinds of physics is to situate the sort of physics that we're going to be talking about.
And again, yesterday, if you remember, I was saying that the Bayesian mechanics we're going to be talking about is like a sister to other sorts of physics,
because they all depend upon or derive from this one expression.
But there's something different about the kind of physics that we're going to be talking about, and that difference depends upon the Markov blanket.
So just having a Markov blanket changes the gain quite fundamentally.
And it does so in a couple of ways.
First of all, it means that the internal states and the active states look as if they're performing a gradient flow,
a hill climbing on the same functional, which is the free energy function.
So it looks as though the states over which I have control, namely my internal states and my active states,
but not my sensory states, they're influenced by the external states.
So my autonomous states can be described as performing a gradient flow on the same quantity.
So it looks as if my internal organisation and the way that I act upon the world are trying to do the same thing.
Furthermore, this free energy functional, we will see, is just a function of the blanket states and the internal states
that we're going to interpret as beliefs about the external states.
So I'll say that again, we'll come back to this, but this is absolutely crucial for understanding
what this kind of mechanics or physics brings to the table that you don't get in these kinds of mechanics.
First of all, it means I can describe the behaviour, the autonomous behaviour of a particle, particular states,
in terms of a common mission to minimise in this lecture a free energy functional.
That is just a function of everything that I know about, basically my internal states and my blanket states,
where crucially we're going to interpret the internal states as parameterising beliefs about the external states,
and that introduces an interesting interpretation in terms of meaning, representation and information geometries.
So there are two important moves that are implied by having a mark-off blanket, by existing,
in the sense that my internal states can be separated from the external states.
The first is that internal and active states, namely autonomous states, share the same function of a function.
This variational free energy functional does not depend upon external states.
So that's why it's autonomous.
So the rest of this talk will really be trying to understand what that means in practice, what are these systems,
what behaviours must they possess if they exist.
To give you an intuition about that, let's first of all put the mark-off blanket back,
so we now have the two Helmholtz decompositions with the gradient flow and the solenoidal flow,
but now expressed in terms of the internal rate of change of internal states and active states.
I've just separated that basic dynamic, that solenoidal gradient flow into autonomous states,
dividing into internal states and active states,
and we're going to be associating that separation with perception and action.
Oh, sorry.
Could you clarify why you told that F is a functional, not a function?
Yeah, so technically a functional is just a function of a function.
So most of the operators you find in information theory and thermodynamics and indeed in quantum mechanics
are usually functions of probability distributions.
So, for example, entropy would be the expectation of a log probability distribution,
but because the probability distribution is itself a function of some state, for example,
what you've actually got is an expectation of a function log of a function.
So that becomes a functional.
Things like entropy, information, energy.
In fact, nearly every physics property, really, strictly speaking, is a functional.
Anything that's a function of a probability distribution,
which I sometimes will be referring to as a belief, a probabilistic belief distribution
over some variable can be called a functional.
It just distinguishes it between a simple mapping.
In particular instance, we're sort of getting ahead of ourselves here,
but the reason I call it a functional is that it's a function of the blanket states,
in the same sense that a flow operator is just a simple function that could have a tailor expansion,
a high order expansion of where I am in state space,
but it also is a functional of a probabilistic function over external states,
and that's where the belief gets into the game, and we'll come back to that later on.
So if this was a function, if I wanted to express this free energy as a function,
I would say B and the internal states mu, but I'm not.
I'm twisting the story here, which means that I'm writing it as a functional
of a probability distribution which will become our posterior beliefs
parameterised by the internal states and bound to the external states,
and this is what gives Bayesian mechanics the belief-based structure,
the information geometry that characterises it as distinct from the safe thermodynamics.
So in thermodynamics, you have Markov blankets.
They are the heat bath. They are the environment.
So you have the separation there, but you don't have the internal states
having beliefs about external states, whereas you do when you explicitly
consider the form of the Markov blanket.
So what will happen is that free energy functional will be based upon
or shown to be related to this quantity here, which is driving the gradient flow,
which is the log probability of being in any particular state,
or because we're talking about autonomous states,
the log probability of being in a blanket state.
So what is this log probability?
So I'm going to try and unpack it from a number of different perspectives,
which I'm hoping that most of you will recognise at least one of these perspectives.
So first of all, I've just said that this is a log of a probability distribution
over the states that I typically occupy, the states in which I am usually found.
These are the attracting states that constitute that are my attracting set.
So because they are attracting, this log probability, this energy function,
becomes a value function.
It just scores, it quantifies how likely it is I am in an attractive state.
So if I look at this flow, this gradient flow here,
it says basically that my autonomous states will look as if they are trying to maximise value.
And if I looked at that behaviour through the eyes of a psychologist,
or a behaviourist, or an economist, or an engineer,
it would look as though these systems are described in terms of reinforcement learning,
in terms of optimal control theory, or an economics expected utility theory.
So this gathering together, this gradient flow towards these attracting states,
looks as if there is a goal behind the behaviour, the average behaviour,
as quantified by the flow of these states.
Excuse me, what is B stands for?
B? Yeah, B are the blanket states.
So the blanket B are the combination of the active and the sensory states.
In this instance we can think of them as sensory data, or outcomes, observable outcomes.
They actually also include the active states, but they are like sensory outcomes.
And on condition that M exists, that I exist?
Exactly, yes. Well actually it's me, I exist, not you.
We'll come back to that in a second. Yeah, absolutely.
So assuming that there is a Markov blanket, so you can interpret that M in three ways,
and perhaps it's nice to talk about that now.
So I've been sort of jokingly calling it me, but technically what I mean by given M,
given there is a Markov blanket.
Now there has to be an attracting set for there to be a Markov blanket,
because the Markov blanket is defined by conditional dependencies,
and of course the conditional dependencies depend upon the probability distributions having stopped changing.
So just by having a Markov blanket, I am already, I already exist.
So given that there is a Markov blanket, it will have some blanket states, like sensory states,
and it will look as if everything that I do in terms of my internal and active states
is in the service of or is trying to maximise this log probability on this value function.
So that's one interpretation, a completely viable interpretation, but there are others.
Another interpretation is to ask, well what's the negative log probability of some blanket state
or some data given M, given a system with a Markov blanket?
So an information theory that's known as surprising or self-information, I'm just going to call it surprise.
And in fact, it's what this free energy functional approximates or provides a bound on.
So the free energy functional is either equal to or bigger than this self-information or surprise.
Minimising free energy or minimising surprise is the same as describing systems that are informationally efficient.
And we've seen these principles throughout, well certainly in the last century,
expressed in terms of the principle of minimum redundancy, the principle of maximum efficiency.
Indeed, the free energy principle can be thought of in this class of principles.
Informax principle by Ralph Linsker is just saying that inference and action and behaviour
or even the evolution of our sense organs can all be understood as trying to maximise the mutual information
between the inside and the outside, and we do it very efficiently.
And that all can be explained by this tendency to minimise or interpretation of this graded flow
as a minimisation of surprise or self-information.
Now if you did information theory or thermodynamics, you will also know, sorry, yeah.
Yes, does it mean that the less system changes, the less free energy it has? Yes?
Absolutely, yes. So that would be, if you like, the physicist's interpretation,
that the system may start off, say it's been perturbed by some random fluctuation,
or it's found itself in a different part of the attracting set.
And what it will tend to do is, on average, move towards the points of most likely,
the points on the attracting manifold, which of course are the ones that have the biggest log probability,
which have the smallest surprise, which have the smallest free energy.
So the system will naturally reduce its free energy as it gets back to its attracting set.
So this is not quite the same as the principles of minimum free energy in equilibrium physics,
because we're not talking about equilibrium physics anymore.
We've moved to non-equilibrium or far from equilibrium physics, and the reason for that is that these systems are open
because the Markov blanket allows exchange of energy and information across the Markov blanket.
So it's the equivalent of a steady state in a non-equilibrium context.
But the principle of minimisation of thermodynamic free energy applies,
and it is, if you like, the thermodynamic version of this more general principle of free energy minimisation.
So the system will always, getting back to its attracting set, minimise its free energy.
And you can see that here, which on a minimised free energy, which means that the surprise must be even lower.
And furthermore, if you remember, or if you knew, that the average self-information or surprise was entropy,
it looks as if, on average, these systems are trying to minimise their entropy,
minimise the functional that scores their dispersion.
So this is exactly the behaviour that we were trying to explain before with the oil drop that gathered itself together.
It's resisting the second law of thermodynamics.
So if you were doing synergetics, this is the holy grail of self-organisation,
is how do you resist the natural tendency to disorder, dispersion, decay, death, diffusion?
If you're a physiologist, it's just homeostasis. That's all it is.
It's just saying that systems that self-organise that have a non-equilibrium steady state,
self-organised to an attracting set, technically a random dynamical attractor,
must show some form of homeostasis.
They must keep the characteristic physiological states within some bounds that render them recognisable.
So I'm only me because I have a particular temperature and a shape and a morphology.
You change any one of those things outside my comfort zone and I will cease to exist.
I will decay, I will die, I will be squashed, I will explode.
The last interpretation, and that average is expressed in terms of expectation over time,
the last interpretation which is the one that I want to pursue is just the probability of some blanket state,
some sensory data given a Markov blanket interpreted in terms of the Markov blanket standing in for a model of the outside world.
So now this will come back to what is M. Is it me or is it you or is it the Markov blanket?
What I'm going to do from now on is M is a model and it's going to be a generative model.
So what we're now saying is it looks as if all systems that exist in the sense of possessing a Markov blanket
look as if they're trying to maximise their model evidence.
So this is Bayesian model evidence, also known as the marginal likelihood, also known as integrated likelihood.
I will call it model evidence, which is nice because what that means is that we can understand this basic behaviour
from the point of view of the Bayesian brain hypothesis, evidence accumulation, predictive coding,
and a whole range of predictive processing interpretations that are now popular in cognitive science and cognitive neuroscience.
In the sense most of what I'm going to say from now on will be taking that perspective,
but before we do that I just want to illustrate this minimisation of free energy
as we self-organise through an attracting set and what it looks like in terms of the coupling or the representation of things out there
in terms of dynamics inside, in terms of the internal states.
So what we've done here is to simulate a little primordial soup, a collection of 128 macromolecules
that have autonomous dynamics, in fact the dynamics would be based on equations of flow
that are the same used to model weather systems that are rents attractor.
The actual equations are completely irrelevant, you get the same kind of behaviour for any set of coupled dynamics.
Here each molecule is equipped with strong repulsive forces depending on the distance
and a weak electrochemical attraction that depends upon these autonomous electrochemical dynamics written down as a rents attractor.
So why have I done this? Well if I just let this system evolve it will settle into a non-equilibrium steady state.
It will just keep bubbling around here like an active matter or an active soup.
But because I have written down the equations of motion that means I know the arrows that define the Markov blanket.
I know exactly what influence is what, and if I know what influence is what
I can write down an adjacency matrix of what states influence which other states.
I can look at the parents, the children and the parents of the children.
I can define a Markov blanket operator and I can go in and identify any internal states and their Markov blanket
and see if there's a little creature living in this soup that has a Markov blanket.
And indeed there is, in fact there are hundreds of them.
But the one that I've shown you here is based upon the blue internal states that are surrounded by the red active states
that support the magenta sensory states.
And then this little creature, this little virus or prion-like particle,
just sits there wiggling its tail, its cillium here, sitting in these molecules that are the external states.
So this little creature is living there, that's its characteristic non-equilibrium steady state.
Those are its attracting states.
And I can now ask questions of this synthetic virus.
Does it represent, does it self-evidence, does it make,
can I interpret the internal states as gathering evidence about the external states?
So one simple, oh sorry.
How do you define external states?
Good question, yeah, yeah.
How do you decide which of those are internal states?
In this instance I used the principal eigenvert of the grappleplast of the adjacency matrix.
If you, does that mean anything?
No, so there are simple rules.
Just heuristically then I identified the eight states anywhere that were most densely interconnected causally.
I took the matrix of causal influences and set the values to naught or one depending upon whether there was some dependency that is determined by the proximity of the molecules over time.
Is there an additional heuristic that is not connected to anything?
Yes, yes, yes.
Which is why I said there are hundreds of little creatures here.
So for every set of internal states there, well that's not quite true.
For a large number of combinations of internal states there will be a Markov blanket.
And everything that I'm going to show will be to a certain extent true of all of those different combinations of internal states and their Markov blankets.
So if you just imagine yourself being a composition of Markov blankets and Markov blankets and Markov blankets, you could say, oh I am interested in me as a Markov blanket,
which means that I've been looking at all the surface states and active states that constitute you.
But you could have said no, let's just look at my hippocampus.
And then the hippocampus would have had a Markov blanket.
And you could say no, let's look at a particular cell in the Markov blanket.
And then that cell would have had its own Markov blanket.
And then so I'm right the way down to the quantum level.
So the system really is comprised of Markov blankets and Markov blankets and Markov blankets at different scales.
And of course you are at liberty to choose the scale that you want to choose.
It means that we have no first principle to define what the organism is because it has a setromium in the state.
Yes, absolutely.
So there is a certain scale invariance here.
The sorts of random attractors that we're talking about will have a nested scale invariant property.
So for you to exist with your Markov blanket means that you must first of all,
that is a necessary condition for your cells to preserve their own Markov blankets, to organise themselves.
So the fact that you have a homeostasis is a necessary condition for all your cells and their Markov blankets to survive or to exist.
In exactly the same sense, for you as a person to exist, you have to live in a stable society.
And you have to live within certain climatic, social, political bands.
So your existence as a Markov blanket also depends upon there being a bigger Markov blanket,
a Markov blanket around this institution, for example, or this nation or this biosphere.
So there are Markov blankets within Markov blankets.
So we're talking about incredibly complicated systems where you're right,
there is no one unique Markov blanket or level of analysis.
So a related idea here is that due to Ernst Mer, that there is no unit of selection.
Selective pressure in evolution can operate at multiple levels from the cultural level
and say evolutionist psychology right down to the genomic level.
The same sort of argument supply here that we are here just focusing on one particular Markov blanket at one particular scale
to illustrate the properties.
But the same kinds of math and mechanics will apply to every different scale
and have to for the scales below to exist.
Does that make sense?
So this is the scale we're looking at, which can be thought of as the scale of a small virus, if you like.
Or it could be an institution or a country. It depends on how you want to interpret it.
But what we want to know is what it would look like if it is the case that these internal states
are forming or gathering evidence about and becoming a model of the external states.
Then I should be able to predict what's going on on the outside of the Markov blanket
just by looking at the internal states.
So if you were an electrophysiologist, that would be the same as saying,
I expect to find electrophysiological correlates of some, for example, visual motion in the visual field.
I'd expect to be able to see evoked responses if I just changed the outside and you could sense that
and then changes on the inside were in effect gathering evidence for changes on the outside.
So what we've done here is ask whether there is some mixture of time delayed internal states
that can predict the motion, the physical motion of the external states.
The predictability is shown by the intensity of these cyan colours here
and the activity of these time delayed internal states is shown down here.
The actual motion is shown in a dotted line and the prediction in the solid line.
It's very easy to find this predictability.
It's very easy to find mixtures distributed, if you like, representations.
There could be neuronal representations or visual motion, for example,
which look very similar and detect these catastrophic events where this molecule here was kicked out of the soup
and then fell back in again.
We can see almost by eye the fluctuations in the internal states
that start to have a correspondence with the sorts of fluctuations that we measure,
say with event-related potentials or local field potentials or evoked responses.
But there's an interesting aspect of these synthetic, say, neuronal correlates of something on the outside.
If you look closely, it looks as if these internal states start changing before this event on the outside.
So that poses a question.
Are the internal states causing fluctuations in the external states through the active states?
Or are the external states causing, through sensory impressions, changes in the internal states?
So who thinks it's from internal to external?
One, two, three, four.
Oh, you spoiled it. That was the answer, yes.
So I'm teasing you, I'm joking.
So it's both, obviously, and as you say, they are entangled.
It's interesting you should use that word because quantum entanglement speaks exactly to that conditional dependency.
Mediated by the Markov blanket.
I was going to describe that.
So the right answer, I'm afraid, was both.
It's both cause and consequence because of this action perception cycle.
There's a circular causality here.
And that has been something that people have been talking about for hundreds of years.
Christian Huygens first noted this when looking at old pendulum clocks that used to have,
that when suspended from the same wall or the same beam,
would, after a period of time, always come to oscillate in synchrony.
That was there, if you like, non-equilibrium, a steady state, a retracting set,
was this manifold where they were synchronised.
Another example of the importance of this solenoidal flow.
And indeed, this is a little drawing here by Huygens himself,
showing the two clocks with the pendulum and the pendulum
and the beam from which they are suspended.
And from our point of view, we can associate one clock with the internal states,
the other clock with the external states, and the beam with the blanket states,
which are mediating the coupling.
And it is almost inevitable that there will be a form of generalised synchrony
between the internal and the external, or in this instance, the two clocks.
And I'm using this example just to emphasise the symmetry here.
Mathematically, there is no difference between external and internal.
I could swap them around. It's the same Markov blanket.
Which means that if you interpret this Bayesian mechanics as you trying to make sense of your world,
you are also committed to the view that your world is trying to infer and understand you.
And that sounds silly when you first say it,
but in fact, there's a lot of interest in understanding how eco-niche construction
can be understood in terms of the world learning about you.
My favourite example here is elephant paths.
Do people know what elephant paths are?
Can you put your hand in the air? No.
This is, I think, maybe a Danish expression.
So you're walking to work and you have to cross a patch of lawn, some grass.
And the concrete path goes around the corner.
And the shortest way to the cafe that you want to buy your cappuccino at
is to cut across the grass.
And so many people have done it.
They've left a little muddy track for you.
That's an elephant path.
It's been carved out by creatures like you all wanting the same thing
and just wearing away the grass to leave a sign.
So from the point of view of the environment, though,
that is a physical manifestation, a plasticity in the environment
where the environment has started to learn about what is typical of the behaviour
of the phenotypes that it plays host to.
So, in a way, from the perspective of this formulation,
the environment now becomes the internal states
and you become the external states
and the environment is learning about you
just by these elephant paths.
And if you take this right the way through to eco-niche construction,
if you look around you,
everything you see has been made by somebody like you.
Everything from the projector to the walls to the straight lines.
Everything is all being constructed by things like us.
But from the environment's point of view,
it's just now remembering the sorts of things that people like us like to build.
Mathematically, this is another expression of this generalised synchrony
between the environment and the inhabitants,
the denizens of that environment.
I think that's quite interesting.
It's not what I work on, but it's an interesting symmetry.
Anyway, so let me summarise all the hard physics part of this.
All we are saying is that the existence of a Markov blanket
necessarily implies a partition of states into internal states,
the Markov blanket states,
namely the sensory and active states,
and the external states or the states that are hidden behind the Markov blanket.
And because active states change but are not changed by external states,
they minimise the average surprise or the entropy of internal states on their blanket,
and this means that action will appear to maintain the structural
and functional integrity of the blanket states.
And some people have spoken about this in terms of self-assembly in computational chemistry,
self-organisation in non-equilibrium and steady-state physics,
and auto-poesis, self-creation in the biological sciences.
Internal states appear to infer the hidden causes of sensory states,
basically by maximising marginal likelihood or Bayesian evidence,
and influence those causes through action,
and that's what I'm referring to as active inferences.
Perceptual synthesis, optimisation of the evidence of my model of the world,
that is active to induce this generalised synchrony that we've just been discussing.
Oh, so this is a brief interlude.
This is a picture of the daughter of one of our students who went to America a few months ago,
and his wife, after having had the baby, bought the little baby a blanket,
and this is Markov.
So this is the Markov blanket.
You can buy Markov blankets in America now.
So I was sent this by the father,
and this is Little Kira making inferences about her world through her Markov blanket at two months.
Right.
So for the rest of the talk, the last half of the talk,
we're going to effectively say exactly the same thing,
but using a different rhetoric, a different sort of language,
a more heuristic sort of language,
that reaches the same conclusion,
that starts off with an assumption that we are self-evidencing Markov blankets, basically.
And much of these ideas are nicely illustrated by this 16th century oil painter,
who was famous for painting still life pictures,
that when viewed from a different perspective, have a very different interpretation.
If you now see a face there,
the important thing is that you made that face.
It's not out there.
You made that face as a model or a hypothesis
that best explains the most likely pattern of stimuli that you're currently experiencing.
So that's the basic idea.
We heard yesterday about inside out versus outside in,
and this circular causality, this action perception cycle,
emphasises the constructive aspect,
and the importance of being able to generate models of hypotheses
about what could have caused the sensory impressions.
And those ideas have been around for a long, long time.
You could even argue from the students of Playtone.
But to my mind, most clearly articulated by Hermann von Helmholtz,
with phrases or sentences like the following,
objects are always imagined as being present in the field of vision,
as would have to be there in order to produce the same impression on the nervous mechanism.
So again, what he is saying is,
they have to be on the inside to be perceived
in the sense that you're comparing what you're actually sensing
with what you would have sensed if you were looking at this sort of thing
or that sort of thing.
So the idea is that you're using sensory evidence,
your blanket states or the sensory component of your blanket states,
as a source of sensory evidence to confirm or reject internal hypotheses
about what could have caused those sensations.
And of course, that's very, very close to 20th century psychological thinking
about perception as hypothesis testing.
Again, this notion that perception is just the art of disambiguating
or selecting among different hypotheses.
And you can fool the brain using illusions into selecting the wrong hypothesis
if you're careful in the way that you provide that sensory evidence.
And those ideas have been taken and combined with Bayesian probability theory
and the variation of free energy of people like Richard Feynman.
But I should also acknowledge the same physics story that comes from
a homograph algorithmic complexity.
Combining these ideas in the service of machine learning and artificial intelligence
to propose a Helmholtz machine and a Bayesian brain
that is exactly the same as the perception side of what we've been talking about.
So let's come back to this notion of impressions on the nervous mechanism.
From our point of view, these are sensory impressions on the sensory part of the Markov blanket,
a sensory veil that separates the inside from the outside,
the internal states from the external states.
So it would be like receiving some impressions or some shadows on our sensory sheet.
And we have to or we are compelled if we exist.
It would look as if we are trying to model or explain
what caused those particular sensory shadows or impressions.
So how can this be done? How would it be done by a brain?
Well, it's much easier to understand than you might think.
We already know how it's done in broad terms.
We know it must be done by a gradient flow,
which may or may not have solenoidal parts to it, on this free energy functional.
And I'll explain why that is the case a little bit later.
So we know that this must be true.
The internal states must be changing according to this gradient flow
on this free energy functional that stands in for this value or surprise.
And what we can do is rearrange this equation
so that the solenoidal bit becomes the prediction
and the gradient bit becomes an update term
where I have expressed the update term, the gradient part,
in terms of something called prediction error.
And I've also waited by its precision.
So what's a prediction error?
Well, a prediction error is just the difference between
what I expect to see if my expectations are right
and what I actually see.
So let's just go through this slowly.
Let's imagine I had these sensory impressions.
This is my pattern of retinal input,
and I wanted to infer or perceive the causes of that input.
And I may have an expectation encoded by my hidden states that it was a dog.
And if it's a dog, I can, using a generative model,
I can generate the sensations, the sensory states,
that I would have seen if it was a dog.
And I could use those predictions,
send them down to compare with the sensory states,
and then take the difference,
which is the mismatch or the prediction error.
And all that this gradient flow is saying
is I'm trying to resolve or minimise
the precision-weighted squared prediction error,
just as if you were estimating the mean or doing a t-test.
So what that means is we can interpret anything that exists
as trying to minimise prediction error.
And if it does that, then that's all that it needs to do.
Now notice we're not saying it ever knows the external states.
We're not even saying that there are external states.
All we're saying is that we've got a good explanation
for the external states.
The actual cause of my sensations may be completely different.
But if I have a good enough explanation for them
that minimises my prediction error, that's okay.
That's all that this gradient flow is describing.
So if that's true,
it means we can forget about all of that complicated physics
and just summarise the imperatives for action and perception
through the minimisation of prediction errors.
And that means that there are two ways of doing that.
We can either change our brain states or internal states
that encode expectations and beliefs about the causes of our sensory states
to make our predictions more like the sensory states
that we are trying to predict.
And that will look as if the sensations are causing changes
in our beliefs' driving perception, belief updating,
and in particular, Bayesian belief updating,
using the equation on the previous slide,
which for those of you who are not engineers,
is actually a Kalman filter or a predictive coding equation.
But there's another way that I can minimise prediction error.
Instead of making my predictions like the sensations,
I can make the sensations like the predictions.
I can change my sensations to make them more like what I predicted.
I can literally palpate the world in a different kind of way
to produce outcomes that I predicted.
So almost using my actions, my motor system, my autonomic system
to fulfil my predictions.
And again, we come back to homeostasis.
What is homeostasis?
It's just basically using feedback to adjust bodily states
in the outside body with reference to some top-down predictions
or set points that I think I should be occupying.
For when, what is the difference between those two models?
Should we use perception in changing our beliefs in one case
and changing the world in another case?
In terms of biological imagination,
what are the different functions
that define these types of...
...remoting these models?
I think the manifestation of that would depend very much
upon the geraintiff model and the priors
and the biophysics of the system that you're interested in.
Mathematically, they're both occurring hand in hand.
They're both occurring together.
So if you think of this in terms of the very simplest sort of active influence,
a Bayesian thermostat, for example,
then it is sensing the temperature
and it is trying to adjust the temperature at the same time course.
But as we discussed yesterday,
there may be more complicated...
...maybe more complicated scheduling of action and perception
in higher-order organisms.
But mathematically, they're occurring as part of a circular causality,
part of the cycle coincidentally.
So they're both trying to move in the same direction,
flowing up the same gradient from orthogonal directions
in terms of the internal states
and in terms of the outer states.
My question is that actually an organ in the brain should decide
either it should recheck some hypothesis about current state of the world,
or to do some action to align the sensory state within its expectations.
So how the brain chooses between these two different modes of operation?
Right.
You see here that in one case, in the first case,
when I'm just rejecting hypothesis,
it's like I'm learning or I'm trying to understand current situation.
In another case, it's like acting like a goal directly behind me
when I have a goal and I'm trying to produce actions
which will lead to this goal.
In this sense, I understand the current state of the world.
It's matching my current beliefs,
but this is not the state I want to speak.
Right.
So I think that question,
because you're talking about choosing whether to change my beliefs
or try and change the world so I don't have to change my beliefs,
because you're using the word choose,
you've made a jump to the end of the talk.
At the moment, we're talking about action as an instantaneous reflex
of the sort that a thermostat or a virus or a knee jerk reflex would have.
So there's no generative modelling of the future consequences
of different potential actions from which I have to choose or select.
So I think your question becomes much more potent, more important
when we get to the notion of selecting among different sorts of action
that has consequences in the future.
And we will see then exactly the phenology that you're asking about.
It depends very much upon your prior beliefs about the consequences of action.
So if you have a particular set of prior beliefs that something cannot be changed,
then you're likely to update your beliefs about the state of the world.
But if it is easy to change something,
you will probably just change it without being aware of it.
So, for example, things like the oxygen concentration in your blood
can be quite easily changed by changing your respiratory rate or your breathing rate.
So then, at a level which you're not thinking about,
you will be acting and that acting will be of a homeostatic sort.
So you will accelerate your breathing or your cardiac rate
to restore the supply of oxygen to various parts of your body.
On the other hand, there will be other consequences
that are more anthropomorphic that you may even be subjectively aware of
that are not reflexic and do involve the consequences of behaviour
minutes into the future, sometimes hours into the future.
So we'll come back to that.
In this talk, we won't be talking so much about homeostasis,
but gathering information as opposed to resetting some preferred state of existence
for the same principles apply.
What we will see is exactly the scheduling of perception,
belief updating, action to get some new data, perceptual belief updating,
action, perception, action, where the temporal scheduling is actually separated in a cycle.
But at this point, we haven't got that far.
All we are doing now is trying to see the phenomenology from the point of view
of the biological sciences and in particular neuroscience
that we believe must be there if we were physicists studying systems
that are equipped with Markov blankets.
So we're talking about sub-personal, non-conscious level
that you could associate with a virus.
Sorry.
Am I right that in case, I mean, this algorithm, this one,
does not change in case of human metacognition.
It means that the steel difference between internal and external states in metacognition.
I think this, yes.
To answer your question, this algorithm, which I repeat,
if you're an engineer, you'd recognise as a Kalman filter.
If you were a mathematical psychologist, you would call predictive coding.
When you get into the hierarchical nature of the genetic models,
then you necessarily get into the realm of metacognition,
and these dynamics do occur in the sense that if I am doing belief updating
on the basis of messages from lower parts of my model
and those lower parts are part of me and my brain,
then you now have beliefs about a belief
and that would correspond to a form of metacognition.
You could even go as far as saying that within the brain
there are Markov blankets that divide different hierarchical levels
of, say, cortical hierarchism.
You could now ascribe the visual cortex as external states
from the point of view of the prefrontal cortex.
People are thinking about partitioning neuronal networks
using Markov blankets, where you could certainly talk about
internal states to a particular system,
say, for example, a working memory system,
or a planning system, or an emotional system,
and external states to it.
A form of, it's not really modularity,
but it's certainly using the technology of conditional independence
and Markov blankets to think about message passing
in a sparse network or graph like the brain.
I think it's probably easier, though,
to at the moment just consider all brain states being internal states
and external states being body states,
states of the body and what's on the outside.
Think about the hierarchical structure of the internal states
from the point of view of a whole person.
We might see that.
In fact, we're going to see exactly that now.
If we want to use this gradient flow
that now has been interpreted in terms of Kalman
or Bayesian filtering and or predictive coding
as a metaphor for message passing or belief updating,
Bayesian evidence updating accumulation in the brain,
then we have to have this generative model
that's generating the predictions
that are required to form the prediction errors,
where the prediction errors are now used
to update the expectations,
or the beliefs about what caused the sensations.
So that's, again, another form of circularity.
Crucially, these generative models have to be fit for purpose.
They have to be appropriate for the kinds of worlds
that are going to be modeled,
which means they're going to be very deeply structured.
They're going to be very nonlinear.
They're going to be very hierarchical
with multiple timescales in them.
So that means you might expect to see the dynamics
in that gradient flow among the internal states
reflect the causal structure of the way
in which the sensations were ultimately generated.
So let's just take a simple structure of external states,
which here I'm encoding with v,
subject to random fluctuations.
And let's say we were trying to understand
the generation of sensory signals on the retina,
on the foveal portion of the retina,
which has a high spatial resolution.
So if I wanted to generate predictions of,
or just generate sensory impressions like this,
I'd have to know all the causes of those sensations.
I'd have to know what I was looking at,
or who I was looking at.
I'd have to know all about the trajectories
of my eye movements and where I was looking
at a particular time.
And I'd have to cascade them down this deep
or hierarchical model, adding in random fluctuations
at different levels of analysis,
right down to the sensory data themselves
that then would be provided to a brain
that was then trying to reverse this.
So mapping from causes to consequence,
that's a generation, a generative model of the sensations.
But then we can use our gradient flow
to think about going the other way,
going from the sensations back to what
and where caused those sensations.
And what happens is when we apply this gradient flow,
this predictive coding scheme
to these hierarchical generative models,
you get a very interesting architecture
that looks almost exactly like visual hierarchies
in the brain.
And what happens is that the causes,
the hidden states, the external states
are replaced by expectations encoded
by internal states, mu for mean,
and the random fluctuations are replaced
by prediction errors.
Another important difference is,
as before, the expected causes cause consequences.
But they do it through the prediction errors.
So we have our descending predictions
from one level here that are compared
with the sensory input to form the prediction error,
and the prediction error is sent back up
to the expectations to form a better expectation,
technically basing belief updating
to minimise the prediction error,
and then this happens at each and every level
until you have an explanation at multiple levels
of abstraction for the causes of your sensory input S.
And in that sense, I think you could,
in a simple way, think of these as metacognitive,
because these are expectations about something down here
which are other expectations of a lower,
more elemental sort.
It's not quite what you meant about, I think,
but at least mathematically you've now introduced
the notion of beliefs about beliefs of a simple sort.
So, but it's still within the context
of this fundamental gradient flow.
It's just that we've now got a bit more real
when thinking about the form of the genetic model
that we're applying this gradient flow to.
And just to take that a bit further,
to emphasise the flows that are implied
by this kind of scheme can be summarised,
as in particular coding at least,
as descending predictions from the outside in
that are being informed by ascending
from the sensory parts of the Markov blanket,
prediction errors that improve or update
those expectations that are providing
the descending predictions.
So, just another sort of concrete example.
Let's take visual input here, coming from the retina.
It's taken to the lateral geniculate nucleus.
But if the brain's providing predictions,
say from early visual cortex,
of the pattern of retinal inputs here,
then these inputs are compared
to the top-down predictions to form a prediction error.
The prediction error is then sent up.
It's the news that hasn't been explained.
The interesting part of the sensory information
to cause changes, belief updates,
revisions of elemental sub-personal beliefs
in early visual cortex
that provide better error-canceling predictions
at the level of the lateral geniculate nucleus.
But these predictions, these expectations,
are themselves being predicted by a higher level.
So, these expectations are compared
with the predictions of these expectations
to form a second-order prediction error
that's then sent back up to revise beliefs
about some other more abstract attribute
that could be causing this pattern of expectations
that itself was causing this pattern of sensations
and so on and so on
to any hierarchical depth that you wanted to consider.
There is, however, another sort of input
that the brain has to deal with or does deal with,
and that's input from the muscles.
Say, for example, the eye muscles
that's recording this stretch of the eye muscles.
That comes in, it's receiving top-down predictions,
there's a prediction error.
Now, this prediction error could be sent back up to the brain
and cause a change in beliefs
about where my eyes are pointed.
Now, here's the important bit.
These prediction errors can also be resolved
by being sent back to the external world
to cause a contraction of the muscles
so that the signals, the proprioceptive signals
match the top-down predictions.
So, here's the thermostat.
Here's the reflex trying to fulfil
the top-down predictions
to eliminate the prediction error
by actually changing the world.
So, here, what I've just described
is just a classical reflex arc.
This is how we actually move our muscles.
So, we just send down set points,
equilibrium points, where peripheral reflexes
mediated by alpha, motor, neurons
actually cause a contraction until the pattern of contractions
and sensations that we have from our muscles
matches the top-down predictions
and the prediction error has gone away.
Now, that's not to say that these top-down predictions
mean that this motor control is open loop
in the sense that it's not sensitive
to the consequences of movement.
It is open loop from the point of view
of the reflex in the spinal cord,
but it's not open loop in terms of the entire system
because these top-down predictions
are deeply informed by all modalities
because you've done the perception side
of this message passing.
So, here's your method cognitive bit coming in.
So, these top-down predictions
are deeply contextualised.
They encode the predictions about
what I would sense with these muscles
if I was engaged in a very complicated plan
about going to the restaurant, for example.
So, that's the active bit
in terms of what it would look like
from the point of view of a motor physiologist
if you were just trying to minimise
your prediction errors by moving.
It's not abstract making the world the way you want it
by choosing your friends or building your house.
This is much simpler than that.
It's just moving so that your muscle sensations
match what you predict.
So, we'll finish now with action and perception
and move on to the level of analysis
and sorts of genetic models
that we'd need to talk about proper metacognition
and selecting among different action plans.
Before I do that, let me just give you an example
of that very reflexive, direct active inference
without any planning or choice
and see how far we can get
in using those equations, that gradient flow
to reproduce likely behaviour.
So, a lot of interesting behaviour can be simulated
just by having a sufficiently plausible
and complicated gerontim model.
So, what we've done here is build a set of
basically a central pattern generator
or a dynamics high in the gerontim model
that, from the point of view of the gerontim model
generates points in some pointing space,
some Euclidean space that attracts the finger.
So, the agent has this gerontim model
that doesn't actually exist in reality
where there's some invisible point out there
that's pulling with an invisible spring
its finger round.
And this point keeps jumping around
according to this,
technically it's a heteroclinic cycle,
just a differential equation,
just part of the gerontim model.
And by appropriately mapping
each of these abstract points
in the sequence of the cycle
to points in a Euclidean space
and then getting action
to minimise the prediction error
based upon the feeling
or the proprioceptive predictions,
I can get it to draw,
I can just get it to write,
just reflexively.
Because I've got a gerontim model,
that gerontim model can generate predictions,
it generates predictions of what I should feel
and what I should see.
And then the feeling,
the proprioceptive predictions are realised by
reflexes at the lowest level
in the spinal cord,
and it just does what it expects to see
and it sees itself writing,
which is very nice.
Interestingly, because
it's generating both
proprioceptive predictions
which become motor commands effectively
and visual predictions
which can be thought of
in terms of classical motor control theories
corollary discharge,
I can now just cut the feeling sensations,
I can remove them,
assign them a zero precision,
that pie that waiting before,
and I can simulate what it would be like
if this agent was seeing
a hand doing writing
but couldn't feel it.
So it would be as if
it was watching another agent
doing some writing.
So this is a very simple model
of action observation
and that's basically what's being shown here
in terms of the
activity of this unit here,
every time it hits
more than 50% of its maximum activity,
I'm putting a red dot
depending upon where we are
in this space here.
Just to show that this,
if I was an electrophysiologist
and recording the activity of this part
of my pattern generator,
or my generative model, or my forward model,
it would look as if
it had a place field
or a sleel activity
for where in this space
it activates or encodes.
It also would look as if
it had a direction selectivity
because it likes the down strokes
and not the up strokes.
And more importantly,
it's the same neuronal,
simulated neuronal response
when I am actually acting
and when I am observing
the same action just
in the visual modality.
So it would look like a mirror neuron.
It would look as if the same machinery
is both encoding
particular movements
and watching
someone else that I can't feel
perform the same movements.
Now from our point of view
we know that's almost trivial
because both of those
consequences are generated from the same
generative model, so it has to be the same
neuronal infrastructure.
But it's nice because it provides
a very simple perspective on the phenomena
of things like the mirror neuron system
and some insights
into the way that we use
our models of our own behaviour
to understand and infer
the behaviour of others, provided
others are sufficiently like me
to make that a viable
generative model.
So
last few slides now
that was used as an example
of basically using reflexes
in the context of this very elemental
form of action
to eliminate prediction errors
right at the level of the active
states or the sensory states reporting
the primitive
or elemental consequences of action.
I just want to close now with
more sophisticated planning
and choice
and I'll show you
the equations that we
saw yesterday behind this, but first of all
just intuitively
this is how you might motivate it
if you were talking to say philosophers
possibly not.
If
now by action we mean things
that we choose to do
this means that if I act
I must have beliefs about the consequences
of action
and these
in this Bayesian mechanics
are posterior beliefs.
But if I have posterior beliefs
of action I must have
prior beliefs about the consequences of action.
So what are those
prior beliefs?
The argument is very simple
if I am a self-evidence
in creature that performs these
gradient flows and therefore
must minimise this free energy
functional
I must therefore a priori
believe as if I behave
as a self-evidence in creature
I can't have any other beliefs
for internal consistency
or self-consistency.
So technically what that means is
a priori
I will select those actions
that minimise
expected free energy
or expected surprise
given a particular action.
So what does that look like mathematically?
Well we already know what that looks like
because we did all that in depth yesterday.
So here we won't
go through this in any great detail
but just to briefly rehearse
or perhaps answer any questions on this
the arguments yesterday.
So here is our free energy functional
Here is the entropy term
and the energy term that we talked about yesterday.
Yesterday we focused on
decomposing
this expression
a function of beliefs Q
about external states here
parameterised by internal states
that are the expectations
which I've dropped for simplicity
from this expression
decomposing it into complexity and accuracy
there is another
decomposition here
which enables me to
separate this term
which is basically
the
an expression
of the log evidence
from these two terms
which is the difference between
beliefs
by approximate posterior beliefs
and the true posterior distribution
over
sensory states
sorry
hidden states
given external states
and because that's the KL divergence
it means that these two quantities
can never be less than zero
which means that this F
always has to be bigger
than the Bayesian model evidence
that comes in
Interestingly though
what we've just said
the more interesting thing my apologies
is we've just said we're going to choose those policies
or actions A
that lead to the greatest minimisation
of free energy
namely
to minimise expected free energy
and we saw yesterday that we can either
think of that in terms of
minimising risk
the difference between what we think
will happen in terms of hidden
or external states given an action
and what we
a priori prefer
to happen in the sense
that these preferences
are the attracting states
that we've spoken about before
and if we put those two together
to combine
them into risk
then we also have to worry about
minimising ambiguity turning the lights on
we can rearrange this and express it in terms of
epistemic value
and expected value
and if you remember yesterday we just walked through
different interpretations
of this expected free energy
in terms of Bayesian surprise
or mutual information that we've already spoken about
taking
various sorts of uncertainty off the table
so removing the ambiguity
just leaving us with the risk
part of the imperatives
for choosing the right kinds
of policies or action
known as risk sensitive control and economics
and removing uncertainty
completely so that now
we can reduce our
imperatives to expected
prior preferences
expressed in terms of external states
or sensory outcomes
in our blanket states
so this is just a reprise
of yesterday
that now provides
the point of convergence between yesterday's talk
and the arguments
from the underlying physics
and the story from the point of view
of predictive coding
but let me just give you an example of that
that speaks to the question about
when do we choose to move
as opposed to change our mind
about what we're actually seeing
I'm going to pursue
not the preference bit
but the epistemic value bit
so I'm going to ignore
preferences or expected value
and just focus on what it would be like
to
do this gradient flow
do this active inference
when I had a choice about where to look
and I'm going to associate
that epistemic value
that Bayesian surprise
with
the expected negative
expected free energy
of
the consequences of looking over there
and what that will look like
is a salience map
in visual neuroscience
so the parts
of the visual scene
that attract my eye movements
should be a priori
those eye movements
that result
in the smallest
expected free energy
which means have the greatest epistemic value
or affordance
or salience information gain
however you want to express it
so I'm going to demonstrate that
by doing simulated
saccadic eye searches
just to demonstrate the form
of these salience maps
imagine that I thought
that my visual impressions
were caused
by this image
but my visual impressions
are formed just by a local
sampling of this image
for real representation
of this extent here
that means that
I now wanted to know
where would I look next
to resolve the greatest amount of uncertainty
about what I am looking at
I can now imagine
if this was the cause of my sensations
and if I looked over here
then I would get these
sensory data
if I had those sensory data
I can then work out the reduction
uncertainty about what I am looking at
and that reduction of uncertainty
of that information gain
is scored by the epistemic value
converse if I looked over here
I would learn nothing
if I looked in the dark
or I looked at a blank
uninformative part
non-salient part of the image
or my eye movement did not have
epistemic affordance or salience
I would learn nothing and that would not be
that would have a very high expected free energy
and indeed when you actually compute
the expected free energy
for this particular example here
you get a salience map
which looks very much like
the salience maps
that attract
empirical eye movements here from the classic work
of Yabus
showing where people actually do look
to resolve uncertainty about
this visual image
I am going to take a slightly
simpler example
but using
a hierarchical generated model
we don't need to worry about this
image passing scheme I described before
but it's now been equipped
with this sort of metacognitive
extension
where I've actually put the expected
free energy in play as part
of the generated model
so now the generated model
has a model of the consequences of the next
place that I am looking
and it has the prior beliefs that it will look
at the most salient
parts of the visual scene
and then those predictions about where
it will look next are sent down
to alert the active states
that then realise
those predictions reflexively and the agent
looks over there or it looks over there
or it looks over there
so when you solve that gradient flow equation
that sort of inactive
generalisation of predictive coding
this is what happens
so this agent
believes it lives in a very simple
universe
and only a face in this universe
and the face can be either upright
sideways
or inverted
and it can only see a tiny portion
of the visual scene
denoted by the circles here
and it's now going to use
beliefs about the sort of moves
or actions that it will make
to select where to look next
to minimise its uncertainty
about whether it's looking at this
this or this
are these causing its sensory
impressions these or these
so as time goes on
it will select an eye movement
and it will do so on the basis of this
expected free energy or salience map
so here covering
where the two eyes are in fact
and it then looks at one eye
and then because it now knows
what happens when you look over there
that now loses epistemic value
so it falls out of the salience map
then the other eye
becomes epistemically attractive
and it looks over there
there's a slight rebound because there's a degree
of forgetting
in this generative model
and it looks from eye to eye until it's
absolutely certain yes
this is not the
sideways face
and then it looks at the nose and the forehead
and very quickly
develops and accumulates evidence
and these are the actual
samples
the sensory states
or the sensory parts of the Markov blanket
that is actually
sampling with these
eye movements every
250 milliseconds
and what we see here is that
after just a few eye movements
its posterior beliefs
its expectations
about the true orientation
which was the upright orientation
win over
the alternative hypotheses
which are these two explanations here
the grey area
is the
90% confidence space
and confidence intervals
and what we see is a reduction in confidence
both during a sacallic eye movement
but more crucially after each sacallic eye movement
which speaks to the sort of scheduling
of perception
belief updating
versus active sampling of the world
they're both happening
in tandem
but the active sampling
has a particular scheduling
here at every 250 milliseconds
then there's belief updating
and then it computes where to look next
then emits the next behaviour
and then it does belief updating
then includes where to look next
so there's a scheduling here that's not completely
in parallel
there is actually a temporal structure
to it
which
is a twice metaphor for the way that we
accumulate evidence
in an active way
so this is basically active vision or active sensing
in a very principled way
you're deliberately choosing where to go
and get your data next
in the service of resolving
the most uncertainty about what could have caused
that particular visual scene
or which object you're currently
sampling or palpating
and like yesterday we'll end with
a summary of that from
Helmholtz
who expresses exactly the sentiment
that I've just tried to describe
using simulations
that each movement we make
by which we altered the appearance of objects
should be thought of as an experiment
designed to test whether we have understood
correctly the invariant relations
of the phenomena before us
that is their existence in definite spatial relations
and very much like yesterday
thank you to all these people
and thank you for your attention
thank you very much indeed
thank you very much
do you need microphones for questions
or we can hear them
is it important for media reporting
that we use microphones to ask questions
I believe so
right
who is recording
ok
what if I have to grab one
just one moment
I have two questions
first one is
related to
surprise minimisation
if
we take it name
I can minimise
minimising my sensory
minimising my action
so why
right
so this is often
referred to
as the dark room problem
so the question is usually
articulated if I just go into a dark room
and switch off the lights
and stay there forever I'm not going to be surprised
because I know exactly what's going to happen
so there are lots of answers to that
and indeed
there are even papers
there's a little philosophical literature on that
I think technically
remember surprise
is a
function of
your predictions
so if you indeed
predict that you are the sort of creature
that can live comfortably
and survive for years
in a dark room
then you will certainly not be
surprised to find yourself in a dark room
but if you're the sort of creature
like me that finds it quite surprising
to be in a dark room the first thing
that I would do would be to switch the lights on
so
having said that of course we all do
actually go to bed in a dark room every night
so there are certain
parts of our daily
cycle where that's entirely
appropriate the point being
surprise here is not
shock and awe
surprise is a very
precise
and formal definition
of the mismatch
or the information
afforded your
predictions by what actually
happens so if I believe
I'm a warm-blooded creature
that periodically has to eat
things it has to also
have social exchanges
with loved ones
and furthermore
I'm a warm-blooded creature
that is self-evidencing so I'm going to be curious
in fact I'm a scientist
that curiosity has become
written into my paid employment
the last the most surprising
thing for me
will be to be locked
or sequestered in a dark room
without access to friends
food or the internet
and in a short
period of time say a week I will probably
then dissipate I would lose
my mark of blanket because I would
dehydrate then I would die
and then I would
literally decay so technically
from a physicist's point of view
the dark room is
minimising surprises not removing
sensations you can't remove sensations
you can make them less precise
and that speaks to
an increase in ambiguity which is
things we avoid which is why
we turn the lights on when we go into a dark room
or you can
you can
make them
very static which is very surprising
you know the whole point of that
generative model where I was
motivating
movements
with dynamics is that
we expect things to change
we expect things to have
solenoidal behaviours for example
and therefore when things don't
change like that when things suddenly stop
or freeze that's very surprising
I'll just give you
another example of
surprise which I sometimes
use more as a joke
so bungee jumping
so jumping with
a big rubber band
why do people do that
now you could say
well it's because
they want to hit the floor
and see what the very surprising
sensations are
when they render themselves
two-dimensional but they don't do that
because they know that they're not
going to hit the floor
is that
they do do they? only in Australia
apparently
let's pretend that never happens
so mathematically
the surprise in bungee jumping
is if your rubber
support broke
and you actually hit the floor
that would be mathematical surprise
but you do not aspire to that
so what do you aspire to?
we're now moving
really into the realm of
epistemic value
and salience and novelty
so
if you think of things
not in terms of just surprise
but really what drives
the selection of our
next action
or policy
that is made by
the active states
that we're talking about minimizing expected surprise
which means we're all about
minimizing uncertainty
and that means that things
that opportunities to minimize
uncertainty become attractive
so novelty
now starts to become attractive
because it provides an opportunity
to find out what would happen
if I did that
what is behind that door?
so the very epistemic
opportunity to minimize
uncertainty
ie expected surprise
starts to acquire
epistemic value
through the expected surprise
so I think it's important
when using words like surprise
and novelty
when translating it into
this rather technical use of these words
to be very careful about what you mean
so when we talk about surprise
or self-information
we mean the sort of surprise you would get
with a really nice birthday present
that's much more the novelty
of the opportunity to take the paper off
and reduce your uncertainty
about what your parents or your loved one
got you for your birthday
that's a very different sort of
resolution of surprise
it's a general theory of self-organization
and
what do you think about
the growth
of complexity
we observe in our
universe
in biology
so how is it related to this
for part of the period
why does it belong
in a more complex system
with these
boring
more blank
around
working one another
so is there any
trend or is there any
forage
method to produce more complex
that's a really interesting question
and it should
invite answers from other people
I can start by saying
that's not
really part of the theory
in an important way because
as you'll notice all this talk
has been about is if
things exist what
properties must they possess
so all we've done is
say if things have attracting
sets then they must look
as if they are self-organizing
to a random
global attractor or
dynamical attractor that looks
as if they are self-evidencing
that looks as if they are modelling their world and so on and so forth
at no point
do we ask
well why do these systems
acquire an attracting
set or a Markov blanket
which is your question
I think it's a really interesting question
my
and it may well be that people have some
clear ideas about that certainly in the physics
of non-equilibrium and steady state
that's I think the big thing in physics
to my eye in popular physics anyway
that's the outstanding
challenge of the 21st century
so most of the physics that you
and I would have done at school
is all about equilibrium physics
things where the second law
of thermodynamic supply these laws
do not apply in a non-equilibrium
open setting so
people have got to rewrite
the physics now
for non-equilibrium
steady state dynamics that are fit
for the purpose in biological sciences
so there are things that generalise it
particularly the fluctuation theorems that can be
generalised to non-equilibrium
physics
so I don't think there's a clear answer
that everybody, I could say
go and read this on Wikipedia
I don't think there's a clear answer at the moment
but one might guess
that just because
you're scoring
the form of these very complicated
attracting sets
with this multiple
temporal scales
and self-similarity and very very
detailed structure
in terms of the probability of these sets being
occupied
you might imagine that if you left any system
alone following the big bang
for long enough
then the most
the last things that you would see
i.e. the longer you wait
what will emerge
are simply those things that persist
for longer
if that is the case
then you might imagine
that those things that persist for longer
are those that have this very
well defined structure
so these particular attracting sets
are
unique in the sense
that they are not point attractors
they're not the sorts of attractors
or periodic attractors of the sort
that describe the dynamics of the sun and the moon
or the parabolic trajectory
of a football
these are much more interesting
attracting sets that seem to
spontaneously emerge
if you wait long enough with the right kind
of systems
so they're characterised
either by
the fact that they explore large regimes
of state space
and yet the measure
or the volume of the attracting set
is very very small
resistance to entropy
so they have a very small measure
but they seem to be in lots of different states
you can find me in a cafe
you can find me in London
you can find me in Russia
but it's still me
so that I can be in different states
but
within that
the number of states I can occupy
actually has a very
very thin bound
of mathematically defining that
is in terms of
information length
that may be a bit technical
but there may be a couple of people here
interested in information geometry
and information length
there are ways of
characterising interesting biological
attracting sets
using information length
and what it reduces to
is that these attracting sets
remember their initial conditions
so
unlike a thermostat
or a virus
or something that snaps back very very quickly
to its preferred equilibrium
steady state
it takes a long time
before I get back to where I started
from
another way of looking at that is
you remember your initial states
for a long period of time
or
you
performing an orbit
on your attracting set
you actually move through lots of different
probabilistic states
so these is another way
of understanding
the complicated information geometry
implicit in these attracting sets
your question is
is it inevitable that any
setter of
stochastic differential equations
or long equations of this sort
in the right kind of universe
that had an attractor
would if left for a sufficiently long period of time
show this sort of behaviour
and my guess is yes
but I can't think of any proofs of that
other than using
numerical Nazis
do you know of any
no I think that's
an outstanding issue
is anybody in Russia looking at that?
Jeremy England
Boston's
I think addressing related issues
the emergence of these
very itinerant
self-organising random attractors
in a non-equilibrium setting
so you might
yes yes
but here
if you're intuitively
I think that if you have a longer
you can have
some constraints on your system
and you want your system to survive for longer
time which means that it should
encounter more and more different
states
states during your slide
and to survive
to keep your
boundary
for the higher
larger volume of these
spaces
you need more
complex model
and these leads you to create
more complex to
similarly
your wall to keep yourself
your vast
interpretation
and indeed that
moving through different states
and if the internal states incurred beliefs
about external states that's exactly what
that information length scores
literally the number of different belief
states you move through as you move
around and exactly as you say if you
live in a deeply
structured world which we have created
for ourselves
then you need a deeply structured
generative model and the
attractors that
describe the behaviour or the interpretation
of your brain as inverting
a generative model must be similarly deeply
structured with long information and so
on absolutely
so
actually it's right in
neuroscientists
and actually
was exactly what
was his theory that
resolvance of the
function
you have to
send me the paper
of the books
they often say
in my
in western culture there are two ways
to solve a problem
you can either spend one year
solving it yourself
or you can spend three years to learn Russian
and find the solution in the Russian literature
so it sounds like
it sounds as if I should
you speak Russian so you can send it to me
and I can
that's exactly the conclusion here
that when you are uncertain
about what to do
when you've got these policies
you want to evaluate in terms of the expected free energy
when you look at the biological
implementation of that
all negative affect states
are associated with
a lack of
certainty about what
to do
is itself trying to minimise uncertainty
so chronic anxiety
or just stress
reflects
basically the lack of a clear way forward
you just don't know what the consequences
of your actions will be
and no one action seems to be
or no one policy
seems to be clearly
the best thing to do
in relation to other things
if you turn that on its head
that means that you feel really good
when the way forward is very clear
and then we go back
to that dopamine story yesterday
so if you look at dopamine
as encoding
the certainty
about what to do
the best link between reward and dopamine
and just being certain about what to do next
Simon Lough
Simon Lough
I may have read that
