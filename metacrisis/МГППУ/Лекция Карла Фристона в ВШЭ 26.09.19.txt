Well, some of you have already heard the lectures of Professor Christon, and we are really
honored that Karl Christon will be in his lectures in Moscow.
Many people have told you that on their opinion, Karl Christon is one of the most influential
cognitive sciences in the world. Many artificial algorithms also suggest the same.
But it's always a personal story for me. The role of Karl Christon is very personal.
More than 15 years ago, the PhD students and I attended a human brain backing conference,
I think in New York, and Karl Christon gave a long-awaited talk there and discussed various
issues and also discussed some statistical issues. I'm not sure that interactions are
more important than many facts. I was so much influenced by the same theory that on my
way back to Helsinki at that time, I designed an experiment, a relatively simple experiment.
This was one of the most satiated studies in my life. It was the first one, in fact.
Next, when I moved from social neuroscience to economics, I also, first of all, had to
learn mathematical approaches suggested by Karl Christon as being one of all these
different machineries to process a summary data. Finally, now we conduct some studies on
learning or N400 effects cognitive dissonance. All of these ideas are suddenly linked by
an idea of reality. So I think personally, I would say, yes, Karl Christon is the most
influential cognitive scientist in the world, and we're really happy that new generation
also can attend this distinguished lecture, and I hope that you all enjoy the talk today.
Thank you very much.
Thank you very much for that great discussion. It's a great pleasure to be here.
Interactions are very important, so I think we'll start this lecture with an interaction
between me and you. What we're going to do is play a game, and it's a game that illustrates
how we have insights, how we form beliefs, how we learn rules.
And the reason that I want to play this game with you is I want to give you the experience
of learning a new insight or having a new insight, having what we call kind of a heart.
And the reason I want you to have that experience is the rest of the lecture will be trying to
understand and simulate what are the influence processes that lead you to that insight.
So this is an ambitious lecture. It's not the best lecture that I give, but it's the most
interesting and challenging. So it's trying to address one of the most difficult problems
in inference, which is learning an abstract concept, and whether we can explain that from
first principles. I'm going to take you through this game very slowly, and you'll understand
why I'm doing it very, very slowly at the end. So let's just play the game. What we have here
are three colors, and I'm going to give you some prior beliefs. I'm going to do that by
giving you some task instructions so you know how to play the game. So again, you're going
to choose the correct color, which can either be red, blue, or green, from the color of
the large circles here. And I'm going to tell you that there's a rule. So the arrangement
of these colored circles tells you exactly which one is the correct color, and the location
of the correct color depends upon the color of the center circle. So this instance is
going to be the blue circle. So the correct color depends upon the color of this circle,
and you've got to tell me which is the correct color. And I'm going to show you the correct
color, which in this trial or this instance of the game was the green circle. So I'm going
to put all three colors down at the bottom, and I'm going to encircle and highlight the
correct color so that you can see the history of your choices and whether you were right
or wrong. I'm going to repeat that game guessing initially until you spot the rule. Is that
clear? Okay, well let's start. Each one of these panels is a little game, so you go that
game, and then that game, and then that game. So remember, you've got to choose which of
these three colors is the correct color, and the correct color depends upon the color of
the top circle here, blue, red, red, green. Right, and the important thing is I'm interested
in how many exposures, how many of these games you have to see before you realize the rule
and you gain the insight to prove. And then remember how good you are as an audience and
see whether you're better or worse than a baseball agent. So let's start. What color
would you answer? There is no way that you can know the right answer, so you just have
to guess at this stage to disclose information about what might be the rule. What color would
you like? You want blue. Sorry, green. Have another go. In this combination, it's a
regent here. What color do you think is the correct color? Red and blue for the audience,
but you're going to choose something. You want red. Oh, I'm sorry, it was green. Now I'm
guessing simply add a clever idea, because there's lots more red, there's a high number
to be red. Interesting hypothesis. Have another go. What's the correct color here? Green.
Red, yes, it has to be. Because it has to be one of these colors, so it has to be red.
So that's not a very informative trial. So have one more go. Take your time. I've deliberately
slowing things down because you'll see why that's important at the end of this lecture.
So think about it. There's something you want to green, and that's absolutely right. That's
interesting. Does anybody of the people that just said green, does anybody have a hypothesis
or a rule that they're like, you think you've got it? Don't tell me what it is at this stage.
Why didn't you make the next choice, and then see whether your hypothesis was correct,
and then see what it was all? What do you think the correct color is here? Green again to be red.
It's a green again, and it's bright. Do you want to have another go? Yeah.
Absolutely right. Do you want another go? Red.
You said red, red. That sounds very confident. You clearly have a rule in your mind. Do you
have a rule in mind? Good. You say red, and you're absolutely right. Did you think it's
going to be red? Yeah. You did. So perhaps you could now share with us what you think the rule is.
What is the color of blue? You just look at the right. If the color is red, you look at the left.
And if the color is green, you look at the top. Perfect. Was that a rule? No.
It's very honest to me. It's important, though. Can you tell me what your rule was?
I just thought you have some kind of range. So just if you have green, then the color is green.
And the green is also if you have green, the right color is green. If you don't have green,
then the red is like the second one, and the third one is blue.
Yes, it's a range in the order. So let's just remember when you started getting it right,
which was that you wanted to. When did you think you had your insight into the rule?
Was it here or was it fourth? Fourth. Okay. And you, sir? Fifth. Okay.
So one point, that's remarkable. That's a world record. The only place I think that's ever got it in five
moves is MIT in Boston. There is a gentleman I don't even know, called Josh Tenenbaum,
who does structurally, who came to the office and said he actually got it in three,
but didn't want to show off. I'm not sure I believe it. So that's very impressive.
And you'll see how impressive that is later on. But the other point to make is that there was a very
plausible hypothesis that you were pursuing as well. So there are lots of potential
hypotheses that could have been right. Lots of rules that could have been right.
And even, in fact, it's not, in fact, the rule can be deduced about four or five of these particular examples.
However, if you were simply a days-old major, we can't, you couldn't actually infer this was true
with just four or five exposures, which means that there are lots of candidate rules
that you'll possibly entertain in your head. And it's interesting that sometimes people do
commit to the wrong hypothesis and the wrong rule prematurely. And that tells us something
about the way that we search for hypotheses or explanations about the way the contingency
is in our world and then elect at a certain point in confidence. Yes, that might be the
best explanation. And then I will explore and try to get the evidence required to confirm that.
So in fact, later on, if you remind me, I'll show you a synthetic artificial agent that did
exactly what you did. Right. So just completing the answers here and they are all consistent
with this rule that if it's blue there, you look at the right, if it's red, you look at the left,
if it's the green, you look at the centre. Right. So remember that. Now we're going to
just stand back and think about how we might explain that remarkable ability to invent a hypothesis
or a rule and a correct law that was a very simple, a very difficult explanation for the
contingencies and the outcomes that you were exposed to. And who should you be able to do
that with? A tiny amount of information. Just four or five data points and you're able to
infer that rule correctly. And the explanation that we're going to use to try to understand
is based upon predictive processing and the minimization of free energy or surprise.
For those of you who have seen previous ventures in this series, a lot of this material will be
familiar, but I will briefly go through it again for those people who are attending here for the
first time. The notion of predictive processing and the minimization of surprise both require
the notion of a prediction against which you can measure the degree of surprise. So if
you don't have predictions, you can never be surprised. To have a prediction though, you
have to have a model of how the outcomes were generated that you are surprised with that.
So everything that I'm going to be talking about rests upon the notion of a generative
prediction or a model that maps from causes to consequences, contingencies to outcomes.
And do you need that in order to evaluate your predictions or generate predictions and
evaluate free energy or outcomes given a genetic model? Or particularly given your beliefs
about what is causing the outcomes of your predicting and forming those beliefs, updating
beliefs is going to be referred to as active inference. But we're going to go in this lecture
a little bit beyond that. So in most of the talks that I've been giving this week, we've
limited ourselves to inference. What I want to do now is to put inference in context because
there are other sorts of optimization and learning that conform to exactly the same principles
but have a very different kind of them up, them up, them up. And I'm going to focus in this talk
on inferring hidden or latent states of the world that are at this moment causing sensations
of outcomes that I can observe. Learning the parameters and the contingencies and the
rules of this generative model that have enabled me to learn about the world. And that's going
to be associated with learning the parameters of the model that I'm using to infer the time
depended in states. We're actually going to get even one level further. Not only are
we going to be considering the processes by which we learn the parameters of models that
generate predictions from hidden states, we're going to be thinking about learning the very
structure of the model itself. So if you worked with neural networks, for example, how many
hidden layers do you have? How many units do you have? How many things? How many dimensions?
How many colors are there? All of these formal structural aspects of the model themselves
have to be optimized. For example, during neurodevelopment or during evolution. And that's what
I've seen possibly even during Sweden. So we're going to be talking about three levels of free
energy minimization. Inferring states that change very, very quickly. Learning parameters
of models that change by slowly rules and contingencies in our world. And structure learning
or Bayesian model selection of the best structures and forms of models that are appropriate for
explaining the kind of data that we're exposed to. In summary, inferring states of the world,
learning model parameters and learning model structure. I'm not going to have time to go
into the detail, but Kelly, for all of these. So this network is just to overview the principles
that you might use to understand these three processes that all depend on each other and
that are all contextualized by each other. And we can use that rule learning as the
vehicle to understand structure learning and insight. So this whole story is based upon
finding simple explanations for our sensory impressions. For those of you who haven't seen
this before, you're meant to be looking at this picture of thinking it's a bowl of fruit.
But in reality, if you look at it in another direction, it's meant to be a face. And the
idea here is that you make that face. This is your hypothesis about what caused this
artist to draw in this particular pattern. Paint that produced these sensory impressions.
So the idea here is that the frame is a constructible that we actually actively construct explanations
for our sensory input. As opposed to the sensory input, somehow exciting meaning or
all that you are extracting information from or meaning from sensory input. The idea here
is it's more of an inside-out process. You have the notion that there is an explanation
for this particular pattern of sensory or visual inputs and that hypothesis is then
used during predictions and then those predictions are either confirmed or disconfirmed by the
actual sensory evidence in your sampling. And this idea is very old. Many of us have seen
this slide before, most explicitly articulated by Helmholtz. For example, objects are always
imagined as being present in the field of vision, as would have to be there in order to produce
the same impression on the nervous mechanism. So these impressions are the sensory
impressions. So if you know something that is out there that would have caused the
impressions that you are actually experiencing or sensing at this time, then you have provided
a sufficient explanation for your sensory engagement with the world. And this is very
closely related to Richard Dregler's notion of perception as hypothesis testing. What
we see is actually the result of an internal hypothesis testing about what could have caused
this pattern of disfiguration. And there are lots of very nice illustrations of that in
terms of illusions where we deliberately change the sensory information such that a false
cause can be inferred or a false hypothesis can be selected to explain what we see. These
ideas you will see throughout cognitive neuroscience, cognitive science, how to be
learning and how to do no industry in the massive information age. I have hired a group
of people like Geoffrey Hinton and Peter Dan here who actually proposed a Helmholtz machine
in one of Helmholtz's original ideas, borrowing from Bayesian theory and free energy formulations
of the pathological formulations of quantum electrodynamics, bringing them together into
a mathematical algorithmic description of this process of testing hypotheses by minimizing
prediction error or minimizing variation of the energy as a measure of surprise. So let's
come back to this notion of impressions on the mark of black entry to bear spoke about
on Tuesday. Sensory impressions on say our retina we can think of in terms of shadows
and perception of this view is really the game of trying to find a good explanation for
what caused these sensory shadows here. And the approach that we are going to use is going
to be based upon a minimization of this free energy, variation of free energy. I basically
give these a form substitution error so whenever you see an alpha-sumptuous it's going to be
a dot above the expression which means a real change. So I'm not going to go into any technical
detail here, other than to say later on we will see expressions from this free energy
functional. All I need to note at the moment is that it provides a lower bound, it is always
the lower probability of some outcomes that we witness. That's known as log evidence, so
the probability of some data under a gender model is the evidence of those data. So this
quantity here free energy is also known as machine learning as an evidence lower bound
or an alpha ELBO. And the idea is all you have to do to do this inference, to do this
structural learning is effectively minimize this alpha, so maximize the alpha or minimize
the negative free energy. And if you do that with a gradient descent and I can write out
the gradient descent, the changes say in expectations about states of the world as a gradient flow
on this free energy functional, crucially a functional means a function of functions that
function by current release. So we're reducing our uncertainty in a particular way about
the states of the world that cause these sensory impressions. For those people who do
engineering, this is the same as a Kalman filter, which is the same as predictive processing
or predictive coding implementations or predictive processing in cognitive science. All it says
is that the changes of our expectations or beliefs about states of the world objects
in say the visual field causing sensory observations can be decomposed into a prediction given by
beliefs about the states of the world at the moment and updated with a prediction error
here. So what's a prediction error? Well, let's imagine we have this sensory impression
and my expected state of the world, S, was that there was a dot out there. And if I have
a generative model that operates on this expectation to generate the observable outcomes that I
would see in the words of Hell House if there was a dot out there, that I can compare the
observed observations outcomes with the predictive outcomes generated from a generative model
based on my beliefs and form a prediction error. And all of this equation is saying is
I'm going to change, we use that prediction error to change brain states that encode my
expectations about what's happening and what is out there and why it is out there to
minimize the squared prediction error. Now, notice that all of this says is that perception
is just finding good enough explanations from observable outcomes or sensations. It's not
saying that you have to know what actually caused this sensory input. In this instance
there's actually the shadow of a cat, not a dot dot, but that doesn't matter. If you
can get through your entire life by keeping your prediction errors low, that's good enough
from your point of view you've survived and you've provided a good account of your lived
world. So this provides a very simple perspective on cognition and action and perception in
the sense that they are both in the service of minimizing prediction error. So we can
shape our expectations or beliefs about the world and beliefs about contingencies that
cause transitions in those states to make our predictions generative by a generative
model like the sensations and that can be thought of in terms of perception. Alternatively
and coincidentally we can also reduce prediction error by sampling sensations that are like
predictions and this then regards action as essentially changing sensations to make them
more like what we predicted. Previously we've talked about that in terms of motor reflexes,
or reflexes in our culture. Indeed some people talk about that in terms of automatic reflexes
in other spaces. A bit more technical here. I've replaced prediction error with free energy.
So under some simplified assumptions free energy can be associated with the amount of
prediction error, in fact the precision weight of prediction error and predictive coding. Also
remember that free energy is an evidence lower bound. So it can be used as an approximation
for model evidence. So what we're saying is if we commit to a view of action and perception
that is just there to minimize prediction error, what we're also saying is that action and
perception are both just there to maximize model evidence. In other words, gathering as
much evidence as I have for my generative model of the world. And that sometimes referred to
in recent philosophical circles as self-evidence, gathering evidence for my model of the world.
And that should explain everything in principle. Again, please forget the non-substitution.
I don't think it's going to matter too much. I'm just now trying to, for those people who do
like maths, can you put your hand up if you like maths? Can you put your hand up if you hate maths?
Okay, now my math is clean. Excellent. Good. So I'm going to do the equations properly, but not very deeply.
So the free energy technically is just the log evidence, the log probability of some outcomes
or data given a model. Minus is bound. The bound is just the difference between the free energy
and the evidence. And we want to keep this very, very small. This bound is a KL divergence
or a difference between our posterior beliefs are encoded by expectations of that space
of the world S and the true posterior probability of a space of the world given by observations.
So by maximizing this quantity here, I need to minimize this so that it goes to zero, so that the f
of the energy becomes the evidence and the bound shrinks to zero. But if the bound has become zero,
then my approximate posterior beliefs, my recognition density, my variational density distribution
has lots of different names. We just call it Q. Q then becomes the same as the true posterior.
So my beliefs become days optimal. They are the true posterior beliefs about the states of the world
given those observations. So that's why we need to minimize, sorry, we need to maximize in this talk
this evidence, no bound or variational free energy. We've talked about that in terms of perception
and changing predictions, which for us basically means changing beliefs about the world given some way
of sampling that world. So we're going to be understanding perception as maximizing this evidence,
no bound here as with respect to our beliefs under a particular policy. So notice I've got a little
bit clever in here now. I'm now thinking about a situation in which I also have to infer not just
space of the world out there, but what I am doing actively to sample and change those states.
We're going to summarize that action upon the world, action here, in terms of a sequence of acts
or behaviors or moves denoted by time or policy. And crucially, in active inference, also known as
planning as inference, we have to form beliefs about what we're going to do and then we can sample
from those beliefs the next action to actually take care and execute. So an interesting twist here,
if we are actively to change sensations and we have to form beliefs about what to do next, we don't
know the outcomes. So we don't know the observations before they happen because they are in the future.
So this mathematically requires then, if I want to optimize or maximize this evidence variation
for the energy, it means I have to take an expectation or an average under what I think will be the outcomes
if I pursue this policy. And this is known as expected free energy. So we're going to essentially
choose our beliefs about what we're doing in order to maximize the expected evidence low bound
or to minimize the expected surprise and that's effectively the same as saying I'm going to select
my actions to minimize uncertainty. So changing the beliefs to maximize the evidence low bound
or to minimize the negative of that which is the surprise or prediction error and then thinking about
what we're going to do in terms of a minimization of the expected free energy which is essentially
to minimize the uncertainty. Uncertainty being the average surprise of something before it has happened.
So surprise is an attribute of something you observed. Uncertainty is the average surprise
of something before you have observed it. This leads to the following scheme for active
inference, the action perception cycle. Very briefly we have our outcomes here, observations.
We use those to optimize our beliefs in terms of maximizing this variational boundary of
more evidence that can always be decomposed into two terms, complexity and accuracy.
So we're going to be minimizing my complexity and maximizing my accuracy. For those of you who are
interested in complexity, this is the difference between the beliefs about states of the world
and my posterior beliefs and my private beliefs. So it scores the degree to which I've had to change
my private beliefs into posterior beliefs in order to explain the data that I actually see
and the accuracy is just the expected log probability of those outcomes given their causes
or in states of the outside world. So this is the linear model. It would just be the residual sum of square M, for example.
So we're going to find those beliefs for every possible policy that minimizes complexity and maximizes accuracy,
could fit under Malcolm's principle the simplest explanation possible, and then using my beliefs about
states of the world now and in the future underneath each of these policies, I can now evaluate
the expected free energy and form beliefs about what I should be doing, and then, also, I've got my beliefs
about my policies, I can select an action, I can then generate a new outcome or sensation, and so the cycle continues.
So what does this expected free energy look like? I've written it down here as negative G for the goodness
of a particular policy. It's just the average of complexity and accuracy in the future.
So what's the average complexity in the future? That's not an economics as a risk.
It's the difference between what I believe will happen in the future if I do that
and what I a priori expect to happen in the future if I did that.
And these are my preferences. These are my prime views about the sorts of states that I will aspire to
or I will find myself in, and that's why it's called risk.
We want to be risk-aversa. We go to, therefore, choose a policy that minimizes risk
at the same time as minimizing expected accuracy or negative accuracy, and that's known as ambiguity.
So we're going to choose policies or actions that make the outcomes that I would see if I looked over there
as unambiguous as possible. So a simple example of this would be the difference between ambiguous visual signals
from a dark or visually noisy room versus unambiguous signals from a room that is rather lit.
Or another example, ambiguous mortuary signals of a noisy bar, a restaurant,
versus very precise unambiguous signals that are just missing to one person in isolation.
So I'm deliberately going to be looking where there is unambiguous high-city noise data,
but at the same time I'm going to be choosing my policies so that what I see is close to what I prefer
or the states that generate what I see is close to what I prefer.
And that can be construed in terms of the policy selection.
So perception becomes free energy minimization, perceptual inference,
and then the maximization of expected free energy would become a policy selection.
I'll just make one, and then the psychology reform, and I will make it again, can be confusing.
In machine learning free energy is seen as a good thing and you want to maximize it.
It has an evidence lower bound, and you push the lower bound up, and it maximizes the evidence.
In physics, free energy is about things, the negative of that.
You want to minimize it, and then it becomes a upper bound of the negative evidence.
So I keep chopping and changing, and I keep making mistakes on the slides as well.
You just have to do that, depending on what you're talking about.
So I'm going to talk about maximizing free energy in this talk, in a physics talk about minimizing free energy.
There won't be many more equations, so please, if you don't mind the equations,
but I'm just writing that out again without the substitution parents here.
So all we have now is a more complete and labelling of these equations
to show you the difference between the expected free energy and how they are similar.
So again, we have the complexity, the difference between what we predict will happen,
what we prefer to happen, and the accuracy term here.
We take the expectations, and this becomes the expected complexity that now becomes risk,
and this becomes expected inaccuracy, which becomes ambiguity,
and I can just rearrange these to express them in terms of what we sometimes call epistemic value and expected value.
These are lots of different names.
This is also intrinsic value or intrinsic motivation,
and it just tells you how much information or uncertainty you resolve, i.e. information that you have gained,
in terms of the differences in your beliefs with and without observations in the future,
given a particular way of acting or sampling the world according to this affair of Pi here.
This expected value is sometimes called utility or extrinsic motivation or extrinsic value,
and it just scores your preferences, so sometimes it's called pragmatic value,
so there's epistemic value and pragmatic value.
It just shows that the expected free energy has two parts to it,
there's one informational uncertainty reducing part and epistemic part,
and there's other for reward-related pragmatic utilitarian part.
So most of reinforcement, for example, just cares about this extrinsic or expected value
and doesn't care about the uncertainty and the epistemic parts.
Just to make that clear, let's just ignore the utility or the pragmatic part
and focus on these two terms.
I'm just re-expressing them as a KL by Heardens to show you that it's the difference
between posterior beliefs with and without observations.
This is known as Bayesian surprise in the visual search literature.
Interestingly, the expected Bayesian surprise is just the mutual information,
the shared variance between states of the world and their consequences
or observed outcomes in the future.
So this is a very consistent with all principles of maximum efficiency
or minimum redundancy or maximum future information.
All about information, no reward, no utility, no preferences at this stage.
But let me bring the preferences back into play.
But I'm now going to eliminate one particular sort of uncertainty,
which is the one that inherits from the ambiguity here
and leaving me with just these two terms,
where I will assume that every outcome is completely unambiguous,
which means that I can see states of the universe directly.
There's no ambiguity.
If I see business, this is the way the world is.
So S states of the world and O outcomes become the same thing.
O becomes S.
If O becomes S, then this conditional uncertainty disappears.
And I'm left with these two things,
which is just the difference between my predicted states of the world in the policy
and my preferred states of the world after policy.
So this is that risk I was talking about before.
This now pertains also to the outcomes because S's are the O's,
which means that I've now found a special case of this expected free energy,
which is risk sensitive control and economics,
or KL control and engineering logical control theory.
Now I'm going to take the last sort of uncertainty away.
There's no uncertainty about what will happen if I assume this policy,
and I'm just left with this extrinsic motivation,
or expected value, or pragmatic value, or expected utility,
and this of course is just expected utility theory.
No uncertainty, no ambiguity, no risk.
All I'm left with is the imperatives that underwrite exploitation.
So I'm going to choose those policies that will bring me to a state of being
that I am priori expected to be in happy, warm, fed,
and all those good things about being believed by other things.
So some of you have seen this before.
I just wanted to, of the reason for having this slide,
is to give you a more concrete appreciation
of this information gains in epistemic value,
this intrinsic motivation that drives our policy selection.
We're driving along, we're looking over here,
there's a traffic light that's on the other, that way, not that way,
and we can choose to have a look at the traffic light,
or we can look somewhere else, in two pulses, two actions,
two movements of our eyes to go solicitable,
and gather that same information.
So which are we going to choose?
Well, if we look over here, we do not learn anything about
whether the sign is pointing to the right or to the left.
So if it's pointing to the left and to the right,
we're still 50-50, and if it's pointing to the left,
we're still 50-50, or uncertain about the state of the world.
Right of the world is in left.
So it doesn't matter what the state of the world is,
if I choose policy one, my beliefs don't change.
So that is equal to that, and there has been no information gain.
However, if I look directly at the light,
I will now see that the sign is unambiguously,
that the sign is pointing to the right,
and my beliefs change from 50-50 to 0-100%.
Similarly, from 50-50 to 100%,
even if the upper state or true state of the world is to the left.
So it doesn't matter what the true state of the world is.
In both cases, I know that if I look over there,
I'm going to have a big difference between that belief and that belief,
and that difference is the information gain.
It's the resolution or the reduction of uncertainty,
and it is the extended value of the intrinsic motivation
or the information gain afforded by looking over there.
So that will become a very important sort of construct
and component of minimizing expected free energy
or the maximizing expected model evidence condition
upon a particular loop.
So I'm going to quickly go through the material
which many of you have seen before,
to show you the nature of the models
and what kinds of things,
the empirical results you can simulate under your own process there is,
because I want to get to the end of the talk
and come back to that game that we played at the beginning.
So generally, because we have to talk about generating models,
we want to simulate behavior under active inputs.
You've got to commit to a particular sort of generative model.
The most generic and useful for discrete space spaces
is the Markov decision process
where we have fifth states of the world
that generate at each point in time an observable outcome,
and that generation is encoded by a likelihood matrix
usually written down as A.
And of course, now we have to have a joted model of transitions
from one state of the world to the next state of the world
to the next state of the world.
And that's usually expressed in terms of a probability transition matrix B.
That depends upon how we act, where we look.
It matters the way that we change the world
is by changing the flow of the narrative
or the trajectory of state transitions.
We also have to have prior beliefs about the states that we started on
and we have to know what the states are that we started in a similar model.
In order to evaluate the policy, we need to know the expected free energy
and the expected free energy depends upon our prior preferences,
our prize amount, and the final states following the policy.
And I think that's usually encoded by C.
Remember that, the C for cost function.
I talked previously about gathering the confidence of various policies
or beliefs about policies.
We're not really going to focus on that in this lecture.
But do note the beliefs about things can be parameterized
in terms of the number of times that you've seen them.
Formally, if you associate these things with a Dirichlet distribution,
these become concentration powers.
We can think of these little numbers just as a way of accumulating experience
about things that happen and things that follow other things
or things that are caused by other things.
And that's going to be more relatable when you think about learning.
If we've now got our generative model,
we can see where a particular field of approximation to the posterior
and we can use standard belief-updating schemes to simulate a brain.
Try and interpret belief-updating using standard variational message passing
under this generative model with neuronal processes.
With neuronal processes, so the perception becomes
influence about hidden states of the world.
So policy selection becomes basically a soft-max functioner,
an updater of beliefs about my policies, what I'm going to do.
We talked a few days ago about precision.
Learning becomes basically accumulating these counts
or these concentration parameters, Dirichlet parameters,
based on things that I experienced.
And that can be thought of in terms of the assisted plasticity.
And then we have our actual selection is choose the best action
from the most likely policy.
And you can unpack that computational anatomy in terms of functional anatomy
and think about different brain structures that might be doing different parts
of that active infant scheme of variational message passing.
You can simulate paradigms as the paradigm that I showed previously,
so rattle in T-Maze, and I would go straight to the reward
and stay there for one move.
Walker spent the first move looking for an instructional cue
or looking for a conditional stimulus that would tell it where to go and look.
And it starts off in a maze where the reward can be either on the right or the left.
It will usually, thought ineptively, go out.
First of all, resolve its uncertainty by selecting the policy
of the greatest intrinsic motivation or epistemic value,
which is its uncertainty about whether it's in a right or a left context.
And then when it knows where the reward is,
or it's resolved in some certainty,
there is now no longer any epistemic value in making policies
that go to the instructional cue or the conditional stimulus.
So the only thing that's left is the pragmatic value or the extrusive value,
namely the value that drives the selection of exploitative policies
that goes straight to the reward.
So after a while, if I just keep putting the reward on this side,
slowly but surely, it accumulates these counts in the initial state.
And so at some point, there ceases to be any epistemic value in going down there.
It's a waste of a move.
So the right and down become familiar with this environment.
And the best policy now is the ones exploitative.
It's a natural and deterministic switch from exploratory policies
to exploitative policies where Iraq goes directly to get its reward.
Now, there's no randomness here.
This is just a natural emergent property of minimizing expected free energy.
And because the free energy is a function of beliefs,
and beliefs are now changing due to learning,
that means that the best thing to do actually changes within experience and with beliefs.
So it's the best thing to do at the beginning is to explore,
but later on, when your beliefs change and you are less,
if you have more knowledge of your environment,
then the best thing to do will be more.
The thing that minimizes expected free energy will be to exploit.
And we've seen these things before in an associated section
with the perception of policy selection and precision with various physiological processes,
the accumulation of these counts in terms of learning.
We can simulate delivery responses, system classes, DRPs, this kind of behavior.
We can talk about that in terms of evidence accumulation.
We can look at simulated or synthetic place field responses,
we can even perform mismatched negativity analysis
as if it was a mismatched negativity paradigm
because we've got the same stimuli responses in a strange and familiar environment
that can be likened to an oddball and static stimuli in an experimental context.
What I want to do, though, is come back to our game.
I want to use that kind of model that we've just seen simulated exploration in the maze
to do a much more difficult, more abstract sort of exploration
named explorations of hypotheses or insights or rules.
So, remember, this was the game that you played before.
Your prior beliefs were that you're going to see yourself looking at the correct color
is determined or the color of the red circle's color
is determined by the color of the central stimulus circle here.
Now, what I could do, illustrate with you doing this task
that I actually have to think about very carefully
when I now try and simulate that route to do the same task
is actually choosing where to look.
So, I just put this thing wrapping down here to remind me to remind you
that the behavior here requires a particular sequence of eye movements
deciding where it's best to look next
and when it's accumulated enough information under its beliefs
about it should select the best color,
it then looks at the appropriate target down here.
So, this is the active bit here.
It's a visual search that's driven by where it thinks
informative cues and colors will be.
And built into this is a preference that is going to end up after a few moves
with circling to the correct path.
So, you can now perform and if you view the route
report the correct color just like you did by telling me what color is
or putting it up your hand.
So, we just need to write down the generated one for this task
and then use those equations to find the free energy minimizing solution
and then we've reproduced.
This is the route doing the game that you were engaged in at the beginning
and we want to see how well does it perform.
I'm going to give it exactly the same stimuli that you had,
exactly the same priorities
and we're going to see whether you can get it in five moves.
Do you think you can?
Put your hand up if you think it's better than better than you.
One person who's really committed to top space there.
Who thinks the route's not going to be as good as you?
Two people, good.
We'll find out.
Don't worry about the complicated figure here,
it's just here to remind me to tell you about the important bits of the
generated model.
Generally speaking, when you build in these models,
all you need to do, all the hard work and all the heavy you've lifted
is just thinking about what you would need to know
in order to specify this particular outcome in this modality.
So, for example, here we've just got three outcome modalities,
what we're actually seeing, where we're actually looking
and whether we're right or wrong.
Or we haven't chosen yet, so I'm still looking at these three cues up here.
If I wanted to generate what I'm seeing,
if I knew where I am looking and whether I'm right or wrong,
I'd go to need to know four things.
First of all, I'd go to need to know where I'm looking
and that would be a one-to-one mapping here.
So there's an isomorphic mapping between this and this state
where I'm looking and the outcome that I sense where I'm looking.
I also need to know the root in terms of whether I am right or wrong
and I need to know the colour that I am currently going to be selecting
and so I'm looking at the colour of the stimuli
and the choice that I'm going to make or not make at this point.
So these are basically the hidden states put simply.
These are the things that I need to know in order,
if I knew all of these things,
I could uniquely specify what this agent would seem, feel and say, yeah.
So all the important, if you like, structure in this generative model
is really the mapping between these hidden states
and these observable outcomes of different modalities
and that's what I've tried to summarise here
in terms of this likelihood mapping here.
So all the interesting stuff in terms of learning about a world
and inferring hidden states of that world
and what to do as you navigate through that world
depends upon knowing the right light of the mapping.
So I've tried to show this mapping in terms of all the important hidden states
or factors here in general outcomes.
So here are the outcomes down here.
I can see this, this and this
and these are the outcomes that I would see
if I was looking at the left and the rule was left
which means that the colour that I see is the correct colour.
But if I'm looking in the wrong place
the correct colour is not determined by the colour that I see.
I have a uniform mapping here.
If I'm looking at the centre then I will always see green
so that has a different probabilistic mapping.
So if the rule is the correct colour is on the left
then as we've noted before when we solve the game
that basically means we have to have red in the centre.
So if I'm looking at the centre there has to be a red there
and if you follow through that logic you will see that
you will see that the rules are completely encoded
in the structure of this line of mapping
amongst these different kinds of hidden states
and in our case.
The key thing is this structure here
that only when I'm looking to the left
does what I see correspond to the correct colour
and only when the rule is to the right
only when I'm looking on the right
does if there are one-to-one mapping
and likewise a one-to-one mapping
then I'm looking at the centre.
This is a prior belief.
Now the prior belief is such that you can see
the rule was a unique mapping between a rule
but you don't know what the rule means.
These contingencies, these likelihood mappings
are completely imprecise and informative or flat.
So we know there's a rule
but we don't know what the rule is.
So this is what our simulated rough started with
and this is what it has to learn or discover.
So we now integrate those equations
with that generative model
and now look at the rough's behaviour
and its free energy and its uncertainty
and see how many exposures it requires
before it can behave confidently
and always identify the correct colour.
Remembering that the only thing that's driving
its inquisitive, curious behaviour
and model building about the world
is the minimisation of expected free energy
or choosing policies and minimising expected free energy.
Now what we see is using exactly the same stimuli
or games that you used
and it takes about 12 moves before it becomes
consistently confident about its responses
with a content reduction in negative free energy
and it stops making mistakes
when mistakes are on the red bars here
and these are the policies,
basically the sequences of where it looks
as it gains more experience with this particular game.
So you were right,
we're better than the Bayes-October agent.
It takes about at least 12 moves
before it can actually learn the mapping
but is the implicit learning of this rule
or contingency.
Interestingly though,
let's just look at the first and second to last trial here
where it sees exactly the same initial stimuli
but its beliefs about what it's going to do
here are rather different.
It's uncertain about what to do or where to look
whereas here it knows exactly what to do
and that is purely explained by the learning
of the contingencies in this instance,
that likelihood matrix
that give this first stimulus a meaning.
It's the colour of the central square.
It now knows what that means
because it's now learning the mapping
so it knows the rule
and it knows exactly where to look next
in order to find out what the white colour is
so that you can report it as soon as possible.
So let's just compare and contrast
those two trials before and after
the implicit rule of learning
and see what that would look like
if I did some simulated electric physiology
on this synthetic rut.
So here I'm encoding the expectations
about different states
of the colours that you can see
and other different states of this rut
from the beginning to the end
of the sequence of little moves.
Remember that before
it knows the contingencies in the world
this is afterwards
and the only reason I wanted to show this
is if I now interpret this belief updating
in terms of simulated ERPs
what we see down here in terms of
simulated lower field potentials
or regenerative potentials
is a much earlier onset
a shorter latency of belief updating.
So it's as if this brain is getting excited
by cues that it sees much earlier
in the trial
simply because now they have meaning
under its journey model of the world.
That meaning being
consequent on learning the structure
and contingencies in the world.
So you might expect
if you were spending the hard moments
or inside moments
that you would get a reduced latency
in the electrical responses
and then the empirical studies do show that.
So you might be asking
or perhaps not
why did this subject look anywhere at all
if the consequences of action
had no meaning.
So before it started
gathering information to work out
what the rules were
why did it look anywhere?
There was no uncertainty to be resolved
there were no preferences
that needed to be satisfied
and this takes us to
the second sort of
incentives to learning
that I introduced right at the beginning.
The answer is
there's an epistemic value
of updating
the parameters of the model.
So this is not about in states of the model
it's the parameters of in this instance
the likelihood mapping.
So now we're not talking about inference
we're talking about epistemic drives
for learning
which you can think about in terms of novelty.
So now
novel situations become attractive
because they afford the opportunity to learn
what would happen if I did that
what would happen to me in that context
and that's a subtle but very important sort of uncertainty
that can only be resolved
by choosing a policy that's been a novel situation.
Basically, what happens if I do that?
It's not funny, good electric features.
This is actually a little Russian child
but I don't get it.
It's so sad.
I was googling because
the daughtiest and most emotionally rousing
all would happen if I did that
and this is a pretty simple picture
out of the dark
and the fact is that it's actually a Russian child.
So that's what's pushing us.
I want to close now
with a consideration
of something slightly even beyond
novelty
and the intrinsic motivation
for learning the parameters
as opposed to inferring states
and that brings us to that question
that we've already answered.
Who's better?
There's a base-optimal agent
a mathematician can't be beaten
or you.
So I've just told you
that I've shown you using
exactly the same stimuli
and exactly the same rivalries
that a base-optimal agent
needs about 15 moves,
exposures, data points
before it attains 100% of performance.
You were able to do it
usually people can do it by about 18
but you were actually able to do it by 4 or 5.
How on earth,
how could you do that?
It's mathematically impossible.
So how did you do it
at my rat who couldn't do it?
Experience.
That is certainly true.
Fire knowledge from civil tasks.
Ah, good, yes.
So perhaps the experience of civil tasks is good.
Yeah, absolutely.
And also we know that the upper circle
is the one that's safe
which I'm sorry, of course.
And the red doesn't run.
Yeah, but in other words
I did tell the rat that the upper circle
denoted a rule.
We just didn't know what the rule was.
Good point.
Yes, that's very clever.
That is a distraction.
So you're absolutely right.
In fact, because I haven't put any forgetting
into this evidence accumulation scheme
as if the little rat could see everything
in the past meaning
and that's why I left all those outcomes
on the screen for you so you didn't have to remember anything.
Well, I mean, not the outcomes,
but the rat can see all the services
in general.
Right.
But you can.
You can see what the rat can write.
Yes, that's only its eight of us.
However, there was no description on the rat
and, you know, been sent out on three locations
before it made a choice.
So that's a good point.
It's not actually the explanation.
Oh, I see my thought is right.
I think the explanation that I was
groping and hoping for
was the one that we've already had.
So we have experiences of ruinousness.
Yeah.
We know what a rule is
and we've played games that have rules before.
So in fact, I said,
this rat had exactly the same priorities
as I was lying.
It doesn't.
You know what a rule is.
You don't know which rule was in play here,
but you know the shape of the rules
and perhaps that's what you meant.
You had a notion that there were certain
asymmetries of uniquenesses
that characterize rules.
So from the point of view of the
let's go back to the point of view
of where the rule is contained,
very simply,
rules basically mean that there are precise
contingencies on mappings
or probabilistic associations
in one context, but not in others.
So that certain rules are sparsely
in variances which only need to a particular context.
Which means that
if this rat knew
that there were a small number of
likelihood mappings,
all of which were consistent with being a rule,
it could have used that knowledge
to very quickly and more efficiently
accelerate its learning
by comparatively free energy
if it had started with prize
that were consistent with rules.
So what I'm trying to say now
I'll unpack this but that would be in a second.
If we just accumulate experience by adding
every time there's some expected
hidden state or some outcome here,
we can just accumulate experience.
After enough time,
then this pattern will start to emerge here.
So even after 15 accumulation events
there was enough of a structure
in this learned A-matrix
to generate reasonably confident behaviour.
But if we do a priori,
this A-matrix has to have a room-line structure.
We can actually go and get those structures and say,
well, is it,
is my observations,
are the patterns of accumulated coincidences
or contingencies in this likelihood mapping
like this room or like this room
or like this room or like this room.
So in the sense that you were probably
in your head thinking about
is this the right room or are they looking
at an absolute fit?
Is this the right room or an absolute fit?
Is this the right room?
Oh yes, yes, oh yes.
So what you were doing is you were testing
hypotheses about essentially the form
of this likelihood mapping here.
And when you found one
that was consistent with
or rendered the actual outcomes in the past
likely under that rule,
that particular rule or model
now had greater evidence
and you selected that or you did.
And that's technically basic model selection.
Also known as structural.
So that is the process that we're going to assume
is an explanation for that certain
aha moment with, oh, I see the room.
It's basically you found the right structure,
the right form of this sort of mapping here
that suddenly has more evidence than any other form.
And then you commit to that and you have that insight
and you have that understanding
and it can enormously accelerate your learning.
So at the moment,
this is learning.
But you had insight.
You didn't have to wait until you accumulated
an infinite amount of lots of experience
in order to learn the contingency.
You jumped to that insight
by comparing all ways, all plausible ways
that the world could be operating.
So technically,
what we're talking about here
is the maximization of one evidence
in relation to the structure of the model.
Remember, no evidence low bound,
there's an accuracy minus complexity.
So we want to maximize the accuracy
while minimizing the complexity.
And one way we can minimize the complexity,
again please forgive the false substitution,
one way we can minimize the complexity of a model
is by removing redundant model parameters.
So when we apply the complexity term to the parameters,
what we want to do is to reduce
and simplify the model as much as possible.
So one process, one free energy minimizing,
sort of maximizing process
or complexity minimizing process,
that can be applied here is a basic observation.
I can now get this rectangle in there
and test all room-like A matrices
until it finds a good match,
the model or the matrix with the biggest evidence,
and then use that to determine its behavior
and subsequent evidence accumulation.
The metaphor here is
what some people use to understand
complexity minimization in the brain
in terms of the sleep wave cycle.
So I'm just going to tell a story here
that rests upon this imperative
to minimize expected free energy
with a special focus on the...
minimize free energy with a special focus on the complexity,
so making it maximally efficient and minimally redundant.
So the idea is that we accumulate
lots of experiences and associations
and we accumulate those in the previous example
by accumulating conjunctions of causes and consequences,
S's and O's in the A matrix likelihood accounts,
and they correspond in our head to synapses
and connections in the brain.
So during the day, we are exposed to lots of things,
and then everything is connected to everything.
We, from all these spurious associations,
we have an over-parameterized generative model
where everything seems to be associated
with causes and consequences and everything else.
All those quintessential associations
that have been harvested and remembered
by our cell, that's the stage during the day,
then gives us a very, very complicated model,
and then we have a highly accurate model.
But this very accurate model
has a tendency to over-fit and generalize,
so to optimize the free energy,
we also now have to prune it to minimize its complexity.
And that pruning is basically
that free energy minimizing step
with a special focus on the accuracy.
It has a very simple form, which you can't see
because of the font's restitution problem here,
but effectively what it does is it removes redundant
parameters or connections from the generative model.
And people have proposed that this is what sleep is for.
Indeed, the very first kind of algorithms in machine learning
that did this pruning.
So you gather data, you do your associative learning,
and then during the generation of data,
fictive data, whether it's dreaming basically,
you then remove the redundant parameters
or weights in a deep network.
The same idea.
That was called the sleep algorithm,
the wake-sleep algorithm of Geoffrey Hinton.
But exactly the same idea has been taken
by people like Julia Tonoli,
although not articulated in formal terms,
but the notion that we accumulate synapses
during the day due to assistive plasticity,
we have an overly complicated or complex generative model,
and then during sleep,
we ignore the accuracy term
because there is no data to fit,
there is no accuracy,
but we can focus on minimizing complexity
through a process of Bayesian model selection.
And this is because we remove it,
it is also known as Bayesian model reduction.
And that's what this equation says here.
And this is actually a semantic decay.
So it's the decay part of the semantic term,
the spaces that you might see during sleep
or indeed during just thinking,
just thinking about a problem.
As you rise and think about a problem,
rehearsing different solutions,
eliminating redundant connections
to reveal the underlying sculpture,
which is the correct,
or the best maximum evidence explanation.
So when we give this rat,
in prior belief, about the form of the A
in the sense that A's have to be rule-like,
they have to have a form of a rule in the sense
that A unique mapping between causes and consequences
in one or more of these different contexts.
Then after about 12 minutes,
it starts off with this belief about contingencies.
This is the true structure of the world,
the rule, if you like.
And it's accumulating all these experiences,
all these councils, rich-rate parameters.
And after 12 experiences,
this is what it looks like.
This is what it has learned.
And then we ask it to go to sleep.
And then it goes to sleep.
And then under all possible rule-like models,
it removes redundant connections,
namely these connections of diagonal terms
of these quadrants here.
And then if it selects the model,
or in fact just selects those connections
that do not need to be proved or removed,
this is the result.
Just by knowing that this has to come from the set of rule-like.
Like in the matrices,
on the basis of this evidence,
it has accumulated so far,
it can infer by making the model simpler,
this is a structure,
which is now almost identical to the true structure.
So it's this sort of phase transition
that's switched from a learned to a selective model,
a reduced model,
that we are going to try and associate
with this insight and the harm learned.
So these are the results
by comparing this rat with and without
a little sleep between each trial.
So if you remember, as I said,
I've been taking very, very slow,
I was deliberately getting you to think about things,
have a little sleep or introspect
between the evidence,
so you could do your basic model reduction,
your hypothesis testing,
and explore a few possible A-metracies.
Literally with your silences,
even if you weren't able to verbalise it,
you might have had some insight into the rule.
So here is the average performance
with a bit of sleep or thinking about it
without any clear advantage
in terms of performance,
similarly in terms of the negative free energy
without model reduction,
obviously the average difference is good,
so without model reduction,
there is a slower decrease in negative free energy.
With model reduction,
we attain a confident behaviour,
on average, after about six minutes.
So the first trial,
sorry, I think it's the third trial there,
it's the first trial.
So this, the rejected circles here
show the point of insight
where the model that was selected
was the correct model,
and what it shows is that it now can
approximately attain the same sort of performance
that you were able to show earlier on,
possibly slightly less quickly than you were,
but certainly in the same range of the 64 sets of games.
The interesting thing here is that on this trial here,
the blue squares are actually
the basic model selection commitments
to the wrong model,
and it's interesting that,
and these are the superstitious models,
it's interesting that just like you, sir,
after the third exposure,
this rat also committed to the wrong model.
A plausible model, it's a good rule,
it was just the wrong rule in terms,
and had he waited a bit longer,
it would have found the correct model
that he didn't, it was committed to,
because the probability of that being,
in its head, the best explanation
was as great as anything else.
So you won't be things optimal
even though you were wrong.
So that essentially is it,
this is something that we learned here.
The purpose of this talk,
and I apologise for it being
covering a lot of material very quickly,
but what I wanted to do was to
paint a landscape of
inference and free energy optimisation
that contain different levels of optimisation,
and make it clear that
inference and things like
inferring what's actually perception
is placed in the context of learning,
and learning is placed in the context
of what's selection of structural learning,
and you can go on to place
the structure in the context of neurodevelopment,
placing neurodevelopment in the context of evolution,
and so on and so forth,
it attains into cultural learning
and evolution in psychology.
The idea here being that we see
the same kind of process unfolded
at different time scales
and different levels of description,
but it's always the same imperative,
it's basically to minimise
surprise at the moment
or to choose those policies
that entertain and come to the future
that minimise expected surprise,
namely minimise uncertainty.
So just to summarise that,
sources of uncertainty,
we can have uncertainty about
state-of-the-world now,
and we resolve that uncertainty
by perceptual inference,
and active vision or active listening
or active or generally inference,
and planning as inference.
And the things, the intrinsic motivations
or the epistemic value
that drives that particular sort of
optimisation
is essentially the epistemic value
or the salience
that one would afford a particular move
or a particular way
of sampling the world.
But we also have uncertainty about
the parameters of the models
or the contingencies in the world,
and these don't change quickly with time,
these endure over much longer periods of time,
and that requires parametric learning,
it's evidence accumulation
in terms of counts,
the social capacity in the brain,
short and long-term capacity
of the sort that underwrites
experience-dependent capacity.
And the imperatives for
resolving uncertainties about
what we should learn
is basically novelty,
basically learning a new sport,
doing something you haven't done before
so you can learn what it is like
to be in this world
and optimise the parameters of your
generative model
so that you can be skilled
and unsurprised
in this particular context.
Finally,
we have uncertainty about
the very structure of the world itself,
namely the generative model,
and this requires resolution
to try out different models
and learn the right kind of structure,
and I've illustrated this in terms of
basic model selection,
in particular basic model reduction,
by assuming everything,
every part of the structure is possible
and then reducing or pruning
and removing redundant model parameters
again until you've maximised
your model elements
or it's no better than
free energy.
And we can associate that with the
accumulation of knowledge
that drives things like
new development, possibly even evolution.
And we're associating
this structure learning
and consolidation in sleep
as I've hinted at
in maybe just our capacity
to think about things
that underlies
this form of optimisation.
So I will end where we began,
which is with
this classic quote from
Helphoes that captures
the nature of learning
structures and relationships
in the world. Each movement we make
by which we alter the appearance of objects
should be thought of as an experiment
and design to test whether we have
understood correctly the invariant
relations of the phenomena before us,
namely the rules, that is their existence
and definite spatial relations.
As always
it just relates to me to
thank the people whose ideas
that I have been talking about
and to thank you for your attention. Thank you very much.
Thank you.
Also,
one other session
of questions, and I suggest
we'll make sure that it was great to get some
research here.
It's okay.
Let me tell you it was great.
I have a cigarette.
Please tell me it was great.
I think we can continue.
We can continue.
We have a space for questions,
so I suggest
one question at a time
and we have a couple of questions.
We can come back here.
Let's give space to the other way
to cross the question.
Let's ask a single question.
Please.
Thank you so much for the wonderful lecture.
I hope I understand
things that I should understand
or ask a good question.
It seems to me that
as soon as we are moving from
unicellular organisms to
let's say more complicated systems
that the sport believed
could mean potentially at least
two different things
in which in a simpler organisms
it could literally be much more complex.
But as we go more
complex, it's something else
and also the added emphasis
that in which I would like to call it the illusion.
So let's say
a more subjective organization
and so on.
I'm not quite sure whether
we're maximizing that
in the Asian logic
because I would as well not update
the beliefs or I could update
only in a kind of relation by a sense.
And so I don't know how
would that fall into the frame
of this experience.
Very good time.
You're absolutely right.
When I talk about beliefs
I mean some personal beliefs
that are used in the sense
of Bayesian belief,
and technically believe in propagation scenes.
So it just means
the posterior
or the consummation to
a posterior probability distribution
over the causes of something.
So perhaps I should have said that.
So I'm not talking about
a subjective beliefs
on the sort that I can share in the conversation.
So beliefs here
would actually be appropriate
for a thermostat.
If you associate
electrical by physical
properties of a thermostat
as encoding a probability
distribution
then a thermostat can have beliefs.
A bias can have beliefs.
So that begs
the question that you can ask.
What would you have to do
to lift the scheme
to talk about
subjective beliefs
and my beliefs about my beliefs
in particular my beliefs about
my uncertainty
and what I'm going to do about
resolving that uncertainty
or that uncertainty.
I haven't got any simulations
of that sophistication
and one would hope that in the next few years
you will be able to do that.
You talked about optimism bias
I think that's very interesting
because you can see
by the construction of that
respective free energy
into it, prior preferences
which means that
in order to survive
in terms of me
fulfilling my sub-personal beliefs
I have to believe
that the world
will conform to my preferences.
I have to have
optimistic bias
or I will not engage
in those policies
that take me to where I think I should be.
So there's
a slightly counter-intuitive
structure
to this that has to be present
from what you do with physics
that
to be based optimal
you've got to be optimistic
otherwise you'd fall to pieces
or you would try out policies
and then destroy them.
So there is an optimism bias
that even a thermos matter of bias
that's what underwrites
its existence.
Technically
those prior preferences
really shape
the attracting set
or the score different states
in terms of how attractive they are
in terms of what I believe
the source of states are likely
that I should mention in Canada.
So
I can ask the audience
to talk about beliefs
that I am aware of having
and how I might choose
to search information
that may or may not
change my beliefs is a very important
issue.
First of all though
it requires a notion
that you have a alternative model
that includes itself model.
You have to have
a representation
or a self-person belief, a minimal self-hood
that is me actually
forwarding the information
that is perfect if I
will be in the next step
in this next wave
of artificial intelligence
that you equip
these synthetic creatures
with a belief that there are
a single entity
and that their policies
are all embodied in a single entity
so becoming a self
now becomes a simple
complexity minimising
hypothesis
that explains the sorts of sensations
that I get
clearly a virus of it
doesn't have a self-model
but you and I clearly did
and of course once you have a self-model
that means you can start
to talk about
your beliefs
with another
creature that is
sufficiently similar to you
and then we can get into the issue
of self-discretions
and so we can talk about that
for a long time
and I'm sure lots of people here could
come back to your question about
choosing what sorts of uncertainty
and when to resolve that uncertainty
that's a very, very interesting question
I think probably psychologists would be more skilled
at answering that kind of question
the kinds of scenarios
we're talking about
are
if you know you're going to see
say I gave you a book
the doctor's notebook
and
I gave to you
and you're going to read it
I said do you want to know the ending
and you would say probably not
you probably don't want to know the ending
that would be a spoiler
so that's a
paradoxical situation
in which you deliberately
do not want to reduce your uncertainty
so that seems to go against this
to resolve uncertainty
so you have to think of what sort of
alternative model for a prior release
would license
that kind of behavior as ultimately
uncertainty
reducing
I think the argument would probably go
along the following lines
that I'm removing the opportunity
for novelty or the savings
when you actually read the last page
you can see what actually happens to the doctor
so you know that
in the future
you will have a resolution of uncertainty
and you don't want to destroy
that epistemic affordance
or that epistemic value
you know now there's an intrinsic motivation
that will take you to a place
where your uncertainty will be resolved
but I can destroy that
by telling the end of the
end of the story
but that's exactly the
one I'm asking so
so
what I'm asking is
exactly that
as a system with beliefs about beliefs
all of this stuff as a conscious
whole type of system
and maybe my goal is not just
actually my main goal
is not just certification and survival
there are underlying goals of that definitely
but one of my main goals
is just to feel good
simply as that
and so I'm not sure if
maximizing that
feeling good is necessarily
easy
I don't know if it's
clearly for these total problems
that you mentioned, definitely easy
and takes a lot of sense
but in that particular thing
of subjectively feeling good
I don't know if it's easy or not
that's basically what I'm asking
so you've introduced another dimension here
which is feelings and emotions
so
that now
leads to the table of discussion
how do you put emotions
and feelings and evidences
of these structures
that itself is another
industry which
I would say psychologists probably know more about
but in fact I think physiologists
would also have a lot to say about this
so at the moment
my reading of the literature
and discussions
is that we're now trying to
repair
things like
well the
James Landlach
hypothesis on the one hand
and
effective control theories on the other hand
through this active influence framework
so the reparation
here
is really
about
controlling your body
so active influence
when put in and a body
physiologically situated
context
means that much of the world
that I am trying to predict
is not out there, it's actually here
so I have
exactly the same imperatives
when trying to understand the contingencies
and infer
hidden states or external states of the world
that apply
to my body states literally
explaining my gut sensations
in terms of gut feelings
but of course it's a deep judging model
those
hypotheses have to predict
not just the
heart rate
blood pressure, blood loss, morality
with
spiritual rates
all the interceptive
pain sensations
some aspects of somatic sensation
they have to also predict
all the
hearing and seeing and all the pro-social
states of the world
so what I now have
is a hypothesis
or a sub-personal belief
at a very high level in the judging model
that has consequences for
or is able to predict not just
bodily states
but also the
external non-bodily
pro-social perhaps
states
of affairs out there
that I would expect to encounter
when I felt like this
or when I sensed my body like this
so the move there is really to
endow
unemotional
unveiled unfeeling
influences
about states of the world
so you couldn't say that this simulated
rant and had any feelings
about what was going on
but if I gave it a little synthetic heart
and
was producing
predictions of a particular heart rate
when it was uncertain about what to do
you can start to see
if you could build deeper and deeper layers
and say
have the hypothesis or the rule
who I'm in
a stressed and happy state
what does that mean
well that means I predict
I would feel sensations
of arousal from my autonomic nervous system
it also means
that I am unlikely
not to know what to do next
I'm going to be very uncertain
in terms of my belief in distribution
of the policies
which means that
you will start to associate
a
some person of belief about uncertainty
about what to do
as encouraged by dopamine
with a particular set of
bodily feelings
I think at that level you've got all the ingredients
for a subjective feeling
it's not an explanation for
but certainly you've created
a hierarchical objective model
with multimodal capacities
that now
accommodates at least
the arguments about feelings
and valence and subjectivity
and selfhood
in terms of
spanning both body feelings
and ex receptive beliefs
and continuities
one final twist to that argument
is if you are now
in possession
of a genuine model
that has a very high level
self-modeling
level to a deep
model
it means that you can now learn
what to attend to
in terms of
optimizing the precision of various likelihood
in that case between one level
and the next level
and the precision of the uncertainty
of the volatility of state transitions
and if you can learn that
that effectively means that
you can now predict the precision
of these likelihood mapings
and transition converters
which means that now
you have the form of mental action
you can actually now change
your inference message passing
lower in the hierarchy
without actually moving anything
or engaging any autonomic reflexes
so I think
the challenge would be to build
these models that are sufficiently deeper
to support mental action
and things like sensory attenuation
and attention
that will be expressed
in not only in terms of what I say
and what I do or where I look
but also in terms of my bodily states
through engaging autonomic reflexes
I think on that stage you start to have a much more
complete
model of a feeling
creature. We haven't seen that
there have been some very
provisional moves in that direction
by a gentleman called
Michael Allen
from Denmark where he's built
one of these little rats
that actually has a heartbeat
and gets frightened
when it sees a cat
so you actually see him getting frightened
and he doesn't have representations of himself
he doesn't have a selfhood
but it certainly starts to show
that's actually Marko
which Marko is of
my question is regarding
the inspirations that people
have taken from the report about
free energy
so a lot of your literature now
regarding AI
is just the basics of it
as you know the gradient center
is causing some of the error
so
as you know
while there have been a lot of strides in narrow
AI
they haven't been able to produce a model
of general AI
so for this explicit
as in brute force
minimization
prediction errors have not produced
general AI
and
my question is that would you think
this approach would eventually
to general AI
and one maybe parallel to that would be
because we evolved
from much simpler creatures
whose maybe some
other modules of sensory
processing
could be more easily explained
with linear functions
maybe and amoeba is
not an amoeba but a creature
that is living on the surface of the ocean
with very simple neurons
that when a prey comes
it catches it
so maybe we can model that with very simple linear models
and
because we evolved from creatures like that
we have some other
modules that work
best
with other types of simple models
and maybe
should we implement that in AI
and
this is basically what was
better
I mean again you bring up lots of
very interesting themes
so
let's take the last theme
first
can you look at
the structure learning
of what the soul will be talking about
from an evolutionary perspective
from single cell organisms
to not necessarily arrangements
through to not
cessile creatures
that swim around or ramble down the right through
but
if we are on the top of the evolution
of the structure learning
I think that's exactly
the right way to look at this
I think
if you just tackle deep
questions about
the move from
first of all conglomerates
or macro molecules to a cell
that's a big thing
and immediately it suggests
if you remember back to the Markov blacking forms
certainly a macro molecule has a Markov blacking
because it has internal
electrochemical configurations
in computational chemistry
it can't be seen from the outside
so it has a Markov blacking
there's not sort of surface
that we can normally associate with things like cells
so now you see the emergence
of the Markov blacking
so the Markov blacking is moving from
a set of macro molecules
so proteins
and nucleic acids
and a single cell
with its own Markov blacking
now becomes the cell surface
and then you have the interesting question
of how an Earth in multicellular organisms
arise because there's a paradox here
in order to be a multicellular organism
and to reproduce
the stem cell at the middle
still has to retain a reproductive capacity
but if it's now
enshrined itself with
non-stem cell, non-reproducing
surface cells
that constitute each Markov blacking
to be a multicellular organism
it means that some cells
have given up the capacity to reproduce
but that sells what it means
a city to do from evolution
from the perspective of evolution
from the single cell perspective
but from the point of view of the multicellular
perspective that's the right move
to make in terms of minimizing
free energy
in the sense of maximizing
adaptive fitness so I'm assuming
here
one can read
the mechanics of evolution
so price equation
or replicated dynamics
as a basic filter
and in that context, adaptive fitness
just is marginal output
and it just is model evidence
so we can read the evolution
as optimizing free energy
so from the point of view of the multicellular organism
now you have
if you let me resolve
the paradox by saying
well actually unit of selection
is now a multicellular organism
not a single cell
and in that sense
I think you can easily
motivate
there being very elemental
multicellular
not deep
simple feedback
only static light mechanisms
that are perfectly good
to describe regenerative models
of increasing cell organisms
that then become more and more elaborate
as you build
active infant schemes
of increasing
hierarchical depth
then you will lose
the
you won't be able to describe the behavior
of complicated multicellular organisms
just as simple homeostatic
they'll think about what actually is homeostatic
so a thermostatic
so very often
in my Sussex University
I try to
demystify the free energy principle
which I hate doing
the more magic it is the better
but they try to
they try to demystify
by talking about the basic thermostat
just because you're exactly that sort of manager
a simple linear feedback
dynamical system
or a gender model based upon a simple linear differential equation
and that's perfectly
good to describe
a thermostat
and it's also perfectly good
for describing homeostasis
in you and me
when you start to layer upon layer upon layer
you start to contextualize
homeostasis
then you get to allostasis
and then you get to
regenerative models
whose inversion
will really optimize
putting the animal in a situation
to make the homeostasis work better
because the homeostasis notion
of this homeostatic
and in one branch
of self-organization
that was really inception
of psychetics in the UK
it was exactly that sort of
growing more deeper and deeper models
and in using the word deeper and deeper models
of course you then
are talking about the key architectural
principle
key structural learning that has emerged
in machine learning
so the warm
structural aspect
of regenerative models
that keeps coming
at you
from all lines of theorizing
is the very thing that defines
state of the art in the second wave
of artificial intelligence
it's just hierarchical
so in that sense
I think there's a lot of sympathy
implicit
plausibility in
using deep convolution
networks for example
that have this hierarchical structure
so I'm trying to get to your first question
so
I think your first question would be
okay if that's run
and we've had deep models
deep convolution networks around
for now
some people argue this might end
but in terms of
or probably it's the other person
five or ten years ago
so why haven't we
realised the dream
of having
robotic companions or friends
or robo pets and robo dolls
and I think
the answer there
let me rephrase that
why haven't we had the third wave
of a good old-fashioned AI
I think that's wave one
now if you had neural networks
that propagation
wave two
things that play alpha go
things that play chess in the life
and then now we're waiting for wave three
which is you present
he said our instinct
what I meant was
he said AGI
so that's generalised
artificial intelligence
that's the next home in the world
artificial intelligence community
I think so people call that the third wave
so
my question is that some of the more contentious
popular
science AI writers
are now bored with second wave
deep models come to life
now, intellectually
and it's doing
as best as it's ever going to do
it's basically being used as universal
function approximators
that basically give an input
that you want and you write that down
and that's a prime reference for a value function
then you could
leverage more in the justice and computer science
to do that
so
are we stuck there?
is there going to be a third wave?
I'm sure there will be
and I think you can see glimpses of that
now
and I think that third wave
will only arrive
when people
get to the central model of a general model
so
most neural network stuff
is actually cast just in terms of universal
function approximators
that do mapping from inputs
to outputs
with no explicit ability
or structure or mechanics
to get in there and think about the general model
or its deep temporal structure
although it has deep structure
in terms of static
functions approximations
usually have
a well structured temporal structure
that would do
language or would
actually be able to
plan to go to the shops
or have a conversation with you
roughly where you are going in your narrative
or in your story
so the question then becomes
where can you find generative models
in current deep learning
and you can find them
people often cite generative
adversarial methods
I don't actually think they quite got it
I do think variation order encoders do have them
so
is this interesting
for everybody?
probably
for these two it's interesting
so
variation order encoders are
the long day version
of an auto encoder
that has a characteristic bottleneck architecture
and the reason that I
liken those to
something that is
sympathetic to a generative model
is that in their very architecture
there is a very crude
prior which is probably
absolutely correct
that a high dimensional data space
is actually caused by
a low dimensional manifold of causes
so the very fact that they try
and represent
by passing the
encoder
the decoder through
a small number of hidden nodes
in the waist
or the pinch of bottleneck
means that they are effectively
trying to
predict
with no prediction
a high dimensional consequence
within data space
center space and image space
in terms of a low dimensional
belief
in low dimensional structure
so they've short circuited
the structure learning
and found the generic architecture
that all good hierarchical generation models
will have
that the world is actually much simpler
in terms of the weights cause
than it might appear in another data channels
available to you
and I often look at
variation order encoders that have
this waistline structure
so by bottleneck
I'm talking about
the data coming in this
so
the variation order encoder
is basically learning
inputs which will cause safe observations
mapping them to hidden states
then generating
predicted outputs
from the
the hidden states
and the objective is to minimize effectively
the difference between
the observed and the predicted inputs
in fact it's a bad propagation
and there is a greater descent
on variation family tree
it's a new gradient descent
so f
physicist Myles South
is equal to the elbow
and the elbow is
the quantity that you work out
I look at this as basically
if you now imagine
flipping this down
what you actually have
is the sort of
architecture we've been talking about
ascending
sensory evidence
likelihood terms or low levels
of a hierarchical gender model
with descending messages
that in a predicted coding architecture
exactly the predictions of
what's going on down here
in the NDP scheme
so there's still descending messages
but a slightly different sort
so I think variation order encoders
are getting very close now
to
a technology that you could
start to take control over
the structure of this bottleneck
and put your prime knowledge
about the generative models
within the purpose of your deland
you have to think very carefully about
what would I need to know
to generate that meaningful amount
that had meaning, that had a story
if I wanted to do visually
for example
what would I need to know in terms of
this manifold or some other dimensional state
that would enable me to generate
both a video picture
of what I would see when I had a camera
and also what I would hear
if I was looking to science or psychics
and the like
and you start to build instruction
to these very short encoders
that isn't deeply informed by
prior knowledge about the world
in which this particular artifact has to validate
I think we will start to get
a
phase transition in terms of performance
of course once you've got it
in this hierarchical form
you can now do what we were talking about
which is now putting a cell phone on top
and at that point
it starts to get very interesting
two little twists
or two things
that people
dealing with deep learning
very short encoders need to
contend with
which have not been contended with
one other thing is
if you notice that the data here
are just supplied
but the whole point of active inference
is really
to make inferences about space in the world
that give you your
inspected free energy
which is not part of current deep learning
to act
while you're acting to select the data
so the real problem
is how you use
these high level representations
to actually select
or attend to a small part of
data space
to go and look at that Wikipedia page
or go and mine that part
of this big text
or go and look at these examples
so the moment
the way that machine learning gets around this
in my view
is by looking at all the data
hence big data
that's I think
the best direction
is to think about
active learning and active inference
and I mean active learning in the sense of
data with high in the 1990s
what data point would you really like
if you wanted to resolve your uncertainty
so I'll give you three points
and what's the shape of the line
collecting these new points
is it like that
is it like that
is it like that
you don't know
you have an uncertainty over these functions
and I'll say to you
now
what point would you like to measure
and active learning is a problem now
or say
a 90% assurance like that
a 10% assurance like that
if I knew this data point
I would be able to resolve my uncertainty
so I'll
ask for a better point here
I'll go and query the world
query my data
at that point because that is the one
that has the greatest spending value
this is the intrinsic motivation
we're talking about
I haven't seen anything like that currently
now I'm sure there are some very bright young
PhD students here
working in the Google
doing the sort of thing
they may have or have already
discovered and expected free energy
you know, maybe calling it something else
I think that's the next
one of the two
in fact
I've put them together
the two points I was going to make wasn't it
active data selection
choosing, interrogating the world
in a way that is sensitive
to your beliefs
and in so doing
emphasising
the fact that these things have to be active
it's not just
passive isn't it
you have to devise artifacts
or go and get the data
they need to do what they think they should be doing
the second thing is dynamics
there are no messages
cache or temporary embed
or
keep a record
of time
so these are good for recognising
pictures
they're not very good for recognising
films and movies
so you need to
unwrap these
in time
before I can get
that sensible sort of
generalised artificial intelligence
but I don't see why that can't be done
with the two years but I repeat
I'm sure somewhere there's an IPS paper
with the neuro IPS paper
being built
do you have any
what are your favourites in that sense
I think
evolutionary algorithms are a way to go
because they
remove some of the rigidity
of the classical
sort of algorithms in AI
whenever I saw
anything
that implements a good version
of genetic algorithms
in AI
the result is always
so I think
genetic variation
is available in the
evolutionary books
we can get two structures like this
but implicitly
without coding it
I tend to agree
because genetic algorithms
are
universal optimisers
given the right objective function
and if their data fitness
is real-energy I'm happy with that
thank you so much for that
it's a brilliant chance also
of asking questions
so I don't even use a speech
thanks for the lecture
I have just one question
what do you think about
Penrose computer model
and the second
for example if we
use VR game
not a game with real objects
but a game like as a
Riseners card when we use not real
objects but only your imagination
to predict the situation in the future stage
will your model change
or likely change or will it change
or thanks
oh sorry
that's the question
to tell me about Penrose as well
no
we need to know what Penrose is
what do you mean by Penrose
I mean if I write
your colleague Penrose said that
in the brain there are some
important models I mean narrow models
and he tried
to find that
so I agree with your colleague a lot
I mean
when you try to meet some
like brain machine computers
or models
of narrow learning
and so on
so it's this theory
by Roger Penrose
that says that
you can't explain the function
without all the quantum computing
so that's a simple statement
and he already
I think it was probably
maybe like 20 years ago or more
he teamed up with some like
new physiology guy
actually tried to find structures
with like 1.10 pollutants away
but it's highly controversial theory
and basically this basic theory
has been disproved
but there are new investments
like phosphorus atoms
phosphorus I think
and some new guy
I don't remember his name
tried to compute like 1.10
pollutants for those atoms
like in brain structures
so that's the thing
is that enough input
I'm learning something
that's good thank you very much
okay
I don't know a lot of Penrose
I know this person assisted
a funny story
but I don't know his name
so we're talking about the
sort of the quantum
aspect of microchip
if I learn the good old theory
why do I like doing the brain
so if I learn
the good old theory because brain
like humans can invent everything
and prove that it's not
doable and there's like
a blue crawl with quantum computer
so very much the quantum computer
and
okay
right
so
I'll try out the first question
but I'll have to get the imagination
a sort of fictive simulation
if you're letting like
implementation is that is
if you want a facetious answer
I think it is a
technically a low number of rubbish
and
the reason I think it's a low number of rubbish
is that if it's predicated
on things like girl's theory
that's predicated on
that's what
precisely
and so it's a
formal local logic
and yet all the physics that I know
and everything that I have
spoken about
inherits from probability distributions
and density dynamics and indeed
the way it functions in quantum mechanics
where there is no modal logic
the only inference you can
make is adaptive
so any book that I read
with that I actually was made to read
girl's theory and I went to
the script and I still
thought it was a low number of rubbish
are you ready?
have you read beyond the theory?
well I've read before so I've been great
oh you should read the original
I think it is
mostly just like some general RRCs
that should rebuild
quite a bit more
I was being deliberately
entertaining
and controversial
so don't tell anybody else sorry if I said that
but the point I'm making here
is all the mechanics that we have
been talking about
rests upon probability theory
so the girl's
the girl's theory does not belong
it is
it is
not if you like
a motivation to invoke
some loophole or some
repair or some
violation of logical deduction
because there is no logic to be reduced
you can only act as
and that I would imagine would be a much more comfortable
than that position for people
in quantum computing
so it comes to do I think
is there any quantum level
influence in the brain
and
the reason I said rubbish confidently
is that it rests
very much on this hierarchical
issue we were talking about
so in
the physics of
or the density
or the sonic dynamics of physics
that covers statistical
thermodynamics as stochastic
thermodynamics and quantum mechanics
what happens is
as you move from
one hierarchical level to the next
things slow down
so
what we're looking at
at this is that the very
very small happens very
very quickly
so from the perspective if you remember
of that launch that formulation
where you have the random fluctuations
what we're talking about
at the quantum level
the random fluctuations are occurring
so quickly that
it doesn't have any meaning to say that
something is here the same position
is one state and there's a way to change
a position and its fluctuation
is so quickly it makes no
it has no meaning to say I could measure
this thing that this state of it
from that point of my eyes at the
certain principle
but as you then move up through
ensembles of small things
like atoms and molecules
past
so we are the quantum
planets
special this is
about a millimetre
then the
the ensemble dynamics slow down a lot
and then you get to the realm of
electro physiological
so no longer we're talking about
Pica sense of mass we're talking about
the essential sort and the like
and as you get up to
the motion of the parachute bodies, whether other fluctuations disappear completely,
you're left with time scales on a cosmological scale.
So what I'm saying is that if you want to do the cognitive science,
or you want to build generalized artificial intelligence machines
that operate at this space-time scale, they cannot be quantum machines.
There have to be all the random fluctuations
that make the wave equation and appropriate description
of very, very small quantum particles.
Effectively, I won't fit the purpose if that may become absorbed
into the random fluctuations at the higher level.
So what you inherit from the lower level are just the slow things.
So these are one of the principles of, in physics, the center map of theorem,
instead of getting something called the enslaving principle.
It simply means as you get bigger and bigger and bigger
of the size of you and me, or by the ARG machine,
it cannot operate at the quantum level.
So the great inflows that look as if they're doing inference,
i.e. quantum computation, cannot operate at the quantum level.
That's why I said it's rubbish.
Does that mean that quantum computing doesn't have a real,
no, I think they might not be important at all.
So I've heard people in quantum computing
talking very convincingly and very competitively
about the possibilities for quantum computing over the next 20 years.
But I think it will be in the service of interfacing with the scale
on which we actually live and operate.
That's the big challenge, is actually scaling them up
in such a way that you can harness the power of
probabilistic inference as opposed to logic,
or fashion AI, in a way that talks to the microscopic world
which we actually operate in.
So do you think quantum computing here?
I don't think so.
Well, you tell us.
There's all sorts of different niche ways that you can solve.
Maybe some of you read a couple of days ago,
like there was a leaked paper that the Google
actually achieved quantum supremacy,
and there was a lot of headlines,
but actually what they did is they built a random quantum circuit
and then simulated it, and this relation to cumulotype
actually sent into distribution from the circuit.
But circuit was actually quite big compared to, like, toy circuits.
So it's the first time that quantum supremacy...
There is no use of it, but it's a large quantum supremacy.
And yeah, scaling up is very, very, very hard
because humans can harness very quickly,
and especially if you have a very high temperature,
like human brain temperature.
I forgot to say that quantum supremacy,
that was what we were all talking about,
and there's a wonderful term, isn't it?
Jones sulfide.
There's a bit of off-topic.
It's very entertaining in quantum computing.
You talk about what's going to happen in the next few years.
They're also very convincing.
There's good physics behind it.
So the second thing, does this hold in imagination?
Yeah, absolutely.
In a sense, the whole story about having this influence
by selecting those policies
that are going to optimise the expected free energy
or the free energy that you expect
literally means that you are operating in an effective,
imaginal, counterfactual world
that can only be described as imaginal.
So this is just like simulation theory and psychology.
So all of the panning of even in the simple simulated
parameters during the rule-learning task,
it was simulating looking at the circles in different orders
and working out in its head, in its imagination
how much uncertainty would I resolve on average
hence the expected bit
and how likely am I going to be getting the positive feedback
you've got for the right colour.
So it was literally exploring a whole bunch of plausible sequences
in parallel and then evaluating each of the sequences in parallel
and then scoring them in terms of the expected free energy
and after you've done that, it usually takes literally
on the computer, literally, about 250 milliseconds to do that
which is interesting because that, again,
brings us to this sort of clock time
for interrogating the world that we actually use.
Then it selects its favourite policy
and then chooses an action unit that goes against the expected data.
But everything up to that point is effectively in imagination.
So I think that's important because just technically,
when you write down in your software
the generated model to reproduce this kind of behaviour
which has an intentionality about it, it has a purpose
and it also brings to the table the notion
of selecting a model of plausible policies.
Also the software thing has a choice about it
and it's not free will but certainly there's a selection operator
which is a thermostat wind tower.
So I think having that capacity to model the future
takes us from the world of thermostats and possibly viruses
into the world of animals and people
and generalised artificial intelligence.
And technically it requires you to have a generated model of the future
and that's a big thing because once you've got a generated model
of the future which you need to roll out the consequences
of a particular policy, policy A, policy B, policy C
then you essentially have a primitive or an elemental form
of prospection and post-action.
And as you move through time,
those same neural populations that were encoding the consequences
after four moves into the future, after you've made those four moves
the neurons that were encoding the states of the world now
now are encoding the past.
So these are members of this, this is what we're going to do.
So another way of looking at this if you're an electric physiologist
or a linear psychologist is that in order to act
in a prospective way to do planning as infants
then you need to have a generated model of the past
that necessarily entails a generated model of the future.
So you need to have a generated model of the future consequences of action
that necessarily entails after a period of time a model of the past
which is the same, so this probably needs to have a working memory.
And as soon as you've got a working memory
then you've got the opportunity to move around in time to a certain extent.
Technically what that means is you now have to generalise
the great recorded descent of back propagation of errors
now you have to generalise this in the sense of a pacing filter
and you now have to use something called a variational message passing
which is similar to something in machine learning
coming forward and backward and backward.
But now we're passing messages from the simulated future
back to the present and from the present into the future to do the simulations
and some really interesting things in the message passing schedule
that comes along with that capacity.
So I think it's an important question to which the answer is absolutely yes
for systems or creatures that have generated models that include their future
not every generated model does, don't you?
I think we do.
I think that's the temple bit of investment I was trying to intimately have
and I've got it in a second way in AI.
There certainly is a lot of work on deep free searches
and in economic or neuro-employment studies
but you won't find very much of that in variational indicators
on deep learning how to learn it.
You may learn less from it.
I just want to ask a question.
We'll see if there are any approaches that I'm going to leave.
I have a very simple question.
When you showed this Archibald door painting
you said that a brain tried to find the better way to explain sensory input.
My question is why the face is better than an onion?
Is it clear?
Yes.
So the question is when forming a percept when looking at this image
why is a face better than an onion?
It's a deep philosophical question.
Why is a face better than an onion?
The people is important for us as surroundings.
Okay, so this is just why people are more important than onions.
That's not to say that you don't think onions are good.
I use this really just to entertain and to emphasise
that most of our perceptual synthesis is actually a very constructive act
that is not a product or sensation.
It's something that we went to the tables.
So I walked over and interpreted my technique over and interpreted the message
which was meant to be conveyed in this particular example.
However, if you wanted to quantify better,
why is a face better than an onion?
Whenever somebody asks me that question
I always hear them to say does it have greater or lesser evidence or free energy?
That's why I hear them.
It seems that I don't have much more evidence from this instance.
Yes, you could argue that, but you could also argue.
Let me try and operationalise my interpretation of better as the most evidence
or the least physics free energy.
One way that you could score the evidence for a particular model
is on how easy it would be to generate a painting.
For example, this image here.
So one simple way of thinking about what is the best perceptual explanation for this painting
is what is the smallest amount of information I would have to give you
to go away without seeing this painting and reproduce a painting that looked as similar as possible.
If you take that as a measure of goodness
which has implicit in it a complexity constraint
in the sense I can only tell you one thing.
So I'm just using one degree of freedom to generate, to ask you as a generative process
something that looks like this.
If I ask you to go away and draw a face with one colour, so I paint a face
and I asked you to go away and paint an onion
and you brought your paintings back and I asked the audience
which is the painting that most resembles this actual realisation.
My guess is it would be the face, not the onion.
So it's best in the sense that it provides the most information
about how you generate these sensory consequences.
Best in the sense if under the constraint of minimum complexity
I can only tell you the name of one object, onion or face
you can then score the goodness in terms of the similarity
between a whole set of paintings or faces
and a whole set of paintings of onions.
I'm just literally measuring the k-art divergence between
you could use some residual or some square error
to actually measure which was better.
So I think faces matter, they do obviously,
but because they matter they are going to be
they matter because they are such a common currency
so the k-art in the world is populated by more faces
than onions in my house.
So in that sense we have that construct
as part of our derivative models and that would enable us
and that's what we see effects.
If we lived in a world of onions I would have chosen different examples.
Does that make sense?
It's an interesting question, it forces you to find
better and then how would you measure better?
Because sometimes you don't have access to the derivative model
so you can't work out the variation of the energy.
One trick is actually to use cross-validation accuracy
so that's very common as a distance.
If you don't know the form of the derivative model
when you've got some say variation auto-coder here for example
one way of estimating the evidence for the implicit derivative model
and you couldn't actually write it down formally
because it's not exquisite in this formulation
is just to look at the log probability of some observed data
under the privatisation of your genetic model
based upon some prior experience of lots of faces.
So people use that all the time,
using the cross-validation accuracy in machine learning for example
now this is a better model of my data than this model
because this one has a better cross-validation accuracy
it more accurately predicts these new data
i.e. the test set after being trained on the training set relative to this kind of model.
So what I'm suggesting is it will produce a cross-validation accuracy
to effectively answer your question
why some face better than the plan on here.
It's all in the eye of the development.
It's all in the eyes of the genetic model making the inference.
I use that phrase because this picture actually came from a book by Henry Cadell
who emphasised this constructive aspect
from a wonderful treatment of V&E's cultural revolution in the term of the Massachusetts
and emphasised that this is all in the eye of the development.
So it's only better in the eye of some of us as beholders
fully acknowledging that maybe audio beholders here would think the other is better.
So we have a space for all this last question.
Very good that you thought about which parts of our intelligence are innate
and which parts are acquired with experience.
So in other words, if we are building an artificial agent
how much and what exactly should we incorporate into that agent
so that it will then interact with the world and become intelligent on its own?
So I think a lot of it will be...
The answer to that actually speaks a lot of what we've all been talking about
which is the capacity to do structural work.
So sometimes referral has learning to learn.
So if you're an evolutionary biologist
this would be selective susceptibility.
So sometimes it's more important to get your mutation rights right
than it is to actually get the right genotype
that specifies the adaptably fit phenotype.
If you're in a fast changing environment
then you need to actually optimise your mutation rights
so you can adapt on the rate that matches the volatility of your particular environment
where that volatility could be caused by other species or a video consequence.
So I think that question is an excellent question.
Certainly in the context that everything we'll be talking about
is a result of structural learning at multiple scales including evolution.
We need to take it to cosmological level.
So we're looking at the products of a structural learning of Bayesian model selection process.
We can look at it to see if it is the products of a Bayesian model selection process.
The question is what is selected?
I think that's a really interesting question because I suspect it's really the opportunities to learn
and the ways in which we learn should be selected.
Not the content of what is learned.
Again, I'm outside my comfort zone so I would imagine we don't have a genetic code for language
but we do have a genetic code for the auditory hierarchies
that have the right number of levels and potential for connections
and the right degree of neuroplasticity and critical planes of development
that enable us to learn languages very efficiently
more than save them for a fish.
So I think it's a great question because it highlights
what are the really important things that need to be innately inherited
during some very large-scale, long-term, belief-updated structural learning.
I think it's more the hyperparameters that actually enable the learning.
So we've got planning resilience but we've also got learning to learn
the right level of learnability.
In that answer, I think it's one of another big outstanding challenges in machine learning and statistics
which is structural learning.
I think a lot of people are focusing on that to my knowledge.
How do we actually grown different hypotheses or how do we, if you like genetic algorithms,
how do you make the slight variations on the existing models to explore a new part of model space
and then select it on the basis of the pathogen from the expected time-advanced free energy or adaptive fitness?
Once you can specify a space of field time such as miles or miles or whatever,
it's easy then to prove away the ones that are not better than the other ones.
But how do you actually explore that space in the first place?
And of course, if that's what really important, then it must also be subject to evolutionary pressure.
So there must be something about us that not only are we born to learn from our course specifics,
our mothers and our fathers and our teachers,
but there must be something in that which also enables us as a species to explore the right sort of phenotypic configurations
that are the physical instantiation of our genetic models.
So evolution has found some way to do good explorations on model space,
which is why I like genetic algorithms.
My suspicion is that genetic algorithms themselves have evolved.
The genetics and genetic algorithms are themselves a product of a large-scale evolution pressure
to find really robust ways to explore structure spaces or the interior spaces.
But in cognitive science, I think that is a particularly pressing issue.
Technically, we can reduce the models.
You've seen an example of Bayesian model reduction by the pruning.
What we deliberately have is a very efficient way of growing those models
that do a lot of genetic algorithms, which is extraordinary models,
in a landscape of fitness or free energy that may not even be continuous,
I don't know, sort of at the level of the students' properties.
So what do you think?
So I'm just now puzzled with the multitude of AI systems that exist.
So we have neural networks, we have decision trees, we have every, I don't know,
every month there is a new model suggested for intelligence.
But on that hand, if we look at what we know, a lot of what we know we acquire during life.
So people say that this kind of decision-making is important,
but if we trace it back, we have learned it.
So we don't need it at birth explicitly.
And if we look at all the things, there are very few probably things left
that should be explicitly there at birth.
So this essence of intelligence seems to be quite simple.
It should be, because if we incorporate a lot of specific stuff,
like specific models that are good for this situation or that situation,
this is all well and good and it works in animals very well,
but it doesn't generalize, so we tackle the realm of problems that humans deal with.
The conclusion is that there should be some more basic and much more general approach,
maybe along the lines of this basic thinking.
So if we incorporate just this basic idea of its own,
can we develop from it by interacting with the world
and by incorporating some rules that are there in the world,
incorporating them into our intelligence,
can we describe all the other things and all the other models that exist in machine learning
and don't get insist there.
This is just a hypothesis and maybe if we push this far enough,
we could get there.
I agree with you, Tali, it's all about the learning and the acquisition
and providing a lot of structures that are needed to adapt to the journey of the government.
So, Professor Fiston already gave four lectures in Moscow.
I think we're all grateful for this visit and exciting opportunity to chat with you.
On behalf of the audience, I can thank you very much for the reducing of ambiguity about our brains.
Thank you so much and let's say many thanks to Professor Tali Fiston for his thoughts.
That's it. Thank you very much for visiting us.
