Thank you very much and welcome back to those of you who have been at the previous lectures.
I promised yesterday that I would be repeating the open lecture on Monday.
I've decided not to do that because of the questions that I've been asked.
So I thought it would be more interesting for you to see how these theoretical and mathematical ideas translate in terms of how the brain works and what can go wrong when the inference is somehow broken.
So I'm going to use schizophrenia as an example of false inference.
So I'm going to give a lecture really about how to use active inference, the free energy principle, to understand in detail some of the signs and symptoms of psychiatry using schizophrenia as a canonical example.
Many of the principles that I'll be talking about though can be applied to other conditions like autism, depression, anxiety, anything that you can think of that comes under the heading of a psychiatric syndrome can often be understood in terms of a broken basal brain.
So I hope that's all right. Is that all right? So this is going to be slightly more practical, clinically motivated use of the theory.
Having said that, I did promise I will repeat some of the slides and I will be repeating some of the slides that you saw yesterday and on Monday.
So please feel free to interrupt at any point. This is the purpose of these sessions so that we can engage in conversation.
So this talk is going to have three parts. The first is going to be a brief introduction to schizophrenia as a psychiatric syndrome for those people who do not or are not familiar with schizophrenia.
And I'm going to take the received knowledge, the received wisdom and our understanding of the things that might be wrong in schizophrenia to explain those false computations in terms of false inference using exactly the same ideas that we talked about yesterday.
So the second half of the talk will briefly summarise active inference and predictive coding, reprising the ideas and the formalism that we talked about yesterday with a special focus on something called precision.
So I mentioned that a number of times in terms of psychatic suppression, attention, the inverse variability of the quality of sensory inference.
So this talk is going to focus very much on the importance of estimating not hidden or latent or external states of the world, but the precision or the quality or the reliability of the sensory evidence about those states of the world.
And what we will see is many psychiatric syndromes can be understood as a failure to properly estimate the precision of the sensory evidence that we're going to gather actively in our social and physical world.
And just to emphasise the utility, the usefulness of having a formal or a computational model of inference, I'm going to use simulations of a small creature that has hallucinations and delusions.
It's going to be a songbird and we'll see that later on.
So the basic idea is as follows.
If it's the case that psychology in its broadest sense, and I'm absorbing or subsuming action and perception under all of psychology, if psychology is inference, if it is the process of inferring or estimating
what's out there beyond our sensory veil, beyond our Markov blanket, then it means that psychopathology, abnormal psychology, extreme psychology has to correspond to abnormal or false inference.
And I mean that in exactly the same sense that if you did an inference statistically on your data by performing a statistical test, what would happen if your statistical test was broken?
If your t-test in your computer somehow became broken, what would it be like?
And you can see immediately as scientists, if you couldn't trust your statistical tests, you would be led to very, very wrong conclusions about the causes of your data and that would lead to accepting the wrong hypotheses about the way your part of the scientific world was actually structured.
And that's exactly the story that we're going to tell now.
And in particular, it's not any statistical test.
It's a very special part of a statistical test, which is the, if you were doing a t-test, it would be the standard error or the amplitude of the observation noise.
So it's estimates not of the thing you're measuring, but the quality of your measurement.
It also has to be estimated, but we'll come back to that.
From the point of view of this talk, and indeed engineering and active inference, the quality of the data, the sensory evidence is usually referred to as precision.
So precision is just inverse variability.
So a quantity that has a probabilistic distribution can be written down as a probability distribution.
And the very precise distribution is one which is very, very narrow.
It's very reliable, not dispersed, whereas a very low precision means you don't know what you're going to get as a distribution that's very, very broad.
So precision is one over the variance or the variability, the reliability.
What that means in computational functional terms is that a failure of encoding the precision of the quality of the reliability of the confidence of our beliefs can be, can underwrite or cause false inference.
So functionally that means a barren precision is the computational cause of psychopathology.
Physiologically speaking, it would be very useful to understand how precision is encoded in the brain, because if we understand that, we can then understand the site of action of the pathology, the pathophysiology,
and also any therapeutic intervention such as giving drugs or behavioral therapy.
So it's important from a physiological or a neuronal process point of view to map the computational pathology onto the processes that are responsible for those computations.
So much of what I'm going to be saying is really translating the theory that we spoke about yesterday, in particular predictive coding, into neural networks and message passing in the brain, and trying to identify where this pathology can manifest.
So, for those of you who are not clinicians, I'm just going to briefly summarise the symptoms and signs of schizophrenia.
On the one hand here, we have the symptoms of a positive sort.
So they're not really symptoms in the sense that symptoms are something that the subject complains about.
So really these are positive signs of schizophrenia.
So delusions, hallucinations and thought disorder, believing things to be true that are not true, perceiving things to be there when they are not there,
and having disorganised thoughts sometimes described as loosening of associations or by people like Bloyla, a disintegration, a fragmentation, a dissolution of mental life, of the psyche from our point of view,
of our ongoing narrative or inference about what's happening to us and what's happening in the world around us.
The key thing that I want to observe here is that all of these can be neatly and simply...
What's it saying?
So all of these can be thought of in terms of false inference.
For example, a delusion. It's a false conclusion. It's an inference I've made which has taken me to a conclusion, say that I am being persecuted by the CIA from America.
Even though there's no evidence for that, I will interpret the evidence that I do have falsely as evidence in favour that I am being persecuted, or that the person reading the news on the television is talking to me directly.
This is an example of false inference of a high level conceptual sort, usually involving the relationship of me with somebody else or the rest of the world or some institution.
In exactly the same sense, hallucinations can be thought of as false inferences of a perceptual sort. I see things that are just not there.
So the sensory evidence does not support the inference that I commit to, the belief that I behold about what I am perceiving.
I see things, I hear things. Usually in schizophrenia it's hearing voices that are not actually out there.
They could actually be inside, it could be yourself speaking, but the key thing is they are false in relation to the evidence, the sensory evidence that you have via the sensory states of your Markov blanket.
If you go through nearly every psychiatric syndrome you can think of, you can interpret all signs and sometimes symptoms of psychiatry in terms of the formation of abnormal beliefs, again false inference.
So I've just listed here a number of psychiatric conditions that can all be understood in terms of false inference, for example dysmorphophobia, so having false beliefs about the morphology of your body.
So say I had alorexia nervosa, I may believe that I was very, very fat and I was too big for what I considered to be a normal size.
Even if I was looking in a mirror I would still infer on the basis of what I saw that I was too wide and too big.
Delusional mood often occurs before the onset of a psychotic episode as in schizophrenia, the sense that there's something not quite right in this world.
It's not real, or I am not real, depersonalisation. Again false inferences about the nature of being a self, compulsions, intrusive thoughts, somebody put that thought into my head.
Again it's a false inference, it's not supported by the same data that you would use to reach a very different and usually a normal conclusion.
And I can go on, obsessional beliefs, effective symptoms, depression, beliefs that nothing that I do will change the situation.
Nothing that I can do will make things nicer or better. This may be a false inference that underwrites a low mood, depression, doing nothing, anodonia.
Anxiety, functional medical symptoms, it's the new way of describing hysterical symptoms.
Doctor I can't feel anything in my left hand and yet you're still withdraw when a painful stimulus is applied.
Persecutory beliefs we've talked about in the context of the newscaster saying nasty things to you or the CIA persecuting you.
That was the phenomenology, the symptoms and signs of schizophrenia.
Let's just turn to the neuronal mechanisms, the synaptic mechanisms that have been associated with schizophrenia.
I've listed here some of the synaptic hypotheses and listed here some of the key etiological hypotheses such as genetic predisposition, neurodevelopmental factors,
the action of psychomimetic drugs, psychosocial factors and we spoke earlier today in fact about autoimmune mechanisms that might lead to schizophrenia form like conditions.
What I want to focus on are the more mechanistic hypotheses at the level of message passing and brain function in the here and now.
I've divided them into different classes of synaptic receptors that might be implicated in schizophrenia.
The oldest and the most established is obviously the dopamine hypothesis of schizophrenia on the basis of responses to neuroleptic medication.
The glutamate hypothesis that focuses on excitatory glutamate neurotransmission.
More recently a focus on GABAerogic GABA receptors in mediating inhibitory control and in particular excitation inhibition balance in schizophrenia and other conditions.
The key thing here is all of these hypotheses talk about the strength of connectivity, the synaptic efficacy, the sensitivity of neuronal responses to the messages that they receive or their afferent presynaptic input.
We can see that in the context of dopamine theories of abnormal learning, reward learning, abnormal plasticity.
We see that in the context of sensitivity to important sensory information in terms of aberrant salience hypotheses, in terms of glutamate hypothesis.
We have a whole series of theories focusing on an MDA receptor dysfunction and that is important because the MDA receptor doesn't itself cause a response in the neuron but it affects profoundly the responses to other signals.
It's a form of gain control in virtue of its non-miniarity and modulatory aspects.
There are whole literatures on how you'd link that to gain control as expressed by coupling between pyramidal cells and fast-spiking inhibitory interneurons and the way that these interactions cause fast oscillatory dynamics, usually in the gamma range.
As a signature of the excitation inhibition balance or gain control in cortical and sub-cortical systems.
Again, the same theme emerges in terms of gamma-ergic hypotheses and aberrant gain control that functionally in terms of micro-circuit dynamics can be summarised in terms of a loss of excitation inhibition balance.
A dominant theme, one simple theme runs throughout this physiological review of mechanisms that are associated with schizophrenia.
It's a loss of gain control at the synaptic level, an abnormal sensitivity control at the level of how a neuron or a neural population responds to its inputs.
In summary, a barat neuromodulation synaptic gain control, why is that important?
It's important because it speaks to a notion of schizophrenia which has been around for centuries that has divided into two sorts of understanding of the functional problems in the brain of someone with schizophrenia.
And I'm illustrating that dialectic, that distinction or dichotomy by comparing and contrasting Vernica's ideas with Bloyla's ideas.
Both of them saw that in some sense schizophrenia and a lot of these other disorders are a subtle and pernicious form of disconnection syndrome.
They're all abnormalities of connectivity right down at the synaptic level that are very consistent with this notion of a disintegration of the psyche.
But there are two very different ways of understanding that kind or a disconnection syndrome.
You can either think of it in terms of a rupture or a disruption of the wires that connect different parts of the brain.
A pathology of the axonal connections or the white matter tracts that carry the connections from one part of the brain or the other, which I have illustrated here in terms of an old circuit board where literally we've cut the wires.
So that would be an anatomical sort of connection beautifully described by Vernica in terms of his subjunction hypothesis as literally a disruption of the organs of connectivity.
Of the sort you would get if you're a neurologist if you're looking at a leukodystrophy, an abnormal form or structure of white matter tracts that form the connections in our brain.
However, a more natural form of functional disconnection or disconnectivity that accommodates or is easier to reconcile with the evidence for a failure of gain control at the synaptic level is a functional form of disconnection.
And this is much more closely associated with the thinking and the writings of Bloyla describing schizophrenia in those days.
It was known as dementia precox.
So this is much more a disintegration that in terms of this circuit board analogy would be a failure of the transistors.
So I'm using that very deliberately.
So the pathology here, if you remember, is about gain control.
It's not about broken wires.
It's about the control of the currents through those wires.
So there has to be a modulatory or a third component gating the messages that are passed.
So when I was a young man, that used to be done by transistors.
Now they're done by quantum gates and silicon chips and the like.
But they are at the heart of all our physical computers.
So this leads to a notion of disconnection that's best thought of in terms of disintegration, as Bloyla would express it, a disintegration of mental life, of inference of the psyche.
And he wrote very eloquently, very fluently about this form of functional disconnection.
And that's going to be the hypothesis that we're going to use now to revisit these old ideas from the point of view of modern 21st century ideas of predictive processing and active inference that we've been looking at for the past two days.
I should just say that the evidence for abnormal gain control at the synaptic level comes from many directions and there has been an increasing realisation, even from the point of view of the molecular biology and the genomics of schizophrenia,
that nearly every gene that contributes to the genetic load or insult that may predispose schizophrenia, nearly every one of them,
certainly those that actually are found inside the brain in some way affect post synaptic sensitivity or gain control.
So whether we're within the cell or we're talking about genes that control the expression or efficacy of dopinergic neurotransmission and or its receptors,
everywhere you look, these genes have something to do with the long chain of events that culminate, that end in the control of the changes in synaptic efficacy or strength.
So with that background, I'm going to ask you for a moment to forget about the schizophrenia and then we'll just briefly review active inference and in this talk I'm going to be focusing on predictive coding as the process theory that provides the best metaphor for active inference.
And I'm going to do so because that has the easiest and nicest way of accommodating the importance of this gain control that may be encoding precision.
So active inference, predictive coding and precision. We've seen this before. This is the water colour, sorry, the oil paintings that were viewed from the right perspective and suddenly inducing you another and more simple, more plausible hypothesis about what's causing these sensory impressions.
And then we had spoken about the very long history, the pedigree of these ideas about the brain as a constructive organ, an organ that in a statistical sense is generating hypotheses, explanations for sensory data, trying to explain what caused those sensory impressions.
And in a sense, I think we even discussed this either in questions or conversation, our perception is just controlled hallucinations. They're just fantasies.
So, literally, the brain is a fantastic organ. It is fantastic in the sense that it deals in fantasies. Those fantasies are reinforced and confirmed by sensory data.
But at its heart, this view would say that we are just here to construct the best fantasies that are consistent and generalisable explanations for what we actually sense in our physical and social interactions.
Consistent with the notions of Richard Gregory describing perception as hypothesis testing, that has led to great advances in machine learning pioneered by people like Geoffrey Hinton and his young colleague Peter Diane, who together wrote about the Helmholtz machine in honour of Herman Helmholtz, who did a lot of the early theorising in this area.
Which I am told, inherit a lot from the philosophy of Kant and that philosophical pedigree.
So, the translation into machines and algorithms, as we know, rests upon probability theory that's been around since the days of Laplace and Thomas Bayes, made much more generalised and operationalised by concepts such as free energy or comalgraph complexity, as found in universal computation.
Again, impressions on the nervous system, the way of understanding this constructive imperative to maximise Bayesian model evidence or minimise surprise or minimise prediction error, can be seen in terms of this gradient flow.
There's a very simple equation that I've just expressed here in terms of a Bayesian filter. So, for those of you who do any engineering, you may recognise this as having the ingredients of any Bayes optimal evidence accumulation scheme, any gradient flow or dynamics that can be described as moving or encoding
hidden states of the world. But in this instance, I formulated it in a very particular way so that it can be seen in terms of providing a prediction of what's going to happen.
Expectations, literally mathematically means or averages, expectations about some state of the world that suggests this is going to be how the world is changing and then the actual changes are updated and are made accountable to the sensory evidence via these prediction errors that are weighted by the quality of those prediction errors, which is the precision that we've been talking about.
So, no surprises here if you've seen these slides before.
An expectation might be there's a dog, we generate the sensory impressions that would be there if the dog was a right explanation, we compare it with the actual sensations, we form a prediction error and then we send the prediction error back into the brain to change the expectation
if the expectation does not generate a sufficiently good prediction of the sensory input. But if this is a perfect explanation, then the prediction error will be zero and there will be no change to the expectation.
So that's how this thing works. You can see that the rate of change of our expectations is zero when E times E, the prediction error squared, is zero.
Notice that this is nothing more. This is exactly the same idea that you use when you perform a t-test or you do any statistical test. It's just minimizing the residual sum of squared errors or the sum of squared residuals.
It is, in essence, a maximum likelihood estimator of the causes of your data, like a group mean for example.
But let's just pursue that analogy. So a t-test, now who has not used a t-test?
Okay, maybe a trick question, you haven't, oh dear. Who has used a t-test? Who feels fluent in describing how a t-test...
Vladimir, what is a t-test?
You should compare our mean and standard deviation in practice.
Excellent. So he said you should compare the mean, usually a mean difference, with the standard deviation of that mean that you would expect under a particular level of noise, sometimes called the standard error.
So that comparison is at the heart of the correct answer. Let's pretend we have two bunches of data, two sets of data from group one and group two.
And we wanted to test the hypothesis that the average from this group is somehow different from the average from this group.
So that would be my generative model, that I'm generating data here by taking say zero and adding a group difference and then adding some usually Gaussian or random noise to the data.
That's how I'm going to generate the data from that group and then I don't have the mean difference and I generate some data from this group.
The null hypothesis is that the difference is zero. And now I want to do effectively Bayesian model comparison between the null hypothesis and the hypothesis of a group difference.
Now I do that by comparing the probabilities or the likelihoods of the data under the two models.
And that can be written down as a simple ratio, a log likelihood ratio, or a difference in log likelihoods or a log of a ratio.
And that log ratio happens to be, in a classical context, a tista stick. It's a ratio of the difference divided by the standard error or the inverse precision of the difference that depends upon the noisiness of the data.
So that's important because I've just told you that precision is the inverse variability.
So a t-test would be the group difference divided by the standard error, which is a measure of the variability.
And if the difference is big in relation to the standard error, I get a big t-stistic and I can say yes, my alternate hypothesis of a group difference is the best expression for the data and that would be my inference.
There's a group difference there. But that depends upon getting the estimate of the standard error correct.
Now I've just said that the precision is the inverse variance and then the inverse variance is the precision.
So what we're talking now, a t-test, is the difference, the prediction error under the hypothesis of no difference, the null hypothesis, times the precision because it's divided by the variability times the precision.
And that's why the notion of a precision weighted prediction error is so important in predictive coding.
It has both of these estimation problems within it. Yes, we have to estimate the group mean, but the more difficult thing is estimating the variability, the level of the noise, the signal to noise ratio.
That's very important to get that t-test working properly. And that's where the estimates of precision come in and why they are so important.
So here, this brings us back to this construction, the degree to which these prediction errors influence or change the expectations depends upon these precision weighted prediction errors.
If you're engineers, I wouldn't be calling them precision weighted prediction errors. I would be calling them prediction errors times gain.
And if I was working with a linear model, gerative model, I would be referring to this equation not as predictive coding but as a Kalman filter.
And that gain would be called a Kalman gain.
So this idea has been around for nearly a century, possibly not quite that long.
But understood from an engineering point of view as the Kalman gain, how much you weight the evidence from your sensory feedback, the prediction errors, in terms of updating your beliefs about the state of the thing that you're trying to estimate.
And again, all of this just depends, is just driven by the minimisation of prediction errors.
So we're not trying to pretend we'll ever actually know the true states or the things that are actually causing the sensory data.
All we want is a good enough description that minimises precise prediction errors.
And we've seen this graphic before. It just says that there are two ways in which we can minimise prediction errors.
We can either change our predictions, basically update our perceptual beliefs to make the predictions more like what we're sensing.
Or we can, well I say or, but I've been asked twice now, do you do perceptional actions?
So I'll say and. So you're changing your beliefs whilst at the same time and changing the sensations you're trying to predict.
So you're trying to bring your sensations into alignment with your top-down predictions so that the prediction error is minimised or the precise prediction errors are minimised.
And at the same time you're also trying to predict the precision and that's going to be a crucial aspect of what I'm going to be talking about.
So this is a simple view of psychology in terms of action and perception, deep perception, both in the service, both complying with the imperative or the drive to minimise prediction errors.
So here's a little worked example. It's not a human because I want to use simulations of a bird, a songbird to demonstrate hallucinations and delusions when we break this inference machinery.
So here we're going to take a songbird, very well studied in the biological sciences.
So let's start at the beginning. Here we have some sensations and I've expressed these sensory inputs or states of the sensory states of the Markov blanket as a function of frequency as time progresses.
So you can see frequency glides here and little chirps that constitute this particular song emitted by this songbird here.
And these sensory signals arrive at the thalamus. They're compared with top-down predictions so that we retrieve or retain just the prediction errors.
And these prediction errors are sent forward according to this gradient flow or this predictive coding equation or this Kalman filter.
All names are the same underlying dynamics in order to improve our expectation about what is actually out there causing the pattern of sensations that we're currently sensing.
These ascending messages then correspond to the prediction errors.
And in that sense they are the sensory information that is newsworthy, that is worthy of listening to.
It's what carries the information. You learn nothing from a TV news channel if it's telling you what you already knew.
If you could predict exactly what the newscaster or the news broadcaster was going to say next, you'd learn nothing and it would not be worthwhile using that source information to update your beliefs.
It is what you don't predict. It is a prediction error that contains the newsworthy or the interesting information.
And that's what's broadcast to the next level of inference. There are mathematical interpretations of that in terms of efficiency and compression, which is an interesting perspective on the origins of predictive coding, which will in fact compress sound files in the 1950s.
The idea being, again, we're trying to find the most efficient way of reaching inferences and in this context, in the context of predictive coding, the most efficient thing that you can do is just to pass the messages that you need, which are the prediction errors.
But of course they also have to be precise prediction errors. There's no point in attending to prediction errors if they are contaminating with lots of noise, if they have a high degree of variability.
So we also have to adjust the gain or the precision with predictions of the precision just to wait the ascending prediction errors. So effectively we're tuning in to the newsworthy radio channels.
We can turn up the volume of the radio channels that have the most precise informative information and we can turn down the volume by decreasing the gain control or adjusting the excitation inhibition balance so we can ignore imprecise information or imprecise prediction errors.
So these things are updated by precise and selected prediction errors and they provide better predictions and again in deep inference or deep generative models, these expectations themselves have top down predictions from high level constructs.
Here they're going to be not about what chirp is currently in play, but the sequence of chirps and what chirp is going to come next and you can extend this higher up inference to any level.
As with yesterday, these expectations, because they're a generative model, can generate predictions of the consequences in any modality.
So if I was producing the song, if I was singing, I could predict what I would hear, sending it down to the predictions to the auditory thalamus, but I could also predict how I would feel my voice box, my voice muscles move if I was singing.
So if I have the concept, the amodal concept representation of singing, I can generate the consequences of singing in both the proprioceptive, the feeling in the hypoglossal nucleus of a bird and the extra receptive, the auditory input in the auditory thalamus of the bird here.
So we get these predictions that are unpacked in a modality specific way.
And the nice thing about that is, as we saw yesterday, we can now interpret or use these predictions to drive reflexes.
So I can actually use my representation of a song either to listen to a song or to generate the top-down predictions of the set points that will actually cause me to sing the song.
So I can use the same representations to listen or to speak or to sing.
And the only thing that I have to do to switch between singing and listening is control the gain or the volume of these ascending prediction errors here.
So notice what I've done here to switch the bird into a singing mode to make it sing is I've switched off the precision or the gain of the auditory prediction errors and have enabled the gain of the proprioceptive feeling, strep receptor prediction errors here.
And if you remember from yesterday, these prediction errors do not need to go back into the central nervous system.
These can eliminate themselves by exciting muscles to supply the predicted proprioceptive feedback via the primary afference so that the prediction errors are minimized through action and the bird is happily singing.
So we've got this sort of two modes of behavior, listening, talking, listening, talking.
And the only thing that changes is the deployment of the precision that controls the ability of ascending prediction errors to revise beliefs.
So here there is no revision of beliefs so the beliefs now become enacted.
They actually become expressed and manifest in the real world by talking.
So I am making my own world.
I am the author of the states that will actually produce her song.
I can hear myself singing although of course I can't because I've switched off the precision of the auditory input.
So that's an important feature of the scheme.
It means that in principle, if you're acting under this active infant scheme, if you want to make a move on the world, if you want to act upon the world, you have to attenuate or you will attenuate the sensory consequences of that action.
Which means that you are going to be unable essentially to hear yourself speak.
Now that may sound stupid but in fact in psychology that's known as sensory attenuation.
It's the attenuation of the sensory consequences of self-made acts.
When you produce something, your perceived intensity of the thing that you produced is always much less than when somebody else produced the same stimulus.
And we'll come back to that and why it's so important in measuring deficits of precision control in schizophrenia later on.
From the point of view of motor physiology and theoretical motor control, what we're saying is that having a generative model of singing can produce or generate predictions of what your motor plant motor apparatus would do.
And these can be regarded and possibly are regarded by many or certainly were in the 20th century as motor commands.
Whereas the perceptually extraceptive consequences of singing can be regarded as corollary discharge.
From our point of view, I'm going to interpret this really as a reflection of precision control.
Precision engineered message passing in the brain that can move us between a motor intention where we increase a proprioceptive precision
and attending to sensory consequences of other people's speech in terms of extraceptive precision and when we're speaking this will become sensory attenuation.
So notice what I've done here is I've used a word from psychology's attention to describe the operation of this optimization of the precision of the prediction errors.
So the idea or the story or the claim here is that attention just is the adjustment of the precision of various prediction errors.
So taking the analogy of listening to one radio channel or another radio channel, you can literally tuning to the precise radio channel that has the precise information by attending to that
and not attending to another radio channel.
So this process of selecting which prediction errors you allow to update your expectations is, from a computational point of view, just attentional selection.
So when we talk about gain control, we are just talking about attentional gain.
The psychological description of turning up the volume or the gain control on information from that part of the receptive field with spatial attention or prediction errors referring to particular attributes,
features, if it's feature attention, depending upon where you are on the hierarchy, by selectively adjusting the precision you can emulate all sorts of different attentional controls.
So I have spoken about sensory attenuation and I wanted just to illustrate something that we've talked about on at least two occasions which is the notion of sacallic suppression
as a very tangible, a very easily recognisable form of sensory attenuation.
So I just want to give you an illustration by playing a game of how powerful this gain control can be and how quickly it can work.
So this is the game. This is Sherlock Holmes and in a moment this Sherlock Holmes is going to move, is going to jump from this crosshair to that crosshair.
And your game, or the challenge that you have, is to see whether any aspect of Sherlock Holmes changes. Does his clothes change? Does he change colour? Is there any change whatsoever in Sherlock Holmes as he jumps across from here to here?
And I'll count down three, two, one, jump. And while he's jumping, does he change? For those of you who do psychophysics, you could think of this as a change of blindness experiment.
Have you seen this before? You're grinning. You look as though you know the answer to this. Right.
Just to make things interesting, I'm going to ask this half of the room to focus on Sherlock Holmes and keep your eyes on Sherlock Holmes.
This half of the room, a different challenge. You have to fixate the central cross. You're not allowed to move your eyes when Sherlock Holmes jumps.
This half of the room, you can move your eyes with Sherlock Holmes. You can't.
And I want to know who's the best at detecting any change if there is a change. The people who do and do not move their eyes.
Right. Are you all ready? Have you all decided which side of the room you're on? You can choose in the middle, whichever you think is going to be the easiest to perform the task.
All right. You ready? Three, two, one. Let me ask the people who are following Sherlock Holmes. Did Sherlock Holmes change? No.
Oh dear. Are you sure? That's an honest answer, thank you. What about this side of the room? Did Sherlock Holmes change a lot? He did change.
Are you sure? Could you describe how he changed? Bigger. Any other answers?
That's interesting. He became another person for a while. Do you know who that other person might have been?
That's because we were talking about gender. Interesting answer. Did anybody notice anything else that was outside the experimental paradigm?
So notice the answers here all came from the side of the room that kept their eyes still. So let me show you what you saw that made it look as though he was extending and what you saw as this magic female.
There's in fact another man who was elongated. This is actually Moriarty. This is Sherlock Holmes' arch enemy. And there's a UK TV series with a very famous phrase, did you miss me?
Because Moriarty gets killed apparently, but comes back from the grave. The point I'm making here with this little demonstration, now you know it's there. You should be able to see the very brief flash of did you miss me?
Now you know it's there. The point I'm making with that demonstration is that you were not subject to sensory attenuation. Because you were moving, you did not have to attenuate the gain from invisible signals and you were sensitive to the illusory effect of the elongation of the stimulus, you even actually saw another person, which was actually there very briefly.
On the other hand, you have cyclic suppression. Because you were moving your eyes, you attenuated the sensory information so you were much less able to perceive the visual stimulus because you were moving.
So that's an illustration of sensory attenuation. And no matter how much you try, you would not have been able to see or read that text, for example. So this is, I think, a very powerful illustration of sensory attenuation.
It's not something you have volitional control over. Well, you probably do at certain levels, but there are certain forms of sensory attenuation which are really essential just to be able to move and to secard, for example, in this instance, during visual searches, that can operate on a 50 millisecond timescale.
For all that 50 milliseconds of moving, the neurons reporting the optic flow that you were inducing at the retina were not able to send those messages to V1 or V2 or all the visual hierarchy. You were unable to perceive it, so you were ignoring that information whilst you were moving.
So it's a very real thing with sensory attenuation and it comes out in many, many different forms.
So what do you show everyone at one time?
Yes, of course. All right, now you can try. Why don't you try following Sherlock Holmes and see if you can read the message?
It wasn't the first, the first.
Yeah, so I'm going to count to three so you can do it both ways. Three, two, one, jump. I'll do it again now, do it the other way now, if you follow it the first time. Three, two, one, jump.
Are you convinced?
It was similar.
Yes, it's not at all or nothing. Obviously we can hear our own voices, obviously we can see things.
So the psychophysics of this is usually a change in discriminability or detection thresholds.
Yes, the object didn't change, that was a trick. All right, that was just a trick just to divert your attention. There was no change, there was no change blandus.
The only thing that actually occurs of interest is that stimulus in the middle that you would globally see if you haven't applied a sensory attenuation to your visual prediction errors.
So how does that help us, how does a phenomena of sensory attenuation fit in with this theme in terms of understanding schizophrenia as a failure of false inference that may be mediated by an aberrant gain control.
Now that we know that gain control is the thing from a computational perspective that optimises the precision.
So gain now becomes the precision, it's the waiting, the precision waiting of the prediction errors.
So anything that changes post-synaptic gain, whether it's synchronous activity in terms of synchronisation gain or whether it's classical neuromodulators like astalcholine or dopamine or 5-ht serotonin, anything that changes post-synaptic sensitivity computationally will be changing the precision.
And it will be mediating some form of attention or sensory attenuation, up or down.
How does that fit now with the story about schizophrenia and indeed possibly autism? Well the story that seems to have the most traction and it's emerged in many different forms in the past five years now.
This was a paper by Rick Adams, my colleagues who wrote down the simulations that I'm about to show you.
So the general idea is this, that people with schizophrenia and possibly other syndromes have a failure of that sensory attenuation, this side of the room experience, but at multiple levels in the auditory and visual hierarchies.
Which put simply means you could not ignore sensory evidence in situations where you would normally ignore it.
That basically leads to a number of behavioural and belief-updating abnormalities that look a lot like the signs of a negative schizophrenia.
So, as we'll see later, attenuated violation responses, a loss of perceptual gestalt, a difficulty in moving, so remember in order to move I have to do a sensory attenuation, if I can't do a sensory attenuation I can't move basically.
If you want a heuristic understanding of that, if I am using my predictions to move, say my eyes, and I haven't yet started moving my eyes, and I don't ignore the fact that my sensations from my eye muscles are reporting no movement, then I will revise my belief that my eyes are moving to they're not moving.
In order to move my eyes, I need to ignore the evidence that they're not moving, therefore I have to attenuate that information so that my prior predictions, my prior beliefs are actually fulfilled via these reflex arcs.
But of course if you can't do that, you can't move, and perhaps the most extreme example of this failure of neuromodulation leading to a failure of sensory attenuation is Parkinson's disease.
Because you've got such a profound abnormality in dopaminergic neuromodulation in the motor system, you end up not being able to initiate a movement at all.
Once you've started it's fine, but you just can't start the movement, you're essentially achonetic.
And you see zemblances of that in things like psychomotor poverty in certain forms of schizophrenia and certainly in catatonia, which we don't see anymore.
We also have an explanation for a resistance to illusions because we're always attending to sensory data, it's very difficult to fool our perceptual inference.
So people with schizophrenia paradoxically are more robust or resilient to illusory phenomena.
So they perform actually more accurately in a psychophysical sense where people like you and me who do do the sensory attenuation miss certain bits of evidence and we are more susceptible to illusions.
So it has a broad explanatory scope, but it doesn't explain hallucinations and delusions.
To explain that, what we imagine is that there's a compensation for this failure of precision control at the sensory level so that we have to compensate by increasing the precision of higher level prediction errors
that can either be at the level of perceptual constructs or at the level of concepts very high in the hierarchy.
So we have this sort of cycle between a trait of normality characterised by failure of sensory attenuation leading to this cluster of signs and possible symptoms that occasionally during the life course of the individual or the time course of the illness
will be compensated for and possibly induce a psychotic state which then recovers and decompensates back to a burnt out schizophrenia or a negative schizophrenia.
So this provides a computational metaphor or hypothesis for false infants in schizophrenia that has an underpinning that relates directly to the synaptic game control theories.
We've talked about four in terms of the dopamine, glutamate and gabaragic hypotheses, operationally in terms of electrophysiology, all about the EI balance.
So this excitation inhibition balance is just another way of talking about the sensitivity of the output cells, the pyramidal cells in terms of their responses to their inputs.
If you're wrong, you get a failure of this precision control. So let me just try and illustrate what we think is going on with another example.
So you saw this and you're trying to work out is this pattern of sensory stimulation best explained by this hypothesis where there are lots of pebbles up here or lots of coins.
And you may well be attending to this and ignoring your prior beliefs by increasing, attending to increasing sensory precision or the precision of sensory prediction errors.
So you're quickly accumulating that sensory evidence to test this hypothesis and that hypothesis and you may well end up confirming this hypothesis, this is caused by a pile of pebbles.
But let me now give you a new hypothesis that in fact it was caused by a face.
Let me do another thing now. Now I put that new hypothesis, that new prior belief in your head, I'm going to take it away.
But of course I haven't taken it away. You can now remember that. You've accumulated that. You've now got a new hypothesis in your visual hierarchy that provides a good explanation.
So you cannot unseen that face. Even if you didn't see the first time now before I inverted the figure, now you can't not see it.
And what's basically happened is that because I've given you a very simple explanation, oh it's just a face.
You've now got prior beliefs that are very precise and you have, if you like, balanced your prior precision or the precision of your high level prior beliefs to now dominate over the sensory precision
or the sensory prediction errors, the precision of the sensory prediction errors.
So now that's why you can't see the same thing that you saw before because there's now been a rebalancing of the contributions to your percept.
That now the prior beliefs that this is a face can't be eliminated. Again you don't have any conscious control over that.
So although we're talking about attention and sensory attenuation that we often think of, we can voluntarily attend to this or that, at this level of perceptual synthesis under predictive coding,
you don't have the machinery to really change these things. So in effect now you're having the illusion on the face and you can't get rid of that.
Somebody with schizophrenia probably could. They could probably unsee that once you remove the stimulus.
Just to make these notions more concrete or easier to visualise, I'm now going to illustrate the formation of illusions and delusions of assault hallucinations
using that songbird brain that I talked about before. So remember we now have a little bird brain and it's attending to its sensory input so it's listening for its conspecifics and it's expecting to hear a particular song and it sounds like this.
It's not going to stop is it?
So that song is in fact engineered. It's actually produced synthetically from two Lorenz attractors. We've seen a Lorenz attractor before.
These are the attractors that were originally devised to model convection flows in fluid mechanics and meteorology.
What we've done here is to place one Lorenz attractor on top of another one where this one has slow dynamics and it predicts the control parameters of a faster Lorenz attractor.
So we've got this sort of temporal depth now. We've got a deep generative model with a separation of temporal time scales and then we're using two of the states of this three dimensional Lorenz attractor to control the amplitude and the frequency of a synthetic voice box.
And that's basically Lorenz attractors are singing to you. They sound very much like a bird.
And we can simulate, we can generate, we can use this bird in fact to generate these synthetic bird songs and then we can play the sensory information back to the generative model and then we can work out the, or we can extract the prediction errors during this predictive coding, active influence,
Bayesian filtering, all of them being essentially the same thing.
So this is the prediction. It looks very much like and sounds very much like the actual sensory input.
And these are the prediction errors that are driving very fast updates of these expectations throughout the auditory hierarchy.
So the bird latches onto it, it aligns itself, it synchronises itself with the actual fluctuations and dynamics of the auditory input.
So it's effectively doing a very short term prediction anticipation of what's going to happen next and then using any slight deviations from that as recorded by the prediction errors to update its trajectory in terms of the hidden states,
the representations of the external states that it thinks are generating these stimuli.
What I want to do now is to use this simple setup.
Right. I'm going to use that simple setup to demonstrate what would happen if I started to change the precision at the sensory and the higher vocal centre level of the generative model.
And then ask, does this look like the kind of false perceptual influence that we see in people with schizophrenia?
So what we did here was play the song that you've just heard, but remove the last few chirps.
So the bird's expecting to hear that song, but what it actually hears is silence after one, two, three, four, five, six chirps.
It doesn't hear the last three chirps.
And what we're interested in is how the brain responds and the percepts as encoded by the posterior expectations.
What does the brain actually hear? What does it perceive?
So this is the normal response where both precision at the sensory level and the higher level are in balance.
And what we see is, let me just play it for you.
So that last shim is the perceptual response to the omission.
And what you'll see here are two things.
First of all, it's actually hearing that, right at the end, even though there's nothing there.
So even though there's no sensory input, because of its expectations, it expects something to be there,
but it takes a bit of time for the prediction error to say, no, there's nothing there.
So it actually hears something that's not there.
But very, very quickly it corrects itself.
And how does it correct itself?
Well, there's a massive prediction error.
That prediction error tells the higher level expectations, no, there's nothing there.
And very quickly it correctly infers science.
So this would be a normal response to an omission.
Think of this like a mismatch negativity.
A mismatch negativity that is elicited by omitting an expected sound.
So for those of you who don't do mismatch negativity paradigms,
it's a kind of paradigm where you go beep, beep, beep, beep, beep, beep, boom.
And it's a boom. That's the exciting bit.
Then you compare the evoked responses to the boom with the beeps.
If you get very sophisticated, you can actually do beep, beep, and just omit it completely.
And if you get a mismatch response to that, that tells you something quite fundamental,
because you've got a brain response in the absence of any state-sensory stimulation.
And that is a very, very clear piece of evidence that the brain is a constructive organ.
If it responds to nothing, it has to be from the inside.
It has to be a predictive organ of some sort.
And that's what we're seeing here, a prediction error to an omission
which, in compressed time, can be regarded as a synthetic mismatch negativity.
Let me know, right, I see, it's playing automatically, it's very clever.
It's a Russian computer, it's preempting me here.
Which is good, I'll have to...
So what we did there, I think, because my hearing is not good enough,
is impaired sensory attenuation.
So now what we're modelling now is the sort of primary deficit
we think underwrites things like schizophrenia,
and I have to say also autism.
So there's quite a big story in autism,
which suggests that people with autistic phenomenology
have difficulties attending away from sensory information in all modalities,
perhaps it's possibly particularly intra-oceptive modalities,
but also extra-oceptive modalities.
So you can think of this as an autistic songbird,
or a songbird that has a negative form of schizophrenia.
So what happens here?
Well, because we have impaired sensory attenuation,
now we have effectively increased the precision of the sensory information,
the sensory evidence, the sensory prediction errors,
relative to our prior beliefs and the prediction errors
at high levels in the hierarchy,
which effectively means that I am reducing the influence of my prior beliefs,
my anticipated, my knowledge-based contribution to the perceptual synthesis,
and increasing the evidence base,
the actual sensations, the prediction errors from the sensory levels of influence.
And that has benefits and costs.
It has benefits in the sense that I am less subject to the illusory final chirp
that was never there.
So I more quickly, correctly infer science.
So I'm more accurate at detecting the omission than the normal though.
But the price to be paid is that I'm actually missing stuff that I knew was there
but didn't quite hear because it was very soft or very imprecise.
So notice that it's missed the third chirp.
It hasn't perceived, it hasn't heard the third chirp
because it required the prior beliefs in order to properly form that posterior belief.
What we see from an electrophysiological point of view
is an attenuation of the mismatch negativity.
And again, that's a cardinal.
If there are two things in schizophrenia research that reproduce,
one of them is abnormalities, in fact,
hyperacute performance on eye-perceived tasks,
and the other is an attenuation of the mismatch negativity on violation responses
in terms of measurable behavioural and electrophysiological features.
Those are the two things that survive over decades of research.
And a simple explanation for this is a fact that
because you're attending to everything, there can be no surprises.
So there can't be any violations.
Everything is equally surprising and interesting
and therefore there's no difference between beeps and boops from your point of view.
You're attending acutely to both the beeps and the boops
and therefore when you compare a response to a standard stimulus
with an op or an omission, you have an attenuated response.
In layman's terms or in sort of folk psychological terms,
this must mean that if you were autistic or had schizophrenia
and this explanation is right, you're going to find everything surprising.
You're not going to be able to ignore anything.
A full sensorium is a line that's demanding your attention all of the time.
And you can argue from that a failure of central coherence in autism
because not because you don't have the capability
of forming deep models of your world or other people in your world,
it's just that you are continually enslaved by the sensory impressions
and indeed generating those sensory impressions
through self stimulation or repetitive behaviours.
So let's now pretend this was a bird with schizophrenia
of a negative sort and let's say that it tried to compensate
for the failure of attenuation.
The failure of sensory attenuation by increasing the precision
of its higher level prediction errors in the deep hierarchy.
And what happens here is interesting from the point of view
of schizophrenic psychopathology.
Now, effectively, we're over-weighting the prior beliefs
and we're ignoring the sensory evidence.
So now there's a literal disconnect in computational terms
between what's out there and what these autonomous dynamics
in these Lorenz detractors think is out there
and basically what we have is a hallucination.
So this is what it actually thinks is, which is not a natural
but it's nothing to do with what was the stimulation at all.
It's just hallucinating a very different sort of perceptual trajectory
and of course because we switch off the precision
there are no prediction errors because they have very, very low precision.
So these are the precision weighted prediction errors.
So in summary, what we have is a picture,
a very crude picture of schizophrenia in particular
but possibly a number of psychiatric disorders
cast in terms of false inference and very much like a statistician
that that false inference can be manifest in terms of false negatives
actually inferring something is not there when it is
or false positives, for example, inferring something is there when it's not.
So this is exactly type 1 and type 2 errors in statistical inference
and we've associated false negatives with a resistance to illusions
that we saw with that simulation, a psychomotor poverty
that follows from a failure to engage sensory attenuation
such as sacallic suppression, a loss of perceptual guess-telt
because you're always focused on the sensory detail
and attenuation and violation responses which is known empirically
namely the negative signs or trait abnormalities of schizophrenia.
We compensate for that by increasing the precision of our beliefs
at high levels, posterior beliefs at high levels in the hierarchy
and we necessarily encounter hallucinations and delusions.
I'll close with just a note that some of our colleagues
Vladimir Litvak who's visited here before working with Noah Fergelsen
have taken these ideas and used MEG or EG in this instance
to look at the postsynaptic sensitivity that you would predict
if this model of schizophrenia and false perceptual inference
was correct by using not a violation or a mission paradigm
but very closely related in terms of visual violations
and then for those of you at this morning's seminar
used dynamic causal modelling just to look at the change
in the intrinsic excitability, the EI balance
in terms of intrinsic connection strengths in people with and without schizophrenia
and the bottom line here, we don't need to...
this is just an illustrative application of these ideas
when it comes to understanding or analysing empirical data
but the bottom line here is that control subjects
show a very much greater effect of predictability
on intrinsic gain or intrinsic connectivity gain control
or EI balance relative to people with schizophrenia
which are the white bars here.
This is a summary of all the papers that have been published in the past few years
that try to understand a whole variety of
psychiatric syndromes specifically
in light of this notion of abnormal precision control.
So lots of papers here in schizophrenia.
You don't have to pay any attention to this
but you don't have to remember this.
We put this list together when commenting
on a paper in biological psychiatry.
This was a commentary on Cambridge University
so you can find this in the literature called precision psychiatry.
The equivalent ideas in autism and ASD
interesting applications to stress and anxiety and depression
and as we have seen hallucinations and hallucinosis.
Again, I'd like to thank those people whose ideas I've been talking about
and finally thank you for your attention. Thank you very much indeed.
Thank you very much for your interest and hope.
Do you know any experimental evidence
that is used in autism?
I don't know about autism.
It certainly is a case in schizophrenia.
There are hundreds of papers in schizophrenia.
You generalised it to autism.
There are also mismatching activity.
There are reports that find reduction of mismatching activity.
Some report quite normal.
Yes.
That's good that there are some that show a reduction
because you can't interpret a negative finding.
There was one study
that shows that
depending on the attention direction
they can be reduced or normal.
Yes.
Well, no explanations
but interesting things that one could address
in conversation or indeed in empirical studies.
The first issue is how does voluntary attention or attentional set
modulate the mismatch negativity because, of course, classically
it's been sold as a pre-attentive phenomena so it shouldn't
show an interaction with different levels of attentional set.
Of course it does.
That calls into question then what is the underlying, from the point of view of predictive coding
what does that mean in terms of the game control
and how would you model that and think about that from the point of view of setting up different
expectations about precision and then actually violating
expectations and simulating that.
That's a very interesting game and then you get into the difference between attention and expectations
in terms of different levels of the
auditory hierarchy. I think the more interesting issue
can you tell me what modalities the mismatch negativity
was assessed in Oreson?
Yeah.
And certainly the most compelling empirical
demonstrations of mismatch and admission responses have been in the auditory one.
You can get in normal subjects visual mismatch responses
much more difficult. The reason I ask is
most of the theorising I'm aware of in autism
would suggest that the more important modality
was actually interceptive, not extraceptive.
So if you apply these ideas to interception
what you're talking about now is an abnormality
of interceptive sensitivity and accuracy.
So do you know of any mismatch studies in the
interceptive domain?
Pain.
I mean that's where I would start looking.
If you read the theoretical literature
on predictive coding and hypoprias and precision abnormalities
in autism they normally focus
on the neurodevelopmental aspect very early
in terms of exchanges with the mother
and all the interceptive cues
that they normally entail.
I think a pain experiment would be difficult to imagine.
Good point.
Yes. Affiliative touch.
That's a good one. I haven't thought about that. Absolutely.
My understanding is there are two sorts of somatosensio
depending upon whether it's glabus or hairy skin
versus non-hairy skin and if you get
the right sort of touch sensation it could be
constituted as affiliative touch of the sort that you could lump under a form of
interception at least. Yes, that would be interesting to see
whether there's a mismatch abnormality in autism in an affiliative touch.
Thank you.
Thank you very much indeed.
I have a question to your brother's
experience. Our experience and your experiment
with appearance
of a face from friends.
This is forced appearance.
My question is
I feel some
slight easy movements
when my partner's recognition
has a lot of impact.
Can we regard it as motion illusion
during my obituary
re-recognition, re-patterning?
No movements.
Yes, I felt no movements. I can imagine other
faces after
the back
tone. Again, I can
see not a face at all, but
these obituary recognitions
don't
accompany with any movement.
My forced recognition was
accompanied with a motion.
Is it illusion or nobody regarded so?
I think if I understand the question properly
I think it's all illusion, but there are good illusions and bad illusions.
Motion illusions.
I think the more
sophisticated paradigms of that sort that you've probably
read in the literature have moved on to using dynamic
faces and emotional expressions of moving
faces, which of course is something that we are particularly
good at recognising, having illusions about because most of our
life is spent trying to infer
the intentional stance of somebody else based upon
the visual cues afforded by their eye movements or the muscle movements
and the like. So I would imagine that
much of the visual system is devoted one to biological
motion, and in particular the biological motion that
attends facial movements of an
expressive sort or a gestural sort.
So I think that biological motion is going to be a very
important perceptual attribute that
dominates in the structure and the form of the
visual hierarchist heritage models simply because we spend most
of our time making inferences on the basis
of the precise information, the information that matters
that is inherently biological motion.
Or indeed I'm just thinking about my
lived world, my daily life, I spent either avoiding
cars in the street or talking to people and most
of that involves basically analysing visual motion.
So I think that visual motion would be a very
potent paradigm to, and I presume
already has been used to illustrate these things.
Was that your question then? Is the motion
an important attribute of
the stimuli that produced these illusory realities?
Was that your question or?
Was not precisely such
forced recognition
accompanies with emotion in my voice
and habitual recognition didn't.
So my
I'm sorry, my
I propose, I suggest
my
biologically explanation of
appearance, forced appearance of the face is
the face is really
the face really, the face did really
there, so it moved. I see. Yes, and if I
obituary
to see faces
they can be
they can be still, they can be
they could be here
and I tried and I
see them, they didn't.
My biological explanation might
be
I already asked you
this question regarded in literature
or we first pose it.
Thank you. It reminds me.
The difference between obituary and forced. Obligatory and forced.
Yes, yes. And also your I think
introducing the interesting aspect that there are different sorts
of information that might have a differential
role in the obligatory versus forced perception. So I
reminded of the rather hand illusion where you've got some of the sensory
information and visual information in conflict and what do
a priori you give priority to. So I think it's another interesting example
of putting one modality say visual
opposition against, you know, Newman's contrast in opposition to try and
determine or understand what
the best hypothesis is.
You were describing different mechanisms
of how
Gabba and Lutman
instances distribution of weights. Could you please
be more specific on how it works and how
different drugs help to distribute
these weights properly? Yes.
So you can find answers to that question at
different levels of analysis. So you can go right down
to single compartmental computational neuroscience where people
build populations of neurons with multiple compartments
and use software packages to emulate the neural kinetics and then
look at their excitability. Now I'm thinking here of the work of people
like Bart Irving True and Nancy Coppell and the like trying to
understand the biophysics of
detailed micro circuitry and in particular the
coupling of super granular and infragranular
layers in terms of the
sensitivity of one to the other and vice versa.
You can get right down to the molecular biology and the
synaptic mechanisms even to the level
of how various drugs sit
in the molecular structure of ion channels and
nuance the way that they respond to changes in
transmembrane potential and mediate their neuromodulatory effects
or anaesthetic effects to the sample at the molecular level.
Or you can move up to the macro scale modelling level
which we normally operate at which is a sort of using
neural mass models and neural field models.
At that level the emerging story
in my world looks like the following that the
overall excitability of a neuronal population
that constitutes the superficial
excitatory or pyramidal cells of a cortical mini column
for example has its excitability
determined by reciprocal interactions with
fast-spiking inhibitory into neurons and that most of the
gene control seems to be mediated by the control
of the excitability of the inhibitory into neurons.
Now these inhibitory into neurons receive inputs
from some other statin positive inhibitor
into neurons in their one
that themselves are also the target of classical neuromodulators like
acetylcholine and those projections
on these inhibitory into neurons onto the fast-spiking
ones that are dancing with the pyramidal
cells have lots of NMDA receptors.
So in that story you have
a motif of microcircatory that involves
one sort of inhibitory into neuron that may be in receipt
of distant inputs, a top-down inputs from other
cortical areas connecting to
fast-spiking inhibitory into neurons
where these have classical, clonernurgic receptors
for example or possibly dopernurgic receptors
and they project to cells that are
tightly reciprocally coupled to pyramidal cells that the output cells
so they're going to be reporting the prediction errors when passing messages forward
or if the deep layers, the predictions passing messages
backwards in the hierarchy.
The synaptic tie constants
and the effective connectivity between these fast-spiking inhibitor into neurons
and these pyramidal cells in the superficial layers
tend to make them fire with characteristic
frequencies in the gamma range. So you've got now
a synaptic motif
or a microcircatory motif that has all three
neuromodulators that are implicated that I talked about. So we've got the classical
neuromodulators, so I spoke about
dopamine as an example of that. You've got the gabarurgic receptors
there that are
on the inhibitory into neurons of both levels, gabarain and gababee receptors
and you've got the glutamate excitation of the
pyramidal cells both in terms of driving amper receptors
on the pyramidal cells and on the NMDA receptors, the voltage
sensitive receptors on the inhibitory into neurons.
So changing in the functionality of any of one of
those three synaptic mechanisms is going to change
the overall excitability of the pyramidal cells
if you do any input mediated by the inhibitory into neurons
and that will usually be expressed in terms of the
frequency and possibly the amplitude of gamma light responses.
I'm not saying there'll be oscillations, I'm saying that there will
be a predominant high frequency gamma signature
to this interaction between the fast-spiking inhibitor into neurons
and the pyramidal cells, which interestingly itself
sets the synchronous gain because
if it is the case that high excitable cells
that have an EI balance in favour of excitation
are persistently depolarised
and become leaky, they're membrane time constants shrink
which means that their integration window is smaller
which means the only way to keep firing is to fire more regularly
and in synchrony. So the idea is that you get an increase
in the frequency but possibly a decrease in the amplitude
of gamma activity as you increase the coupling between
excitatory and inhibitory subpopulations. The reason I've told
that very complicated story is not to try and convince
you any of it is correct but to make the following
point that any plausible story will have
to accommodate the fact that there are probably multiple ways of controlling
EI balance that range from classical neuromodulators
right the way through to the molecular
configuration of NMDA receptors, right the way through to population
dynamics that mediate the synchronous gain through
fast synchronous interactions
at the population level. So I suspect it's a very complicated
story that you will only really understand when you actually simulate it in detail
and if you change one sort of gain control
you're probably also changing another sort of gain control
and that means that people who think that schizophrenia is caused by gamma problems
probably right, the dopamine people are probably right
the gamma oscillation people are probably right, they're all right because
all of these biophysical mechanisms and neurochemical
and molecular mechanisms all work together in setting
the dynamics that ultimately determine
whether this pyramidal cell sends a spike out in response to this pattern
of input from the outside. Does that make sense?
Yeah, I was thinking about this dynamic
equilibrium between
different parts of our brain and different groups
of neurons and
just curious how we can
make some
typology of how schizophrenia works
based on types of errors
inside this equilibrium.
Yes, well you should write papers about it
if schizophrenia
or autism or depression or anxiety
was just a failure of gain control
a failure of a Kalman gain in a predictive coding
then there would be no specificity to the symptomatology
so we know immediately that it's not every gain control
that's broken, it's in very specific systems
from my point of view, very particular parts of the generative model
or at least the factor graph that inverts that generative model
and then you can start to look at the phenomenology of the symptoms and signs
and when you look at schizophrenia you realise in fact
although there's a lot made of cognitive deficits
and wide-ranging perceptual deficits in schizophrenia
in fact the majority of the symptoms and signs
are very circumscribed, they're all about behaviour
in the context of interactions with other people
so the actual content of the symptoms and signs
it's either me hearing voices that I may think you have produced
or you put thoughts into my head
or I can't move because you've made my acts
so these are not generic brain failures
or they can't remember numbers, they're very particular
and that sort of suggests that the neuromodulation
mechanisms that are broken in schizophrenia may indeed
I think probably be tied to dopinergic dysfunction
because the regional specificity of the dopinergic projections is exactly to those brain systems
that from the point of view of active inference are all about
generative models of the consequences of action
in particular possibly me as an actor in a social world
that is populated by other creatures like me
so that would be one example of using what we know about the phenomenology
of a broken brain to get some regional specificity
into that and then you can play similar games
or have similar arguments with the selectivity of different drugs
why do hallucinogenic drugs
cause visual hallucinations
but not auditory hallucinations
why don't they cause delusions
there are all sorts of interesting questions about the neurochemistry
so although I tried to make the case that the etiology of gain control is very complex
and multidimensional that's not to say that
the path of physiology or a particular psychiatric or neurological condition
everything is broken
I'm just saying that there will be downstream consequences
if you had Huntington's disease we know exactly what the molecular pathology is
and how the synaptic gain control is broken
but it will probably be manifest in many different ways at the level of synchronisation and gamma
or say beat oscillations in Parkinson's disease
but the synapse is a brief in schizophrenia
and you should write that down and send it to biological psychology tomorrow
that's what I would do
thank you for your talk
I have two questions
the first one is about your model and hallucinations
according to your model hallucinations
your prior dominates over your
sensation
for example I see a dinosaur in my room
which is actually not there
and I cannot adjust my belief
because my sensation is insignificant
but when subjects experience illusions
there is a component
which is reality testing
when you test your reality not against sensations
but against the context
so somewhere I have a higher level
belief or prior that conditions
to the fact that I am in my room
and it is a strange situation
because
we have too strong prior
at lower levels
but too weak prior at higher levels
so another question
very physiological
is specifically about ketamine
because it is well known that ketamine reproduces
negative symptomatics quite well
and it disconnects from the external world
so your prior dominates
but surprisingly ketamine and astesia subjects
have excessively strong generations
more than a weak subject
because in this scheme
there should be some how related to
transmission of sensations of information
so do you have any explanations
or suggestions about why it is happening
I have explanations and suggestions for everything
where there are good explanations and suggestions
is another question but as I am here
speak to the two very challenging and interesting questions
and I don't pretend to have the answers
I know that I have a delusion
and yet I still have the conviction that my delusion belief is correct
so I know that there can't be an elephant or a dinosaur here
and you believe that there is a dinosaur elephant here
which speaks as you say to an even deeper hierarchical
inference where you've got this metacognition
I have beliefs about my perceptual beliefs
it reminds me of Max Colhart
an Australian psychologist who says there has to be two deficits
you have to have the deficit of perception
I don't subscribe that it's too complicated
but it does speak to this having beliefs about beliefs
but I think that's perfectly plausible
in the context of this hierarchical inference
in a sense illusions are a bit like that
if you have a classic hollow mask illusion for example
or even the illusory perceps that I expose you to
there are multiple demonstrations here
you know their illusions but you can't not have them
so imagine what it would be like not to have an illusion of a hollow mask
but to have an illusion of a dinosaur
you can't not see the dinosaur in that part of the room
you know it's an illusion but it doesn't stop you perceiving the illusion
so I think that
of perceptual beliefs or personal beliefs
possibly even equator or qualitative experience perceptual experience
and the knowledge that it cannot be real can be completely divorced
and you can have both experiences at the same time at different levels
in a hierarchical model and as you rightly point out that does suggest that there is
it's not just simply sensory precision from the point of view of predictive coding
and prior precision it's a delicate balance at multiple levels
that would be necessary to explain the very complex
experiential phenomenology
and the way that we explain it to ourselves as sentient beings
when we know we are a sentient being
so it's probably much more complicated than that
and indeed philosophers like Andy Clarke have now started talking about sensory levels
intermediate levels and very high metacognitive levels
and worrying about the imbalance, what it would look like from the very highest level
if it knew there was an imbalance between the levels below
being aware that I was having an illusion or an illusion
or indeed an illusion
so it's a very interesting question which strays into philosophy
I still think it's probably amenable to electrophysiology because you can measure
in say language you can have different levels of hierarchical violation
in language for example the phonology verses the semantics verses the
say the emotional content, the prosody which will be represented at different levels
in the hierarchy you can use an odd ball like paradigm to try and get at
which level it may be
implicated in conditions like schizophrenia
or people will certainly have a higher schizotypal score
so that's a very interesting question, no answer, do you have any answers
you're still, you're going to write your biological cycle
and we, yeah it should be like this
like both metacognition because
it could be a sensory deficit for example
with absolutely normal brain
and it would open up hallucinations
yes absolutely, yes that makes a good example
of sensory deprivation or indeed hysterical symptoms
some very low level abnormality
either avert or covert in terms of sensory position but you know he's there
and you just have to adjust to that it's a good example
and the second question was equally challenging and that was about
yeah it was about genman ketamine
much of this theorising actually arose from the observation
that ketamine and NMDA receptor
challenges to NMDA receptor function
could almost identically produce a lot of these signs of
schizophrenic perception and possibly even thought disorder
so that was important just to stand back and put in context
dominant theories which were all about genetics and first hits
and second hits and neurodevelopment
no you could actually reproduce schizophrenia here and now
in a normal person just by changing synaptic game control
and that was I think a very important observation
you didn't need to have a schizophrenia philic gene
you didn't need to have a refrigerator mother, you didn't need a life event
you could have schizophrenia here and now within about 30 seconds
if I gave you the right game control in your brain
you know one of the initial motivations
to look at that from a point of view of active inference what does that mean
this is not about learning, this is something that is broken
in the here and now in terms of inferring what's happening now in the brain
what is the important if you like likely candidate
that could support that and it's the synaptic efficacy
that is mediated by voltage dependent NMDA receptor
it was exactly that sort of contextualisation
of postsynaptic sensitivity
that is given by NMDA receptor function that is the primary
target of ketamine which led to this
particular formulation of a functional disconnection syndrome
and your interesting very practical comment about
the predictions about altering
NMDA function and how it would be expressed in terms of gamma
that I should almost ask you to speak to
but notice I said that as you increase the
excitability for example but even that's a little bit difficult
sorry I should just stand back I think to answer
your question definitively you really need to simulate the particular circuit
you are interested in because you can get lots of
dynamical feedback and count intuitive results once you start to put
feedback loops in, feedback loops in, feedback loops in,
so sometimes very difficult to intuit what's going to happen
but even in a simple excitation in abyssin pair
increasing the coupling strength between the two
increases the frequency of gamma but usually decreases
the amplitude so increase the excitability in very simple
neural mass models or yes neural mass models of simple
circuits you would actually predict a decrease in the amplitude of gamma
but an increase in the peak frequency so that may be
and that is a
is this Wikipedia again?
Is that your experience
why don't you talk about your interesting results when they go up one side
or down the other side with visual motion?
I mean there is some very interesting
work that speaks to
the importance of gamma versus lower frequencies in terms of a
symmetries in message passing between cortical levels as well which I think speaks to this issue
so the story at the moment
is that forward connections
that from the point of view of predicted coding would convey the prediction errors of the news where the information
are conveyed at fast frequencies from superficial layers
whereas the descending or top down backwards predictions
are mediated at lower bits say beta or alpha frequencies
from the deeper layers so I think these oscillatory
oscillate frequency dependence are
going to be very important in terms of understanding
what part of the message passing is abnormal
or where is the site of action of ketamine for example
using models of these
microcircuits
I see that there is a different kind of prediction errors
and for example you said that when we ignore an external stimulus we hallucinate
and we can perceive
some external stimulus
as an external stimulus
and we can perceive
some external stimulus
as a stimulus with a lot of ambiguity
so I wonder could it be connected to
with a free energy principle
and if we had a subject that perceive a lot of ambiguity
does he have
a lot of free energy
or can it be connected?
there are some consequences for this theory
yes I mean
the reason for giving this talk really was to show
how the free energy principle can be unpacked in
progressively greater detail to answer some key questions about
functional anatomy in conditions like schizophrenia
so all of this ties together
so you start off from my point of view
this is not how everybody sees it
but from my point of view it would start off with
the free energy principle that inherits from the basic physics
of 17 things
what that looks like from the point of view of a life or biological scientist would be active
inference
and the processing of an inactive sort
comes a number of different schemes or implementations
that people have used to understand the brain
they include belief propagation in the brain
and predictive coding
so predictive coding is one example of a scheme
that is consistent with active inference
and is an example of something that conforms to the free energy principle
and predictive coding is a particular process theory
when it is used as a metaphor to understand neuronal processes
so predictive coding as a scheme I repeat was originally devised
in the 1950s to compress sound files
but now has become known as describing an algorithm
the brain might use in terms of explicitly
computing prediction errors and passing those prediction errors around to improve predictions
so this is just one process theory
that one could entertain under the general
principles of active inference that themselves
are consequences of a system
that are all consistent with or comply with the free energy principle
and if you accept that then there is a direct
quantitative relationship between free energy
and the amount of precision weighted prediction error
so that means you're absolutely right that if I
had false inference and it thinks the question what is false inference
because of course from the point of view of a person with schizophrenia it's not false
that's what they believe because you might believe something different doesn't make me false
and you true
you didn't say that before
no no I'm saying it now I can even say there's a mathematical proof of that
so when we talk about false inference
what we mean is that
in relation to a free energy minimising
generative model of a given world there is a base optimal
maximum evidence generative model
and that maximum evidence minimum free energy generative model
will also include a model of the precision
so the predicted precision or subjective precision
after inverting that model which means for any given world
or any given pattern of sensory inputs there will be
a base optimal
encoding of the precision so when we say false
what we mean is not base optimal
from the point of view of some imaginary set of
flux or patterns of sensory stimuli
that that patient is experiencing
because that's a silly concept because everybody has different stimuli
and we make our own stimuli anyway
and of course the evidence is as much a function
later as it is of the model so remember throughout I've been conditioning
everything upon the model and the model has primes
which means that every person will have a different optimum
scheme that is defined by their private beliefs so in that sense
you can't be suboptimal it just means you've got a different
sort of private belief and that's the bit that's mathematically provable
and it's called the complete class theorem
ok thank you it was very
very important yes I remember that you said
on your first lecture that predicting is something that evolutionary
help us to survive it's simple
and if we can't make good predictions
then we are not
developing as an even organism
for example a black drop of
in the water it can be
whole it can
it will not react on the
external stimulus it's a bad example
ok how it can be connected with the evolutionary
principle and it is not like
we are different and we
don't have both decisions and
we don't have both prediction models
it has an evolution
therefore
so let me restate the question
I think the spirit of your question is absolutely right
that there's a direct link between adaptive fitness
the survivability of a particular phenotype
that particular phenotype and their ability to suppress prediction error
and ultimately getting the precision of those prediction errors right
when you're conditioning on a particular context or a particular set of primalities
they're all part of the same thing absolutely
so I think the simple concept here is that the free energy
or the model evidence
measures the goodness of fit between the system that has
it's mark of blanket so they all drop with a mark of blanket that doesn't diffuse off into the
into the solvent and
what's going on on the outside so
it's a measure of the relationship between you
and your world and in that spirit
there's only one relationship there's only one you and there's only one experienced world
therefore everybody is unique in the way that they make
their base optimal inferences and that's what the complete
theorem says it says that for any pairs of behaviors
or actions upon the world under some loss or adaptive
fitness function there are some primalities that make
those actions base optimal in accordance
with the free energy principle so that's just a fancy way of saying
that anything that exists is base optimal but becomes
interestingly unique in virtue of the
privacy it brings to the table so that if you like
challenges the notion of false inference if all inference is base optimal
under some primalities
that constitute the generative model you can't have false inference
your question is why am I talking about false inference
and the reason I'm talking about false inference is that people
who suffer in the sense
that they do not survive mental illness
can be understood as having generative models that are not
fit for purpose and they are not good models of their world
so imagine in a very simple sense being
blind and walking into the road and being run over by a car
this would be an example of somebody who has not
optimised the precision of their visual input
because it has zero precision because they're blind and therefore
they are not able to model the consequences of their action in their world
and they would not survive very long now of course there are lots of therapeutic
and remedial moves you can make to accommodate
that kind of false inference but it's in that spirit that I meant
it's only false in relation to the lived world of that moment
it was a simple example that I understood
but you're right prediction error is just
precision where to prediction error is just the energy under predictive coding assumption
so if you've got the wrong precision and your precision where to prediction errors
are therefore mishandled you're absolutely right
you're not realising your free energy minimum
you're not realising your full adaptive fitness
it's much simpler than you might imagine once you assume
Gaussian random fluctuations and continuous variables
the log probabilities that we're talking about are just simply
squared residuals or squared prediction errors
that's how you measure the mathematical under Gaussian assumptions
the square of the precision where to prediction error
that feels
one more question please
and for example
due to a transfer of blocks
you can show off the relative and elective strategies
to go further than the relative
so the kind of question at the moment is how it could be
compared to the general level
and we can, from a point of view of psychology, explain how we just
provide things with error models
but the very important train piece of this process
is a new physiological process
how is it possible to invent a pre-operative strategy
you've told that
you have some explanations
I didn't catch all of the
question but from what I understood you're asking
how do you get in therapeutically to enable people to explore
new strategies or put new strategies in place
when old ones are not free energy minimising
or that's somehow causing distress
I mean that's a very important question
of course it's a sort of question that people are actually dealing with patients
ask themselves all the time
and I could speak for a long time about this
and I enjoy listening to myself speak because I'm not very good at sensory tenuation
but I'm not sure
it's almost common sense
certainly let's take either behavioural
or indeed psychoanalysis of a psycho-dynamic
you're creating a context in which
new hypotheses particularly about policies of interpersonal
exchange can be explored safely
so you're now revising the prior beliefs about the sorts of policies
and the evidence for the consequences of those policies usually in the context of
either a therapeutic relationship or
as actually documented in say using diaries
in cognitive behavioural therapy providing a new context
and a new source of sensory evidence that enables people to re-evaluate
their prior beliefs about the policies and the strategies that are available to them
so I think that would be a simple way of
using these mathematical constructs to rationalise and to explain to the patient
this is what we're trying to do we're just trying to broaden the scope
of a repertoire of strategies that you could try
and work out why you a priori
have just been focusing on these and whether there's enough
evidence to support a particular focus on these strategies
so let's now in this safe environment talk through another way of
doing things and what happens so you can revisit the prize
so that sounds simple of course it's incredibly different to do
therapeutically from the point of view of physiology though to do that properly
you're going to have to get into this precision control and get control over
the synaptic efficacy which basically allows
you to adjust your prize which from a psychology point of view is basically
attend so you've got to train people to attend
to different sorts of evidence or attend to different ways of doing things
and that I think if you could do that then
you're in a very powerful therapeutic position if you can train people to get control
for example imagine what it would be like
to be able to willfully suspend
your mechanic suppression to actually look at the optic flow
imagine what it would be like to both
hear and speak at the same time to suspend the sensory
attenuation associated with acting upon the world and at an even higher level
to be able to attend or attenuate bodily signals
that you may not be able to ignore so imagine that you're anxious
and because you're anxious you cannot attend away
from all the bodily signals that say yes you're in an aroused physiological
anxious state your blood pressure is high
your heart rate is high your respiratory rate is a little bit too high
and if you can't ignore that the best hypothesis is
yes I'm in a state of anxiety and that feels horrible
but if you could get somehow conscious control over the precision of those
intraceptive prediction errors you could functionally ignore
that evidence and create a new hypothesis
I'm happy because once you've got that hypothesis
those descending predictions will then be realised by your autonomic reflexes
and you will actually be up calm and happy if you can gain control over it
because that raises the deep question can you mentalise
these subconscious processes can you actually get
voluntary control over them and I imagine the job of the therapist is basically
to go as far as you can in terms of mentalising these things
is that what you had in mind is that the
no, no it's not, it's something else
listen to if you have a big question
you can ask me afterwards
thanks anyway
I have two questions although they are more related to just this topic
and any practical prediction
ultimately depends on the form we assume for it
that it really has for each agent
but other than the constraint that it exists
which is I guess already quite strong
we don't have any clues about the specific implementation
of this model right so because it's so important
and everything like predictions and action
I guess it would be important for this
for being
for being
accepted more readily
having some way to
find out
what's the specific form of the genetic model
those are the first questions
and the second is about
it's also related but more in the opposite sense
so I've noticed you usually
when you implement the recondition density for inverting the model
but I haven't read all your papers as a few of them but
you use the means to approximation and you assume that the factors are independent
more for
because it's practical because it's easy or just
you really believe this is actually what the brain is doing
before we go
again to very interesting questions
which I think have fairly clear answers
the heavy lifting and the challenge as either scientists or therapists
the message passing on the implicit
generative model and in a sense that's our job
that's why I do brain imaging
the principle, the free energy principle does not help in the slightest
doing that it's a little bit like knowing that the
principles of evolution
constrain and lead to natural selection but that doesn't tell me
the mechanisms by which my eye evolved so you have to get into the
process theories and the actual detailed structures of these generative models
to understand how me
or a human brain or an animal brain actually operates
under these principles all the hard work is still there
and that hard work is exactly what we do as a scientific community
so we do it through neurochemistry, neuroanatomy, neurophysiology
that is trying to discern the hierarchical structure or not
hierarchical structure or small world connectivity
of the neural networks that we think must have the same form
as the implicit generative model that's why I showed that link between the generative model
and the factor graph but we don't know what that form is so that's why we spend so much
time studying and reading about neuroanatomy
and what causes hierarchies well what is a hierarchy
it's only defined in terms of asymmetries being forward and backward
connections between different hierarchical levels which is why
an enormous amount of neurophysiology is all about getting at those
anatomy, those asymmetries, all of these I think are just
in the service of understanding the form of
the generative model, the implicit generative model
that is being inverted by the message passing and then you get to all the
physiology of the message passing and you have deep questions like
is the generative model assuming hidden discreet
states or is it continuous so we haven't talked about that at all
but it has an enormous implication for
your predictions of electrophysiology or psychophysics
so the difference between a discreet states based model
of the sort used in those Markov decision processes is that your internal
or generative model of the world means in every attribute
you're either in this state or this state or that state and no other state
so discreet things are either hot or cold but not both at the same time
which is completely different from a continuous
states based generative model where this is the temperature
that contains values from minus infinity to plus infinity
with prior constraints of course but in principle these are continuous values
now the message passing for these states based models with continuous states
versus the states based models with discreet states
are fundamentally different in terms of the implementation
mathematically they share a lot of features and in fact you can cast them both in terms of prediction errors
but the message passing would look very very different
so on this hand the discreet the sorts of models you might think
would be good for language or categorising things
discreet states based models they'd use belief propagation or variational
message passing the continuous stuff which would be good for detecting
visual motion or moving arms around in a continuous
space they use things like predicted coding and calmen filtering
and of course they have very different neural and process theories
associated with these versus these so all of these unknowns
really now become
you can start to address
but you don't know by looking at the empirical data
so for example it may be the visual system uses
calmen filtering or predicted coding with continuous states based models
whereas the prefrontal cortex uses discreet states based models
with discreet representations of grandmothers or past and future
hot cold like that don't like that me you
so there's no answer there other than to say that you've identified
where all the heavy lifting has to be done
it is in defining the form of the geritim model that is a useful
way of framing the way that you and I work
or our conspecifics work and that's just neuroscience
systems neuroscience at least. The second question
although it sounds much more mathematical and different
actually addresses a very very similar point
so the question for the non-mathematicians is
what I've spoken about Q
in these presentations what I implicitly
mean I don't actually have one here
what say mu here these are the parameters of a belief
and a belief is the probability distribution that has been
directed by Q throughout. One thing about
that belief is not the posterior probability
it's not a posterior belief which would be a very very complicated object
which would be almost impossible for a brain or a physical object to
parameterise or encode. It's an approximation
and the particular approximation is that you carve it into factors
implicitly assuming conditional independence between various factors
and that means instead of doing exact Bayesian inference
when you optimise a model in relation to Bayesian model evidence
you're doing technically approximate Bayesian inference
and that's where the variational free energy function comes in
so the variational free energy is not equal to the evidence
it is always bigger than the evidence if you try to minimise it
or lower if you try to maximise it hence the evidence lower bound
and the very definition of approximate Bayesian inference
is variational Bayes i.e. the use of a variational free energy
is that mean field approximation where you factorise or carve your beliefs
into conditionally independent factors
hence factor graph
and the question is is that for convenience or is that how the brain works
and the answer is it's how the brain works
and the deeper answer is
that the factorisation itself
can simplify your generative model which minimises the free energy
so once you cast everything as approximate
Bayesian inference there are good factorisations and there are bad factorisations
so you can imagine taking this sort of factorisation
this sort of factorisation and measuring the free energy
and there will be good factorisations good in the sense of minimising variational free energy
or maximising model evidence
and one of the most beautiful examples of that for me
is one of the most enduring large scale
functional anatomy principles of the brain
that people like Leslie Angolider and Mort Mishkin articulated
in the 1980s which is this distinction between the what and the where pathways
in the human brain. Is this familiar
to the brain being more about where I'm reaching
so also in terms of Goodale and Milner
the dorsal stream being more concerned with where
you're looking or what way you're going to move
whereas the ventral stream is much more about encoding what the object is
so this dorsal ventral
dissociation is a physical anatomical factorisation
between two fundamental attributes what and where
and you may ask why is that a good factorisation
why is that a good mean field approximation
but it's obvious knowing what something is doesn't tell me where it is
so if the world contains objects that have some
translational invariance then it must be the case
that there is a degree of conditional independence between what something is and where something is
which means that I can predict the sensory
impressions of an object that is there simply by
taking the interaction between these two marginal distributions
so for me the separation of the dorsal ventral streams
speaks to a fundamental mean field
approximation that provides a much simpler account of a world
that has a minimum computational complexity and algorithmic complexity
and KL divergence between the prize and the posterity
as therefore a minimum variable free energy
that is better than a world that didn't have that factorisation
so that's why I say the answer is it's because that's the way the brain works
because it's trying to find the simplest factorisation
it can use to maintain an accurate prediction
of the sensory inputs that it's trying to predict
and I'm sure you'll find that sort of factorisation wherever you look in the brain
and I would imagine that evolution as a base optimal process
has found the best mean field approximation in our brains
for the sorts of worlds that we live in that you could possibly have
Does that sound convincing?
I'm curious what are you working on right now
what makes you curious and
maybe seems promising to work on?
Well the honest answer is I generally work on what my students want to work on
so I'm very in response mode
so wherever I get a new PhD student every year
I get the right idea or some ambition so I normally do what I am told
This year it just happens to be two of my younger people
colleagues interested in language so this year we're doing language
language understanding
I don't know what we'll do next year
It's very dope and if you're doing our language
I would like to ask you how human language capacity
affects to active inference
We sort of addressed that the other way
What can active inference do for understanding
the functional anatomy of language and in fact the language
speech recognition and speaker recognition and language understanding
so it's just a question of building even deeper
generative models based upon that Markov decision process
scheme with that temporal diachronic depth as well
and seeing what are the minimal structures and factorisations that we're just talking about
that would sensibly reproduce synthetic conversations
One insight is that you have to do questions and answers
You think about language it's just sort of communicating beliefs
but when you really drill down to what is the purpose of everything
it's to minimise free energy
or expected free energy which is just uncertainty
then language should just be there to minimise uncertainty
which means that you should be able to use it to simulate me
asking questions of a world in the same way that I
move my eyes around but now I'm asking questions of you
I literally have to ask you a question and I have to ask you the questions
to illicit the answers that minimise a greatest uncertainty
about something that we share knowledge about but you're very confident about
and I'm very uncertain about so that's how we're now
building these Markov deeply structured Markov decision
processes where one generative model is
talking to another generative model by asking a question the other one's
returning an answer and then they switch roles so they try to share their beliefs
between themselves using spoken language
so that's how we would address that
the purpose of that really is to understand
speech and noise and from the point of view of the
clinician ways of intervening in terms of
active listening or active
noise cancellation for example to adjust the
generative precision into an active hearing aid for example
helping people who are losing their hearing or have age related hearing loss
on the commercial side they should give you
things you can talk to in the sense that they start to ask questions about you
and they're showing genuinely interest in you
which is always very nice
another question not about language
I'm interested in
so different species
it's normal but I think have different
amount of anxoesities around them
so how to quantify it
I mean the anxoesity of phenomena around cat
is not equal to mine and how to formalise it
operationally you do something called
computational phenotyping so that's a fancy
name for assuming a particular
generative model for your cat or your human being
and then using the observed behaviour can be choice behaviour
for reaction times or it could indeed actually be electromagnetic responses
as measured with things like MEG
the game would be then to optimise the prior beliefs
of the generative model the prior part of the generative model
such that the observed empirical response of your cat
or your human being was made as likely as possible
so you maximise the likelihood of empirical behaviour
or electrical responses with respect
to the generative model that when inverted
the same choices or same stimuli
would predict the behaviour of responses
so it speaks a little bit to the question that we were dealing with before
that there is no real false inference and that everybody can be completely
characterised in terms of their prior beliefs
they are just the prior beliefs that you can use to understand
that person's responses, that person's choices
your anatomy, that person's electrophysiology
and that's important to realise from the point of view of biomarkers
certainly in psychiatry because what that means is
although you can never find
although you can tell what's called a just so
story about Bayes optimality being an exhibition for this behaviour
given the fact that any behaviour can be
described as Bayes optimality under the right priors
what it does mean though is that there must exist
a set of prior beliefs that fully characterise your responses
and if you can estimate them empirically that means that I can phenotype you
in terms of your pri beliefs and part of those pri beliefs will be the encoding
or your pri beliefs about precision which is exactly the pri beliefs about
uncertainty and I'm sure you're absolutely right, the more sophisticated
treatise like you or me will have much greater
latitude and accommodate much more uncertainty in our posterior beliefs than a cat or a virus
that probably has very precise beliefs about the consequences of their action
It looks it's based just as what was going on there
old really old
being as a un bent in 20th century psychology
really familiar with it
there are experts in that who have written to me about that
I'm not a scholar in that field but I have read about the
connections between them
I want to get back to language and ask is there any difference
between cultures where we
found words using letters and cultures
where there's a hieroglyphic
way to express some
images and
sense
I know that there is some
that they called time differently
first linear, second more
circle of
what
expectations do you have based on
principle we know
of differences we could expect to see in how they
express languages
I have no answer but that sounds like a really interesting field where you can make some very strong predictions
under an active influence framework
so if you take language of the linear sort where you have to read through a long sentence
on the ordinal structure of that sentence a linearisation of the semantics
of the syntax
and you think what sort of derivative model would you have to have
to infer the meaning of that particular sentence as opposed to an iconographic
or pictographic structure where the ordinal
aspects of the sequential sampling with eye movement for example didn't really matter
you'd expect some profoundly different structures in the generative models
and you would predict very different structures in terms of
the auditory hierarchy and the electrophysiological responses to violations
that were not dependent upon the ordinal structure
so I have no answers it's not my area of expertise
but it is a fascinating question because it speaks
to the way to a fundamental capability
we all have which is to read and I think reading is a wonderful example
of epistemic foraging
of active perception where you really are in charge of the next visual
impression on your phone wheel representation on your retina
the words that you target you're in charge of
so this is not this passive presenting stimuli for you to make sense of
you actually have to guess where to look next to go and get that
and of course in language because it's got a deep temple structure
it's in this active inference now has this deeply diacronic structure
which means the generative model must also have which means the anatomy
and the connectivity must also have that
so if I had to do it I would try and simulate
the reading of both sorts of language and then optimise the model
using basic model comparison look at the factor graphs and then make some predictions about the
neuroanatomy in electrophysiology
but we haven't so don't know enough about that
and this will end soon so I think we have time for two questions
Thank you so much
May I ask you about predictive coding and sleep
According to event-related potentials your predictive coding doesn't work here in school
so we can't register this electrophysiology
for responses to external stimuli
but during our dreams we interact with other people
so we can't predict their actions
so we can call it predictive coding
are these two different processes of predictive coding
during dreams and during behavior
waiting
I'm just wondering if I have an excellent question
I would answer and I think most people
in the predictive processing field would answer no they're exactly the same process
but one is in the context of
switching off the precision of sensory information at the level of sensory organs
so the
the aminergic control of the sleep state basically
involves neuromodulators that you can imagine as it's basically turning down the volume
on all sensory inputs
with one exception and that one exception is the ocular motor system
so you can still move your eyes when you're asleep
you can't move anything and obviously breathe but apart from breathing
and moving your eyes you can't do anything else when you're asleep and that's because
not that you can't issue commands which is the prediction errors have zero
precision then zero gain
so they can't issue commands so you're effectively paralyzed
but everything else is exactly the same
and dreaming is a nice example because if it is the case
that you are
very much like that bird that was hallucinating the song
on the basis of its autonomous dynamics
endowed by the Lorenz attractor, if the same kind of process
is operating in our generative models we have orbits or attractors
and autonomous dynamics generating fictive narratives
and fantasies and exchanges flying through the air going to parties
having very fantastical experiences that are all being generated
by these autonomous dynamics sending predictions down to the low level
and we spoke about this yesterday of a more
and more fine-grained, more detailed, more featureally
specific sort all the way down to the visual cortex in fact not quite that far
but the important thing is they're never corrected
by ascending prediction errors because you've switched off
the precision of the sensory prediction errors
so they're unconstrained so this would be
illusory percepts that are not constrained by
any prediction errors but the same process is exactly in play
where you may be asking well can you possibly minimise your prediction error
or free energy in the absence of any new information
and the answer is yes and the answer is very simple
if you remember free energy is accuracy minus complexity
so in the absence of any new data or the data
that has zero precision the accuracy term goes away
but you're still left with the complexity term
and now simplify your generative models by running
them as dreams and eliminating the redundant
parameters and synaptic connections so to minimise the free energy
in terms of the synaptic connections simply means to minimise the complexity
of your generative models by removing redundant associations
so the story here which in part absorbs
Julia to know the synaptic homostasis hypothesis is that we go around
sharing the daytime remembering or encoding
learning all these spurious associations
that are caused by the things we experience
so we have an over parameterised, overly complex
generative model when we go to sleep and then by switching
off sensory input we then sleep and then we
rehearse those experiences and get rid of the unnecessary complexity
by removing the redundant connections and in fact interestingly
what happens is you actually get those clean
factorisation so you get a better mean field approximation as you sleep
and remove those redundant connections so you build up the synaptic
connectivity during the daytime and then you get rid of it during the night
so this is what Julia refers to as synaptic homostasis
but mathematically it's just keeping free energy down all the time
but in the absence of any new sensory information
I will actually show an example of that tomorrow in Silico in simulations
you may be asking well that's fine but what about
the prediction errors and how they are expressed
in terms of what you can measure remember I said that all the
sensory information apart from ocular motor prediction errors are suppressed
during sleep and you may ask why well the eyes
are part of the central nervous system not the peripheral nervous system
well what that means is that your predictions about your eye movements
are actually still realised and
if you're doing cat studies these will be manifest as PGO,
Pontyn genicoloxipitur waves so they're like the ERPs
associated with eye movements or actually rapid
eye movements observed during sleep which some people think
explains the association between rapid eye movements
dreaming and REM sleep characterised by these
internally generated mismatch, not mismatch
vote responses, belief updating responses
as you are enacting your top down proprioceptive ocular
motor predictions which would suggest that most dreams
certainly during rapid eye movements have to have a visual content
and have this active scanning searching aspect to them
does that make sense? again if you're here tomorrow
I'll very briefly mention that and have a pretty picture of somebody who's asleep
just remind me to
so just another language question
is there such an element called cryptophagia
when there are identical things in terrible language
and there is a recent series of this conversation
came through an actual language
a few months ago
I'm just wondering that you're about to conduct
a computational experiment very similar to that
you have similar priors that can predict each other
with minimal errors
I'm just wondering what prevents
a single agent in fact like
bearing new behaviours in situations compared to
the agents talking to each other
so what is the essence of this
talking to agents very similar
compared to just one agent dreaming
in your characterising probability distribution
a very important question and observation
that much of our generative model inherits from the world that we are
trying to model which is largely constituted by conspecifics
and parents and teachers and the like
there's a very strong cultural aspect to the developmental
structure learning that underwrites the nature of our generative models
language is a beautiful example of that
so in principle you're absolutely right if you simulate two free energy minimising
mark of blankets or two creatures that like to
predict everything they will converge on exactly the same language
and generative model so that bird, simulated bird that I showed you
we actually simulated two of them talking to each other
and you're absolutely right they converge on exactly the same manifold
as we discussed in terms of this notion of generalised synchronisation
that the best solution for two people is just to become the same person
so what you do is what I do, what I do is what you do
all I need to then is to work out who's turn it is through sense attenuation or attention
and we've got a perfect dialogue in our own private language
don't have to, we could sing together as in a choir
and you know just listen and talk at the same time
but the same language so you generalise that to multiagents
then you're going to have a shared language so no longer
will just you, our dyadic language be sufficient
we now have to share it with all dyads of dyads and dyads and dyads and dyads
which brings in the cultural aspects so now they become
a teacher and a master slave teacher
student mathematically skew product structure to the
learning and that's also can be simulated with these birds so you can put a
grown up bird that has very precise beliefs but exactly the same generative model
in hearing distance of a young bird that has very
imprecise beliefs and then they'll both learn each other
but the bird with the imprecise pride beliefs learns much more quickly
so it moves towards the older bird
so they're still sharing the same, they learn the same generative model
but because one bird is more confident about singing
because she or he is older, he will become the teacher
or she will become the teacher and the younger one moves to it
and then they have this perfect generalised synchrony
so that speaks to the importance of culture
the importance of synchronisation and the fact that the worlds
that we have to actually model are largely created by creatures like
us, what would happen in the absence of that is not a really interesting question
so let's say by magic with a vault of phenotype that
suddenly was born into a world or a universe where there were no other creatures
like itself so the
the empirical example would be a feral child
so a human infant is left accidentally
by some careless parents in the wood and it never sees
another human being and is raised by wolves for example
so you can see what the consequence is
for the generative model that is entailed by its brain
when that child is recovered and brought back into
our society
there is enormously impoverished language, there is no need to exchange
in fact you might argue in extreme cases very much like autism
there is no theory of mind, this child never needed
to develop the hypothesis that there is a me in the world
because that child never ever needed to distinguish between you and me
why? because there was no shared generative model
there were no other conspecifics where the model
of itself served the purpose as a model of what
you might be thinking on doing or intending when you act
and if there's no need for me to disambiguate
between did you do that or did I do that, there's no need to have a
model of me so you don't have a theory of mind
so you might guess that the individual raised an isolation on the feral child
or because of failures of oxytocin neuromodulation for example
who fails to develop
any truthful communication but also
the theory of mind which is simply a hypothesis that things like me exist
outside, that's all it is really
and there would be not only deep problems with language but just getting that child
to appreciate that other things around it are other human beings
that would be my guess, do you know the literature on feral child
I know a little bit about the critical theory
just because it says that the critical theory is like extended by genetic mutation
and some like two tweens actually talk about it
it's just that it's blown enough to invent a curse of language
and that's thought against
70 million years ago
estimation is funny
because all genetics was in place and like all the global stuff was in place
so people learn to talk to each other
but very very simple language
I don't know if some Russian scientist in the US
I think his name is Andrei Buschetsky
he's a neurologist in Boston University
and he deals mainly with altitude
is this part of evolution
is this part of evolution in psychology or is it beyond that
evolution in neuroscience
original contemporary human mind
there you have it
ok thank you very productive
discussion after the lecture
can you hear me
thank you
