So, my name is Kasper Hess.
I put like two of the pictures that resemble, yeah, like refer to my background here in
the sense, and this is an actual picture of my brain.
I got it sneaked out of the lab in Amsterdam where they have a rule that actually they
can't give pictures people's brain to people because there often tend to be little cysts
in this kind of scans and then people start to think like, oh my God, I have a tumor in
my emotional part of my brain.
So they kind of like, yeah, they hear that my friend picked a slice without any cysts
for me, but that's kind of like one of the interesting things that we'll be focusing
on this, sensitivity to statistical aberrations to RIS, like sensitivity to departures from
our expectations and that in this case can lead to emotional like stress that they want
to avoid, which is why they don't give pictures.
And on the top you see an object, I think this beamer doesn't show it very well, but
it's actually a galaxy and a black hole in the, oh yeah, that's better, a black hole
in the center and you see this like, yeah, we call them jets, basically flares of energy
coming out of the system, basically perpendicular to the galaxy.
And yeah, my previous work I would focus on how black holes, the environment of a supermassive
black hole can generate those kind of flares, but why I put it here is in the sense that
I was interested in how this part, a piece of biological tissue can grasp the logical
structure of an object in the universe out there.
So we're going to be focusing on, in a pretty fundamental way on how that biological tissue
can capture logical, let's say, Bayesian structures in the out there in the universe, like as
a consequence of the fact that we do that for ourselves all the time.
So on the schedule, I promised to give you a modeling tutorial and given that Ryan already
treated a lot of the modeling part, I decided to focus a little bit more on the fundamentals
and then building up towards like, why do we even use active inference in that sense?
And how do you get from active inference to a story that is relevant to an emotional,
emotional function, psychoretic context, and in the end also the social context, the embedding
in a cultural setting.
Let's see.
So this will be like an approximate outline.
You will see along the way how those different points fit into the story.
So I think many of you are familiar with the basis of the framework, but I think repetition
can always be a good thing.
So I just started with a very fundamental question, how we model biological systems or
living systems is basically answering the question, what do you need to be, to keep
being a thing?
What do you need to distinguish yourself from your environment?
How does a living system keep existing?
And the way Carl Friston has been developing this question and many of us kind of working
on that too is in this very abstract sense of statistical independence.
That's why I put a really boring picture here, because exactly that kind of abstractness
will allow us to extend this formulation to different skills, organization.
So the point here is that the fifth node is independent of the first node, exactly because
it's embedded, it's hidden behind a blanket of other nodes.
So there is a statistical independence between the fifth node and the first node in this
little network.
But then the problem, this looks very nice in theory, but the problem in practice for
biological systems is that these kind of boundaries tend to decay over time.
We know nature, the entropy tends to increase.
So these kind of boundaries tend to be worn down over time.
So biological systems need to somehow keep this boundary stable to keep being a thing.
So we need to counteract those kind of cables that inevitably is going to be arriving at
their system.
So just as a kind of intermezzo, I would like somebody who hasn't heard of the concept
of Markov blanket before this to give me an example of Markov blanket in nature.
So who hasn't heard of Markov blanket before this?
One, two, three.
So could one of you give me an example of any kind of Markov blanket in nature?
Yeah?
So a form of statistical independence between an inner world, in this case, you can think
of any level of organization, basically.
Yes, that's one example.
So in mitochondria, we know that they basically used to be unicellular organisms themselves
and integrate into a cell.
And the internal world of the mitochondrion is isolated from the external environment
that is created by the cell.
You can think of this on any level of organization.
You can also think of it in terms of social bubbles.
That is, I mean, five can be like Johnny and never talks to Mary, but that's number one,
but she talks to his friends.
So that's how she can influence his environment indirectly.
But she never has a direct impact on him.
Everything that every influence that comes to five has to travel through, in this case,
the friends of Johnny.
I mean, I'm saying that in such a silly way because you can apply it literally on every
skill of organization.
That's going to be the way we unite these levels.
And active influence then kind of focus on how you answer that question.
So when you have this kind of set of internal states, the sensory and active states as forming
the blanket with the world and the outer world being the kind of providing the sensory inputs
and the inner worlds generating the active states.
So the answer that we use is to stabilize it by limiting surprise.
And you can think of it in terms of anticipation.
So you have to limit somehow the chaos on your Markov blanket to keep it stable, to keep
it within a physiologically like possible or like feasible bounds.
Did you want to ask a question?
Yeah.
Yeah.
So that's a very good question.
And I mean, this framework is entirely statistically defined.
So it doesn't need the physical boundaries.
It's just that the statistical boundaries tend to correspond with physical boundaries.
So you can think of it, the cell being a very logical example where it's just physical necessity
that any influence from within the cell has to travel through the boundary of the cell.
So then you get the kind of overlap between the statistical and the physical.
But if you think about social Markov blankets, like children in a classroom, there's nothing
that keeps them from communicating.
But it's just their social expectations, that's a belief that keeps them apart, like
has different social bubbles that's in the same physical proximity.
So it really depends on which level of organization you're considering.
I would even say that the physical boundary is not that important when you go to the
level of culture.
But it does tend to play a role in bias.
And like I had an interesting conversation with David before about the fact that on student
campuses, the social networks tend to be like, if you look like a student flat, then students
are more likely to know people on their own floor.
So in that sense, they're kind of like also physically equals.
So both the physical and the physical, and then on the physical, on the physical, and
then usually both the physical and the physical, and then on the physical, and then on the physical,
and then on the physical, and then on the physical, and then on the physical, and then on the physical,
and then on the physical.
That's clear in the case of human body, where you have the skin.
And so I like to use just our like most relevant unit
of functionality being individual.
And I mean, it's very clear that all our effective responses
that we know of are focused on maintaining those boundaries,
like pain to your, like damage to your skin leads pain,
but like damage to your social circle
also leads to distress.
So like you have this effective responses,
all coordinated towards like maintaining your,
your existence or your reproduction.
And we're going to be focusing on basically
our generative models,
the ones that we are going to discuss
can implement that kind of bias.
So here I, well, it's a little bit more of the theory.
So the way an organism never knows
the external states of the world precisely,
but the only way, the only thing it can do
is estimating the external, like inferring the external states
to minimize the divergence between expected expectations
and observations.
And that's where the free energy thing comes into the story
because statistically when you define that limit,
that bounds on surprise, you get, yeah,
that is what the free energy means in this context.
So free energy is like a limit on the entropy
of the system, information theoretic entropy.
And you can think of it as a model complexity
plus a prediction error.
So it's, it's basically the statistical version
of our cancer laser that we're talking about
where you're kind of guaranteeing,
like we talked about overfitting before,
but that's the whole point here
that the whole framework incorporates overfitting.
Like it's correct for overfitting
because you punish model complexity.
So extra parameters will punish your model in the end.
And the point here is that,
so the question here is that it just answers the question of
if a system can maintain its boundary,
then it must be minimizing free energy
because otherwise the boundary will dissipate and be destroyed.
So that's like, it's a very principled answer
to the question of what do you need to exist?
And kind of tautological in the sense that to exist,
you need to minimize free energy.
Otherwise your boundary will, will disappear
and you won't be a thing anymore.
You will be, you can think of like a dropping ink
in a glass of water, the boundary of the drop,
ink drop will dissipate over time.
And as that happens, the ink drop will be indistinguishable
from the water in the end.
So the ink drop has stopped being a thing
because it didn't refresh its boundary anyway.
So that's the fundamental background.
And we're gonna be like going through that.
I mean, when you extend that to creatures,
like we have limited knowledge as I already implied.
So we're gonna be like going through the way
how active influence answers these different questions.
So the first step would be a sense of perception
and dissipation of the states of the world.
And then when you have that part,
you can also think about changing the world
to fit your expectations.
So it's kind of taking like taking the back door.
You're, you're instead of like,
because in the end you just want to limit that chaos
on your boundary, but you can also do that
by making your environment more stable
or like corresponding with your expectation.
So that's what the role that action plays here,
that you basically change the world
according to your expectations
instead of updating your beliefs.
Or like actually together, of course,
it's always a tension between those two.
You can think of being in the shower,
you can choose to adjust to the temperature
of the shower when it's too hot,
or you can choose to change the temperature
of your, of the shower itself.
Is that the question there, isn't it?
Oh, yeah.
Yeah.
Yeah.
Yeah, so that comes into the story
when you consider the shape of the generative model,
like the relative weighting of these two
will depend on the way you set up your generative model,
where if you make more predictions,
you can have larger possibility for error.
And if you add more parameters to your model,
you get larger complexity.
So you can like, I mean, you,
it depends basically on the way you set up your model,
how these two relate to each other,
which one has more power, let's say,
in the, in the updating of the model.
Yeah, that's what I mean.
Like that depends on the structure of the model,
the relative weight of those two elements.
But you will see in the end,
it's both updating, like improving,
like beliefs about the world,
but it can also be improving beliefs about actions,
improving beliefs about parameters,
and those come down to these two elements.
We will come back to that later.
I intentionally removed the maths here
because I think the maths are often
distracting people from the point.
So I like left out the equation,
especially since there tend to be some people
who get, who zone out whenever there's an equation
on the screen in a presentation.
So I tend to leave that out.
Anyway, so action.
So then the last thing would be learning
where there's inference over parameters,
over this model structure.
So that's like updating internal model
to improve your predictions.
Then let's see.
So, I mean, as you probably know,
every one of these inferences can be caused
in terms of base rule,
where you like try to find the most probable model
given your observations.
And then you can transform that
into this kind of equation where the first thing is,
we're gonna see that actually in the next slide.
So I'll just transition to that.
So, I mean, I would make this kind of daring claim
that nothing makes sense in statistics
except in the light of base rule,
in the sense that you can derive
frequent statistics from base rule
in terms of a uniform set of prior beliefs,
because then only the frequencies matter.
If you don't know anything about the world,
then the only thing you have is the frequency
of occurrence of events to handle, to deal with.
And, well, so this would be like an organism
inferring the state of the world, giving sensory data.
And that's the green part would be the organism's
own model of the world,
instantiated by the physical structure of the organism.
You could think of connection weights in a neural network,
something like that,
that generates sensations given states of the world.
And the other things are the prior,
yeah, you have the prior belief about the world.
And then there's this little problem here
is the prior beliefs about sensory data,
because you can never,
organisms can never determine these two independently,
because they only have sensory data,
but they cannot independently determine
the prior beliefs about the world
if they only have sensory data in the end.
So that's why we need to go towards variational approaches
in the sense that you cannot know these two independently.
So like the prior beliefs about sensory data
versus the prior beliefs about the world.
So you need to estimate.
And we'll get to that in a moment, yeah?
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Well, the point is, I guess I left out most of the theory
and the mathematics behind Bayesian statistics,
because I don't think that a lot of the terminology
really adds to the story.
So marginal priors would be, yeah.
Basically, you could think of the prior beliefs
about sensory data in practice.
It's a marginal prior,
because you need to sum over all possible states to get like.
Yeah, I mean.
Yeah.
Yeah.
So, I mean, that's basically hidden in the mathematics
where you can, like there are people in our groups,
who model hallucinations based on a broken generative model
where you assign too much value to incoming data.
So you jump to conclusions on various things.
And then once you do that,
once you jump to that conclusion,
it's very hard to depart from it.
So you're like, at such too much value
to initial conclusions.
And then you are close to the contradictory evidence
at some point.
But I mean, that comes back later.
I mean, not in a very mathematical way,
but I mean, we tend to first work out
like the functional element
and then see how you can break the model
to get various psychopathologies.
So you're starting a little bit at the wrong end of the equation
in the sense that we first work with
how does it tend to work
and then how can it break down into certain stable,
yeah, stable phenotypes,
like meta-adaptive phenotypes, I should say.
Okay, so we're gonna work with this kind of embeddedness
of perception, action.
And here I've added a number of layers
to give you a sense of why I'm even talking
through this fundamental story.
Because the way we're considering organisms
is considering all these loops of inference
within inference, within inference,
where you need to know,
you need to perceive the world
in order to consider actions.
You need to know what you're doing,
to know how you're doing,
like is this kind of fitting your,
yeah, is this fitting my preferences or not?
Is it going well?
And you can, again, contextualize that
in terms of beliefs about how,
yeah, like kind of bias in beliefs
about your affective and contextual states.
You'll see that coming back later.
I mean, now I'm just hand-waving
through that part of the story,
because exactly because I'm working on models
that implement all these different levels.
And the parts that are in gray here are,
I think, some of the hardest problems here,
where you get really the kind of gene culture,
co-evolution story,
where the levels of development and evolution interact.
And the cool thing is that on every one of these levels,
you can cast inference in terms of Bayesian model evidence,
where on the evolutionary level,
the model evidence will be the traditional fitness,
like the organisms that transfer to the next generation
or like reproduce themselves
would be basically providing evidence
for their own fits with their environment.
So it's literally survival of the fittest,
but then in a very different sense of the word,
the statistical sense.
The ones that fit their environments are able to reproduce,
so they're providing evidence for their model of the world.
And so that kind of picture,
I think I thought would be useful to have in mind
whenever you read one of these papers on active inference,
that's like the one by Maxwell,
where you kind of suddenly talk about
all these necessary levels of organization
and the mark of blankets at these different scales.
So this would be like the kind of one way
to cut up the system.
And I'm gonna show you how that can be modeled
in a relatively simple way, yeah?
Yeah, like subjective fitness, I call it.
Yeah, so...
Yeah, so that's kind of the trick here,
is that free energy is always defined
with respect to a generative model.
So while we started out as purely minimizing entropy over,
the bounds of our system,
you can also think of minimizing entropy
over the bounds of your descendants.
And so you're trying to maintain your line or your lineage,
the existence of your lineage,
and your system is biased towards maintaining
that kind of prolonged existence
that extends beyond your lifetime.
So it would be okay to sacrifice your own boundary
to extend the, let's say,
organismic boundary of your descendants
into the further into the future.
Yeah, it would be more common actually.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah, so...
Yeah, that's why I'm talking about the level of description,
where you're, I mean,
once you get to higher levels of organization,
you kind of start to be able to go
against the original imperatives,
because you're like,
when you sacrifice your own system
to kind of propagate the larger...
Yeah, you could think of it like this.
Yeah.
Yeah, you have some kind of group model.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
I think...
as soon as you go to...
as soon as you go to multiple levels.
Joshua?
Yeah.
Yeah.
So, I tend to be kind of careful
in terms of like the genetic focus,
because I think we're also,
we're biased towards maintaining our existence
in various forms.
Like we also,
when model evidence can also increase
when you see that other people
take over your view of the world.
So that would be a form in which
you reproduce cognitively,
but there's no genetic component to it.
It's like an environmental reproduction where you...
Yeah.
So what this formulation tries to do
is like trying to figure out
how can we have generative models
that are not only focused on the genetic part,
but also on some kind of cognitive reproduction.
And cognitive...
Yeah. Yeah. So it will provide a natural link
to cultural evolution.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
And so you can see every version of culture
that exists now as a kind of
self-evidence structure
in the sense that it still exists.
So apparently it's a stable mode of existence.
And that's this kind of circular story here,
the self-evidence,
where any kind of structure that still exists
apparently has the capacity to maintain its own...
Yeah.
Distinguish features from the environment.
But I guess...
Yeah.
Yeah.
Oh, okay. Yeah, it's okay.
Anyway, yeah, it's all for the discussion.
So I think this is good anyways to work through.
I'm just going to walk you through a little bit
of the kind of models that Ryan also discussed yesterday,
but then a little bit of a different notation
in terms of how you visualize it.
And just to start with a very fundamental thing,
like the first thing you start to predict about the world
or like...
It's just perceived the current state.
So you have some hidden states,
the blue circle, S,
and you translate it with some likelihood mapping,
the A thing translated to how it generates your observations.
And when you do inference,
you basically take your prior belief about hidden states,
the D, and you add your sensory evidence.
That's what you see on the right.
And then you transform it back to probability.
That's what the sigma function does.
And in a sense, this is not mathematics.
This is like symbols that just describe relationships
because we didn't define the shape of the A matrix.
We didn't define any part of the...
This is an extremely general formulation.
And to do actual mathematics,
you would have to specify the shape of the state space,
the shape of the outcome space.
And then this would get an actual mathematical implementation.
And this kind of generality allows you to apply
two very different types of problems,
whether you're working on the family level,
on the emotional level,
like even the lower level,
just the affective states,
more subconscious processing, that's it.
I mean, the logical extension that you get when you think of it
is that once you are able to perceive kind of the current states of the world,
what's the next step?
What's the next boundary of your existence that you need to do?
Well, imagine I'm pretty good at maintaining my current boundary,
but what if I see a sharp object coming my way,
or train, or whatever?
I derive benefits from knowing that this object is in the future
going to hit my system.
So it doesn't really matter to know the current state only.
I also need to know how to translate that state into the future.
And that's how you get to this kind of temporal formulation,
that's the B matrices to translate states of the world into the future.
And then you get a very different story on the level of the evidence on the right,
where now the past and the future start to influence your beliefs about a certain point in time.
So I'm saying past and the future in the sense that when I observe things,
they are the third time step that can constrain my beliefs about past states.
They can falsify certain hypotheses about the past that I used to have,
or they can confirm particular hypotheses very strongly in the sense that...
You can see this as different types of evidence entering into the story.
So now the past state provides the prior for the present state through the B matrix,
and then you get evidence from the observations that you're having.
And in this case, the future state becomes a type of observation
that constrains what the past state is because of the fact that...
So how you read these diagrams, these Bayesian graphs,
the direction in which the data is generated is following the arrows,
and the direction of inference is always going in the upstream, so it's going up the arrows.
So in this case, you infer hidden states from sensory outcome.
Just to give you a sense, when you read again some paper on active inference,
you understand the conceptual background of these figures,
that the arrows tend to indicate the direction of the generative model,
and then to actually do something with it, you need to invert it,
so you need to go up the arrows to reverse engineer states of the world.
I put here... I sneak some equations, or snuck some equations into the story,
but I try to keep it non-mathematical in terms of the explanation.
But oh yeah, one more trick.
If you ever want to try and understand an equation in active inference,
then these little dots between the matrices mean that you're doing inverse multiplication,
and that's like a really strange convention that we're using,
because in all other fields, it means a dot product means something very different,
but in this field, for some reason, I think Karl or Thomas, maybe?
Yeah, I think Karl.
Well, I thought it would be useful to use a dot product to explain that you're going up in the inverse direction of an arrow.
No, they are not dot products. They are like inverse matrix multiplication,
because the A matrix says how you translate from states to outcomes,
and then to inverse it, you need to like... that's a different matrix,
because you end up doing opposite multiplication.
Yeah, and Adam?
Yeah, so what I'm doing here, or what I wrote here,
sorry, it would be basically like the end result of gradient descent would end up at that belief.
So here we just write down the end result. If you write down the gradient, the free energy gradient,
and you let the system converge, it will converge to this equation, like this relationship.
But actually, we get to that at the point how you get there.
After, I will have a little slide on why this is even a variational approach.
Anyway, so the next step, so we're just building up the model, right?
I mean, we had M1 being perception, M2 being anticipation,
and M3 is the next logical step is when you can not only anticipate how the world is going to change,
but you can control how the world is going to change.
So you can select which B matrices you are going to employ.
So that's where action comes into the story.
And then we define an action sequence or policy as a sequence of these kind of B matrices.
So every action, possible action before policy becomes a possible future,
and you're trying to find the possible future that corresponds best with your phenotype, in a sense.
Yeah, I mean, if we go a little bit more into the mathematics, I don't know if there is time for that.
I thought, so the people who don't like mathematics, you can ignore the equations.
But for the people who are interested in why this is even a variational story,
I'm going to try and explain that in words.
So what you do here is basically free energy is defined as the difference,
the expected difference between your posterior and your prior beliefs.
So you try to minimize the amount of updates that you need to arrive at your posterior beliefs when you get the new data.
So that's minimizing like putting a balance on the surprise of the data that you have with respect to the data.
So you find this posterior beliefs by minimizing F.
So QS is the posterior belief about hidden states, which you obtain by, I mean, we define them in terms of minimizing F, minimizing free energy.
So that would be written like that in mathematical terms.
But then we have a problem because our expectations depend on our posterior beliefs.
So how can we optimize F to arrive at our posterior beliefs if F depends on our posterior beliefs, right?
So we can only, that's why it's a circular variational story.
You're minimizing a quantity that depends on your beliefs to arrive at your beliefs.
So this circular nature of this story is exactly because F depends on the thing you're optimizing, namely your current beliefs about the world.
I mean, so that would be the answer to this last question.
We work our way towards accurate beliefs through a circular form of inference where we update our beliefs, then the free energy will change, then we update our beliefs again.
And free energy just defines like the kind of gradients locally where to go, but it changes as our beliefs change.
So it's like a landscape, you can see it as a landscape and the landscape changes form as you change your location landscape.
So in that sense, a very flexible function, actually we call it a functional.
A functional is in mathematics of functional functions.
So it's like a circular thing because in the end it's a function of something that is used to arrive at beliefs and it's a function of those beliefs.
So that's the circularity that active inference kind of embraces.
We fully embrace the circularity of biological existence to arrive at this kind of formulation.
Anyway, so that was the mathematical interludes you can just for people who couldn't follow it, you can now zone to back in because we can we are going to go more conceptual again.
So I have a question for you.
And I would like somebody who doesn't know this answer to this question already didn't like doesn't know it already, but just is hearing about it for the first time today to try and answer this question based on what I've told up to now what actions would be most optimal.
It's like, um, who has for who is this part of story new. Let's put up some hands. Let's see who heard this part of story before or like who didn't hear before.
Yes, we had a few persons. Yeah. Okay.
And that's cool. So, could one of you make a guess at through like what would define an optimal action in this framework.
Not exactly something, something that direction.
Yeah, it's something more fundamental than I would say base just based on the story we have had. Yeah.
Anybody else would like to give us right.
And just based on the story I've been given.
Yes, that's that's the trick. But it's a little bit more complicated because now we once we have our actions are possible models of the future.
So we need to minimize expected free energy. So that would become even more complicated because you remember free energy was based on an expectation.
So we're going to make an expectation about how this expectation will change. So we're like orders of expectations to get into the, like to minimize expected free energy basically your will get.
If you write that down you take the free energy and you take the expectation value with respect to the future where you get a very interesting result because you can divide it into a utility focus term.
So you expect this phenotypic risk that I label it like that for and also an expected perceptual ambiguity.
So kind of epistemic component.
So by with this very principle way you get the utility, the reinforcement learning kind of utility, but also a kind of epistemic component that could that can power exploration exploratory behaviors where you try to minimize ambiguity about parameters about states.
Yeah.
Yeah, so in this case,
in the generative model in second, but in free energy there in the free energy paradigm or active influence paradigm.
There is an amount of uncertainty about which action you actually select. Because if you remember the mark of blankets and necessitate that your active states are influences that travel out of your system.
And you don't have perfect certainty about what's happening out of your but outside of your system. So you need to gather perceptual evidence to kind of like proof or like find evidence that's what action you're performing.
And in reinforcement learning they kind of focus on level of the agents and assume the agent has perfect knowledge about which action is selected.
So that's like, there's no uncertainty about which action is selected.
Yeah, so the generative process. Yeah, so you have a distinction here very important is between the generative model of the world and the generative process, which would be like the set of interactions that actually generates the outcomes.
Yeah, so to do this story and you need a model of yourself acting in the world. I mean that's basically the thing.
And that's also what reinforcement learning would be doing trying to make a model of that. And where I think active inference goes one step further is also incorporating the uncertainty in the model about your own actions.
Where as in a reinforcement learning you just adds like you get you have an action then you have an outcome and you add like a kind of you reinforce the action that generated the outcome if it's a good one and you you like subtract value from it when it gave a bad outcome.
So don't know for sure which action you actually selected there's uncertainty about it then hard to update that kind of in that kind of way.
And the next step is also, I mean, it's basically you could say that in the reinforcement learning people are kind of hiding the generative model a little bit in the utility function.
And we're just, you could see this as unpacking what kind of generative model you could stack into the utility function and where the utility function is now the expected free energy.
And which includes all those kinds of uncertainties that we're talking about.
We'll get to that, we'll get to that.
And one more thing is that.
See, actually, I lost my point that we'll get through it. I mean, in the moment I will show you one another one of those pictures so remember what I thought to do.
I was talking about embedded levels of inference. And now we get we see that unpacking itself because we see now that you have the perception taking place in the lower level.
But then you have your on a higher level you have an action component where you're inferring your phenotype congruent policies from perceived hidden states.
So your, it's a kind of self fulfilling prophecy, you have expectations about which kind of actions in this kind of context are kind of correspond with your phenotype.
And then those that prior belief will bias the way you integrate perceptual evidence.
And that will bias the way you, you're the rest of your system will behave. So it's like a kind of so fulfilling every action is in the sense of kind of self in proxy.
The way you can read this figures is kind of like starting on the laptop, top left, where you kind of read what all the priors are.
I can't even read it from here but I can look at the screen.
So, if you all were looking at like listening to Ryan's presentation of yesterday you pack certain priors into this beliefs about policies, the pie, which modulates what can be matrices you're selecting.
Or you're, you're using as a possible model.
And what you're doing is basically is is biasing that belief about policies in terms of the expected free energy, the G.
And then the G contains one more trick.
Because you can now when you start to talk about expected free energy, you have the opportunity to pack one more prior into your thing because
the lower level is about perceiving the states of the world.
But when we think about an action model, we get the opportunity to now think about what is, what is the preferred state of the world.
What is the kind of outcomes that we would prefer to observe in the world so we can, we get the kind of tension between the world as it, as we observe it in the world as we would, as we would like it to be.
So we, our system would as will be congruent with our phenotype.
And then of course you have so you have the G kind of putting the that kind of bias into the system.
And then, once you're all the way down to the level sensory outcomes, you can go back up the hierarchy.
Now the posteriors will travel.
They are kind of the model evidence that travels them back up the hierarchy.
So sensory outcomes provide evidence for hidden states.
The hidden states provide evidence for, for the policies.
So the perceptual evidence will be kind of could counteract the kinds of action model that you would like to implement.
And we're going to use that as a trick, because now that you have this kind of action model, that's like your internal, your phenotype, that's this kind of characterizes your phenotype, you can think of the friction between that model and the world, yeah, the actual outcomes that you get.
So that's when you get the kind of implicit matter cognition where you start kind of, where you start estimating the fits of your model, your internal model, your expected energy to the world as it actually ends up unfolding.
So I would, I like to call this subjective fitness where this they have the gamma here it says subscribes kind of the amount of confidence I have in my action model.
When I make inferences about the world.
So, if you look at the little equation, I don't know if you can read some distance, but the prior on policies is like this kind of baseline prior the E minus the precision times my actual model.
So, how much, how much confidence do I have in my action model depends on how, how, how well it fits with my environment.
So this gamma depend ends up being a kind of subjective fitness estimate where it estimates how well I fit with my environment.
Yeah, sorry.
Yeah, so you can think of each of these levels as doing free energy minimization.
There are not one formula but they are like instance they're embedded in each other because like your minimize free energy in perception, and then you use those conclusions to minimize free energy in action.
And use those conclusions to minimize free energy in this kind of implicit medical. So it's really an embedded set of inferences.
And the level is again free energy minimization. That's the trick, like energy kind of provides a common currency for these levels.
Let's see. So we're going to move on to like a kind of implementation of this because I did promise you that you could use this to model your fictional environment of your choice, even though I'm not going through the scripts themselves but
I'm going to show you a simple possible test that we pretend to use as an illustration, but you can use it's an operational tasks so you can use it to model many different kinds of behavior.
This is the T-Maze. So T-Maze is a traditional kind of thing in biology where you have food on the left or on the right and a shock on the opposite side.
But because there are one way doors, the red needs to figure out which door has to take.
And after a while, I mean, it learns that there is a Q on the bottom part of the T that tells as predictive value over where the food is.
In this case, you kind of can separate it into an epistemic component where the red checks the Q to minimize ambiguity about the world.
And the other component is pragmatic because then it knows how the world is, so now it can exploit that to generate the outcome that is preferred, namely the food.
This is a standard solution.
But you can play around with that because the red can also accumulate knowledge about where the food is.
So we can put the food every time in the same location and see if the red can learn the context that maybe the food is always on the left, so it doesn't even need to check the stimulus.
And when you do that, when you implement that, yeah, we're gonna, I mean, you can work through the different matrices that you can define.
So that's the same guarantee model on the top.
Don't get scared from the equation.
You can just pretend they aren't there.
So here you see like the first kind of priors you define the initial state priors, the D matrix.
So that would be kind of describing your prior beliefs about how the world is.
And the thing on the right will be preference over outcomes.
So that's kind of how you would like the kind of outcomes you would like to generate the world to give you.
So the rest of the story will be a kind of tension between these two because of the fact that your preferred outcomes are not the same as the actual world.
So the way you get from the states of the world to the outcomes you observe is defined as A matrix.
Here you see every kind of translation from whether the food is on the left or the right.
It depends.
So if, yeah, then you have like four different locations for the rats and the outcomes it will get will depend on the context is the food on the left or on the right.
I mean that kind of this kind of thing you can do for any kind of task you can think of where there are conditionalities and the fact that the rats is able to perform this task.
Accurately means that somehow it internalized not consciously most cases.
Like in humans you could think of some kind of logical, consciously accessible form of problem solving.
But now we're just thinking about the kind of emergent form of the generative mold that this rat apparently is able to execute.
Because it does learn to go first to the queue and then to the food.
Somehow it has a sense of how these contingencies in the environment depend on its location.
And on the context, yeah.
Yeah, we're coming there.
Like this is the perception component, the A matrix.
Like it's just prescribes your perception model.
But if you go further, then you get like the B matrix that specifies how states change over time, conditioned on your action.
So the rat has only four possible actions.
It can go to the center, to the left, to the right, or down.
That's the possible kind of transitions that the rats can make.
So again, like the columns are like the current states and the rows are the future states.
And the last part is like a sequence of these B matrixes.
You can combine these different actions in terms of possibilities.
And that's where the structure of the environment gets in, because you get, for example, and wait, I can also point at the screen.
If you look here, you see that if the rat, so if the rat in the first step stays in the center, then this second step, it can go all four locations.
But if it goes left in the first step, it's stuck there for the rest of the trial, because as I said, there are one way doors.
So you can think of this structure of the policies, kind of describing the structure of the environment, because you can, yeah, it's like, you can get stuck in a corner.
So if you select a certain policy, your limits, or like you select a certain sequence, certain B matrix in the first step, then you're stuck for the rest of the trial.
And this would be also the kind of thing that the organism will accumulate over as it learns as it goes through trials.
Let's see.
Let's see.
So now, I mean, that's a very like kind of mathematical story.
In a sense, but I'm now going to work a little bit towards how you can use it to conceptualize talents.
And later on, you'll see the figures coming back to it when we implement the teammates.
So that's gonna come back.
So here we characterize feelings as approach versus avoidance, but in a very like abstract sense of that term, like, you want certain scenarios, certain things, certain observations to to reoccur because they're like
associated with increases in subjective fitness in this kind of action model precision.
And so other things you want to remove from your possible worlds of the future. So those those will be negative things.
And one way to remove that is to approach. So that's the kind of nuances you need is that you can attack us and an opponent to remove them from your future.
It doesn't mean necessarily physical approach or avoid avoidance, but let's say statistical approach and avoidance you're trying to increase certain events, their likelihood and decrease other events in terms of their likelihood.
And the way we packet often pack those prior preferences over this events.
In that story is through like evolutionary priors.
But then when you go to want to have a more general. So that will be like kind of the utility function.
But if you want a more general story, you get the question, how evolution can prepare us for new things and details.
Because I mean, we like to play with our smartphones, but the observational outcome like the sensory outcomes that our smartphones generate are pretty unique to like the past 20 to 30 years, maybe like when we started to develop something like a phone.
And that's, it's pretty recent so there are nothing about the particular sensory consequences that we like.
It's about inferences that we make based on those sensory consequences about our social environments, but then travel through like a higher level of your system.
We can play with that by integrating the architecture that I explained to you before into another level.
Or now we use the lower level subjective fitness to infer effective states like where there were more in it, where there were fuel that we're in a good, we have a good fit or a bad fit with our environment.
Let's see. So as I said, like this gamma, the precision term ends up being something like a subjective fitness where it says the precision of your phenotypic action model and how congruent it is with your world.
And the interesting thing is that this precision term has been consistently linked to dopamine discharges in the stratum.
So we have kind of characterized dopamine discharge in this context as tracking a kind of parameterizing the precision for the system.
So that would make it kind of evolutionary stable to have like a neurotransmitter that communicates this precision to different parts of the brain.
And what I label here as effective charge would be like the changes in that updates in that precision.
So effective charge would be positive if you're like action model, if your subjective fitness goes up and would be negative if your subjective fitness is going down, which is logical because your whole system has to be focusing on your fitness in the end.
It's estimating that fitness and trying to bias itself towards situations where it goes up.
What you see here is like the same trial with the teammates, but then putting the food every time on the left.
So as you see on the bottom over time, it accumulates this knowledge that the food is on the left.
So as that happens, at some point it has the courage to go directly to the food to skip the epistemic forging and go directly for the reward.
You see that the fluctuations in the dopamine discharges, let's say, as you see them simulated here within the trials, these fluctuations then their positive peak gets, gets higher.
And in the sense that the the organism now our synthetic red has more certainty about being able to realize phenotype congruent outcomes.
This is like the story that you could simulate based on active inference as is included in the MDP scripts that Ryan show just today.
But we're going to go one step further, and that is not yet integrated in the current MDP scripts.
But yeah, I mean as soon as this paper that I am working on this out then we will update SPM 12 that you can also do that kind of stuff but
we have another layer on top of this where now you can use this effective charge.
Let's say this charges the dopamine discharges as an internal evidence internal signal to estimate whether your action models doing pretty well, or, yeah, you're, you're going through tough time.
And then you can use that to come anticipate the precision and modulate it in an anticipatory way.
You can have a model about that characterizes how your subjective fitness evolves over time.
And to illustrate kind of a little bit how this subjective fitness or like action model precision also goes towards internal in the in our internal world.
There's little sample, how you can model humor in this context.
So how do you call somebody without a body and without a nose.
I hope that somebody would hadn't hasn't heard this this question before give it a try.
Anybody.
And nobody knows.
Yeah, so that's like the little cognitive trick.
If you think about it, if you have a hierarchy of representations and suddenly you have like conflicting representations between being without the body and without the nose and you can't find some concept to unify them.
But then suddenly you get an answer that is both a true is a truthful answer to the question and also unifies these concepts into one thing.
So it corresponds again with this kind of reduction in free energy.
Where the complexity and the prediction arrow went down in this kind of setting.
So effective charge with kind of track this reductions in in free energy.
And that's how we're going to link, make a kind of domain general formulation of talents in the rest of the story.
So we have another figure I hope you're not getting tired of all the diagrams but I use the color so you can keep track of how the hierarchy is organized so we have the same colors for observation white, light blue, then blue, dark blue for action, the kind of grace for the precision.
And on top of that we have an effective state space that characterizes the parameters of the lower levels.
So you would get something that's called like deep parametric active inference. Yeah.
Yeah, it's like a subjective confidence in your actual and when you try to infer things about the world.
Well, yeah, how well it's going to fit your world.
It's like how much confidence can you assign to your typical action model when you look at the world.
And that's, I mean that's one parameter but the nice trick here is that you could assign precision terms to different parts of this model.
It's just that we kind of in early stages just focusing on the expected free energy but you could also talk about the precision on your preferences.
And that's how you could modulate exploitation versus exploration. If you increase the precision on C in a top down way then you kind of increase the priority of your needs.
So when you're hungry, then it's not like you're going to play around to kind of explore the state space just for fun, you're going to prioritize getting your food.
And that's why it can also be adaptive to selectively upregulate parts of this mobile top down. Yeah.
And that's my point like you use that kind of gamma does exactly that you skills to make them then in the Bayesian way inferential way, so you can.
Yeah.
Yeah, so basically do that in a Bayesian way where instead of like scaling it yourself by hand when you program you can like have the organism infer how to scale that component model based on.
Like you could have like an hungry hunger state that kind of regulates your preference for food when you're hungry so you can prioritize your food intake based on your eye level needs effective states.
Oh, is it as he or not. Okay, I thought there was a question.
Okay, so here we can do the same trick where you define priors and you define. So every time it's like a story where you.
We always say I mean it's kind of like a joke but also it's true that when you define generative model you're done technically, because then the rest is just like figuring out the implications of the generative model so that's like, this is one hypothesis about the
system where the inferential structure of the system, and then it's like defining different components in terms of the, the quantities that they took the priors for being happy yourself, versus the priors of where the food is left to right.
These are contextual priors and these D with a superscript C will be contextual priors and with superscript a will be the effective priors.
Then you have, yeah that's effective prior relates to observations. Now, the funny thing is now, if you remember what I said about lower levels providing evidence for higher levels.
We define priors for the lower levels that kind of circle again comes back, because now, after the now the evidence for the higher level the orange part is gathered from the precision, the lower level.
We just extend the circularity the embeddedness of the system one one level up to get the active inference, or deep parametric active like inference over hyper parameters hyper priors of lower levels.
So I'm not sure it makes sense to work through all these things but I mean you can see that this is close to an identity.
Effective inference tends to be quite like riddled with uncertainties so you can map incorporate those kinds of uncertainties in this. Yeah.
Yeah.
So, in principle, mathematically it can be infinite. In fact, there are models where you have infinite, like infinitely hierarchical and the piece.
You can show that the parameters open infinite from an infinitely infinite hierarchy of MPs can mobile with a finite number of parameters.
So, that's like exist mathematically but in practice.
The idea is that you can like depends on the energetic constraints of the system. How much like some point doesn't help to have additional model complexity when you just the model doesn't need to be right or correct it just needs to be good enough to survive.
So, this will be the only constraint like your energetic like metabolic constraints on your system where you.
Yeah, like complexity and accuracy, right, so if you don't gain.
Yeah, so.
So, in this kind of model.
Yeah.
Like the and the E thing where it's like.
And you can see it as a hypothesis. Let's say the structure and we just construct minimal model for what you would need what kind of inference you would need to do to do the kinds of things that biological systems like like rats do.
Yeah.
Yeah.
It's like when you're.
But in that case, you're talking about the case where the G and he basically have the same structure.
But so you would need to feel good or confident to depart from E to have the, the expected energy kind of overshadowed the structure.
So like to depart from your habits, you need to do good or confident.
Otherwise you like, otherwise G won't increasing precision G will make a difference.
If you're, if it's already aligned with your habits.
Right, so you, it's kind of course is also has an intuitive sense that you need to feel good or confident to depart from your habitual motive behavior.
Like if your action model is contradicting your habits.
Like if you have no confidence in it, then you're, you're going to follow your.
Like what I'm saying is only matters when there's a difference between these two, which one you're prioritizing.
In a sense, like cycling.
Yeah, I think that's kind of happens with like cycling behaviors where I think you do have a kind of automatic mode of action for cycling.
If you feel bad, you will also just do that.
But like to, to depart to kind of try a new, I know, try a new methods of cycling, whatever it like you would probably have to feel in it.
You know, like,
I think this is kind of an important point that these models are entirely statistical.
So even though we put certain verbal labels on it, interpretations, it's very flexible in terms of how it in the end ends up being so in the end, we care more about the phenomenology of behavior and the internal and external phenomenology
than the specific interpretations that we give to certain, like, in the sense that the generative model is just constrain everything.
So the way what we think a habit is in the end doesn't really matter that much as long as you.
Yeah, like, because in the end, you would have a con confluence of habitual and action model driven kind of like the E and G driven behavior.
We don't need to like have a perfect label for everything but it's often maps quite nicely on intuitive categories so that's kind of reassuring.
Just one more thing I want to say so.
If you look up you see this be matrix that's kind of like the real trick here or the cool thing is that now you can transfer knowledge from present trial to the future trial and you can incorporate uncertainties about how the precision your action model precision is going to vary over time,
and about how the context is going to vary over time. So you can, you can automatically get things like recency effects where
if there's a little bit uncertainty in which state is going to be next, then more recent states are going to be more informative than more like that distance states that more distance in the past.
So you're automatically going to weigh recent events more heavily in your memory than more distance events.
So if you in this way you can also kind of work towards a genetic model of memory, where you can explain why do we like how organisms can weigh recent events more heavily.
And the similar kind of thing is like, yeah, so these kind of matrices would be something that you would have to learn over time.
And by playing around with this matrices you could get all kinds of interesting behavior like dysfunctional behavior in the sense like imagine you're putting too much certainty on your effective states.
So you would end up kind of getting stuck in an effective state over time so your effective state could be too stable and not responsive to new situations.
So in a negative state, pessimistic about the world, when you're too top down in this kind of thing. Yeah.
Yeah.
Yeah, that's also why I mean, serotonin is a bit of a difficult case because it has functions also in the justice system and it's, I think, yeah, but I mean that would be an interesting thing to kind of sort out empirically to see
Yeah.
Yeah.
Yeah.
Yeah.
Okay.
The idea is that this kind of computational like Bayesian fundamental architecture can be used to sort out all this kind of questions because you have like the kind of parameters that minimally would be necessary to perform this kind of inferences that extend across situations.
Different kinds of, because, as I said, I mean, one of the purposes is to develop domain general generative models where the lower level the blue part you could just plug in any kind of action or like any kind of task doesn't matter like.
You could have, you could really test explanations across domains, if with this form of mobile because it doesn't matter what you plug in in the blue part.
You can have, you can just test whether the same set of hyper parameters can like explain a large range of different actions and tasks.
And then you can like start sorting out this kind of difficult questions about serotonin, for example.
I think, and one more thing I guess I didn't explain where the beta comes from so for anybody who's confused by the beta don't don't worry about it is just like the parameter that's described the way the gamma is this like this distribution that you expect for the precision.
It's, it's, if you look at it mathematically it's like the inverse of gamma, like the expectation value of the precision is like the inverse of beta.
It has a historical thing that's not like it was originally defined in that way.
It's like some of the problems of working in conventions.
So, I think we treat everything here. And I just want to, I think I don't, how much time.
Okay.
So I just want to show you a little bit of how, yeah, then you can use this to simulate emotional responses in different kinds of thoughts.
So here's like inferring your own your emotional state to prepare your system for, yeah, like to prepare the parameters of your system for certain kinds of, yeah, certain settings certain contexts.
And if you apply the model that we've developed to like this teammate setting where now for 32 trials you put the food on the left and then for the next 32 you put it on the right.
So you change the context in the middle context reversal.
What you get that is this very intuitively logical dynamic where the organism, the, in this case, yeah kind of synthetic red transitions from a kind of anxious state where it still has to check the stimulus and kind of still there's a little bit of
uncertainty in may still get the shock so it has like an anxious state that starts with until develops a certain level of confidence, like the, the valence believes there, the positive blue means like high certainty, yellow low certainty.
And then you can at some point getters enough, like confidence in his action model to, to just go to the food directly to just ignore like not even check the stimulus it just trust that the food is going to be where it has been for the past number of trials.
And at some point that trust is going to be betrayed that's when the context is reversed and that's when you see that it drops back into an anxious kind of state where precision on this action model drops, and it goes back to this kind of information foraging method.
So it's less in certain about action model so it starts to go back to gathering, gathering information about the world before it's like gets goes left to right.
And then if you go back, you have to learn, you have to have a great body, then at some point you can go to one of these, you see like a circuit there, and you can literally go to get to see which are you favor, and then just rely on the ones that are always there.
But sometimes that will why, why keep going like that.
And the next thing I can also show like, you can still think like why do we even need this high level like what I like you like a young us.
Well, yeah, is it really necessary to have the higher level in the sense or how do you test where it's necessary.
And you can kind of also in this simulation can also just lesion the higher level and see what's going to do so I did something like that, where you have like the orange is the affected agents, and the gray is the lesion one.
And what you get is, well, on the top you see the beliefs about where the foods left to right and the gray the lesion agent is just accumulating from like counts so it's just like counting the number of events like a frequentist approach kind of.
So it gathers 32 counts for the left side, and then 32 counts for the right side and ends up being 5050 after the 64 trials.
Well, our effective agent that has also contextual states can like realize that oh I'm in a different context. So it's just like changes is believed about which context is in.
It's possible because there's uncertainty in the B matrix that allows it to like entertain the hypothesis that the context changes.
Yeah.
You know, it's all simulations. So I just lesion.
No.
No, that's what biologists have done for a long time. That's the kind of benefit of this paradigm is that whatever you think of biologists have done it with like hundreds of rats.
So it's like, we have the data already.
Yeah, so if.
Oh, no, I mean we have we have not used their, their data for this. This is all simulated data.
Yeah, this all simulated data and this, even these responses that look like dopamine discharges that they have measured they, they are not pitted to the like, these are these are emergent results from the formalism and how the precision varies within trials.
Then you get something that looks very strongly like the type of dopamine discharges they observe.
Exactly.
Yeah, yeah, so it's like, yeah.
Yeah, they're all virtual rest, but it's a good point that we are just, it's kind of like an empirical question you have the parameters and then you're kind of thinking, okay, well, to do this inference reliably, there may be some way to code it in your in the neural system also in a reliable way,
because apparently it's important for the inferential system to track this parameter, for example the precision.
Yeah.
Yeah.
Almost everything in this model is support to call. Like almost everything I've written here is support for actually, there doesn't need any country.
Yeah.
So the point is that recently more recently they found that the kind of even the most fundamental dopaminergic discharges tend to also track something like information gain.
So like, they used to think that it there was like a pure pure utility thing. And then, but now it turns out to be really hard to separate reward in terms of information gain from rewards in terms of like
Yeah, I totally agree.
And that's also the thing that this model allows you to do like just plug in another experimental paradigm on the lower level and see how well it does there. Yeah.
Yeah.
Yeah.
Yeah, so, um, yeah, the nice thing here is that the structure doesn't commit to a particular neuro anatomical like interpretation it's like, it does guides.
It's a hypothesis hypothesis about neuro anatomical structure. But yeah, yeah, yeah.
Definitely. So, yeah, so what, yeah, actually, it would be nice to kind of use that as a segue to continue a little bit because I don't have that much time left.
Yeah.
That's all one more question. Yeah, okay.
So we can use that as a, those are like empirical questions to sort out.
So what you see in on the top is just the fact that, okay, if you can't entertain multiple hypothesis about context and you're gonna, you're kind of like a, you can't forget events or you can't do recency evaluation so you can't, you're just accumulating counts and you end up being very
Before the context reversal is doing a similar thing, but then when the context changes this agent just kind of collapses and cannot really adjust its belief towards the new context only given like a lot of events when it changes that.
And then what you see in terms of the prior beliefs about action is that our effective agent has the possibility to modulate expected precision.
So, in that case, it's like the gamma precision times the action model, which then determines what is the strongest prior belief about policies.
Well, in the other agents, you also see variation in it, but it's much smaller because it's only the only variation comes from the action model itself and not from the precision that modulates the impact of that action.
So you see that. Yeah, I can use my mouse again so this orange line is the effective agent that just has a very strong peak here and then drops and then goes like that.
Well, the gray line has a much slower dynamic because it just only changes based on like the beliefs in the action model that slowly update gets updated in the kind of without this kind of higher level generative model about how beliefs change all the time.
And what's what gets interesting is when you look at the updating of the expected precision in the effective charge kind of story.
You see that the high like the large peaks for the effective agents mostly happen during the transitions in effective states so effective charge ends up tracking transitions between states more than the states themselves.
Well, in this other agent, you see that it keeps being because it can't adjust the expected precision keeps being surprised and surprised and surprised and again like changing the expected precision within the trials but then not storing it for the next moment because
doesn't have a way to translate that knowledge to the next like that inference to the next step.
And then when you put these things together like an inflexible precision or like an inflexible beliefs about context then you get you see the difference really strongly here is that the behavioral behavior is kind of similar is well actually identical before the trial before the reversal.
But then, while the effective agent just transitions to the information foraging strategy for a while and then goes back to exploiting the direct thing.
You see that our other agents gets first he starts avoiding everything so what he does he checks the stimulus but he doesn't go left to right.
He just so unsure about his environment it doesn't even try because it has a very strong belief that the food is on the left, but it gets secured at the food is on the right.
So this ends up not going there because it's not sure enough about where the food is so it just stays. It's just so confused it doesn't even go there until it has observed often enough that the food is on the right that it kind of has the courage with the use of the queue to at least when it's
has seen the queue to go and get the food.
But in that like it doesn't transition back to the direct foraging the direct kind of exploitation because, as you can see it only only at the end it gets back goes back to 5050 distribution between left and right.
So it will take another 64 trials or 32 trials maybe to get through a more confident structure.
Let me see.
You have another question. Yeah.
Yeah.
Yeah.
Yeah.
Yeah, so.
But what we see in practice is that reds are much more flexible than just blindly accumulating events they do tend to weigh more recent events more heavily.
And while this the red without the context layer just blindly just every have done it every observed has the same weight in this story for the context, the red without the higher level context.
So then, yeah, that's, well, I agree with you that this probably is observed in some, I mean, surely is observed in some animals when you change a very like reliable context that used to be reliable to somebody changes them.
I guess you could kind of see it as like a maybe also a response you often see in autistic individuals that have very high precision like expectations about their environment and then when that reliability collapses, then go into like avoidance reaction and
effectively, yeah, extreme reaction. And that's also what you see here.
That's when you're not able to integrate that evidence over time you kind of keep getting effective response again and again and again, because you're at trouble integrating that.
So these would be interesting ways in which the model could break down to model different kinds of dysfunctions and disorders, when you know it's able to integrate, or maybe even too much integration, like overweighting the evidence.
So you get stuck in certain states.
Yeah.
Okay.
Okay. Yeah, I'm actually kind of at the end so I will just.
So, I mean, this is like kind of summary of the model in terms and the arrow show like Campbell day was explaining the priors that travel downwards, and then you start on the top left and travel downwards and then when you get the evidence.
Again, through the nesting all the way back up to update every level, and it travels all the way back up to the phenotype that then provides evidence on that kind of larger time scale.
So we have handled failings but I mean working on formulations to deal with different part dimensions of the effective space where you think about arousal like estimating energy costs, but power like controllability.
How much control do I have over the situation or how much control does the other agent have over the situation can think about guilt and anger, those kind of things coming into play when you can make those kind of inferences.
And this is like the last slide almost that we're working towards this account of shared attack. So I haven't been able to go into that a lot, but it touches upon, yeah, the work of young I think also with the mirror neural story.
Where you have multiple systems kind of aligning as they interact.
And basically then effective if effective inference will be estimating the higher level parameters other people systems to understand their behavior to interpret their behaviors over time.
Too bad I don't have more time to discuss, but this will be like the future where we use this agents to construct multi agent systems, and then you can think about shared affect and shared belief dynamics.
Well, thank you.
And yeah, we have to go.
