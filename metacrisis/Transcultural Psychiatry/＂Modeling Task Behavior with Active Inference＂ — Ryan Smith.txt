All right, well, you know, you guys are stuck with me again for for better or worse.
So hopefully you guys will find this helpful.
The way I've tried to structure it is kind of like a mixed talk where I'm going to both
walk you through some paradigms and some empirical results from those paradigms and then that
are based on modeling and then I'll walk you through putting together the actual models
for those tasks.
You first get to see why it's useful and what the results that can come out of it look like
and then hopefully you'll be even more motivated to know how to do it.
So so I sent out a few matlab example matlab scripts.
So they are they were definitely in an email that everyone should have gotten.
If you haven't downloaded them then I'm sorry if there's a way you can still get access
to them via the internet or whatever then I hope you can otherwise I'll show everything
up on up on here I have matlab I have the scripts open.
If you do have the scripts open right now don't look at them yet I specifically don't
want you to have gone through them before because I mean you'll have the answers to some things
ahead of time or I want you to try to come up with them on the fly as we build stuff.
So okay.
So general outline like I said so I'll give you some task descriptions and some preliminary
empirical results from fitting models to behavior from people that have done these
tasks and a bunch of different and healthy and a bunch of different clinical populations
and there's there's three of them there's this approach avoidance conflict task which
is the one I'll probably spend most of the time kind of building the model in detail
and hope is that that'll give you kind of the thorough sense and then I'll walk you
through the next task then we can kind of go through the model or briefly and hopefully
it'll just and even if we don't walk through it in as much detail for the latter ones I'm
hoping that'll sort of enforce some kind of generalization and so you can see how it's
not just how the kind of general abstract structure of these is actually very similar
across specific models.
And and so actually the way I've kind of depicted this is a little bit wrong for each of these
I'm going to give you somewhat with different levels of detail kind of hands on tutorial
where we'll build the models we'll simulate some outputs I'll make sure you guys have some
sense how to interpret the simulation plots which are honestly not all that straightforward
always in papers by Carl and Co and and and then I'll actually even show you a little
bit about how to estimate parameters so to actually get individual difference parameter
for each person who's done the task and so at least you know I'm not I'm not saying you
walk away with this being able to do absolutely everything without any further help but it
should give you a good sense of the process and give you kind of the resources to you
know practice and move from here and I'm happy to answer any questions or go through further
things after at some other point during the week so oh and then and then additionally
it occurred to me in the end that not most of this is much more inference based as opposed
to learning based and active learning is certainly a really important part of active
inference especially when you're doing learning tasks and so there's a little bit of that
and free arm bandit that I'll go over with you in a couple of slides mainly to give you guys a
sense of how you would do something like reward learning in active inference and how you kind
of think about it it gets cast a little bit differently and then I might also walk you through
some of additional kind of simulation paper that we have a free print out on right now on
how to use active inference for structure learning which is more or less just an extension of the
way you do active learning but I think it's a little bit more illustrative of how of how learning
parameters actually works in these models so we'll see what we get there depends on how long
some of this takes so okay so first the approach avoidance conflict task so so what's approach
avoidance conflict basically this is something that happens that's a big problem in psychopathology
especially in emotional disorders is that a lot of times you have to choose to approach a situation
when both some bad stuff and some good stuff will happen at the same time or you can avoid
the bad stuff and go do something safe but that means you actually sacrifice a lot of the good
stuff you would have gotten as well and so oftentimes what happens in the context of anxiety
and depression is that you'll kind of actually end up sacrificing a lot of good things a lot of
sort of well-being promoting activities in life due to a kind of fear of the inability to handle
some of the possible negative outcomes that you get as well so the and this is a primary target
of a lot of interventions both cognitive behavioral and other basically most psychotherapy
interventions in one way or another target this and so the and this this applies for instance
in depression and anxiety you know it can be things like avoiding social activities because
of fear of judgment or embarrassment in substance use disorders it could be like drug taking to
avoid withdrawal symptoms things like that we're just avoidance is a big is a big thing
and so people have come up with these these tasks that are supposed to mimic having to
sort of deal with that kind of conflict and so I'll tell you about one specific approach avoidance
conflict tasks that we have used and and how we how you can build a model of it the data
so this um this particular study and the other ones I'll talk about too
all come from this fairly large data set that we've collected in in Tulsa Library where I am
where I work called the T1000 and this is basically from the first 500 participants of the T1000 it's
essentially the exploratory half of a larger exploratory conformatory sort of study approach
that we've been taking and so for this 500 this is just kind of like a quasi-consort
diagram but more or less what we end up with is 59 healthy controls 261 people that have
depression and or anxiety so this is really meant to be more for kind of trans diagnostic
it's kind of our docking sort of and then we also have 159 in substance use group which is
largely methamphetamine but there is also some opioid and some other ones in there so it's
and I should say these this group also can have a bunch of anxiety and depression in it
so it's really not meant to be this clean diagnostically parked up sample
so okay so the approach avoidance conflict task basically the way that it works is you're shown
a little avatar guy like this and you're on this kind of runway and it has one of nine positions
and your job is to choose where you want to move this avatar to now the kind of happy sun picture
over there that stands in for seeing a kind of mildly positive safe stimulus whereas the kind
of rainy cloud that means that you are going to see a fairly strongly negative stimulus
but you'll also win a certain amount of points where this kind of little gauge here tells you
how many points and it can be filled either to two points four points or six points and so
and the idea is that each one of these different positions carries a different probability
of getting that outcome or that outcome so basically if you're all the way to the right
that means it's 90 chance you'll get this guy 10% you'll get that guy 80 70 60 50 40 etc
so the more you're over here the higher the chance you're going to get the safe
something more over here the higher the chance you're going to get the rainy cloud and points thing
but it's not fully deterministic is the point
that's one of the negative stimuli that you'll get if you if you go for the points
so point being these are legit aversives you know to the to the degree that you can get
through IRB so so you know it's not we've tried our best to make it so it's not just these purely
kind of you know mildly negative you know you can rate it but you don't still have that much of a
reaction etc but then you get this nice happy you made points right so so there hence the
conflict so but say instead so I should say so different trial there are different trial types
different conditions where you know there's a rainy cloud over here sun over here you know
zero points or two points etc I'll show you the different conditions in a second but so say instead
you get tried to go for the safe thing then there's your so there's your happy safe thing
um and this is really just meant to be kind of like an anchor right like an anchor kind of safe
you go to it if you are too afraid of the negative thing to get the points more or less right
you made zero points uh okay so here's the five different task conditions in this case
so one we just call avoid threat basically there's just rainy cloud and sun there's no
points involved so really you ought to just all the way over here every time um in this case
it's the approach reward condition there's both both suns on both sides but you get points over
here so again it should just be move all the way the right what you should do and then there's
conflict two points conflict four and conflict six in which case it's either the sun or rainy
cloud plus certain amount of points again two four six so those are the different conditions
so the again we're going to walk through this in a lot of detail but the way that we set up
the model um for this the exact same structure as what I showed you guys yesterday hopefully
some of you still remember a little um but so the it is you have some prior preferences up here
the c thing and there are two parameters that we fit for this essentially it's the value that the um
the agent assigns to the negative stimuli so essentially how much it dislikes the negative
stimuli and then this vp thing which is the value it assigned the subjective value assigned to each
point you could win um and so and so these two things essentially control um ought to govern a
fair amount of behavior because the more you dislike the negative thing and the less um points
are worth um the more conflict you ought to have um in the ought to be driven to you know toward the
sun um but then there's beta so this prior on expected policy precision um which for the paper
we've just kind of also labeled decision uncertainty just to kind of make it more intuitive to people
who aren't active inference experts um and so we're fitting that as well so it's a measure
essentially of uncertainty or or the lack of confidence in your own kind of action model
and in this case because we're not building in any prior habits um so e is just flat um
then this more or less has the effect of making behavior just look more random
just less deterministic um it would be different if they had strong habits but since
since e is flat then it basically just governs deterministic versus random sort of uh less
stable behavior in a sense um the states here are more or less the agent has to have beliefs about
where it is on the runway and the agent also has to know things about what task condition it's in
um policies are pretty straightforward you can choose to move to one of any of the places
of nine places on the runway um and the actual observations right so what the agent needs to
observe to figure out the runway position in the task condition um and to make a decision
so there's three different sets of observations one is just runway position cues and the other
is the task condition cues um so literally just what you see on the screen um and then three are
going to be observing the outcome stimuli so the the negative and positive images and sounds
and the points um so again pretty simple and just to give you a sense of what the what the
a matrices look like so how you'd actually set this up um so this is the essentially the probability
of the outcomes um given the different positions you could be in um and when the trial type or the
stimulus or the the trial condition uh is a particular condition so in this case it would be
the avoid threat condition um so basically each column here is one of the different possible
positions on the runway um each row is one of the different combinations of uh outcomes the outcome
stimuli you could observe um so under the avoid threat condition essentially the farther you are
to the left the higher the probability is that you'll see the negative stimulus and the farther
you are to the right the higher the probabilities you see the positive stimulus um but the idea
is that these probability distributions differ depending on which trial type you're in right so
in in uh in uh when the trial type is six then this joint distribution ends up looking like
the higher the more you are over here the higher probability is that you'll see the positive
stimulus but the higher the more you're over here to the right the higher the probability
you'll get negative stimulus plus six points uh sorry conditions right here oh oh this yes
right so essentially the different observations that you can make the way we set it up are
just sun just clouds um sun plus two points clouds plus two points clouds plus four points and
five plus six points right um and then you just have scaling factors essentially that that govern
how much uh the value is the value of those outcomes are determined by how much you dislike
the negative stimulus and how much each each point is worth to me um so uh anyway so yeah and
again we'll go through the model in detail yes the rows are the different observations that you can get
right so you can observe the negative stimulus you can observe the positive stimulus you can
observe negative stimulus plus four points or positive stimulus plus two points you know i'm
not showing all of them on every one just because it's cluttery but across all limits just negative
positive positive plus two negative plus two negative four negative six
oh yes sorry sorry i didn't explain it so so that the just for convenience um in that it just turns
out i mean there's a bunch of different ways you could you could choose to model this task um the
easiest way to do it here is to just give the agent a kind of starting position um that's not
sort of that doesn't represent any kind of committed position that you've chosen on the runway
so the first column is just kind of this starting position before you've chosen one of the nine
positions and the first row then is just the essentially this like blank observation before
you've observed where you've put the avatar so again it's it's just it's just for convenience
sorry i should have explained that so then one thing that we did um so one thing that's important
to do a lot of times if you want to try to convince people that your parameters that you're
estimating in a task are actually tracking something interesting or something useful or
something real um is to try to validate it against some other measures that you think
are also going to be relevant to the constructs that you're trying to uh tap into um and so in
this case you know one way to do that is to ask a bunch of questions about self-reported experience
or self-reported behavior on the task um and so in this case we just asked them a bunch of things
you know like i found the positive picture is enjoyable you know one to seven how much do you
agree or disagree um i found it difficult to decide which outcome i wanted i always tried to move
always towards the outcome with the largest reward um the negative pictures made me feel anxious
or uncomfortable uh this one this one about how anxious or uncomfortable you feel um will be of
a lot of interest uh to me later theoretically um and then also you can measure other kind of
standard behavioral things like reaction times and things like that that don't technically go into
the model so if the model can say something about reaction times and they didn't go into the model
then that can also be validating um so just to give you a sense of what this ends up looking like
in terms of the parameter validation um these are just Pearson correlations um for uh between
each of the parameters and all those different questions we asked and also with average reaction
times and um we actually get you know pretty much what you would hope for um in a lot of cases
so the more you value subjectively valued the points um the stronger your self-reported drive
to approach the reward was um the less difficulty deciding you had um the less you avoided punishment
the faster your reaction times were um etc whereas beta the more kind of uncertain the more like a
priori unconfident you were in your action model um so you could think of that again as like decision
uncertainty in this case um was um positively associated with reaction time so the more
uncertain you were the longer it took you to make a choice um and uh there's a bunch of these actually
are interesting right like um the more you chose to use a certain uh negative emotion
regulation strategies for example um and uh probably most interestingly to me is this guy
um which you have to remember a sample size of like 500 is actually super significant um
is that it wasn't actually the negative image uh value or disvalue in this case that uh said much
about how anxious or uncomfortable they self-reported feeling it was this um decision uncertainty
parameter um so so what's essentially what this kind of suggests is that inactive inference the
thing that amounts to or contributes to how subjectively unpleasant the valence of your status
probably has a lot more to do with um how confident you subjectively think you are in your action
model which again is just theoretically interesting yeah this is based on data yeah this is based on
the data and the 500 people um so it's just a parameter that you estimate um so in this case
it's largely going to correspond to um something like uh inconsistency in choices across trials
in the same condition um but um kind of it will also be in a way that's disentangled from um
things about say how much you um act as though you value the points um and dislike the negative
stimulus right so it's it's kind of a way of nothing about modeling this case is right you
can kind of separate out the estimates of the most likely relative contribution of these different
sort of independent psychological factors um so we'll go through the estimation though
but it's just an element in the parameter that you tweak until the model generates behavior
that looks the most like the actual participant's behavior and so then here's um some of the
interesting group differences that we find so when you just break people up into the healthy
controls the people with depression and or anxiety that don't have substance use and the people with
substance use most of which have either depression or anxiety um and what it turns out is that um
actually both of the clinical groups um show a greater um negative value assigned to the
negative images so they they're more sensitive essentially they act as though they expect the
negative stimuli to be more aversive um and in the substance use group they actually show
lower beta values which means that they are less uncertain so they act more confident
than the healthy people about what the what they think the right thing to do is
um so in this case they're more avoidant and substance use people are more confident that
the avoidant strategy is right um and uh so it's interesting that and there was actually no difference
in the point value subjective point value um we also found a difference in uh prior policy
precision um between males and females too where um females actually looked like they had uh
less confidence in their action model um than men on average they're more uncertain or you can
call that less impulsive too you know like defense how you want to spin it
so then so that's kind of just a you know group difference kind of thing
but obviously it's a lot more interesting is whether you can use this kind of thing to
say something interesting about treatment outcomes and so here what we did is this is a
an I say I shouldn't I didn't I did I'd analyzed the data and I built the model but this is data
that's being collected um largely by a colleague of mine named Robin Offerly who's a clinical
psychologist um at Liber and basically this this uh study involves 100 people um that had
were recruited with generalized anxiety disorder and they were then randomized into two groups
either and one got 10 weeks of exposure therapy and the other got 10 weeks of behavioral activation
a bunch of these people also had um depression um as well as um other uh anxiety
related disorders um so again somewhat heterogeneous group um as you can see here
response to therapy was also quite heterogeneous um but on average both therapies worked
and so we were just interested do each do any of these three parameters that we estimated at
baseline um can they predict anything about treatment response um and pretty cool what we
found is is that uh it actually predicts outcomes for both groups um so uh more or less the the higher
your beta value is so the more uncertain you are in your action model um the better your response
was to both therapies so you measure this thing at baseline then they go through the therapy and it
turns out you can sort of say in advance fairly well um who's gonna do better it doesn't differentiate
responders to one treatment versus the other which is one thing we were kind of hoping for um
but it does say you know is therapy gonna work for you in general relatively speaking um and
you know i mean my best way of making sense of this and i should say this this is for um this is
based on uh the promise depression scale um there was a similar result for the uh promise anxiety
um and um so again my my best way of making sense of this at the moment
is um just to say that like look if you're really confident in your action model and
that action model is avoidance um then you're probably going to be a lot less malleable right
you're going to be a lot less open to trying new behaviors in therapy and being open to
sort of counter evidence right so so interestingly the less confident you are in your action model
the more malleable you're going to be to update um what your model is um about what you should do
in therapy um is kind of that my current sort of pet interpretation um and i should say so this
uh these results we have a pre-print up on bio archive about right now um or actually it's on
fsl or osf or whatever that one is but uh these we don't this we don't have anything out on yet
because uh we're actually waiting to collect more data to finish it up um but it's close to being
gone um so summary here is that you know it looks like avoidance and depression anxiety and substance
use um looks like it could be driven by more negative expected outcomes and um greater policy
precision um looks like it may indicate greater confidence in the avoidance strategy in substance
use um and the less confident you are um in your avoidance strategy the uh the better your
you look like you respond to therapy um so um and i should say that one thing when you're building
trying to build a model something like this that's worth emphasizing is that there is no learning
in this model um this is a purely inference-based task you know you know what the stimuli are going
to be um and you just infer okay you know what's my best bet based on the outcomes i want to observe
based on um my relative preferences about each outcome um and the probabilities um so using something
like a kind of standard reinforcement learning model um that requires you to be simulating
learning um doesn't necessarily work so well whereas active inference is kind of nice for this
because you can do a purely inference-based model um it's not the only model that can do it but it's
it's a nice feature of active inference models that you can do that um okay so now that hopefully
i've motivated why this can be useful um let's uh let's try to build it um so i showed you guys
briefly how i structured the model maybe you've forgotten by now um but so kind of just walking
through the logic so let's take you know part one of the model that i discussed yesterday so just
observations states how states are related observations and what your prior expectations over
states are so those are the first four parts right so what so what are the states is the first thing
that you need to specify when you're going to build a model so then the question is what does the agent
need to know to do the task um so anybody i mean i told you a while ago but uh if you don't remember
any any any thoughts as to what what you would need to know to do it
that is one you gotta know where you are to be able to know where you want to you know
yes you need to know where you are and uh yeah the other places that you could be right so position
on the runway that goes and it's going to need to be one hidden state factor um what else not Maxwell
no so the points aren't actually something that needs to be a hidden state the points are going
to be something that you observe right they're going to be an observation that may or may not be
preferred right so so what else what else so the image is also going to be an outcome it's going
to be a thing you observe uh the probabilities are going to be in the a matrix those are the
things that map the states to the observations values of the points is also an observation
the subjective value of the points is and that's going to be in the c matrix which we haven't gotten
to yes trial type or task condition remember because where you're going to want to move on
the runway is going to depend on knowing what condition you're in right so so those are two
things you need to know where you are on the runway and you got to know what condition you're at what
task condition you're at so minimally a minimal model of this task requires that the agent knows
those two things um and it's sufficient that it knows those two things yeah
um yes and in this case they're fairly trivial it's just what you see on the screen
so there's no uncertainty so we'll we'll get to that but i mean it's just an identity matrix
it's very yeah any other questions at this point yeah
um well you won't know where the sun is
so i mean actually that's that's a good point um well
interesting um yes i agree that if if no matter what um
well so that would i mean that would kind of make you go wrong in condition too here right
because it's sun versus sun plus points and so in this case if you just
had the policy of always going toward uh the sun then um you wouldn't you wouldn't have any
idea of what's on the other side um
mm-hmm
mm-hmm
sure sure well that's i mean that is about the negative the expected you know negative value
of the outcomes right ahead of time that governs decision making and that is dependent on the
context right that's dependent on the knowing that they're getting on the train as opposed to
being in some other situation um yeah i mean so in this case i mean point point totally taken um
you know in this case uh you know when it comes to the automaticity of what actions get selected
i mean i remember these i mean i didn't say that but the um it's not as though the left and right
positions of these are always stable right so i mean they do flip back and forth and in counter
balanced ways so it can't just be like i'm always going to go right or i'm always going to go left
or something like that right so i mean if it's at that level of automatization then i don't think
it's a problem but but i agree you can adopt the just play it safe no matter what sort of strategy
which i'll show you some people behave that way um so which just falls out of the model
so okay so states normally position and task condition all right so um so in practice
in the actual scripts that we'll walk through in a second um so this is just a little kind
of cut and paste from part of the script um in practice you specify the hidden states
and the number of levels teach hidden state by specifying the d vectors which are your priors
over states um so in this case um there remember there are two time points in the trial right
there's the starting point where you make the choice of what position you're going to go to
and then there's the position you go to um and so the d here is your prior or initial states
so um so one then how many different levels do we need for hidden state vector one which is the
positions um anybody uh so nine is nine is what you would think right but for because of the
little kind of trick that we use it's actually nine plus one starting position starting plus
the nine positions you can choose um and what's the prior what's the prior over that going to be
yes exactly yeah exactly one and a bunch of zeros you're completely confident that you're
always going to start in the starting position right that's all that means um so how about
four d two uh so how many how many uh levels are going to be in that hidden state vector
there are five yes five and what will the what's what should the prior look like over that
what's the prior distribution going to look like
um yeah so in this case yes it'll be flat um so so in practice it can just be once
I mean this this is just going to get normalized so it doesn't it doesn't matter
so uh so it's just saying the agent has no prior beliefs that one task condition is more
likely than any other yeah you could and and for this is kind of getting the stuff that we'll
get into later with learning for capital D makes no difference because it's just going to get
normalized um when it comes to learning priors which will be a little d um then it matters
because they stand in for concentration parameter values um but again everybody ignore that for me
um okay so so then so by setting up that that way you're saying there are 10 levels
in hidden state factor one and there are five levels in hidden state factor two
and those are the priors over those levels
okay so then we already kind of talked about this a little bit but what does the agent need to see
or to observe to know what states it's in right so that's going to be your observations um
I already told you about anyone
right so it needs to see this thing on the screen right it needs to it needs to see what position
it's in um anything else
exactly so it needs to see these queues because those are what it's going to those are what's
going to tell it what task condition it's in right um anything else
uh the oh well that's I mean that's just that's just going to be part of the queue that tells you
what condition you're in this is really actually really really obvious
what else does it have to observe
yeah it has to observe the the actual outcome stimulate that it uh does or does not want to
see and hear right um so so uh so yeah so then those are so those are the three observation
modalities your runway position queues your task condition queues and what
all right whatever that was
okay so so right so runway position queues task condition queues and outcome stimuli right
so the idea then is just to make this clear is that the um that those observations those
three sets of observations are going to be generated based on interactions between
these two where you are in these two hidden states right so for instance being in that
position combined with being in this condition um will convey a 90 with 90 probability generate
the observation of um the negative stimulus and six plus right
the the probability is true the agent the agent knows the probabilities and they're correct
yeah so there's no like I said this is purely an inference task there's no learning
okay so in practice when you look at the script um the way that we set the number of outcomes per
outcome modality is just with this kind of um bit of code that more or less just sets up the
a matrix um where all it's really doing is it's saying based on d based on the number of levels
in each hidden state factor make that many columns um in in the a matrix and then for each outcome
modality give it the number of rows that correspond to the number of observations per observation modality
um so in this case it's just going to be um so you have so 10 positions
right five conditions um because these are just one to one right you observe the position you
know the position you observe the condition you know the condition and then for the stimuli
um what I showed you earlier is the way that we chose to do it is there's seven different
formal observations that could be made in terms of the stimuli
um go over what oh uh right so I will um
uh when I go through the a matrix um so so then the a matrix um right so the idea is
is that that is explaining how states generate outcomes um which you can then invert to use
for inference so you observe the outcomes you say conditional on those outcomes what states
am I most likely in um and so here you need one a matrix per outcome modality so per
type of observation that you can get um so in this case we'll need three a matrices one
specifying how states generate the outcome stimuli one specifying how the states generate
condition cues and one specifying how they state position how they generate position cues
um so essentially it's just that expanding out this arrow um about how states mapped observations
um and so here is what uh the first a matrix will look like so the one specifying how states
generate outcome stimuli um so remember here so the way we've set it up is there are one two three
four five um
okay yeah all right uh yes sorry that is my fault so you should have this one as well
but so so there should be one two three four five six um different outcomes that you can observe
plus the starting position one right um so that corresponds to seven rows
in your a matrix um so the columns then are each of the different positions you can be in
one being that starting position um and then each row is going to be one of the different
observation one of the different observations you could make um and then the entries here
encode the probabilities um of observing each outcome given which date you're in
um but remember um these distributions are conditional on what condition you're in right
so that's what this kind of third the third dimension of this a matrix is is saying conditional
on being in the you know this condition you know the probabilities look like that conditional
on being in condition two here which is this guy the probabilities look like that etc
so it's encoding these probabilities again which are known
um then the other ones are really just kind of trivial right so you can either observe the cues
that you are in the avoid threat the approach reward the conflict two four and six um and
the states here remember are so are um you know this one stands for being in condition one
condition two condition three etc so this is just an identity mapping um along the third dimension
and essentially so if you observe these cues then no matter what position on the runway you're in
you're in the you believe you're going to believe you're in the avoid avoid threat condition
that that all makes sense um if anyone is confused by anything uh again this is like
for you guys to learn so um we can take like whatever as long as we need with us
very very it's just saying what you observe tells you with 100 probability what state you're in
so there's no basically there's no uncertainty at all um in in uh what you observe tells you
about what's going on in the task
yes no yeah the partial partially observable aspect of this doesn't matter at all for the
aspects of the task um that don't involve any uncertainty right so in this case in this case
it it's necessary because of the probabilistic reward and punishment outcomes right um and the um
i mean yes exactly because that's for your specifying probabilities right um and uh
equally trivial for position cues um again it's just i this is just saying it's an identity matrix
for each of them so just one's down the diagonal position equals position right so again totally
trivial um but that's the true psychological ground truth right when you see the you know
where the avatar is you know where the avatar is um okay so any questions about any of that
because the next thing we'll we'll do is we'll add in the the temporal dimension
sure sure Yeah
Formally, it's very easy to do, you just make these things not once.
The, so the, what the task condition is.
Oh, misreading the context cues. Yeah, absolutely. That's actually really interesting. Yeah. I mean, the context of this task in particular, it's probably, yeah, you'd have to design a different task where the context cues are a little more ambiguous, but absolutely. Yeah.
And then formally, yeah, you just instead of this just being, you know, ones, you could just, you know, make it blurrier.
Yeah, I mean, you'll, I mean, obviously, like, you know, using this, again, where we're using models that aren't fully sort of Bayesian inference based when making decisions when there's probabilities, right, like, it's still, I mean, I still thought it give us interesting results with the parameters, right.
But clearly, these models are fancy enough that they can handle much more difficult problems. Absolutely.
Okay, so then specifying the B matrix here. So again, conditional on being in state one, how you think, you know, what your prior is for what state two is going to be, or what the state is going to be at time to that then you're going to need one B matrix for each
one B matrix per state factor, right. So basically that's saying how do you runway positions, how do you believe runway positions are going to change over time.
And how do you believe that conditions are going to change over time within a trial.
That should be that needs to be clear, because the agent is just going to be equipped with the belief that the condition is always stable across a single trial.
Right, it's not like the cues are going to change as it moves the avatar around.
But you're going to need one matrix for the runway position observation or the runway observe runway position state factor.
But because this runway position is under the control of the agent.
There are actually going to need to be nine B matrices for this one B matrix corresponding to each possible action that the agent can choose where an action is here specified as a transition.
So, so this is important that that, well, I'll get to this in a second but basically what the policies are going to be when the policy pi thing appears up here is just selecting one of a bunch of possible B matrices one of a bunch of possible transitions that are under the control of the agent.
So, again, trivially, just a little chunk of code here that just says for condition it's an identity matrix, you know, the, the task condition is not going to change while the agent is trying to make a decision about where to move.
Whereas this, the second part here is just saying that for each of the different levels in hidden state factor one, right so 10.
There's going to be a bunch of ones for each across one line for every possible action.
So, at the end of the day that just looks like this right so action one. So the be matrix one here action one just basically says, no matter what position you're in. If you choose this be matrix you're going to transition to state one.
So columns or states and rows or states at t plus what time at the next time point. So if you're in one you move the one if you're in two, you move the one if you're in three move the one, etc.
So that's action one action two is, no matter which state you're in, you move to state two, and then, you know, dot dot dot until action 10 is no matter what state you're in, you move to state 10.
So each of these is a possible action that can be chosen each of those transitions.
And in this case, policies are going to be identical to actions, because there's just one actions just one step.
But in practice, when you have tasks that require making selecting sort of chains you know sequences of actions, which is what a policy is sequence of actions, then allowable policies are going to be allowable sort of sequences of these right so can be like, you know, be matrix to then be
matrix three, and so forth.
And that's why you need a separate you need to separately specify what the agent is allowed to do in terms of allowable policies, because maybe you want to say, you know, action 232 is allowed, but not 222, or something like that right so you have to specify what policies are actually plausible for the agent to pick.
When those consist of different possible sequences of actions or sequences of transitions that the agent has control over.
So in this case, it will just look like this, it will just for the, for the second factor. So for condition, it's just all, it's all just going to be ones, because there is only one action that can ever be chosen just stay in the condition.
But for the runway position. This is saying that the agent can choose action to or action three or action four action five or action six seven eight nine or 10.
There's no one because one would correspond to staying in the starting state, which they're not allowed to do.
Again, a little snippet of code for that.
Yep.
So, there are some cases where what looks like learning can be captured as inference. There are many other cases where it can't the particular case you're talking about. I don't think you can capture that purely as inference at least in the ways that pop into mind right now.
In which case you'd have to activate the learning component of these models, which I will talk about a little later, but more or less you're just with each observation. So on each trial, you know, whatever belief you are, whatever belief you have about what state you're in.
If you attack on account that says that state is, you know, that much more likely because I observe myself starting in that state in the past, or every time I believe I'm in a certain state, and I observe a certain outcome, then attack on account in the a matrix that says okay now I believe that that
outcome is a little stronger.
So, so over time essentially you're building up priors about what states you're going to start in, and what states are going to generate what observations just through previous just through repeated experience.
It's essentially it's essentially just heavy and just heavy and coincidence learning.
Just six.
So there are. So the reason I said that is because so one way to model that is to have different additional different possible observations, where some observations could involve even more negative stimuli or something like that.
In which case you would learn that hey being in this position actually generates even worse observations, in which case that would be a matrix learning.
The other way you can do it is by somehow involved a somehow model learning the actual preference distribution itself, which you can do, but it's a little bit kind of be on the scope of this.
That's what I just said. Yeah, you can you can fiddle with see but learning see is a little bit more complicated.
You can certainly fit see values like we do right to see what see is for each person for each possible observation.
So, so, yes.
Yeah, I mean hopefully hopefully when I get to some of the learning stuff that can be a little bit more clear.
Okay, so here. So preferences here so your your C vectors.
So just starting out saying, you know, these are just a bunch of zeros for each possible observation modality.
And then I'm saying okay so the for the first one right the one where I'm observing what the stimuli are that I observe at the end of each trial.
These have a particular value assigned to them at time point two.
One is time point one column two is time point two. And I'm parameterizing these. So here I'm saying that the value of negative image is just this parameter neg underscore IMG.
And then positive underscore IMG. And then this one is the same parameter again but plus two times whatever my subjective point value is.
And then so on and so forth. So there's two parameters and fitting here there's well there's three there's negative image positive image and point value.
But as you'll see what we actually end up doing to make this invertible is you just fix the positive image value is zero, it's kind of like an anchor value, and then the other two get adjusted around that.
So if I were to choose the parameter value of negative one for image, zero for positive image and point value equals subjective point value equals one, then that's what the C vector would look like.
It'd be negative one for the negative image observation to for the positive image plus two, one for negative image plus two, three and five.
So this is just saying how much the agent prefers one over the other but again I mean this is just arbitrary based on the, based on the values of the parameters that I picked, but we're going to fit those and figure out which ones.
Best reproduce each participants behavior.
Okay, and so then the last part of the model then right as you have.
Yes, sorry.
So, so each column is a time point.
Yeah, so a time point one it's just always going to be in the starting position so it doesn't matter their time point to it prefers its preference distribution looks like that we're higher equals more positive.
So then he, like I said we're not assuming any kind of habits here so he's just a flat distribution with ones over each possible policy.
We have to set some value for for beta so for prior policy precision. In this case, we're fitting that as a parameter as well.
So, I just put it as an example you just by baby equals four but but this is something we're fitting so we're going to get a value for this for each person.
And that's really it, then you know you've defined also T is just the number of time points so just T equals two in this case but so you've defined each of these things you just throw them into a structure that's labeled little mdp.
In this three I've commented out because as I'll go into if you want to if you want to include learning, then you also include you can also include little a little b or little b, or little c or little e actually but I just in practice don't use this much.
But these are all commented out because we're not learning isn't happening we're not separating out the, the actual generative process from the agents model.
And again beta just equals mdp beta she will data here which we said in the example being for there are these other two that are worth mentioning.
So mdp.alpha is a it's essentially a an inverse temperature parameter. It's like, you can think about it as just encoding some some level of like behavioral noise or motor stochasticity.
So think kind of like shaky hand, you know when you're pushing a button there is always a little bit of that in human behavior.
And then at a is a learning rate, basically governance how much of a count you actually tack, you know, tack on every time you have a new observation.
But again, that doesn't matter in this case because we're not including learning.
So, once you have all that, then you, you know, say your parameter values for a simulation, and then you plug the whole mdp thing into this very, very dense and opaque function that Carl wrote, called the SPM to mdp.v or underscore vb underscore x script.
And it will take your model, run it and the the message passing algorithms of minimize free energy, and it will spit out the results with the agent shows.
And so this is an example of the kind of simulation outputs that you'll get. There's different plotting scripts, depending on what you want to look at but in this case the plotting script for a single trial is this mdp underscore vb underscore trial function.
And these are all just built into SPM in the DEM toolbox so they're all just as long as you have SPM 12 year. It's all there.
But the way that you can kind of read this is the darker the color, the higher the probability. So in this case the stronger the belief of the agent, the little cyan dots represent kind of the ground truth.
In this case, the agent believed that action 10 so moving to position nine was the best move to make at the end of the day, and that is what it chose.
This is its beliefs about position so it believes that at time one it was in position one and believes that time to it was at position 10 or state 10 state, state factor one level 10 so position nine.
And that was true.
And it believes that it was in the first condition, the avoid threat condition.
So more or less it just moved all the way toward the sun because that was the, that's the thing that made the most sense to do.
Because it was correct about what condition it was in. Now like, like you were saying if Lawrence if, if it had some uncertainty about what condition it was in, then it wouldn't have been near that confident about where the right place to move would have been.
Right. So this could have been more kind of gray, like a more kind of diffuse probability distribution over different states then you'd get different behavior.
Ignore that that doesn't really mean anything.
This is just saying how it's, this is just the posterior probability over policies so it's how the policies essentially how confidence and policies change over time so this is saying it was before it saw what position and condition.
It was completely uncertain about which of the nine possible policies it should choose. And then after it saw the position and condition it jumped up to being very confident in just the policy of going to the ninth position.
That update in confidence from from before to after seeing the position and condition stimuli corresponds in the neural process theory to getting a little kind of bump in dopamine.
So that's where that that kind of change in policy precision is what what you would simulate as a kind of phasic dopamine response, if you're going to try to use this and like a model based fMRI analysis to see if you could, you know, throw that in to throw that like trial by trial prediction into like
an fMRI simulation and see if you can actually track things that look like they plausibly correspond to dopamine sending or receiving brain regions, for example.
Philip Chortenbeck's actually done that in a paper from a couple of years ago and showed really nice ventral tegmental area activation, as well as cortical activation in a bunch of known dopaminergic target regions.
So,
well, I mean basically it just, it just, like, it's just very confident in what the best decision is, right so I mean the second, the second it, because remember in this condition, this is the avoid threat condition it's basically there's a bad thing and there's a good thing.
There's no conflict there's nothing.
So it's just fully rational, like there is no uncertainty at all about you should just move toward the thing that's going to be good.
It would be different, you know, in the different conditions, especially if beta was a higher number, because it would have a lot more uncertainty.
Anyway, so then here at the bottom this is showing the actual observations and the colors here correspond to the relative preferences and the preference distribution.
So basically it doesn't care what conditions it's in it doesn't care what position it's in, but it does have interesting preference differences over the observations it can observe the outcomes it can observe at time to.
In this case, the distribution looks like that and what it actually observed was this guy which I'm pretty sure is just the positive stimulus or something.
So that is more or less how it works beginning to end.
So you can then you can then run that repeatedly to get multiple trials. So in this case for the task in question, we have 60 trials in total.
And so you can run the thing under different parameter values and have the agent do it 60 times in a row, then you can get a sense of what the actual behavior across the task would look like under different parameter values.
You can do that a bunch of times under different parameter values until you find the set of parameter values that best matches a given participants behavior.
And then those estimated values we'd actually use in further individual difference analysis.
And the way to do that, more or less the way that we've done it is using this variational base function that's also just built into SPM that minimizes variational free energy via variational base.
SPM underscore an LSI underscore Newton function.
And so what you need to do is you just take an agents observations from the actual task and what the actions they chose were on the task. Those go into these you and why
And then you have to set some priors for what you think good kind of starting parameter about good plausible starting parameter values might be for anybody.
And so what you need to do is as it will just start with those priors and then iteratively adjust them around until it finds values that minimize the log likelihood, ie maximize free energy, ie produce the behavior that's most consistent with the actual agents
behavior or the actual participants behavior.
Moving it this way when you start with particular prior values. This particular algorithm it penalizes moving the parameter estimates farther from the prior values that you set.
And that's a way of avoiding overfitting.
And farther you need to move parameters from the prior value that you thought was plausible the more kind of the more overfit something might look so by penalizing that your it's a way of preventing overfitting.
So it's finding the best fit parameters without moving them too far from the priors you set.
And so in practice, if you do that, and it will look like this, where over time so with each iteration, it will give you a log evidence or free energy.
And over time, what you hope to see is that this goes up every time and eventually converges on the parameter values that fit pretty well with the agent behavior.
So in this case you can just see this is just showing how the other parameter values change from the prior values. And this is probably a clear depiction of this where for the negative image I started it at a value of one which would be negative in the model.
And the posterior was less for point value went up and for beta went up.
In that case, the direction in which those move was consistent with what the actual values were that I plug into the simulation, which is what you want to see to know that your models are coverable.
So, so I can actually just.
Yes.
Yeah, I mean it's just it's just depending on the sign right like in in engineering a lot of times it is cast as maximizing.
Yeah, I know I agree. You can think about this. Yes, it's identical to.
Yeah.
Yeah.
And that is one of those things that, you know, Carl, Carl could have been clearer about when he wrote these scripts, but
right. Yeah.
Yeah.
Yeah.
No, and that's a that's a good point I am that I should have mentioned and didn't.
That yeah, I mean it's just the point is yes that it is converging to an optimal value.
So,
just to actually show you a little bit in the actual so here is so now if you guys want to look at it here is the actual this AC underscore model underscore example.
This is the script that I just kind of went through with snapshots piece by piece. You know so clear all close all. I just set the random number generated a shuffle, just so it doesn't produce the exact same thing every time.
I set my parameter values to take your values.
This is just, again, exactly just piece by piece, what I showed you.
I don't think I really left anything out.
One thing I guess worth mentioning is is that you can either specify policies in terms of you or V you as if you want policies to be shallow which means they only look one step ahead, and V is in practice.
If you want the policies to be deep so the agent looks ahead all the way up to the end of the head all the way till the end of the trial.
You can specify it as V even if your trial, if it's only one step though, which I, which I kind of like to do because if you specify it as you then it won't spit out the dopamine simulations.
But, okay, so then anyway, I mean this is this is just basically giving it labels, giving the plotting routine labels.
The checking thing is just a thing that helps you know that you didn't mess up something you didn't write something inconsistent in your generative model before you run it.
Then you run it, then this is the baby trial plot.
Then this is just a couple other plots that you can generate the Carl's written plotting strips for.
And then here is if I want to run more than one trial, then I can just expand the mdp that I made. So it's just replicated over several trials.
And then you can just run that whole thing again with the same function. And then they're plotting routines for looking at behavior across trials.
So if I do this run one of these.
Normally it's very fast but on this computer it's kind of slow.
So I probably won't run the group one. But, but here you go. This is the this I mean this is basically the exact same thing as the one I showed you.
And this is another plot that I'm probably not going to go through this is not all that interesting in the context of this task but they're just, it's just other depictions of things in the model or what the neural process theory would predict.
But if you want to know more about this kind of thing I can, we can talk about it later it's just going to be more confusing for now I think.
But if, if this, if it wasn't so well maybe I can do it.
I'm just going to say, I'm going to say it might take too long to run the multiple trials version but it's probably worth you guys seeing a little bit what it looks like.
Plus this is just so this is showing five trials, and these are just different sorts of plots that more or less just encode the free energy are expected utility of the observations on each trial.
So different kind of ways of coding the same thing as in the, the single trial plots, but just over multiple trials. And again, if you want to know details about what each of those mean that I can, I can, we can talk about it individually later it's not that, not that important for now.
But in the, and then in the second script that I gave you guys, which is this inversion example script. This has everything you need to actually do the parameter estimation model inversion that I that I showed you the example of where the thing will converge on the optimal for energy value.
I'm also probably a little involved to go into in detail but here I just set the number of trials.
And then I set the parameter values that I to fix values that I don't want to estimate.
Then I set the some values that don't actually are that well in this case I'm setting the values the true parameter values for the simulation I'm going to run.
I'm just going to specify them here so that it will spit them out at the end so I can compare them to the actual model estimates.
This is just putting together the you and why all this is just putting together the you and why vectors that are going to be the observations and the actions that gets that into the estimation routine.
And then, down here, more or less I just specify which of the parameters, I actually want to estimate. And then all of that goes into this DCM structure, along with the you and why, which are the stimuli and the actions.
And then you run this.
And then you turn that into you take that DCM, and you run it through the actual AC inversion script that I wrote which is just this function that's appended below.
And this is the one that in turn you set the priors for.
So this is basically saying if I want to, I want to say that my prior for beta is one, and I put that in log space to keep it positive, so that during estimation it doesn't become a negative number, because beta can't be a negative number.
And then same thing for point value, and for any of these others, but it will only catch them if I say I want to estimate these. So in this case, it'll estimate beta negative image and point value.
And so here are the priors I put are. Yes.
Yeah, so it will. So basically what I'll do is I just, I just exponentiate it down here so it stays non zero it's it's literally just a trick so it never becomes negative during the estimation.
But, but so anyways you do that. And then, at the end of the day it goes into this guy, which is the actual estimation routine that runs the variational base.
And you also need a log likelihood function, which is written here basically it just takes you start out with an L value of zero, and then it will repeatedly just add the previous like likelihood value to the log probability of the
probability that the model assigns to the action that the agent actually chose.
And so it tries to maximize that over over iterations.
What do you say, oh it's just it's just adding a little kind of small number to make sure that it never becomes zero.
And this is just reproducing the generative model that we already walked through but just as a function.
And then that's it.
So you can toy around with this. Obviously I've skipped through a little kind of just procedural things to just assigning different variables to other things just for sake of time but
on this, then it will simulate 60 trials, and then it will try to fit them.
And I probably will take way too long to even show you any of it.
Give it a sec. Well that's that's just a single trial result.
Yeah.
Give it like another.
I don't know.
See, it produced its first log likelihood estimate of negative 59.
And it'll so basically it'll it'll bounce around like small fluctuations around that number for a certain number of iterations basically until it's kind of tried out little adjustments to each of the parameters.
And then it'll jump on the next iteration to whatever goes down the gradient.
But I'm not going to not going to run that the whole way because no way at any time.
So, okay.
So anyway so that's, so that's kind of beginning to end thoroughly through one task.
Which already realized is probably a lot.
So, whatever, you know, maybe we don't have to go into as much detail on the other ones but I mean, another task that we're kind of excited about that's like an even kind of simpler model is this model that we built of an interceptive awareness task.
And I capture this idea that a lot of people in computational psychiatry now, kind of like this idea that what might be going wrong is something about the precision estimates assigned to afferent signals from the body, and the way that contributes to either
representing an understanding bodily states, and also subsequent viscer motor regulation.
But no one's actually kind of tried to explicitly show in any kind of model that different clinical groups actually do assign look like they assigned different precision estimates to afferent interceptive signals.
So we thought we'd give it a go.
And so we use this kind of really simple heartbeat tapping task.
And the idea is literally just for 60 seconds, we just say, tap a button whenever you feel your heartbeat.
We simultaneously record EKG and pulse transit time and things like that.
So we know when their actual heartbeat was and what the soonest is that they would have felt it.
And so we did this under three conditions plus a control condition, one where we said, just guess, do your best. Another where we said, don't guess only press the button if you're totally sure.
Another where the no guessing stipulation was still in place, but we also did an interceptive perturbation where we had them hold their breath with the hope of kind of amplifying the afferent signal.
And the reason that's important is because most people at rest have a really, really terrible conscious interception.
Only about 35% of people in these tasks typically show greater than chance performance when they're trying to like indicate when they feel their heartbeat.
So to really get things off of four values in a way that would be meaningful, you probably have to do some kind of perturbation where you're getting the kind of variance will be more interesting.
But you had to like control right for like something like their guessing rate and also something like how conservative or liberal they're being.
And so we did that again we did that in that same sample.
But in this case we actually just did to try to we just decided to divide it up a little more fine grain so it's healthy people, people with depression, people with anxiety, people with anxiety and depression eating disorders we had like 18 people and then the substance use group.
So it's a pretty broad clinical group.
This is the way we set up the model.
It looks more complicated than it is.
Nevertheless, what we did is we just divided.
We took their actual cardiac events.
And we divided, and then we added 200 milliseconds for pulse transit time. And then you, we just basically divided the epics in half so that between heartbeats if you tapped in the first half of the time before the next heartbeat.
Then that counted as tapping during a systole.
Otherwise it was tapping as a day under a diastole.
And so that's it. So the observations here were just a systole or a diastole, which was just based on their UKG plus plus transit time.
And then the other set of observations is just the conjunction of whether they tapped and a systole.
Didn't happen a systole tapped on a diastole and no tap on a diastole.
These are, this is considered the effort and input that the brain is getting from there from the body.
So it is there, it's equivalent to like a visual stimulus but just coming up in the heart.
And then this then is just basically encoding what they want, right, they're going to prefer to tap when there's a systole to tap and to observe that they tapped and observe a systole and not observe a no tap and a systole, etc.
So this is just just true positives, false positives, true positive, false negatives.
And so what we fit here is then just so the C values, essentially how much they wanted to observe tapping and systole, and the inverse for no tapping and systole.
And the same thing for diastole and tapping versus no tapping. And then it's a little bit hard to understand intuitively but what this ends up acting as is just a bias for tapping versus not tapping.
So the, the higher the if you look at if you just take the PNT value which is essentially how much they prefer to not get a false positive versus PT which is how much they, what the magnitude is that they prefer not getting a false negative that it
just ends up this ends up just giving you essentially a metric of their bias against tapping.
So just it's just kind of a measure of, you know, are they the kind of person that likes to tap a lot or not very much it's kind of like a kind of treat conservatism about it is that the other kind of thing that you can do and again this isn't
really all that intuitive but if you just take the average of those two. So they the average magnitude of their preference of the preferences they have for both of those things.
Then if that's a positive value, then what that means is that what that means is that they actually had a strong preference to tap after feeling a heartbeat.
Whereas if that if that is negative, then it means that they were actually an anticipator they reliably tried to tap right before a heart rate.
So you can kind of differentiate between anticipatory versus reactive strategies.
Okay.
And then, but the most interesting one that we wanted to estimate was interceptive precision.
Essentially the, the precision of the mapping from systoles and diastole's to felt heartbeats or not heartbeats where the other where the controllable hidden state factor was choosing to tap or not tap.
Right so the higher the IP value equals more precise. And then I already explained this kind of tapping no tapping bias thing.
So, this is the kind of thing that we find.
So this is the bias against tapping measure. And as you can see, as you would, as you would hope that this value is much higher in the no guessing condition and the breath hold condition.
Then in the guessing conditions, people are a lot more conservative and when they choose to tap in these two, which is exactly what you would expect. This is just again a kind of parameter validation kind of thing.
Precision is really high in the control tone condition. Sorry, I didn't explain that basically the control condition is they just kept hearing a tone and they had to tap whenever they heard a tone.
And so precision estimates are really high for auditory precision in the tone condition, and they're super super low.
You know, except for some people who look like outliers in the others. And for the anticipate versus react thing, the average of the C magnitudes, what you get is basically a big chunk of people that are around zero, which basically means they were really inconsistent
in their tapping, which is just correlated with a sensory precision. But then you have a good chunk of people that are on either end that are kind of reliable anticipators versus reliable reactors.
And I think some is like relying a lot more on prior expectations and others relying a lot more on sensory input.
And so in terms of further parameter validation, we just at we just looked at a bunch of different things. So pulse transit time, a number of heart beats a heart rate for each person self reported difficulty of the tasks self reported
resistance that they did it right. Self reported felt heartbeat intensity, and then age and BMI, so body mass index.
There's all things that influence cardiac perception. And so what we found is kind of cool is that for inter receptive precision.
It was positively associated with confidence and intensity, even in the guessing condition.
Whereas in the no guessing and the breathable condition both where this conservatism was higher.
The bias against tapping was associated in the directions you'd expect with difficulty and confidence and intensity.
And those are actually pretty similar.
What's kind of interesting, even in the tone condition when like precision is essentially maximal, you do get this kind of age difference in whether people are anticipators versus reactors, which is kind of interesting.
There weren't really all that many interesting differences, or any interesting relationships here, basically sensory precision in one condition didn't really map on all that well the sense of precision and other conditions.
Between the no guessing condition and the breathable condition there was more similarity and sensory precision, but not that much biases against tapping was actually looked like it was kind of more trait ish across conditions for people.
But the interesting results is that the main interesting result is that when you look across groups for our interceptive precision estimates.
They, the healthy controls do have significantly higher interceptive precision than all of the clinical groups.
And the, and then the other interesting effect that we saw was that actually medication status, strongly wanted interceptive precision. So if you were on any kind of psychiatric medication, your interceptive precision was much less.
And anyway, doesn't look like we have time but I could have gone through basically how we specified the model for that.
And the only thing probably worth mentioning is the way that we specified interceptive precision. So in this case the way to fit this kind of precision parameter is you start just with diastole insistly mapping as an identity matrix to feeling heartbeat and not.
But then you can pass this thing through a softmax function with a particular temperature parameter.
So what ends up happening is if this PA or interceptive precision is high it ends up looking like an identity matrix, whereas the lower you may have to get this thing, the more imprecise it ends up looking.
Okay, so if person has values down here, that means that the signal they get from Sicily and diastole provides a lot less precise evidence for the presence of a beat or no beat.
And yes.
I mean, and again, I can go through, you know, all this with you guys more, you know, just in person one at a time or whatever if you guys want to.
But these are, you know, I just kind of listed the whole model up here.
In case you guys want to look at it, but the way you estimate parameters is the same.
Definitely won't have time to go into the three arm band or reward learning version or our, or our structural learning stuff, but hopefully this was helpful.
And Casper is going to have a bunch of time tomorrow to take us further so yeah.
Any questions or anything like that? I mean, obviously we've been asking some along the way.
