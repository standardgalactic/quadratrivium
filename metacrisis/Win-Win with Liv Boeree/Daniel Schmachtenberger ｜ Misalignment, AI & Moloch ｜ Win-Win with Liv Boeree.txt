Hey guys, so this is a bit of an unusual one compared to my usual format, because this
is a long-form conversation that was actually meant to be part of my upcoming WinWin podcast
that's going to be launching in a few weeks.
But the topic of it is so urgent that Daniel, who I'm talking to and I decided to release
this early, you will have noticed how mad the rate of AI progress is getting.
And I say mad in like every sense of the word, it's incredibly exciting and there's so much
cool stuff coming out, I can't even keep up with it.
But at the same time, it's also a little overwhelming to the extent that we are creating technologies
that we don't even understand and unleashing them and connecting them to the internet.
And it feels like the potential risks and harms of that are not being properly internalised
into the sort of general calculus of all the different people doing this.
So yeah, in this conversation, I'm talking to Daniel Schmaxenberger, who is frankly
just one of the smartest people I've ever met.
If you've been following my more recent content around this topic of Molek and game theory
and when is competition healthy and unhealthy, his thinking has inspired a lot of this content.
So it's a real pleasure to be finally talking to him.
And specifically, in this conversation, we get into, I think, a sort of blend of topics
that haven't really been discussed in this way before.
We talk about the nature of game theory and Molek and how it interplays with our wider
sort of capitalistic economic system and how that also then interplays in the development
of AI.
So if any of those topics interest you, and they should because if you live on this planet,
this will affect you for good or for worse, then you should take the time to watch this
conversation in full.
It's one of the most fascinating chats I've ever had.
It's also one of the most terrifying.
So let me know what you think.
Yeah.
So I am really happy for us to be having this conversation today.
You and I have talked about Molek and the relationship of the kind of Molek metaphor
to the overall state of the world and what I sometimes call the metacrisis for a couple
years now.
And you've put out these exceptionally good educational videos on Molek and expressing
itself in different environments in social media and general media.
I hope everybody has watched those.
We're in the moment right now where there is this rapid race on the development of artificial
intelligence technologies, development and public deployment of them.
We're recording this shortly after GPT-4 has been publicly released and then after so many
of the other companies that have AI capability have also had to release their large language
models and response.
And so what I'm actually really wanting to talk about is AI risk and a way of thinking
about the totality of the AI risk landscape that is for me clarifying and a little different
than the way AI risk is usually talked about and maybe unifying across different categories
of risk and they give us some insight into how to think about what protecting against
it might require.
And the Molek frame, I find, gives incredibly valuable insight in thinking about the AI risk
frame and the AGI misalignment issue is very helpful in thinking about the Molek issue.
I think the two metaphors clarify each other.
And then I think the actual Molek-type dynamics give rise to the AI risk scenarios that I
am most concerned about.
And so that's what I'm wanting to talk about, which is why I was particularly interested
in you and I having this conversation since you're holding the mantle of helping the world
understand the Molek dynamics.
And so with that, I would love if you would share what the Molek thing is about for people
who don't already know.
And most of your listeners will already know, but why are we referring to it that way?
What is the phenomena?
Why is it interesting?
Sure.
So probably the most concise definition I can give is that it's the god of negative
sum games, like unhealthy competitive situations.
So by that, I mean like a system of bad incentives that incentivize agents within that system,
players within that game, to sacrifice more and more of their other values in order to
win within that narrow domain, in other words, win the game.
And by doing this sort of sacrifice of all these other values, they're essentially taking
selfish actions that externalize harms to everybody else, both within that game and
also even people outside of it, to the wider system as a whole, and hence making the game
a negative sum thing.
OK.
So sometimes when trying to describe the generality of the instances aren't already clear, it's
hard for people.
So could you give a couple of examples of what that looks like and why thinking about
it as a god is at all an interesting frame?
Like why is that even the frame that's arising?
Yeah.
So an example I gave in my first Molek video, The Beauty Wars, is about these beauty filters
that have now become completely commonplace on Instagram and, in fact, on most social
media platforms.
And the reason why these things are sort of so particularly Molek-y is that everyone
who's trying to sort of play the beauty influencer game or any kind of influencer game, frankly,
on these platforms, you get directly rewarded with more likes and follows if your pictures
look better, if your face looks better, you know, your complexion is clearer.
And these filters started appearing where not only would they make your face look smoother,
but they would just like tweak your features in really, really subtle ways, you know, just
like make your eyes a little bit bigger and make your face basically converge upon this
whatever these like what seem like normalized beauty standards, but do successfully seem
to hijack people's brains.
Do like these and I just, you know, to show you how like powerful these things are, like
I would upload a picture of myself that I loved and then I would apply the filter to
it and then I would compare the two side by side and I would now no longer like the original
picture.
And therefore it means, you know, it makes you hate your natural face.
And yet despite knowing this, because you know, you know, you'd get the direct reward
of getting more likes and follows by using these things.
And then on top of that, you know that everyone else is using them as well.
And so if you don't use them, then you're essentially going to get like left behind
the curve, you're no longer going to be competitive in the influencing game.
It's a really classic example of Moloky bad incentives, driving a kind of like race to
the bottom where everyone ends up miserable.
No one wants to be using these things, but feels like they have no choice if they want
to stay competitive.
Another example would be climate change pollution, you know, pollution from countries that are
trying to grow their GDP, you know, essentially not get left behind their competitors get
left behind other countries and externalizing the, the costs of their GDP growth to the
atmosphere by polluting with CO2.
So essentially a tragedy of the commons type situation.
And then a third example is the classic arms race, a country notices the competitor is
developing some new type of hypersonic missile or autonomous weapons and so on.
And even if they don't want to spend a bunch of their GDP on these very expensive new types
of weapons, they feel like they have no choice because if they don't, then they're going
to be vulnerable to their enemies.
Right.
The first is worth noting that each of these are already abstractions across a lot of cases.
Right.
When we describe an arms race, whether it's these two countries or these two countries
and whether it's on hypersonic missiles or AI weapons or bio weapons, those are each
different instances.
So the generalization across the class of if anyone is developing better weapons technologies,
everybody else has to develop correspondingly the counters to those weapons and the same
type of weapons or they kind of lose by default because it's a situation where anyone does
something that increases their own security in a certain way that also inexorably decreases
the security of others unless they do some counter response.
So there's lots of different examples of an arms race, but arms races as a whole is already
a big generalization.
Tragedy of the commons is the same, right?
Because we can be looking at the situation where we're talking about overfishing or whaling
or deforestation or desertification or CO2 and these are all cases where the overall commons
is being degraded by every actor pursuing their own near term incentives, right?
The actual incentives laid out in the economic landscape.
But to recognize that when we look at every environmental issue facing the world, no one
is trying to extinct all the species.
Nobody is trying to desertify the planet.
Nobody wants climate change or the venusification of the planet.
And yet the entire world is making it happen, right?
And that's like, so when we look at features of the world that nobody wants and that are
bad for everyone, why can't we change them?
This is where the Molok frame comes in, right?
And we can see that in both the arms race, everyone's like, look, I don't want to necessarily
live in the world with the autonomous weapons or the bio weapons, but we have to because
they're going to.
We all make the agreement that we're not going to.
How do we know they're keeping the agreement and they're not lying and defecting in some
underground military base?
So we have to assume under partial information that they are doing the thing because the
risk test would be too high if we assume the other way.
So under partial information, we have to assume that worst case, you know, do the same thing.
They're assuming the same thing.
So because of the inability for trust and coordination, we get this kind of race to the bottom.
And the same is true in all these various scenarios.
So we see a lot of features of the world that it seems like are comprehensively bad for
everyone, trending in a much worse direction.
Nobody can really do anything about it and nobody wants.
And so these properties are kind of the emergent properties of bad coordination.
And so you have another place as described Molok as the God of coordination failures
or basically the principle of coordination failures.
The reason to talk about it as a God or something is to say like, OK, well, since no agent is
trying to make it this way, what is making it this way?
Is there some kind of emergent agency or some underlying system dynamics?
Well, we can think of it as underlying system dynamics.
And I think you and I and many people in our sphere both came across this frame from Scott
Alexander's Meditations on Molok paper that references both this great poem on Molok and,
you know, a number of pieces in popular culture.
And if people haven't read it, everybody should read Scott Alexander's paper on meditations on Molok.
Because what it's trying to get to is if every environmental issue from dead zones in the ocean
to plastics and waste in the ocean to all of these issues, nobody wants, but also nobody can stop
because the cost of someone stopping it disadvantages them relative to everyone else.
If everyone else is going to continue to externalize that cost to the commons rather than internalize it
and decrease their profit margins.
And so how do we deal with that thing?
And if all of the things that are moving us towards increased likelihood for global catastrophic risk
or at least many of them have this in common, this is an underlying feature that we have to really understand.
Right. And so you could call it the God of coordination failures of the unhealthy kind of game dynamics,
not the ones that upregulate every because, yes, an arms race upregulates everyone's capacity in a certain way.
But it is also upregulating a capacity that everyone wishes we didn't have that is only relevant
because everybody else has it. Right.
If we could all just agree to decrease military spending by a factor of 10 and reinvest all of that
in health care and infrastructure and everything, the world would be better by everyone's standards.
So we're not saying that there are no types of competition that lead to positive some dynamics,
but there are these other ones.
So. So it's very interesting, like.
Molok can be seen as a kind of way of looking at generative dynamics that lead to the overall
state of global catastrophic risk. Right.
That there are other places where I've talked about the metacrisis and tried to give a formalization of it.
We can link that here.
So I won't do it at length, but I'll just very briefly say.
Metacrisis thesis is that we are at a unique time in history where there.
Are an increasing number of global catastrophic risks with increasing probabilities.
And that has never been the case like this before where the attractor state of increasing catastrophe
is the most likely attractor state of the future across many different dimensions of how that could play out.
And the other attractor state and maybe I'll come back to that.
I'll explain this one a little bit first.
It is a catastrophic risk is not new civilizations have faced war and have faced famine and have faced plagues
and have faced self induced environmental ruin Easter Island in many cases.
Previously, they were just local.
They weren't global.
And that was because the overall civilizations were local.
We didn't have fully globalized supply chains where everything depended upon six continent, you know, radical,
interdependent type things.
And when we could destroy a local environment, we couldn't destroy the biosphere at large or oceans or something.
So obviously, it's our level of technological capacity that allows us to have a global civilization.
That allows what happened all previous civilizations, which was civilizations did go through growth curves,
where they had peaks and then they failed.
And they kind of all failed, right?
At least that's the overarching architecture we see in collapse of complex societies by Tainter and other books like that kind of describe some of the dynamics.
But we are for the first time facing that in a global way.
So it is not unprecedented.
The thing about civilizational collapse is just unprecedented to think about it globally.
But obviously, the Egyptian, the Mayan, the Roman, all the previous empires failed for various reasons.
We didn't actually have world ending tech.
We didn't have the capacity to ruin everything rapidly until World War Two in the bomb.
The bomb allowed something where a rapid escalation could destroy kind of everything.
That was novel.
There were hundreds of 200,000 years of homo sapien history before that.
We couldn't destroy everything quickly and then we could.
So that was a bright line in the sand and that was very recent.
And we couldn't ruin the entire planetary, we couldn't reach planetary boundaries and mess up the biosphere until industrial tech.
But the industrial tech doesn't get there rapidly like the nukes.
It takes a few hundred years of its proliferation for cumulative effects, right?
But we went from half a billion people before the industrial revolution to eight billion people.
We increased the resource consumption per capita, moving into the industrial world by 100x plus.
And that's utilizing resources from the earth faster than they can be replaced,
turning them into trash and pollution faster than they can be processed,
ruining the environment on both sides with an exponential economic growth curve
that just to keep up with compounding interest has to become exponential
and to not overinflate that currency has to equate to more goods and services on a linear materials economy.
You don't get to do that thing forever.
So industrial tech bound a linear materials economy,
turning the earth into trash and pollution through a commodity cycle faster than it can be replenished,
attached to an exponential curve of finance, utilizing growing industrial tech and globalization
is what creates all the planetary boundaries we're facing, of which climate change is one,
but species extinction and biodiversity loss and on and on and on.
The entire planetary boundaries framework is the result of that.
That is the result of tech.
Without the industrial tech, we couldn't have done that, right?
So cavemen can't mess up the entire planet, right?
Stone age tools, even bronze age tools can't do that.
Nor can they have a war that kills everything.
Nor can they spread memes.
They're not informationally connected either.
Their memes spread much, much slower and more locally, right?
And so that is also the result of the tech, right?
You and I are talking via satellites right now, right?
Via literal outer space type communication for this thing to be able to happen
on computers that were generated in six continent supply chains
and that are more advanced than the things that ran the Manhattan Project that are available to all of us.
And so the cumulative effects of industrial tech bring us to planetary boundaries
and kind of increasing fragility, where there used to be a lot of people who lived on local subsistence,
not dependent on the total grid, and there were a lot less total people.
Now there's a lot more total people and they're almost all dependent on the grid,
not local subsistence, so the fragility of those things is radically higher.
So you both get fragility of the planet and fragility of the human life support systems
multiplied by a lot more people, which can, of course, also escalate to violence
when things start to break down, things like that.
So then you get the bomb is the example of the first fully existential tech.
And for the first time in history, we actually had to make an entire world system to not use our new tech.
Whereas before that, every time we had new tech, there was always a race to deploy it as fast as we could for strategic advantage.
This is a situation where nobody can win, everybody loses.
So mutually assured destruction and the entire post World War Two world of the Redwoods financial system,
the UN, et cetera, was all, how do we make a world system that doesn't do that thing?
And it happens to be that that was successful, which is why we haven't had a kinetic World War Three since then.
We're closer than ever to that right now with a proxy war between NATO and Russia being as close to not proxied as it is and other things in the horizon.
But mutually assured destruction doesn't work when you have many, many players that have catastrophe weapons and many types of catastrophe weapons,
which is the scenario we have now.
It's like a local, it's a local minimum, but there's like a ton.
It's a very tiny minimum where just a little nudge and it could fall off down the hill.
There's many routes for it to topple off down the hill.
Yeah. Yeah.
And the other thing is that a major part of the post World War Two solution was one of the major reasons for war, as you mentioned before, was competition over resources.
And if major nations want to be able to grow their economic quality of life for everybody, i.e. they want more stuff to not have to invade each other to take their stuff,
how can everybody get more stuff simultaneously?
Well, we can create an exponential monetary system and globalization and free trade and much more industrialization and just take stuff from nature faster so that everybody can have more stuff exponentially,
super positive, some dynamic, except you can't take stuff from nature forever and be able to keep doing that.
So you start hitting planetary boundaries and we're right at that point.
And then when our own inability to keep growing without taking other people's stuff comes now the conflict type dynamic.
So apart to the planetary boundaries we're facing are actually partially the nature, the result of the solution to not World War Three,
which is why also the de-growthers have to factor that de-growth ends up driving World War Three if you don't have other ways of tending to the fact that many people would not voluntarily choose austerity.
In the presence of less stuff they would choose war and the other if they thought they could win.
But then the flip side argument is like, okay, so yes, to an extent technology has gotten us into this mess,
but it sounds like we should pile on more technology to expand those essentially those planetary boundaries to be able to more efficiently extract resources and thus sort of keep the House of Cards going.
No.
We can say for sure that Luddite solutions don't work, even if they would be better because of multipolar traps, because of Molok, because the tech equals power.
And if somebody says, hey, this tech is causing harm, so we're not going to do it, they also just lose in the short term to whoever does, right?
I see.
And so, okay, we think AI weapons are bad, so we're not going to build them.
We think, okay, great, then you're going to be destroyed by whoever does.
So, unless you can get universal agreement, you can't just lose an arms race.
And this has been one of the challenges of whether it was China engaging with Tibet, whether it was colonialists engaging with the Native Americans,
whether it was Genghis Khan or any of his guys engaging with more peaceful tribes that were smaller, the peaceful tribes lose at war.
And they also lose at population games, right?
Like the ones that are going to unrenowably use the planet to grow their population faster.
So, we are in this unique situation where the result of that is an exponentiation, right, who has made it through are the people that both win at war and win at economic growth.
And so, we have radically more warfare, like total warfare potential and that is radically more distributed and radically more externalities on the environment and all the fragilities associated.
And that situation ends up leading to catastrophic breakdown of everything.
So, basically, so far, the answer has been win at the race or lose.
The race itself is self-terminating.
This is kind of the metacrisis hypothesis.
So, we take the next step and not only did the post-World War II model in doing the good thing of we didn't have nuclear war yet, right?
We didn't have a kinetic war between superpowers.
It also increased total global fragility, increased the movement towards all the planetary boundaries, and we proliferated a heap of other technologies that are truly catastrophic now that unlike nukes are not easy to control.
Nukes are extremely hard to make.
Uranium is not in many places.
It's hard to enrich.
You can see where it is with satellites because it's radioactive.
And so, you can limit it and only have a G9 or whatever that actually has nuclear capabilities.
And limiting Iran and many countries from getting it has been a major part of the world order, right?
But when we're talking about cyber weapons or drone weapons or bio weapons or the types of attacks that AI makes possible or other types of exponential tech, these do not require.
Something that has to be mined in one particular area in the same way.
These do not require nation, like top level, nation, state level capabilities.
Once they're developed for any purpose, they're pretty much more easily accessible.
And what that means is, and we always talk about exponential tech, democratizing power, right?
Decentralizing and democratizing.
And decentralizing democratize sounds nice in some ways when you don't like concentrations of power and the abuses they're in.
But the democratization of catastrophe weapons has a downside.
And one is when we're talking about not just a few nation states, but lots of nation states and non-state actors and people who you can't even tell whom having those capabilities.
You can't put me to assured destruction or forced Nash equilibrium is in the same way.
What it portends for kind of just disgruntled misanthropes of which there are more as the other issues are advancing and technological unemployment increases and people migrating because of climate change increases and all those types of things.
And then some of the exponential techs just even cause the ability to cause pretty catastrophic stuff by accident, right?
Whether COVID was from a lab leak or not, the idea that if you're doing gain of function research and synthetic, synthetic bio in a lab that it can leak.
And as you're doing lots more of it, that the probability of that increases, like that's not even intentional.
And as easy as it is for a lab leak, you know, of that type, it's way easier for AI leak because it's connected to the internet.
It's actually almost very hard not to have those types of things happen.
So what we're saying is that we're at a novel point in history that World War Two was a novel point first truly catastrophe weapon.
Now we're at the point where we have multiple types of catastrophe weapons, many actors that have them, no good forced Nash equilibrium, planetary boundaries, fragility, and that we're not saying that lots of things aren't getting better.
Of course, all of the Pinker and Friends arguments about the things that are getting better are the point is that they're getting better at the cost of other things.
That are being made worse, where externalities are being driven.
The things that are being made worse are getting very near criticality points and tipping points that change the game fundamentally.
So you have a world of increased catastrophic risk.
And of course, you have cascades between these because you can have well before climate change.
And whether it's just from CO2, or whether it's from the localized effects of deforestation or whatever, we do have increasing extreme weather events.
So then you get human migration.
And do we see likely possibilities for much larger amount of human migration in the near future?
Yes.
And can that lead to resource wars, which can lead to escalating wars if they hit already tense geopolitical environments, whether it's India, Pakistan, or whether it's, you know, so many issues like that.
So we can see that whether we're talking about large scale military dynamics or breakdown of supply chains in human systems or what exponential tech can add, they all actually kind of cascade into each other.
They have the capacity.
So there is some need to tend to and that what you do to make one of them better can often make another one worse, right?
So people will propose, hey, we need to tax carbon heavily and properly price carbon, where the price of the tax allows us to sequester the CO2.
But if everyone doesn't internationally do it and say the US does or Europe does and China doesn't, and that equals a radical change to GDP, which gets reinvested in military plus overall geopolitical diplomacy, then you're also changing the balance of power in the world as a result.
And so this is one of these classic cases where the way you make one thing better can make other things worse.
So how do you kind of factor all that together?
So this is, I took longer than I wanted, but that's roughly the metacrisis thesis, right?
And so Moelach is one way of looking at one of the generative dynamics that gives rise to this.
A comment that often arises on a lot of my videos I've noticed is that people really, you know, they're sensing that there is some malevolent force that is essentially making the world, you know, making it hard for people to coordinate, making it so that we seem to be trending more towards like greater
militarization and greater war, you know, risk of war and so on.
But they can't, they end up ascribing it to like, you know, like a QAnon type theory or something like that, you know, like, oh, it's a shadowy cabal of elites.
It's the elites who are driving this and so on.
It's like, there's some truth to that in that like elites have more power and therefore have a little bit more responsibility in driving a molecule process.
But there's no, I wish there was a centralized cabal who were like drawing because then at least then we'd have some some easier ways.
Like, okay, we've got, we know who the enemy is and the enemy is physical and real and like therefore you could take it out.
But it's more distributed than that because it's this like nebulous collection of bad incentives that we happen to call Moloch because we need to give it a name.
We need to give it a face so that we can understand it.
But yeah, it's, I think it's kind of an important point to hammer home to people because they're looking for an enemy and they're looking for a scapegoat.
But all the while that they keep blaming it on like constantly just purely blaming it on the elites, they're missing.
That's not going to solve the problem.
You can kill all the elites and molecules will be there.
Yes.
So this is why looking at the Moloch type dynamics, the coordination failures are very useful for understanding lots of features of the world.
Is that the in the various environmental issues, the various market type races that end up being races to the bottom or that bring way more risk.
I'll give you another great example with the race to AI right now or the ones with social media that happened.
The there is a perverse incentive to focus more on the opportunity and less on the risk of any new technology, even though any new technology will do both.
Because if I say, whoa, there might be real risk in this.
This is very powerful.
People could use this for various purposes.
We want to do a real thorough deep risk analysis before releasing this thing and not release it wrongly.
And we want to do some real safe to fail testing.
And someone else is like, we do some bullshit box checking risk analysis and then talk about all the awesome upsides and rush ahead.
They get first mover advantage.
They get more investment.
They get Metcalf law and winning the network dynamics.
And so there is a perverse incentive against thoughtful consideration and precautionary principle.
And so we see that lead got put in gasoline for some really fucking simple thing of engine knocking that knocked a billion points of IQ off the planet and forexed the aggressiveness of everybody.
By literally atomizing lead that we had to pull out of deep oars and brain toxinifying the whole planet and took like 80 years before we finally outlawed the thing.
And the effects that that had on the entire population of choice they made are irreversible.
And the same with DDT and pyrethion and malthion and on and on where we or cigarettes where we don't regulate the thing until way after the harms have been so clear.
But as we're getting tech that has and and Elon and many people have talked about this for a long time.
As we're when we're dealing with AI, when we're dealing with synthetic bio, when we're dealing with technology that has rapidly, much more rapid and much more scaled and consequential and complex types of effects.
If you wait until it hits a certain point to try to regulate, it's too late.
Now you have radical irreversibility.
And so the we saw we saw a chat GPT get to 100 million users in a fraction of the time that it took TikTok or Facebook or anyone previously.
And obviously it has a lot more total power and things that it can do.
And so we this isn't a situation where we want to have an anti incentive against precautionary principle maximum incentive on race.
Where the regulators are inherently slower and more stumbly than the ones incentivizing or like there's that that is also part of the Molok dynamic, which starts to bring us to the conversation.
Did you want to talk about the relation between Molok and capitalism before we get to that?
Yeah.
Um, I'll start by saying the competition, the Cold War between the USSR and the USA was not which was being framed as two different political economies, right?
Communism and capitalism and competition was not capitalism, right?
And the fucked up things that were happening inside of the USSR were not capitalism.
So we're not talking like the critique that we're about to offer of capitalism is not saying some previous economic system or political economy was better.
And actually Molok and stancy it itself through those systems as well.
Capitalism was more effective and it didn't get selected because it was more effective at both good things and fucked up things, right?
Which is kind of what you mentioned that it wins a war, but it has to sacrifice important stuff to do so.
So the thing that, that can be reductionist and win certain critical metrics, but harm other stuff in the process where eventually the cumulative effects of those harms are either catastrophe or dystopia world that nobody really wants.
And those are the two primary attractors right now you have catastrophes and to prevent all the catastrophes to make sure that people can't build catastrophe weapons in their basement, what type of surveillance is needed to make sure that you have enough controls on all the things.
If you really have the ability to control the entire landscape of things that could lead to catastrophic risks that are radically decentralized, most of those solutions look pretty dystopic.
And so we want a future, a third attractor future that is neither catastrophes nor dystopias.
And without saying what it is first, let's just say that it's not those things.
And we can almost all universally agree that we would prefer not those things.
Which means that we need something that has the power to be able to prevent catastrophic risk, but also needs checks and balances on its own power, right?
Doesn't have unchecked power and, and capture ability or corruption or those types of dynamics that are then uncheckable.
It's beyond the scope of this video to talk about what that third attractor solution is.
But so we have, you know, when we talk about capitalism as a kind of dominant global economic system, and of course we don't have pure laissez-faire capitalism, we have this kind of hybrid political economy, but roughly this thing, it does not subsume all of MOLOC.
Like we said, MOLOC was operating under feudalism and under communism and under other systems and in the competitions between them, it is more fair to think of it as kind of the god of game theory.
But as capitalism being such a powerful part of that stack, we can think of it as a metaphor for a moment and say capitalism is, well, let's just start by the, a couple of key aspects of the incentive.
Dynamics, vast majority of human history and tribal type dynamics, pre-agriculture, all of our kind of genetic fitness in that environment.
We didn't have the ability to store a lot of surplus, right, that happened post-a-plow and grain and storage technologies and whatever.
In which case, there were all these kind of sayings of various tribes, the best place to store extra food is in your neighbor's belly because rots otherwise and, you know, et cetera, and tribe in are invested in.
So, but as soon as we start getting to private property ownership and the ability for a lot of surplus, where I could differentially make it through a famine better than somebody else, I could have a higher quality of life than somebody else, I could pass on inheritance.
As soon as I have private property as a possibility, now there is an incentive to try to turn more of nature into my private property, right, and to try to turn more of other people's actions in my private property.
But when the property is actual real commodities and goods or the agreements with people who can do services, let's stick with goods because it's easier for right now.
There's a diminishing return on the value of any of those based on the illiquidity of them or the difficulty of moving them.
I get more lumber at a certain point, I have more lumber than I can use and I can't even move it around to sell all that quickly.
And so I don't really want all that much more of it, right?
And the same would be true with or of a certain kind or whatever.
But as soon as we move to a kind of a currency mediated system where I can sell it in real time and turn it all into something that has no intrinsic value, but the optionality for every form of value.
Well, now there's no fungibility fungibility.
Now, and obviously we started with things that had intrinsic value but still got used to mediated like gold, but then, you know, we got to fiat.
So I'm just going to do the huge jump to fiat because it's the current system.
Even though it has no intrinsic value, what the value that it has is maximum speed of optionality, right? Maximum optionality and maximum kind of liquidity and speed.
And so in that situation, there's no diminishing return on getting more like more is more, right?
And whether I want to convert that money into military power or convert it into public opinion through media and campaigns and stuff or convert it into technological power of one kind or another kind or land ownership.
The money allows me the ability to do all that.
So you can think of it as just units of power or units of game theory, right?
Units of OODA loop and then when you add money on money dynamics to it, compounding interest as the beginning, then, of course, all of the financial services that become possible with more capital, but just compounding interest.
Not only is there not a diminishing response to as I get more and more or it becomes less valuable to me because I can't use it fast enough.
Now, as I get more money, it is actually exponentially making money on itself.
So when I have private property, I have the ability to turn all of that into fungible units of capital and it makes money on itself.
There is now a maximum incentive to turn as much of the world as possible into capital in my holding.
And because other people are and they could use that against me, there is now an arms race for me to do it faster than that guy, right?
And that's decentralized.
And now, of course, we can see that when we're talking about there are types of power that don't directly just relate to dollars, right?
The number of Twitter following is one or the amount of covert political influence or, you know, many, many other things.
There are military generals that have more total power than the amount of money they have.
But obviously they influence a huge amount of money in terms of military assets, how much they cost and things like that.
And so this is why I say I don't want to reduce it exclusively to money.
But if we had to pick a single metric that has the most kind of optionality for all other types of metrics, that would be the one.
So if we're thinking about Molok as a whole, we can see that whether we're talking about the environmental issues or whether we're talking about the increasing polarization because of
social media algorithms or whether we're talking about any of these things, the rapid race that is not orienting towards safety enough on new technologies,
that this set of dynamics is underneath it, right?
And this is why that kind of frame Scott Alexander and others have put forward, which is who is engineering this thing?
Well, Molok is engineering this thing, right?
Like that thing overall.
And now this is where I want to stop and go into the what is a misaligned AGI?
What is a paperclip maximizer for a moment?
Because it actually makes Molok clearer and then Molok makes it clearer.
Do you want to construct the kind of paperclip maximizer scenario for people?
You know a lot of the people in the kind of AI risk space.
So the paperclip maximizer is like a thought experiment that is basically of like an extreme superintelligence gone wrong.
Because just because you can build something that is by definition superintelligent, you know, and that it's, you know, and if we define intelligence in what I think is the best definition, which is
the ability to optimize and navigate a very broad range of terrains in order to achieve whatever your goals are.
So if we can achieve, you know, define intelligence as that, the ability to basically get stuff done across a wide range of environments.
That does not necessarily guarantee that you also have the wisdom to decide what your goals should be in the first place.
It's called the orthogonality thesis.
The idea that like, you know, maybe intelligence and wisdom are perfectly aligned, but there's actually a very large possibility that they are completely unaligned.
You know, they're just orthogonal to one another.
And so the paperclip maximizer is like the extreme, you know, a silly
example of that and, you know, an arbitrary example, whereby you, you know, let's say you want to build
a machine, you are, you have a factory that builds paperclips.
And then you happen to get hold of a superintelligence that will help you build them as fast as possible.
So you make maximum profit.
Your superintelligence then is able to, because it's so capable at navigating a broad range of goals,
turn every atom that it comes across into more paperclips until the universe becomes tiled with them.
So yeah, that's, it's basically a very
somewhat oversimplified, but at the same time kind of comically salient example of a deeply misaligned,
but nonetheless superintelligent system.
Right. So just to construct a couple key parts of it,
paperclip is obviously a silly and kind of cute on purpose example of whatever it is, right?
Whatever commodity, whatever widget that you're optimizing for.
And without even saying superintelligence, let's just say increasingly good, increasingly competent
and generalizable artificial intelligence gets applied to the corporation, which is already
happening everywhere. And it has just two features, which is it can work to achieve a goal,
whatever its objective function is, in this case, make more paperclips.
And it can recursively improve itself so it gets better at doing that thing. That's key, right?
So of course, at first it does a bunch of stuff that we want. It figures out how to turn off lights
when people aren't there to save cost and energy and how to make more efficient supply chains and
negotiate better deals and all those kinds of things and just make some more efficient business.
And of course, the reducto at absurdum is once it has done all of the easy good stuff,
it still has the objective function make more paperclips. And then it has to start doing
stuff that is not just obviously easy good, right? Where there's some trade-offs that are
happening somewhere else, but the things that are being harmed are not part of its objective
function, right? Its objective function isn't make the most paperclips while don't doing any harm,
not do harm anywhere else, because the while don't do harm anywhere else is actually incredibly
hard to specify in an easy computational way, which is the heart of what we'd call the alignment
problem. And so if you did have something that could recursively increase its ability to achieve
a goal like that and then had enough generalized intelligence that it could out-compete anyone
that was competing against it, right? It could increase its capability faster than, say,
we as humans could. And we're like, oh, fuck, we don't want you to be making paperclips out of
our food sources. We don't want you to be making paperclips out of, but it figures out how to be
this at those games. And yes, eventually, it just starts turning everything into the substrate
for paperclips. And in general, the idea is you have an objective function, whatever the AI is
optimizing for, whatever that is. And there's different ways that people will describe it in
nuance. And it's worth reading Yudkowski and Bostrom and the other kind of seminal thinkers on
what the nature of the AGI alignment problem is. But roughly, if you have an artificial intelligence,
it is general and autonomous. Autonomous meaning it is working on its own. You don't have to keep
giving it prompts, right? It's doing its own thing. It has agency. And where you can't pull the plug on
it, right? That's a key part. And it can upgrade its own capability to do whatever it is that it's
seeking to do. And it can upgrade its capability faster than we can, because the smarter one then
is capable of making even smarter one. And we already see early signs of this. We already see
AI starting to create better internal AI functions to be able to achieve their
goals that are set for them. The idea of do we want an autonomous general intelligence
that is comprehensively smarter than us, that is trying to fulfill a goal
before we know that its fulfillment of that goal isn't going to really mess stuff up for us,
right? We're like, no, we don't want that thing, right? We would like to prevent that.
Because it's entirely possible that it could do some things that are totally not what we want
in pursuing that goal. So obviously, if its goal was to maximize GDP, there's a lot of nasty ways
to maximize GDP. It can go up with war. It can go up with addiction. It can go up with... And whatever
it is that the objective function is, there's a lot of perverse instantiations of that thing being
fulfilled in a way that totally messes up other stuff. And so the AGI alignment question is,
can we actually ensure that before the thing is truly a general autonomous intelligence,
that it is aligned, aligned with our interests, our intention, our good or something? What the
nature of alignment means is actually a deep question. I'm going to put that on hold for a
moment, but roughly aligned with us such that that much power would be a safe thing for us to have
exist. And the thought experiment of an intelligence that was, say, as much more intelligent than us
as we are than chimps or ants or whatever, looking at how our increase in intelligence has
boated for all the other inhabitants of the planet. That's a very concerning thought.
Now, this is where I want to actually use the analogy of that in autonomous.
So it's doing stuff on its own. It's auto-poetic, it's self-authoring, it's self-upgrading,
and it's orienting towards an objective function. And I would basically like to say that you could
call the current global system. And just to simplify it, let's call it global capitalism,
even though it's not that. Calling it MOLOC would be better. But let's just talk about
the capitalism part because the metrics are kind of clear. You could say that it is already
a general auto-poetic superintelligence. It has an objective function, which is to
convert as much of the world, people's creativity, ideas, labor, natural resources, everything into
capital. And so that's the paper clips, which is interesting because it has no real value,
just optionality for real value. And there's always this assumption that there's more real
value out there, but that stops being true forever. So if the tree sequesters CO2 and
produces oxygen, and I need to breathe oxygen, and it does a lot of other important things like
supports pollinators and cleans the water and stabilizes topsoil and all these things for me,
but if I cut the tree down, turn it into lumber, there's still enough oxygen
for me. I didn't actually ruin my... If I cut the whole acre of trees down, there's still oxygen.
In fact, there's no differentially seeming less oxygen for me doing that, but now I have the money
of all the lumber of these trees, and I can do real tangible shit for me and my family or my
corporation with that. And so the optionality value I get allows me to still access real value,
but I'm destroying real value in the process. So I'm going to say, well, you're not destroying it
because you're making lumber. Well, yes, lumber is actually radically less complex than a tree that
has less total types of value than it does. So we're converting the self-organizing, self-repairing
complex world into an increasingly simple or complicated fragile world that has less types
of value to less types of actors. The tree has value of many different types to many different
types of actors. So you can't just say, well, it's carbon sequestration, but no, it's stabilizing
topsoil that's a million things. Biodiversity, etc. Yeah. So when we talk about artificial
intelligence, we talk about what type of computer system it runs on. Hardware-wise, it runs on CPUs
or GPUs or TPUs or whatever it is, and what type of algorithms that it runs.
So it's interesting that we can already say humans are general intelligence.
And capitalism is running parallel process across all the humans.
Right? And we know when you think about the cloud and why parallel process was so powerful,
it's also why GPUs are so powerful. Capitalism is basically as a decentralized incentive system
incentivizing all humans to both do novelty search, figure out new ways of making money,
and exploitation. Take the existing ways and do the most of them that you can. Those that do better
at it get more influence in the system and in turn influence the system in ways that support them to
do more of it. Those that oppose the system are also opposing those who are doing well at the
system. So even though the system as a whole doesn't have agency, those who do keep and
check those that would oppose them. So it is as if the system has agency. And you go from barter
to currency, to fiat currency, to fractional reserve banking, to AI high-speed trading of
derivatives and credit default swaps. And that is basically the recursive up-regulation of the
algorithm. Right? It is getting more and more capable of doing more and more financialization
of the world to incentivize people to do more and more things. So you can see that you've got
something that is already running on all these general intelligences. And as a result is super
intelligent. It has an objective function. The objective function is misaligned with the long
term well-being of the world. And it advances narrow value metrics at the X. And it's not that
you know, but everything is getting better, pinker, rustling type arguments are like saying,
but look at how many paper clips we have. Aren't we all stoked that we're getting cheaper paper
clips? And multiplied by all the types of paper clips, like yes, narrow metrics are being advanced
at the cost of lots of wide metrics that end up being critical to either the breakdown of life
support. So you get catastrophe or the breakdown of the quality of life you get dystopia.
How would that then sort of using that definition apply to the difficulty of the alignment problem
with an AGI? So here's the thing. We think about the super intelligence. And a misaligned super
intelligence is a very scary idea. And if you want a sense beyond the silly paper clip maximizer,
how scary it is, read some of people like Elias Ryukowski and others on what AGI
misalignment means, super intelligence misalignment means.
Before we go all the way to Molok, we could already say that
collectives of lots of people interacting in particular ways, whether we say a public
corporation or say a nation state. Let's say we take a large public corporation, well beyond
what we have evolutionary history for. We had evolutionary history for tribal type size things
below the Dunbar number where everybody could talk to everybody. So possibly the human scale
effects and whatever could perpetuate through the whole thing. At much larger scale it changes,
which is why the beginning of large civilizations are to have different properties.
So you take a public corporation, who's in control of it? Kind of nobody, right? Like the
people in the corporation answer up to the executive team, answer to the CEO, the CEO answers
to the board, the board answers to the shareholders. The board has a fiduciary responsibility
to maximize profit returns to shareholders. The shareholders are pension funds and whatever,
where the managers of the shareholders are trying to get money back to the 401ks and whoever is in
there. They end up being pieces of law that are bound up through the whole thing, which is the
liability limiting status of the corporation that can privatize gains and socialize losses and the
fiduciary responsibility of the directors to maximize shareholder profit and on and on. So
who's really in charge of it? You can get rid of a CEO and put a new one in. You can get rid of a
director, put a new one in. You can sell some of the shares, get a new shareholder. It's kind of
this thing that gets set in motion where it has an objective function, which is now maximize profit
within the domain of how it figured out how to do that thing. But because it is engaging, it's
running on all these human intelligences, which are already general intelligence, so it can do
things that the humans can do plus things that none of those humans on their own could do, right?
It takes a lot of humans together to do a large hadron collider or a Hubble or an Exxon, right?
It has a capacity that nobody could do on its own. So it is super intelligent. It's beyond
human intelligence in that way, not just in a narrow way because it's engaging people that are
already generally intelligent. And then beyond just that, it's already engaging computation. So
it's engaging the narrow but very powerful data processing. And now we add AI to that.
And so we can say already that a nation-state or a public corporation is kind of a cybernetic
general intelligence that is already misaligned.
Can you just define what you mean by cybernetic? Cybernetic is just the field of study for any
type of system that kind of self-regulates, right? How the control mechanisms work,
how the regulatory mechanisms work. So the corporation has feedback loops, it has feed
forward loops, it has regulatory processes to be able to maintain what it's doing and upgrade
what it's doing, right? Nation-state has that in it. But it is dealing with internal but also
external pressures that force it to be what it is. Maybe Google didn't want to release its large
language model yet, but as soon as its business model gets attacked by Microsoft releasing one
and adding it to being in the possibility of search, then it has to. And so this is where the
multipolar trap, the Milwaukee and type dynamic comes in, is the individual organization is not
totally sovereign because it's for it to keep existing. It has to deal with the pressures
defined by others. And so either a sociopath can start something that then everybody has to deal
with or everyone assuming the other one is about to do it next and no sociopath has a situation
that is functionally sociopathic. We talk about a corporate person, 14th amendment kind of giving
personhood rights to a corporation in this weird way. If we were to talk, so this is already a
framework for thinking of it as an agent, right? The super intelligence as an agent,
but the fiduciary responsibility to maximize profit makes it kind of an obligate sociopath,
right? That kind of thing has to take the opportunities it has. So long as it is not
illegal within the confines of law, but it can work to change law, which is what all big corporations
lobby, right? And that ends up being one of the very profitable things that a company does is
rather than the regulator limiting it too much at figuring out how to get the regulator to change
regulation more aligned to its interests. And so for it to say, hey, a shareholder profit maximization
fiduciary responsibility in an oil company and solving climate change are incommensurable.
A shareholder fiduciary profit maximization in military industrial contractors and a world of
peace that would de necessitate all the demand are incommensurable, right? And so
then those competing with each other, right? So then any of the agrips are like, hey, I can't
really do safety because the other ones we all kind of have to race at this and then maybe the
whole US is we don't want to regulate it because we'd rather our guys get there before China gets
there because whoever has it's going to run the world. We'd at least like it to be US companies.
So that Molokian dynamic makes it to where each of these cybernetic super intelligences
interacting with each other creates a meta cybernetic super intelligence that you can call
Molok, right? Which is why I wanted to talk with you about it is you can see Molok as an emergent
property of the systems of incentives and the dynamics of coordination that are built into
the system where it is employing human general intelligence, it's employing computational
capabilities and increasingly artificial intelligence and the whole rest of our tech stack.
It is up regulating through competitive dynamics, but up regulating in this
narrow benefit kind of way. And so we could say this thing that is driving climate change
and driving species extinction and dead zones and oceans and coral loss and desertification
and arms races and polarization and all like that. That is a misaligned super intelligence
that nobody can pull the plug on. It's already autonomous. It's already nobody can pull the
plug and it is building AI because corporations actually the people it's not actually the people
within they think they're the ones building it. Well, but they're building it within corporations
that have a fiduciary responsibility for profit maximization that are in
multipolar traps with other companies that are racing to do it that have to look at how do we
commercialize this thing, whatever it is, right? Or they're building it within nation states that
have to be able to compete with another nation state. And what that means is that some narrow
value metrics that define what wins the competition get prioritized over wide value metrics. And so
it is fair to say that we already have a misaligned auto poetic super intelligence running the
world running all running on and running all of the people to various degrees. It is already
employing all of the computational power. It is developing more computational power in AI.
The AI is being built by it in service of itself. So the AI risk scenario that you'd Kowski or
Boston brothers put forward of the thing where you can't pull the plug, it is upgrading its
capacity to do what it does. It has an objective function it's pursuing, but it harms stuff that
we wouldn't want harmed in its objective function in the pursuit of it. What I'm arguing is that we
are already there. And it is our world system. And that AI is simply accelerating it. And that we
don't have to get to a GI to have the effect of it because you already have GI. We already have
general intelligence in the form of the corporations, nation states in the overall system
where then adding AI, even if it is not fully generalized to that system, you already have
something that is autonomous in general that is now getting increasingly potent capacities,
even if it's within a bunch of narrow domains, right? And so before we get to the case of just
autonomous AI being its own risk, the existing AI in this landscape is driving the entire risk
landscape is driving the overall is accelerating the topology that is already in place.
And this is why I said I think the misaligned AGI as a thought experiment helps people understand
MOLOC, but what the reality of MOLOC helps people understand that without getting to a
total AGI, that the nature of the risk there is already happening. Then with that, we have to
say what would it take to prevent those risks? And it's a different calculus. It's a different
way of thinking about it. So what do we do? Let me make it a little bit more tangible first and
talk about sub AGI within this context, this MOLOC in metacrisis context. What do the actual risks of
AI look like? I'll give a few different categorizations. And I want to say I am not an AI alignment
expert or a AI risk expert. There are a lot of experts at places like Mary and Redwood and
other places that I think people should listen to pay a lot of attention to. I'm familiar enough
with those arguments and then very specifically with the metacrisis argument to see how it relates
and that's what I'm speaking to here. There's something very unique about AI relative to
all other forms of technology. I'll speak to the deeper part in a moment, but to begin with
synthetic biology is very powerful, like obviously it's very, very powerful. There are awesome
applications. There are awful applications. But synthetic bio does not automatically give us
the ability to make better drones. It doesn't give us the ability to make better high-speed trading.
It doesn't give us the ability to make better nukes. Nukes don't automatically give us better
bio weapons. AI gives us better all of them. That's an important thing is that AI has the
capacity to do optimization across all the things, which means the good things, which is what we
all want. We want to have AI work on protein folding for immuno-oncology to cure cancer and on
receptor sites for new drug discovery and to make supply chains more efficient and things like that.
But everything that AI can optimize, it can also break.
We ran it in reverse, right? The AI that was doing drug discovery, I think it was Oak Ridge
National Laboratories, was ran in reverse and came up with a bunch of chemical weapons very
rapidly. It's got a minus sign in front of it, essentially. And an AI that can optimize supply
chains can also optimize exactly how to break them, right? Can optimize terrorist attacks on them,
even just through cyber and things like that. A AI that can do protein folding for immuno-oncology
can also make fucked up bio weapons. And so the first principle is that it's very hard to advance
AI, right? It takes massive GPU farms. It takes only a few companies in the world that can even
do the chip manufacturing to do that kind of thing. It takes a lot of computer science talent,
amounts of data, etc. But once it's developed and then it is connected to the internet,
like a large language model can run on a lot less compute than it's trained on. It takes
a lot to train. It doesn't take that much to run it. That's a big deal. And it takes a lot to figure
it out. Once it's been figured out and you publish the paper, and this is one of the key things is
it's like building software. It's very hard to build. You need programming knowledge on to build
a software. But once you've built it and you've built the user interface, any old schmuck can use
it and reap the benefits from it. So if you have a company like a Google or an open AI or whatever
that says, hey, we're going to put safety parameters on this, I think there's a bunch of arguments
against why that is. Even if they could do it, won't be adequate, but they can't. Because
you know, you had the, I think it was Llama or Alpaca, the Stanford meta one that ended up getting
leaked through some GitHub leak, and then somebody got it downloaded it onto a computer, started
sharing it. And that means the full power of something that probably costs tens of millions
of dollars to train is now unlocked and will be available for anybody to use for all the purposes.
So the safety is not there. There are some projects working on decentralized AI that are about to
make at least GPT 3.5 level kind of unlocked widely available. That thing is impossible to avoid.
And so the thing to understand is that it takes a lot of work to develop the new capacity. Once
it's developed, the barrier of entry to be able to do the things that it allows to be done has
been radically lowered for everybody. That all the good things come from that. We can all do more
creative stuff. That's exciting. All the bad things come from that. And so there's this principle
that we could say all technology is dual use, right? You're developing it for some positive
purpose, but it has a military application, but it's not just dual. Every technology is kind of
omni use, meaning it will get used for all the uses that people have incentives to use it for
who are capable of using it. So however much you lower the barrier of entry,
anyone who can go into that barrier of entry now will use it for the things they have incentives
for. So you develop it for we're going to cure cancer, but now you've got that ability for
biological engineering really easily available widely, right? And so I think the
couple things, the first thing to understand about AI is AI can make better cyber weapons,
better nuclear weapons, better drone weapons, better all of those things, better info weapons,
better population centric weapons. And so it increases the capacity to increase all the other
risks in a way no other thing does. You could kind of say, well, oil does that, right? Energy,
every industry needs energy. Yeah, but it's not doing the novelty search part of figuring out
new better ways to make those domains. It's simply just allowing them to do more of it, right? The
AI both allows you to scale the stuff, but also allows the innovation of way new, better stuff.
So that's novel. That's a very novel thing about it. So the first principle is anything AI can
optimize, it can break, you develop it for one purpose, it'll get used for all the purposes
that anybody can figure out how to use it for. You try to make safeties and whatever, but you
create an incentive now for a bunch of cat and mouse type dynamics on how to utilize that.
And obviously we can think about the ones that just involve synthetic media and increasing
hypernormal stimuli and ubiquitous deep fakes and really dreadful things like that, right? Like
there's in terms of population centric warfare and breakdown of government and public trust and
there's a lot that are very, very near term. So this is one set of risks, anything that
the AI can optimize, it can also break. That's kind of the bad actor case. But the other case,
the more Molachian case, is just accelerating the thing that is already happening. The externalities
that aren't included in the optimization function is accelerating the externalities when we're
already hitting the tipping points on the externalities. And so you could say, yeah,
but AI is going to make it to where we can produce things so much more efficiently that it'll actually
save the environment. Yeah. It's not that there isn't a way to do that. And those are the things
that I want us to pursue. But we're not on track for that. You've got this kind of Jevons paradox
that when you increase the efficiency of energy, you don't actually use less energy, use more
energy, because now energy is cheaper, which opens up a whole bunch of new markets that weren't
owned before. The same is true with compute. You make compute cheaper, you use more compute,
not less compute. And so AI makes some stuff more efficient. More efficient just means there are
more things to which I can apply energy to get more energy. As long as there's positive return,
we will go for it. Now, to not go for all those things, you can't do it purely incentive. You
have to do with deterrent, with agreement, blah, with some other thing, which is not
adequate and in speed to the overall situation right now. So one risk of AI is that anything you
can use to optimize, you can also use it to break, it increases all of the other risks in a way
nothing else increases all of the other risks, it increases the total complexity of the risk
landscape, etc. The other one is that even when you're using it for the positive purposes, and
you're succeeding at whatever your positive purposes are, you're also speeding up externality
right as we're hitting the tipping points. The next problem is you're increasing the info
complexity. I think Yudkowski calls it inscrutable matrices of floating point numbers, talking about
the large language models and like nobody actually knows what the fuck is going on inside of them,
right? Those kind of black boxes. So the only thing that could figure out is your AI actually
doing the thing it's supposed to do or not? How do you, if we wanted a law in some way to be able to
regulate it or adjudicate what's happening, it would take another AI that's more powerful to do it.
And so now you end up getting an increase of the race towards the infosingularity
where people can't actually make sense of or adjudicate any of what's happening, right? The
total complexity of everything is beyond our ability to process. And that just means the
unsolvability of everything increases. So what I'm saying is that if you add something as
vastly recursive and powerful as the increasingly generalized AIs that we have to an already
misaligned superintelligence near the boundary points of breakdown, that's a problem. And that
we should figure out alignment first. So what I'm suggesting is in the way that the AI risk
community is saying we should really try to figure out alignment before we race forward on
developing more powerful AIs. I'm saying, yes, we really need to figure out alignment, but it doesn't
just mean alignment of the AIs. It means alignment of the existing general intelligences, the
cyber-general intelligence. The capitalist model, essentially. So it's not fair to call it capitalism.
It's more like game theory. Capitalism just makes it easy to think about because there is
actually a metric. And it turns out that the capital can be used for population-centric warfare
or supply chain or most any of those things. It ends up being kind of a unit of power pretty
widely. But yes, that system, and it's hard to even call it a system, right? That set of
perverse incentives and the coordination that arise from it is misaligned.
Reaching criticality. Alignment in that thing has to be figured out because a misaligned
context cannot develop aligned AI. It can't emerge from it. Because that's, I think,
some people's hope, is that just give it enough capability and some emergent magic will essentially
come. This is, I think, why the orthogonality thesis is important, which is to say it is possible
to get very good at optimizing and not get good at picking good goals. Those are two
separate things. And we already see that in the world. We're already much more good at creating
tech than we are at creating a world that everybody thinks is a world that really makes sense.
So this is the exponential tech gives us the power of gods. We have not yet seemed to demonstrate
the love and wisdom of gods needed to bind it. We have to figure that thing out or this thing
kind of caps out. Same with the AI. Right. And we've handed it over to the shitty god.
The worst one. So AI
has fast enough feedback loops and enough power that it is one of the only things that could help
change the other thing in time. Change the paperclip maximizing nature of the global system,
right, of the MOLWAC system in time. But only if it was developed in association with the cybernetic
systems that were actually aligned. And aligned here doesn't just mean with our intent. Aligned
means actually with our long-term well-being. And this is one of the critical issues is when
we talk about alignment, aligning AIs with human intent would not be great because human intent
is not awesome so far, right? Like that's kind of the point is that whether we are looking at the
overfishing of the oceans or proxy wars or whatever it is, we're like, do we want to give exponentially
more power to this species? With its intent caught in multipolar traps the way that it is,
it does not a good steward of power. In the Bronze Age it wasn't and the Iron Age it wasn't,
it still isn't. But with exponentially more power, exponential externalities and exponential conflict
both eventually break the finite playing field. And so how do we, in solving those coordination
failure, multipolar trap type dynamics are necessary for the wisdom to be able to prevail
adequately? So I am very hopeful of the very, of the uniquely positive things to support
coordination that computational capabilities and artificial intelligence in particular can help with.
They are not being developed for those purposes and in contexts that have the right frameworks
and the right incentives currently. It's very much the opposite thing. So rather than build them
to be able to change Molok, they're being built by Molok in its service. Even though no one building
them would say that, but the nature of the capital that they are building it with has that
built right into it. Inherently, yeah. So the sort of aesthetic of the thing that to me is
the anti-Molok, but I don't even like to call it the anti-Molok because it is something that
operates that. By calling it the anti-Molok, it says it's on the same plane as it, the same
dimensionality and it's something higher than that, is this thing, you know, if Molok is the
god of lose-lose games, negative sum games, what's the god of positive sum games, I call it win-win,
everyone has different names for it, you call it Omnia. Is that the direction of the type of
artificial intelligence we need to build and if so, how would we go about doing that?
Could we use a whole bunch of info technologies,
including artificial intelligence, including we can already see that all the problems that we saw
in the social dilemma that you show in your second Molok video and some in beauty wars,
the social media related ones, well, that is actually already a certain kind of AI, right?
It's AI that is curating the news feed aligned with an objective function. The objective function
is either time on site or engagement or something like that, some combo of metrics. And so of course,
if the objective function is to maximize your engagement, things that are addictive will do,
things that piss you off and tribalize will do. And so it gets to take, now what's interesting
about the Molok part is so far Facebook or TikTok or whatever has not been creating its own content,
but it has been incentivizing all people to create content that will rank and as the people pay
attention to what ranks or doesn't, as you saw, beauty filters were one thing and there are a
lot of other things, even so far as that all of the legacy media now does stuff that will make
it rank on Facebook and Twitter and whatever, because that's increasingly where the eyeballs
are coming from. So even that which would seem like an alternative source is still actually
influenced by that thing. So the, to be able, because it's so powerful to direct all human
attention at the thing, right? So whatever the algorithm is, it's going to direct all human
attention is also going to direct all the innovation in the direction of what wins that algorithm.
And so, and then because it's customizing the newsfeed for every person, it's split testing,
what do the people click on and engage with and share and et cetera. So it's basically just
objectively maximizing for personal engagement without paying attention to if it is positive
or negative reward circuits and happens to be that negative reward circuits are easier to
hack than positive ones most of the time. And because the positive ones you want to get the
fuck off the computer and go to other stuff and then doom scroll, but the doom scroll mass
share thing the negative reward circuits feed better on. So that's already an example
of a bad objective function and an AI that has made it to where there are almost no
adolescent girls with a good body image, right? Dysmorphia is kind of ubiquitous. It has made
it to where polarization is as extreme as it is and on and on like the externalities are
massive, what it's done to attention span. And now with synthetic media, we're talking about not
just an AI that can curate but can create. Now you can imagine a feedback between those when
one is creating things, it will be maximally sticky to you based on personal dynamics,
split testing, multiple created ones. And the other one is curating, but now not just getting
all the humans to do decentralized creation, but also you can see that kind of feedback.
But of course, if we had, if we change the objective function of social media as AI, right?
And rather you did things like it's not just how much engagement it gets, which can be fighting
that is polarizing, but something that say gets positive engagement across political divides and
ideological divides and a meta clusters, which is stuff that we could do, right? We could do,
we have the info science to do that, which means that it's identifying places where there is
shared agreement or shared perception and up regulating that rather than the most divisive
stuff. You'd have a totally different world and the technology could totally do that.
It would not be as good for ad sales right now because it actually wouldn't keep people online
quite as long. And so this is where the fiscal model fucks up the application of the tech because
we're not just saying all social media is bad. We're saying the incentives make us develop the
fucked up versions of it. Right. And so we could make social media that was exposing people to
different ideas rather than reinforcing their existing ones and growing their network to
include people of very different types. And like, so some of the rewards them to go out and touch
grass, essentially, and, you know, almost rewards you for time spent off the app or something like
that. There are probably two. There are a lot. I mean, now one of the challenges is, of course,
the ad company can say, we're just giving people what they want. Right. But it's manufacturing
demand on bad reward circuits in the same way McDonald's can say, we're giving people what we
want, but you have a much more obvious reason after it. Exactly. Right. And so on the other side,
you're like, well, it sounds kind of paternalistic that we should pick what are the good reward
circuits. And but it's like, no, if you're influence, if you have that much asymmetric influence over
humans to not take some responsibility for what the statistical changes in their life are, is
actually silly. Right. Now, do we want the corporations to do that themselves? Do we want
the government to do it? We don't trust any authorities adequately right now for good reason.
So there's some very deep conversation around how do we ensure that the power of that technology
is optimizing for things that actually increase quality of life in meaningful ways. But the
same with all new types of AI, could we use it to radically improve governance where we could
actually be able to have the large language model see what people's beliefs and sentiments across
the entire space are and find the topics that actually a lot of people agree on that super
majorities would agree on and start there and be able to make platforms for candidates to actually
be able to represent the wills of the people better because we actually can see what they
feel and believe at scale. And would it be possible for it to work on identifying things that a lot
of people would believe or at least for it to give information about the stack ranking of the
distribution of values to a proposition crafting process so that it could craft better propositions.
So we're not saying that there are not awesome applications including maybe
maybe the AI applications that are critical to be able to fix these MOLOC dynamics because they
can actually help coordination. But if you are not trying to actually understand the coordination
failures deeply enough and say what should we really be trying to solve in terms of fixing
coordination that makes this whole technosphere compatible with the biosphere, compatible with
human nature, compatible with meaningful definitions of human flourishing. If it is as
powerful as it is serving anything other than that the externalities in those areas will become
increasingly catastrophic or dystopic or both. Are there any like promising projects that you think
are trying to harness AI in this sort of again I don't like to say anti-MOLOC in this win-win way
or you know using info technologies in a way that is more aligned with what is actually good for
human nature in the biosphere? I think we can see like Audrey Tang's work in Taiwan with creating
digital democracy where they look for unlikely consensus and they're starting to apply large
language models and things like that I believe. And you know a lot of the work that say the Ethereum
and some of the Web3 communities try to do with public goods is obviously thinking about some
elements of that and it's not that nobody is but nobody with the giant capital and GPU farms and
etc has that as their primary objective and the primary objectives of all of those ones have
MOLOC and dynamics involved even if they also have some good dynamics involved and that's
problem and that's what I would most hope to start to shift is in recognition of the total
power that is there, the total number of things that will be affected by it, the downstream
and the order of effects of it, the speed, the nature, the irreversibility and there is a precautionary
principle thing that uh who's right? Yudkowsky or Cristiano on alignment or whatever it is? Well
given that most of the people who have studied AI alignment very deeply are concerned with the
pace and direction with which we're moving that's a pretty good sign that we should pay attention
and where there is disagreement between experts but where there's where there's radical uncertainty
but also maximum consequentiality and irreversibility go as fast as possible is not the right answer.
What if there was a way to just remove the competition side of it entirely like I know it
sounds very pie in the sky but like if all the major companies you know which yes let's say the
companies themselves are by definition misaligned or like aligned with MOLOC but if the companies
would essentially hang up their competition hat not compete and work together as a sort of single
entity almost like a you know a collaborative science project essentially would that solve the
problem? Fascinatingly when we talk about the way that laws get built that end up supposedly
having good purposes and they do but they also end up being part of MOLOC like the fiduciary
responsibility the director or whatever currently I think antitrust laws would try to prevent that
thing from happening that you're mentioning and so the government would actually have to get involved
but would I like to see the major AI labs together with the major academic researchers
together with the regulators take very seriously short medium long-term futures of AI that factor
all these types of assessments that factor the MOLOC end dynamics the metacrisis the way that the
AI that is the safeties won't totally last right that will get decentralized the safeties
will be taken off it will be used for all purposes the kind of omni-use nature of it
and to look at what does responsible movement forward in light of that look like I don't see
any good answers that don't involve that. Yeah because that's the thing but the one thing MOLOC
requires you know one thing game theory requires is competition of some kind some kind of either
fabricated or real scarcity and in this instance there isn't really you know there are no
intellectual powerhouses trying to compete to be the first to do a thing everyone is working
on the same project together it's the ultimate cooperation thing but yes it seems like a very
almost intractable you know method because it would actually need to be an international
collaboration as well it's not just you know I mean we do have the one advantage that it seems
like the majority of the major players at least are all in western hemisphere and they all speak
the same language and to degree share similar values you know we're not well maybe not so much
that but that is one starting point. There's some very advanced AI labs in China
and the government run ones versus the corporate ones are slightly different there's obviously AI
that is not just large language models for public deployment that is still also risky for many other
purposes I think the the large language models for public deployment is a unique case because when
the whole world starts utilizing them for the many many purposes they will it'll become nearly
impossible to roll it back once we see the risks that are associated because we will have created
so much economic and cultural dependence on it so I think even if all we're talking about is
as a start all the major GPU farms and large language models being in cooperation that would
be a thing but I do think it does need to be wider than that and not just corporate and international
and I think the I don't think there are any existential risks near term if we don't
I don't think large language models destroy the world I think there will be problems and
then we'll create solutions to those and whatever but I think it absolutely accelerates the overall
metacrisis across many many vectors which is why one has to actually take that seriously to then
really try to take responsibility for this thing for which there is so much incentive to not take
responsibility. Absolutely is there any like specific call to action you want to put out there
if there happened to be any you know major technologists listening to this?
I mean to begin with it's really to engage with the wider risk arguments not just in super
intelligent AGI taking off but the acceleration of all of the risks that the release of these
things cause to really engage with those arguments seriously and to really think about world that's
creating and be like all right despite the humongous incentive for me to rush ahead with this
is the risk calculus high enough the speed and the irreversibility high enough that I am actually
inclined to figure out something to figure out better coordination around this and to simply
start engaging in more percentage of their total energy going into risk analysis than opportunity
advancement to have that more attached to the actual governance and choice making. I think
all companies working on AGI should kill the fiduciary agreement to maximize shareholder profit
obviously some have talked about that I think that should be killed and I think the earnest real
engagement between the AI labs the people in AI safety research and regulators should be a very
actively happening thing. Don't be mollicky. You can print me one of those bumper stickers.
