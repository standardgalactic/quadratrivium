Music
Welcome back to meteorology.
In these lectures, I have shown many simulations
made using numerical models of a variety of types.
Even some of the observations I've shown,
like average 500 millibar height in January,
average monthly sea service temperature,
actually involve models.
Observations are blended with model forecast output
to create what we call reanalyses.
In this lecture, I discuss how numerical models work.
The discussion will encompass how we make predictions
and what the limits of predictability are.
And along the way, we'll gain some insight
in the difference between weather and climate.
So let's get started.
The model recipe starts with equations.
Amongst them, Newton's second law,
force equals mass times acceleration.
This is the equation that governs winds and flows.
The forces include real forces we've seen,
like pressure gradient force, gravity and friction.
And those are parent forces, Coriolis, centrifugal,
and the curvature force we talked about with Big Bertha.
Combinations of these forces produce things like
geostrophic and gradient balance,
which gave us low pressure to the left in the northern hemisphere
counterclockwise flow around cyclones.
Next, the first law, thermodynamics.
This describes how temperature changes in response to heating
and volume changes.
Third, the ideal gas law,
which relates pressure, temperature, and density.
And last but not least, the Clausius-Clapperon equation.
It sounds like a disease, but in some ways,
it's the most important of all.
It's the relationship between temperature
and saturation vapor pressure,
and this determines when water substance changes phase.
Next, we choose our domain,
a three-dimensional volume of the Earth's atmosphere.
The domain can be local, regional, or global in scale,
depending on our interests and resources.
There are many different kinds of models
and modeling strategies, but I'll describe a common one.
It involves partitioning the volume into grid boxes
or grid volumes, and the points of intersection
amongst the boxes are called grid points.
It's like a massive Rubik's Cube,
or rooms in a very large hotel,
except the walls in this hotel are virtual and not real.
Next, we need data for each grid point and each grid volume.
Information like temperature, pressure, humidity,
wind speed, and direction.
These data come from surface stations,
weather balloons, satellites, radar ships, aircraft,
and our imagination, because our eyes are not everywhere,
and what we don't have, we must invent.
With the initial data and equations and a repertoire,
it's time to make some forecasts.
The first thing you should realize is this.
Forecasting is extrapolation.
That word should worry you somewhat.
Let's say we know values of temperature, pressure, et cetera,
at some time I'll call t, which represents now.
How do I know these values from observations
or a previous forecast?
We need to calculate the tendencies,
how quickly the values of temperature, pressure, whatever,
are changing now,
and we get these tendencies from our equations.
Next, we extrapolate from time t to time t plus delta t,
delta t being sometime in the future,
it's our forecast time step.
Our time step is seconds, minutes, hours, whatever,
depending on the specific application.
And then we recalculate the tendencies at time t plus delta t
to extrapolate on to time t plus 2 delta t and so on.
In this way, we see that forecasts may start with observations,
but subsequent forecasts are based on previous forecasts.
So forecast quality depends on how good the initial data are
and how accurate our tendencies are.
And I think it's easy to see that forecast quality
can degrade with time.
All we have to do is be off each time step a little bit,
and that will add up in the long run
and maybe even the short run as well.
So let's do an example.
Let's say you're measuring temperature in your backyard
and you're plotting it as a function of time.
And it's been rising.
And the red dot on this figure indicates the present temperature.
And you're wondering how hot will it get later in the day?
You want a forecast.
The model forecast for your high temperature
will consider a very large number of factors
that can affect temperature in your locality,
whether there's warm or cold advection,
is a sea breeze or a thunderstorm gust front approaching
to have upslope or downslope flow nearby.
Are you subjected to subsidence or ascent?
Are clouds gathering or parting in your vicinity?
How is the angle of the sun in your backyard changing with time?
The model equations will use all that information
to determine now how quickly temperature should be
changing at this point in time
and then extrapolate that a short distance in the future
making a forecast.
So as an example, let's say the temperature
is 20 degrees Celsius in your backyard
and your model tells you that the temperature tendency
is one degree C per hour.
If your forecast time step is one hour,
your forecast for one hour hence is 21 degrees C.
But you in your backyard,
you don't have all that information available.
So you might guess that the temperature
will just keep increasing
at the same rate it has been for over the morning.
You can also extrapolate into the future.
So let's draw a line tangent to your temperature curve
and extend it ahead a little bit.
That's your forecast time step.
And now you wait to see how accurate your forecast has been.
The model, however, won't wait.
The model will use its forecast
that's just made to recompute the tendencies
considering all those factors I mentioned previously
so it can make the next forecast.
Every time step, each model forecast depends
on how good the previous forecast was.
Well, let's say here's how temperature
varied in your backyard for the rest of the afternoon.
And it turns out your forecast wasn't too bad.
Just a slight overestimate of the actual temperature.
But suppose you extrapolated your tangent line out farther,
taking a longer forecast time step
since after all, forecasting is difficult work
and you may want to minimize the number of calculations
that you're making.
So this would have been your forecast instead
and it's way too high.
This demonstrates the forecast time steps can't be too long
because tendencies have the tendency to change.
The numerical weather prediction,
or NWP model forecast for your locality,
will calculate radiative energy balances
between incoming solar and outgoing far-infrared radiation.
It'll compute how water vapor becomes cloud droplets,
freezes into ice, agglomerates into snow,
compacts into grapple, falls within the cloud,
melts into rain and evaporates back to vapor.
The model will create, deepen, move and weaken
things like troughs and ridges
and extra-tropical and tropical cyclones.
It will form, move and dissipate frontal
and other kinds of conversion zones,
from sea breezes to large-scale cold fronts,
warm fronts and dry lines.
It will prognose ascent and subsidence
and upslope and downslope flows.
It will compute surface friction,
surface fluxes of heat and moisture
from the soil and the ocean and turbulent mixing,
and much, much more.
All of these things can potentially influence
how temperature will change over the next few hours
or days in your backyard.
How models handle these processes and how well
depends on the model's resolution, its grid spacing.
If the model cannot resolve processes or features,
then we call them sub-grid, smaller than the grid scale.
First of all, let's see what we can see on the grid.
Things we might try to resolve in our model include
clouds, mountains, lakes and rivers,
hurricane eyes and rain bands,
fronts and dry lines, tornadoes, whatever is important.
But we need at least two grid boxes across a feature
for the model to even see it.
And in practice, we need at least six grid boxes
for a feature to be handled in something
that's remotely close to called properly.
Let's see why.
I think we have a simple wave.
It can be a water wave, a planetary wave,
which ridges and troughs,
or some fluctuation in temperature
or pressure across space.
Maybe it represents some hills or valleys.
Wave-like things are ubiquitous in nature,
and it's easy for us to see the structure.
The model, however, samples this wave only at its grid points.
And suppose there were four grid points
across each trough and ridge pair.
The model doesn't see the smooth undulating structure
that we see. It connects the dots with lines.
It sees something like this.
It could be better.
Six grid points across each wave
would have provided a better representation of this feature.
And it could be a lot worse, too.
Look what happens when we only have three points
across each wave.
That doesn't look much like our original wave at all, does it?
And what if we had only two points across each wave?
Depending on where those two points
actually are relative to the trough and ridge,
we might not see it at all.
I showed this beautiful and terrifying
NOAA satellite picture in the last lecture.
This is Hurricane Katrina,
bearing down on the U.S. Gulf Coast.
This is a relatively high-resolution image.
The picture is made of pixels,
box-shaped picture elements,
that are like grid boxes in a numerical model.
Each pixel in this image is very small,
on the order of a couple of kilometers or so.
And as a result, we can see a lot of detail,
not only in the clouds, but also in the landscape.
We want as much detail in our models as possible.
But high resolution.
High resolution is very expensive.
Suppose I wanted to cover the Earth
with one kilometer grid boxes.
And in some ways, think about that.
That's pretty coarse resolution.
A one kilometer grid won't see your house, trees,
all but the widest rivers and things like that.
If they're important, the model's not seeing them,
they're sub-grid.
But covering the Earth with one kilometer boxes
would require hundreds of millions of points
and trillions of calculations,
and those calculations take time.
Initializing the model, running the model,
analyzing and distributing the model output,
all take time.
The more grid points you have, the more time you need.
And what's more?
The smaller your grid boxes,
the more time step that you're allowed to take
because you cannot extrapolate out as far into the future.
So the number of calculations required
actually increases exponentially.
We need to get the model forecast for tomorrow done
before tomorrow actually comes.
So we start compromising.
And the first compromise is to decrease the resolution.
Here's a somewhat lower resolution view of Katrina.
The pixel is now maybe 10 kilometers in size instead,
and we can still tell it's a hurricane,
but the sharpness has been degraded.
And we can still see the eye,
but a lot of the small features have been merged
or otherwise been lost.
Let's continue to compromise.
Many models have to use something like a 30-kilometer grid,
19 miles on a side,
in order to run quickly enough.
At that resolution, we can't even see Katrina's eye anymore.
We won't resolve her fastest winds and her heaviest rains.
And a lot of those little clouds we saw over the southeast U.S.,
they're gone too.
Well, more compromise.
At what point would we not be able to tell
that's a hurricane down there
if we had not known it from the start?
This is the world as seen through the eyes of global models
not so long ago.
It's smudged, myopic.
This is the way the world looks to me without my glasses.
Such a model is only capable of getting the very largest features right,
if that.
But models are improving quickly.
Not only because better and faster computers are available,
but also due to technological improvements.
More and better input data from satellites, for example.
Better ways of processing those data.
Strategies for using more computers all at once,
like musicians in an orchestra,
each doing his or her part of the concerto.
New and better numerical techniques for solving our equations
and higher resolution,
so there's less subgrid scale stuff to worry about.
But, and you knew there was a but coming,
there will always be features and processes we cannot resolve.
And many of these subgrid phenomena are very important
to local, regional, and global scale atmospheric circulations.
If we cannot resolve them, we must parameterize them.
That's a word that may be new to you, parameterize.
This isn't a Greek or Latin term.
We've had so many Greek terms in this course so far.
It's a scientific term,
and it means, in essence, to make things up.
In essence, parameterization is an attempt to represent
what we cannot see based largely on what we can see.
Parameterization is in a typical weather forecasting model,
include and are not limited to boundary layer processes
and subgrid scale mixing.
Cloud microphysics,
which is when we can resolve a cloud,
but we can't follow every cloud drop,
every raindrop, every ice crystal, etc.
Convective parameterizations.
We use these in situations when we can't even resolve clouds.
We know there are clouds inside our grid boxes,
and we know they're important because they're causing heating,
they're causing cooling, they're moving mass around,
and we need to incorporate the effects of these clouds
in our models, but we can't see the clouds,
so we have to infer their existence,
and we use convective parameterizations to do that.
Surface processes, like heat and moisture fluxes
from the land and the sea surface.
Subsurface processes, like soil models and ocean layers,
how does heat and moisture move through the layers of the ocean and the soil?
This is a form of radiative transfer,
including how radiation interacts with clouds.
Here's another example, or another view of Katrina,
as it was moving towards landfall.
This time, however, I'm focusing on all those small clouds over land,
Georgia, Florida, Alabama.
In lecture 11, I discussed roll clouds,
which form over land owing to uneven surface heating.
The roll clouds represent the process of boundary layer mixing,
and this is important.
Let's see why.
Consider the sun warming the land during the day.
With uneven heating, wind and a little bit of vertical wind shear,
roll-like circulations can start going.
As the land heats up, the rolls get deeper.
The rolls are accomplishing what heat conduction cannot,
mixing heat vertically from the heated ground
to the much more poorly heated air above.
Remember, air is a lousy conductor of heat.
The heating is strong enough.
If the lapse rate becomes steep enough,
if the vapor supply is high enough,
clouds will form above those roll-up drafts,
and we see those roll clouds in satellite pictures.
The clouds themselves may not be important,
and they may be important as well,
but in a sense, being able to simulate the roll clouds
in a model means we've gotten many things right,
radiation, winds, mixing, saturation processes, et cetera.
But in many models, the clouds and the mixing
that created them are sub-grid.
But that mixing is important.
It influences surface temperature.
It alters relative humidity and cape and stability
and all those important things.
If we can't resolve it, we must parameterize it.
So NWP models are full of parameterizations,
each a potential model shortcoming,
each a possible source of problems,
of error, of uncertainty regarding the future.
We've discussed how models are made,
initialized, and executed.
Let's shift our focus now back to the beginning,
to the birth of numerical weather prediction.
In a 1901 paper, Professor Cleveland Abbey,
the first head of the U.S. Weather Bureau,
presented this confident view of the future.
He said, and I quote,
there is a physical basis for all meteorological phenomena.
There are laws of mechanics and heat
that apply to the atmosphere,
and as fast as we acquire the ability to discover
and reason out their consequences,
we shall perceive that law and order prevail
in all the complex phenomena of the weather and the climate.
We shall perceive that law and order prevail
in all the complex phenomena of the weather and the climate.
Certainly a very bold and confident view.
In 1904, Wilhelm Björknas,
who later with his son Jakob described
the extra-tropical cyclone life cycle,
put forward his vision for scientific weather forecasting.
Björknas lamented the unscientific basis of meteorology
at the time, which, in contrast to astronomy,
astronomy was making precise predictions of orbits in eclipses,
and meteorology wasn't doing anything.
His goal was to make meteorology a more exact science.
According to Björknas,
the two key ingredients to rational weather forecasting were,
first, sufficiently accurate knowledge
of the state of the atmosphere at the initial time,
and then sufficiently accurate knowledge of the physical laws
that govern how the atmospheric state evolves.
Björknas identified seven fundamental variables,
temperature, pressure, density, humidity,
and the three wind directions,
and the equations that calculated their tendencies.
But these are very nasty equations.
They don't have simple solutions.
Only numerical methods could be brought to bear on them.
Lewis Fry Richardson was among the first
to try to solve them numerically,
producing the first numerical weather forecast.
His prediction was stunningly incredibly wrong.
Richardson painstakingly laid out his grid,
collected his observations,
created novel numerical approximations for his equations
that we still use today,
and crunched his numbers all by hand,
using only a slide rule.
You know what?
It wasn't even a forecast.
Actually, it was a hindcast
that took laborious computations
using old, dusty, tabulated data.
And its result?
His forecast?
A predicted surface pressure rise of 145 millibars
in only six hours.
And the reality was,
the pressure hardly changed at all.
What went wrong?
Richardson extrapolated too far.
His technique made monsters out of meaningless oscillations
that happen as air wiggles up and down in a stable atmosphere,
such as all the little wiggles you see
on this pressure trace with time.
Far from hiding this blown forecast,
Richardson revealed the details in a book
entitled Weather Prediction by Numerical Process,
published in 1922.
And undaunted,
and perhaps confident of future progress,
Richardson imagined his pencil and paper technique
applied to the entire global atmosphere.
He actually described a huge circular amphitheater
in which calculators, human beings in those days, of course,
calculators would do arithmetic
and guided by a conductor at the center of it all,
who would use his baton to control
how calculators would pass the results around to their neighbors.
So calculators would be sitting there,
pencil and slide rule furiously making calculations,
and then they would take their calculations
and pass them over to their neighbors
in whatever way the wind was blowing.
That was infection.
Richardson's confidence was not unfounded.
Advances by mathematicians and computer scientists
revealed the flaws of his techniques
and created the means to realize his vision,
the digital computer.
In the 1950s,
the meteorological observation network was expanding rapidly,
and so was computing power.
It seemed only a matter of time
until the vision of Abbey, the goal of Bjorkness,
the dream of Richardson became a reality.
And then in 1962, Ed Lorenz did his little experiment,
and everything changed again.
Lorenz's model wasn't even a numerical weather prediction model,
not really.
It didn't even have grid points.
It was three simple equations
intended to describe fluid flow in a cylinder heated from below.
He called his variables X, Y, and Z.
X indicated the magnitude and direction
of the overturning motion.
As X changed sign, the fluid circulation reversed.
Y was proportional to the horizontal temperature gradient,
and Z revealed the fluid's vertical stability.
Three simple equations.
But in important ways,
there were like the equations we used in weather forecasting.
First, they were coupled.
We see the tendency in the X equation
the very first equation.
Look at the left-hand side.
That's dx dt.
That is the tendency how X is going to change
as a function of time.
And if we look at the right-hand side of this equation,
we see that the tendency of X depends on X itself,
but also Y.
And Y's tendency, the second equation,
depends on X and Z.
Second, they have nonlinear terms,
places where the variables are multiplied together.
X times Z appears in the Y tendency equation.
X times Y appears in Z's equation.
The nonlinear terms prove to be the source of the problem.
This is a plot of one of the variables, X versus time,
for a simulation similar to Lorenz's original experiment.
X was the variable that indicated
the circulation strength and magnitude.
The model was started with initial values for X, Y, and Z,
but they weren't very well-balanced values,
so the model experienced a shock on startup.
The model was scurrying to find a suitable balance
that all X, Y, and Z would be happy with.
After the shock, there was a spin-up period
in which the fluid circulated in only one direction,
clockwise or counterclockwise.
It doesn't matter which one,
with waxing and waning strength.
But look carefully.
The swings between the faster and slower motions
are becoming larger and larger,
until the fluid started chaotically shifting
from clockwise to counterclockwise and back again.
The system spent some time in one circulation direction
and then suddenly lurched back to the other,
only to keep lurching back and forth,
back and forth as time went on.
But actually, that wasn't Lorenz's discovery.
This was.
He ran the model twice
using slightly different initial conditions.
For a while, the solutions were indistinguishable.
But after a while, they started diverging.
On the movie, the two solutions are indicated
by the red and blue curves.
And these two solutions ultimately represent
two completely different forecasts
as we look farther out to the future
regarding the state of the system.
One of those forecasts might be predicting
that the system was circulating slowly counterclockwise.
The other quickly clockwise.
If one forecast was right, the other had to be wrong.
And since the only difference between the two
were very slightly different initial conditions,
what were the chances that both forecasts were wrong?
That chance was very high.
We anticipated earlier in this lecture
that forecasts might go bad,
because they're based on previous forecasts.
So if our model contains some errors, some limitations,
we'll start making bigger and better mistakes.
Lorenzo's model showed it's worse than that.
Even a perfect model, even a perfect model,
and we're presuming Lorenzo's model is perfect
for what it's trying to do,
even a perfect model will produce completely different forecasts
from initial conditions that are only slightly different.
The differences were bred within the model's nonlinear terms.
We're small variations after repeated multiplications
over and over again every time step
could grow to become large differences.
And because there are always flaws in the initial data,
as well as always flaws in how we make and run our model,
in our approximations, our parameterizations,
our numerical techniques,
Lorenzo's experiment demonstrated
that there is a limit to predictability.
And accurate, long-range weather forecasting
will never be achieved.
This result represented the birth of, quote,
chaos theory and, quote, sometimes termed the butterfly effect,
because small differences, such as the flapping of a butterfly's wings,
could potentially amplify to exert a great effect.
The scientific term for this
is sensitive dependence on initial conditions.
Sensitive dependence on initial conditions.
In many people's minds, chaos means random,
but actually Lorenzo's model was not random.
It actually was deterministic,
and the title of his paper was Deterministic Non-Periodic Flow.
You may be wondering,
if we cannot produce accurate weather forecasts for next week,
how can we trust climate forecasts for the next decade or the next century?
And the answer is simple.
Weather is not climate.
Let's take an example.
Here are two forecast solutions made using the Lorenzo model
For sake of example, we can interpret one sign of the variable X
as being warm and sunny weather,
and the other is cold and rainy.
One forecast is predicting nice weather
while the other indicates bad weather is happening.
And you can decide which one is which for you.
But if you look closely,
and you average the two runs during the period where they disagree,
if you average the period while they're oscillating seemingly out of phase,
note that the mean conditions are the same.
In other words, the climate hasn't changed.
And that's what the climate is.
It's the mean of the system.
As it fluctuates around back and forth, that's weather,
but the mean is climate.
Now climate models used to look into the distant future,
they have their own flaws and limitations,
and everyone will admit that.
But we don't use these climate models to try to forecast the weather
at 9 a.m. in London on a day two decades into the future.
That would be an exercise that is less than useless.
We do use them to try to determine if the statistics of weather,
that's what climate is,
if the statistics of weather will change in the future.
Numerical weather prediction is a great tool
for understanding our present and predicting our future.
It takes the equations of the atmosphere
for its important properties like winds, pressures,
humidities, temperatures,
and then it integrates them forward in time.
The model calculates tendencies.
Tendencies telling us how pressure, temperature, and winds
are changing at a precise moment in time.
And so the model needs initial conditions,
and it needs a time step,
and it uses initial conditions and the tendencies it calculates
to project forward in time.
It uses extrapolation to make a forecast,
and extrapolation is inherently dangerous.
Models are limited by resolution first and foremost of all.
Even if we knew everything about the physics of the atmosphere,
we may not be able to represent all of that in our models.
There are things we can resolve, large-scale cyclones, for example.
There are things we should resolve,
squall lines, cloud clusters, troughs and ridges.
There are things we cannot resolve, the condensation process itself,
how cloud droplets become rain,
vertical mixing over strongly heated land,
how sunlight courses down through the atmosphere.
What cannot be resolved,
what cannot be handled properly in equations,
must be parameterized.
Parameterization is trying to explain what we cannot see
based largely on what we can see.
And because what we can see and what we can resolve
is the only information we really have anyway.
Parameterizations come in all forms, simple, complex.
Some are realistic, and some are really little more than wishful thinking.
And they, along with the errors and gaps in our initial knowledge
of the atmosphere's initial state,
introduce errors into the model and into our forecasts.
And since successive forecasts are based on previous forecasts,
forecast error will increase with time.
But it's worse than that.
There's a limit to predictability, at least for the weather.
That's what we learned from the Lorenz experiment,
sensitive dependence on initial conditions.
Even if the model is right,
even if the model is error-free, perfect in every way,
small discrepancies in our initial measurements
will grow to produce completely different forecasts
at some time in the future.
And that time would probably come a whole lot sooner than we would like.
For the weather, it's about a week or two at most.
That's the limit to predictability.
Our models are improving.
Resolution is increasing.
Input data is getting better and more complete.
More is on the grid instead of unresolvable, sub-grid stuff we have to parameterize.
And parameterizations are getting better, too.
And it's all worth doing,
but will uncertainty be our undoing?
That's a question that we need to think about,
because our models or observations are imperfect,
so our forecasts will be imperfect.
In lecture one, we looked at a case called the perfect storm
for the final lecture.
We'll continue looking at the imperfect forecast.
