This lecture rounds out our presentation or exploration of the sort of process style
of thinking as an alternative to, I really prefer, an enhancement and complement to the
atomistic style of thinking.
In the last lecture, I ended by noting that the idea of evolution opened a particular
door.
Well, I didn't use exactly that language, but the idea of evolution opened up multiple
doors, but the one that I focused on at the very end of the lecture was that it opened
the door to attaching probabilities to fundamental natural processes.
This is another major idea innovation in the 19th century that is fundamental to 21st
century science and that flies in the face of, the so to speak, the historic heritage,
the heritage of modern science, which is rooted, as we saw in multiple early lectures, which
idea that deductive reasoning is the basis of reasoning to truth, that theories that
claim to describe reality, to give us knowledge of reality, must have this deductive character,
which means that the science, the knowledge has to have a, the description of nature that
our theories incorporate has to be deterministic in character, that the relationship of causes
to effects is, as I mentioned in the last lecture, the same as the relationship between
premises and conclusion, that events in the natural and the social and the human world
must be related to earlier events as causes are to effects in, for example, the treatment
in physics of moving material particles, and those reflect the relationship between premises
and conclusion.
So this is a fundamental theme of modern science.
This determinism is a fundamental theme of modern science, which means that if you claim
that probability, that natural and social phenomena and human phenomena, our decision
making, for example, that they are fundamentally stochastic, a nice word which refers to statistical
in character, probabilistic, then you have to redefine what the words knowledge, law,
truth, reality mean.
It's not a matter of adjustment.
You have to redefine what you mean by knowledge if knowledge is no longer universal, necessary,
and certain, because if nature has a fundamental, fundamentally stochastic character, if natural
process, if elementary natural processes have a stochastic character, then the knowledge
must also have a stochastic character.
The theories must be stochastic theories.
And then we cannot have universal, necessary, and certain knowledge of nature.
We cannot have a strictly deterministic theory of nature.
What does law mean if it is not deterministic?
What does it mean to say that there is such a thing as a statistical or stochastic law?
It sounds like an oxymoron, a contradiction in terms.
Law already incorporates this notion of rigidity, that given a certain set of circumstances,
the law means that what follows follows necessarily.
There's a really wonderful comment.
It is an insight into a way of thinking that when it occurs, there's a kind of a delicious
gem in Galileo's dialogue concerning the two great world systems.
The book that got him into so much trouble in the 1630s with the Inquisition, one of
the characters, his hero character, the one who embodies the Copernican theory and who
is, so to speak, Galileo's mouthpiece, says that while God's knowledge is infinite, quantitatively,
so to speak, qualitatively, when we know anything, we know it the same way God knows it.
That's because knowledge for Galileo, as for almost all of his peers among the founders
of modern science, knowledge was deductive.
Knowledge is universal, necessary, and certain.
You can't be any more certain than that.
If we know, if we have knowledge of a phenomenon, for example, of knowledge of a phenomenon
in nature, then that knowledge is as pure and as perfect as God's knowledge of that event.
Galileo acknowledges that God knows infinitely more than we do, but the intensively speaking,
qualitatively speaking, knowledge has this absolute character.
That attitude towards knowledge, that understanding of what we mean by the term knowledge is completely
transformed if we start associating probability with knowledge.
That knowledge can only have a probabilistic character.
I mentioned in an earlier lecture that probability theory begins in the 16th century, although
it really only becomes developed in a more sophisticated mathematical form at the close
of the 17th, the beginning of the 18th century, and I particularly highlighted Jacob Bernoulli's
art of conjecturing.
But earlier in the 16th century, Jerome Cardano had published a work on probability theory
applied to gambling, to games, and was not published for over 100 years, so the impact
of that publication was lost.
But in the course of the 17th century, probability theory began to be developed earlier in the
century by Pascal and Fermat, I think I had mentioned in that lecture, and then most sophisticatedly
at the turn of the 18th century by Bernoulli and Abraham de Moivre and others.
What happened through the early 1800s, including the seminal essays on probability of Pierre
Simon Laplace, who was this great spokesperson, as I've mentioned several times, for determinism,
for the deterministic view of science, that probability was associated with ignorance
on our part.
We used probability because we did not know the underlying causes of events, or we did
not know the relevant information in order to deduce the outcome of some event or some
phenomenon.
So probability was a measure of our ignorance.
What we're talking, what I've been talking about in the 19th century is the beginning
of a recognition that probability is a feature of natural and social phenomena.
In the previous lecture, I said that the door was opened by Darwin, the Darwin Wallace
Theory of Evolution by Natural Selection, acting on what we call random mutations, what
they call chance variations, spontaneous variation, and as I said, there were a growing number
of physicists who were very interested in the theory of evolution and philosophers who
recognized that, while Darwin was being too cautious, that the theory of evolution required
recognizing that chance is a fundamental feature of the processes by which life reproduces.
And this is a really startling innovation, and it comes at a time when there was a kind
of a cascade of interest in, not just interest, a cascade of attribution of stochastic features
to fundamental processes, and why we ordinarily think sometimes that the way, the intellectual
way in science is led by the hard sciences, especially physics, but in fact, it was in
the middle of the 19th century, even earlier than that, 1835, was when the Belgian sociologist,
mathematician, astronomer Adolf Kettelay published his treatise on man and the development
of his faculties, in which he really introduced this idea of statistical laws.
Kettelay had studied astronomy, and in the course of studying astronomy and working as
an astronomer, had learned the error theory that astronomers used to correct for the ways
that different observers unconsciously, of course, subtly make small errors in observation,
and then a mathematical technique, what we now call the normal curve, was developed for
correcting these errors mathematically, without knowing the right answer, looking at the distribution
of the data, figuring out, so to speak, what really is the correct measured value is likely
to be within, obviously, within a certain range, what Kettelay did was, he took this
error correcting mathematical technique, and he applied it to social phenomena, he started
collecting, well, he didn't start it, but he took the data that people were beginning
to collect, the social statistical data that people were beginning to collect, in response
to 18th century probability theorists who claimed that better political decisions could
be made if we had more information about human affairs, and he started showing how there
were statistical correlations among things like crime and its distribution as a function
of age, gender, poverty, he started collecting data on suicides, for example, and one of
the things that he startled people with, we're not startled with it today because we've
become enured to this, but in that 19th century, people were startled at the claim that he
could predict within a fairly narrow range how many suicides there would be in Paris
the next year, or in Brussels, or in London, that there was something like a statistical
law, he said, and he used that language of a statistical law, which is a startling notion,
the idea that you can have a lawful phenomenon that is only statistical. Now, I don't want
to apply the term random to this particular phenomenon, we don't think that people commit
suicide randomly, but the distribution of suicides, who commits suicide, is not predictable,
but that in a given population, based on statistical patterns that we have observed, we can predict
how many suicides there will be over Christmas week in New York City, within some relatively
narrow limits, and if you include things like the weather, whether it's sunny, or has been
raining for 17 consecutive days, and the temperature, and what the economic situation is, you can
probably get even closer than that. So, Kettle, and in England, the historian Henry Buckle,
both popularized in the middle of the 19th century, roughly speaking, from the 1830s,
Kettle's book was published in French in 1835, it was translated to English only in 1842,
so from 1840s on, there was this a growing sense that there were such things as statistical
laws for social phenomena. It's interesting that this led to a tremendous obsession with
collecting statistics all over Europe and then in the United States, in which just about
every government set up a Bureau of Statistics to collect more and more complex information
about the population. Oh, we need to know in any given area how many people there are
of by ethnic distribution, how many Catholics, how many Protestants, how many Jews, how many
Muslims. Once you collect that, you want to know what are their health statistics, what
are their mortality statistics, what are their financial statistics, what professions are
they in, etc. Because all of this is supposed to lead to better government, but what initially
it led to at any rate was the accumulation of huge amounts of data. And the question
was what do you then do with this data and how do you organize that data? Well, we will
be hearing it very soon in a lecture on the computer that a seminal moment in the history
of the computer was when Herman Hollerith got a contract from the U.S. Census Bureau
for the 1890 Census, which now included by Congressional mandate a wide range of information
about Americans, not just counting them. And then the question was how are we going to
collate and organize this information? And he got a contract to develop a tabulating machine
that would automatically tabulate and collate this and organize the information that the
Census Bureau collected. We're talking about how that ramified into IBM, as I said in a
subsequent lecture. So here we have in the 1850s and 60s, we have this notion of statistical
law in social phenomena. We have a statistical character associated with the process of evolution
with the origin and certainly with the reproduction of life, explaining the multifariousness of
life. We also have that James-Clarke Maxwell, who was very sensitive to the work of Buckle
in England and through him of Ketley, to the idea that he thought that one could rescue
free will by accepting this idea that statistical laws were governed human affairs, but there
was no necessity on any given individual, whether that solves the problem philosophically,
I think philosophers would say not, but Maxwell was very involved with this and that's why
he was interested in the theory of evolution and the role that Chance played in the theory
of evolution. Maxwell and in conjunction, well, together with, at the same time as Ludwig
Boltzmann, the Austrian physicist Ludwig Boltzmann, developed a kinetic theory of gases in which
the behavior of gases, especially the characteristics that we call temperature and pressure of a gas,
are fundamentally statistical phenomena. The kinetic theory of gases successfully explained
the pressure-volume relationships by assuming that matter was atomic and that the motions
of matter generate the, obviously, the kinetic energy of these molecules or atoms generates
what we mean by heat, they have a temperature, and the pressure that a gas exerts is derived
from these equations which have a fundamentally probabilistic or statistical character. So
the kinetic theory of gases from 1860s on became a very important theory in physics and
it assumed that pressure and temperature characteristics were essentially statistical
in nature. They were also involved in the statistical interpretation of thermodynamics
as I mentioned in an earlier lecture in an attempt to undo the arrow of time. They attributed
a statistical character to the fundamental processes by which energy is measured in thermodynamics,
so it's the same as the kinetic theory of heat, really, that heat is a statistical quantity
having to do with the motion of the atoms and molecules that make up a substance, let's
say a gas. So when the second law of thermodynamics says that heat can only flow from a hotter
body to a colder body, now initially that was understood to be a deterministic phenomenon
that there is no possibility of it going back the other way, of energy of heat flowing
from a colder body to a hotter body. Boltzmann and Maxwell argued that that was merely a statistical
claim that statistically there always is some likelihood of heat flowing from a colder
body to a hotter body. Maxwell invented a thought experiment and involved something
that he called a demon and everybody subsequently called it a Maxwell's demon who could sort
of open the door when the faster molecules in the colder body, which may be moving more
rapidly than the slower molecules in the hotter body because it's a statistical distribution
of motion of kinetic energy in any particular gas, that's what we mean when we talk about
a statistical theory of motion, and so the demon could open up the door whenever the
faster molecules on the cold side were moving and let only the slowest molecules from the
hot side go through, and what would then happen would be that the hot side would get hotter
and the cold side would get colder because we were taking the fastest molecules out of
the cold side, taking the slowest molecules out of the hot side, this in effect allows
for the reversibility of time, but an extremely improbable event. That claim though means
that there is a fundamentally statistical character to the processes that underlie even
such a fundamental concept as time, and the statistical interpretation of what had been
deterministic scientific equations continued at the end of the 19th century with the development
of statistical mechanics, especially by J. Willard Gibbs here in the United States at
Yale University. Statistical mechanics became a fundamental tool for physicists and continues
to be so today, again looking at a phenomenon that from the 17th century on had been deterministic,
the behavior of the motion of material particles and treating it statistically. Now there is
a really interesting and seminal event, and I use that word too many times, an epical
event which is the discovery of radioactivity in 1896, that when beyond attributing a statistical
character to explicitly attributing a random character to fundamental natural processes,
and this randomness expanded so to speak conceptually into quantum theory in the 20th century, and
has become deeply embedded in 20th century physics. So it's not merely a statistical
character, but it is a statistical character that natural and social phenomena, but especially
here natural phenomena, have a statistical character because they have a fundamentally
random character. Let's take a look at this in a little more detail, and I think you'll
get a feeling of what I mean. In 1896, a French physicist named Henri Becquerel, himself the
son of a French physicist, went to a lecture on the newly discovered x-rays by then still
called Runken rays after their discoverer, and he went home and decided to see if phosphorescence
substances also emitted rays. This was in 1896. It so happened that his father's career
was, as a physicist, was based on the study of phosphorescence, and 15 years before he
remembered that he had prepared a series of specimens for his father to study, and one
of these was a collection of uranium salts, a block of uranium salt, a block of uranium
that was stored in the laboratory. So he went and he got the specimen, wrapped it in
a thick black cloth, and put it in a drawer where there was also a photographic plate waiting
for a sunny day, because through his father he was, like others believe, that phosphorescence
substances phosphoresced more strongly after they were exposed to sunlight. If you expose
the phosphorescence substance to sunlight, that stimulates them in some way, and then
you take them away from the sunlight, and then they give off this phosphorescence. Now,
does the phosphorescence substance give off rays like x-rays, because phosphorescent rays
can't go through stuff? So it turned out, accidentally, that it rained day after day
after day. So the stuff was sitting in the drawer, day after day after day. Finally,
the sun came out, he takes the uranium compound out, he's got the photographic plate, something
about the plate looks off, so he develops it, and he discovers that the photographic
plate, even though it has been in a drawer, and that the phosphorescent stuff has been
wrapped in a thick black cloth to make sure that whatever phosphorescence it has, weak
phosphorescence can't get out, that the photographic plate is clouded. That means that the uranium
salt was giving off some kind of rays that were not phosphorescent, but he was able to
establish quickly that it was also not x-rays. So this is a new kind of radiation, and by
the end of that year, 1896, still within the same year, he reported to the Paris Academy
of Sciences on this observation in 1897, he published a short note about it. It did not
initially get a lot of attention, but coincidentally Marie Curie was looking for a doctoral dissertation
research subject, and her husband Pierre Curie suggested that she study this weird phenomenon
that Becquerel had come up with. He had a hunch that it would be important, and in order
to help her, because she needed to have a detector, in order to detect what radiation,
to prove that there was really some kind of rays coming out of this compound, he invented
an extremely subtle electroscope, a device that can detect electric charge. It was a
very fragile instrument, very complicated to use correctly, and Marie Curie mastered
it, actually goes back to the whole question of how do you know when you're using a new
instrument that's measuring something that you don't know anything about, how do you
know you're using it correctly, but we'll let that go for the moment. And in 1898, Marie
Curie announced, gave the name radioactivity to what was going on in Becquerel's sample,
and she started studying different substances and ranking them in terms of their power,
their radioactive power, and studied uranium, thorium, and an ore called pitch blend, which
really turned out to have powerful radioactivity in it, and she isolated from pitch blend,
from a tremendous amount of pitch blend, a minute quantity of a brand new element called,
that she named polonium, after her native polon, and that left a residue which contained
something even more radioactive, but which required processing tons and tons of ore for
a minute fraction of a gram of radium. She and her husband Pierre Curie, in 1903, shared
the Nobel Prize in chemistry with Becquerel, who, one could argue, had an original good
idea but didn't do much with it, but he shared the Nobel Prize with them. Marie Curie got
her own Nobel Prize, that was in physics, in 1903 physics prize, in 1911 she also got
a Nobel Prize in chemistry for her subsequent work in radioactivity. Now this made radioactivity,
if you'll forgive the expression, a very hot research topic, and into this topic moved Ernst
Rutherford, one of the most influential physicists of the first third of the 20th century, although
obviously not nearly as well known as people like Einstein and Max Planck and Niels Bohr,
but Rutherford's own work, Nobel Prize winning, as well as the work of the many generations
of students that he trained, was extremely influential. Rutherford, initially alone,
started publishing in 1900, and then with a chemist named Frederick Soddy, developed
a much deeper understanding of radioactivity. Pierre Curie, in 1899, did not, when his wife
started publishing and announcing the discovery of these new elements, Pierre Curie did not,
let me put it positively, believed that the atom was solid. Two years before, J.J. Thompson
had announced that discovery of the atom as an internal feature of the atom, of the
electron, as an internal feature of the atom, so the atom had an internal structure, but
Pierre Curie didn't buy that, and believed that the atom was solid. Marie Curie went
along with that, at least superficially, but in 1899 speculated, just throughout a hint,
that maybe this incredible energy from radioactive substances comes from an internal disintegration,
a disintegration of something inside the atom, and that this radiation of energy is,
maybe correlated with a loss of mass. That will become famous as equal MC squared when
Einstein gets on the scene in 1905, but this was a hint. Rutherford, who was a student
of J.J. Thompson, immediately bought into the idea that atoms have an internal structure,
and his research was all based from the beginning, he and Soddy, on the idea that atoms have
an internal structure, and that radioactivity is the result of a disintegration process,
which we now call typically a decay process, of the nucleus within, that was not a term
that would have been used in 1902, 1903, a disintegration process within the atom, because
the atom has an internal structure, and Rutherford and Soddy identified that there are at least
two kinds of rays associated with radioactivity, alpha rays, which act just like and then were
identified as helium nuclei, helium was an element that had only recently been discovered,
and that they are helium nuclei, two protons and two neutrons, to use our language, not
available in 1902, and that beta rays were in fact electrons, so what's happening is that
these radioactive elements are releasing helium nuclei, helium atoms stripped of their electrons,
so they have the helium nuclei and electrons are coming from inside the atom. What's really
important to us is that Rutherford and Soddy announced that radioactivity was a fundamentally
random process, and they used that language, that it was a strictly random process in which
any, you could not predict any given uranium nucleus disintegrating in this way, but they
claimed they could measure with precision the distinctive, what we call half-life, for every
radioactive element, that every radioactive element has a distinctive half-like, the way human beings
are supposed to have distinctive fingerprints, that half of any large sample will decay in
a particular period of time, 10,000 years, 100,000 years, 11 days, 6 hours, 2 minutes,
whatever it happens to be. This is a fundamental development because it gives body to the idea
that a phenomenon can be random and lawful. There is only a statistical probability to
be attached to whether any given electron atom of radioactivity will disintegrate, but
give me a large enough sample and I will predict with precision how much energy will be released
in any given unit of time. Once I know the half-life of that particular, associated with
that particular element, and that's what Rutherford and Soddy got their Nobel Prize for, that
was a really powerful development, but now radioactivity, radioactivity was taken by
Rutherford and was used as a probe, as a tool, as a new scientific instrument, given
his new understanding of radioactivity, and especially that what we get is helium nuclei,
which are relatively heavy, weighing thousands of times more than an electron, the helium
nuclei come out of a specimen, a radioactive, let's say, of uranium. If you put it inside
a box with a little hole, then out of that hole is going to come a stream of helium nuclei,
and he put some of his graduate students to work to see how we could explore the interior
of the atom by means of bombardment with these helium nuclei, the alpha rays. In 1900, physicists
named Paul Villard identified that there's another ray associated with radioactivity,
gamma rays, and that those rays are very similar to x-rays. That completes that particular
picture. And in 1909, 1910, Rutherford and his graduate students came up with data that
strongly supported the idea that the nucleus was that an atom was largely empty space with
a tiny, tiny solid nucleus surrounded by orbiting electrons, and as we will see more specifically
in the electron quantum theory, two years later, Niels Bohr founded quantum theory on
the basis of rescuing Rutherford's solar system model of the atom. Between 1913, when
the publication first came out, and 1917, Bohr and Einstein, sort of separately, but
also together, showed that the change of orbits of electrons, which is the source of all absorption
and emission of electromagnetic energy, is a random process. And nevertheless, it can
be described with statistical precision. In fact, the work that Einstein did in 1917 and
up through 1919 in this particular area, and Einstein did fundamental work in statistical
mechanics, is really the basis for lasers. Although it was only almost 40 years later,
it was about 40 years later when the first lasers were actually built, but the idea that
lasers are built on, that you can have, that you can precisely, you can get a precise physical
phenomenon, you can get a laser beam that you can use for eye surgery, that you can
use to read music CDs and DVDs, out of this random statistical process is another illustration
of the idea that randomness and lawfulness, statistical stochastic character and lawfulness
are not exclusive of one another. And that means that we really do need to go back to
the drawing board and see what we're going to mean by the terms knowledge, law, truth,
reality, in science. It is a small irony that Einstein and Bohr disagreed about this for
the decades.
