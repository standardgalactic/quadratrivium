Tomorrow, an example from Thomas Gilovich's excellent book, How We Know It Isn't So.
Let's consider a male and female in a couple relationship.
Oftentimes the male firmly believes that he always puts down the toilet seat.
While the female might believe that he never puts down the toilet seat.
Both people are dealing with the exact same set of data, and yet they come to diametrically
opposed and extreme conclusions.
How is this possible?
And chances are both of them are actually wrong.
In this lecture, we're going to talk about cognitive biases, the many ways in which our
thinking is inclined or biased, often in very subtle or subconscious ways.
Whether or not you believe the evolutionary explanations for those biases, psychologists
have documented their existence over countless experiments.
Of course, the worst kind of bias is the one that you're not aware of.
Cognitive biases affect the way we argue, the way we think.
It's like water following the path of least resistance or flowing downhill to a lower
energy state.
This is the way our minds will tend to think unless we make a specific high energy effort
to step out of these processes and think in a more clear and logical manner.
In fact, these biases are often related to logical fallacies.
They lead us into invalid or fallacious thinking rather than into clear and correct formal logical
ways of thinking.
These biases are numerous, they're pervasive, and they can have a very powerful influence
on how we think.
The term heuristics is applied to one type of cognitive bias.
A heuristic is a rule of thumb that we subconsciously apply, a mental shortcut if you will.
The conclusions that we arrive at through these heuristic methods may be correct much
of the time.
A little bit of accuracy has been sacrificed for the efficiency of decision making.
We often think of these heuristics as simple common sense, but they are not strictly logically
correct which means some of the time they can lead us to incorrect conclusions.
You can also think of them as rules for making decisions or making judgments or even for
solving problems.
They may be practical like the trial and error approach is a common heuristic.
However, in our complex modern world, heuristics of which we are not aware can lead us astray.
When we use that to deal with probability, I will leave for a later lecture on enumeracy.
In psychology, the man credited with developing the concept of heuristics is Herbert Simon.
Much of the developmental work, however, was done by Amos Tversky and Daniel Kahneman.
Kahneman, along with another psychologist, Shane Frederick, theorized that heuristics
are a way of substituting a simple computational problem for a more complex one without being
aware that we're doing this.
It's a way of simplifying a very complex world, simplifying a very complicated decision-making
process.
Again, this could be very practical and very useful as long as we're aware that it's
first approximation of the truth and not strictly true.
It's like confusing a schematic, which is a representative drawing or a diagram of a
complex machine for the actual complexity of the machine itself.
Let's explore a few common examples.
The first heuristic I want to discuss is called anchoring.
We have a tendency to focus or anchor on a prominent feature of an object, a person,
or an event, and then make decisions or judgments based upon that one feature alone.
Again, this is a way of simplifying or oversimplifying the complexity that we're confronted with.
Often, the feature that we anchor on is the first piece of information that we encounter,
especially if it's something that we learned at a very young age.
We tend to persist in those perceptions.
For example, an inexperienced computer purchaser may focus excessively on clock speed.
Their computer is 2.4 megahertz.
Advertisers take advantage of this anchoring by prominently advertising the increasing megahertz
of their product.
However, this is only a single feature, and it is used inappropriately as a measure of
overall computer performance.
Advertisers may focus less specifically on other features like RAM, the design of the
chipset, and the hard drive speed that also have a dramatic effect on computer performance.
They simplify their calculations by anchoring to one feature, one feature that they can
deal with.
There are similar examples in other technologies as well, like buying a digital camera.
The number of options can be dizzying, and therefore, advertisers typically will boil
it all down to megapixels.
How many megapixels is the camera, ignoring other very important features like the quality
and size of the lens or the CCD receptor?
There is also what is called anchoring and adjustment, which is an aspect of the anchoring
heuristic.
We may anchor to numbers and then use the number that we anchor to as a starting point
for later estimations or adjustments.
When exposed to numbers, we do tend to anchor to them, and then that biases our later judgments.
In other words, the first number we encounter tends to bias all of our later thinking on
that subject subconsciously in a way that we're not aware.
A classic example comes from a study by Tversky and Kahneman.
They asked subjects, what percentage of African nations do they believe are members of the
United Nations, the UN?
To one group, they first asked, is it more or less than 10 percent?
Then they asked them to give a specific number.
The average number of the response was 25 percent.
To a second group, they first asked, is it more or less than 60 percent?
Then they were asked to give a specific number, the average of which was 45 percent.
The researchers were able to dramatically affect the estimation that the subjects were
making just by a little bit of suggestion, just by asking a bit of a leading question,
more or less than 10 percent versus more or less than 60 percent.
The subjects were not aware of the fact that they were anchoring on that number that was
presented to them as part of the question.
Anchoring is used widely in negotiation and marketing.
Anything that you can use to manipulate another person's choices or behavior, you will find
that the marketing industry will hit upon that as a way of influencing your purchasing
decisions, for example.
In negotiations, it's common to start with an initial extreme bid in the direction that
you desire, in the hopes that that will anchor future negotiation.
Start high and then when the person brings you down to a lower number, you're still going
to be ultimately higher than if you started with where you ultimately thought you would
end up.
Or marketers may sell products, for example, labeled as three for 1995.
This subtly encourages shoppers to buy three of the item, even though they don't have to.
They could purchase one or two, but they anchor to the three for 1995 because that's what
they're offered.
Anchoring and negotiation and marketing comes up in many other ways as well.
Let me give you a personal example.
I recently bought a car and was told that the value of my trade-in was between $2,000
and $3,000.
This made the ultimate offer of $3,000 for my trade-in seem like the high end of the
range that was being offered and therefore more desirable, when in fact it could have
been a low figure or it could have simply been what the car was worth.
If the salesperson had said that my car was worth $3,000 to $4,000, I probably would have
felt very different about the $3,000 figure, even though it was the exact same figure.
Another heuristic is what is called the availability heuristic.
This one could be very subtle and powerful in its influence over our thinking.
Essentially, what is immediately accessible to us, what we can think of, we assume, must
be important and influential.
We tend to reach for the familiar.
The assumption is that if we can think of an example of something, then that thing must
be common or representative.
It also gives weight to events that are recent, vivid, personal, and emotional.
This concept comes up frequently in medicine and we teach medical students not to fall
prey to the availability heuristic, meaning that we tend to remember our recent patients,
especially recent dramatic cases, and we shouldn't assume that what happened with that patient
is representative of patients with that condition in general.
It is a biased data set.
It just happens to be a recent example.
This is related to anecdotal evidence.
I'm going to cover the notion of anecdotal or personal uncontrolled observations more
in future lectures on science and scientific methods.
Essentially, anecdotes are things that we personally experience in our everyday lives
that are not part of a controlled or experimental condition.
We use them as a method for estimating probability.
For example, if we're trying to estimate, well, how common are allergies to strawberries?
The approach that we will tend to take is to see if we can think of examples of people
who have allergies to strawberries.
Well, my cousin Vinnie has an allergy to strawberries, therefore we conclude, using
the availability heuristic, that strawberry allergies must be common.
We don't appreciate the fact, naively, that this is anecdotal.
It just happens to be that I have a cousin with a strawberry allergy.
This doesn't mean that they're common.
They could be very uncommon and it could be just a minor coincidence that I happen to
know somebody with a strawberry allergy.
But we have this innate tendency to be very compelled by our personal experience as opposed
to abstract statistical numbers, reviewing data about 1,000 random people in the culture
to see how many have strawberry allergies would be a much better approach.
But we tend to follow the availability heuristic because it speaks to us more emotionally.
This has implications for the effects of the media on how we think and believe about things.
If the media shows stories of disasters and crimes over and over again, we will tend to
think that they are more common, even if they are showing rare events.
This has huge implications for viewers watching news outlets, for example, that have particular
biases.
Research has also showed, for example, that this has an actual effect, that news outlets
covering disasters tends to cause people to overestimate the frequency of those disasters
or those events.
Similarly, research shows that people who watch soap operas, for example, tend to overestimate
and have much higher estimates of things like the percentage of the population that are
doctors or lawyers because those professions are very commonly depicted in soap operas.
Another example of a common heuristic that we fall prey to is that of exemplars.
Those are cases that represent a phenomenon and are vivid and dramatic examples.
And they tend to have a greater influence on our judgments than statistical information
about the base rate, the statistical rate at which things occur.
This is a reflection of the availability heuristic.
It also reflects our storytelling bias.
We are social creatures programmed to respond to stories, especially emotional stories.
This is why marketers, whether for selling a product or influencing your voting decisions,
for example, will use a dramatic story to make a point rather than just giving you dry statistical
information.
A classic example from a presidential election from a couple decades ago is the Willie Horton
example.
Willie Horton was presented as an example of a criminal who was convicted and put in
prison and then who was let out of prison on furlough who then committed a crime while
on furlough.
Willie Horton was prominently displayed in commercials in order to criticize the candidate
who allowed Willie Horton out on furlough.
This however was a very emotional and exemplar type of approach that did not really give
a fair consideration of the statistical data on how likely such things are to occur.
Another heuristic is the escalation of commitment.
Once we have committed to a decision, we tend to stick to that commitment.
We feel like we have invested in it and therefore that biases all later judgments about that
commitment.
We're overly influenced by what we have already committed to even if further commitment is
a losing proposition.
This includes money, time or other resources that we might invest.
Even lives of soldiers in a war.
The saying do not throw good money after bad is a recognition that there is a tendency
to do just that, to escalate our commitment once we have already gone in a certain direction.
The representative, the representativeness heuristic is the assumption that causes typically
resemble effects.
There really isn't any specific logical or scientific reason to believe this, but this
is definitely a bias in our thinking.
Emotionally charged effects, for example, we assume must have emotionally charged cause.
The assassination of John F. Kennedy is one example.
This was a big world changing event.
Many people felt that it must have an equally big cause.
It seemed incongruous.
It just didn't make sense.
It didn't feel right that the so-called lone nut hypothesis or theory could be true.
Could it really be that one deranged man acting on his own caused something so momentous?
Just doesn't feel right.
This needs to a number of other fallacies as well, such as a failure to consider the
base rate, which we'll discuss in a further lecture.
The representativeness heuristic also leads to magical thinking.
This is the source of the belief in what is known as sympathetic magic.
Some cultures have beliefs that are, in essence, what we call sympathetic magic, that items
that resemble something will have an effect that relates to that thing.
For example, many cultures believe that an item like the horn of a rhinoceros, something
that resembles the erect male penis, can be used to promote virility.
The basis of that claim is based upon this notion of sympathetic magic, of the representativeness
heuristic.
Homeopathy is also based largely on sympathetic magic.
The homeopathic law of like cures like that something has an essence which can be used
in order to treat something similar to it, for example, a substance which causes symptoms
can be used to treat those same symptoms because it's affecting the essence of that thing.
It also derives from this representativeness type of thinking.
The effort heuristic, this is similar to the escalating investment heuristic, tells us
that we value items more if they require greater effort to obtain.
Again, this is a good example, I think, of how this is a mental shortcut.
It's likely to be true in some cases that the more difficult it is to obtain something,
the more rare it is and the more valuable it is.
However, this is not necessarily strictly, logically true.
We may obtain something easily which happens to be highly available.
For example, we will spend $100 that we earned through hard work much more carefully than
we will spend $100 that we found lying on the sidewalk.
This is what psychologists call the found money effect.
If we come by money easily, we feel like we can spend it easily as well.
Another cultural example is the people of Yap who use a very bizarre system of currency.
They use large carved stones, stones that may be 3, 4, 5 feet in diameter or larger.
They are far too heavy to carry around, but this is their currency.
The value of these Yap money stones depends upon the difficulty in obtaining them.
Islanders have to travel to a distant island by canoe, a hostile island.
They have to then fight off the natives of that hostile island, gather their stone and
bring it back in their canoes.
The extreme difficulty in obtaining these rocks is why they are valued as much as they
are.
This system was noticed by Europeans who then tried to import large numbers of rocks that
they easily obtained.
Interestingly, even though these rocks were identical to the much more difficult to obtain
rocks, the Yap people did not value them as highly because they were obtained much too
easily.
I am now going to talk about one of the most pervasive biases in our thinking.
This is a really important one to understand thoroughly, and that is confirmation bias.
There are more strict heuristics, but I am going to move to other types of biases, like
cognitive biases.
Again, I think this is probably the one that has the most pervasive influence on people's
thinking.
Remember the example that I gave at the beginning of this lecture about the toilet seats?
Well, how is that possible?
How is it possible that a husband and a wife, for example, would come to completely different
conclusions about a very factual event, is the man putting the toilet seat down on average
or not?
Well, what happens is that each member of the couple is noticing events that support
their prior beliefs.
They may have argued about it in the past, and the husband takes the stance that I do
put the toilet seat down.
Therefore, every time he remembers to put the seat down, he notices that event.
It becomes data which confirms his belief that he always puts the seat down.
When he forgets to put the seat down, he doesn't register or notice that non-event.
Almost by definition, if he forgot to do it, he probably also forgot to notice.
If he noticed it, he would have then put it down.
The wife, on the other hand, will notice every time the husband doesn't put the toilet seat
down and probably will not notice when he does remember to put it down.
It's a non-event.
They are both falling prey to confirmation bias, which is leading them to opposite conclusions.
But the confirmation bias is more powerful than simply remembering the hits and forgetting
or not noticing the misses.
We tend to also accept information and events that support our beliefs and we interpret
them favorably.
This may be literally interpreting a scientific study.
If the conclusion of the study is something that we agree with, we accept it as a good
solid study.
If the conclusion of the study is something we disagree with, we're going to look much
more carefully at potential flaws in the study to try to find some way to dismiss the conclusion.
We in fact, research shows, expend a great deal of time and effort to do just that.
We tend not to ignore disconfirming data that we notice.
We actually expend a lot of energy to find reasons to rationalize away that data.
Here's an example again from my personal experience.
There are those who believe that in the so-called lunar effect that when the moon is full, that
will affect people's behavior.
People will act a little bit more crazy.
Maybe there will be more crime or emergency rooms will be more busy.
One time I was working in an emergency room when it was a particularly busy night and
a nurse that I was working with looked up and said, wow, this is a crazy busy night.
Is there a full moon out?
It turns out there wasn't a full moon.
She promptly said, oh, and then forgot the event entirely.
It was a non-event.
She didn't notice it.
There wasn't a full moon.
If there were by coincidence a full moon, that would have confirmed her prior belief
in the lunar effect.
That would have been powerful confirmation for her that that belief was correct.
In essence, confirmation bias is also a way of cherry picking data.
We notice, accept, and remember data which confirms our beliefs.
We do not notice or explain away data which doesn't confirm our beliefs.
We underestimate the degree to which there is a vast noisy data set out there in the
world that we are cherry picking from.
We are by noticing things which confirm our beliefs.
We are picking out bits of data from a lot of potential data that's out there.
That's why we need to systematically look through data in order to draw any meaningful
or reliable conclusions about it.
Just going about our everyday lives, we are likely to be influenced powerfully by confirmation
bias.
Yet another example of this bias is the exception that quote unquote proves the rule.
This is actually often a misinterpretation of the phrase.
Proof in this phrase does not mean to actually confirm as is often misinterpreted.
It means to test.
The exception tests the rule.
It doesn't mean that the rule is correct.
But often I hear it used as a way of taking disconfirming evidence as if it confirms the
rule that we have an a priority or a preexisting belief in.
While events that seem to confirm the prior belief are taken as evidence, evidence that
disconfirms the prior belief are taken as just the exception that proves the rule rather
than as evidence against the belief itself.
Another example of this is called the toupee fallacy.
This is a classic example of confirmation bias.
Some people believe that, for example, this is just a representative example, they can
always tell when a man is wearing a toupee.
Because when they notice a man wearing a toupee, they take that as confirmation of their ability.
However, they're not accounting for the fact that they don't notice when they don't recognize
a toupee.
That data is completely missing from their data set.
They would have to systematically survey sequentially 100 or 200 or so men and then decide who's
wearing a toupee and who isn't and compare their judgments to some gold standard objective
outcome like asking the men or having them demonstrate whether or not they're wearing
a toupee.
Just by naively taking in the data, confirmation bias will lead us to the false conclusion
that we, for example, can always detect a toupee.
There is also another bias called a congruence bias.
This also can be a very subtle one that can powerfully lead to confirmation bias.
This is the tendency to test our own theories about things, but not to also test alternative
theories.
We tend to avoid this, you know, in order to avoid this rather, we need to conduct observations
that are designed to test multiple hypotheses, not just our own.
Let me give you an example from, again, my own background in medical diagnosis.
I may have the hypothesis based upon previous experience that lack of sleep or a sleep disorder
leads to migraines or significantly increases the frequency of migraines.
Let's say I start asking my migraine patients how their sleep is and I find that many of
my migraine patients have impaired sleep or poor sleep.
I might take this as powerful confirmation that my hypothesis must be true.
However, there's a tendency not to do what I should be doing, which is to also ask people
who don't have migraines whether or not they have poor sleep.
It may turn out that the rate of poor sleep is the same among patients with migraine or
without migraine.
I also might ask my migraine patients if they have other stresses in their life and I may
find that they equally well have other factors that could be contributing to their migraines
in addition to the sleep.
But the tendency to simply test your own hypothesis and take a positive result as confirmation
is a powerful cognitive bias that leads people to firmly hold conclusions which may really
have no statistical basis in reality.
The exposure effect is a form of familiarity bias.
We tend to rate things more highly the more familiar we are with them.
That's why repetition is often used in marketing.
The more you are exposed to a brand, the more familiar with it, the more likely you are
to purchase it.
This also explains the familiarity effect in early elections.
The front runners are typically those candidates that the public is simply most familiar with.
This can be turned to an advantage, for example, amusement park rides with names that are difficult
to pronounce were judged to be much more risky and adventurous because of a lack of
familiarity.
So the flip side can be used if your intention is to create the impression that something
is risky.
A choice supportive bias is a bias where once we make a decision we then assess that decision
much more favorably.
This is a way of relieving perhaps some of the anxiety or angst over whether or not we
made the right decision.
When we buy something we therefore then have a tendency to rate what we purchased much
more favorably than we did prior to deciding that that's what we were going to purchase.
In essence we're trying to justify a decision that we already made.
This leads to a very interesting effect.
We may also downgrade our assessment of the number two item on our list.
Let's say we were trying to decide among five items to purchase and our choice came down
to our first and second choice and we ultimately decided to go with what became our number one
choice.
We then will justify that decision by increasing our assessment of our first choice and downgrading
our assessment of the second choice.
If this is done experimentally and subjects are then told all right well number one is
no longer available to you what now are you going to take as a replacement.
Many people will skip over their second choice and then take their third choice their prior
third choice as an alternate because they've already invested mental effort in downgrading
their assessment of the number two second choice which lost out to their previous number
one choice.
Another very powerful bias is what psychologists call the fundamental attribution error.
This is an actor observer bias or a tendency to explain the actions of others according
to their personality traits while downplaying situational factors.
However we explain our own behavior with situational factors and downplay personality
traits.
For example if someone trips while walking down the sidewalk if we see someone else do
this we're likely to conclude that's a clumsy person.
If we trip when walking down the sidewalk however we say oh there was a crack in the
sidewalk there was this external factor that led to me tripping.
Racial thinking is yet another bias towards favorable ideas that are emotionally appealing
to us regardless of the logic and evidence this is also called an optimism bias.
This for example motivates people to seek highly implausible even magical treatments
for their ailments even over warnings that such treatments do not work.
Their desire for the treatment to work overwhelms their logic in that case.
We tend to favor the optimistic possibility that the treatment works because we wish it
to.
The lottery industry is largely based upon this bias.
People know how phenomenally unlikely it is that they'll win but their wishful thinking
becomes determinative in the end in terms of their decision.
I'm going to read for you now a psychic reading this could be an astrological reading or perhaps
a palm reading but I'm going to right now personally to you divine your personality.
Think about how accurately you think this describes you.
You have a great need for other people to like and admire you.
You have a tendency to be critical of yourself.
You have a great deal of unused capacity which you have not turned to your advantage.
While you have some personality weaknesses you are generally able to compensate for them.
Disciplined and self controlled outside you tend to be worrisome and insecure inside.
At times you have serious doubts as to whether you have made the right decision or done the
right thing.
You prefer a certain amount of change and variety and become dissatisfied when hemmed in by
restrictions or limitations.
You pride yourself as an independent thinker and do not accept other statements without
satisfactory proof.
You have found it unwise to be too frank and revealing yourself to others however at times
you are extroverted, affable, sociable while other times you are introverted, wary, reserved.
Some of your aspirations tend to be pretty unrealistic.
Security is one of your major goals in life.
How did I do?
Was that a pretty accurate description of your personality?
Most people rate that very description very highly.
This is known as the Forer Effect after the psychologists who first described it back
in 1948 also called the Barnum Effect.
It reflects a tendency to make judgments about vague or general descriptions and interpret
them as being specifically tailored for us.
Vague statements in fact tend to be rated much more highly accurate than more specific
statements.
Essentially confirmation bias and familiarity heuristics and availability heuristics kick
in.
We look for examples that support the statements that we are being told and when we can find
them, we take that as confirmation that they're accurate.
What I've just described is really the tip of the iceberg.
These are I think the most common and pervasive cognitive biases that plague our thinking
but there are many others.
They do lose much of their power however over our thinking when we're aware of them.
We can step outside of ourselves a little bit and engage in metacognition, thinking
about the process of our own thinking and that loosens the grip that these cognitive
biases have on our thinking.
