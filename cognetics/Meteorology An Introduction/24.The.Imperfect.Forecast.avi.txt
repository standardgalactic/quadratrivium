Welcome to our final lecture in meteorology, which I've entitled the Imperfect Forecast.
In this lecture, we, in the last lecture, we gained some insight into how numerical models
are made, the equations they use, the compromises they have to make, and the assumptions that
underlie them.
And we saw some things that models can do over this course, the things they get right,
the things they struggle with, and the things they'll never get right, and the seemingly
endless ways in which things can go wrong.
In the first lecture, I used an example, a case we called the perfect storm.
There was the extra-tropical cyclone that intensified on interaction with a hurricane,
and then later became a hurricane itself.
Welcome to the Imperfect Forecast.
The subject here is 2005's Hurricane Rita, which we saw in lecture 21 on tropical cyclones.
The story is why sometimes hurricane motion, hurricane track, is difficult to predict.
In the United States, responsibility for forecasting the motion and strength of tropical cyclones
that form or move into the Atlantic and East Pacific Basins resides with the National Hurricane
Center, or NHC.
Intensity forecasting remains extremely challenging.
Hurricanes can strengthen or weaken seemingly at the drop of a hat.
Why and how hurricanes change intensity is a very active area of current research.
Another active area of research is tropical cyclogenesis, hurricane formation.
By some clusters of thunderstorms or tropical squall lines develop into tropical cyclones,
and others don't.
Track forecasting, on the other hand, is a lot better than it used to be.
Let's take a look at track error trends over time.
This plot shows the NHC's average position forecast error over a period from 1970 to
2007.
The horizontal axis is time.
The vertical axis is error in nautical miles.
This is the hurricane season average error for the one, two, and three-day forecasts,
and those are indicated by the red, green, and orange markers.
For each of those forecast intervals, a best-fit trend line is superposed.
Days four and five are also included for more recent years.
Unsurprisingly, the longer the forecast lead time, the larger the error.
But note those strongly downward-sloping trend lines.
They demonstrate that accuracy has improved markedly over the years.
The three-day forecast was as good in 2007 as the two-day forecast was just a decade
earlier.
And the average two-day position forecast error that year was only a bit under 100 nautical
miles or 110 statute miles.
Those are great forecasts.
But I'm going to show you what roughly 100 miles can look like.
In September 2005, Hurricane Rita neared the Gulf Coast only three weeks after Hurricane
Katrina slammed into New Orleans and coastal Mississippi.
Katrina was a major hurricane, which means its intensity based on maximum sustained wind
speed was Category 3 or higher in the Sapper-Simpson scale.
In fact, Katrina was Category 5, with wind speeds topping out at about 175 miles per
hour.
Its sea level pressure got as low as 902 millibars, 11 percent below average sea level pressure.
Rita was also a Category 5.
In fact, it was stronger.
Its sea level pressure got as low as 895 millibars, around 3Z on September 22, representing
at the time the third lowest reading ever recorded in the Atlantic.
Rita's estimated maximum winds were five miles per hour stronger than Katrina's.
And it was heading for Houston.
This is the National Hurricane Center's forecast for 3Z September 22, just as Rita was reaching
peak intensity.
We can see the Gulf Coast stretching from northeastern Mexico past Mississippi on this
figure.
Rita's initial position is indicated by the orange dot, and she was 54 hours away from
landfall.
But the real question was, where would she land?
This forecast put Rita on a collision course with the Houston metro area.
In fact, Houston and Galveston Island were on what we sometimes euphemistically called
the dirty side of the storm.
That refers to the side where the storm's winds are enhanced by its own forward motion,
and those winds are also pushing seawater very well toward shore.
Houston's residents remembered Katrina, that happened just three weeks earlier, so they
got out of town.
But Rita didn't hit Houston.
In the last 48 hours before a landfall, something changed.
A ridge 500 kilometers away shifted amplitude or position a little bit.
Maybe it was a butterfly.
But Rita turned away, moving more to the north, and wound up making landfall on the
Texas-Louisiana border instead, a place with a much lower population density.
Now, don't get me wrong, Rita was a tragedy for anyone affected by her.
And I'm not trying to minimize the horror that they experienced, but it could have been
a whole lot worse.
And the distance between the expected and actual landfall locations?
About 100 miles.
It is important to understand that the 54-hour forecast for Rita from the NHC was very good.
In a historical context, it was excellent.
100 miles just isn't that much.
Think about a distance of 100 miles in your own life, in your own state.
In terms of distance, in terms of time.
In my city, in the metropolitan area of Los Angeles, you can easily drive 100 miles between
two points and never leave the built-up urban area.
And in Los Angeles is horrible traffic.
Sometimes my office feels like 100 miles from home.
But we already know the forecasts are never going to be perfect anyway.
Errors in initial conditions.
The atmospheric state, at any time, is not, cannot, will not ever be known to sufficient
precision.
We will improve our sampling of the atmosphere with great new spaceporn and other instruments.
That's an effort that's worth doing.
It's an effort that must be done, but there will always be gaps in our knowledge and things
we cannot know or measure.
And besides, there are things that even if we could see them and feel them and take their
measure, we cannot model them properly.
We cannot include them in our models.
We cannot see them on our grids.
Maybe we don't know their physics and what controls their existence and behavior with
moderate, let alone perfect accuracy.
Maybe they'll always be sub-grid to our models.
So those are the things we parameterize.
And each thing we parameterize presents us with potential for error.
Each process, radiation, cloud processes, turbulence, surface fluxes, what's going on
in the soil and oceans is an opportunity for uncertainty.
Ed Loren showed us that uncertainty will always be with us and it will affect our forecasts.
But uncertainty need not be our undoing.
If we accept it, if we embrace it, use it, exploit it, make it work for us, all we have
to do is accept the fact that our models and approximations are flawed and our observations
are flawed.
And we can do the next best thing to making a perfect forecast for the day after tomorrow.
We can estimate the uncertainty of our forecast.
We can estimate uncertainty by making more than one forecast for a day, for a place,
or event.
Models of observations have flaws.
So let's run the model multiple times with slightly different inputs each time, representing
error and uncertainty.
That's what Ed Lorenz did, if only accidentally.
But sometimes genius is accidental and genius is being able to recognize what the accidental
really means.
There are also flaws in our model physical approximations, our parameterizations, what
we call model physics, by their very nature.
They have on-off switches, such as when will the LFC be achieved, because that makes such
a big difference to vertical air motions, how much, where, and when.
They have estimations, such as the relationship between how strongly the wind is blowing across
the sea surface and how quickly water is evaporating.
And that affects humidity, temperature, and CAPE.
How intensely do you have to heat a surface until the vertical mixing that produced our
roll clouds starts occurring?
And once it does get going, how far up in the atmosphere has it felt?
How do you keep track of countless cloud particles, cloud droplets, raindrops of various size,
ice crystals of myriad habits, particles like hail that are important to lightning and severe
weather, let alone ice nuclei?
Are our drops super-cooled or not?
Are they small or large, falling quickly or slowly, it makes a difference?
So we can run the model multiple times, each time with various thresholds, alternative approximations,
different ideas regarding how to guess at what we can't see based on what we can.
And in so doing, by varying initial conditions and model physics, we create an ensemble of
forecasts, each forecast a little different from the others, and maybe a lot different.
Model forecasting is partly based on the idea that the mean of a group of forecasts is potentially
more accurate than the forecast of any randomly chosen member.
Its greatest value is the information it provides regarding potential variability and uncertainty.
As an example, suppose a set of distinct models, each using different initial conditions, model
physics, and numerical approximations, were used to forecast the track RETO would take
after reaching maximum intensity at 3Z September 22nd.
That was done, and the result looked like this.
On this figure, there are 16 forecasts from 16 different sources all initiated approximately
54 hours prior to landfall.
One of them was made by the human forecasters at the NHC, and another is just extrapolation,
the presumption that the storm will keep on keeping on what it was doing at the same
bearing and speed.
The rest were made by models of some kind.
Some of these models were very sophisticated, employing the highest resolution, the most
advanced computational techniques, blending in the largest amount of available data from
satellite ships, aircraft radar, what have you.
Others were much simpler models, much more humble.
A couple of them are statistical models based on expectations in past history, and a few
weren't any more complex than Ed Lorenz's three little equations.
Those models had the virtue of running very quickly, at least.
And what they've done is given us a range of answers, the range being a measure of uncertainty.
They had all given us the same answer, well, it's possible that they could all be making
the same mistake, sharing the same erroneous assumptions or same flawed inputs.
But if the model ensemble of tracks are all over the place, as they are here, then that's
a signal to watch out.
In the case of Rita, the mean of the ensemble is somewhere in the middle of this mess, and
we see it has some error.
The landfall location is too far down the coast and too close to Houston.
But still, the position error for this ensemble mean is smaller than the average for this
lead time, so it's a pretty darn good forecast.
But look at that spread.
There's a lot of differences of opinion here, and these opinions are not all equally valid.
Some models have better track records than others, and their contributions should and
will be given greater weight.
But this spread suggests a sizable amount of uncertainty in this case.
Rita is only 54 hours from landfall, and there's a large stretch of coastline that needs to
be on guard.
One of these forecasts is going to be the winner, the one that's more right than the
others, or at least the one that is less wrong.
In this case, if you look carefully, you'll see there's a landfall forecast that's actually
exactly right.
Of course, you don't know which one that will be in advance.
You may be wondering which model won this contest.
Does it actually matter?
Maybe I'll tell you later, and maybe not, but more likely than not, it wouldn't win
the next time out anyway.
All glory is fleeting.
I think we should be surprised that the dozen-plus models produce such a variety of tracks.
After all, Rita was not that far from shore in terms of distance or time.
So why did this variability appear so quickly?
Why do different models, even with their potentially different initial conditions and model physics,
yield different forecasts?
They might have very well had produced different tracks, even if they started with exactly
the same initial data.
Yes, the Lorenz experiment, sensitive dependence on initial conditions, but that doesn't tell
us why.
It doesn't tell us why the tracks diverged.
There has to be a reason.
These models aren't random any more than Lorenz's model was.
If we run the model repeatedly, we get the same answer over and over again.
So I think we need to look more closely at why hurricanes move.
As we know well by now, the Bermuda High dominates the atmospheric circulation of the Atlantic
during northern hemisphere summer.
This massive anti-cyclone squats majestically in the central Atlantic, acting like a traffic
cop pushing incipient hurricanes around that are trying to form over the warm tropical
Atlantic waters.
Some of these storms wind up in the central Atlantic and never make landfall before losing
their tropical warm core.
Others enter the Caribbean, heading towards Central American and Gulf Coast destinations.
The simulated motions of these storms have a lot to do with the intensity, size, and
precise position of the Bermuda High in the model, and how well the model's equations
evolved the high with respect to time, which is influenced by model physics.
Is the high intensifying or weakening?
Is it strength fluctuating with time?
Is it shifting around in space?
All of those things can happen.
And all of those things will change the large scale winds.
The hurricane may have swift winds, but they're mainly directed around and around in a circle,
and these winds, ostensibly anyway, aren't doing anything to move the vortex.
But you can move a vortex by putting a large scale wind across the hurricane.
And that's what the Bermuda High and other weather features are doing.
So two models that place the Bermuda High in slightly different positions, having slightly
different intensities, will put slightly different winds across the hurricane vortex,
giving it a distinct trajectory.
Now picture that trajectory being extrapolated in time, and now we see that minor differences
can quickly grow to become major.
Various models may also develop the hurricane in the vertical direction differently, influencing
the simulated storm's depth.
One model's rita might have been a little shallower, a little weaker than another's,
for a whole variety of reasons.
The models might have different parameterizations regarding sea surface fluxes of heat and moisture.
They may have started or developed different sea surface temperatures.
They might be using different treatments of the ocean mix layer, different boundary layer
schemes, all of which contain a large number of approximations and what we call model knobs,
things that can be twisted, things that can be tuned.
This could also reflect resolution differences.
Some models can resolve rita's clouds better than others.
And the ones that can't have the parameterized the heating and moistening effects that the
clouds they can't see can have, and they would need to compensate for that.
And they may undercompensate for that subgrid activity, and they may overcompensate for
it as well.
And depth may also influence the storm track owing to vertical wind shear, the horizontal
wind changing direction with speed or height.
We recall that hurricanes don't like vertical shear too much, but it's hard for them to
avoid shear completely, especially when they move into the middle latitudes where horizontal
temperature gradients are larger.
Because of depth differences, shear differences, and the combination of the two, the wind blowing
across the vortex may differ from model to model.
One might be pushing the storm more strongly to the northwest, another a little bit more
gently northward, directly influencing the track and making the difference, making the
difference between a Houston landfall and one 100 miles away.
At some point here, I'm hoping your thinking has shifted from how could they get it so
wrong to how could any of them gotten it right instead?
That will mean that you more properly appreciate the complexity of this problem, which to some
of us might have seemed pretty simple at first blush, and that's good.
Because the track problem is about to get even more complicated for a very curious reason.
One of the amazing things about tropical cyclones is that they can move themselves, they can
self-propagate.
And this self-propagation mechanism exists because the Earth is curved.
Recall we have two sources of vertical vorticity, our term for spin in a horizontal plane.
We have the spin due to the Earth itself, planetary vorticity, we called that F. And
the spin relative to the Earth, relative vorticity, we called that Zeta.
Recall that F increases with latitude away from the equator because the Coriolis force
vanished there.
And absolute vorticity, F plus Zeta, the sum of the two, is conserved in the absence of
sources and sinks.
So let's consider our hurricane.
North is on top.
It's a cyclone with positive relative vorticity, so Zeta is positive just like F. On the cyclone's
west side, notice air is moving towards the equator.
If it's moving towards the equator, its planetary vorticity is decreasing because that decreases
towards the equator.
But absolute vorticity is conserved.
So its relative vorticity must be increasing to compensate.
And the increasing relative vorticity on the tropical cyclone's west flank means that counterclockwise
spin is being enhanced there.
Meanwhile, on the east flank, it's the opposite situation.
Polar bound air parcels are encountering more planetary vorticity, so they're losing some
of their counterclockwise relative spin.
Relative vorticity is decreasing on the east flank.
So we see there's already a tendency for the cyclone to shift westward towards where the
positive relative vorticity is increasing away from where it's decreasing, and this
has nothing to do with the large-scale winds.
But wait, there's more.
Those relative vorticity changes induce circulations.
Counterclockwise where the vorticity is increasing, clockwise on the other side where it's decreasing.
And further, these circulations come together to cause a wind to develop across the vortex
directed towards the north, a wind that didn't exist before, a wind that only exists now
because the earth is curved.
This by itself will tend to move the vortex forward.
So let's combine these two effects we have identified, the westward and northward tendencies.
These along with counterclockwise advection around the cyclone induces a northward motion
in the vortex, even if the environmental winds are calm, even if the Bermuda high didn't
exist.
Tropical cyclones born in the tropical central Atlantic naturally want to move towards us
in the U.S.
This is called beta-drift because the variation of planetary vorticity f with latitude is
dubbed beta, the second letter in the Greek alphabet.
The beta-drift represents vortex self-propulsion, and it only exists because f varies with
latitude, which forces relative vorticity zeta to increase or decrease as parcels move
away or towards the pole, and f varies with latitude because the earth is curved.
Let's take the next step.
We've already seen that hurricane depth matters if only because of vertical wind shear, and
we keep our eyes on the tropical cyclone's strongest winds just outside the core and
beneath the eye wall.
However, the beta-drift means that track forecasts are also sensitive to the hurricane's width
as well as the strength of its winds far beyond the eye wall.
This is because it's those winds, weak as they may be compared to the furious flow closer
to the center.
It is those winds that are advecting the planetary vorticity on the storm's west and east flanks.
So picture two hurricanes.
They have equal fury near the center, but one has stronger winds, 200, 300, 400 kilometers
and more outward from the core.
Those winds are counterclockwise and part of the hurricane's circulation.
Remember the variation of f with latitude is the key.
The eye wall region winds may be incredibly intense, but the circuit that the air parcels
are making there is not that large.
A parcel's latitude doesn't change much as it goes around and around and around the eye,
but those outer winds are traveling an enormous distance and the winds can turn poleward and
equatorward and as they do so, f changes.
Now it becomes a matter of figuring out how quickly f is changing because that determines
the magnitude of the beta-drift and how quickly f changes for an air parcel depends on how
fast it's moving.
So the wind speed at outer radius, far from the core, will be tremendously impactful on
the path of the hurricane.
And those winds will be stronger if the hurricane is wider.
So we see sometimes size does matter.
Earlier in this course, I mentioned that I consider myself a cloud dynamicist.
I study clouds, how they form and form into organized storms, how they move and how they
influence their surrounding environment.
I'm also interested in all the pieces of the cloud, the droplets and the drops, ice, snow
and hail, and how they're represented in numerical weather prediction models.
I became interested in hurricanes a few years before RITA because I thought that tropical
cyclones would represent an interesting example in how all the little physics of the cloud
and we call that cloud microphysics, by the way, might impact the simulations.
So as RITA was bearing down on the Gulf Coast, I was simulating it from the comparative comfort
and safety of my office.
But I didn't just make one run.
I made an ensemble.
I varied all this little physics along with assumptions regarding how clouds we cannot
resolve influence temperature and humidity of their grid volumes.
When I plotted each of my little RITA's tracks, this is what I found.
There are 15 model forecast tracks shown here, as well as RITA's actual path.
These model runs use 30 kilometer grid boxes, 19 miles on the side.
These runs started when RITA was 54 hours out from land, but I'm showing just the last
days motion, and that's why the tracks aren't all starting at the same place.
And most of the tracks are too far to the west and make landfall near Houston, much
like the NHC model tracks I showed you earlier.
Actually, this plot is eerily similar to the ensemble track plot that was comprised
of many different models and their initial conditions.
But this plot was created using one model, sharing one initial condition, and yet considerable
variation in track was still obtained.
Just by varying model physics relating to clouds.
It turned out that varying these little physics, the simulated hurricane change becoming deeper
or shallower, wider or thinner, stronger or weaker, many of those factors that we've already
seen influence the track.
Let's take a closer look at what we call cloud micro physics.
Picture a cloud.
Even in a well resolved one, with many grid points spanning it, we cannot track every
cloud droplet, ice crystal, raindrop, condensation, nuclei, whatever that the cloud is made of.
But we need to know how condensation particles grow, how they change form and fall.
Because that determines how much heating and cooling is occurring as well as when and where.
Not to mention how much rain forms and falls.
And even large hailstones are pretty small when compared to the grid volume they exist
in, so each of our model boxes will have a huge number of them.
For any given type of condensation, cloud droplet, raindrop, ice crystal, snow aggregate
or hail particle, the most important piece of information we need is size.
The reason is because this determines how quickly that the particles are falling relative
to still air.
Generally larger particles fall faster because they're heavier.
And the size and fall speed of particles both directly and indirectly influence the growth
rate.
Let's take a look.
Another two raindrops, one small, one large, falling through a cloud of smaller cloud droplets.
Owing to their greater weight, the raindrops are falling faster than the cloud droplets
and as a result, collisions are taking place that lead to further raindrop growth.
The large drops growth rate is fast because it falls more quickly and its wider girth
sweeps out a larger area as it descends.
And growth favors girth as I pointed out earlier.
While we have many different raindrop sizes all vying for encounters with cloud droplets
and other particles, we cannot track all these particles.
We cannot track them so we start making assumptions.
Let's examine how we might parameterize this behavior in the raindrops in our grid volume.
A common approach is to presume we have exponentially fewer larger drops than smaller ones.
So picture a plot of the number of particles versus their diameter.
The vertical axis is the log of the number, so an exponential relationship will appear
as a straight line.
And there it is.
We can characterize this distribution with two parameters, its slope and its intercept.
The total rainwater mass in our volume is the area beneath this curve.
And from this information, we can calculate what the mass-weighted average particle diameter
is.
I know the density of liquid water, so the average diameter tells me its weight and how
quickly it falls on average.
Then all of that is used to represent the fall of all of my grid volumes rain, even
the little ones, everything in my grid box.
I've reduced the problem of incredible complexity to a single number.
Next, suppose my raindrops are successful at gathering up cloud droplets and the mass
increases.
Usually, we assume the intercept is fixed.
I hope you recognize there are a lot of assumptions being made here.
So if the mass increases, that increases the area beneath this curve, and our distribution
pivots upward.
And you can see there's more large particles now, and they're going to fall even faster
on average.
And this is only the beginning.
Next my raindrops, my freeze into ice.
Ice may clump into snow, snow may harden into hail, rain might freeze onto that hail,
which may also grab snowflakes on the way down.
There are many, many potential collisions and interactions to parameterize.
And when particles collide, there's always a chance they'll bounce off instead, or break
apart, becoming small again.
And of course they may evaporate, melt, or sublimate on the way.
So our microphysics parameterization has many, many knobs affecting fall speed, particle
size, collection, efficiency, etc., all adding up to different amounts of condensation, falling
at different speeds in different parts of the storm.
And all of that contributes to uncertainty.
So why do we care?
Here's an ensemble of forecast tracks in which some microphysical assumptions were
varied, such as how quickly snow crystals fell, or how fast cloud droplets developed
into rain.
And that quickly resulted in different tracks, and a spread as wide as what we saw before.
The reason?
Because changing how quickly snow falls affects how far it spreads horizontally before melting
or turning into something else.
And where these particles form or melt, they cause temperature changes, through latent
cooling and warming.
It also changes how large and how thick the Hurricanes Anvil cloud shield is, and that
affects how much sunlight reaches the ground, amongst other things.
And these all change how temperature varies in the horizontal.
And these different temperature gradients make different pressure gradients that drive
different winds, not only in the storm's inner core, but also far away beyond the rain bands,
the winds that influence the track.
I mentioned in George Stewart's novel, Storm, I mentioned that in lecture 21, in his novel
he wrote that a shift in the storm track can ruin an empire.
Certainly it can ruin a city, and the storms I just showed you were steered by snow crystals
and cloud droplets.
By the way, in the National Hurricane Center's Rita Ensemble, the best forecast turned out
to be from the simplest model on that day anyway.
And although Rita turned away from Houston, that turned out to be an excellent 1,084 day
forecast for 2008's Hurricane Ike, which did put Houston and Galveston on its dirty side.
In fact, in many respects, it was a better long-range forecast for Ike than the ones
issued during Ike's own Gulf Coast approach.
Rita was the storm that was supposed to hit Houston and didn't.
Ike was supposed to miss Houston and did.
It's part of the difference between weather and climate.
I started this series of lectures on meteorology with three little words, nature abhors extremes.
It seemed ridiculous at the time, but I hope it doesn't seem quite so silly now.
Despite the fact that the last 23 lectures we have focused on nature, it is most extreme.
It's hail, high winds, typhoons and tornadoes, lightning and thunder, floods and droughts.
Each of those things has turned out to be a symptom of an underlying disease, a manifestation
of imbalance, of stress.
Nature differences driving winds, charging balances making light.
It's not that nature really wants to do this.
She gets frustrated by lousy conductivity of heat and electricity, selective absorptivity
of air molecules, the rotation and shape of our planet.
And like a frustrated child, nature throws tantrums sometimes.
Okay, she throws tantrums a lot.
I created this course guided by Einstein's dictum, make things as simple as possible
but no simpler.
I think I've made a few things too simple and probably a few things weren't quite
simple enough.
In the first lecture, I asked a whole lot of questions and we answered those through
the course.
Why the sea breeze is cool, why you get bumped flying over mountains, how barometers work,
the blue sky and so on.
We saw you can't flash freeze New York City by pulling air down from the stratosphere
and we saw how El Nino sloshes around in that big bathtub called the Pacific.
But actually, there's one question I didn't answer.
I left you on an airplane in the exit row worried about accidentally opening the emergency
exit in your sleep.
But I think you know the answer already.
If you don't, next time you get on an airplane, look at the doors.
They open outward, but you have to pull them inward first against the pressure gradient.
In mid-flight, against the gigantic pressure gradient, you couldn't open that door if
you tried.
Sometimes pressure is your friend, so sleep tight and thank you for taking my course.
Thank you.
Bye.
