At Harvard's Kirkland House dormitory, on an autumn afternoon in 2004, a sophomore
computer science major named Mark Zuckerberg is tapping away at his computer. He was born
in 1984 in New York. In his dorm room, he is working on one of his many software projects.
In those days, some colleges produced little booklets with the photographs of incoming freshmen
as a way for people to get to know one another, to start recognizing people one had classes
with or perhaps passed in the quads. These booklets were called Facebooks.
The semester previous, in October 2003, Zuckerberg had aggregated these photographs, sometimes
by hacking into dorm systems, and had created a website called FaceMash, which invited Harvard
undergraduates to vote on who was most attractive. Harvard shut that project down, responding
to complaints, including issues of privacy. Those photographs had not been taken for this
project. They hadn't been volunteered by students. It had simply been taken. But one
other fact was especially striking, the intensity of usage of this website. Apparently, students
who started clicking through in the program would not stop. The activity somehow had a
compelling and almost entrancing aspect. Let off with a relatively light censure, Zuckerberg
now launched a new project, initially called the Facebook. Later, the was dropped, so I'll
refer to it here as Facebook, all this from his dorm room. On Wednesday, February 4th,
2004, in the afternoon, it was active. It was a platform with a minimalist appearance
which allowed individuals to post information about themselves and a photograph, later,
many photographs. Using their real names, users chose what to post about themselves,
and then established links with others, called friending, with whom they would share content.
The element of flirtation was there too, of course. Users poked friends. No one knew exactly
what that poke signified, which of course made it all the more exciting.
At first limited to Harvard, Facebook soon expanded to other Ivy League schools, then
to universities and colleges across America, and then overseas. Soon, Zuckerberg left Harvard,
moving to Palo Alto in Silicon Valley, to work on the company full-time. Its growth was phenomenal,
unprecedented. One collaborator in Facebook described its early aims. Not to get rich,
but motivated by the idea, let's build something that has lasting cultural value and try to take
over the world. In the summer of 2012, Facebook was approaching 100 million users worldwide,
and it's now an integral part of a new digitally linked modernity that we inhabit today. Today,
2 billion people are online. E-commerce sales now are at $8 trillion a year, but only a few
decades ago, this world would have seemed improbable, almost magical in some of its capacities. Yet,
in fact, there had been some who had dreamed of a world like this in dim outline. Those dreamers
were librarians and creators of encyclopedias. In some sense, the dream of making information
accessible is as old as libraries. Think of the tremendous aggregation of knowledge that the
ancient library of Alexandria represented. It was this impulse also that was the motivating concept
for the project of Diderot's Encyclopedia, which we discussed in an earlier lecture, to demonstrate
and make useful to everyone the interrelatedness of all knowledge. Even closer to our own times,
another figure elaborated on this dream. This was H.G. Wells, that prolific and visionary British
author of scientific romances whom we've met in many earlier lectures in this course. We encountered
him wrestling with the problem of human evolution, and warning of the destructive impact of human
flight when turned to air war. We saw him anticipating the fearful outlines of atomic weapons,
and also looking forward to humans reaching the moon. So, it should hardly surprise anyone to hear
that Wells also projected an idea that anticipated the Internet. In 1938, H.G. Wells published a
book called World Brain, in which he argued that we would have world peace if everyone were given
the same frame of reference so that science might rule the world. That frame of reference was to be
provided by what he called a world encyclopedia as a new global institution. Wells quite deliberately
referenced Diderot's encyclopedia as his inspiration for this idea, but he wanted to push it further
to make it something far larger and more extensive. His project would gather up all the most up to
date and authoritative knowledge of experts and make it available, perhaps by the new technology
for those days of microfilm, so that it would be, as he said, the mental background of every
intelligent man in the world, an undogmatic Bible to a world culture. We might even call it a
planetary consciousness. Universities and research laboratories would constantly work on updating
this world brain, or world encyclopedia. The world encyclopedia would, he promised, end up
holding the world together mentally. The result of such a new institution would be virtually a
network of nerves that spread worldwide, a world brain. As the storm clouds of the Second World
War gathered before the world had even recovered from the First Total War, Wells undertook a
lecture tour, including to the United States, all as an effort to stir up general enthusiasm for
the world encyclopedia, but he actually found very little response. Shortly afterwards, however,
another important scientific figure, even though he's largely unknown today, also weighed concepts
that foreshadowed the Internet. This was a person we briefly encountered before in a previous
lecture, Vannevar Bush, a scientist and one of the administrators of the Manhattan Project to
build the atomic bomb. In a sense, Vannevar Bush was right at that crucial intersection of science
and policy. In 1945, while the Second World War was still raging, Vannevar Bush published an article
in Atlantic Magazine in which he proposed what he called a memex machine as a new kind of encyclopedia.
Memex stood for memory extender. A memex would be a desk with a built-in microfilm reader and a
collection of a fully indexed set of microfilms containing all the contents of a full research
library. Following links and establishing connections between pages of different texts,
the memex would in a sense become a browsable auxiliary mind.
The hardware that made such shadowy anticipations of the Internet a practical reality, obviously,
was the modern computer. Computers likewise had their origins in World War II. The first
electronic general computer is often considered the ENIAC machine of 1946. This acronym stood for
the Electronic Numerical Integrator and Computer. It was developed at the University of Pennsylvania's
Engineering School for the U.S. Army Ballistic Research Laboratory for use in computing artillery
firing tables. Originally, at the start of the Second World War, a computer was not a thing,
but a person, someone who did these complicated mathematical calculations himself or very often
herself. Now this task could be automated. Then in 1950, at Princeton's Institute for
Advanced Study, the next step was taken with the building of an all-purpose computer for
the calculations that were needed to build a hydrogen bomb. Thus, as the historian of science,
George Dyson concludes, quote, the digital universe and the hydrogen bomb were brought into
existence at the same time, end quote. Building on this growing work on computers,
the stage was set for the Internet. It is sometimes called the network of networks,
a way of connecting up computer networks. In 1969 in the United States, the ARPA net was
developed by ARPA, or Advanced Research Projects Agency, of the U.S. Department of Defense.
The ARPA net linked a number of West Coast universities where government-supported research
was taking place. Then, through a program called Internetting, other research networks were linked
up to the ARPA net, using standardized protocols for communication. The National Science Foundation
also worked on spreading access more widely. After 1993, commercial users, who earlier had
not been allowed to participate in what was meant to be really a purely research undertaking,
were included, and expansion now accelerated. Let me now state what almost rings like an
understatement. The Internet has revolutionized how information is accessed and used. It truly
merits the label of an information revolution. The Internet enables email, news groups, digital
libraries, and has transformed commerce, spawning e-businesses. From the late 1980s, the Internet
was doubling every year, first concentrated in the United States and Europe, and then spreading
worldwide. This, in turn, set the stage for the World Wide Web, known informally as the Web.
This is an Internet application which gives web users the chance to survey documents connected
by hypertext or hyperlinks. To begin with, the British engineer and computer scientist Tim
Berners-Lee and his co-workers at CERN, a laboratory in Geneva, Switzerland, developed a protocol for
standardized communication and a browser by 1992. Building on this work, the Mosaic browser was
developed at the University of Illinois and made available in 1993, and this was followed by the
Netscape Navigator System in 1994. The use of the Web became increasingly popular and indeed
taken for granted. We can sense just how ubiquitous and common the Internet had become
by mentioning a famous cartoon in the July 1993 issue of The New Yorker. In this famous cartoon
by Peter Steiner, two dogs are sitting in front of a computer terminal, and one dog is explaining
to the other. On the Internet, nobody knows you're a dog. This has actually become the most
reprinted cartoon from that magazine, because it says something about the fluidity of online
activities and online personality. But it was also a key cultural moment oddly enough because
of what was not said, the dog that did not bark, in other words. No explanation was provided in
that cartoon or needed for what the Internet was. It had now become common cultural property.
Now in these early stages, not everybody was enthusiastic. Dubious reactions included the
comment that was made in 2001 by that great American philosopher Homer Simpson who asked,
the Internet, is that thing still around? Back in 1994, Time Magazine concluded that the Internet
was not designed for doing commerce, and Newsweek called it a trendy and oversold community.
Others declared that the Internet would really be the CB radio of the 1990s, a fad, not something
really lasting and durable. But others were entranced by the seemingly unlimited potential they saw,
enlivened with an almost utopian sense of the possibilities and the commercial potential
of the Internet, investors and speculators poured in money and enthusiasm and drove up what
afterwards was known as the dot-com bubble, or the Internet bubble, from 1995 to the fall of 2001
when it burst. This shakeout, paradoxically, allowed for the rise of a new set of approaches to
the Internet, what's been called the Web 2.0, a new version of the phenomenon. The term was
popularized by Internet entrepreneur Tim O'Reilly to describe patterns, practices and programs for
the Internet that were interactive, collaborative, focused on empowering the user and as O'Reilly
puts it, to harness collective intelligence. Part of this includes so-called social media,
in which the user is not a passive absorber of content that's provided, but a creator,
a sharer, a shaper and an exchanger of content. This includes a wide and ever-expanding range of
technical approaches, email, instant messaging, text messaging, photo sharing, blogging, and
social networking sites like Twitter or Facebook. From its origins that afternoon,
in a Harvard dorm room in 2004, Zuckerberg's enterprise grew to fantastic and today still
expanding proportions. In the summer of 2012, Facebook was approaching 100 million users.
In 2011, it had earned $3.7 billion. In 2011 as well, Facebook reportedly became the first
website to receive a trillion page views in one month. The intensity of usage is striking.
Over half of the users of Facebook log on daily. In a book by the same name, David Kirkpatrick
calls this the Facebook effect. He considers it a fundamentally new form of communication
with fundamentally new interpersonal and social effects, as information becomes viral and moves
swiftly across networks. Individuals now can take on the role of publishers, initiators of new ideas
and messages. So consider this, each man or woman is potentially his or her own Gutenberg.
Every month, some 30 billion pieces of information are posted by users, photos, links, messages.
And all of this was directed by a young man who taught himself to be a CEO while sticking to his
trademark style of aggressively informal dress, t-shirts, hooded sweatshirts and rubber flip flops.
Like Thomas Edison, who we discussed in an earlier lecture, Zuckerberg was adept at
integrating earlier inventions and earlier developments in a new way. In fact, there were
lawsuits alleging that the idea for Facebook was originated by others, but these were settled.
As his company grew, Mark Zuckerberg refused repeated offers to be bought out.
Why? His ambitions apparently ran and run in a different direction. On his own Facebook page,
Zuckerberg describes himself and his aims. Openness, breaking things, revolutions,
information flow, minimalism, making things, eliminating desire for all that really doesn't matter.
Striving to express what is distinctive and new about Facebook,
Zuckerberg repeatedly has described it as a utility. There's an echo there of the way in
which Dieter Rose and Cyclopédie had stressed utility and usefulness in its project to diffuse
information as well. But Facebook breaks new ground. It is perhaps the fastest growing company
of any kind in history. It is also resolutely global, with about three quarters of its users
outside the United States using a multitude of different languages.
In Spring 2012, Facebook's initial public offering was the largest internet stock market
launch in history. At first, it did not do well, but the future remains to be seen.
The key question really is what revenue model can be developed out of the promise of knowing
more about consumers so that advertising can be targeted as it never has been before?
But the impact that Facebook and other social media have already had in current events is just
beyond question. A few examples. In 2008, activists in Colombia used Facebook to rally massive
demonstrations against the FARC guerrilla movement and its kidnapping of civilians.
In 2009, the youthful green movement in Iran used Facebook and Twitter to protest against
what they charged were fraudulent elections. Also in Iran, a movement called the One Million
Signatures Campaign seeks to collect signatures for a petition for women's equality in legislation,
using both personal contacts face-to-face and the internet. Social media also played a role
in the uprisings in Arab countries since December of 2010. And I can make a confident prediction.
If you read or listen to the news today, you will find your own examples of this ongoing impact.
As the examples indicate, not every movement using this media succeeds, but its use has become vital.
Some thinkers see this as part of a broader democratization
that's being forced worldwide by the internet. University of Tennessee law professor Glenn
Reynolds, who blogs as Instapundit, calls this the spread of horizontal knowledge.
What he means is this, if earlier media focused on one authoritative voice
speaking down to the many vertically, this new technology encourages a more diffuse,
participatory, horizontal mode of communication. And Reynolds sees this as part of a new dynamic
of the 21st century, in which an army of Davids confronts Goliath, or centralized authority.
Decentralized and self-motivated individuals aggregate, and they can challenge an earlier
model of centralized authority. Now, while it is too early to make definitive statements,
what can we say of the promise and perils of this new media as part of our modernity?
We need to, in our discussion, treat both the hopes and fears in tandem,
because often they're actually linked. What is the internet doing to us?
Is being connected constantly to social media making us more global and connected
with richer social lives and constant contact? Or is the internet actually inviting us to become
more insular, locked into small niche societies, and not interacting with those who are different
from us, however that difference is defined? Internet use also can lead, as has been recognized,
to compulsive overuse or addiction. Just recall the trance-like states that some of the first
users of Facebook experienced, clicking through again and again, unable to get enough of the
experience. Others worry that social media fosters exhibitionism and narcissism. Well,
certainly there's a lot of both offline, but online, will we soon be drowning in a sea of blogs
that nobody reads, with individuals revealing more and more of themselves to less and less
effect without an audience? Is internet use making us lonely with illusory relationships
rather than deep real bonds? Some commentators speak of an internet paradox,
noting that internet usage correlates with reports of feeling lonely. Is the virtual world
of being online, displacing what has been throughout history until now the norm of interaction
offline? So some authors speak with a tinge of irony of there being cyberspace and then
meatspace. By meatspace, they mean the opposite of cyberspace, the flesh and blood world of physical
reality. At an even more basic level of concern, some are also asking whether and how use of the
internet and digital technology might actually be rewiring our brains. As one article asks quite
bluntly, is Google making us stupid? Numerous news reports speak of changes in reading habits,
which had already been declining even before the internet, but now these reading habits are being
recast again. Lesson detention spans, scanning rather than deeply engaging with ideas as we read.
Might we say that Gutenberg's invention of the printed word is dying? Are e-books
instantly downloadable, replacing Gutenberg's original invention? In 2012, for instance,
there were over a hundred million e-readers and tablets being used in the United States,
and e-books generated more sales than traditional adult hardcover books. Or might we say that a
revival of reading is being pressed forward in this way? In studies, many workers today
complain of simply being swamped by a flood of data, overwhelmed by information. Well,
that flood apparently will not stop. In fact, it was estimated in 2011 that by 2020,
data will expand 40 times over. Some optimists propose that we're actually already seeing
human adaptation to these challenges. They speak of the younger generations as digital natives,
who are really in their element because a digital world is the only one they've ever known.
Indeed, those with young children can probably testify how readily and fearlessly
they take to the media. All of the rest of us, the older ones, are called digital immigrants,
who may adjust more or less successfully, but never completely, to the new digital environment.
Some experts argue that education will have to be fundamentally remade to speak to the digital
natives. But if the internet empowers, it can also empower the destructive and the dangerous
criminals and terrorists and small-scale thugs alike. We read reports of cyber-bullying, identity
theft and deception online are everyday events. Remember that Peter Steiner cartoon about how
nobody knows who or what you are on the internet? Governments warn of the dangers of cyber attacks
and cyber war. These are not merely potential. In fact, in 2007 and 2008, Estonia and Georgia
both experienced attacks from Russian computers during international disputes,
and then there is the danger within. What does the new technology imply in terms of the power
of governments to keep people under surveillance or to control their access to information?
In this context, some critics speak of China's intensifying efforts to block access to websites
it does not want Chinese people to visit, and some call this the Great Firewall of China.
But it is also part of what has sometimes been called the dictator's dilemma.
If a modern ruler shuts off access to the internet, and thus the world at large,
the country will suffer economically, and the dictator may provoke even bigger discontent
and revolt as a consequence, because now, to be modern, is to be online.
Throughout this course, we have asked how the turning points we examine
have redefined authority. In the case of the internet and digital media,
it may be too early to say, but certainly earlier formulas seem up for grabs. For instance,
does the Westphalian system of territorial sovereignty that grew up after 1648, and which
we discussed in an earlier lecture, does all that still apply in cyberspace? The internet is
abolishing borders. Also, are individuals empowered enough in this new world to escape
later waves of technological change that might be devoted to regimenting and suppressing individuals?
All of this is new, and yet, we've also seen aspects of it before. In many ways,
the internet and social media essentially supercharge key earlier developments we saw in this
course, including print culture and media, encyclopedic knowledge, an increasing global
reach, and the elaboration of civil societies. At a far slower pace, Gutenberg's printing press,
and then motion pictures, work to diffuse knowledge in their day as well.
The impulse to be encyclopedic, comprehensive, and to diffuse useful knowledge was the core idea
of Diderot's encyclopedia, and of the research of Levenhoek and the discovery of penicillin.
The theme of global reach is one that we've encountered from the beginning of our course,
even if at first it was achingly slow and uncertain. The voyages of Chenghe and Columbus,
the trading travel of the East India companies, and then later, powered flight.
Globalization as a phenomenon was also there in a negative way during the opium wars,
and as imperialism spread, and it also appeared as a global challenge to imperialism during the
Russo-Japanese War. The perils of the atomic weapon, and then the great achievement of humans
walking on the moon, likewise, were global and planetary in impact. Issues of how to build a
civil society for modernity we also saw repeatedly. At the 1648 piece of Westphalia, in the American
Revolution, the French Revolution, and in the abolitionist movement, in the movement for women's
suffrage, and in the fall of the Berlin Wall. What is new is the intensity of interconnectedness
and its speed, but we see that interconnectedness as such is not new or unique to our times.
And yet, let me here introduce a radical alternative that some thinkers have proposed.
What if we're actually now at the turning point of turning points today in our own lives?
Some thinkers have called this the singularity, and they claim that the singularity is approaching
fast. What they mean by singularity is a transformation and progress in technology
so profound that at present we cannot even begin to predict what happens
after this unique moment in the future. Will technology, for instance, be embedded in human
bodies with the result of creating something beyond the human? What follows on the manipulation of
the human genome, or the creation of artificial intelligence, and to what ends? Does the Internet
and its proliferating connections eventually grow into the kind of world brain that H.G. Wells
envisioned? What happens next? As we end our course on the turning points of modern history,
we should ask what it all means for us, who are so busy being modern in our everyday lives.
In this course we traced old turning points, and ones that continue to unfold around us today.
We can conclude, I hope, on ultimately an optimistic note. As all these turning points
have appeared and left our world transformed, a constant has been the deeply creative response
that humans have found to meet those changes, or even to push them further. That creative response
to the unending challenge of the new is in great part what being modern means to us today.
We need thus to be creative and attentive as we ask urgent questions. Is a turning point
building in our world at this very moment? Are we living through a turning point right now?
The ancient Greek philosopher Heraclitus uttered a thought which is as true today as it was 25
centuries ago. The only thing that is constant is change. Thus, achieving a kind of creative synthesis
of what is abiding, time-tested, and valuable, with that which is new, exciting, dynamic,
and promising, that is how we will survive and thrive. Thank you for being a part of this course,
and let me wish everyone the very best turning points and creative responses to those turning
points for the future.
