You can never be too safe. Is there a mother anywhere that has not ended the conversation
with one of her children by saying just exactly that? So you should probably pity the mother
of economists because you know exactly what they're going to say. They'll say, actually
mom, you can be too safe. And that's really the topic we're going to get into today in
lecture number seven in our series on how to think like an economist. We're going to
explore the reasoning behind that conclusion that an economist would reach that there is
such a thing as being too safe. And you know basically the reason for that. If there's
no such thing as a free lunch, there's no such thing as free safety either. Every increase
in safety has its cost, if not in dollars, at least in opportunity cost. And we also
know that with diminishing marginal value and rising marginal cost at some point to be just
a little more safe, adding a little more safety is not going to justify the rising cost of
achieving it. Like ignorance, there's an optimal level of risk and there's an optimal level
of safety. So the question to start with is how much safety is too much? How much risk
is too little? If we remain consistent to the position we've adopted, the one we borrowed
from Pareto, then individuals get to decide for themselves what is the value of safety
to them. No outsider, no economist, nor their person can decide what should matter most
to them or how much. But every single time you get in an automobile and drive somewhere
or you cross a street upon which others are driving, you do take a risk of harm and death.
And yet we do it willingly. And if we believe in rationality, that in itself is proof that
we must value getting to the destination enough to justify undertaking that risk. Now whenever
public policy issues are under consideration, economists think we must have to put some
measurable value on human lives, at least in a statistical sense. If we're going to
drill a tunnel through a mountain, there's a high likelihood that someone will die in
the process. So we must decide whether or not it's worth it. Now, I made that argument
in class once and I got a very heated response from a student. He said, the value of human
life is infinite. Now I think he was really talking about his own human life, but in general
he was making that argument. But I said to him, I'm not putting a value on your life.
You are. So I asked him, how did you get here this morning? He said, I drove. I said, what
did you drive? And he listed some small old car appropriate for a limited student budget.
And I said, you should have come in an armored personnel carrier. You can get them on surplus
from the U.S. government and you would have been much safer if you'd done that. And he
said what you would have said. That's absurd. But then I said to him that the reduced risk
of driving in that carrier apparently was not worth the cost to you. And I rest my case.
You don't value your life as highly as you thought you did. Well, there the conversation
just lapsed into contemplative silence. But our own behavior proves over and over again
that we do evaluate ourselves at times as being too safe. And that's true whenever the
cost of reducing or eliminating the risk becomes too high. I suppose we could live in hermetically
sealed bubbles and bomb proof bunkers underground and we could live long lives. But literally
those would be lives not worth living. It would cost us too much, not just in terms
of money, but in terms of all the experiences we'd have to give up. There certainly are
some aspects of life that we cherish even more than a bit more safety. And many of the
things that give life its meaning come with risks. You can be much too safe. Now, safety
isn't just about fatal risks or grievous bodily harm. That reasoning applies to all the various
risks we face in life. That car that you just bought might turn out to be a lemon. The retirement
plan where you've been putting your money for the last 30 years could go broke. The house
that you built so carefully could burn down. That career choice you made could turn out
badly. All of those are risks we take in a world in which everything affects everything
else where some consequences and influences are always unknowable where no one is in control.
There will always be risks. It makes sense for us to understand them, to manage them,
to contain them. But it's beyond the realm of practicality or even wisdom to imagine
eliminating them. The issue is how do you live with risks? We're going to talk about
that today. Economists tend to distinguish two things when they're in this kind of an
area of discussion. The first one is uncertainty and the other one is risk. Now, uncertainty
for economists refers to a situation where the information we have is incomplete. And
that was the topic of the last lecture. And, of course, uncertainty, I guess, in one sense
can be cured by having more and better information. We would be more accurately able to predict
the future. If we had perfect information, uncertainty would be eliminated. But, of
course, last time we found out that to try and have perfect information would be irrationally
expensive. Today, what I want to talk about is thinking like an economist about the other
source of the future's unknowability. And this is a situation that economists call risk.
There are places where we could have more and more and more information and we still
wouldn't be able to know what we really want to know. We would like to know some future event
is going to come out in certainty. Some cases are absolutely unknowable. If you spent your
entire fortune gathering data on coin flipping, you got information on the massive coins, on
rotational forces, on wind velocity, on the bounce response from different surfaces, no matter how
much information you got, you would never know for certain what's going to happen the next time
you flip that coin. If you try to predict the outcome from a large number of coin flips,
even with all the information available to you, you're going to be wrong half the time.
That's the kind of an event that is marked by randomness. The certainty about that outcome is
impossible. It's what we call probabilistic. It depends on chance. There is unavoidable risk.
So, what is the smart thing to do when faced with a decision that involves this kind of risk?
Faced with probabilistic decisions, economists use this formalized concept that goes under the name
of expected value. Now, economists know that life is not like a video game. There's not a redo button.
There's not a play over game. There's not a chance to redo it time and time again when we face
situations of risk. But what expected value is conceptually is what would happen if we could
replay decisions over and over and over again? What would be the average outcome if we remade the
decision and experienced its consequences time and time again? What would be the average result?
To find the expected value, you need a couple of things. You need to know the probability
of each possible outcome, and you need to be able to multiply that by the value of the outcome
and then add them all up. We'll try it in a minute to make sure you understand what I'm saying.
It sounds like it's complicated, but it really isn't that hard to do. As long as,
as long as, you really know the probabilities and you know the payoffs. Now, our purpose in this
course is really to focus mostly on the conceptual vision of economic thinking. I don't think any
of us wants to do a lot of problem sets or work through a number of quantitative examples,
but I'm going to go through one simple example just for the purpose of trying to make these
concepts a little bit clearer for us. So first, what we have to do. What is the official formal
mathematical definition of probability? Well, formally, the probability of any event, such as
flipping heads, rolling a three on a die, the probability of any event is that possible outcome
divided by the total number of possible outcomes. You flip a coin, how many possibilities are there?
Two, heads is one of the two. So the probability of coming up heads is one divided by two,
one half or point five. And I'll leave it to you to figure out that that's also the probability
of flipping tails. If I were to roll a single die, coming up three is one of six possibilities.
So the probability of rolling a three is one over six or probability of point one six six seven.
And I'm going to tell you that if I roll that die over and over and over again,
I have an extraordinary amount of confidence that a three is going to show up about one sixth
of the time on average. But I'm not very confident about what's going to come up the very next time
I roll. Once I know the probabilities, then if I can accurately estimate the value of the outcomes,
I can calculate expected value. So let's try it. I'm going to propose a game to you. It's a really
fun game, heads I win, tails you lose. I like that kind of a game. So here's the rules. You're
going to flip a coin. And if it comes up heads, you pay me a dollar. If it comes up tails, you pay me
50 cents. It's not like a good game to you. I like it. If you do that over and over and over,
half the time I'm going to win a dollar and half the time I'm going to win 50 cents. But my average
winnings and your average loss will be 75 cents per coin flip. Now no flip at all are 75 cents
actually going to change hands. But that's the average that takes place. That's nice to know,
I suppose. And I don't see many of you saying that you really want to play that game with me.
And knowing expected value probably didn't have much of an influence on that decision.
You don't need a lot of mathematical sophistication to know that that game would be all about losses
for you. So let me make it just a little more useful. Let's change the rules of the game.
Now suppose I say I will pay you 80 cents to flip the coin under the rules I just announced.
Would you do it then? You would certainly gain 80 cents from each coin flip. And what would be
the expected value, the average of your losses over the long run? On average, you'd win 5 cents.
If instead I said I'll pay you 70 cents a coin flip to play this game, you could be pretty sure
that over the long run you'd lose a nickel on average. Knowing the expected value lets you know
how to make a rational decision in that kind of a situation involving risk.
Casinos get very rich because they're very good at calculating expected values.
A lot of gamblers get very poor very fast because they do not know how to calculate
expected values or they just don't believe the numbers apply to them.
Textbooks are very fond of using examples for probability like a coin flip or a roll of the
die or pulling an ace out of a deck of cards because it's very easy to determine the actual
objective probabilities in cases like that. And I suppose there are some places in real life where
objective probabilities can be determined. In my home state, the lottery has a lot of
different prizes and they tell you on the website what the probabilities are. For example,
the $175,000 prize coming from a $1 bet happens once in every 135 million tickets.
If you go through the numbers and try it if you want, if you go through the numbers,
the expected value per ticket then comes out to be about six cents. People trade a $1 bet
for an expected return of six cents. Scratch tickets in my state also, some of them have a
one in one million chance of getting a $20,000 prize. That's going to come out to an expected
value of about two cents. It's a lot like the game I tried to play with you. No lottery ticket
ever pays two cents. $999,999. Pay nothing. One pays $20,000. On average, people get back
two cents for their dollar bet. That's probably the basis behind the bumper sticker that says
the lottery is a tax on people who are bad at math. It's probably also bad news for the one
fourth of Americans who polls tell us, think that winning the lottery is a viable means of
achieving financial security. Maybe those people aren't completely rational, maybe just they're
ill-informed. But there's a lot in our lives that's much more complex than flipping a coin or
rolling a die. For a lot of the risks we face in life, life there really aren't objective
probabilities that are well and easily defined. Each event, each participant, each situation is
just a little bit different from the others. I would like to know what are the chances that
my house is going to catch fire, that my car is the one that's going to be stolen, that I'm going
to need dentures or that I'm going to have to spend time in the hospital during the coming
year. I can't simply count the sides on a die or the number of cards in a deck in order to
calculate those probabilities. So we estimate them using historical data, but I always want to know,
is that directly applicable to me? Is the future going to be the same as the past? Is that the
correct comparison? For example, the national risk of dying in a hurricane in any given year
in the United States is about one in six million. But I've got to believe that if I live in North
Dakota, it's not going to be that high. And I've got to believe if I live in New Orleans, it's going
to be higher than that. If I live on high ground, my chances are better. If I live below sea level,
they're probably worse. If I want to know about the probability of dying in a fire, if you take
the total number of fire deaths in the U.S. divided by the population, the average risk of a fire
death is about one in 91,000. But I'm pretty sure that if I live with a smoker, the chances go up.
And if I install fire alarms, the chances will go down. So mathematicians can tell us with certainty
what the odds are for well-defined games of chance. I wish it was as easy to find out what the real
odds are in situations that we live with in the real world. But unfortunately, there is uncertainty
about probability and risk. We don't have enough information to tell us what the true probabilistic
risks are. And that can certainly make applying this concept of expected value more difficult.
I want to give you a personal example of ignorance about consequences. And this is a true,
and believe me, a profoundly creepy story. We awoke one night last summer with a bat
circling around my wife's head. I immediately decided I would be the hero of the hour. I grabbed
a tennis racket, we opened a skylight, and I herded that bat back out into the night.
And I thought when we climbed back into bed, adventure done. Hero status achieved. Maybe
some bad dreams for a couple of nights, but the adventure was over. Not quite, my wife informed
me. She said the state of Massachusetts has a strict protocol. Bats are known to have high
incidence of rabies. They have sharp teeth. If one had bitten us in the hairline while
we were asleep, it would have been invisible. It's highly unlikely. The probability was tiny,
not zero, but small. But the state said, you must be treated for rabies. I said, why? And the answer
is, of course, the state was calculating expected value. Certainly, the probability of exposure
was small, but the consequences of being wrong would have been sure, certain, and painful death.
There is no cure for rabies once contracted. That's a payoff I did not want to incur. I very much
wish to avoid. So I did my own expected value calculations and I agreed with the state. We
got the shots. So let's review some of the things we've talked about. Remember the whole idea of
thinking like an economist means that you're going to be ever cognizant of your own incentives and
the incentives that others have. So I thought maybe we'd take a moment and look at an example
of thinking about different incentives for different players in situations of risk. And the example I
want to talk about is the way in which hedge fund managers get compensated as they manage funds.
So in recent times, there's been a relatively large movement of people shifting,
particularly wealthy people, shifting some of their funds into hedge funds. These are funds,
they're not publicly traded. The rules are you can only take your money out at preset intervals.
You're turning over control to a professional manager. And originally, they used a lot of
complex strategies to buy some of this to hedge against inflation and some of that
to hedge against market movements. But in general, their private investment funds
professionally managed. It's in the compensation formula where it's interesting to think like
an economist. The standard in the United States today is what they call two and twenty. And that
means there's a fee of two percent on the amount of money involved, plus the manager gets 20 percent
of any gains that he or she can generate. But it's kind of a one way street. The manager does
not share in any losses. And that means, of course, that the manager's payoffs are really
different from those of his clients. So here's a hypothetical example. Suppose you had a million
dollars invested in a hedge fund and the manager's considering two possible investments.
One of them is a risky investment. There's a 50 percent chance it'll gain a half million
and a 50 percent chance it'll lose a half million. The other alternative is to hold a safe one million
dollar investment. So your choices, hold a million or take a coin flip bet that you can gain or lose
half a million dollars. So pop quiz, let's review. What's the expected value of the return on this
risky investment? Well, it's 0.5 times a half million dollars, which is plus 250,000. And 0.5
times a loss of $500,000, which is minus 250,000. And that's total flat zero. Maybe you like the
thrill. Maybe you like the gamble. But in a probabilistic sense, the expected value of return
on that investment is zero. So you can either have the zero return safe investment
or a zero expected value risky investment. That's your option. But let's think about the manager.
The manager keeps the money in the safe one million dollar asset. He gets a $2,000 fee for
managing the money, two percent of your million. But if he puts money in the risky investment,
his expected value is not zero. His expected value is plus $52,000. Why? Well, if the investment
pays off and there's a 50% chance it will, he'll get 2% of 1.5 million. That's 3000. Plus 20%
of the gain of half a million. That's 100,000. So if it pays off, he gets $103,000. If the investment
loses and there's a 50% chance it will, his fee falls down to $1,000. And 2% of that is from the
remaining half million. And his expected value is $52,000. And yours was zero. That means the
manager has an incentive to take on quite a bit more risk than many of his clients might deem wise.
Understanding this concept of expected value helps us define our own optimal strategies when we
face risks. And that's an important part of thinking like an economist. Using it to anticipate
how others like the hedge fund manager will behave is at least as valuable as figuring out how we
should behave. So now let's return to a new wrinkle in considering the difficulties that arise from
uncertainty about risks. It's not hard to see the usefulness of this expected value concept,
but the actual values of probabilities or payoffs can be very difficult to figure out and they're
not precisely known. Even worse, there's ample evidence that our knowledge of them can be systematically
distorted partly by our own psychological processes. Remember we talked before about the
wisdom of crowds in a book by James Sir Wiki? Today I want to talk about the foolishness of crowds.
There's a subfield in economics that really comes out of psychology. We call it behavioral
economics. We'll return to that in more detail in lecture 11. But it spends a lot of time looking
at perceptions and how they're formed. In Sir Wiki's book, he talked about doing experiments
estimating the number of jelly beans in a jar and he told that unique story of finding a lost
submarine by averaging the independent predictions of multiple experts. But what was crucial in that?
Remember, each individual had to come up with an independent estimate or conclusion. They couldn't
confer, they couldn't share ideas, they couldn't push each other to conclusions. It was independent
conclusions that got aggregated and that led to the wisdom of the crowd. But we compare that to
what we find in a lot of the psychological evidence and that is when there's a lot of group
interaction, when the people who are making decisions are feeding off of each other,
were as likely to cascade off into a madness of crowds rather than the wisdom of crowds.
Rationally ignorant people, and remember we're all rationally ignorant,
are susceptible to what are called information cascades that can lead us all wildly astray.
How about a hypothetical? I try to keep my children healthy and so I decide that I want to feed them
apples and apple juice and applesauce as alternative to some of the junk snacks that are so popular
these days. And let's suppose that apple growers in order to try and reduce their costs have found
that there's a chemical that will just make all the apples ripen at the same time so that it's
cheaper to harvest. Now, I really don't know anything about this. I'm pretty sure I couldn't
pronounce its chemical name if I had to. I have no idea if it rapidly breaks down. I have no idea
if there's any residue in what my children eat. I have no idea if it has potential carcinogenic
properties. But someone, anyone, makes a public assertion that they have figured out that that
chemical poses a significant risk of harm to my children. I have no way to evaluate that claim.
Remember, I'm rationally ignorant. The assertion itself seems to me to be evidence that somebody
must know something that I don't know. Somebody must know that they're harmful effects. And then
if a second person comes and persuaded by the first says, yes, I too believe they're harmful effects,
I know that I'm ignorant of the facts. I don't know if they are. But if a third and a fourth
and a fifth are moved by the evidence, the evidence that two other people think there's a risk,
then it's more and more likely it's going to make sense for me to assume they know
something I don't know. And a social information cascade may follow. Each new adherent to the
assertion becomes new evidence that it's true. And I have no grounds to public challenge that.
The conclusion of the crowd is based on the cascade. And I dare not conclude that the probability
of harm from that chemical is not something I should be concerned about. After all, everyone knows
it's harmful simply because an ever growing number of people seem to know it.
I've been misleading you. I've tricked you. That really wasn't a hypothetical. In 1989,
the chemical I'm talking about was sold under the common name of Allar. And a public interest
group released a study saying that they had concluded that this was a highly dangerous chemical.
And you know how the media are. Once this was out once, media reported on it over and over again.
60 minutes made it the lead story and put a skull in crossbones behind the correspondent
to emphasize the danger. Hearing all of that, there were harsh congressional hearings on the
floor of the Congress and the Senate. Movie stars wept tears saying save our children.
And Allar was removed from the market. Polls in America showed that American people thought it
was one of the major risks to their children's health. Years later, the American Medical Association,
having looked carefully at the actual data said, quote, when used in an approved regulated fashion,
as it was, Allar does not pose any risk to the public's health.
But the hard evidence came long after that information cascaded fed on itself. A fact became
true simply because so many people came to believe it. Rationally ignorant people,
reinforcing each other's subjective and inaccurate estimates of risk, collectively led to a result
that was really foolishness and it came from the crowd.
Cascades like that feed financial panics and bubbles. I think one of the most famous was
what's known as the Dutch tulip mania in the 1630s. There was a wild cascade at that time.
Tulips had just been developed and the value of a single bulb rose higher and higher seemingly
without limit. They're lovely flowers, but there's no logical reason why a tulip bulb should cost
more than a house. It seems irrational. By the end of the tulip bulb bubble, the price of a single
bulb cost several times an average working man's annual salary. But everyone knew what a great
investment they were and they were a great investment. The price just kept rising until
it began to fall. And what cascades up can cascade down. Others sell and they again know
something I don't know, I better sell too. So the panic was fed with the same kind of information
that inflated the bubble. In the nearly four centuries since, we've seen all kinds of bubbles
and bursts. There was the 1987 stock market crash. There was the irrational exuberance of the dot
com tech bubble in the 1990s. And of course the housing price inflation that preceded the financial
crash of 2008. There have been bubbles like these all over the world and they result from three
things. Rational ignorance, unavoidable risk, and our own susceptibility to information cascades.
And those are three of the things we've been talking about in this course.
So what should you take away from all of this? Well, many of the decisions we make in life
really involve inescapable probabilistic risks. The rational approach to those decisions
really means we have to take the time to estimate carefully both the probabilities
and the payouts from various outcomes. And with those, we can think in terms of expected value,
a sort of a benchmark to help us decide. At what point do you forego comprehensive
insurance on that aging clunker? What's a reasonable price to pay for an extended warranty on the new
furnace? Should you buy the trip insurance when you book the cruise? Which investment portfolio
is best one to fund your future retirement? Is exploratory surgery a wise choice? And of course
most important of all, should you get rabies shots if you awake with a bat in your bedroom?
But it's also important to remember that in most real-world cases, the expected values we actually
use are going to be dependent upon subjective probabilities. And those can be systematically
distorted by experts whose interest diverged from our own, as in the case of the hedge funds,
or by social cascades, when we're pushed to irrational exuberance or even irrational gloom.
We have one more step to go in our examination of ignorance, risk and information.
Most of the information we need or want starts out in the possession of other people.
And if we think like economists, we know they're going to respond to their incentives.
They're going to use that information strategically. And in a very real sense then,
that's what makes knowledge into power. And that's the topic of our next lecture.
