We've been talking a lot about processing information, using electronics and nanoelectronics,
and about conveying information with the aid of lasers, lasers based on nanomaterials like
quantum wells.
Well, how do we gather this information?
Where does this information about our world come from?
Light provides an incredibly rich source of information about our physical world.
Obviously, our eyes are our most rich, most information expressive sense.
To convey that, to convert that into the language of the digital, to feed it into our image
processors inside our computers, to feed it into our fiber optic pipes.
We need something analogous to our eyes, and that is the world of image sensors.
Whereas we've been speaking in the previous lecture about generating light, converting
electronic information and power into the photonic domain, we're interested now in the
reverse process.
We're interested in how we transduce information about the world around us into the electronic
domain for subsequent processing and communication.
It's pretty incredible what we can do already with our megapixel digital cameras, the ones
in our phones, the ones in our larger, more formal photographic cameras, our professional
cameras.
But in fact, I'll make the case in this lecture that that just scratches the surface of the
information that we can garner about our physical world using light.
In fact, in wavelengths where you and I can't see with our own eyes, there is additional
information to be harvested.
You can think of any material, any chemical compound as having a color or having kind
of a fingerprint in the spectral domain.
We are able to tell what's coming out of a smokestack that's a kilometer or two away
by looking at the colors that that absorbs or that it reflects or that it emits when
we stimulate it with light.
We're able to look off in the distance at a safe standoff and potentially identify whether
something emitted into the air is some innocent spore or whether it's anthrax.
We're able to use colors in the ultraviolet to start looking for the early signs of skin
cancer.
But these are all wavelengths that I just mentioned that you and my eyes don't see.
And so here we have the chance to use advances in materials and specifically in nanomaterials
to perform spectroscopies or to image in wavelength ranges that on our own we aren't able to access.
In a sense, we're able to augment our senses using the capacity of imaging and imaging
materials.
But let's start with what's so ubiquitous now.
Let's start with our digital cameras.
It's interesting to just step back for a moment and think about film-based cameras, which
of course were so widespread and so used even though now we've converted over to digital.
And even in film-based cameras there was nanotechnology, even though it wasn't deliberately included.
The way in which these silver-based films worked was that there were 50 to 100 nanometer-sized
particles, each independent of the others, in which a photocatalytic reaction occurred.
So when a photon came in there would be many knock-on effects where one of these silver-based
salts became soluble.
And so the resolution, the spatial resolution of these photographic films was tremendous
and that often comes across in the beautiful images that we were able to generate with
film-based cameras.
Well, with silicon-based cameras we're also very focused on resolution, being able to
heighten the number of pixels of information that we're able to read out.
It's remarkable how we go around now with this camera.
It's so small, it's just something that we take with us everywhere because it's embedded
inside our mobile handset.
And in a lot of its advantages, its convenience relates to its small size, also relates to
the fact that it's got a direct interface to information processing, the processor,
the integrated circuit inside our cell phone, many integrated circuits in our cell phone
in fact, and it's also directly linked to the internet.
It consists of a lens, the lens has to be a certain distance from the image sensor integrated
circuit, that the image is focused using the lens onto that image sensor, and essentially
there's just an array of millions of pixels of photo detectors of light sensors which
are then read out using this integrated circuit.
Now one of the challenges in making these optical modules more and more appropriate
for use in cell phone cameras going forward is the need to make our cell phone slimmer
and slimmer.
That's one of the key driving forces for mobile devices that they should be more compact
and more convenient.
And so there's great interest in being able to make the module slimmer because today it
often limits the thickness of a cell phone.
Well we'll talk in a moment about how using nanomaterials we're able to make image sensors
that allow us to build optical modules that are slimmer, more compatible with the mobile
form factor.
But let's take a look at this picture of one of these image sensors.
This is actually rather a large one by comparison.
In your large professional digital camera they can be an inch on side which is actually
quite large for integrated circuit and leads to higher costs.
These sensors are called CMOS integrated circuits and the CMOS is an acronym for the
kind of baseline process that we use throughout silicon electronics.
And what it really communicates is the fact that the image sensor industry has been able
to take the available processes used to make silicon electronics.
Normally no light involved, no photons involved in what your typical processor does inside
a computer, but it's been able to leverage that capability, those set of foundries invested
in making silicon.
And it's been able to use that existing resource to build our image sensors.
That's been great news for achieving very, very low cost cameras.
But what it also does is it puts the image sensor world onto the Moore's Law track.
So it means that there's this constant pressure to be able to shrink the dimensions of the
components of these integrated circuits and the integrated circuits themselves to become
smaller and smaller, more and more compact.
That is great news for fitting more megapixels into a given area.
But there's some compromises that we'll talk about in a second when you shrink pixels down
further and further.
The bottom line one is that you end up having fewer and fewer photons and pinch on each pixel.
So if you're taking a picture with your cellphone camera and haven't always been satisfied, maybe
especially if it's in low light, that's a consequence of this shrink down in the number
of photons that impinge on each pixel as a result of shrinking the pixel.
Let's zoom in on one of these pixels and take a look at it in a little bit more detail.
The left image is representing the conventional silicon image sensor approach.
At the bottom of a well is the silicon-based light sensor.
Now silicon is not a particularly strong absorber of light.
And you can't blame silicon for this.
Silicon was not selected as a material because of its optical properties.
It's its brilliant electronic properties that has driven the emergence of the electronics
industry all based around silicon.
But silicon does also have optical absorption.
It's just a little bit on the weak side.
So that's one of the first challenges in existing image sensor approaches that use silicon and
only silicon to achieve their functions.
Now the other challenge you can also see by looking at this image, if you look at what's
above the silicon, you'll see various layers of materials.
And those are called the interconnect stack.
Now to make an integrated circuit, including an image sensor, we need those wires that
we were talking about in our lectures on microelectronics and nanoelectronics.
We need to connect together our transistors.
In the case of image sensors, we need to reach in and see our pixels electronically.
We need to read out what our pixels have seen in the electronic domain.
And so we need wires.
But now, because we build those wires on top of silicon and silicon functions as our substrate,
we end up putting wires in the way of the light detector itself.
So this problem, which is known as the fill factor problem, wherein the area of a pixel
is taken up quite a bit by wiring that obscures illumination.
This is a problem that has led to deteriorating images as we shrink the size of our pixels.
Now here's where nanotechnology comes in.
Recent research has shown that we're able to build light detectors that can function,
in a sense, the way silicon does, as light absorbers that transduce photonic information
into electronic currents, we're able to build these using a new material set, not necessarily
even silicon-based, involving quantum dots, what are called colloidal quantum dots.
Now these particles, they're semiconductor particles, they're synthesized in solution,
they're dispersed in a solution, so they're just like a paint.
And you can coat them onto a silicon wafer, and you can coat them on the very top.
And so now, as shown with this film at the top level of this nano-enabled integrated
circuit, we're able to absorb the light closer to where the light is coming from.
The light's no longer responsible for passing through these layers of wire, for passing
through an optical aperture, and then being absorbed in the silicon beneath, instead,
we bring our sensor closer to where the light impinges from.
So this is a first advantage, this use of nanomaterials overcomes the fill factor problem.
The other challenge that it overcomes is that, whereas silicon was not engineered in the
first place for light absorption, we're able to engineer these quantum dots to be extremely
strong light absorbers.
So now we're able to make a very, very thin film.
It's almost like putting photographic film on top of a CMOS image sensor, and we're able
to capture the image up top, and then read it using the silicon electronics below.
And so this now is a return to using silicon for what it was originally intended for, and
is so effective for, which is the electronic function.
And we've built a second story on top of the silicon using nanomaterials, and this second
story now reaches out and senses the light to be read by the silicon beneath.
This is also, I think, a great illustration of where some of the greatest opportunities
in nanotechnology reside.
It's not just in making revolutionary products that are so disruptive that they have nothing
to do with previous generations, but instead in overcoming some of the limitations of existing
micro-based technologies and augmenting them, but not throwing the baby out with the bath
water, and continuing to utilize very attractive properties of silicon and silicon electronics
but augmenting them with the distinctive properties of nanomaterials.
Now there's another particularly useful property of these quantum dots that bears directly
on imaging, and that's depicted in this array of colors that we can see here.
These quantum dots can be synthesized to have different sizes, and when we synthesize
them to have these different sizes, they can have different colors, different ranges of
wavelengths in which they absorb or emit light.
And so in this set of beakers of quantum dots, you're actually seeing a set of semiconductor
particles that are compositionally identical.
The atoms, the elements that are making up these particles are in all cases the same,
but their colors, their absorption and emission, are very much different, spanning the entire
visible spectrum depending on the beaker we're looking at here.
And the mechanism by which that occurs is the quantum size effect that we've already
discussed, wherein if we compress an electron wave down to even smaller than its wavelength,
we really squeeze it.
Well in that case, we push up its energy levels through the quantum size effect, and we turn
it into a blue emitter.
Or in the alternative, if we make a somewhat more accommodating quantum dot, if we make
our particle of a larger size that doesn't compress or condense the electron wave as
much, we no longer push its energy as much up
above the bottom of the box.
We allow it to take on a lower energy, and as a consequence, be an absorber of lower
energy photons.
So this concept is useful in a number of ways.
I was explaining how we're so good with our own eyes at seeing visible light, and to a
large extent, that's what our digital cameras see today as well.
Because silicon's wavelength, silicon's band gap, the regions in which it's most prone
to strongly absorbed light, are substantially confined to the visible wavelength region.
But let's look at some examples of where we're able to do new things and see in ways we
haven't seen before, if we're able to look at wavelengths that go beyond the visible.
This image is illustrating the case of a forest fire.
And on the right, you see what you can see if you just use your own eyes or a traditional
visible image sensor, which is usually a whole lot of smoke.
And it doesn't tell you much about where the fire is actually emanating from.
It doesn't give people trying to extinguish the fire the detailed information they want
to enable them to go straight to the source.
But on the left, you can see what happens when you're able to take an image in the infrared
wavelengths.
As a result of the different interactions of these different wavelengths with the particles
that reside within the smoke, we're able to see through the smoke all the way to the
fire.
And so we're able to get more information about the scene, more information about our
physical universe by going to different spectral regimes in the electromagnetic spectrum.
Here's another example, this one not to do with fire, but to do with fog.
This is taking a picture of the Bay Bridge and then across to Oakland in the San Francisco
Bay Area.
And on the left, you see the usual view on a foggy day where there's not much to see.
But on the right, where instead of using a visible camera, we use an infrared camera,
for similar reasons to the case of the smoke, we're able to see beyond the absorption and
the scattering of the fog.
And we're able to see things that with our own eyes, we're not capable of seeing.
The connection to the quantum dots is the following.
These quantum dot nanoparticles are not limited to only tuning over the visible spectral range
like we showed in the Vickers.
In fact, they can extend all the way to these other wavelengths that have traditionally not
been the purview of low-cost silicon-based cameras.
Instead the images that you saw in the previous slides were taking with a much more costly
technology.
These used the compound semiconductors, things like the gallium arsenide that we talked about
in the context of lasers.
They used these kinds of compound semiconductor technologies that are able to see out to these
wavelengths, but that aren't the basis for very low-cost silicon electronics.
What these quantum dots are now doing is taking this very low-cost, a high-volume consumer
appropriate, consumer technology appropriate building block, which is silicon, and augmenting
it and making it capable of seeing colors that exotic semiconductors have seen in the
past, but that now we can see with just a top layer of nanoparticles coated on top.
Here's another image, making another point about the properties of materials and how
they're distinguishable from one another based on spectral information.
On the left, you see our subject taken with a visible camera, nothing too suspicious,
but on the right, you can see which parts of his facial hair are his own and which parts
are synthetic.
We're able to distinguish between biological materials and synthetic materials using the
spectral reflective properties of these different classes of materials.
Here's another example of something that we can do when we can see beyond the colors
that you and I can see, which is we can create bullet tracer technologies.
We can image a scene, and because the bullet whizzing through the air is so hot, it has
a signature, it's actually a thermal signature, that we can resolve with an infrared camera,
but that's not available to us using a visible only camera.
As a consequence, we're able to image a scene and not only see where the bullet ends up,
but exactly where it came from, so we're able to trace it back to its point of origin.
This is very useful in military applications and also in law enforcement applications.
Now, you may have been wondering a bit about how we see color at all in today's digital
image sensors.
The answer is that at the moment, we put little arrays of filters on top, optical filters.
You can probably picture a color wheel in which we have a red and a green and a blue
filter that can transmit those corresponding colors of light from behind a white light
source.
Those color wheels seek to map on to our own eyes sensitivity, where we have our color
sensing also based on different sensors in our eyes that are differently attuned to the
red and the green and the blue.
To make a digital camera that can take color images, what we do is we take this array of
pixels that we've made on the silicon and then we overlay that with a little matrix
of red and green and blue filters.
Now that works, it's effective, but in doing that, we are throwing away quite a bit of
light.
When I say a filter, I really mean that we're only letting through our sieve a certain photon,
say just the green photons, and we're catching within our sieve the red and the blue ones.
When we do that, we're further reducing the number of photons that end up impinging on
each of the pixels within our image sensors.
This is what we call lossy color.
It's an approach to seeing in color that does so by throwing away information.
When we're light starved, when we're trying to take an image in very low light, that's
a regrettable thing to be doing.
What quantum dots and nanomaterials enable through their wide spectral tuning and the
sharpness of their cutoffs is they enable a new strategy called stacked pixels, wherein
instead of using filters that first throw away light, followed by sensors that sense
all colors, they enable the realization of a stacked pixel in which a first top sensor
senses the shorter wavelengths, the higher energy photons, and it detects them there.
It doesn't just filter them, it detects them, but it also filters them, passing the green,
and then a green light sensing layer detects the green photons, and then passes the red,
and then a further layer beneath absorbs and senses the red.
You can think of this as kind of a perfect pixel, because rather than sacrificing photons,
sacrificing information to the cause of sensing color, it makes the best use of every available
and precious photon.
Scientists have recently enabled a very exciting phenomenon that has been the preview until
now really of chemists and physicists.
They've enabled researchers to see how we can use this new concept called multi-exiton
generation to make better image sensors, more sensitive detectors.
Let me start by explaining what this multiple-exiton generation concept is, and then how we've
managed to use it in order to achieve more sensitive detectors.
Non-generation refers to the fact that normally the way we expect photon detection to occur
is in a one-for-one way.
One photon comes in, and the best case scenario is that we see one electron worth of information.
Now, in today's image sensors, we're often trying to see very, very weakly lit scenes.
In fact, we can be interested in getting down to the range where we're only seeing a few
photons, a few particles of light impinging on every pixel.
In that case, when we only have a few electrons, it starts to become very challenging to read
out the information conveyed by those few electrons.
We need very, very sensitive low-noise circuits to do so.
What if instead we could, for every photon that comes in, what if we could impart multiple
electrons worth of information, and thereby ease the burden on our circuit?
Well, physicists have been studying in these colloidal quantum dots a phenomenon known as
multi-axotone generation wherein every photon comes in and generates not one, but many electrons
inside the material.
In fact, it's a multi-stage process.
Initially, light comes in and does just generate one excited electron, but then there's a little
amplification cascade.
You might even think of it as kind of a billiard ball knocking into a couple of other billiard
balls and generating a number of excited electrons.
Energy is still conserved.
We need a lot of energy in this primary photon that leads to this primary excited electron
that generates this cascade, that generates a couple of lower energy electrons.
But the ultimate outcome is that we have achieved gain.
We have achieved multiplication from one photon into multiple electrons.
Now, until a couple of years ago, researchers had studied this as a fascinating phenomenon,
something that occurred with higher efficiency inside confined structures like quantum dots.
We were able to enhance the probability of interactions, essentially make more billiard
ball collisions occur by putting our electrons into a small box.
But they hadn't actually made a device with it.
And then in 2009, researchers found that they could make a photodetector that took advantage
of this technology.
And as a result, they were able to see multiple electrons worth of signal collected in a macroscopic
current and something that you could measure using an instrument, an electronic instrument.
They were able to see the benefits of this, see the benefits of this amplification.
And prospectively, we should be able to take advantage of it in making even more sensitive
image sensors.
This is particularly prone to being useful in the ultraviolet because this is where we
have an abundance of photons already and abundance of very energetic photons.
And so the ultraviolet is a wavelength range that's prone to exciting multiple excitons
inside these materials.
So x-ray detectors are another example of another spectral regime for photons as well,
but this one into the very, very high energy range.
And where new insights into new materials has led to extremely promising sensors.
One of the key challenges and opportunities for the field of x-rays for medical applications
is that we would like to get the maximum of information, but with the minimum dose of
x-rays.
Obviously we want to get information without imperiling the patient.
In fact, sometimes these days doctors choose not to take x-rays in order to avoid overexposure,
but as a result, they're surrendering a greater amount of information they could learn about
their patient to do a better diagnosis and ultimately a better treatment.
And the advent of digital mammography, digital x-ray technologies has led to even more sensitive
x-ray imaging systems that have allowed us to reduce dosages and therefore either enable
us to take fewer risks with patients in x-ray exposure or to put another way to perform
x-rays a bit more liberally and therefore get more information about the patients.
This arises again as a result of being able to achieve these single photon detectors, very
sensitive detectors for light.
The other great advantage, of course, once we get our information, be it visible or ultra
violet or infrared or x-ray, once we get it off of these image sensors is that we immediately
have the capacity to connect it into a computer, to get it onto the internet or and or to use
the information and to process it, to use machine recognition or image recognition to
work up these images and in the case of x-rays to give the physician more information than
he or she would otherwise have.
Let's talk about another spectral regime, one that's just emerged from having been
a no man's land, as you can tell.
It seems like whenever we enter into a new spectral regime, new information becomes available
to us, a new and a different set of colors becomes reflected and transmitted.
Another one is now emitting and so we're able to achieve contrast, distinction based on
chemical properties, compositional properties.
Well one of these spectral regimes that until now has been very hard to access is known as
the terahertz.
The megahertz range is familiar because when you tune your radio in your car, you'll often
see megahertz frequencies.
The gigahertz is quite familiar because that's the basis for your cell phone's communications.
And then as we go to much, much higher frequencies than that, we get into the optical frequencies,
the infrared and the visible and beyond.
But in between the gigahertz and the optical is the terahertz regime.
And until recently, the reason why we had little information in the terahertz regime
is that we weren't able to make efficient sources of terahertz radiation.
So this is where quantum mechanics has come to the rescue recently.
A new class of lasers and light sources and photo detectors have all been realized based
on quantum phenomena that allow the creation of terahertz photons.
And this is a bit like a waterfall or a cascade.
In fact, it reminds me of when I was in the Black Forest in Germany and I saw an ad for
going to the highest waterfall in Germany.
I thought, wow, this is going to be pretty impressive.
And it was beautiful, but it actually wasn't a single drop of water over 163 meters.
It was over seven sequential cascades.
And so this notion of the cascade, where we go down one level and another level and another
level, each of those levels, in the case of these cascade lasers, the same height, each
of these cascades produces these relatively small energy photons inside the cascade laser.
And the way in which this is achieved is using quantum phenomena, using quantum states, whereby
putting, again, our electrons into a box, we take advantage not just of the first level,
the lowest lying electronic level, but there are, in fact, many levels of electronic energy
states that are available inside a box.
And then we can couple many boxes together and achieve this sequential cascade, achieve
a waterfall of electrons falling down.
And every time an electron falls down one of these levels, we see the emission of a
photon, of a particle of light, that we can use in Terahertz imaging.
Terahertz imaging is now coming up in security applications, because it turns out you can
see through fabric, and so that's transparent or transmissive, but you can see whether somebody
is concealing either something metallic like a weapon, or you can even get information
on the chemical composition of what's behind the veil, and so you can learn about the presence
of things like drugs based on their Terahertz signatures.
Let's talk about one final example of where this could all go.
The field of imaging in a standoff has been taking advantage of the fact that you can
shine a beam of light over miles, kilometers, and you can image from a distance for a while.
And we've been using spectral signatures to see at these distances, to see chemical
composition information at these distances.
But sometimes the kinds of molecules we're looking for are too subtle, too complex.
Their spectral signatures seen from kilometers away can't be discerned.
And so there's an emerging field for biodetection at a standoff, where we're looking for very,
very specific and subtle differences between two organisms, such as whether something is
anthrax, or whether it's just posing as anthrax.
Anthrax have come up with new ways to launch an aerosol into the region of the positive
threat, have binding occur between nanoparticles that themselves contain a distinctive fingerprint,
and the subject of interest, let's say the anthrax bacillus.
And only if the particles are sticking to the subject of interest do they change color.
They take on a different light emitting color that can be detected from these very, very
long distances.
So as a result of that, we're able to do, from a safe distance, from a safe standoff,
we're able to do biosensing or biodetection using light, but we're able to see more than
what you can see if you only sent photons in, instead by sending particles in that change
their behavior, change their optical spectral signatures as a function of what they're stuck
to, what they're bound to, we're able to see much more than the purely spectral fingerprints
would allow us to see.
The examples that we talked about today, they're about really coupling and interfacing the
optical with the electronic.
They're about using nanotechnology as a basis for transduction, now from the optical into
the electronic.
The way they leverage nanotechnology first and foremost is through their capacity to
engineer materials and their properties towards a particular application.
It's our ability through nanoengineering to tailor the properties of materials to our
desires, to particular spectra, to being strongly absorbing, to being readily integratable with
conventional silicon platforms that has led to some of the most exciting work in this field.
And it also illustrates the point that whereas nanotechnology may sound exotic or new, some
of its most powerful and practical applications will come from using nano where we need it
and building upon well-established technological platforms such as the field of micro and nano
electronics where these are the most powerful solution.
