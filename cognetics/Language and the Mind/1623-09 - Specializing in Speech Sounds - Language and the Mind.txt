I want to tell you a little story.
It's kind of embarrassing, but it sets up this lecture perfectly.
Earlier, I mentioned that I'm trying to learn Japanese.
I'm doing it because I've always liked the way Japanese sounds,
and also, it's the native language of my wife, Yukari.
Now, the best way to learn a foreign language is to use it as much as possible.
So, whenever Yukari and I are in Japan, I always try to practice.
My wife is a linguist, and a while back, the two of us were attending the same conference in Kyoto.
At the opening reception, I saw Yukari talking to a few of her Japanese colleagues that I didn't know off in the distance,
so I thought this would be a great opportunity to surprise her with some new vocabulary I had just learned.
After practicing a few times in my head, I approached the group and gave it a shot.
My sentence had a few mistakes, but one was particularly amusing.
I wanted to say,
What I actually said was,
Can you hear the difference?
Well, apparently, neither could I,
because instead of saying,
Hello, I'm Yukari's husband,
I said,
Hello, I'm Yukari's prisoner.
My mistake was that I mispronounced a single phoneme in the Japanese word for husband.
This is an example of a minimal pair in phonetics.
Shujin, husband, has a short u vowel,
but Shujin, prisoner, has a long u vowel.
In Japanese, that's the difference between your spouse being your wife or your jailer.
These sorts of little mistakes and mispronunciations happen all the time
when adults learn foreign languages.
So common.
This is noteworthy because it's much less common for young children learning their own native language.
And when they do make a mistake, they can easily correct it.
This is clearly one case where children are a lot smarter than adults.
In today's lecture, we'll talk about the mechanisms that explain both of these phenomena.
As you'll see, what makes it relatively easy to learn native speech sounds as a child
is the flip side of what makes it so hard to learn them as an adult.
There are many cases, there are many areas where you can say babies are more skilled than adults,
but the ability to differentiate a diverse set of speech sounds is definitely one of them.
Before we explore why this is, let me just say one thing about this section on development.
For the next batch of lectures, I want to spend some extra time explaining the innovative methods
that developmental scientists have used to explore the minds of infants.
Unlike adults, we can't ask babies directly what they know and don't know.
Instead, we must find creative ways to get inside the heads of these little creatures.
This is no easy task, and it's just as challenging as any other scientific puzzle.
There are almost 7,000 languages on the planet,
and all together, they're composed of hundreds and hundreds of different phonemes.
Faced with such a diversity of speech sounds, you might see this as a classic blank slate sort of problem.
It's not unreasonable to think that babies come into the world with very limited auditory abilities,
and only through massive exposure and experience do they start to hear the sounds in their particular language.
Makes sense, right?
Well, that's not how evolution has solved the problem.
Rather than coming in with blank slates, babies are evolutionarily designed to enter the world
with some very impressive phonetic abilities.
We have learned this from a groundbreaking line of work by several researchers on infant speech development.
The pioneering paper was published in 1971 by the speech scientists Peter Imus and Peter Jusik at Brown University.
Imus and Jusik were interested in whether babies could process phonemes categorically like adults do.
For example, Japanese adults know that the short u-phoneme in shu-jin belongs to the category of short vowels,
but the long u-phoneme in shu-jin belongs to the category of long vowels.
This categorical distinction between vowel length doesn't exist in English.
Elongating vowels in English may sometimes add a nuanced connotation to words,
but it doesn't qualitatively and invariably change their meanings as it does in Japanese.
If this is hard to get your head around, let's use a more familiar example from English.
Consider the difference between the minimal pair of bat and pet.
These words are phonetically identical except for the ba and pa sounds.
These are called stop consonants,
and they're produced by a speaker completely stopping the flow of air from the vocal system,
and then abruptly releasing it.
The key difference between ba and pa is what the vocal cords do in relation to the release of airflow.
Ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba, ba.
For ba, the vocal cords vibrate before the release, and for pa, they vibrate after it.
Try it for yourself.
Put your palm in front of your lips and then say the two words.
The first thing you notice is that your lips, tongue, and vocal cords do the same thing for ba and pa.
The second thing is that you can feel a puff of air with PA before you can feel the vibration.
Try it again.
This delay is called voice onset time or VOT, and it lasts only about 1 20th of a second.
Now here's the interesting thing about how we perceive this difference.
We do it categorically, not continuously.
Here's how we know.
You can use a computer to make speech synthesized versions of these two phonemes and then gradually adjust the number of milliseconds between the release of air and the vibration of the vocal cords.
Then you can present this continuum to people to determine when they stop hearing BA and when they start hearing PA.
Let's try it.
Did you notice how the BA sounds switch to the PA sounds suddenly and not gradually?
That shift happened right around zero milliseconds, which means that negative VOTs are perceived as BA and positive ones are perceived as PA.
This suggests that the transition is categorical in nature.
The differences in VOTs doesn't seem to matter much when they occur within a category, but they make a huge difference across categories.
This phenomenon is known as categorical perception.
Now that you know about categorical perception in adults, we can ask what babies do.
But how could we ask young babies if they hear different phonemes categorically or continuously?
Solving this problem was the creative insight of Jusik and Imus.
Their solution was to build on some very basic skills that human babies have built into their DNA.
Taking advantage of the fact that babies are innately drawn to novel stimuli at birth,
Jusik and Imus use preference for novelty as a way to determine whether babies could tell the difference between the phonemes.
The way they measured this novelty preference was through a technique called habituation.
The idea is that because babies prefer novelty, you can present the same thing to them, like a BA sound, over and over until they get bored with it.
And then you can switch the stimulus on some key dimension, like presenting a PA sound, and record if they increase their attention to it.
So how can you measure this increased attention in babies?
Imus and Jusik took advantage of something that babies are born to do, suck.
All babies have an innate reflex to suck more when they're interested and less when they're not.
So the idea was to present babies a phoneme from one category over and over until they got bored.
They sucked less and less, and then measure what happened when the babies heard a new phoneme from a different category.
If their sucking went up, that meant the baby heard a difference.
If it stayed the same, that meant the baby didn't hear a difference.
Pretty clever, huh?
So what did they find?
Remarkably, Jusik and Imus found that even babies as young as one month old could categorically distinguish BA from PA.
It was categorical because when babies heard speech sounds that differed by the same VOT, but within a category versus across it, they did not differentiate the sounds.
In other words, when they heard a PA sound that had VOTs of plus 30 milliseconds and plus 10 milliseconds, they didn't treat them differently.
It was only when the VOTs crossed the boundary, going from plus 10 milliseconds to minus 10 milliseconds, that babies noticed the difference.
This means that just like adults, one month old babies categorically distinguish phonemes.
It's worth adding that more recent work has shown that even newborn babies can do this.
So this truly seems like an innate skill.
But you may object.
Maybe these babies are learning these speech sounds in utero.
After all, we know from acoustic recording in the womb that a fetus can hear its mother's voice.
Not only that, research from Judith Gervain at Universite Paris Descartes has shown that prenatal babies learn the intonational patterns of their native language from their mother's voice.
So it's possible that they could also be learning to categorically perceive native phonemes.
How might you rule out this alternative explanation that prenatal exposure is the sole mechanism?
The answer comes from one of my favorite studies in all of language development.
In 1984, Janet Worker and Richard Tease of the University of British Columbia conducted a study with English speaking six month olds to see if they could perceive phoneme differences in a language that they've never heard.
Hindi.
Hindi has particular phoneme contrasts that are very different from anything in English.
in English.
And for native English speaking adults, these are very hard to hear.
For example, this is a minimal pair that differs mainly in how much breath is released in producing the ja sound.
These two words have very different meanings in Hindi.
The first means bad, and the second means drowned.
Here, try them again.
If you don't speak Hindi, that pair was hard to do, wasn't it?
Now, if it's hard for you, it must be even harder for an infant, right?
Well, actually, the surprising finding is that for babies, this is actually a very easy thing to do.
In their study, Worker and Tease conditioned six month old English speaking infants to turn their heads when they heard a difference between the two phonemes.
Not only could babies do this easily for minimal pairs in English, but they could do it for minimal pairs in Hindi, too.
These babies had never been exposed to a single Hindi contrast in their entire lives, so this ability was not learned from linguistic experience.
Instead, it must be innate.
By the way, this finding has been replicated with dozens of languages, and with much younger babies, too, so this is a very powerful and robust form of innate knowledge.
Okay, so you may be wondering something.
If you could differentiate all these phonemes when you were zero to six months old, and now you can only differentiate ones in your native language, what happened between then and now?
In a follow-up study, Worker and Tease found that it was between the ages of six and twelve months of age that infants lost this magical ability.
So, at one year of age, the babies in the study could differentiate only English phoneme contrasts, but not Hindi ones.
The explanation for this curious finding is that after six months of age, infants begin to commit to the native language that they hear the most.
In other words, they are born with the ability to keep their options open, and then they lock in to the speech sounds that are present in their linguistic environment.
This makes sense from an evolutionary standpoint.
It's useful to have an innate capacity to categorize speech sounds in all languages, because there's no way to know what language a baby will be exposed to.
But once exposed to a language for long enough, it makes sense to focus all attention and energy to processing it.
In this way, human infants get the best of both worlds.
They get a head start on categorizing sounds, and then they are able to use their experience to commit to the ones that will be most useful for them for the rest of their lives.
Before we move on to the neural mechanisms of this process, let me add one more interesting twist.
Humans are not the only ones who are born with the ability to categorically perceive phonemes.
Research by the psychologist Jacques Meller has shown that young cotton-topped tamarind monkeys also possess categorical perception,
which seems to help them recognize different types of alarm calls.
This further supports the argument that phoneme development is built on domain-general innate abilities that are homologous with closely related species.
Now, I want to talk about the brain for a bit.
To get a more complete picture of how all this works, we need to discuss the neural mechanisms that underlie this phoneme specialization process.
First, you should know the concept of topographical organization.
You already know that different lobes of the brain have different functions.
Along the same lines, there are smaller regions within those lobes that have more specific functions.
For example, you learned that at the outer edges of the occipital lobe, the fusiform face area, or fusiform expertise area,
is specialized for processing complex visual objects like faces.
This is in contrast to different parts of the visual system in the occipital lobe that are specialized for other complex objects,
like furniture, plants, and even words.
The same thing is true for auditory processing.
The primary auditory cortex, also called A1, is located in the superior temporal lobe along a stretch of cortex called Heschel's gyrus.
After a sound reaches your ears, A1 is the first part of the neocortex that processes those sounds.
Just like the visual cortex, the cells in A1 that process these sounds are highly organized.
Because sounds are vibrations of air molecules, each one has a signature frequency.
Just like different instruments in an orchestra have different frequencies.
A1 is organized such that certain cells are dedicated to certain frequencies.
The function of this topographical organization is to ensure efficient and accurate processing of sounds.
This makes sense if you think about it.
Most of the cells that process a certain frequency are located in the same area.
So this allows for redundancy in signals.
And the brain loves redundancy.
Now, phonemes are more complex than single nodes from an instrument.
And they are processed in a network extending beyond A1, but the idea is roughly the same.
Scientists believe that there are dedicated and specialized cell networks for individual features of the phonemes in one's native language.
Like ba and pa in English, and oo and oo in Japanese.
Again, the function of having specialized cells for aspects of native speech sounds is to help people process their language quickly and accurately.
So where does this organization come from?
One possibility is that it works like the fusiform expertise area.
It starts off unspecialized, but then with experience becomes dedicated to speech sounds.
But this doesn't fit with studies showing that babies can make phoneme distinctions in all languages immediately at birth.
So what's going on?
The answer comes back to the principle of neural plasticity.
Which, if you recall, is the brain's incredible ability to change itself.
This is the quintessential domain general mechanism because it applies to everything.
The brain starts with some innate structure, but through experience, plasticity allows those structures to become more specialized and efficient.
To understand how this works, I'll need to introduce two complementary aspects of plasticity.
Synaptogenesis and Synaptic Pruning.
Synaptogenesis refers to the process of the brain creating an increasing number of dendrites to make new synaptic connections.
The flip side of this, synaptic pruning, refers to the process of the brain cutting back on these synaptic connections.
Research by the neuroscientist Peter Huttenlocker at the University of Chicago has shown that over the first few months of a baby's life, A1 undergoes a rapid period of neurogenesis.
But after about six months, it undergoes neural pruning.
So what's this have to do with the story I've been telling about phoneme development?
We now believe that babies are born with neural connections that allow them to immediately start paying attention to whatever sounds they may hear in their particular language environment.
Over the first half year of life, babies start to expand the neural connections that are involved in processing those native speech sounds.
But then, starting at six months, the brain starts to prune like the trimming of a tree.
It prunes the connections that are not involved in processing native speech sounds, while strengthening the ones that are.
The end result is that after one year, there is a reorganized auditory system that has roughly the same number of synaptic connections in A1,
but the nature of those connections has been radically transformed.
At first, the connections allowed the baby to process all speech sounds pretty well.
But after one year, the neural reorganization allows the infant to process her native speech sounds extremely well.
Of course, there's a cost to this plasticity.
It means that the cells that used to process phonemes in other languages are gone.
This goes back to the idea of use it or lose it in Hebbian learning.
These connections have been pruned away because they serve no useful purpose for the developing baby.
By the way, things are different for bilingual babies, but we'll get to that in a later lecture.
All this has some pretty significant consequences for how our brains hear speech sounds.
Pat Kuhl is one of the leading experts on neural aspects of phoneme development,
and she describes a fascinating function of all this neural reorganization.
She calls it the perceptual magnet effect.
The idea goes like this.
When the brain commits to certain phonemes in one's native language,
it creates a prototypical category of each of those phonemes.
Just like you have a prototype for what a house or a tree or a car looks like,
your brain has prototypes of what your native language phonemes should sound like.
These prototypes are useful because every speaker of your native language produces slight variations of their phonemes.
This is true for people speaking the same dialect of English.
You take it for granted, but nobody pronounces words in the same way,
and your brain is always assimilating those differences into their prototypes.
For more extreme examples of this, consider what happens when you hear someone speak a different dialect of English.
I grew up in the Midwest, and in the Midwest, I would say, park the car.
But in Boston, you might hear, park the car.
In order to be a successful speaker of English, your brain needs to gloss over these variations
and recognize that they're actually the same words.
Otherwise, you'd get confused and think someone was saying something different.
Of course, if the variations are too great, your magnet can't quite reach the word, and you'll get confused.
This has actually happened to me a few times when someone from the South has asked me for a pen,
but pronounced it as a pin.
As long as the phonetic variations are not too great, your perceptual magnet is doing work all the time.
It constantly bends what it hears to assimilate variations into your prototype.
So in a very real way, your brain ignores differences around your prototypical category.
Most of the time, it unconsciously translates those differences into the sounds that make the most sense to your particular brain.
Let's pause to consider the implications of this. They're pretty big.
Being exposed to your native language has changed your brain to warp what it actually hears in the world.
Not just some of the time, but all of the time.
Whenever you hear a sound, that sound is perceived based on your unique neural architecture.
That means that you do not have objective access to auditory information in the environment.
It means that everything is distorted.
Using our 3D framework, you might ask what possible function this could serve.
Why on earth would we want to distort things in this way?
Well, one possibility is that it helps with our survival.
Donald Hoffman, a cognitive scientist at UC Irvine, argues that evolution has designed humans to massively distort our perception of reality.
He argues that the function of perception, and cognition more generally, is not to accurately process the world around us, but to help us better interact with that world.
This is no accident, no glitch in the system.
Hoffman argues that millions of years of evolution have built brains that distort reality and allow us to live in an illusion that helps us function better.
This is really not as out there as it might sound.
Take another function of the mind.
Consider that human memory is not a perfect recording of past events.
Human memory is highly subjective and malleable.
Often we remember what we want to remember.
By the way, if this sort of thing interests you, you should check out the film Memento, which is all about memory distortions.
This may not be some strange anomaly in the system.
It may be a key design feature of memory.
The same may be true of perception.
Coming back to the perceptual magnet, it makes sense that our brains would gloss over variations of phonemes if it ultimately allows us to understand a wider range of speakers in a faster and more accurate way.
The goal is not to hear every little variation of what a speaker says.
The goal is to accurately identify what a speaker means with a spoken message.
That's the problem that evolution has tried to solve, so it makes sense that we'd adapt our brains to do it better.
But all this distortion comes at a cost.
One consequence of having a strong perceptual magnet for your native language is that it becomes very hard to relearn how to hear sounds in a foreign language.
This is why it's so difficult for English speakers to distinguish those two Hindi words and why I introduce myself as my wife's prisoner, not husband.
Here's another example from an earlier lecture.
Recall that Japanese speakers who learned English late in life have great difficulty distinguishing R&L phonemes, both in language production and comprehension.
So rice versus lice is a classic problem.
There are two reasons why this distinction is so hard for Japanese adults learning English.
The first reason is that the R&L contrast does not exist in Japanese.
And the second is that Japanese has a phoneme called a liquid R that is midway between R and L.
The La sound in Laman is a liquid R.
Can you hear that? Ra and La in English and La in Japanese.
By the way, here's an interesting side note.
The French word for ramen is laman.
So it seems that English went with the R and French went with the L.
That's interesting.
Knowing about the perceptual magnet effect helps explain why it's so hard for Japanese speakers to master English R&L distinctions.
I mean, every time they experience these two different phonemes, the liquid R phoneme distorts what they hear and draws it into the only category that exists in Japanese.
That's the perceptual magnet effect.
They're quite literally hearing English differently than native English speakers.
Okay, one final thing before we wrap up.
It's possible to speak multiple languages with perfectly native accents.
But the key is to learn them both early in life.
For phoneme development, there's an optimal window of time when it's easiest to learn the speech sounds of a second or third or fourth language.
This developmental window was called a critical period by the great Eric Lennberg, a linguist and neurologist prominent in the mid 20th century.
The way to think about this critical period is that plasticity is greatest during this optimal window, but it gradually decreases as you move away from it.
There are critical periods for various aspects of language development, and the one for phoneme mastery is about the first five years of life.
Most scientists now refer to this as a sensitive period because it's clearly possible to learn new phonemes after the window is closed.
Otherwise, I'd be totally out of luck learning Japanese.
Also, there are individual differences in how sensitive people are to this sensitive period.
For example, it's well documented that for people learning a foreign language later in life, trained musicians are much better than non-musicians learning native accents.
In fact, musicians seem to be better at learning many other aspects of a foreign language, too.
This brings us back to where we started.
Earlier, I said that what makes it easy to learn native phonemes as a child is the flip side of what makes it so hard as an adult.
Now you know what I mean.
An innate ability gives us a head start in mastering speech sounds.
And when we are showered with the phonemes in our native language, plasticity allows our brains to reorganize themselves to truly master those sounds.
But this doesn't come for free.
The specialization causes us to lose the ability to accurately hear and use speech sounds outside of our native language.
So the next time you're frustrated struggling to learn a foreign language, try to see the bright side.
Just accept that it's the price you must pay for speaking your native language so well.
