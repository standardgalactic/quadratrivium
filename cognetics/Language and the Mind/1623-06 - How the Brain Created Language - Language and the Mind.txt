In framing the past two lectures, I've talked about language as an organism, bacteria specifically.
In this lecture, I'll add the final piece to make this analogy complete.
We've talked about bacteria as a trigger for disease, but now let's think about it in a more positive way.
Perhaps you've heard of the human microbiome.
The microbiome refers to the world of microorganisms, mostly bacteria, that live on or within human tissues and bodily fluids.
By the latest conservative estimate, these microorganisms comprise half of the total number of cells in our bodies.
Before you get too grossed out by this, you should know that in many cases, this is a good thing, a very good thing.
Without most of these microorganisms, we'd be in big trouble.
For example, bacteria in our gut not only help us digest food and absorb nutrients, but they also strengthen the immune system, foster tissue regeneration, and may even promote good mental health.
In this way, we have a symbiotic relationship with these bacteria.
We need them as much as they need us.
Now, here's the important thing.
Even though we desperately need these bacteria to survive, we're not born with them.
We must be physically exposed to them after we're born in order for them to fully become part of us.
By the time the colonization is complete after a few years, we are transformed.
These bacteria turn us into what biologists call a superorganism and make us larger than ourselves.
Okay, by now, I hope you're already making connections to language on your own.
If language is like bacteria, it doesn't need to be part of us innately.
All we need is a suitable and receptive biological system.
And once we're immersed in language within the first few years of life, it transforms us.
Language makes our minds bigger than our biology.
This is not as out there as it sounds.
As I mentioned earlier, Darwin himself viewed language as an organism.
And other scientists have followed suit.
Most famously, the evolutionary biologist Richard Dawkins treated language as a biological thing in his 1976 book, The Selfish Gene.
In that book, Dawkins introduces the idea of a meme to explain the replicating power of cultural artifacts like language.
A meme is like a cultural gene.
If it's useful to humans, it'll reproduce itself.
And unlike the fleeting memes that live and die on the internet,
the memes that make themselves indispensable to us like language are here to stay.
In the context of our bacteria analogy, you can view language as a meme that attaches itself to the mind of its host
to ensure that it and the genes of the host replicate and survive.
Running with this comparison a bit further,
it's not a stretch to say that language memes and biological genes have co-evolved and transformed one another.
Just as with bacteria, language has given us many advantages.
Not only does that increase the fitness of our genes that host it,
but it in turn allows those genes to propagate further and provide ever-expanding fertile ground for language.
So how did this symbiotic relationship get started in the first place?
Well, that's the million-dollar question.
In attempting to answer it, let me give you some good news and some bad news.
The bad news is that we'll never know the answer for sure,
the way we know that the Earth revolves around the Sun,
atoms compose matter, and neurons communicate with electrical signals.
The language evolution puzzle is much harder.
It's what's called an inverse problem in science,
a mystery that must be solved by observing current phenomena
to make inferences about mechanisms that produced those phenomena.
Now the good news.
There's been great progress in solving other inverse problems in science.
The evolutionary biologist,
Tecumseh Fitch of the University of Vienna,
points out that the mystery of language evolution is no different
from other big inverse problems that science has already solved,
like cosmologists understanding the Big Bang
or geologists explaining plate tectonics.
Using basic tools of empirical science,
these other fields have pieced together
well-established and falsifiable theories in their disciplines,
giving hope that we can do the same in the language sciences.
There are many theories of language evolution,
and I can't cover them all in a single lecture.
So instead, I'll step back and focus on a broad conceptual divide
between two competing ways of thinking,
the nativist camp and the anti-nativist camp.
The nativist camp believes that language is the product of biological evolution
and that human brains at birth are genetically endowed
with specific abilities to learn language.
The anti-nativist camp argues that language is the product of biological
and cultural evolution,
and that human brains at birth are genetically endowed
with general abilities to learn language.
I want to warn you right up front
that this will be one of the more complicated things we discuss in this course.
But if you draw on what you've learned up to this point,
I think you'll get the picture.
First, let's start on some common and neutral ground.
Everyone in this debate agrees that there is a unique and special fit
between language and the human brain.
That's what we've been talking about for the last two lectures,
so hopefully you're with me so far.
The main disagreement is in what initially drove this relationship.
Did the brain adapt to language,
or did language adapt to the brain?
Proponents of the nativist camp will be familiar to you.
Linguists like Noam Chomsky and psychologists like Steven Pinker
believe that the human brain gradually evolved a specialized mental organ
that was specifically designed for language.
The idea is that at some point in our evolutionary past,
there was a genetic mutation that changed our ancestors' brains,
transforming their regular communication into something more language-like.
For example, connecting back to the Charles Hockett's design features,
it allowed our ancestors to creatively construct and combine arbitrary symbols
in rule-governed ways.
The argument is that because this type of communication was so adaptive
and so key to the survival of individuals who used it,
it rapidly spread language genes throughout the population
from one generation to the next.
By this account, there was an evolutionary pressure
to make brains better and better at language.
In other words, biological adaptations increasingly shaped our language organ
the same way biological adaptations shaped other organs
like our heart, eyes, and lungs.
Note that this is a very language-centered view.
If you think about language as an organism or a meme,
it's as if language was a fully formed thing
that came into contact with humans
and our brains gradually adapted to it.
The brains that adapted best thrived
and passed on their language-ready genes
and this led to more and more language-ready brains in the population.
Over time, this would result in human minds
that are specifically programmed for language.
That's the nativist view in a nutshell.
In contrast, the anti-nativist view
approaches the relationship of language and the brain
in a totally different way.
Recall that in lecture four,
I introduced Morton Christensen
and Nick Chater's theory of language evolution.
Their theory turns the language-brain relationship on its head.
Instead of viewing language as shaping the brain,
they see it as the other way around.
The brain has shaped language.
This theory has many parts,
but I'll simplify and focus on the core argument.
Christensen and Chater point out
that a fundamental tenet in evolution
by natural selection
is that organisms need stable environments to adapt to.
Because it can take millions of years
for an organism to radically change itself,
it requires that the new environment
does not also change.
For example, consider that it took eons
for ancestors to translate from aquatic environments
to terrestrial ones,
from sea to land.
If oxygen was not always present in the air
during this transition,
the environment would have been a moving target
and made it impossible for lungs to evolve.
Now, extend this same logic
to when two symbiotic organisms,
like humans and bacteria,
meet for the first time.
Humans have a much slower life cycle than bacteria.
Some bacteria can reproduce as fast as every 12 minutes.
Because of this,
which one do you think is going to adapt
to the other and change first?
Obviously, it's the bacteria.
Okay, final step.
Now, if language is like a mental organism,
or as Dawkins calls it, a meme,
would its reproductive cycle be closer to bacteria or brains?
You can answer that question
just by doing a simple thought experiment.
Imagine traveling back in time
to ancient Greece and meeting Aristotle.
What aspects of Aristotle would be more different from you?
His DNA and brain,
or his language and culture?
Or more realistically,
if you were to travel to a new country
and meet someone that speaks a totally different language,
what would be more foreign to you?
The person's speech,
or the person's biology?
Of course language would be more different.
That's because language and culture more generally
changes much faster than our basic biology.
Given the tenet that natural selection requires
a stable and constant environment,
this means that rather than our brains
initially adapting to language,
as Chomsky and Pinker argue,
it's the other way around.
Language adapted to the brain,
or more accurately,
communication adapted to the brain,
and the brain transformed it
into what we now call language.
This suggests that language
is not part of our genetic endowment and birth,
but rather a cultural invention
of our unique brains.
To illustrate this point,
consider an analogy
that the linguist Daniel Everett uses in his book,
Language, the Cultural Tool.
Everett compares language to arrows.
Here's how it goes.
If I had told you
that all hunter-gatherer societies
use arrows in hunting,
which seems to be a universal truth,
would you conclude
that the brains of those hunter-gatherers
must be genetically endowed
with specific abilities to make arrows?
No.
That'd be a stretch,
and Everett dismisses this possibility
as unnecessarily complex.
Instead, he argues
that it's much more parsimonious
to conclude that they are born
with general problem-solving skills
that they then apply
to learn how to make arrows.
Well, it's the same story with language.
To sum up,
the nativist camp believes
that human brains are built
specifically for language,
and the anti-nativist camp
contends that human brains
are built for more general things,
like problem-solving,
and this allows us to learn language
as part of cultural immersion.
Is there a way
to reconcile these two views?
Let's try.
First, I'll build the case
for the anti-nativist view,
and then I'll finish up
by showing how the nativist view
may also be right.
Let's start with some common ground.
Everyone agrees,
nativists and anti-nativists alike,
that the human brain
is genetically endowed
with some impressive
cognitive and social skills.
We just talked about
problem-solving,
which is a major function
of our highly developed
frontal lobes.
In the last lecture,
we also talked about
a couple of other
frontal lobe skills,
making rules
to explain novel combinations
and taking others' perspectives
to achieve shared goals.
The interesting thing
about these skills
is that they apply
to more than just language.
Here's a short list
of how these general skills
might have helped
our early ancestors
in other domains, too.
Hunting,
building shelters,
making tools,
farming,
navigating,
going to battle,
and choosing mates,
just to name a few.
In psychology,
skills that serve
multiple functions
are called domain general.
They're not specialized
for just one particular domain.
They have multi-purpose functions.
So, one likely scenario
is that humans possess
some powerful domain general skills
in our evolutionary past
that they co-opted
to create language.
This didn't happen overnight,
but it didn't need to, either.
Perhaps some individuals
in a community
started to apply
their general skills
of problem-solving,
pattern recognition,
and perspective-taking
to how they communicated
with others
in their community.
Over time,
from one generation
to the next,
it's not hard to imagine
how these communication skills
could have gradually transformed
and become more and more
like what we now
call language.
This is the genius
of Christensen
and Shader's theory.
There's no need
to posit any special genes
for language
because language
could have arisen
out of old genes
that were simply
repurposed
for language.
Does this sound familiar?
It's the same message
that Elizabeth Bates
had about language.
Language is a new machine
built out of old parts.
This is a nice story so far,
but I'm sidestepping
one huge elephant
in the room.
If language didn't arise
out of a specific
genetic change
to the brain,
what general brain changes
gave birth to language?
Well,
this is another
million-dollar question,
and truth be told,
we're a long way
from answering it.
Scientists don't even agree
on when language
first evolved.
Estimates are anywhere
from 100,000 years ago
to 4 million years ago.
That's a huge range.
Without knowing the timeline,
it's very hard to know
what changed in the brain
to enable language.
There are many theories
out there,
but most of them share
one common denominator,
the prefrontal cortex
in the frontal lobe.
Archaeological records
suggest that over
the past 2 million years,
the human brain
has tripled in size,
and much of that growth
was concentrated
in the frontal lobes.
And don't forget
that it's not just
the size that increased,
it's the connectivity, too.
Recall from our last lecture
that cross-species comparisons
clearly show
that the wiring
of our frontal lobes
is much more elaborate
and widespread
than our primate cousins.
So, putting all this together,
one reasonable hypothesis
is that the frontal lobes
of our pre-language ancestors
evolved to become
better problem solvers,
pattern recognizers,
and perspective takers,
and these general skills
supercharged
our communication system,
giving it the spark
that would eventually
turn it into language.
But how did that happen?
How did domain general skills
of the brain
help our ancestors
turn general communication
into what we now call language?
To put it in Christensen
and Shader's terms,
how did the brain
shape language?
When confronted
with an inverse problem
like this one,
we have to make observations
in the present
to make inferences
about mechanisms
in the past.
Here are four lines
of evidence
that provide clues
into how this may have happened.
First, we can use
written records
to understand
how languages
have changed over time.
This falls under
the vast fields
of historical linguistics
and sociolinguistics,
and I'll only scratch
the surface on these.
But if you want
to learn more,
you can go much deeper
by checking out
John McWhorter's
great courses
on this subject.
One way languages
change over time
is that they become
more learnable.
When a language propagates
over several generations,
many aspects
of that language
that are hard to learn
become less prominent,
and many aspects
that are simple
to learn
become more prominent.
For example,
we've already talked
about the ma sound
being the easiest
for babies to produce.
This may be
one big reason
why the M consonant
and aval
are two of the most
common ones
across all languages.
And there's a sweet spot
for morphological
and syntactic complexity
that fits the average
working memory capacity
of humans.
That may explain
why languages
either pack
structural complexity
into syntax
or morphology.
Too much in both
may be too hard
for learners
to cognitively handle.
Along these lines,
languages change
to exploit
the strengths
of human cognition.
For instance,
the fact that humans
are so good
at reading each other's minds
through perspective taking
allows our pragmatics
to become less blunt
and more refined.
Think back to our
indirect request example.
The reason we can afford
being polite
with utterances like
I'm getting really hot
is that we can count
on others
to understand
our true intentions
like wanting
the heat turned down.
Another strength
of human cognition
is that we're very good
at seeing relations
among things.
And this has a number
of implications
for language.
Phonetically,
it allows our language
to exploit iconicity,
which, if you recall,
was what helped you guess
that the words
kiki versus buba
referred to pointy
versus rounded objects.
And semantically,
consider how our
cognitive system
naturally maps meaning
from one thing
onto another,
making it the perfect
breeding ground
for figurative language
like metaphor,
idiom,
and analogy.
In these ways,
languages don't change
randomly.
They are constrained
by the people
using and learning them.
A second set of clues
comes from modern-day
language creation.
Have you ever visited
New Orleans?
If so,
you may have heard
Louisiana Creole,
a language created
by colonists
back in the early 1700s.
It's a mix of French
and Spanish
and English
and various African languages.
Linguists like
Derek Bickerton
see Creoles like these
as living fossils
that can give us insights
into how language
evolved in the past.
However,
a problem with Creoles
and other blends
is that because
they emerge
out of a mix
of established languages,
it's almost impossible
to disentangle
what is borrowed
and what is actually
created by the users.
What kind of language
would emerge
if it was totally
unaffected
by existing languages?
Thankfully,
nobody has intentionally
done such an unethical,
forbidden experiment,
but there are natural
experiments
that provide hints.
One such natural experiment
has taken place
on a kibbutz in Israel.
The Al-Sayed Bedouin
community comprises
about 3,500 people
living in an isolated village
in the Negev desert.
Largely because
of their isolation,
they've had an unusually
high rate
of congenital deafness,
about 4%,
since the mid-1930s.
This makes for
a fascinating question.
How do these
deaf individuals
communicate?
That question
was answered
by a team
of international researchers,
Mark Aronoff
and Carol Patton
in the U.S.,
and Irit Mir
and Wendy Sandler
in Israel.
The answer
is that they have
invented
their own language
called the
Al-Sayed Bedouin
Sign Language.
Because it's such
a tight-knit community,
the language
is spoken by deaf
and hearing
individuals alike.
We'll talk more
about sign languages
later,
but here are
two key takeaways
for now.
One,
the language
is indeed
a fully functioning
language.
All five linguistic
components work together
as a system
as they do
in any language,
and it shares
all of Hockett's
basic design features.
Two,
it has evolved
over time.
Video data
shows that
the first generation
of signers
had constructed
a very limited
vocabulary
and created
a simple
prosodic structure
that served
as a basic syntax.
The second
and third generations
of signers
increasingly
enriched
the vocabulary
and created
a much more
elaborate syntax.
Interestingly,
the morphological
complexity
of the language
is still pretty
immature,
suggesting that
it may take longer
for some parts
of language
to change
than others.
Connecting back
to the debate
between the nativists
and anti-nativists,
this suggests
that something
must be innate
for the language
to be invented
in the first place.
But the fact
that the language
dynamically changes
so much
over generations
suggests that
a specific structure
of language
is not built in.
Instead,
some general skills
may be innate
and then the learners
may use those
to mold the language
as they acquire it.
In this way,
the brain
is shaping language.
A third line
of work
comes from
an interdisciplinary field
called computational linguistics.
The field brings together
computer scientists,
philosophers,
psycholinguists,
anthropologists,
and neuroscientists
who use computers
to simulate
language development,
evolution,
and change.
The beauty
of this approach
is that the programmer
has total control
over how much structure
is built into
the virtual learners
and the learning process.
This allows
for direct tests
of what is necessary
for languages
to evolve.
One of the leaders
of the field,
Simon Kirby
of the University
of Edinburgh,
did a series of studies
in which he created
a computer program
that taught
virtual learners
novel symbols
and meanings
over many trials
through a process
of iterated learning.
This paradigm
was meant
to simulate
real learners
who come into contact
with new words
and meanings
from teachers
for the first time.
And then the program
has those learners
becoming teachers
themselves
for a new batch
of learners.
And the cycle repeats.
You can think of this
as a giant game
of telephone
where you can compare
the message
that started the chain
to the one at the end.
Some key points.
One,
there was no structure
built into the initial input.
The original pairings
of symbol and meaning
were totally random.
Two,
no special language skills
were built into
the virtual learners.
All they possessed
were general algorithms
for pattern recognition.
And three,
not all meanings
were taught
to the virtual learners,
so they needed
to generalize
their knowledge
of what they learned
to new items
that they had never
been exposed to.
Together,
this means that
any structure
found in the language
must have been created
by the virtual learners.
After a thousand simulations
of this iterative learning,
a remarkable thing happened.
From the chaos
of original random
symbol-meaning pairings,
there emerged
a systematic organization
that looked a lot
like language.
By the end
of the simulation,
the virtual learners
had produced an output
that revealed
consistent word meanings,
syntactic orderings,
and even morphological structure.
From these results,
Kirby concluded
that virtual learners
imposed organization
on what they were learning
at every iterative step
to make it
increasingly easier
to learn
for the next generation.
Although a far cry
from real language evolution,
this is a useful
existence proof.
If you liken
the learning algorithms
programmed
into the virtual learners
to the domain general
pattern recognition abilities
possessed by humans,
it shows that linguistic
complexity can arise
from very humble beginnings.
It also suggests
that languages
are dynamic
and change
even if learners don't.
The fourth line
of research
involves using
this iterative learning
paradigm
with real-life people.
Kirby and colleagues
have done a number
of these sorts of studies
in the laboratory,
and not only
have they replicated
these computer simulations
with living,
breathing humans,
the team has shown
that these changes
can happen
over far fewer trials.
In a study
with Erica Cartmill,
an expert on hand gestures
at UCLA,
human subjects
were asked
to learn and teach
the meanings
of hand gestures
that were paired
with dynamic scenes
on a video,
like a ball rolling
or bouncing down
an incline.
Among early generations
of learners,
the gestures
that people produced
were extremely
idiosyncratic,
not only across subjects,
but within the same
subject, too.
For example,
people would use
several different
hand shapes
to represent a ball,
and they would order
the gestures
in different ways,
like sometimes
gesturing the ball first
and other times
gesturing the action first.
But after only
ten iterations
of learning,
the form and order
of gestures
all became
much more uniform,
both within
and across subjects.
most subjects
settled on making
a sphere shape
with their hands
to represent ball,
and most ended up
with a consistent order
of gesture sentences.
First, they gestured ball,
and then they gestured
the path and manner
of its movement.
So it seems that
just like virtual learners,
real learners
impose structure
to make the learning task
easier and more systematic
over time.
Taken together,
these four lines of work
suggest that brains
do not necessarily
need to change
to make a tight fit
with language.
The very act
of learning itself
may exert
a powerful pressure
that turns
unstructured communication
into structured language.
Most experts
now accept
this anti-nativist
and cultural
learning mechanism
for language evolution.
However,
accepting this
does not necessarily
mean rejecting
the nativist
biological adaptation
account entirely.
Both mechanisms
may be right,
just on different
time frames.
Let me try to explain
by connecting back
to our bacteria analogy.
Once the right bacteria
mesh with the biology
of our gut,
they start to adapt,
which makes the bacteria
even better suited
to the environment.
That's the anti-nativist view.
Communication adapts itself
to the brain
to become language.
Now for the next step.
Recall that I also mentioned
that bacteria and humans
have a symbiotic relationship,
meaning that not only
do bacteria adapt to us,
we adapt to them.
And this adaptation
transforms us,
allowing us to do things
that we could not do before,
like better digest
more types of food
or fight different
types of illness.
Over time,
we come to count
on these benefits
because they make us stronger.
In evolutionary terms,
they enhance
our genetic fitness.
This means that
our own biology
gradually evolves
and become more
and more receptive
to these bacteria,
just as the bacteria
have done with us.
This is the essence
of symbiotic co-evolution.
With this in mind,
it's not hard to imagine
the same thing
happened with language.
Even if communication
originally adapted itself
to the brain,
it did eventually evolve
into what we now
call language.
And once it became language,
what's to stop it
from exerting
its own powerful influence,
little by little,
back on the brain?
Remember,
brains do change
over evolution.
They just change
more slowly than language.
And there's good reason
to believe that brains
do adapt to language.
Think back
to the first lecture
when we talked about
language as the most
powerful tool
on the planet.
If our ancestors' brains
got used to a tool
with such incredible power,
why wouldn't their brains
eventually change
to become even more
receptive to language?
It's similar
to Maslow's gavel,
the adage that
when all you've got
is a hammer,
everything looks like a nail.
When the brain
has grown so accustomed
to using language
to get things done,
it becomes a default way
to pursue goals.
And when we so regularly
and effectively
achieve those goals
with language,
this feeds back
on the brain.
The more useful language is,
the more the brain adapts
to become receptive to it.
This is the story
of bacteria,
and it makes sense
with language, too.
Now that we've built
a foundation
on the evolutionary
and historical timeframes,
it's time to look ahead.
The strength
of the 3D framework
is to bring every dimension
onto the table
when trying to understand
something as complex
as language.
What we've covered
in the past three lectures
is a big-picture context
necessary for making sense
of our next section
on language and development.
But before we get there,
I want to help you practice
using the framework
to work through hard puzzles,
like the inverse problem
of language evolution.
In the next lecture,
I'll guide you through
a compelling mystery
that we've not yet discussed.
What did our ancestors'
first attempts
at language look like?
There are many
fascinating theories out there,
everything from structured
vocalizations
to social grooming
and tool-making
to song and dance imitation.
There may be elements
of truth to all of these,
but I'll explore
a different theory with you,
not only because
I think it's right,
but also because
it lends itself perfectly
to our 3D approach.
Tune in next time
to see what it is.
