One of the biggest themes in this course is to view language as a system.
In the last few lectures, we've approached that system from the perspective of individual
brains that produce and comprehend language.
In this lecture, I want to expand that system to extend beyond the head.
I want us to look at language from the vantage point of brains, highly social brains interacting
with one another.
To appreciate the social nature of language, let's take a moment to listen to the birds.
As you know, birds are one of the only other species born not knowing how to communicate
with their kind.
Like us, they must learn it.
Now here's the interesting thing.
There seems to be something special about birds learning directly from other birds.
Consider these laboratory findings.
If a young zebra finch does not have direct visual contact with a tutor, it won't learn
to sing properly.
Not only that, the preference for the zebra finch song can be overridden if a Bengalese
finch feeds it and sings to it.
And this, white-crowned sparrows will reject an audio-recorded version of another species'
song, but they will readily adopt it if it's sung by a live tutor.
Even the critical learning period is affected by social interaction.
A barn owl's window for song learning ends at 50 days, but this period can be extended
the more the owl directly interacts with singing adults.
Humans aren't much different.
Perhaps you've heard of the sad case of Jeannie, who was physically, emotionally, and socially
neglected by her parents for the first 12 years of life.
Essentially, she grew up in isolation.
When she was rescued by social workers, she had no language abilities.
Although she did eventually learn to speak, it was significantly impoverished.
And recall that the social limitations of children with autism spectrum disorder make it difficult
for them to learn language in a typical pattern.
In contrast, you also know about the deaf children of hearing parents who create their
own sign language.
Why do they do this?
It's because their need for social interaction is so strong that it overcomes their lack of
conventional language input.
So let's talk more about this highly social brain.
How do human brains connect to other human brains?
What role does social information play in the brain's processing of language?
And will our drive to be social ever be satisfied by something other than real life humans?
Let me set up this lecture with a little historical context.
Traditionally, many psychologists and neuroscientists have studied social interactions by focusing mainly
on the individual as the appropriate unit of analysis.
So the question has been, what's inside an individual's head that allows for social communication
with others?
But towards the latter part of the 20th century, scientists interested in highly complex social
activities like language began expanding this narrow focus.
Instead of zooming in on the individual, many now focus on the social activity per se.
The appeal of this approach was nicely captured nearly 100 years ago by the eminent American
psychologist Robert Woodworth.
Way ahead of his time, in 1924, Woodworth gave us this to ponder.
Two boys between them lift and carry a log which neither can move alone.
You cannot speak of either boy as carrying half the log, nor can you speak of either boy as
half carrying the log.
The two boys coordinating their efforts upon the log perform a joint action and achieve
a result which is not divisible between the component members of this elementary group.
This is a simple example, but there are countless more complex ones.
A team of surgeons operating, tango dancers dancing, a quarterback throwing to a receiver,
jazz musicians improvising, carpenters building a house, lawyers building a case.
As with the log example, it doesn't make sense to say that each dancer is doing half a tango,
each surgeon is only partially operating.
The real action is one level up.
Does this sound familiar to you?
This is the essence of emergentism.
The idea is that you can't reduce truly joint activity to its constituent parts because the
parts combine to create something that goes beyond their sum.
It's just like how heart cells aggregate to pump blood.
And ants solve problems together that they couldn't solve alone.
So let's turn to the neural mechanisms that allow these joint activities to emerge.
Do you recall simulation theory?
That's the idea that the brain processes something by reactivating the parts involved and directly
experiencing that?
For example, think about holding an apple and then taking a bite.
As you do this, you're activating the same brain network that is actually involved in
you seeing, holding, and tasting a real apple.
Now do the same thing, but this time think about a friend taking a bite of an apple.
So what'd your brain do there?
You guessed it.
The same basic network is active.
So there's not much neural difference between when you bite an apple, imagine it, or imagine
someone else doing it.
This sort of shared simulation has been shown in many fMRI studies done by Mark Jennerod and
John Desede.
And it suggests a possible neural mechanism for how people subjectively understand the perspectives
and actions of others.
At this point, let me pause and address something that might have occurred to you.
I bet some of you are wondering how your brain can tell apart simulation and reality.
Despite the uncanny neural similarity between simulating actions and actually experiencing
them, Desede has identified an additional neural network spanning the parietal and frontal
lobes that monitors the difference between the two experiences.
This is a very important network.
When it's not working properly, due to pharmacology or psychosis, it can become nearly impossible
to distinguish your own thoughts from what's actually happening in the world.
Okay, so let's return to understanding the perspectives and actions of others.
What we've got is a simulation theory that can explain more than just what happens in individual
heads.
The theory also includes how two heads get linked as one.
Probably the most well-known mechanism for making these neural links comes in the form
of mirror neurons.
Originally discovered in the prefrontal cortex of monkeys by Giacomo Risolati, the mirror neuron
system in humans is activated both when you produce an action like reaching for an object,
and also observe someone else produce that same action.
Over the years, our understanding of the function of this mirror neuron system in humans has
greatly expanded.
Not only do we know it's responsible for understanding the intentions of actions or feeling the pain
of others, it's now also being implicated in autism spectrum disorder, psychopathy, altruism,
and even addiction.
Some researchers, such as V.S. Ramachandran predict that the discovery of mirror neurons will
earn Risolati the Nobel Prize.
So needless to say, there's a lot of excitement around them.
There's also a lot of confusion.
One of the most misunderstood aspects is where they come from.
Are they innate or a product of learning?
Here's where the 3D framework can be useful.
Let's take the motor mirror system as an example.
When very young infants view the reaching behaviors of others, the mirror system won't be triggered.
That's because infants must first have enough experience over development to master reaching
for themselves.
Only when reaching becomes part of their own motor repertoire can they activate mirror
properties of the neurons.
And only a fraction of the motor neurons have mirror properties.
So it seems that just a subset of them have innate mirroring potential.
In this way, the answer of where mirror neurons come from requires us to consider mechanisms
on multiple levels and different timeframes.
The system is the product of genes conserved over evolution and plasticity and experience
over development.
To illustrate the role of experience, let me share with you one of my favorite experiments
on the human mirror system.
It's a 2004 study by Beatrice Calvo-Marino, Patrick Haggard, and their team in London.
Their subjects were professional dancers.
It went like this, ballet and capoeira dancers were put in an fMRI scanner and shown videos of
either ballet or capoeira sequences.
The main finding was that the mirror system, spanning the premotor cortex to the parietal
and temporal lobes, the mirror system was more active when ballet dancers viewed ballet
and capoeira dancers viewed capoeira.
The researchers interpreted these findings as showing that people use their own motor experience
and expertise to understand others' actions.
So it seems that it's much easier to simulate something if you've already mastered it yourself.
Since the turn of the millennium, the field of social neuroscience has greatly progressed,
and it has gone well beyond mirror neurons.
Not only that, methodologies have advanced too, allowing for more dynamic questions about
how people get in sync with one another in real social interactions.
For example, rather than just measuring one person's brain in response to a social stimulus,
we can now simultaneously measure the brain activity of multiple people engaged in a joint activity.
One of the first studies to pull this off was done in 2011 by Ullmann Lindenberger and his
team at the Max Planck Institute for Human Development in Berlin.
They measured the EEG activity of pairs of guitarists playing short melodies together.
Recall that EEG uses electrodes on the scalp to record different electrical frequencies emitted
by the brain.
At any given time, there are many different frequencies of brain waves, with cognitive activity occurring
mostly within the 0 to 14 hertz range.
The idea was to simultaneously measure the cognitive EEG activity of both guitarists as
they played together.
The more their brain waves became aligned in phase and frequency, the more neurally in sync
they would be.
The main finding was that the two guitarists synchronized their brain waves not only when
they were playing together, but also just before they began to play.
The most synchronized activity, the frequency range, was about 2 to 8 hertz, a range associated
with voluntary control of motor actions.
This means that brains of the guitarists were on the same page, so to speak, even before they
started playing.
Now this doesn't show that neurosynchrony is the cause of playing in sync with one another.
Perhaps things like mutual eye gaze, head nods, and foot tapping to a beat are what synchronized
their brains before playing and while they played.
But at the very least, this innovative study shows that brain synchrony is a valid neural
marker of jointly coordinated social interactions.
So if neurosynchrony is not the cause of social coordination, what is?
We have one answer from an ambitious project that measured brain synchrony in a high school
classroom, a real classroom, over the course of a whole semester.
David Popple and his team at the Max Planck Institute for Empirical Aesthetics in Munich.
The Max Planck has lots of institutes.
Popple and his team had 12 students and their teacher all wear portable EEG headgear over the
span of 11 sessions of a biology class.
There were a number of interesting findings.
But the most relevant concerns the alpha frequency of brain waves, around 8 to 12 hertz.
Alpha reflects how deeply someone is paying attention.
This simultaneous alpha frequency of all 12 students and their teacher was most synchronized
when they reported being more cognitively and socially engaged in class.
This suggests that a common focus of attention may be what unites brains to become synchronized
with one another.
Again, neurosynchrony is not the driver of joint engagement.
But even if it's not the immediate mechanism, it may ultimately serve a downstream function
of facilitating social coordination.
Just like a bell continues to resonate long after it's been struck, neurosynchronization
may have enduring social vibrations.
For example, consider motor mirror neurons.
As I reach for an apple, your motor system will also resonate.
Can you see how this resonation can help you predict my goal?
I mean, think about it.
You don't need to wait for me to actually grasp the apple to know what I want.
Your brain inferred that long before I got there.
When viewed this way, neurosimulation allows people to anticipate end states of actions.
In other words, they help predict the future.
Let's take this idea and apply it to the joint activity of language.
Traditional models of communication are pretty static.
There's a sender, there's a receiver, and they serially transmit information in an orderly
fashion.
But real face-to-face communication is nothing like this.
It's almost always much more dynamic and messy.
As the Scotland-based psychologist Martin Pickering and Simon Garrod put it,
communicators are moving targets, and coordination, coordinating between them, is much more like
a dance than a transmission.
In their model, Pickering and Garrod view this tangled overlap between speaking and listening,
not as a flaw, but as an actual design feature that allows communication to work so well.
Consider all the overlap in real, everyday conversation.
Like, you know, imitating head nods and hand gestures, or mirroring facial expressions, repeating
the other's words and finishing their sentences, coordinating gaze patterns, back-channeling with
all the yeah's, uh-huh's, and okay's.
These things align people's brains.
And this resonates over time to help predict what will happen next in the interaction.
I want to emphasize that last part.
Just as you can predict that I'm reaching for an apple before I get there, you predict
where utterances are going all the time.
These predictions help you know where conversations are heading before they get there.
So you can plan your next moves quickly and strategically.
Now, we don't always make the right prediction, but that's okay, because we can easily adjust
on the fly.
These spontaneous corrections are what linguists call conversational repairs, and we do them
constantly.
If you think of a conversation in this way, as comprising simulations, predictions, and
repairs, it's easy to see how it's a lot like dancing.
This dynamic way of viewing conversation adds an important dimension to the dual-stream model
that we talked about last few lectures.
As you process the meaning of speech along the ventral route, you activate motor representations
along the dorsal route.
And these motor representations can reverberate backwards and help the ventral route predict
what may come next.
In this way, language production and comprehension have a bi-directional relationship in which each
mutually guides the other.
Doing helps doing, and doing helps knowing.
So far, I've been very concrete about language as a type of joint activity.
But social coordination through language entails a lot of abstract information, too.
When we use language, it draws upon a large reservoir of what is mutually known among speakers.
The Stanford linguist, Herb Clark, calls this mutual knowledge common ground.
He argues that without it, successful communication is impossible.
In the most extreme example, communicators must have common ground in what language they speak.
That's obvious.
But there's many nuanced layers to this.
For example, imagine using one of those portable devices where you speak into it in one language
and then it spits out the message in a different language.
Does this now mean that your message will be completely understood?
Of course not.
There are all sorts of ways communication can break down.
Your message could assume cultural knowledge that only you have, or it could be blind to
cultural knowledge that only the other person has.
And, hey, things don't get easier if you even speak the same language.
Breakdowns happen all the time.
I'm sure you've all sat through lectures that are pitched at way too high of a level.
Or the opposite.
A non-expert explains something to you despite you already understanding it way better than
they ever will.
And my least favorite, by far, someone goes on and on and on about a topic that is utterly
uninteresting to everyone but the speaker.
If all this talk about common ground has you thinking about pragmatics and Michael Tomasello's
idea of shared intentionality, then nice connection.
Common ground is all about I know that you know that I know recursion.
Let's take a look at some of the empirical evidence showing how this shared social knowledge
influences real-time language processing.
One of the main paradigms used to test this question is to present identical semantic
information but to manipulate its social context.
To the extent the social context changes the meaning of the message, it shows that social
knowledge is a powerful mechanism for language processing.
Let's start with a phenomenon we've discussed before.
Indirect requests.
Recall that these utterances are subtle ways of asking for something without directly requesting
it.
Like saying, it's getting hot to get someone to turn down the heat.
It turns out that we draw on all sorts of social information when we judge whether someone
is requesting something versus simply making a passing remark.
In one of my favorite social psychology experiments, Thomas Holtgraves from Ball State University
manipulated whether indirect requests were spoken by people who were either higher up in
the social hierarchy, like a company boss, or more like an equal, like a co-worker.
The common ground in this case is that it's much more socially permissible for bosses to
ask people to do things for them.
And that's what Holtgraves found.
Participants processed the utterance as indirect requests, these are sentences they interpreted
as indirect requests, more often and much faster when the speaker was of a higher status.
Not only that, people seem to bypass the literal content of the message more quickly in this
higher status condition too.
This last part is particularly interesting because it suggests that people aren't just using social
positions to interpret speech after the fact.
This social information seems to be built into the original message itself.
To pin down the precise timing of when social context enters a message, researchers have turned to ERPs.
Most of these studies have measured an ERP component called an N-400.
This is a negative going wave peaking around 400 milliseconds after hearing or reading a target word.
The N-400 was first identified by Marta Kudis and Stephen Hilliard at UC San Diego back in 1980.
They found that it indexed the brain's processing of the meaning of words.
Specifically, the larger the N-400, the more the brain had to work to integrate a word into the semantic context.
For example, if I may, let me introduce an N-400 in your brain right now.
Ready?
I take my coffee with cream and monkey.
You weren't expecting monkey, were you?
Well, that semantic surprise shows up as a big N-400 in your brain.
Since their discovery, over a thousand papers have been published on the N-400.
And it's now a highly reliable measure of how semantic contexts affect very early stages of the brain's processing of words.
So the question is, can social contexts set up similar expectations?
That was the question asked by Joes van Berkham of the University of Utrecht in a clever experiment with Dutch speakers.
Subjects listened to sentences spoken in different Dutch accents, while ERPs measured the N-400 to the target words.
For example, take this sentence.
Every evening, I drink some wine before I go to sleep.
Imagine that sentence spoken either in a posh accent or a working class accent.
Accents are powerful social signals, and for better or for worse, people use them all the time to categorize speakers.
In this case, a posh accent in the Netherlands socially signals that the speaker is probably a wine drinker, whereas a working class accent, not so much.
The main finding was a larger N-400 effect when target words were spoken by the socially mismatching accent compared to the matching accent.
This finding surprised many people.
For decades, researchers thought the N-400, really they thought of it as a strictly nuts and bolts language effect.
But this study and others showed that it's also driven by social factors.
Despite there being nothing semantically wrong with the mismatching utterances, the brain tagged them as socially amiss.
And what's most interesting is that this happened so quickly.
The temporal resolution of the N-400 shows that social knowledge affects the earliest stages of how we process the meaning of language.
One final example. In the spirit of the 3D framework, this one takes a broad view of the social level.
Remember, the social level is not just about people, but also about things in the world.
So does shared world knowledge also penetrate down into the brain's semantic processing of language?
In an influential study addressing this question, Peter Hergurt and his team at the Max Planck Institute for Psycholinguistics had Dutch speakers listen to sentences that were perfectly appropriate semantically, but that violated shared world knowledge.
I'll give you an example. And if you've spent any time in the Netherlands, this might work on you. Ready?
Dutch trains are white. Anything?
Well, with no knowledge about the actual color of Dutch trains, the word white probably didn't do much for you.
But for Dutch natives who have all shared cultural knowledge that Dutch trains are in fact yellow,
not only that, this N-400 effect for a culturally incongruent color was no different than hearing semantically incongruent sentences like Dutch trains are sour.
So the brain seems to treat social and semantic violations the same.
Oh yeah, one more cool detail. In addition to ERP, Hergurt measured fMRI responses to these shared knowledge violations,
and he found that it was this left inferior frontal gyrus, that left IFG that we talked about,
that differentiated socially true statements from socially false ones.
This is interesting because the left IFG is most traditionally associated with processing linguistic information.
It's another example of how important social information is to language and the brain.
I want to finish up by going back to the birds. Birds communicate best with actual other birds.
Is it the same for humans, or for us, will any old surrogate do?
One interesting place to start is to consider the highly social behavior of human laughter.
In the 1990s, the University of Maryland's Robert Provine did a series of studies on this subject, and here are some highlights.
Humans are 30 times more likely to laugh in a social context than when alone.
People laugh not necessarily in response to something funny.
Laughter most often springs from just sharing certain experiences with other people.
Lastly, Provine found that laughter is highly contagious.
If you've ever giggled uncontrollably with a friend or family member, often at the most inappropriate time, then you know exactly what I mean.
One function of this shared laughter is to socially bond with other people.
There seems to be something special about sharing a good laugh that brings people together.
But it turns out that any sort of social mirroring can function in a similar way.
Frannie Spengler and her colleagues at the University of Bonn had pairs of people play a pantomime game where they either mimicked or did not mimic each other's actions.
Diads that mirrored one another produced higher levels of a hormone called oxytocin, which is involved in social bonding.
This suggests that there may be something biologically special about synchronizing behaviors with others.
Being in sync may bring out our social best.
Face-to-face communication appears to be the ideal context for this synchrony.
For example, Jing Jiang and her colleagues at the Beijing Normal University measured neural synchrony during naturalistic conversations using an imaging technique called functional near-infrared spectroscopy.
The tool measures neural activity by detecting light patterns on the scalp that are produced by hemodynamic or blood flow changes in the brain.
The study showed that dialogue in which people face each other produced much higher neural synchrony in the left IFG than when people had their backs to one another.
Apparently, being able to see things like eye gaze, head nods, hand gestures, facial expressions, these things give brains in conversation much more to latch onto.
But what about even more indirect interactions?
What about interactions mediated by technology, like through phones or Skype, or even just a recording like this one?
This is technically not an interaction per se, but it's still communication.
How different is this from the real thing?
With regard to mirror neurons, recorded action definitely dampens neural activity compared to live action.
As far back as 1996, Risolati himself observed that videos of hands reaching and grasping did not activate monkeys' premotor cortex.
Only real hands did.
Granted, monkeys don't have much experience with video, but even humans have an inhibited brain response to video.
Using MEG, Rita Hari and her colleagues at University of Helsinki showed that although video recorded and live hand movements both activated the primary motor cortex, live movements boosted activation by almost 20%.
There's even some evidence that these differences between video and real life play out in language learning.
In one landmark 2003 study, Pat Kuhl, you know, from the University of Washington, she and her team explored whether it was possible to reverse the inevitable decline of phoneme perception during the sensitive period.
Remember that?
We're born able to hear all phoneme contrasts, but we lose that ability from 6 to 12 months of age?
In the experiment, Kuhl provided intensive language training of Mandarin to 9-month-old American infants.
Half the children got live instruction and the other half, they got video.
At the end of training, only the infants exposed to live Mandarin reversed the natural decline of phoneme discrimination, at least in regard to Mandarin.
Findings like this have led some theorists to claim that our brains are optimized for face-to-face environments because it's the same evolutionary context in which we evolved.
The argument's kind of like a paleo diet for communication sort of thing, but it makes a lot of sense.
Even the staunchest supporters of technology seem to recognize there's something special about this old-fashioned form of interaction.
It says a lot when the technology giant Google still gathers its top execs to meet in person when they need to talk about something especially important.
So all this points to something unique and privileged about face-to-face social interaction.
But I want to leave you with something to think about.
Just because we've been built to communicate in a particular way over millions and millions of years, does that mean we're stuck with it?
After all, aren't brains also built for change?
In the next lecture, I'll talk about a form of technology that has definitely changed brains.
And in doing so, it has elevated our minds and transformed our civilizations.
Any guesses what it is?
Tune in next time to find out.
