To start us off, I want to ask you to do something.
When you get a chance, do a web search on this song title,
To or Not Let Go.
What you'll see is the Australian singer-songwriter
Megan Washington doing a live performance for her TED Talk in Sydney.
After listening to her, you probably won't be surprised
that she has won multiple awards for her singing.
Her voice is just so captivating.
But what might surprise you, and really, you've got to listen to the song,
what might surprise you is that despite her beautiful voice,
the singer is terrified of public speaking.
And here's something even more unexpected.
Her fear comes from a stuttering problem that she developed as a child.
How is Megan Washington able to sing so well if she stutters when she talks?
In this lecture, we'll explore the mechanisms involved in language production.
As with comprehension, the process of speaking involves a complex network
of specialized brain regions honed by genetics and experience.
You'll soon see that puzzles like the one with Megan Washington
make much more sense when you view language as a dynamic and widely distributed system.
But before we dive in, a brief side note.
For simplicity, I'll focus on spoken language in this lecture,
and I'll leave sign language for another time.
The similarities and differences between speech and sign are fascinating,
and they deserve a lecture all their own.
If we're going to talk about language production,
we need to start with what prompts someone to speak in the first place.
This initial spark is a bit of a mystery because it's internally generated.
It's the thing in your head you want to communicate.
Most of us would call these things thoughts.
Thoughts are such familiar things.
Philosophers have pontificated about them for thousands of years.
In his famous cave parable, Plato distinguishes between superficial thoughts and deeper truths.
And the great Taoist philosopher Lao Tzu urges us to clear our minds of thoughts
to achieve an inner stillness and peace.
However, modern-day cognitive scientists are increasingly skeptical about the conventional notion of thought.
First of all, nobody's ever seen one directly.
It's just a convenient construct that we use to explain things we all subjectively experience.
Another problem is that a thought is typically viewed as a stable and discrete thing.
But from a neural point of view, thoughts are dynamically shifting and multidimensional.
Neuroscientists see thought as a distributed neural network that is momentarily coordinated in time.
And this neural web links cortical and subcortical regions involved in the full range of brain function,
from perception to action, language to emotion, and memory to planning.
A final reason that traditional notions of thought are on shaky ground
is that we don't know when it occurs.
Is a thought when we're consciously aware of it?
Or does it exist in some form before subjective awareness?
Is it just a brain state?
Or is it a conscious reflection on that state?
Truth is, we have no idea.
Still, despite not knowing what thoughts are, or when, or how they occur,
many neuroscientists believe that the brain generates something in advance of our conscious awareness.
Have you ever heard of the readiness potential?
This electrical brain response was first observed in an EEG experiment done in 1983 by Benjamin Leibitt at UC San Francisco.
The experiment measured electrical activity on the scalp as people voluntarily reached for objects.
The main finding was that a distinct brain wave, the readiness potential, preceded awareness of voluntary reaching by several hundred milliseconds.
Isaac Fried and his colleagues at UCLA have since replicated this interesting finding with epilepsy patients with electrodes implanted in their frontal lobes.
Using these electrodes to record motor activity involved in reaching,
a distinct readiness potential appeared well over one second earlier than when patients reported they first had the impulse to reach.
If you recall how much happens in just one second of brain activity, this is a pretty incredible result.
Now, there's a range of interpretations of these findings.
There are some, like Stanislaus DeHaan, who believe that the readiness potential is the product of random fluctuations of brain activity,
and this neural noise simply correlates with the overt actions of the subjects.
But others make much stronger causal claims.
Some have argued that the readiness potential shows that the brain knows what it's going to do well before it does it.
And this has led some philosophers to question the notion of free will.
I think those doubts may be a tad dramatic because they suggest an artificial dualism between the brain and mind.
After all, who's to say that an unconscious neural impulse to act is not also the product of the mind's free will?
In other words, does freedom to act necessarily require being aware of that freedom?
That's a really hard question, and I'm not sure there's even a scientific way to answer it.
But regardless of the philosophical implications, the results clearly show that some sort of unconscious neural activity precedes action.
And if this is the case for relatively simple things like reaching,
it suggests that for more complex actions like using language,
there may be a large and clandestine neural network operating behind the scenes.
Now, we don't need brain imaging to prove that thinking precedes speaking.
We can learn a lot just by listening to what people actually say in everyday life.
You may be familiar with spoonerisms,
which are named after the Reverend William Archibald Spooner,
Don of Oxford College around the turn of the 20th century.
Spooner was famous for his linguistic slip-ups.
Here are a few that have been attributed to him.
Instead of a loving shepherd, Spooner is said to have given us,
the Lord is a shoving leopard.
And instead of, let's raise our glasses to the dear old queen,
we have, let's glaze our wrasses to the queer old dean.
And my favorite, in place of a half-formed wish,
we get a very unappetizing half-warmed fish.
The beauty of these mistakes is that it only takes one of them
to reveal that speaking unfolds according to a premeditated plan.
What else could explain why parts of words that should appear later in a sentence
mistakenly show up earlier?
Spoonerisms are cousin to a different type of pre-planning error
involving swaps of entire words within an utterance.
Imagine someone saying,
Can you get some kids for the snacks?
Or, Once I stop, I definitely can't start.
No doubt, we've all made these sorts of verbal mix-ups from time to time,
and what's interesting about them
is that they almost always involve swaps within the same syntactic category.
Nouns are swapped with other nouns,
and verbs are swapped with other verbs.
But you almost never see a noun swapped with a verb.
Psycholinguists argue that these two types of speech errors
represent two different stages of speech planning.
Exchanging entire words within a syntactic category
provides a glimpse into the initial abstract stage of conceptual planning.
The technical word for these pre-planned abstract concepts
is a lemma, which in ancient Greek means premise.
Once the lemma has been planned, the next stage begins.
The second step involves putting these abstract ideas
into the proper phonological forms.
And it's during this step where spoonerisms happen.
One of the strongest pieces of evidence for this two-stage model
is in the location and timing of the two types of errors.
Because the lemma stage is so global and abstract,
the swapped lexical items can span many words within an utterance,
but not across utterances.
In contrast, because the phonological stage is much more local,
the syllable swaps are almost always between adjacent words.
Hey, I bet these verbal slip-ups have gotten you thinking
about the most infamous of all speech errors,
Freudian slips.
Are phonological and semantic errors evidence that Freud was right
about the unconscious mind revealing our true thoughts
and repressed feelings?
I think the answer is both yes and no.
No, in that most verbal slips can be explained
by much more parsimonious syntactic and phonological mechanisms.
Research shows that the vast majority of errors
are harmless switching of two verbs, two nouns, or two syllables.
Not only that, but these errors can be induced pretty easily
with made-up words in completely neutral laboratory contexts.
This suggests that deep-seated emotions
are probably not always at play.
However, this doesn't mean that unconscious emotions
are never involved.
Recall that neuroscientists believe that thoughts
are multidimensional and contain cognitive
and emotional elements.
Keeping this in mind, speech errors may sometimes reveal
something deeper about the speaker.
In fact, researchers have come up with a creative way
to test this sort of thing in the lab.
In a clever experiment by Michael Motley and Bernard Bars
in 1979, phonological speech errors were experimentally induced
by having subjects say nonsense word pairs
containing swapped syllables.
For example, subjects had to repeat non-words like
ludgegs and shadbach.
And the experimenters recorded the number of speech errors people made.
And here's the Freudian twist.
All of the subjects were heterosexual males.
And in one condition, the experimenter was an attractive woman
dressed in a skirt.
In another condition, subjects had to wear fake electrodes
and were falsely told that they may receive shocks
during the experiment.
The main findings were this.
When the experimenter was an attractive female,
male subjects were more likely to make speech errors
related to pleasure.
For example, saying good legs instead of ludgegs.
But in the electoral condition, subjects were more likely
to make errors related to pain, saying bad shock instead of shadbach.
Now, cognitive psychologists don't need Freud to explain these findings.
After all, these errors could simply be caused
by a generic cognitive mechanism called priming.
The idea of priming is that presenting one piece of information,
like electrodes on the skin,
causes a cascading spread of activation in the brain
that triggers conceptually related information,
like the anticipation of a painful shock.
But still, even if run-of-the-mill priming explains these findings,
they clearly show that linguistic planning stages
involve more than just neutral nuts and bolts of language.
This opens the door to a wide range of information
being part of the production process.
Keep this in mind because we'll come back to it in later lectures.
At this point, let's see if we can get some clarity
on the different stages of language production
by shifting to the neural level of analysis.
As we did with language comprehension,
we'll start with what brain damage can tell us.
Wernicke's aphasia is traditionally viewed
as a problem with language comprehension,
but that's proven to be overly simplistic.
It's true that, on the surface,
a patient with damage to Wernicke's area
often appears quite fluent in speaking.
In fact, if you were to listen to a Wernicke's aphasic
speak in a totally foreign language,
you'd have no idea there was anything wrong.
It's only when you can understand
what the person is actually saying
that you realize something's missing.
One of the tell-tale signs of Wernicke's aphasia
is called a paraphasia.
There are many forms of this.
There are phonemic paraphasias,
which involve missaying parts of words,
like producing ragging instead of wagon,
and there are semantic paraphasias
that involve perfectly articulating words,
but having them show up
in really incorrect places semantically.
So instead of saying wagon,
someone may say bus or ship or even carrot.
A patient with Wernicke's aphasia
can have both types of paraphasia,
but they often have one but not the other.
In these cases, it's hypothesized
that phonemic paraphasias are caused by damage
in the dorsal stream.
Remember that?
The dorsal stream.
And that's involved in phonological processing.
And semantic paraphasias are caused by damage
in the ventral stream involved in meaning.
It's difficult to reliably map these areas
with functional neuroimaging
because all of the plasticity that takes place
after brain damage.
However, there are some more direct techniques
that have more nicely shown
functional specialization at the brain level.
Do you recall the procedure
that Dr. Wilder Penfield used
to map brains of epilepsy patients
before extracting tumors?
Remember, he opened up the skull
and electrically stimulated certain parts
of the cortex to determine localized function.
That technique is still being used decades later,
and it continues to be one of the most direct ways
that neuroscientists can map the specific functions
of the human brain.
In 2010, David Carina and his team at UC Davis
used the same technique to map language functions
over 100 epilepsy patients having tumors removed
from their left hemisphere.
While lying fully conscious on the operating table,
with their brains exposed,
patients were asked to name objects presented in pictures.
While naming the object,
the surgeon electrically stimulated the left hemisphere
to determine a safe place to operate.
There were many interesting findings,
but the most relevant is that, on average,
stimulation to the dorsal language stream
produced more production errors,
such as phonemic paraphrases,
and stimulation to the ventral stream
produced more semantic paraphrases.
This finding fits very well
into the two-stage model
of semantic and phonological speech planning.
Let me just pause for a moment.
If you're having trouble picturing this whole thing,
you're not alone.
I find it absolutely remarkable
that patients on an operating table
are actively performing language tasks
while having brain surgery.
But if you think that's something,
check this out.
In 2018, there was a news story
about a 19-year-old up-and-coming singer
from Washington State named Kira Iaconecti.
The singer had a rare form of epilepsy
that was triggered, ironically,
by listening to and performing music.
MRI images showed that the seizures
were likely caused by a marble-sized tumor
in a right temporal lobe,
and the doctors determined that brain surgery
was required to extract it.
Here's the amazing thing,
and this almost doesn't seem real.
To make sure the surgery did not damage
any parts of Iaconecti's brain
contributing to her musical talents,
the doctors requested that she sing aloud
during the extraction.
If there was any change to the song,
it signaled a danger zone
and the surgeon adjusted course.
This is absolutely fascinating by itself,
but take note,
this will be important for later,
take note that the operation
happened on the right hemisphere.
We'll return to this
when we talk about our other special singer,
Megan Washington.
But for now,
let's focus on the left hemisphere,
which is home to the most well-known mechanism
for speech production,
Broca's area.
Recall that Broca's area
resides in the left IFG
at the end of the ventral and dorsal streams
of language comprehension.
Here's a simple way
to think about Broca's area.
It's where speech sounds
get turned into speech actions.
Sometimes we forget
speaking is a motor activity,
just like walking, texting, and dancing.
In fact,
talking is one of the most complex activities
that humans do.
It's estimated that speech requires
the coordination of over 100 muscles
involving the lungs,
throat, jaw, tongue, and face.
The left IFG is responsible
for planning and executing
most of those motor commands.
Using our 3D framework,
you probably won't be surprised
that the ability to speak
gets wired up
through a combination
of genetics and experience.
Genes dictate
that the posterior part
of the frontal lobe
will ultimately specialize
for planning and producing
speech movements.
And auditory experience
with one's native language
overdevelopment
allows those cells
to eventually mimic
what they hear.
To appreciate the importance
of this early auditory experience,
consider the challenges
faced by someone born
without the ability to hear.
Congenital deafness
makes it so hard to speak
because there's no auditory target
for Broca's area to hit.
Imagine being blindfolded
and trying to strike a baseball.
Without visual input,
your motor system
would be swinging at air.
But you might protest,
surely we don't need
our auditory system
to speak once it's fully formed.
Well, have you ever tried
to talk while listening
to loud music on headphones?
It's kind of hard, isn't it?
And here's a real challenge.
Imagine wearing headphones
and having your own speech
played back to you
with a slight lag.
This delayed auditory feedback
is known to massively disrupt
speaking within moments
of hearing it.
And one of the first demonstrations
of this in 1955,
Grant Fairbanks
of the University of Illinois
showed that just a few
hundred milliseconds of delay
creates a severe artificial stutter
in otherwise fluent speakers.
This finding not only shows
that hearing speech
is necessary for producing
language over development,
but it shows that
even as an adult,
you need to hear your own speech
to produce speech.
In patients with Broca's aphasia,
the ability to link speech actions
to speech sounds is compromised,
and it creates a telltale
kind of disfluency.
Unlike Wernicke's aphasia,
if you heard a Broca's aphasia
speaking in a foreign language,
you could tell within seconds
that something was drastically wrong.
The speech output would be abrupt.
It would be abrupt,
halting, and effortful.
This is often referred to
as telegraphic speech
because the speaker can manage
only a few keywords at a time.
So what's the difference
between speech disfluencies
in Broca's aphasia
and the phonetic paraphasias
involved in Wernicke's aphasia?
Listening to the two types of speech
back-to-back,
it becomes obvious
that Broca's aphasia
involves problems
with the actual mechanics
of saying speech.
If Wernicke's aphasia
is a breakdown
in knowing the right things to say,
Broca's aphasia
is a malfunction
in physically saying it.
This distinction is borne out
in another difference
between Broca's
and Wernicke's aphasia.
Broca's aphasics
are painfully aware
of their condition,
whereas Wernicke's aphasia
seems to be much more oblivious.
When Wernicke's aphasia
use paraphasias,
they act as if
it makes total sense.
But Broca's aphasia
are much more sensitive
to their listeners.
Often, they'll resort
to writing things down
for people
to make themselves understood.
In the advent
of functional neuroimaging methods,
Broca's area
was one of the first
language mechanisms
to receive attention
from cognitive neuroscientists.
In a classic study
done by Stephen Peterson,
Marcus Raichel,
and Michael Posner,
in 1989,
the researchers
were among the first
to systematically study
language using PET scans,
a cousin of the fMRI technique.
PET uses radioactive isotopes
to measure blood flow changes
in the brain's processing
of external stimuli.
Peterson and his team
presented isolated words
to healthy adults
and asked them
to either listen to
or repeat those items.
The idea was that
the brain responses
to the repeat condition
would be subtracted
from the listen condition.
And this difference,
the difference between the two,
would reveal the unique
neural contribution
of producing speech.
There were many other conditions
and findings,
but the main result
for our purposes
was that Broca's area
was distinctly active
when repeating speech
versus just hearing it.
Since the publication
of this pioneering experiment,
there have been countless
functional imaging studies
focusing on Broca's area.
Although scientists now believe
that Broca's area
is responsible
for more than just
language production,
its articulatory function
in producing speech
is firmly established.
Let's switch gears for a bit.
We've learned a lot
about neuromechanisms
for language production
by exploring what happens
when something goes wrong
in the brain.
But what can we learn
from therapeutic attempts
to get the brain
healthy again?
What are some of the
clinical treatments
for aphasia?
And what do they tell us
about language production?
You already know
about the power of plasticity
and language acquisition,
so you won't be surprised
that it also plays
a major role
in language recovery.
Let me share with you
perhaps the most
impressive demonstration
of neuroplasticity
in the human brain.
Have you ever heard
of a complete
hemisphere-ectomy?
This is a radical
and last resort operation
to treat mostly children
who have life-threatening
seizures caused
by abnormalities
on one side of their brain.
The procedure involves
opening up the skull
and surgically removing
the entire affected hemisphere
from the patient's head.
Like I said, radical.
These procedures were pioneered
in the early to mid-1900s,
but they were revamped
and refined in the 1980s
by surgeons
at the Johns Hopkins Hospital.
Oh, and just a bit of trivia.
One of the doctors responsible
was America's most famous
brain surgeon.
The 2016 presidential candidate,
Dr. Ben Carson.
Nowadays, surgeons can isolate
smaller regions
within a hemisphere,
but in some cases,
a full extraction
is still necessary.
When the entire
left hemisphere is removed,
the immediate consequences
are dramatic,
to say the least.
Assuming the patient
had left-lateralized language,
which most people do,
the removal of that hemisphere
completely shuts down speech,
at least temporarily.
There's much individual variation,
but many patients
can start recovering
language functions
within weeks,
and in some cases,
the long-term recovery
is so complete
that if you ever talk
to one of these individuals
as an adult,
you might never know
half their brain was missing.
This sort of recovery
is possible
because the right hemisphere
undergoes extensive plasticity
and ultimately adopts
the functions
of Broca's area.
Now, this is amazing,
but it's not surprising
when you consider
that the right hemisphere
already does a lot of work
during language production.
While Broca's area
is cranking away
at articulating words,
there is a corresponding region
in our right hemisphere
working on a higher level.
This homologous brain region
simultaneously provides
rhythmic structure
that connects spoken words
across an utterance.
Linguists call this
acoustic link prosody,
and you can think of it
as the melody
that guides speaking.
To appreciate the effect
of prosody on language,
consider the four different
connotations
of the following sentence.
I never said that.
All right, here we go.
Number one,
I never said that.
Number two,
I never said that.
Number three,
I never said that.
And number four,
I never said that.
As you can see,
slight prosodic changes
make a big difference.
In this way,
words and prosody
combine to reveal
the pragmatic meaning
of the message.
So when the right hemisphere
takes over the functions
of Broca's area,
it's definitely
not a blank slate.
And because the right
and left hemispheres
normally communicate
constantly over a bundle
of fibers called
the corpus callosum,
each hemisphere
is well acquainted
with what the other
is doing.
This means that
even though the right hemisphere
is designed
for rhythmic aspects
of speaking,
in extreme cases,
it can repurpose
part of itself
to become more like
the left hemisphere.
This sort of plasticity
is most prevalent
in children,
but recall that
the brain maintains
some plasticity
throughout life.
This opens the door
to a number of therapies
for people who suffer
brain damage
as adults.
The standard treatment
for Broca's aphasia
is repetitive picture naming,
which mostly targets
the damaged
left hemisphere.
But speech therapists
have also tried
some more innovative things
involving the right hemisphere.
One novel approach
capitalizes on
a ubiquitous behavior
we've already discussed,
hand gesture.
Broca's aphasics
instinctively produce
hand gestures
as a form of compensation,
which makes sense
because producing gestures
is a well-known way
to help people
find words
in all of us.
By drawing heavily
on the right hemisphere,
the coarse-grained
spatial and temporal
properties of gesture
can help aphasics
find words
and say them
in a more natural prosody.
And from a listener's perspective,
iconic gestures
also serve
a powerful
communicative function.
An iconic gesture
may not be worth
a thousand words,
but it's handy in a pinch.
A second creative therapy
capitalizes on
the right hemisphere's
penchant for melodies.
Back in 1904,
Dr. Charles Mills
published an account
in the Journal
of the American Medical Association
about an aphasic patient
who learned to sing
his words
instead of speak them.
Others have made
similar observations
and they have ultimately
led to an official
musical treatment
called
melodic intonation therapy,
which was developed
in the early 1970s.
Case studies show
that this musical therapy
is highly effective
for treating certain patients
with Broca's aphasia.
Not only that,
but neuroimaging
has shown that this therapy
can create new
neural specialization
for speech production
in the right hemisphere.
This brings us back
to where we started
with the singer
Megan Washington.
Researchers believe
that stuttering
may be caused
by an underactive
Broca's area.
This suggests
that when people stutter,
their left hemisphere
has to work extra hard
to articulate phonemes,
and this leads
to speech errors,
especially in times
of stress.
Well, this is where
the right hemisphere
comes in.
Because it adds
melody to speech,
the contralateral hemisphere
can offer assistance
to the struggling
left hemisphere.
So in a sense,
when you hear
Megan Washington sing,
you're listening to
an impressive duet
between the two sides
of her brain.
The story of language production
is tightly woven
into the story
of language comprehension.
Early in life,
a combination of genetics
and experience
creates an extensive network
for language comprehension,
and this serves
as a neural foundation
for producing language
over a lifetime.
We've mostly focused
on how comprehension
enables production,
but it can work
the other way around, too.
In our next lecture,
you'll learn that
once the two systems
become intertwined,
language production
can in turn
play an important role
in how we comprehend language.
and this is what
we sais the future
that's a spiritual
in the dark
andpruchắng.
We'll be turning
under untramed
in to-
the actual
so that we can
