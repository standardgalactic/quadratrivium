I want to bring you back to February 11th, 1939.
On that day, an Englishman called Henry G. was admitted to the Bristol General Hospital
in a state of unconsciousness after falling from a bus.
Once Henry G. awoke, he gave the impression of having gone completely deaf from hitting his head.
However, after closer investigation, it became clear that Henry G. had a much more peculiar deficit.
He had lost the ability to understand human speech.
Oddly, not only could Henry G. hear other sounds like music and environmental noises,
he could read and write just fine. And even more surprising, he could speak just fine, too.
It's a wonder what his own voice sounded like to him,
but there are detailed medical records on how he perceived the speech of others.
In Henry G.'s own words,
Henry G. had a rare condition called auditory-verbal agnosia, or pure word deafness.
In this lecture, I'll talk about how disorders like this one shed light on the neural mechanisms for language comprehension.
The story I'll tell highlights all three of our neural principles.
Specialization of function, the network property of the brain, and the power of plasticity.
To better understand the journey that speech takes to create linguistic meaning in our heads,
I want to start with a different modality.
Vision.
Vision is one of the most studied processes of the brain,
and neuroscientists believe that some fundamental aspects of vision
are shared by many other systems, including language.
When a photon reaches the eye,
it gets chemically transduced into electrical signals
that are sent through subcortical relay stations to the back of the brain and the occipital lobe.
By now, you know that the primary visual cortex, or V1,
is located in the occipital lobe,
and that's the start of the two journeys I want to describe.
From V1, visual information gets split into two parallel streams,
one going downward and one going upward.
The one going down is called the ventral stream,
and its function is to process the visual details of whatever thing a person is viewing.
So if the object before you is a snake in the grass,
the neurons along the ventral stream increasingly analyze the details of that snake.
The farther down the stream, the more specialized and accurate the processing.
This detailed processing would allow you to determine what type of snake it is.
Is it harmless or is it deadly?
The neuroscientists David Milner and Melvin Goodale named this pathway the what system,
as in, what am I looking at?
At the same time that this is happening,
the dorsal stream is processing a complementary piece of visual information,
the location of the object.
This pathway runs upwards from V1 through brain areas specialized for processing motion
and finishes up in the parietal lobe,
where neurons analyze the relative location of the object to one's own body.
This pathway is often referred to as the how system,
as in, how am I supposed to interact with this thing?
In the case of a deadly snake in the grass,
the answer is probably, get back.
These systems work in parallel to process visual information.
And given what you've learned so far,
you might not be surprised that damage to one part of the system impairs one aspect of vision,
but not the other.
So damage to the front end of the ventral stream disrupts the ability to correctly identify
what an object is,
but spares the ability for the dorsal system to determine where it is.
The opposite is also true.
Damage to parts of the dorsal system make it impossible to detect motion or location,
but spares the ability to identify objects.
Interestingly, there's even an unusual condition called blind sight,
where all of V1 is damaged,
but because there are ancient pathways that bypass V1 and enter the ventral and dorsal streams subcortically,
these patients can't consciously see objects,
but they can successfully guess their identity and location.
Remarkably, when asked how they were able to get it right,
these patients report that it was just a hunch.
Neuroscientists believe that the primary function of having these two parallel visual processing streams
is that vision can specialize its processing while also ensuring that it's very fast.
Evolutionarily, inefficiencies in the visual system would have been strongly selected against.
A visual system that first had to identify an object before processing its location or the other way around
would have been much less adaptive than a system that does both in parallel.
Perhaps the strongest evidence of this adaptiveness of this particular dual stream system
is that it is evolutionarily conserved in many other species.
We're not the only ones on the planet to have evolved this nifty strategy.
This neural strategy of dividing and conquering is so effective
that we now think it is also used for language processing.
Although originally hypothesized by Karl Wernicke himself in the late 1800s,
this dual stream hypothesis for language has gained new life a century later
with the advent of functional neural imaging methods.
Most notably, a version of this model was reinvigorated in 2007
by the cognitive neuroscientists Gregory Hickok and David Popple.
For the remainder of this lecture, I will sketch out the two main parts
of their dual stream model for language comprehension.
But first, a caveat.
We are just beginning to piece together this incredibly intricate network.
Unlike researchers on vision who draw extensively on non-human models,
neuroscientists studying language are stuck with humans
because we're the only ones who have language.
And now, in the golden age of neuroimaging,
we are finally able to tell more complete stories
and more complex stories about language than ever before.
There are still many debates over the details,
but I'll present the basic framework of what we know.
Have you ever wondered why your ears are folded and grooved like they are?
It's not for looks.
The shape serves a very specific auditory function.
These folds and grooves are perfectly designed for capturing sound waves
in the frequency range of 500 to 4,000 hertz.
Hertz are just sound wave cycles per second.
Do you know what type of sounds in our environment
mostly happen within that wavelength?
You guessed it.
Human speech.
When sound waves travel towards us,
they get funneled from the external ear,
called the pinna,
inward to the eardrum.
Once the pinna funnels and amplifies the speech sound waves,
they hit the eardrum.
The vibrations there are mechanically transduced
into action potentials in the cochlea,
which are then sent via subcortical relay stations
to A1 in the temporal lobe.
As I mentioned in an earlier lecture,
A1 is topographically organized for sound input,
just like V1 is for visual input.
That means that there are cells in A1 specialized
for different sound frequencies.
As with V1's retinatopic organization,
the primary function of this tonatopic organization
is to allow efficient and sharp contrasts among the sounds.
This ensures that the sound gets accurately sent
to the next stage of processing.
From A1, all processes move to the surrounding
superior temporal cortex,
which has been called traditionally Wernicke's area.
At one time, researchers thought
that this large area's main function
was to attach meaning to words,
but we now know that it has multiple functions.
One of the earliest stages of processing after A1
is to recognize acoustic features of words.
It was once assumed that this processing occurred
only in the left hemisphere,
but we now know that it is bilateral.
For example, neuroimaging scans show that patients
with lesions to the left superior temporal lobe
with the right homologue intact
can still recognize words as words.
However, bilateral damage to this part of the brain
is devastating for word recognition.
In fact, this is most likely what caused Henry G's pure word deafness.
Recall that he could not hear words as words.
For him, it was all gobbledygook,
and modern-day neuroimaging scans show that the culprit
is likely to be bilateral damage to this brain region.
And consistent with the fact that Henry G
could process other sounds just fine,
researchers have used fMRI on non-brain-damaged participants
to show that this region may be specialized
for the sounds of language.
Many studies have now found
that the bilateral superior temporal lobe
is more active for speech stimuli
than complex non-speech sounds,
suggesting that it is indeed one of the first processing points
that treat speech as a special type of acoustic signal.
Not only that, but this is also the first stage of processing
that distinguishes phonological features
within spoken words themselves.
So this is where the brain distinguishes
between minimal phonetic pairs,
like rake and lake, or shuzhin and shuzhin.
In keeping with the theme of dividing and conquering,
this processing is bilateral.
Hickok and Poppel argue that the left hemisphere
analyzes the fine-grained details of these phonological forms,
whereas the right hemisphere does a more holistic analysis.
This research has been further supported by Meg's study
showing that the timing of these phonological distinctions
occurs roughly around 100 milliseconds
after sounds hit the eardrum.
This means that within one-tenth of a second,
this bilateral superior temporal system
can distinguish among the phonological forms
of every word within one's language.
Processes that occur this quickly in the brain
are usually highly specialized and crucial for survival.
Think back to a time when you struggled
to hear the phonemes in a foreign language.
If it was always that hard to process your native language,
you'd be in a lot of trouble.
From the superior temporal lobe,
the two streams split,
and one heads ventrally and the other heads dorsally.
These two pathways ride along
two well-established white matter tracks.
Recall that white matter pathways
are like the information superhighways of the brain,
and this infrastructure is what allows the brain
to function as a network of specialized mechanisms.
And here's an interesting fact,
courtesy of the psycholinguist Julie Sedevy.
It's estimated that the average 20-year-old
has approximately 100,000 miles
of these white matter tracks.
That's a lot of road in our heads.
Let's start with the ventral speech stream.
The transitional area connecting
the posterior superior temporal lobe
and the middle temporal gyrus
is where words are recognized as having meaning.
Damage to this area produces receptive aphasia,
which was traditionally called Wernicke's aphasia.
Unlike patients with pure word deafness,
these patients can produce and hear speech's speech,
but the speech lacks appropriate meaning.
So these patients may hear the word key,
but they may think it means something totally different
than you or I think it means.
Interestingly, these patients will still be able
to interact with keys in the appropriate way,
so it's not as if they have no concept of a key.
It's just that the link between the word
and concept is broken.
Long before neuroimaging methods corroborated claims
that Wernicke's area was a mechanism
for attaching meaning to words,
Wilder Penfield observed this function directly
in non-ephagic brains.
Remember Penfield?
He was the American-Canadian neurosurgeon
who directly stimulated the brain with electrodes
before he did surgery for severe epilepsy patients.
He was the first to directly show
that electrically disrupting part of Wernicke's area
caused severe language comprehension deficits
in patients on the operating table.
With the advent of spatial neuroimaging methods,
we now have literally hundreds of studies
implicating this region as a mechanism
for attaching meaning to words.
As would be expected when viewing language as a network,
temporal neuroimaging methods have shown
that this semantic processing occurs milliseconds
after phonological processing at earlier stages.
As a general rule,
as information travels further towards
the anterior part of the temporal lobe,
the neural mechanisms become increasingly specialized
for processing more complex semantic information.
Another notable feature of moving
towards the anterior portion of the ventral stream
is that it becomes increasingly more left-lateralized.
So as it gets specialized for complex semantics,
the left hemisphere takes on more and more
of the processing burden.
However, we know from brain lesions
and neuroimaging studies
that the right hemisphere is still involved
in processing meaning,
but it does a much more coarse-grained analysis.
For example, if you were to hear the word key,
the left middle temporal lobe would associate the label
with a relatively narrow meaning of key,
an object that unlocks things,
whereas the right hemisphere homolog
makes much more distant connections,
such as the metaphoric meaning of solving a problem.
So how does the temporal lobe associate
conceptual meaning to words?
It's a matter of much debate,
but one hypothesis is that the anterior temporal lobe
is a neural hub that connects word forms
to various meanings that are scattered
all around the brain.
By now, you should not be surprised
that the semantic system for language
involves activation of a very complex
and widely distributed memory network.
So what is this network composed of?
One of the largest meta-analyses
on the brain's semantic system for language,
Jeffrey Binder and his team
at the Medical College of Wisconsin
reviewed over 500 PET and fMRI studies
and found that there were a few main areas of activation.
One area was located in the cortical area
surrounding the left auditory cortex,
including the left temporal lobe
and parts of the inferior parietal lobe, too.
Connections to these areas
are thought to activate visual imagery.
To appreciate this,
it's good to be reminded of simulation theory,
which we talked about earlier.
Recall that simulation theory explains
that one way we remember things
is to reactivate parts of the brain
that are involved in actually perceiving them.
So one mechanism for remembering
the meaning of the word key
is to reactivate the visual perception
of images of keys.
However, it's important to note
that semantic memory and visual perception
do not completely overlap.
We know this because there are cases
of brain damage to these visual areas
where object recognition is completely lost,
but patients can still correctly understand
the meanings of words associated
with those objects.
So, for example,
a person may lose the ability
to visually identify a key,
but they could easily define the difference,
tell the difference between
a house key and a car key.
This means that concepts linked to language
are not totally the same
as images linked to perception.
Another simulation zone
is located in the motor
and premotor cortex of the frontal lobe.
If you recall from the work
of Friedman Pulvermuller,
when someone hears the word kick,
the foot region of the primary motor cortex
becomes active,
but when they hear the word throw,
the hand region gets involved.
Again, the overlap is not perfect
because there are cases of brain damage
where the motor system is compromised,
but linguistic knowledge
about what objects do
is perfectly fine.
It's preserved.
In addition to neocortex activation,
limbic regions also get in on the act
of attaching meaning to words.
Not surprisingly,
the part of the subcortex
responsible for encoding
and retrieving memories,
the hippocampus,
is constantly active
when searching for word meanings.
And because many words
have episodic and emotional associations,
parts of the cingulate cortex
become active to attach
those associated meanings.
If you think Binder's brain network
is impressive,
consider this semantic atlas
created by Jack Gallant
and his team at Berkeley.
In 2016,
Gallant conducted
a methodologically innovative study
measuring the brain's semantic processing
not of isolated words,
but of whole narratives.
He had seven subjects
come into the lab
and listen to two hours of stories
from the moth radio hour
while he measured fMRI
to activations of each word
in the stories.
And what he found
has greatly expanded
traditional views
of the brain's semantic network.
Gallant's team
found that words activated
practically every square inch
of the brain's surface.
Consistent with Binder,
the activation maps
were highly organized.
Color words activated
brain areas involved
in color processing,
and social words activated
areas involved
in social processing.
And as with Binder's study,
the same words activated
multiple brain regions,
suggesting that word meanings
comprise a multimodal web
of activations.
Lastly,
although the maps
of each of the seven people
differed in the details,
they all showed remarkable
general similarities
of brain locations.
This suggests
that different parts
of the brain
may be optimized
for processing
the meaning of language
in similar ways
across all humans.
Okay,
after the meaning of words
is assembled
from all corners
of the brain,
the information
is channeled
towards the final stop
on the ventral pathway,
the left inferior frontal gyrus,
or IFG.
We've talked about
this brain region earlier,
and you may recall
that it's involved
in the integration
of multiple pieces
of information.
In this case,
the information
that must be integrated
is the meaning of words
and their syntactic context.
This final step
is necessary
for comprehending language
in its full creative complexity.
Many fMRI and PET studies
have shown
that the left IFG
is active
when people process words
as part of a syntactic frame.
MEG and ERP studies
have shown
that the timing
of this integration
is variable.
In the case
of ambiguous
syntactic sentences,
it often occurs
tenths of a second
after the processing
of word meanings.
However,
when the syntax
of a sentence
is very familiar,
like the boys
pet the dog,
the left IFG
actually becomes
activated
in advance
of the word dog
to predict
word meanings
that are associated
with such a reliable
syntactic frame.
This illustrates
an important property
of the ventral stream
of language.
It can flow
in both directions.
To illustrate
how the left IFG
constantly keeps track
of syntactic structures,
Stanislaus Dahan
and his team
in France
have measured
a process
called merge.
The merge mechanism
is theorized
to monitor
syntactic phrase structures
in sentences.
As in,
and this is a real example,
ten sad students
of Bill Gates
should often sleep.
According to the theory,
the merge process
builds up
in the left IFG
with each word
in the first phrase.
And then,
when the first phrase
is complete,
it resets itself
with the start
of the second phrase.
In this way,
merge is a mechanism
for keeping track
of syntactic structures
in a language.
Dahan and his team
measured merge
in epilepsy patients
who had
neurostimulating electrodes
implanted
in their frontal
and temporal lobes
to control
life-threatening seizures.
Because those electrodes
could record,
in addition to stimulate,
it was possible
to get a rare
inside peek
into the neural mechanisms
for syntactic parsing
of a sentence.
Just as predicted,
the left IFG
increased activity
as the phrase structure
built up,
gradually progressed,
and then released activity
at the phrase boundary.
This is some of the most
direct evidence
that the left IFG
is constantly monitoring
and updating
the syntactic context
of language,
which is not only useful
for correctly understanding
complex meanings,
but also for predicting meaning
that is coming down the stream.
As I mentioned earlier,
these syntactic predictions
can work backwards
to help earlier phonological
and semantic stages
anticipate what phonemes
and words to expect.
clearly there's a lot going on
in the ventral stream,
but it's only half the story.
While information moves
up and down this pathway,
a second set of processes
is ongoing
in the dorsal stream.
If you think about words
as objects,
the function of the
dorsal language stream
is to act on those objects.
This is not as much
of a stretch
as it seems.
One way for the visual system
to know an object
is to interact with it.
In a similar way,
one way for the auditory system
to know what a word is
is to produce it.
So for the dorsal stream,
knowing is doing.
The significance of this
knowing as doing mechanism
is clearest when considering
how infants learn
to produce language
in the first place.
When a baby babbles,
it's coordinating
its motor system
to match what its
auditory system is hearing.
Recall that humans
are born with an ability
to hear all phonemes,
but the motor system
is much less advanced.
It requires months
and months of practice
to learn how to produce
the phonemes it hears.
This extensive experience
is what lays the tracks
for the dorsal route
in the first few years of life.
And once it's in place,
we keep using it
after we've mastered
the speech sounds
in our language.
So let's trace out
this pathway.
The first stage
of the dorsal route
goes from Wernicke's area
to the left sylvean
parietal temporal region
or SPT.
Sorry for all the different
SPTs and MRIs.
That's just the way
neuroscience is.
SPT gets its name
by straddling
the sylvean fissure,
which divides
the temporal lobes
and the temporal
and parietal lobes.
As a general rule,
the dorsal stream
is much more
left lateralized
than the ventral stream.
And this is because
motor commands
for producing speech
are located mostly
in the left hemisphere.
Much more on this
in the next lecture.
The SPT
takes the phonological signal
from earlier stages
and rehearses it.
You can think of this
as a neural replay device.
We know this
for a couple of reasons.
One major piece
of evidence
comes from a unique
speech disorder
called conduction aphasia.
Damage to the left
SPT
causes errors
in the production
of phonemes
despite the ability
to distinguish them
during language comprehension.
So these patients
could easily comprehend
the phonetic
and semantic distinction
between
pate
and a latte
but would have
great trouble
accurately distinguishing
the words
in their own speech.
Another function of SPT
is to learn
new vocabulary items.
Spatial neuroimaging
shows that
when adults hear
a new word
in their L1
or L2
in their native language
or second language
the SPT
serves as a mechanism
for sub-vocally
rehearsing that word
to commit it
to long-term memory.
Interestingly
this rehearsing
happens mostly
for complex words.
For words composed
of relatively simple
or highly familiar
phonemes
this stage
seems to be skipped
and is handled
downstream
towards the end point
of the dorsal route.
The main path
for this downstream
connection
is a tract
of white matter
fibers called
the arcuate fasciculus
which in Latin
means curved bundle.
The final stop
of these fibers
is the left
prefrontal cortex
and IFG.
Now we know
that those regions
are heavily involved
in overt speech production.
In fact
damage to the left
IFG
is what produces
Broca's aphasia
which we'll talk about
more in the next lecture.
But Broca's area
and the left IFG
more generally
also serve
a syntactic function.
It's long been observed
that Broca's aphasics
have deficits
in comprehending
complex syntactic sentences.
The syntactic function
of the left IFG
has been corroborated
in many functional
imaging experiments.
And developmental studies
have shown
that children's
syntactic abilities
are positively correlated
with the strength
of connection
between the left IFG
and the more posterior
portions of the ventral stream.
These developmental data
suggest that the IFG
is strongly shaped
by practice
and experience
much like the FFA
with face processing.
In fact
just like the FFA
the left IFG
may be more domain general
than originally thought.
There's growing evidence
that not only
does it process
syntactic patterns
in language
but it's also sensitive
to complex patterns
outside of language
such as music.
For example
in a MEG study
by Burkhard Mace
Angela Federici
and colleagues
the left IFG
of non-musicians
was sensitive
to off-key
harmonic violations
in musical sequences
at around 200 milliseconds
after hearing
the out-of-tune note.
The researchers conclude
that because music
and language
share similarities
in complex
rule-based processing
Broca's area
may be recruited
for both types
of information.
This suggests
that the region
may serve a more
domain general function
than just processing
the syntax of language.
This domain general view
might explain
why the dorsal
and ventral routes
converge in the left IFG.
Peter Hergord
of the Max Planck Institute
of Psycholinguistics
argues that
the left IFG
is a unification site
that combines information
across semantics
and syntax
and as we'll soon see
pragmatics too.
In other words
its function
its function
is to gather
all the relevant
pieces of information
in a communicative utterance
and then piece it together
into a coherent package.
And just like
the ventral stream
the unified meanings
generated by the left IFG
feed backwards
along the dorsal stream.
This helps
early dorsal stages
to quickly connect
motor representations
to the acoustic signal
coming from A1.
In fact
as we saw
with the phonemic
restoration effect
sometimes this
top-down information
is so powerful
that it actually
changes how we
perceive speech sounds.
So Brent
to bring it back
to where we started
if knowing is doing
doing is also
a way of knowing.
As I said earlier
the details of this
dual stream model
are still being debated
but most researchers
now believe
that the general framework
gets it right.
Just as with
the visual system
the efficient distribution
of language
along two parallel pathways
offers real survival
advantages.
Not only does it allow
for faster processing
but the distribution
of specialized labor
allows for better
precision too.
And as an added benefit
if one part of the system
is damaged
the other parts
may compensate
through plasticity.
There's a good reason
why basic structures
and processes
like this system
are evolutionarily
conserved in many species.
Things that work
over evolution
get reused
and this basic system
has been used a lot.
In the next lecture
we'll discuss
how this system
for language comprehension
gets co-opted
for language production too.
play western
and this level
Phase 1
become different
and
when
you
can
stir
пять
and
then
rices
in your
success
and
paar
and
then
turn
on
her
and
«
