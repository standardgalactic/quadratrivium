Let's start again with Quine's conundrum.
Recall, a linguist is presented with the Arunta word gavagai,
just as a rabbit hops by.
Quine asks, what does the linguist think gavagai means?
Consider what makes this thought experiment so effective.
Quine challenges us to determine the meaning of a single word
within a very sparse context.
Think about it.
We know next to nothing about the motivations and intentions
of the speaker and listener,
and we're completely in the dark about what else is going on
in the social and physical environment.
In the real world, these things matter a lot,
and they can totally change how a word is understood.
To illustrate this, let's play with the context a bit
and see what happens.
First, something simple.
What if the speaker pointed to the rabbit as it hopped by?
That would sure help eliminate a bunch of possibilities.
But what if the linguist didn't see where the speaker was pointing?
What if she was looking up at a bird singing in a tree
when she heard gavagai?
From that perspective,
she'd think it had something to do with the bird,
not the rabbit.
Sharing attention on the same object was assumed in Quine's example.
But in the messy real world,
you can't take that for granted,
especially with children.
Or suppose you knew more about
why the two people were walking on the path.
What if the Arunta speaker was thinking about dinner that night,
but the linguist believed that they were just taking a walk?
In that case,
gavagai might mean for the speaker,
let's catch that.
But for the listener,
it could mean,
that's a rabbit.
And for you fans of Monty Python's Holy Grail,
what if,
unbeknownst to the linguist,
the animal that sprang from the vegetation
was a very dangerous creature in Arunta?
In that case,
gavagai could be a dire warning.
But how would the linguist know that?
These puzzles get at the heart
of why the pragmatics of language are so important.
Without understanding the richness of social intentions,
motivations, and context,
there's no hope for successful communication.
In this lecture,
we'll talk about what young children know
and learn about their social world
and how they use that information
to navigate the endless ambiguities of language.
Humans come into the world
naturally drawn to other people.
From the first day of life,
babies prefer their mother's voice
to a stranger's voice
and a stranger's voice
to other sounds in the environment.
Even if these preferences are learned
to some extent in the womb,
they suggest a readiness
to listen to human speech at birth.
Beyond speech,
did you know that babies are born
visually attracted to other humans?
Show newborns a picture of a human face
and a regular old object
and they'll look longer at the face.
Findings like this were originally taken
as evidence that baby brains
are built specifically
for processing human faces.
But we now know that it may be
a more general inclination.
For example,
babies are drawn more broadly
to objects that are symmetrical
from side to side,
but asymmetrical from top to bottom,
just like a face.
Not only that,
they also show a preference
for monkey faces,
not just faces of humans.
There's still debate
about how specific or general
this innate preference is,
but either way,
the upshot is that newborns
come into the world
prime to attend to faces.
Here's another innate predisposition
that draws babies towards humans.
Have you ever heard
of a point light display?
It's created by attaching light sensors
on parts of moving objects
and then making a video
of the sensors.
If the movement is novel,
the resulting video
is really hard to understand,
but if it's familiar,
it's quite easy to recognize.
The biological motion of walking
is one of the most easily recognizable
point light displays
of human action.
When you see it right side up,
it just pops out at you.
Same for newborns.
Francesca Simeon
and colleagues
at the University of Padua
have shown that
two-day-old babies
prefer to look at
upright point light displays
of humans walking
more than inverted ones.
So it's not just random movements
that newborns are attracted to,
it's specifically movements
that are biologically meaningful.
Once a baby locks into its fellow human,
it starts singling out
particular behaviors
that give it clues
about what makes other people tick.
One behavior that seems
to fascinate babies
is what people do with their eyes.
Research has shown
that even as young as two days old,
babies will look longer
at a picture of a face
that's looking straight at them
versus one that's with the eyes
averted to the side.
This gaze locking
seems to be doing something
special for babies,
just as it does for adults.
It's been known for a while
that when two people
make eye contact,
there's a phenomenon
known as pupillary contagion.
Pupils get larger or dilate
when someone is interested
in something
or attracted to something.
And pupil contagion
is when the dilation of pupils
in one person
influences the dilation in another.
The interesting thing
is that mimicking
another person's pupil size
is correlated
with increased feelings
of arousal,
trust, and cooperation.
Not only that,
it seems to activate
brain networks
involved in perspective taking
and specifically a skill
called theory of mind,
which we'll talk about
in a bit.
Christine Fawcett
and her team
at the University of Uppsala
in Sweden
have demonstrated
this same sort of mimicry
in very young infants.
In one study,
four-month-olds
viewed pictures of eyes
with different sized pupils.
And then the pupil sizes
of the infants
were measured
with a device
called an eye tracker,
which is just a special camera
that can determine
where people are looking
and how much
their pupils dilate.
The main finding
was that even
four-month-olds
engage in this
pupillary mimicry,
leading the researchers
to conclude
that this shared
state of arousal
could be an early mechanism
for the development
of social skills
like empathy
and group coordination.
This hypothesis
received support
from some fascinating research
on slightly older infants
done by Victoria Leong
and colleagues
at the University of Cambridge.
The team had an adult
sing a nursery rhyme
to an eight-month-old baby
while measuring
neural synchrony
with an electroencephalogram
or EEG.
While singing
the nursery rhyme,
the adult either
looked directly
at the baby
or averted her gaze
slightly to the side.
The key finding
was that the brainwaves
of the adult
and infant
were much more
in sync with one another
when there was
direct eye contact
versus no eye contact.
Not only that,
this synchrony
was correlated
with how much
a baby communicated
to the adult
through vocalizations.
The greater
the neural synchrony,
the more the baby's vocalized.
We'll get into
the technical details
of EEG
in a later lecture,
but for the time being,
I hope you can appreciate
the significance
of this finding.
It means that
babies and adults
use eye gaze
to link up their brains
and one function of this
may be to create
a joint neural state
that is optimized
for exchanging
social information
during early communication
and learning.
It's as if
making eye contact
can plug one mind
into another.
This high-tech experiment
fits very nicely
with some classic research
done in the 1970s
by the great
developmental psychologist
Jerome Bruner.
Using cutting-edge technology
of his time,
Bruner analyzed
videotapes
of nine-month-olds
spontaneously interacting
with caregivers
in their homes.
He was looking
for social behaviors
that could be signs
that babies were learning
about the goals,
motivations,
and intentions
of adults.
Bruner made many
interesting observations,
but the one
that links up
best with this
language learning
is a behavior
dubbed joint attention.
This joint attention
refers to how
babies seemed
especially interested
in sharing visual
focus on objects
with adults.
Now, this wasn't
just a matter
of a baby
and an adult
simply looking
at the same thing.
That's pretty ordinary.
Bruner's joint attention
was different.
He argued that
not only are babies
and adults
looking at the same thing,
they mutually know
that they're both
looking at the same thing.
This comes back
to knowing about knowing,
which, as we discussed,
seems to be
a uniquely human skill.
Given that joint attention
emerges just before
children start speaking,
it likely serves
as a mechanism
for early word learning.
Here's how it might work.
When an infant
and adult
mutually know
that they are looking
at the same object,
the infant infers
that any word
that the adult says
must refer to the thing
they're both looking at.
It's as if the child
and adult
have some sort
of tacit agreement
that labeling objects
always happens
in the social context
of joint attention.
If so,
this would be
very helpful
for cutting through
some of the
Quinean ambiguities
of what words
refer to.
Given that infants
hear literally
thousands of words
every day,
having a social mechanism
that links
some of those words
to particular objects
in the environment
greatly simplifies
the learning task.
So what's the evidence
that infants use
joint attention
to actually learn words?
In one of the first
systematic attempts
to explore this question,
Dare Baldwin,
a developmental psychologist
at the University of Oregon,
did a series of studies
on children
in the throes
of early word learning.
In one study,
Baldwin was interested
in whether 18-month-olds
would learn a name
for a toy
if there was no
joint attention
with an adult.
The children were
placed in a room
with very few
novel toys in it.
Crucially,
there were toys
that the children
had never seen,
so there's no way
the child could have
known the names
for them.
When they started
playing with the toy
that interested them,
an experimenter
labeled it by saying,
Donu.
There's a Donu.
This labeling occurred
in two different conditions.
In one condition,
the experimenter
was right next
to the child
looking directly
at the toy.
In the other condition,
the experimenter
was next to the child
but sitting on the other side
of an opaque screen
so that the child
couldn't see
where the experimenter
was looking.
In both cases,
the child heard
the exact same speech.
The only difference
was in one case,
the child and the adult
shared joint attention
and in the other case,
they did not.
The main finding
was that when there
was joint attention,
children frequently mapped
the word Donu
onto the object
they were playing with.
But when there was
no joint attention,
children did not reliably
learn the new word.
This learning happens
in much more ambiguous
situations, too.
Picture a table
with two novel toys,
one on either end.
Baldwin had an experimenter
play with an 18-month-old
such that one of the two toys
was the focus
of joint attention.
Just at that point,
the experimenter said,
it's a Modi.
In one condition,
the Modi was said
while looking at the toy
that they were already
playing with.
But in a different condition,
Modi was uttered
while the adult
looked at the second toy
at the other end
of the table.
In both cases,
the child took note
of where the adult
was looking.
And guess what?
When tested later
for what Modi meant,
the child attached
the word to the object
the adult was looking at,
even in the condition
where they were both
playing with one toy,
but the adult was looking
at the other toy
at the other end
of the table.
In other words,
children used
an adult's eye gaze
to flexibly shift
joint attention
to one place
to the next
during ambiguous situations.
These are important findings
because they suggest
that little language learners
are not passively
and egocentrically
linking words to objects.
Instead,
they are actively looking
for good social reasons
to connect a word
to a referent.
And they learn early on
that appealing
to the nonverbal intentions
of a speaker
is an extremely helpful guide.
Eye gaze isn't the only
useful nonverbal tool
in the word learning process.
As we discussed earlier,
hand gestures also
play a huge role.
As you know,
infants and adults
communicate with hand gestures
well before they start
communicating with words.
Pointing gestures
in particular
are especially prominent.
Recall that babies point
not just to get things
they want,
but also to comment on things
and inform people
about them.
For example,
Ulf Laskowski
and his colleagues
have done a series
of experiments
with one-year-olds
showing that they produce
gestures to serve
both of these functions,
to comment and inform.
In one study,
an infant and an experimenter
were in a room
with various objects.
When one of these objects
became salient,
like a light blinking
and making a sound,
the infant almost
always pointed to it
and made eye contact
with the adult
as if to say,
hey, isn't that interesting?
And it's not as if
the infant actually wanted
the blinking object
for themselves.
They seemed satisfied
if the adult
simply looked
at the same direction,
at the same object,
and enthusiastically
showed the same interest.
In another study,
an infant saw
an experimenter
play with a toy,
put it on a table
in front of the child
and then leave the room.
Right after this,
a different experimenter
came into the room
and while the child
was looking,
picked up the toy
from the table
and put it up
on a shelf.
After that second person
left the room,
the original experimenter
came back in
and looked surprised
when the toy
was not on the table.
What do you think
the babies did?
Not only did they point
to the toy
on the top of the shelf,
but they pointed
more vigorously
if the adult
didn't turn up
and look at the shelf.
It's as if babies
understood that
the adult was stuck
and they were just
trying to lend
a helping hand.
I want to emphasize
that much of this
early nonverbal communication
is happening
before infants
begin to speak.
If you think about it,
this is pretty impressive
and implies a highly
sophisticated perspective-taking
skill for a pre-verbal infant.
For example,
pointing to interesting objects
suggests that infants
believe that other people
will also find
the same things interesting.
The flip side
is also likely.
Paying attention
to where adults point
helps an infant
understand what's
interesting to adults,
other people.
All of this early exchange
of information
helps a baby understand
what sorts of things
might be interesting
to others,
and this could be
very useful
for understanding
what words refer to.
And consider
the sophistication
of gesturing
to help others in need.
When a baby points
to an object
that an adult
is looking for,
it suggests that
the baby knows
that the adult
has goals and intentions.
Being aware
that others have goals
and intentions
may be necessary
for any sort
of social perspective-taking.
Perspective-taking
is the key
to savvy,
pragmatic use
of language.
If you can't understand
where someone's coming from,
it's very hard
to know what they mean.
That's why something
as simple as
nice job
can be so confusing.
It can sarcastically
mean terrible job,
it can politely mean
okay job,
and it can even
understatedly mean
great job.
It's all about knowing
what the speaker
intends in context.
As we've seen,
children have some
decent perspective-taking
abilities,
even within the
first year of life.
But as they head
into their second year,
they go to the next level.
Here's a lovely example.
An acute experiment
by Allison Gopnik
and Betty Rapaccioli,
14- and 18-month-old
toddlers,
were presented
with a bowl of broccoli
and a bowl of Pepperidge Farm
goldfish crackers.
And the experimenters
took note of what food
the children seemed
to prefer.
No surprise
that it was almost
always the goldfish.
Now here was the fun part.
The experimenters
then took a bite
from each bowl
and they made either
a disgusted
or a happy face.
Half the time,
they made the happy face
for the same food
preferred by the children,
the goldfish,
but the other half
of the time,
they happily gobbled
down the broccoli.
Finally, the test.
After the experimenter
showed her preference,
she held out her hand
to see what food
the toddler would give her.
The 14-month-olds
almost always handed
over goldfish
no matter what
the adult preferred,
but the 18-month-olds
handed over the one
that the adult seemed
to like,
even if it was broccoli.
This marks the start
of a gradual transition
from having a somewhat
egocentric view
of the world
towards one
that carefully considers
the different perspectives
of others.
There's even a name
for this skill.
It's called
Theory of Mind
or T-O-M for short.
Jill DeVilliers,
a developmental psychologist
at Smith College,
defines T-O-M
as a folk
psychological theory
that we use
to predict
and explain
others' behavior
on the basis
of their internal workings,
their feelings,
intentions,
desires,
attitudes,
beliefs,
knowledge,
and point of view.
One of the classic tests
for Theory of Mind
in children
was developed
by Simon Baron Cohen
and his colleagues
in the mid-1980s.
It's sometimes called
the Sally Ann Test
or the Marble Task.
It goes like this.
A child is presented
with a short skit
performed by two dolls,
Sally and Ann.
There are three props,
a basket,
a box,
and a marble.
First,
Sally takes the marble
and hides it
in the basket.
Then she exits the scene
to take a walk.
While she's away,
Ann sneaks over
to the basket
and moves the marble
to her own box.
When Sally returns
from her walk,
the child is asked
this question,
where will Sally
look for her marble?
It isn't until
about the age
of four years old
that children
stop saying the box
and start saying the basket.
Up to four years of age,
children have a hard time
ignoring what they know
to be true,
the marbles in the box,
to consider what
Sally thinks is true.
This is a pretty
significant shift
in children's thinking.
It not only shows
that they can simultaneously
hold two different views
about the world,
but that they can suppress
their own view
to embrace the view
of another.
It's no accident
that this gradual development
of perspective-taking skills
is highly correlated
with language development.
As perspective-taking
gets more sophisticated,
so does language.
This has led some
to hypothesize
that perspective-taking
is a key mechanism
for language development.
This certainly seems
to be the case
for things like
joint attention
and early word learning,
and it's possible
that improvements
in theory of mind
help with the later stages
of language development
as well.
One way to approach
this link
between perspective-taking
and language acquisition
is to consider cases
in which perspective-taking
goes off the tracks
in development.
One such case
we've already discussed
is Autism Spectrum Disorder,
or ASD.
As you know,
children with ASD
not only exhibit
significant language impairments,
but they also have
difficulty taking
the perspective of others.
For example,
connecting back
to Baron Cohen's
theory of mind test,
children with severe ASD
think that Sally
will look in the box,
and this egocentric perspective
persists even after
massive retraining.
This deficit
is so pervasive
and reliable in ASD
that it's sometimes
even used
as a diagnostic feature
of the condition.
Another diagnostic feature
is impaired
social communication.
One early manifestation
of this
is using hand gestures
differently
than typically developing children.
Actually,
it's a common misperception
that children with ASD
don't gesture.
Actually,
they do.
They just do it differently.
Typically developing children
will point to things
not only to request them,
these are called
imperative gestures,
but as you know,
they also gesture
to comment on things
and to inform people
about things.
In contrast,
the gestures of infants
and toddlers
with ASD
are almost always imperative.
Eye gaze
is also different.
Not only do most children
with ASD
fail to engage
in joint attention,
but they often
don't make eye contact
with people
in the same way.
In one eye tracking study,
Warren Jones
and Amy Klin
of Emory University
showed that
for the first two months
of life,
children at risk
for autism
found eyes
equally interesting
as typically developing children.
They look just as long
at them.
However,
from ages two months
to six months,
the ASD at risk group
looked less and less
towards the eyes
compared to the control group.
The researchers
interpret these findings
as a derailment
of the typical trajectory
of social gazing.
The argument goes like this.
All children
are born
with an innate preference
for looking at human eyes.
Most children
quickly realize
that the eyes
offer social clues
about people's interests
and intentions.
This realization,
in turn,
causes even more
attention to the eyes.
From there,
the cycle repeats itself
and attending to the eyes
eventually becomes
a habit
by the time children
start learning language.
Things are different
for children with ASD.
Although they start life
just as interested
in human eyes,
for some reason,
they never learn
that eyes are a goldmine
of social information.
And because they don't find
eyes socially rewarding,
they never develop
the habit of attending
to them during language learning.
We're still not sure
why they lose interest,
but findings like this
suggest that
the mechanism for ASD
involves a chain reaction
of innate inclinations
that then derail
typical social experiences.
If you think about it,
this is actually
cause for hope.
If we can identify
very early warning signs,
we can develop
therapeutic interventions
to boost attention
to the eyes.
And this may help
at-risk children
stay on track
for typical language development.
Okay, before we finish up,
I just want to say
a few words
about another condition
that has intriguing lengths
between perspective-taking
and language.
When you hear
the word psychopath,
what attributes
come to mind?
Manipulative?
Emotionally detached?
Selfish?
The popular stereotype
of a psychopath
is a cunning person
with no empathy
who's only looking out
for number one.
There's no question
that the labels
of cunning
and self-serving
are justified,
but research suggests
that psychopaths
may be more empathic
than we previously thought.
In a 2013 study,
Christian Kiesers
and colleagues
teamed up
with Dutch
forensic psychiatric clinics
to measure
empathic brain responses
of prisoners
diagnosed with psychopathy.
Under very close guard,
the team had inmates
come into the lab
and get fMRI scans
of their brains
while they viewed
pictures of people
in pain.
In non-psychopath
control subjects,
these images
strongly activate
many of the same parts
of the brain
that are active
when people are actually
in physical pain.
This is interpreted
as a type of
neural empathy.
You understand
the pain of others
by simulating it
in your own brain.
Psychopaths are different.
When the Dutch inmates
were in the scanner,
they showed much less
empathic activation
in these pain regions,
suggesting that
they had difficulty
taking the perspective
of the people
in the pictures.
But when the inmates
were explicitly asked
to take the perspective
of the people in pain,
they looked a lot more
like the control group.
However,
there were some big differences
in the frontal lobe
suggesting that psychopaths
were engaged
in a much more strategic
and controlled form
of thinking.
This finding
adds nuance
to the traditional view
of psychopathy.
It's not that psychopaths
are incapable
of taking the perspective
of others.
It's just that
they're very strategic
about when they do it.
Empathy is a default mode
for most people,
but it appears to be
optional for psychopaths.
This actually makes
a lot of sense
if you consider
some of the common
manifestations
of the disorder.
One telltale sign
is superficial charm.
This social savviness
allows psychopaths
to thrive naturally
and openly
in ultra-competitive careers
like business,
entertainment,
and law.
Turns out
there's many ways
for a psychopath
to make a killing.
Kieser's study suggests
that one reason
why psychopaths
can be such smooth talkers
is that they have
an empathy switch.
When it serves
their purpose,
they can turn it on
and see things
from another person's
perspective.
After all,
the best con artists
are masters
of telling people
what they want to hear.
But things change
abruptly
after the charm
has achieved its goal.
Once they've hooked
and drawn someone in,
having empathy
may actually get in the way
of doing bad things
to a victim,
so it would be
highly adaptive
for psychopaths
to turn it off.
This is actually
the worst
of both worlds,
using empathy
to gain trust
and then shutting it down
to do harm.
That's truly chilling.
Okay,
let's wrap up.
I hope it's clear
that the social side
of language
is a powerful tool
in determining meaning,
not just in word learning,
but in all of communication.
Whenever I think
about this social function,
I'm reminded of how
the great philosopher
Ludwig Wittgenstein
described language
in his book
Philosophical Investigations.
He didn't see language
as a thing.
He saw it as a game.
And as with any game,
the meaning isn't really
in its pieces.
It's in what you do with them.
In the Gavagai game,
considering the social perspective
of the speaker
makes the move
so much easier
to understand.
Rather than facing
a wide-open playing field
of endless meanings,
using the pragmatics
of the utterance
greatly simplifies
the options.
In our next lecture,
we'll explore
how mastering
the structure of language
helps make the language game
even easier to play.
