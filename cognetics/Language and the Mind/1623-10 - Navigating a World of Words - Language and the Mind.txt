The scientific study of language and the mind owes a huge debt to the field of philosophy.
Not only do philosophers ask profound and provocative questions, they're very effective
at using concrete examples that get at the heart of a matter. To introduce the next few
lectures, I want to share one such example in the form of a famous thought
experiment. It was posed by the 20th century American philosopher Willard van
Orman Quine. You may have heard of this as Quine's conundrum, and it goes like this.
Imagine you're a linguist traveling to an isolated land with inhabitants who speak
a totally unknown language. Quine called it Arunta. Picture walking on a path with
one of the inhabitants, and a rabbit suddenly pops out from the vegetation.
As it rushes by, your Arunta companion exclaims, Gavagai. Quine's deceptively simple question asks,
what do you think Gavagai means? First, the obvious answer, the rabbit, of course, but not so fast.
Does it more generally mean a rabbit, or more specifically, this particular rabbit?
Or maybe it's referring to some aspect of the rabbit, like its shape, texture, or color.
Or one of its parts, like the tail or the ears. Then again, maybe it's about what the rabbit is
doing. If so, does it mean hopping, moving, or fast? And how about this? What makes you so sure
it's even about the rabbit at all? I think you'd get the picture. Quine's name for this problem is
an indeterminacy of reference, which is just a fancy way of saying it's not clear what words
refer to. The interesting thing about this thought experiment is that we've all actually performed
it in one form or another countless times over the course of our lives. And by far, the time we did it
the most was as a child learning our native language. Being born into this world as a human
is like being a linguist dropped into the land of Arunta. But unlike the conundrum presented by
Quine, it turns out that babies, and adults for that matter, have many tricks up their sleeves to
crack the reference code. To set this up, let me share a nice little parable. It's sometimes attributed
to Francis Bacon, but actually the source is unknown. It's about a group of village elders
vigorously debating how many teeth a horse has. The elders draw on ancient texts and put forth
elaborate theological arguments. But after 13 days, there's absolutely no agreement. Then on the 14th day,
a young friar comes along and boldly suggests a solution. The elders could just head to the barn and
actually look in a horse's mouth. As the parable goes, the friar was scorned for making such an
audacious suggestion and challenging the elders' authority. On the next day, the question was
declared to be an unsolvable mystery. One of the morals of this story is that many puzzles are cleared
up just by going out into the world and observing how things actually are. In today's lecture, we'll do
just that. In the years since Quine, there's been an explosion of research empirically investigating
how young children actually map meaning to words in real-life language development. As with other
things we've discussed, you'll see that human babies possess some innate and learned heuristics
that give them a head start in solving Quine's conundrum.
To solve Quine's problem, a baby must be able to actually hear the different speech sounds that
compose words. Now, you already know that newborns are innately capable of doing this,
so that gets them off to a good start. Janet Worker has argued that these early phonological
skills serve as a sort of bootstrapping mechanism for babies to be able to attach meaning to new
word forms. This makes a lot of sense if you consider that from ages 6 to 12 months, not only do babies
start to narrow their perceptual focus only on native language phonemes, that's also when they begin to
understand words in their native language. Comprehending standalone words like mommy or bottle,
that's one thing, but parsing words in a sentence is another. One of the major challenges faced by
babies in their first year is to identify where one word ends and another begins. Have you ever watched
a film in a totally foreign language without subtitles? That's what it's like to be an infant learning
language. The good news is that babies have some powerful learning tools that help them crack the code.
In 1996, Jenny Safran, then a graduate student at the University of Rochester, published a paper that
changed the way we think about how infants learn language. I'll give away the take-home message
up front. She found that eight-month-old babies are able to extract statistical regularities
in continuous speech to identify words and word boundaries. For adults, identifying word boundaries
in our native language seems trivial. But try to see things like an infant. Consider the example that
Safran used to set up the problem. Imagine saying pretty baby to an eight-month-old. I'll say it again,
and notice that the two words run together. Pretty baby. How are infants supposed to know that they are
hearing two words, pretty and baby, and not one word, pretty baby, or even three words, pre, t-day, and be.
Safran's hypothesis was that babies solve this problem by paying close attention to what syllables
co-occur in their native language. To test this, Safran presented eight-month-olds with a two-minute
string of English syllables spoken in unbroken secession sounding like this.
Ti-ba-ku-ba-ta-di-ba-gi-ba-du-ba-ki-ba-da. It went on like that for two minutes.
The key manipulation was to vary the probability of certain syllables co-occurring with one another.
For example, over the two minutes, whenever there was a B sound, it was followed by a DA sound 100% of the time.
But whenever there was a ku sound, it was followed by a pa sound only 33% of the time.
If babies use these transitional probabilities as cues to word boundaries, they should be more likely to learn
that beta is a word than kupa. After all, beta always goes together, but kupa does not.
That's great, but how do you measure this? Safran had a creative solution. She borrowed a procedure from her
advisors that involved coupling words with a blinking light. Previous research had shown that
babies are naturally drawn to blinking lights and will look longer at them if the light is accompanied
by some interesting auditory information. The idea is this. If words are inherently interesting to babies,
they should look longer when they hear things that were words, like beta, than non-words, like kupa.
Make sense? Well, that's exactly what Safran found. After just two minutes of exposure, the eight-month-olds
looked longer at the blinking light when accompanied by sound combinations containing beta than kupa.
This suggests that babies actually learn something about the words and word boundaries.
Let that sink in. What this means is that young babies are such good pattern recognizers that they can
learn what is a word and what's not a word from extremely limited auditory input.
This is a very robust finding and has been replicated many times. And it's also been extended in interesting
ways. Here are three. One, the ability appears to be innate. A Finnish team of researchers has measured
electrical activity of newborn brains and found that they also use statistical regularities of syllable
combinations to identify words and word boundaries. Two, research by Safran's advisors, Richard Aslan and
Alyssa Newport, has shown that the ability to detect patterns in speech probabilities is not unique to
humans. Monkeys can do it too. This suggests that the skill is built on mechanisms shared across species.
And three, this pattern recognition ability shows up in other modalities, such as vision, suggesting that
it's a domain general skill and not one specifically designed for language. Altogether, the story should
be familiar by now. Humans are born with some innate domain general knowledge and powerful learning tools
that allow them to get an early jump on learning language. But let's not forget Gopnik melts off
and cools third mechanism. The burden of learning word boundaries does not fall on infants alone.
They get plenty of help in the form of unconscious tuition from adults. If you've ever talked to a baby,
you've probably noticed that your speech does a funny thing. For most people, it becomes very sing-songy and
it greatly exaggerates certain sounds. This distinct speech pattern was called motherese in the 1970s,
but it's probably better known by a term that's been around for over a century, baby talk. By exaggerating
phonemes and accentuating certain words in a sentence, adults are unwittingly giving babies cues about the
structure and rhythm of their native language. This information helps babies identify words and pick
them out in the flow of syllables. If you combine that with baby's excellent pattern recognition skills,
the problem of identifying words and sentences becomes so much simpler.
So now we've worked our way up to Quine's conundrum. Once a baby has identified a word,
how does it know what it means? It turns out that they get a lot of help from two places.
Not only do adults provide useful hints to clear up ambiguities, but babies themselves have some powerful biases
and heuristics that help constrain potential meanings. We'll talk about these biases and
heuristics in this lecture and we'll cover the social hints later. Ellen Markman of Stanford University
was one of the first developmental psychologists to systematically study the cognitive constraints
on children's early word learning. She identified three in particular that greatly helped children solve
Quine's problem. The whole object assumption, the mutual exclusivity bias, and the taxonomic assumption.
Let's take them up in order. The whole object assumption refers to the tendency of a language
learner to think that a word refers to a whole object and not its parts. So when babies hear the word
bottle, they will assume that it refers to the whole thing, not just part of it, like the plastic base or
the nipple on top. This bias is built on another more basic bias for how babies perceive objects
independently of language. Elizabeth Spelke and Philip Kelman argue that babies have an innate tendency
to perceive objects as connected holes and not as combinations of individual pieces. Back to the baby
bottle example. They visually experience it as one thing and not as something with two separate parts.
To prove that babies perceive objects this way, Kelman and Spelke did a study that presented three-month-olds
with a rod that moved back and forth but had the center part of the object occluded. They used a
procedure called a habituation paradigm that's similar to what we've discussed with infant phoneme
perception. Remember, the idea is that babies get bored with a thing and they look away, but then if you
change something up and the babies notice it, they get interested and look again. It's the same idea with
this study. During the preliminary habituation stage, the experimenter showed the infants the
stimulus over and over and measured how long the babies kept looking at it. Over time, the babies grew
bored with the same stimulus, so they looked at it less and less. Next, the key part. Babies were presented with
one of two new stimuli, this time without any visual occlusion. One stimulus was an unbroken long rod that
moved in the exact same way as the habituation phase. The other stimulus was two smaller rods that were
unconnected but moved in synchrony. Any guesses which one surprised the babies the most? When the three-month-olds
saw the unbroken rod, they barely took notice, as if they had been seeing it all along. But when they saw
the two smaller rods moving in unison, they stared much longer, as if to say that they were quite
surprised. This suggests that young babies perceive objects as connected holes. It's worth adding that
this finding has been replicated with newborns, suggesting that it may be an innate constraint in how
infants first see the world. So, how does this connect up to word learning? Think back to the
Gavagai example. This innate constraint on how we view objects suggests that when babies hear words,
they may be more likely to think the word refers to a whole object, like the rabbit, rather than the
parts of the object, like the ears or the tail. So, what's the evidence for this hypothesis?
Building on the work of Markman, George Hollick, now at Purdue, and his advisors from Temple University,
have provided an answer in a clever experiment. In their study, one-year-olds were presented a novel
object that had two connecting parts. One part was plain-looking, very boring, and the other part had
a colorful pattern on it. After babies were presented with this two-part object, they were taught a novel
label for it. The experimenter said, look at the Modi. See the Modi? It's a Modi. The only thing the
experimenter did while saying the word was to show that the object could be broken into two parts and
then putting it back together. So, the baby heard the word Modi both when the two parts were together
and also when they were pulled apart. The question was, what did the babies think Modi meant? The whole
object or one of the parts? To answer this, Hollick then presented the infant with two objects at the
same time. The whole object with both parts and just the most interesting of the two parts. The babies
were then asked, where is the Modi? Can you find the Modi? So, where did they look? The one-year-olds
looked at the whole object over 50 percent longer than they looked at just the most interesting part.
Okay, well, you might say that the babies didn't really learn the words for the whole object. They
just looked at it because it was more interesting. Well, to rule that out, Hollick did a follow-up study
just showing the whole object and the interesting part without any labeling and then measuring how long
the babies looked at each. When there was no label, the babies looked equally long at both objects. So,
it appears that hearing a novel label was the true cause of the whole object bias. You can see how
such an early bias would constrain the problem of word learning to a large extent. If babies' first
guesses about word meanings eliminate a whole range of possibilities, it would allow them to get an early
foothold in language learning that may help them learn additional words faster. But as with any bias,
this does have a downside. As adults, we know that not every word refers to whole objects. They
sometimes refer to parts or descriptions of parts. So, this bias can actually cause children to make an
occasional mistake. In one amusing example, the cognitive psychologist John McNamara describes a
child who thought the word hot was the name for the kitchen stove. Still, even with the occasional error that
comes with the whole object assumption, I hope it's clear how babies can greatly benefit from these early
constraints on word learning. In addition to innate constraints, there are constraints that are acquired
too. The act of learning words actually helps to rule out meanings of other words. For example, let me try
something with you. If I were to show you two animals, like a dog and another animal that you've never seen and I told you that one of
these is called a kinkajou, which one do you think I'd be referring to? If you're like a typical three-year-old child, you'd pick the one you've
never seen. This is Markman's mutual exclusivity bias. It may seem obvious to you that a novel name should
go with a novel object, but how is a three-year-old supposed to know that? For all they know, kinkajou is
just another word for dog, but that's not what they think. Connecting back to the Gavagai example, this heuristic
would be useful for ruling out some potential meanings. Suppose that you had already learned a different
Aruntha word for rabbit. If you heard the Aruntha speaker say Gavagai in the presence of a rabbit,
you would likely assume that it refers to something other than the whole rabbit. Perhaps it refers to
the movement of the rabbit or some part of the rabbit. In this way, the meaning of new words is constrained
by knowing the meaning of other words. The mutual exclusivity bias is one of the most robust heuristics
that children and adults use when understanding language, but there is much disagreement over what the
mechanism is. On one hand, it could be a pretty simple cognitive rule that humans follow. The rule
says that if you hear a new word, that word probably goes with something new rather than something old.
Sure, we'd make mistakes with such a heuristic, but the gamble would pay off more often than not.
On the other hand, the process could be more social in nature. It's not as if you used a simple
cognitive rule to attach the meaning of kinkajou to the novel animal. After all, as adults, we're fully
aware that even objects with established labels like dog can have many other names assigned to them as
well. So you probably solved the problem differently. One way you could have done it is to consider
pragmatic intent. You may have used shared knowledge that you and I both know what a dog is and what it's
called. And you may have surmised that my intention in saying the new word kinkajou was to teach you
something new. This mechanism is different from the simpler cognitive rule because it requires you to
consider the intention of the speaker. Researchers have not yet determined which mechanism explains young
children's early word learning, but it's possible that both are at play. Word learners may rely on a
number of heuristics to break them into basic understanding of language. And as they get older,
they may refine their strategies in a more focused and sophisticated way. Please keep this in mind over
the next few lectures when we talk about other factors in language learning. More often than not,
language learners rely on multiple mechanisms on multiple levels of analysis.
Markman's third constraint is the taxonomic assumption. Once children move past
mutual exclusivity and have accepted that objects often have multiple labels,
they start to really appreciate that words often refer to categories of things. For example,
if you were to point to a dog, you could label it as a dog or you could describe it more generally
as a pet or an animal. In one of the first studies to explore how children use language to understand
categories, Markman and Jean Hutchinson had a puppet introduce preschool children to a picture of a well-known
object, like a dog. By the way, puppets are often used in word learning studies to keep things
interesting for children and actually I think for the experimenters too. After showing the picture,
the puppet said, can you find another one, while showing the children two new objects that had
different relationships to the dog. One object was a cat and the other was a bone. Children selected the
cat only 25% of the time, which suggests that they may prefer to make thematic associations,
dogs like bones, more than taxonomic ones. Dogs and cats are both animals. However, things changed when
the puppet labeled the first object. In a separate group of children, the puppet presented the picture
of the dog, but this time said, see the Dax? Can you find another Dax? In this condition, when asked to
pick an object, children lost their preference for the bone and now chose the cat 65% of the time.
This suggests that when a novel label is introduced, children treat things quite differently. With a
label, they now see the word Dax as referring to a category or type of thing, like dogs and cats are
animals, and not as a thematic relationship between things. In this way, it appears that when language is
present, children are biased to think about taxonomic categories. Let's apply this to Quine's problem.
Suppose that prior to the rabbit, you had both seen a zebra, a snake, and a bird, and for each one,
the Arunta speaker had said, Gavagai. By the time you got to the rabbit, you may use your taxonomic
assumption to guess that the word refers to the larger category of animal rather than some thematic
relationship between the rabbit and some other thing. In the years since Markman's classic study,
researchers have explored just how early babies treat new words as category labels. As with other
abilities, we've learned that babies are surprisingly savvy at very young ages. In one remarkable study,
Alyssa Ferry, Susan Hespos, and Sandra Waxman of Northwestern University tested categorization at the
raw age of three months old. They presented these infants with pictures of different species of fish.
With each picture, they also played an audio track that said,
Look at the toma. Do you see the toma? They did this eight times with eight different pictures of fish,
but the same audio track for each one. Next was the test phase. The babies were then shown two pictures.
One was a new exemplar of the fish category, and one was a picture of a totally new category,
a dinosaur. The key measurement was to record which objects the babies looked at more. Remarkably,
these preverbal infants differentiated the two pictures, looking longer at the image within the
same category, the fish, than the one in the new category, the dinosaur. Now, you might be asking
yourself a good question. Does this really have to do with babies using speech to form categories?
Or is it just that babies had gotten used to looking at fish, not dinosaurs?
The researchers addressed this valid question by repeating the exact same study with a different
set of three-month-olds. But there was one change. They replaced the auditory naming of the objects with an
auditory tone during the exposure phase. So there was a sound that accompanied each picture, but no speech.
After eight exposures of fish plus tones, the babies were shown the two objects, a new fish and a dinosaur.
Now the babies did not differentiate the two pictures. It was as if they hadn't been forming
a category at all without a repeated verbal label. If you think about it, this is amazing. These children
are three months old, which is much younger than when we think children understand words. Now,
it's possible that they don't really understand what the words mean. And they were, they just learned
that when they hear language and see similar things, they should start grouping these things in their
minds. But this is still pretty impressive. It means that very young children might be using language to
help orient their minds to categorize things in the world. And this could help them constrain their early
guesses about what words actually mean. It's worth adding that humans are not the only ones who use
language to categorize the world. As we discussed in the lecture on cross-species comparisons, there is
evidence that dogs can learn words in a categorical way too. Now, the categories that are most relevant to
dogs may be different than those of humans. But still, this suggests that whatever mechanisms humans have
evolved for thinking and communicating in a categorical way are to some extent shared with other species.
This adds further evidence that language may be built on mechanisms that are evolutionarily conserved
across many species.
So let's come back to Quine's conundrum. One of the lessons we learned from using scientific approaches to
explore philosophical problems is that what looks like an impossible puzzle can turn out to have some
ready-made solutions. As we've seen, the problem of attaching meaning to words is not a free-for-all.
Instead, if you actually study how children learn and understand words, empirical research shows that
the process is highly constrained from the very beginning. These constraints give language learners a
head start in determining what words do and don't refer to.
In this lecture, we talked mostly about perceptual and cognitive constraints that operate inside the
heads of language learners. But in the next few lectures, we'll talk about constraints that are more
out in the world, ones that are more social in nature. We'll consider how language learners attend to
the pragmatics of social interactions to learn words and how the very grammatical structure of language
itself can also be a helpful guide. Humans are incredibly resourceful learners. And to overcome the daunting
challenge of acquiring a language, we need to bring all our skills to the table.
