.
Philosopher Ludwig Wittgenstein asked us to think about language as a game.
Let's run with this analogy a bit further and consider how syntax and grammar function in this game.
Or put another way, in the game of language, what's linguistic structure good for?
When it's all said and done, the most important function of syntax and grammar is to organize information and maximize predictability.
For a speaker, this means packaging ideas in a way that's consistent and orderly,
and for a listener, it means accurately and reliably predicting the meaning of an utterance.
When viewed from this very broad perspective, language can be seen in an almost non-linguistic way,
in which it's less important to consider the particular structure of a language, or even the universal patterns in all languages.
What's important is that there's structure in the first place.
In this lecture, we'll talk about how the mind is wired from birth to see structure in language
and use structure to organize and communicate information.
As with other aspects of language development, this innate mechanism interacts with linguistic experience over childhood
to allow the design of language to emerge.
The syntax and grammar at the core of a language rely on the ability to produce, recognize, and predict patterns.
There's no question that pattern processing is one of the most widespread and useful mechanisms that humans innately possess.
There are countless examples, but let's focus on one that has life and death consequences.
The human immune system.
Our immune system employs an arsenal of innate pattern detectors.
There are two general types of defense.
On the front line, there's the primitive immune system.
This system is evolutionarily ancient, and it's composed of so-called pattern recognition receptors.
These receptors are pre-designed to detect molecular signatures of known dangerous molecules.
There's no learning necessary for these receptors.
They're like soldiers who are born knowing exactly who their mortal enemies are, as well as exactly how to fight them.
The next line of defense is the adaptive or acquired immune system, which is triggered by the primitive system.
The adaptive system is also innate, but it requires experience with specific invaders to learn how to fight them effectively.
This learning happens through a process of trial and error, and it remains quite flexible throughout one's lifetime.
Both of these immune systems fundamentally rely on processing patterns.
The difference is that in the case of the primitive system, the patterns are preset and rigid, but for the adaptive system, they are open and flexible.
If this is sounding familiar, that's good.
Connecting back to earlier lectures, I hope you can see how this relates to domain-specific and domain-general mechanisms.
Does the human mind come into the world with pattern recognition skills specifically suited for language,
or does it innately possess more general pattern recognition abilities that get used for many things in addition to language?
I'd like you to keep these two questions in mind for this final lecture on development.
Although the evidence favors the more domain-general mechanisms, this is still an ongoing debate.
Let me take you back a couple of lectures.
Earlier, we discussed how infants are like little scientists who constantly collect data and test hypotheses about their world.
But the key to scientific success is to continue the cycle over and over and over.
After you test a hypothesis, the results affect future data collection, which then leads to new hypotheses and tests, and the cycle repeats.
This is the nature of science, and it's just as natural for babies, too.
The general process underlying this never-ending cycle is a type of probabilistic learning mechanism called Bayesian learning.
The basic idea is this.
We make predictions about the world based on a combination of two things, current evidence and prior knowledge.
The power of Bayesian learning is that once we test a prediction or change our prior knowledge, it updates the probability of future predictions.
Just like the scientific method, this cycle is on a never-ending loop.
Let's try a real-world example with syntactic development.
Suppose a three-year-old hears the following novel sentence.
The dog tibs the boy.
Because tibs is a new word, the child has to determine what type of speech it is.
Let's suppose the child is considering two choices.
A verb meaning something like licking or biting, or a noun meaning something like food.
Now, there's a 50-50 chance of both of those being right from the child's perspective.
Based on this, do you think the child would essentially let a coin toss decide whether the word was a verb or a noun?
If you said no, you just acted in a very Bayesian way.
In addition to considering this 50-50 chance, the child would most certainly also consider past knowledge about the way English syntax and grammar work.
For example, the child would probably call up prior knowledge that most verbs are wedged between nouns, between two nouns.
When you add that to the equation, the prediction that tibs is a verb and not a noun seems much more plausible.
Now, imagine two new scenarios.
In one scenario, the child hears the word tibs again, but this time in a new sentence between two different nouns.
Now, the child has even more data that confirm the original prediction, and this would increase the confidence that tibs is indeed a verb.
But how about this scenario?
Suppose the child didn't actually hear the whole sentence the first time, but the second time they did.
And this is what they heard.
The dog tibs the boy ate made him sick.
Did your brain just do a double take?
This is another example of a garden path sentence, and it illustrates the power of prior knowledge.
To make sense of this new sentence, you had to realize that dog was not a noun preceding a verb.
It was actually an adjective modifying a noun.
Dog tibs may be another word for dog food.
With this knowledge in mind, try the sentence again.
The dog tibs the boy ate made him sick.
With this new information, the child would have to update the contents of prior knowledge in the following way.
In cases of complex embedded clauses, sometimes a noun occurs in a position where a verb typically goes.
Just to drive the point home, let's do another example of something familiar.
Recall Jenny Safran's study showing that eight-month-olds could learn word boundaries from only two minutes of speech input.
One Bayesian way to think of this finding is that children used this two minutes of exposure to gather evidence about which syllables co-occurred and which ones did not.
And they combined this information with their prior knowledge of English syllables.
If eight-month-olds can pick out regularities from a mere two minutes of speech, they no doubt have already extracted a few basic rules from their eight months of previous exposure to English syllables.
This combination of new evidence and prior knowledge is what guided them to identify words and word boundaries in the experiment.
Let me remind you that this sort of linguistic pattern recognition has been shown even in one-day-old babies.
So the capacity for Bayesian learning appears to be an innate mechanism for language acquisition.
To help you appreciate the power of this innate knowledge or this innate mechanism, I want to highlight three important things about Bayesian learning.
First, it allows for very fluid and dynamic processes of educated guessing.
Predictions can constantly be updated based on new evidence.
Second, learning an important new piece of information can change your prior knowledge, and this can radically change how you interpret patterns of evidence.
For example, suppose you're romantically interested in someone, but that person always seems to ignore you in a social group.
Based on these observations, you might hypothesize that the person has no interest in you at all.
But now imagine that a mutual friend tells you that your romantic interest is also very interested in you, but is extremely shy.
This new knowledge would cause you to completely reinterpret the evidence up to that point.
And here's the third thing.
Bayesian learning allows for the creation of powerful but flexible rules about the world.
This is different from traditional empiricist explanations about how people learn from experience.
For example, behaviorists like Watson, Skinner, and Pavlov argue that people learn new things just by making superficial associations between one thing and another.
For these behaviorists, all learning is surface learning.
There are no deep and abstract rules to guide behavior.
This view was vigorously challenged by Noam Chomsky back in the 1950s when he debated B.F. Skinner about language learning.
Chomsky argued that this superficial learning could not account for the apparent rule-like behavior of children during language acquisition.
Chomsky astutely pointed out that there is a lot of abstract learning that goes beyond the superficial linguistic input.
For example, why do children over-regularize verbs and nouns, like when they say goad and fishes, if they've never heard those words before?
As you know, Chomsky's answer was to build in an innate language-specific mechanism to explain these and other rule-like occurrences.
However, Bayesian learning provides a more general alternative.
The advantage is that nothing specific to language needs to be built in.
Rather, a general mechanism can respond to linguistic input and generate specific rules based on experience alone.
It's for this reason that domain-general Bayesian theories are so attractive.
They allow learners to create complex linguistic rules and representations without building them in from the start.
Before we finish up with Bayesian learning, I want to emphasize just how general a tool this is.
Not only is it involved in many other aspects of human cognition, like perception, attention, and decision-making, but it applies to non-human thinking too.
It appears that some species, like bees and dogs, are capable of Bayesian learning, so it seems to be an evolutionarily conserved mechanism across species.
And it doesn't stop there.
There are variants of Bayesian learning, sometimes called Bayesian deep learning, that don't even require biological brains at all.
Artificial intelligence is increasingly using this form of learning.
Everything from email spam filters to computer chess masters and self-driving cars to cancer screening tests.
There's even chilling talk of the military using these Bayesian mechanisms to train autonomous attack drones.
This is just the start, and you can expect to see many more examples of Bayesian applications in the years to come.
Bayesian learning is a nice example of a constraint on language acquisition that brings something extra to the learning process.
In the past few lectures, we've discussed other examples of this too, like mutual exclusivity in word learning, or theory of mind and understanding intention.
But sometimes, there are guides on language learning that involve subtracting rather than adding.
One of my favorite examples illustrating this is a hypothesis by Alyssa Newport, aptly named the less is more hypothesis.
Newport's idea arose in response to a long-standing observation in language learning.
Children learn language better than adults, a lot better.
We know this from decades of research on second language learning, or L2 learning for short,
where children consistently outperform adults in L2 pronunciation, morphology, and complex syntax.
If you recall, Pat Kuhl had one explanation for this paradoxical finding.
Children's brains are much more plastic than adult brains, and this plasticity helps them keep their options open.
Newport provides a different explanation.
In addition to having brains that are more plastic, children possess a much more limited cognitive capacity than adults.
But wait, shouldn't that be a bad thing? Isn't more cognitive firepower better for learning things?
Not necessarily. This is one case where bigger may not be better.
Here's how it works.
Because adults are capable of encoding and retaining so much more information than children,
they learn linguistic structure in a more holistic way.
In contrast, because young children have much more limited memory system,
they learn linguistic structure in a more compositional way.
To illustrate, let's go back to our familiar Gavagai example.
Let's suppose that Aruntha is a morphologically rich language,
and that Gavagai is made up of three morphemes.
Let's say the word as a whole literally means small, furry, hopping,
where ga refers to the size of an object, va refers to the distinctive feature of the object,
and guy refers to the behavior of the object.
I doubt that any language actually uses these morphemes, but they'll work for our purposes.
Now, adults may learn the whole word Gavagai as a complete thing without actually realizing
that it's composed of three individual morphemes.
So, for all they know, the whole word is a single morpheme referring to a rabbit.
This means that for every new word that the adult learns,
they map the whole word onto an independent meaning.
It's only after some time that they realize that the individual morphemes are contained in the word,
which causes them to analyze the parts after they've encoded the whole.
This is a convoluted and cumbersome process, and if you've ever tried to learn a foreign language as an adult,
it's probably very familiar to you.
It's very familiar to me.
In contrast, because young children can't encode and remember as much as adults,
they initially only remember parts of words.
So, in our example, they may learn that va means furry in one instance,
and the next time they hear the word, they may remember a different part, like Gai means hopping, and so on.
Over time, this sort of piecemeal learning will actually be more useful to the child to appreciate the morphological complexity of the language.
In this way, processing less information allows children to learn more.
I think we all have experience with this, even as adults.
Think back to a time when you tried to understand some very complex idea,
and you were overwhelmed by all the moving parts.
If you're like most people, it probably helped to break that idea down into smaller pieces
and make sure you understood those before you tried to grasp how they all work together.
That's basically what young children are forced to do all the time,
and it gives them a real advantage in learning the structure of language.
I want to say one more thing about Newport's less is more hypothesis,
and I want to tie it back to one of our big themes in this course.
Given that maturational limitations on memory are very useful in helping young children learn language,
it's worth asking what role these constraints played in the large-scale evolution of language.
Now, we can quickly rule out that they evolved specifically to help children learn the structure of language.
This makes no sense for two reasons.
First, these cognitive limitations are about as domain general as they get.
They affect literally every aspect of a child's mind.
Perception, action, attention, emotion, language, I mean, you name it.
And second, what makes this mechanism so interesting is that these maturational constraints didn't evolve for any purpose at all.
These constraints are just a natural byproduct of human brains taking a very long time to develop.
That's the unavoidable biology of altricial species.
By definition, we have an extended period of cognitive and physical immaturity.
There's a term in biology for this phenotypical characteristics that are byproducts of some other aspect of an organism.
They're called spandrels.
This term was coined by the Harvard paleontologist Stephen Jay Gould, who borrowed it from architecture.
In large medieval cathedrals, a spandrel is the triangular space between the exterior curve of an arch and an enclosing right angle border.
These spaces are almost always decorated with art.
And if you didn't know any better, you might think that the spandrels were intentionally designed for artistic purposes.
They weren't.
They were entirely accidental byproducts of supporting arches.
And artists co-opted them to serve an aesthetic function.
So what's interesting about Newport's less is more hypothesis is that even though cognitive limitations help children learn language,
we can be sure that they weren't specifically designed for that purpose.
So what's the alternative?
Well, what if we flip things around like Morton Christensen and Nick Chater have asked us to do?
Instead of asking, did maturational constraints on learning evolve in response to the structure of language?
We might ask, did the structure of language evolve in response to maturational constraints on learning?
Recall that Christensen and Shader's argument is that because the basic biology of the human brain changes much more slowly than cultural artifacts like language,
the brain is a more likely anchor for evolution than language.
After all, humans were an altricial species with an extended period of immaturity for millions of years before language even entered the scene.
This suggests that when language did arrive, it had a stable neural environment to adjust to.
And a constant feature of that environment is that brains are in a form of extended immaturity while they learn language.
So whatever structure language initially took in its evolution should be exquisitely adapted to that particular cognitive environment.
This provides one explanation for why languages are organized in a compositional way.
It's why they have a hierarchical structure of more themes that build to words, building to syntax.
If young language learners have no choice but to break down communication into bite-sized chunks,
that would be an optimal way for language to structure itself.
It should take a form that can be most easily digested by young language learners.
Does that make sense?
Try thinking about it this way.
Language evolved to be learnable in the context of the type of brain that was learning it.
Here's a useful analogy.
The high fat and sugar content in junk food is very stimulating to pleasure centers in the human brain.
Why is this?
Nobody in the right mind would argue that the invention of junk food was the mechanism that caused brains to like fat and sugar.
The preference was already well established in our evolutionary past.
Obviously, humans invented junk food because they knew that brains had an almost insatiable love of fat and sugar.
Well, it's the same story for language.
Language initially fit itself to the constraints of the brain, and not the other way around.
Now, you might be thinking, once language and junk food arrived on the scene, haven't they both done a lot to change human brains?
Absolutely. And don't worry, we'll get to that story in a later lecture.
At this point, I want to pause and take a step back again.
It's no longer controversial to claim that there is an innate or there are innate mechanisms for language development.
That battle was fought in the mid 20th century and the behaviorists lost.
As you know, the more recent conflict has focused on the nature of these innate mechanisms.
Are they specific to language or more domain general?
The tide has turned in this clash as well, and now most scientists believe that the domain general theories are the more parsimonious and persuasive explanation.
However, this issue is far from resolved.
And there are some empirical findings that remain active battlegrounds.
I want to share one of those with you right now.
Let me set it up with a little context.
Before the 1970s, the medical community in the United States was convinced that sign language was not a real language.
At the time, researchers and clinicians made assumptions about sign language without really understanding it.
Of course, now we know that sign language systems are full-blown and beautiful languages.
But things were different back then.
There will be a whole lecture on this topic, so we'll come back to this later.
One implication of this view was that the medical community actively discouraged the teaching of sign language to deaf children.
In its place, they advocated for lip reading.
Unfortunately, this often had damaging consequences for both children and parents because lip reading is extremely hard and not reliable.
The story of how the medical community came to see things differently will come later.
But now I want to focus on something remarkable that came from this unfortunate situation.
It's a true testament to the resilience of language.
Despite the practice of not teaching sign to deaf children, many of the children created a language all their own.
It was called home sign, and it was systematically documented by Susan Golden Meadow, then at the University of Pennsylvania as part of her doctoral dissertation.
Now, we're not just talking about random hand-waving here.
Home sign looks a lot like conventional sign language.
It's got structure, it's got consistency, and if you didn't know it, you'd be convinced it was actually taught to the children.
Not only that, it bears uncanny resemblance among children who develop it.
Sure, individual signs differ across children, but the structure is very similar.
And this is true not only in the United States.
Golden Meadow also observed a similar structure in deaf children from Taiwan.
One of the most striking shared structures of home sign is its syntax.
Home signers in America and Taiwan both use an ergative syntax.
I won't get into the details, but ergativity basically concerns how subjects and verbs are ordered in a sentence.
Now, here's the interesting thing.
Spoken English and Mandarin are not ergative languages.
Take a moment to appreciate this.
It's really quite remarkable.
Where is this common structure coming from?
It can't come from the spoken language because even with lip reading,
the syntax of home sign is different from English and Mandarin.
Maybe it's the co-speech gestures of the parents.
Well, Golden Meadow considered that, and she found that the gestures of American and Taiwanese parents are very different from one another.
And it's most likely not coming from the shared social experience of the environment because America and Taiwan are very culturally different.
So if it's not coming from the outside world, the similarities must be coming from within the children.
What gives rise to this commonality?
Here's where the debate rages.
The domain-specific theorists, like Chomsky and Pinker, argue that there is something special, a special module built into the human brain that allows language to unfold in a structured and consistent way, despite not getting conventional input.
This is pretty compelling.
Seriously, the fact that something so language-like can emerge without any formal instruction makes a domain-specific mechanism sound pretty attractive.
It should be no surprise that the domain-general theorists interpret home sign, well, more generally.
They see it as evidence that the brain is wired in a way that is conducive to learning language, and also a bunch of other things.
For example, there may be basic cognitive constraints that guide how meaning is mapped onto imagery and action, or strong social instincts to share and communicate perspectives with others.
Or, as we've discussed in this lecture, probabilistic learning mechanisms that detect and impose regularity onto the world.
These may combine to create language without being specifically designed for language.
This is one of those scientific cases where it's really hard to know what the right answer is.
Maybe both views are correct.
It's possible that language initially emerged from domain-general mechanisms in our evolutionary past.
But once language arrived on the scene, it put selective pressure on brains to accommodate it.
This is reminiscent of the story of the human biome.
As we discussed, bacteria first adapted to us, but then we started to adapt to them.
Language in the brain may work in the same way.
Drawing inspiration from what we know about the evolution of the human biome,
I hope both sides of the debate can work in cooperation and take the best of each scientific view.
Working together, I'm confident that we can come up with a theory that's more unified and powerful.
So let's wrap up this section on development.
We focused on the big three mechanisms that drive language acquisition.
Innate knowledge, powerful learning tools, and unconscious tuition from others.
Regardless of whether it is domain-general or specific, innate knowledge no doubt gives children a head start in the language learning process.
And because children are also standardly equipped with powerful learning tools,
they can combine this innate knowledge with the input they get from the social environment.
This culminates in an adult brain that is built for language.
In the next section, we'll discuss the moment-to-moment consequences of such a fully formed brain.
Even though we'll be focusing on the very small time scales, on the order of milliseconds and seconds,
you should keep the larger time frames in mind.
What happens in the now is always shaped by what happened in our evolutionary and developmental past.
We will take together a lifetime of settings and would have been replaced with our employees.
We will go out and walk tightly to 4 and 5% ofbildung using duty knowing that doesn't come to your target.
Fire, I'm not sure you get a chance to prefer the paint from the branch to turn-up.
