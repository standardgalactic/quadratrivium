I'd like to make a bet with you.
If I win, all I ask is that you share what you learn in this lecture.
If I lose, well, that means that times are changing.
Here's the bet.
When you get a chance, ask someone you know this question.
What is language?
My bet is that the first thing said, and maybe the only thing said, will be about the spoken word.
Now, there's nothing technically wrong with such an answer.
In fact, most dictionaries lead with a definition featuring the oral form of language.
The Cambridge Dictionary is a notable exception.
And you can't really blame them.
After all, the word language derives from the Latin word lingua, which means tongue.
But what's interesting about all this is that in the grand history of language, the spoken form may be a relatively recent phenomenon.
Remember my lecture on gesture?
That's where we talked about the theory that speech evolved from gestured language systems in our evolutionary past.
Many theorists believe that those gestured systems were in place for much longer than present-day spoken systems.
And gestured languages are alive and well in modern times, too.
The World Federation of the Deaf estimates that worldwide, over 70 million people use sign as their native language.
That's a lot.
That's more than the number of native Italian speakers.
In this lecture, we'll talk about this often overlooked form of language, this signed language.
And I have two goals.
By exploring a version of language that operates in a different modality than speech,
I'll try to give you a wider and deeper appreciation of what language is.
And in doing that, I'll also try to dispel some of the big myths about sign language
with the hope that you can educate others about this fascinating form of communication.
Signers around the world are using quite distinct languages.
It's hard to know how many active sign languages there are, but estimates range from 200 to 300.
And these languages are largely mutually unintelligible.
As the president of the International Society for Gesture Studies,
I gave the opening address to our 2018 conference in Cape Town.
As I spoke, I was flanked by three interpreters signing in three different languages.
American Sign Language, British Sign Language, and South African Sign Language.
This was not just for show.
These interpreters were necessary, absolutely necessary, because the languages are so different.
Even within a sign language, there can be major variations.
Have you ever been to Scotland and traveled between Edinburgh and Glasgow?
These cities are about 40 miles apart, and even though the speakers from both cities use English,
their accent and vocabulary can sometimes make it seem like they're speaking two different languages.
It's similar with sign language.
Both cities use British Sign Language, but my colleagues tell me that when they travel between Edinburgh and Glasgow,
they have to pack a different set of signs.
Now, before we dive in and talk about the linguistic nature of sign language,
let me just remind you of some history.
In a previous lecture, you learned that as recently as the 1960s,
many people, including those in the medical community, did not view sign language as a real language.
In fact, most governments around the world have only just recently recognized sign language as a legal national language.
In America, at least, a big part of this view stemmed from the long tradition of oralism in deaf education.
In the late 19th century, there was a push to try to assimilate deaf individuals into hearing culture,
and this led to a heavy emphasis on lip reading and speaking in deaf education.
By the way, do you know who was the most vocal and prominent advocate of oralism in America?
Alexander Grand Bell, the inventor of the telephone.
That's interesting.
Needless to say, the oralist tradition greatly marginalized the signing community.
I mean, imagine being part of a group where your government doesn't officially recognize your language,
and the medical community declares it unhealthy for your children to learn it.
I share this history to give you a sense of why it took so long for science to embrace sign language as a real language.
Don't get me wrong.
There was excellent research on sign done before the 1970s,
but up to that point, larger social forces kept it out of the mainstream scientific view.
So let's start by breaking down sign language according to our five linguistic components.
Pragmatics, syntax, semantics, morphology, and phonology.
And for each, we can think about the relevant linguistic universals.
Starting first with pragmatics.
I don't think there's ever been any doubt about the social use of signing.
It's pretty clear that the deep need to share information with other humans has always been a driving force in the use of sign.
Think back to the home sign systems by the deaf children in Taiwan and America,
or the creation of the new sign system in the Acai Bedouin deaf community in Israel.
There are many other historical examples as well.
You might be surprised to learn that there was a flourishing community of signers
on the island of Martha's Vineyard from the early 1700s to the mid-1950s.
Historical records suggest that Martha's Vineyard sign language
came from an earlier sign language in England called Old Kent Sign Language.
The language flourished on Martha's Vineyard because of the unusually high number of deaf people who lived there
and its acceptance among the hearing community.
The fact that it was used by both hearing and deaf people alike for so long
is a testament to the powerful social drive to connect through language.
It's useful to see how this pragmatic use of sign reveals the linguistic universal of specialization.
Remember, specialization refers to the fact that all languages are designed to intentionally communicate.
It's not like a signer just throws up a bunch of gestures out there and sees which ones stick.
It's not like a sloppy game of charades.
No, signs are highly specific and reproducible, just like any other signaling convention we use.
And just like any other conventional signal, this suggests a tacit understanding
that the signals will be mutually understood to have specific and reliable meanings.
This brings us to the semantic component of sign language, which involves the meaning of words.
Recall that in the Saussurian tradition, words not only serve as symbols for things, but as arbitrary symbols.
Think back to our example of that big shiny thing in the sky.
The English word for sun and the French word soleil are both arbitrary in their relationship to the concept of the sun.
This highlights the second big myth about sign.
Sign languages have zero arbitrariness.
Not true.
Take the American sign language word for sunlight.
It's a small, one-handed, circular gesture off to the side of the head,
and then you open your hand towards your face.
Now, that seems pretty iconic, right?
But still, why the particulars?
Why not have the hand directly overhead?
Or why not make a fist instead of a circle?
Or why not have the open hand move closer to the eyes?
Even with signs that are highly iconic, like this one, there are always arbitrary elements.
Let's take a much less iconic example.
I'm going to describe a sign from ASL, and I want you to guess what it is.
Ready?
I take two hands and put them in the thumbs-down position, right in front of my chest,
and then I make two circular motions with one hand starting upward and the other starting downward.
Any idea what that means?
It means science, of course.
Where's that meaning coming from?
Some believe that the sign iconically demonstrates putting chemicals in different beakers.
But even if that's true, that's still an extremely arbitrary choice.
I mean, there's a lot more to science than chemicals in beakers, right?
This is the essence of what Saussure meant by arbitrary.
It really doesn't matter what form a symbol takes as long as all language users see how it differs from other forms of other symbols.
It's the differentiation, not the particular form, that matters.
So, just like spoken language, sign language has a mixture of arbitrariness and iconicity.
What's interesting is that the balance is different for the two forms of language.
Speech is heavier on arbitrariness and sign is heavier on iconicity.
This makes sense when you think about what each modality affords.
The hands are great at visually and spatially capturing information.
And because many word meanings can be visualized in space, either directly or metaphorically,
signs can invest more in iconicity.
But speech sounds are different.
They are not nearly as good at capturing imagery or spatial information.
So, it's much more natural for speech to break from iconicity.
This opens the door to much more arbitrariness in speech.
Putting this together, the big takeaway message is that signed and spoken languages
each have elements of arbitrariness and iconicity.
So, both are symbolic in the Saussurean sense.
So, let's turn to the rule-governed and generative universal of language.
When it comes to morphology and syntax, a third big myth about sign is that it basically
borrows its grammar from spoken languages.
There's no question that spoken languages influence sign languages, but sign has many unique
grammatical features, too.
Let's take a different sign language as an example.
Auslan is the imported sign language used by the Australian deaf community.
It was brought to Australia by British and Irish deaf signers in the 19th century, and it differs
from English in many ways.
For example, whereas English pluralizes objects by adding the morpheme S to the ends of words,
Auslan signals plurality in different ways.
Sometimes it uses a numeral sign preceding the object, as in five book, and sometimes it repeats
the sign for the object corresponding to the number of the object, like making the sign
for beer two consecutive times to order two beers.
And if you repeat the sign three times, it means many.
Past tense is also different.
English adds the morpheme ED to regular verbs to indicate an event that happened in the past.
Auslan has no such marker, but instead begins the sentence with a temporal statement and then
describes the event, as in, week past, I wash my car.
Auslan also differs compared to English on syntactic word order.
In English, it means two very different things to say, woman likes man, and the man likes woman.
But in Auslan, the order is much more flexible.
It could be the order, this order, woman like man.
Or the signer could use an eyebrow raise to syntactically mark the man as the direct object.
So it could be, man, eyebrow raise, woman like.
Or, like man, eyebrow raise, woman.
This example highlights the fourth myth about sign language.
It's all about the hands.
Not so.
Back in 2012, Michael Bloomberg gave several press conferences as mayor of New York about the administration's response to Hurricane Sandy, the worst storm to hit the metropolis in decades.
At the podium, the mayor was joined by an ASL interpreter named Lydia Callas.
Callas mesmerized TV audiences.
Many people had never been exposed to so much signing, and Twitter seemed particularly taken by Callas' animated and assorted facial expressions.
But contrary to what most people thought, these expressions were not just optional visual flourishes like they are for spoken language.
They were actual parts of the morphology, syntax, and semantics of ASL.
For example, in one speech, the mayor requested that people not leave garbage out for collection because the storm would cause it to spill and create an even bigger mess.
When translating the word spill, Callas distinctly stuck out her tongue.
This is called the TH mouth in ASL, and it functions as an adverbial marker for something being sloppily done.
Another example is squinting the eyes and closing the lips tightly to add the adverb incrementally.
Callas used this when the mayor said that the hurricane recovery would be very slow and gradual.
Other linguistic moves can be made with the whole body.
In one speech, the mayor was talking about the possible road closures due to the storm.
As he said, the FDR may be open or closed.
Callas leaned her body to the left for open and to the right for closed.
Not only that, but if Bloomberg said something about something else being open or closed, like the airport,
Callas returned to the same use of space to emphasize the distinction.
And there are countless other examples, but this should be enough to give you a sense of how the whole body is used in sign language.
Before moving on to phonetics, let me just say a few words about one of the big universals of language, generativity.
Recall that syntactic recursion is the key to producing generativity.
By embedding clauses, it's possible to create ever-expanding meanings.
Sign does this just as well as spoken language.
And what's interesting is that this recursive embedding becomes more and more common the more generations of signers there are.
In 2001, Anne Cengas of Barnard College and Marie Coppola of the University of Connecticut published a paper describing the emergence of a new sign language in Nicaragua.
In 1977, the first official school for the deaf was founded in Managua, the nation's capital.
Deaf children came from all around the country with no formal sign language skills.
In the oral tradition, initial attempts to educate the children focused on fingerspelling and lip reading, but those attempts were entirely unsuccessful.
And here's the interesting thing.
As the students were being officially taught the oral method in the classroom, something different was happening outside of the classroom.
On buses and school grounds, the children started to create a rudimentary sign language all on their own.
This was the start of what was later to become Nicaraguan Sign Language, or NSL.
Cengas and Coppola describe how these early forms gradually became more and more complex as younger children entered the school and adopted it for themselves.
By the third cohort, there was significant linguistic structure in the language that did not exist in the first two cohorts, and one of those structures was a formalized recursion.
By the third cohort, the children were frequently embedding clauses into their sentences.
This innovation gave NSL syntax the power of generativity.
The emergence of NSL also raises important issues regarding the final component of language, the phonology of sign.
Phenology in speech is determined by the physical configurations of the motor articulators, such as the lungs, throat, tongue, and lips.
Phenology in sign is determined by the physical constraints of the hands, face, and body.
But let's focus on the primary articulator, the hands.
If we're going to talk about sign phonology, we must talk about the pioneering linguist William Stokoe.
Stokoe was a hearing professor, bilingual in English and ASL, and he taught at Gallaudet University in Washington, D.C.
Gallaudet is the premier university in the United States for the education of the deaf and hard of hearing.
In 1960, Stokoe published Sign Language Structure.
The book was notable for three big things.
First, it put an official name to the unique version of sign that was being used in America.
It has been known as American Sign Language or ASL ever since.
Second, Stokoe created a notation system for recording sign utterances in writing.
Up to that point, no sign system had ever been reliably recorded on paper.
These two things were made more significant by the third contribution.
Stokoe's book was the first to empirically demonstrate that ASL was an actual language,
just as rich in structure as any spoken language.
One of Stokoe's main areas of focus was the phonology of sign.
The phonology of spoken language concerns several parameters, such as shape of mouth, location of tongue,
timing of vocal cord vibrations, and so on.
Stokoe showed that ASL was composed of similar parameters, but all in the manual modality.
The big three are shape, location, and motion of the hands.
This means that for every sign, there are three things that systematically combine to produce a distinct meaning.
Perhaps the best way to appreciate this is to consider minimal pairs in sign.
Remember, in speech, a minimal pair is when every phonetic dimension of two words is identical except for one thing.
Like in Japanese, shujin, husband, versus shujin, prisoner.
They differ only in the length of the first vowel.
Well, it's the same with sign.
Let's do a few examples in ASL.
Starting with handshape, there are 18 different configurations used for most signs.
And while holding location and motion constant, differences in shape can completely change the meaning of two words.
Consider the two signs for school and impossible.
Both signs involve putting your non-dominant hand outward, with palm facing up.
To sign school, you lower your flat, dominant hand, with palm facing down, onto your base hand twice, like you're clapping.
The sign for impossible is the same, except instead of a flat hand, your dominant hand is shaped in a Y formation,
which is a fist with pinky and thumb sticking out.
Moving on to location, the words for apple and onion are identical in handshape and motion.
A closed fist with your right index finger slightly bent outwards, and at the same time, pivoting the hand back and forth.
But the location differs.
Apple is done right near the mouth, and onion is done a few inches higher near the eyes.
For motion, there's a nice contrast between a sign we've already done, school, and the sign for paper.
Recall that school is basically a double-clap movement, but paper is more of a sliding motion on top of the hand across the bottom one.
Since Stokoe's publication, there have been a few other phonetic parameters added, like palm orientation and signals with the face.
Putting all these parameters together, I hope it's clear that far from the idiosyncrasy of pantomime,
the components of sign are highly systematic and reliable from one person to the next.
Oh, and here's an interesting side note showing just how influential Stokoe's analysis was.
His book came out in the exact same year that Hockett published his famous paper on the original 13 universals of language.
At the time, linguists did not consider sign to be a true language, so Hockett's universals narrowly focused on only speech.
But after Stokoe's book changed how linguists thought, Hockett later revised his list to incorporate sign language too.
The final pieces of evidence that have sealed sign as a real language come not from linguistics,
but from the fields of developmental psychology and cognitive neuroscience.
Developmentally, the native acquisition of speech and sign is remarkably similar.
Verbal babbling and babbling with the hands both occur around six months of age,
and this is followed by non-sign pointing at about nine months of age.
These typically lead to one-word speech and sign at about one year.
And by 18 to 24 months, signing and speaking children are stringing together two signs or words to form simple sentences.
These simple sentences follow reliable word orders.
Native signers and speakers make similar sorts of mistakes too.
For example, you might have noticed that English-speaking children sometimes mix up their pronouns.
So, if a parent says to a toddler,
You blew the bubbles.
A child might say,
Yes, you did.
This mistake is caused by not knowing the rule that a pronoun shifts meaning depending on who says it.
I means me, except for when it means you.
Kind of confusing, right?
Deaf children make the same mistake or on the same age.
On the surface, this is pretty surprising.
In most sign languages, pointing to yourself is the actual sign for I,
and pointing to someone else is the actual sign for you.
Why would signing children mix that up?
Well, the key is to appreciate the linguistic nature of sign.
They aren't just like regular gestures.
They're actual symbols that have some arbitrary relationship to their reference.
This distinction between regular gestures and signs can be seen very clearly
when taking a look at the brain using cognitive neuroscience techniques.
In one study using PET scans,
Karen Emery and her team at San Diego State
had native ASL signers and English speakers
either pantomime or sign the meaning of objects.
For example, suppose there was a picture of a hammer.
The hearing group would pantomime the action of hammering.
In contrast, the ASL signers would produce the sign for hammer.
On the surface, these two actions look practically identical.
If you didn't know ASL, you might think the sign was just a pantomime.
Here's the interesting finding.
When signers made the sign for hammering,
there was a focalized activity in the left IFG,
which is involved in language production.
In contrast, the non-signers showed a more posterior activation
in the left parietal and temporal regions,
which reflects more spatial visual processing of the actions.
And when signers were asked to pantomime the actions rather than sign them,
their brain activations were a lot like hearing subjects.
This suggests that when manual actions are used as language,
they activate language areas more than when they're just regular old pantomimes.
These studies fit well with what clinicians know from patients with brain damage.
Signers can have aphasia.
Just like hearing aphasics, signers with Broca's aphasia,
they understand meaning, but they have difficulty producing phonological forms.
And although the signing of Wernicke's aphasics appears fluent,
they have great trouble understanding and producing meaning.
What happens when you put both patients in an MRI scanner?
You can see that each type of aphasia has a neural profile of damage
that looks a lot like its spoken counterpart.
Findings like this have helped debunk one of sign's biggest myths.
Deaf signers and hearing speakers have totally different brains.
No, in terms of neural mechanisms, language is language,
pretty much regardless of modality.
Let's connect this back to one of our big themes.
Over evolution, the brain didn't adapt to language.
Language adapted to the brain.
That's what Christensen and Shader theorized
produced the tight fit between the brain and language in the first place.
Running with this, we might conclude the following.
Because hearing and deaf individuals are born
with comparable domain-general neural architecture,
both groups have created a language system
that works according to those shared mechanisms.
So although the superficial form clearly differs,
signed and spoken languages are both products
of the same language-ready brains.
Now, I want to be clear.
I'm not saying that sign and speech are the same.
Far from it.
But their differences have less to do
with language components and universals
and more to do with cultural values and practices.
To explain what I mean,
I'll try to dispel one final myth.
It's perhaps the most stubborn and tricky of them all.
It's the belief held by many hearing people
that all deaf signers would rather use spoken language.
Truth is, it's not so simple.
For the first time ever,
technological innovations like cochlear implants
offer deaf children a chance to boost their hearing abilities,
and this can make a huge difference
in successfully learning to use spoken language.
While most people in the deaf community
are enthusiastic about such technologies,
there's real concern that learning spoken language
will come at the cost of implanted children
not learning sign language.
Why would this matter so much?
Well, one answer involves something we'll explore at length
in the final few lectures.
Culture and language are inextricably tied.
For deaf people,
when you share a form of language
that is perfectly suited to the people who use it,
that produces an incredible sense of community.
It's a community where members feel like they belong,
where they can be themselves
and interact with others in a way that's most natural.
This can build a real love for the language.
As much as any culture,
and perhaps much more so,
the sign community is proud of their language,
and they view it as inherently beautiful.
Many hearing people are surprised to learn
that just like spoken language,
sign language can be an art form.
Like hearing culture,
deaf culture has its own revered poets,
talented singers,
enchanting storytellers,
all exquisitely signing.
And these aren't just translations
from the spoken world.
Many expressions are unique
to the language and the culture,
capturing things in a way
that could never be expressed in speech.
From this perspective,
it makes sense why many deaf people
don't want to give up their language,
because it means they may also sacrifice their culture.
For these people,
both are part and parcel
of what it means to be deaf.
This shared language and culture
has made many deaf individuals
into who they are
and who they want to be.
I think we all have a lot to learn from this.
Hearing culture produces its own way of thinking,
and it can be hard to get past that.
But knowing what you now know,
I hope you can appreciate
and even celebrate
this wonderful form of linguistic
and cultural diversity.
How is it just how you will have
to learn from this university?
I should have loved to out at this university.
Let's seeいく and virtual
outside of the university.
Thank you,
