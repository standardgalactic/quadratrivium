Hi, and welcome to our lecture on chemical equilibrium.
At this stage in the course, we've mastered all the tools necessary to tackle this crucial concept.
The notion of equilibrium has been discussed before, but in this lecture we'll look at it in greater detail,
and talk about how to determine equilibrium under realistic laboratory conditions.
And as the basis for this discussion, we'll need to learn about a few new thermodynamic variables,
one of which is named after none other than Gibbs himself. I know, it's an emotional moment.
Let's begin by considering a single component system. That is a system with only one type of material.
One of my favorite materials, which you may have already guessed, is water. So let's consider that as the single component system.
Now, think about ice cubes in a beaker closed at the top.
What happens when I start raising the temperature by heating the beaker?
Of course, as we know, at 0 degrees Celsius, the ice begins to melt to form liquid water.
And if I continue heating to above 100 degrees Celsius, then the water will boil to form a vapor phase.
But why does ice become water above 0 degrees and vapor above 100 degrees?
To put this in thermodynamic key terms, the question we need to answer is the following.
What quantity governs the equilibrium of this material at different temperatures?
And in particular, what quantity can we use to determine equilibrium that is actually easy to measure?
This is where a new quantity comes into play. And as you may have guessed, it's the one I just took an emotional moment for.
It is a new thermodynamic variable called the Gibbs free energy, or G.
The definition of G can be written as an equation in terms of other thermodynamic variables.
In particular, G equals the internal energy U minus the temperature times the entropy of the system, T times S, plus the pressure times volume, P times V.
So written out, it looks something like this.
So that's the mathematical definition, but what's it good for?
Well, let me first skip to the punchline, and then we'll unpack it further, and I'll explain why we needed yet another thermodynamic variable in the first place.
So here's where G is important. It's important if we have a closed system.
Remember, this means the system has fixed mass and composition, since no matter can pass through the boundaries.
For such a system, then G tells us when the system is in stable equilibrium.
Namely, this occurs when it has the lowest possible value of the Gibbs free energy.
Let's look at this statement in terms of the equation we have for G.
If a system is always trying to minimize its G, since this is what it means for it to be in equilibrium,
then for different temperatures, the system tries to find the best compromise between a low value for the combined internal energy plus PV term
and a high value for the entropy, remember, related to the number of degrees of freedom for the system.
This balance is simply due to the plus and minus signs in the definition.
You can see from the equation that at higher temperatures, the effects of increasing entropy will be more and more important,
since a high T will make S have a bigger effect.
When the temperature is very high, the entropy contribution dominates over the contribution from internal energy and PV.
Now, what I'd like to do is a little bit of math to illustrate how G behaves under different conditions.
First, let's look at the derivative, DG.
It's equal to DU plus P times DV plus V times DP minus T times DS minus S times DT.
But remember that wonderful first law?
Well, we can use it here.
It gives us a definition for DU for a reversible process as DU equals TDS minus PDV.
Remember, assuming only mechanical work is being done.
As we've discussed, there are many other types of work that could be important depending on the processes involved.
But for now, as we're just beginning to explore what G is all about, let's keep it simple and include only this one work term.
If we substitute this expression for DU from the first law into the equation for DG,
we see that DG simplifies to V times DP minus S times DT.
Again, for a system where the only work term is from mechanical work.
And from this equation, we can really start to see why G is such a useful variable.
Suppose I were to perform experiments in which the pressure is held constant.
In that case, we could look at the dependence of G on temperature and arrive at the following relationship.
The partial derivative of G with respect to T holding P fixed is simply equal to negative S.
That tells us that in a constant pressure experiment, the free energy depends linearly on temperature
and the slope of that dependence is the entropy of the system.
We can take this analysis back to our water example.
Given what we know about entropy, we know that it will be different for the three different phases of water.
In the solid phase, it's going to have the lowest value since the number of degrees of freedom is the lowest.
While in the liquid phase, it will be higher and in the vapor phase, it will be the highest.
If we construct a plot of the dependence of G on temperature for water, we'd have three lines corresponding to these three phases
and it would look something like this.
You can see that all the lines have a negative slope, which we know must be the case
since from the equation, G goes down with temperature.
Also note that for the solid phase, G has the smallest negative slope while for the vapor phase, it has the largest negative slope.
Because of the differences in entropy, we just discussed.
And finally, you can see that the lines intersect at important points.
In particular, the solid line intersects with the liquid line at zero degrees Celsius.
Notice that for T less than zero, the solid line is lower, but for T greater than zero, the liquid line is lower.
Given what we said about G, namely that it will always be the lowest in equilibrium, we can see how this makes sense.
The equilibrium phase of water is ice below zero degrees, so that curve shows the lowest value.
We still have curves for the other phases, they're just higher in energy and therefore not representative of the equilibrium phases.
We'll see in a later lecture when I cool water below its freezing point, but still manage to keep it as a liquid,
that it's possible to not be in the equilibrium phase.
But, as we've discussed, thermodynamics is about equilibrium.
And you can see that if we have a graph like this, we're able to predict the equilibrium phase of the material.
So, just to finish with this graph, you can see that the curve for the vapor phase is above both other curves until the temperature is greater than 100 degrees Celsius.
Above which point, it represents the dominant phase and therefore must have the lowest G values among the various phases.
And if we go back to the definition for DG, we can see that it can be equally useful for a system held at a constant temperature.
In this case, we would have that the partial derivative of G with respect to pressure with temperature held constant is simply equal to V, the volume of the system.
With this information, we can construct a similar curve for G versus P.
And in this case, for any given pressure, it is the curve with the lowest G that corresponds to the equilibrium phase of the material.
Here's a plot of the free energy versus pressure for two phases of carbon you're probably familiar with, graphite and diamond.
Notice that the two curves cross at a pressure of around 1 giga Pascal.
In case you're used to thinking in units of atmospheres where the air around us represents about one atmosphere, 1 giga Pascal is equivalent to about 10,000 atmospheres.
So, it's a pretty high pressure.
But we can see that above this pressure, diamond is the equilibrium phase.
Well, below this pressure, it's graphite that is more stable.
Now, the diamond some of us have in our jewelry, you may have noticed, is not spontaneously converting into graphite.
Even though we're about 10,000 times lower in pressure than the crossover point in this graph.
But actually, eventually, at one atmosphere of pressure, diamond will in fact convert spontaneously into graphite.
It may take 100,000 years or so, but for these two phases from this graph, we're able to say which one will be most stable, eventually.
In this case, we have barriers that slow the transformation between phases, making it hard to go from one phase to the other.
The Gibbs free energy plots only tell us about equilibrium, not about such barriers.
But remember that in thermodynamics, this is our primary concern, namely the equilibrium properties of materials as opposed to their kinetic properties.
Now, why is it that G is so important?
I already had a whole bunch of other thermodynamic variables that fit nicely into the thermodynamic laws.
Why did I need another one?
Let's go back to the second law of thermodynamics to see why.
As you may remember, the second law can be applied to define the conditions for equilibrium for any arbitrary system.
That's pretty powerful stuff.
But the way that we stated it a few lectures ago, it says that the change in entropy of the universe is always increasing.
That is, for any spontaneous process, it will increase.
Or, if the system has reached equilibrium, then the change in entropy must be zero.
By the way, just to be sure we all remember, a spontaneous process is one that will occur under a given set of conditions without any additional external forces acting on the system.
One of the examples we looked at when we learned the second law was how to predict when equilibrium is reached when two boxes are placed into contact with one another.
We assumed that the system was isolated and at constant volume.
And by doing so, we were able to write the change in entropy of the universe in terms of the change in internal energy of the boxes.
Remember that in isolated systems, the transfer of heat, work, or molecules at the boundaries of the system is not allowed.
In such constant internal energy systems, it's pretty straightforward to directly apply the fundamental equation for the entropy and the second law to calculate equilibrium properties of the system.
But hang on a second. How often do we really have an isolated system?
And how often are we really able to even measure the internal energy?
And for that matter, how hard is it to really keep a system at constant volume?
It turns out the answers to these questions are rarely, almost never, and pretty darn hard.
Controlling heat and workflow at the boundaries of a real system is often not a simple task.
And it's also completely unnecessary for most experimental procedures one would be interested in performing.
And that's where the Gibbs free energy comes in.
In most experiments, what we would like to control at the boundaries of our system are things like temperature and pressure.
Imagine we have a solution in a test tube sitting in a beaker of water.
We leave the top of the test tube open, as shown here.
If we keep the water in the beaker at a constant temperature, either by heating or cooling the beaker,
then we can see that whatever is inside the test tube is in a constant temperature.
And if we keep the water in the beaker at a constant temperature either by heating or cooling the beaker,
then we can see that whatever is inside the test tube will also maintain a constant temperature.
And since the test tube is open at the top, it will also have a constant pressure.
This is a very simple and common laboratory condition, perhaps one that many of you may have seen in a high school chemistry class.
And instead of our thermodynamics telling us what happens when the internal energy of the system is kept constant,
these kinds of experiments require us to apply thermodynamics while keeping the temperature and pressure constant.
And to predict how the system changes when various thermodynamic driving forces are applied.
For the case of our test tube, an example of a driving force would be if we wanted to carry out a reaction on the material in the solution.
So we have two new complications here.
If we have our test tube in a heating bath, as shown,
then internal energy in the form of heat is being transferred into and out of the system to maintain a constant temperature.
This means that we can no longer apply the fundamental equation for the entropy to the system alone to determine the equilibrium state,
since the second law only dictates the behavior of the total entropy of the universe.
So how do we get around this problem?
We'd much rather apply thermodynamic equations to the system alone rather than having to understand the thermodynamic behavior of the system and its surroundings.
The solution is to define a fundamental equation designed for the conditions at hand.
The second law defines equilibrium by the change in entropy of the entire universe.
But as I just mentioned, that's an incredible pain when we are only really interested in what's happening in our test tube.
What we can do is to define a new state function that will allow us to apply the second law by looking only at the changes occurring in our system.
So how do we do that?
Well, we need to stop having to worry about the whole universe for starters.
And the way we do that is to make the following assumption.
At some point away from my system, I can draw a boundary that makes the system plus its surroundings inside completely isolated.
That's a pretty good approximation as long as I'm far enough away.
Take the test tube in the heat bath.
Right up close to the test tube, we may feel heat radiating from the test tube to the heat bath or vice versa.
But if I go far enough away within the liquid heat bath, I'm no longer interacting with the test tube or the surroundings of the test tube.
Given that I draw my box far enough away, then I can make it an isolated boundary.
Meaning that anything that goes on inside the box cannot possibly change the entropy outside.
So as you can see here, inside this box, we have the system of interest.
In this case, the test tube plus its immediate surroundings. In this case, the heat bath.
The second law defines all possible processes by the fact that if a process is possible, then the change in the entropy of the universe must be greater than or equal to zero.
But now I can write the change in entropy of the universe as the change in entropy of the system plus the change in entropy of its surroundings.
And since du equals tds minus pdv for a constant temperature, constant pressure process,
we can rewrite the change in entropy as a change in internal energy divided by temperature plus a change in volume times the pressure and divided by the temperature.
So we're left with this equation you see here.
By the way, this equation is called the fundamental equation for the entropy.
And it's one we've seen before in our lecture on the second law.
But now comes the crucial trick. Since we're working inside an isolated boundary,
we know that both the internal energy as well as the volume of the system plus its surroundings must not change.
This means that du of the system must equal negative du of the surroundings for any process and the same for the changes in volume.
Using these relationships, I can substitute into the equation from the second law.
So ds of the universe now can be written as ds of the system minus du of the system divided by temperature minus dv of the system times pressure divided by temperature.
And according to the second law, all of this must be greater than or equal to zero for any possible process.
If I multiply through by negative t, then I finally arrive at the following equation.
Minus t times ds of the system plus du of the system plus pdv of the system must always be greater than or equal to zero for a possible process.
And this is exactly the kind of equation I needed since it's only got variables that depend on the system as opposed to the surroundings or worse, the whole universe.
And also, it's got variables that I can control experimentally.
As I discussed earlier, the Gibbs free energy G is defined as the internal energy plus pressure times volume minus temperature times entropy.
With that definition, this equation becomes simply that dG for constant temperature and pressure must be less than or equal to zero for all possible processes.
This is because when you take the derivative of G to get dG at constant temperature and pressure, you wind up with exactly the expression we derive from our substitutions for entropy above.
And this equation is for practical calculations, perhaps the most important one in all of thermodynamics for predicting the behavior of materials.
In words, the second law demands that the Gibbs free energy must be lowered by the change happening to a system in order for that change to happen spontaneously at constant temperature and pressure.
It also dictates that the Gibbs free energy at constant temperature and pressure is a minimum at equilibrium.
But what is the Gibbs free energy really?
Basically, if you look at the definition for it, what we've done is taken the total internal energy and subtracted off the thermal energy and the compressive energy.
So what's left?
The answer is that what remains is the energy arising from other kinds of work.
So far, we've mostly focused on two sources of internal energy, compressive work from an applied pressure and thermal energy due to heat transfer.
However, as I've mentioned, there are many, many different types of work, such as the work of polarization or chemical work or magnetic work and so on.
In many practical problems in science and engineering, it is these other kinds of work in which we're most interested.
So the Gibbs free energy is literally the amount of energy that is free to do these other types of work.
That's why it's called the free energy.
Okay, hang on a minute.
Are we all feeling our oneness with this stuff?
Let's do a quick recap to be sure we've got this.
Point number one.
Thermodynamics deals with energy like properties such as the internal energy, U, which are closely connected to the first and second laws.
Point number two.
These two laws can be combined to give an equation that can then be used to show the conditions under which U is an equilibrium criterion.
Point number three.
The constraints that are required to make it a convenient criterion are not ones that are typically encountered in real laboratory conditions.
For example, we rarely have an isolated system and it's hard to measure directly the internal energy of a system.
So point number four, by creating a new variable that gives free energy and examining its differential form and still using the combined statement of the first and second laws,
we showed that G is a useful equilibrium criterion for systems in typical laboratory conditions like constant temperature and pressure.
Now, before I move on to the last part of this lecture, I want to introduce another thermodynamic variable.
I know, I know, it seems like I'm always introducing variables.
I just can't help myself.
But first, it's good to know about these different variables because they're really important.
And second, as we've discussed in thermodynamics, we often rewrite one variable in terms of other ones in order to more easily solve the problem at hand.
The new variable I want to describe is called enthalpy.
And it's equal to the internal energy U plus the pressure times the volume V.
This is a new energy state variable since it only depends on other state variables.
And we like to give it the letter H.
As with the free energy, enthalpy is not a new independent variable, but rather it's a function of other state variables we've already discussed.
In this case, the internal energy, the pressure, and the volume.
The enthalpy is useful when discussing constant pressure processes.
Since in that case, the change in enthalpy, or DH, is related to the heat of the system if only mechanical work is involved.
Let's take a look at why that is the case.
If I write down that equation from the first law that we know and love, then I get the fact that any change in internal energy of a system is equal to the heat added to the system plus the work done on the system, or du equals squiggly dq plus squiggly dw.
Now, remember that U being a state variable means that its value for a system under a given conditions cannot depend on the path taken to get to those conditions, only on the state of the system at that point.
Also remember that heat and work are both not state variables, although as we learned in lecture 6, the addition of the two is a state variable, is a state function.
But what if I wanted the heat content for a process to be a state variable?
We know that q cannot be a state variable.
However, suppose that the only work that could be done in a process were the work of expansion, or pressure times change in volume.
If that's the case, then the first law expression becomes du equals squiggly dq minus p times dv.
Remember, our sign convention is that work done on the system is positive, and so we have the negative sign in front of the pv here.
Now, let's bring the pdv over to the other side, so we have du plus pdv equals squiggly dq, for a process where the only kind of work is that of expansion, or compression for that matter, just depending on which way the volume change goes.
And this is where enthalpy enters in. Since h equals u plus p times v, that means that dh equals du plus d of pv.
And when we carry the differential through, we have dh equals du plus dp times v plus dv times p.
And now we can see that if we go back to our first law expression where the only type of work is that of expansion, or compression, then we have du plus pdv equals squiggly dq.
But I can substitute dh in there, and I get dh minus dp times v equals squiggly dq.
And here's the punchline. If the process is a constant pressure process, then that dp term is zero, and dh simply equals squiggly dq.
So for a constant pressure process, the change in enthalpy is exactly equal to the heat transfer involved in the process, and it's a state variable.
That means that unlike the general variable q, which is path dependent, the enthalpy can mean the same thing, but is path independent, as long as pressure is help-fixed.
Unlike the internal energy, it's hard to give a simple, intuitive physical description of enthalpy.
As I just described, a change in enthalpy is the heat gain or loss for a constant pressure process.
And it turns the variable for heat energy q into a state variable, since enthalpy is a state function.
This is why enthalpy is also referred to as the heat content.
And how common is it for pressure to be help-fixed?
Well, I certainly hope by now from this lecture that you're feeling like it's a pretty common thing, since that was also an important part of our discussion of the Gibbs free energy.
As with the Gibbs free energy, the enthalpy is useful because of the fact that volume is not the most convenient variable to hold constant in an experiment.
It's much easier to control the pressure, especially if the system is a solid or a liquid.
That's of course not true all the time.
Just think of combustion engines where the pressure is changing.
But let's take the case of a beaker filled with some chemical open at the top.
If I add another chemical to this beaker to initiate a reaction, then as long as the top of the beaker remains open, the whole reaction is going to take place at constant pressure.
One atmosphere of pressure if we're at sea level and the top is open to the air.
That's a pretty common situation, both in laboratory experiments as well as in nature in general.
So indeed the thermodynamics of constant pressure environments is extremely important.
And enthalpy is a very useful variable.
You can think of it as being a thermodynamic variable that contains all the information that is contained in the internal energy, but can be controlled by controlling the pressure.
And notice the connection to the Gibbs free energy.
If we look at the expression for G, we see that we can rewrite it as H minus T times S.
The Gibbs free energy is a state function useful for determining equilibrium in the most common type of experimental system.
One where pressure and temperature are maintained at a constant value.
And as we just learned, a system always tries to make it G as low as possible.
You can see from this equation that what this means is that it will always be a compromise between trying to have a low enthalpy and a high entropy
with temperature playing an important role in determining which of these is more dominant.
So to wrap up this lecture, I'd like to return to the point of what is free in the free energy in the first place.
Let's take as an analogy a ball rolling around on a track.
If the track has a part up high that is curved a little so the ball can get stuck up there, then we'd call that a metastable equilibrium for the ball.
But it means that the ball is capable of doing work as it rolls down to the lower point on the track.
Well, at least once it's pushed over the barrier to get it unstuck from that point higher up.
The maximum work that the ball can do is exactly equal to the minimum amount of work it would take to push the ball back up to its higher metastable point.
The Gibbs free energy is similar conceptually.
It's equal to the maximum amount of useful work that a system can do as it changes from one state to another more energetically favorable state.
Remember from our lecture on the second law that the missing ingredient to understanding why a given process goes in one way and not another is entropy.
Entropy and the second law give us the arrow of time since entropy always increases in spontaneous processes in isolated systems.
But a parameter that's useful only in isolated systems is not of much practical use.
So today we defined another state variable the Gibbs free energy that always decreases for a spontaneous process where the system is held at constant temperature and pressure.
Mathematically, entropy and the Gibbs free energy are two sides of the same coin.
One implies the other.
But for solving problems of practical interest, G is the parameter we have been looking for.
