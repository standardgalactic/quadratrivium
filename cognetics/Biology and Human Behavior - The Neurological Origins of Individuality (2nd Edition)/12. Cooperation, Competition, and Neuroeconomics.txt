Lecture 12, Cooperation, Competition and Neuroeconomics.
Welcome back and all things considered, I'm shocked that you're back again to hear more
of this after how depressing the last lecture was.
In fact, the only way I can still be here is I just took a very warm, soothing bubble
bath and I assume you took the same last lecture was incredibly depressing.
All it did was tell you that the modern viewpoint of evolutionary biology just selects for competition
and national inquire headlines of fruit fly, poisons, lover and is nature just bloody in
tooth and claw.
And where we pick up with today is the fact that that's absolutely not the case.
The same principles of evolutionary biology, which in some circumstances select for the
vicious logic of competition and aggression in other settings selects for the equally
inevitable, equally sort of evolutionarily logical aspects of competition.
I'm sorry, of cooperation.
And what you see in those cases is that it makes just as much sense.
Okay, back to two lectures ago, our introduction to the building blocks of this whole field,
individual selection, kin selection and our big third one was the notion of reciprocal
altruism.
Many hands make the task light, cooperation makes things better in lots of settings and
suddenly you have a world in which, in particular circumstances, it makes sense to cooperate
with another individual, not out of kin selection.
This could even be an individual not related to you, but in certain circumstances, cooperation
is an evolutionarily desirable thing.
It now helps everybody to be cooperating.
And what you then have is the issue of when do you get cooperation, when does cooperation
evolve?
Simply because there's great advantages to it.
But what we also saw briefly touched on two lectures ago is even better than cooperating
with the other individual is having them cooperate while you cheat against them.
And thus a selection for when do you cooperate, when do you cheat, when do you watch out particularly
for the other individual cheating.
And what we also heard were some of the features that predict in what species will you see
reciprocal altruistic systems.
You've got to be smart, you've got to be social, you've got to have stable enough social groups
that you could actually interact with the same individual over and over.
OK, those are logical requirements.
But then you get into this much more subtle realm, given that you're in a species where
cooperation makes sense now and then, this very straightforward, horribly difficult
to answer evolutionary question, when is it adaptive to cooperate?
And when is it adaptive to cheat?
And here's where a quarter century ago, evolutionary biologists suddenly let into their fold the
most unlikely people on earth, economists and mathematicians.
Because what these people have often thought about within an economic realm is when is
it adaptive, when is it financially to your advantages to cooperate versus not cooperate.
It turned out there was a whole realm derived by mathematicians beginning in the 1950s looking
at this very complex math of when you cooperate and when you don't.
And this is a whole field called game theory.
One of the main figures in it straight out of a beautiful mind in the movies was John
Nash, the economist.
This was a whole realm of economics and suddenly it became clear to both the economists and
the evolutionary biologists that this was a realm ripe for the same concepts of game
theory applied to these issues of evolution of sociology.
When does it make sense to cooperate and when not?
And very quickly the poster child for game theory applications to evolutionary biology
became this one game.
And again the word game is being used in this sense in a very precise mathematical sort
of setting.
And this game is called the prisoner's dilemma.
You cannot get three steps into the world of game theory without stumbling into the
prisoner's dilemma.
This is the set piece of the whole field.
Okay, here's the scenario.
Imaginary scenario, there's two prisoners and they're attempting to escape and they
both get caught and each one is being interrogated separately and it's not clear whether you
should hold out and not tattle on the other person or if they're doing that already to
you and what the punishments are as when in that setting do you cooperate and when you
don't.
And formally the way this is described in a chart here is you're looking at the two
individuals person A and person B and each has the issue do they cooperate or do they
defect and those are the terms used cooperate versus defection and the way the game is set
up there's very clear outcomes.
Suppose both of you cooperate, you get three arbitrary brownie points in the math of this
and the other individual gets three as well.
That's the payoff for cooperation.
In contrast, you have a setting where the other individual cooperates and you cheat.
You tattle on them, what's the outcome?
You get five points and they get zero.
Conversely in a setting where you cooperate and they cheat, you get zero points and they
get five or finally if you both cheat on each other, you each only get one point.
So suddenly you have this realm where a pretty good outcome is to both cooperate, a fabulous
outcome is for the other person to cooperate and you to cheat and the really depressing
outcome is the reverse and nonetheless both of you cheating against each other is not
a great outcome either.
And thus you get this math of when do you cooperate and when you don't.
And onto the scene around 1980 stepped this economist, the man named Robert Axelrod, who
revolutionized the field with a very, very simple thing that he did.
He set up a computer program that essentially did the prisoner's dilemma and could do it
over and over and over again.
And he wrote to all sorts of his buddies, his buddies, some of whom were economists or political
scientists or mathematicians or sociologists and to each one of them he explained the prisoner's
dilemma game and said, do me a favor, write back and tell me how you would play the game.
What would be your strategy of when you cooperate and when you and some people sent back strategies
where they always cheat or they always cooperate or they do X number of times this or they
could do contingent programs if the person does this, my opponent, then I do that, otherwise
I do this.
He programmed each of these strategies, plunked them into the computer and ran them against
each other for a gazillion generations and the use of this term here as we will see is
probably not a random metaphor but begins to describe something about evolution, ran
these different programs against each other for lots and lots of computer generations
and over and over out popped this one strategy which out competed the others.
And this was one supplied by a mathematician named Anatoly Rappaport who's become like
this legendary figure in the field for having tossed in this program and you would think,
OK, what's the winning one going to be?
Some incredibly complex program and of course as the made for TV movie is supposed to go
at this point instead it turns out to be this wondrously simple strategy which is what was
called tit for tat.
You start off cooperating, you start off taking the program of the stance of not cheating
and the simple rule is as long as your opponent keeps cooperating, you keep cooperating.
If they cheat against you once, the next round you cheat back.
If they continue to cheat, you continue to cheat against them but if they go back to
cooperating, you do as well.
Tit for tat, whatever they did in the previous round, you do in the next one and it turned
out tit for tat out competed all the other strategies.
To use the evolutionary metaphor here which is not irrelevant, tit for tat drove all the
other strategies into extinction.
It out competed all of them and this was this landmark observation.
Number one, you can use a computer to come up with a mathematically optimal behavior
and the fact that on a certain level, the tit for tat strategy is kind of nice.
It's nice, it's cooperative, nonetheless it punishes you.
If you mess up, if you cheat against it, amid punishing you, it's also forgiving.
You go back to cooperating and it does it well and it's perfectly straightforward.
What we saw was there's a bunch of species whose patterns of social cooperation of reciprocal
altruism match in lots of ways a tit for tat strategy.
This was the ideal strategy in these games.
What people soon realized was there's a vulnerability in the tit for tat strategy.
You're sitting there and saying, well, why should tit for tat win?
If tit for tat plays against someone who cheats all the time, the very first round, that individual
gets more points than you, then they cheat for the rest of the time and as a result, you
cheat for the rest of the time.
Every other round equal outcomes, tit for tat is always going to lose against a strategy
that cheats all the time.
Why should tit for tat come out ahead?
The reason why is every time the cheaters interact with each other, they get hardly any points.
Every time tit for tat interacts with another tit for tat, they start getting tons of points.
And a jargon that's always used in the field to describe this is tit for tat always loses
the battle but wins the war.
In the long run, the cooperative strategy comes out as long as tit for tat can find somebody
else who cooperates.
And we'll see just how non-trivial that issue is.
The individual interaction may be disadvantageous, but in the long run, the cooperators outcompete
the cheaters with each other.
But there is this one drawback, which is what if there's a mistake?
What if there's a signal error?
What if the communication between the two sides is not so hot?
What if somebody playing tit for tat, inadvertently giving off a cooperation signal, has somehow
something got messed up in the wires and instead the opponent believes they just cheated?
What if there was a signal error?
That individual following the tit for tat strategy will cheat the next time, at which
point you say, what's the deal?
We have this great thing going and you cheat back and you tit for tat back and forth for
the rest of time.
The tit for tat strategy is vulnerable to a signal error.
And that is exactly what was encompassed in some thriller in the early 60s that I remember
reading as a kid, doomsday scenarios of the United States against the Soviet Union where
there was a mistake, somebody's little fuse burned out or who knows what, and one side
launched a nuclear attack on the other by accident.
By accident we didn't mean to a signal error and at the end of the book the only logical
tit for tat response was to have to do it back to them.
Tit for tat strategies are vulnerable to mistakes in the system.
And thus what you get is a derived strategy which turns out to out compete tit for tat
as soon as you introduce the possibility of a mistake in the system and this one is called
forgiving tit for tat.
You go a certain number of rounds of this horrible back and forth seesawing of attacking
each other and just because of the possibility of there was a mistake the rule with that
one is after a certain number of rounds you forgive them and that can reestablish cooperation.
And depending on the circumstance how likely an error is to occur you can have different
strategies of how many rounds you need to go before forgiveness.
Except there's a vulnerability to forgiving tit for tat which is if the other side has
evolved a defense against that strategy they can exploit you like crazy.
They can exploit your mathematical tendency to forgive and thus what you see most adaptively
is to start off with a pure tit for tat strategy and only after umpteen rounds of the opponent
proving that you can trust them then you switch over to a forgiving tit for tat.
What is this?
This is you start dealing with somebody a new person you're doing business with or
whatever and you've got to go a certain number of rounds before you begin to give them the
benefit of the doubt.
This is a mathematical variant on this.
Okay this is all great but the big challenge in the field is what I blithely passed over
before tit for tat loses any interaction with the cheater but as long as tit for tat cooperates
with other tit for tat they're going to lose individual battles but where's the other tit
for tat player coming from?
You start off with a world in which everybody is a cheater and there is no reciprocal altruism
and if you could only get a bunch of cooperators cooperating with each other you're going
to out compete with those folks but what is clear is there's no mechanism to start it
off with.
Whoever is the first one who says I am the social slime mold and I've read all about
Robert Axelrod and we need a world in which we cooperate and I'm going to make the first
move they are one step behind in this game for the rest of the time and all the other
slime molds snicker derisively at them the big challenge in the field is understanding
what are the circumstances that jump start cooperation.
Once it's established that's great but what is it that gets it going and that has been
a huge issue in the field ever since trying to understand the special circumstances that
select for cooperation to begin to emerge.
Now one of the models for this is as follows and this is one that is very familiar to evolutionary
biologists you get some population and as shown in the chart here it's some big old
population of nothing but defectors there is no reciprocal altruism yet.
And then you get one of the things that happens often over the course of time in some species
some land bridge comes up or some mountain collapses or something floods or whatever you
get a small subset of the population isolated by itself isolated from the rest of the general
population and the term that you have here is this is a biogeographic island you've
got this little subset of population there you have what is called a founder population
some barrier has come up and they're separated.
Now what happens to a little population off by itself it gets a little bit inbred inbreeding
goes on and the second you start having some degree of inbreeding the second these guys
in this isolated population tend to be more related to each other than any given two individuals
in the main population suddenly we are transitioning back to the world of kin selection suddenly
to the world where you could begin to get cooperation emerging not out of this abstract
tit for tat mathematics of dealing with perfect strangers but you begin to get cooperation
among relatives you begin to select for that sort of thing.
And then as is often the case over time the barrier comes down the migratory herd of whatever
that cut you off from whoever for the last 30 generations whatever it is that happens
the founder population rejoins the general one and you've got this little nucleus of
cooperators and what we saw from the math of tit for tat forgiving tit for tat any variant
on that is they are out going to compete all the defectors surrounding them and over time
you are now selecting for the emergence of cooperation so that's one model where this
could come from a founder population. Other circumstances now the way prisoners dilemma
was set up initially was you had only one interaction you had only one round of strategy
against another individual that's not how the social world works you're some elephant
and you're going to be hanging out with other elephants for three quarters of a century
you're going to be having repeated interactions and this is the reality of any sort of cohesive
social group what happens when you begin to have these games played out not over one round
but repeated rounds with the same individual in many cases and what you simply begin to
get is the possibility of somebody getting back at you if you're a cheater they're going
to be far less cooperative the next round you begin to have and the term in the field
is a shadow of the future beginning to think okay I need to be a little bit more reasonable
of a game player here because I'm going to be seeing this individual again in the next
round and it really you know there's the short term advantage to cheating right now but in
the long run it's going to be better to cooperate as soon as you set up these games so that
there's multiple rounds with the same individual you begin to select for cooperation. Next variable
suppose you're only playing one round with this individual and then you go on to somebody
else but now what you've got is the next somebody when they sit down to play you they can look
up what was your strategy in previous rounds does this person tend to cheat or does this
person tend to cooperate and what we've now introduced is the very technical mathematical
concept of you come in there with a reputation and as soon as you design games so that you
enter with a reputation you select for cooperation. Next variable the very few species sit there
and only have one type of interaction with a number of the member of their social group
in other words only a single type of game theory game going on instead the reality is
there's multiple games do you hunt cooperatively do you defend cooperatively things of that
sort the second you introduce the possibility of multiple games things become more complex.
Okay suppose here in this diagram we have a first example we've got game A some sort
of prisoners dilemma game whatever and it just happens that this is in a population where
there's little reason to select for cooperation and there's nothing but defect there's no
reciprocal altruism. Now you have two games going on you switch between game A and game
B suppose game A continues to present no incentive for cooperation whereas game B presents an
enormous incentive and you switch back and forth between the two defection nothing but
defection and game A suddenly switching to cooperation and game B back and forth back
and forth what you wind up seeing is you begin to get in a sense trickle over from game B
into game A as time goes on the level of cooperation begins to rise in game A and what you wind
up ultimately getting is you fixate you reach an equilibrium state of cooperation both of
the games what you have in these mathematical models and out in the real world as well when
you're dealing with multiple games if there's one that very dominantly establishes a reciprocal
trusting relationship what you begin to select for is the same in other circumstances. Other
things that select for cooperation in this mathematical world which makes sense the second
you begin to think about in terms of real life is if you have the opportunity to punish
somebody who cheats to be willing to give up a couple of points of your own to take
points away from them and what's the everyday equivalent of it we are willing to expend
some of our points tax dollars to fund things like police forces criminal justice systems
things of that sort the notion of punishment for cheating in a game theory setting selects
for cooperation where you could punish the individual who's played against you shadow
of the future or you can do what's caused altruistic punishment you punish somebody
else in another game that is cheating in that setting the second you introduce that
you select dramatically for what you get there lots of cooperation you can take this one
step further and around familiar to people who grew up and I don't know military academies
or who knows what you introduce what's called secondary punishment not only can you punish
somebody who cheats but you could punish somebody who fails to punish a cheater you can get
somebody in trouble not only for an honor code violation in your military academy but
for failing to report somebody else for an and that selects like crazy what the studies
also show is another bit of mathematics that makes sense in real life you select for cooperation
real quickly when you begin to get to choose who you play with out of a pool of potential
partners aha and as soon as you have the option of whether or not to even play and look at
these traits what winds up selecting for cooperation multiple rounds of interactions
with the same individual open book play is the term that establishes a reputation multiple
games the possibility of punishing anyone who violates what is this a picture of this
is traditional human society this is a group of sixty hunter gatherers who've known each
other for years this is exactly the circumstances that humans have spent ninety nine percent
of their evolutionary history in small groups that know each other and look at how more and
more of us are living instead large anonymous urban populations and those are precisely
the settings that select against reciprocal altruism now this whole approach to game
theory is far more complex than that and there's other games out there is not just the prisoners
dilemma and they have all sorts of wonderful colorful terms chicken game which is the other
person is driving the car real fast at you and who's going to swear first and there's
a whole mathematics with a different payoff structure and in prisoners dilemma the ultimatum
game battle of the sexes all of these are just mathematical versions of different relationships
between cooperation and defection and each case there are patterns of optimal outcomes
and what you have running underneath the surface here is something a truism that is run through
branches of economics for a long time which is humans are perfectly rational economic
machines and animals can't possibly do that and what we see here is thanks to once again
the wind tunnel of evolutionary selection sculpting optimal patterns you can get the
same rational outcomes of social behavior and other species as well selected for what
we'll see in a couple of minutes is we and the other animal species are not so rational
in our reciprocal altruistic interactions and where people have gotten insight into
that is what is the trendiest hottest new version of thinking about the evolution of
reciprocity what is now called neuro economics which is the whole notion of studying the
brain brain imaging of people while they are going through the process of playing one of
these game theory games putting somebody in a brain imager and there will be some little
computer console where they can press the cooperate button or the defect button and
imaging what parts of the brain are having increases in metabolism while they are playing.
One of the first versions of this a wondrous study looked at people playing the grandparent
of all game theory games praising and playing prisoners dilemma and looking at metabolic
activity excitement activation in different parts of the brain and what they discovered
was something real interesting remember back a gazillion lectures ago dopamine a neurotransmitter
of pleasure of reward of anticipation when do those dopamine pathways activate by all
logic of humans as mindless gleaming machines of rational economic choice when should those
dopamine pleasure pathways light up when the opponent has cooperated and you've cheated
against them you've gotten five points they've gotten zero that's not when the dopamine pathways
lit up in this study they became their most active when both of you were cooperating isn't
that interesting this part of the brain doing something wildly economically irrational
getting most pleased in a metaphorical sense with cooperation not with cheating isn't
that interesting.
Another version of this another one that comes out is looking at the brain basis of resisting
temptation how can we frame this temptation what we saw in prisoners dilemma is there's
this temptation to cheat right now in the long run it's not worth it but right now it's
this temptation what's the brain basis of resisting temptation and here's sort of a
classic economic scenario what you've got are two choices work versus reward you can
do one unit of work and get one unit of reward or a little bit later if you're willing to
do two units of work you get three units of reward a half you're only disciplined enough
to hold out and do the harder thing do the two units of work you're going to get a greater
reward in the long run when is the brain least able to resist the temptation of going for
the cheap fast payoff suppose the cheap payoff comes four weeks from now and the more disciplined
higher payoff comes four and a half weeks from now anybody can resist the temptation
there and hold out for the tougher harder but better choice and what you see is the frontal
cortex that area of the brain that does this gratification postponement stuff doesn't have
to get very active to make that choice now make things a little bit harder now the tempter
occurs a week from now and the more disciplined big payoff is four weeks from now in order
to resist the tempter you see the frontal cortex is a little more activated now the devil
is dangling right in front of you the cheap payoff two minutes from now versus the more
disciplined outcome four years from now in order to resist the tempter what you see in
a study like this is massive activation of the frontal cortex don't do it don't do it
hold out hold out beginning to see the brain basis of don't stab the partner right in the
back there in this game theory game here hold out it pays off in the long run a brain basis
of it now all of this is built around again that model of we are rational economic machines
we've already seen one way in which is not cooperation activates more of those pleasure
pathways and cheating against the cooperating opponent here's a fascinating study a few
years ago showing not only how irrational we can be but how emotional setting can change
the rationality of our choices okay here's a classic philosophy problem you've got some
trolley car that's rolling out of control that's break is failed it's barreling down
the tracks and is about to hit and kill five people you've got a choice you can leap over
and pull a lever and by pulling the lever this diverts the trolley onto a different track
where it kills one person you pull the lever in order to save five lives you sacrifice
that one person second scenario the trolley is barreling down out of control about to kill
those five people and there's some beefy guy standing right in front of you and with your
own hands you can push him on to the track he'll get killed but it'll stop the trolley
do you do it to save five people you sacrifice one in a formal mathematical sense these are
absolutely equivalent choices do you kill one in order to save five in the first case
you do nothing more emotionally soiling then pull a lever in the second case you use your
own hands to push the person onto the track and even though from an economic standpoint
these are absolutely equivalent people are like three times more likely to choose to
pull the lever than to push with their own hands the flatter case is just too emotionally
salient is your own hands that's pushing the person to death what you see there of great
relevance pushing a button and dropping some bomb from 30,000 feet up in the air is a lot
easier to do than using your own bloodied hands about a three-fold difference in which
version people are willing to do so in this wonderful study they went and brain imaged
people when they were considering the two different versions of the trolley scenario
and what you see is depending on how it's framed do you save five by sacrificing one
by pulling a lever or do you do it by pushing with your own hands when the person is contemplating
the pulling the lever scenario what metabolically lights up their cortex and when instead they're
contemplating do I push someone to their death with my own hands what part of the brain lights
up the limbic system in other words depending on how this mathematically equivalent scenario
is presented to you depending on how much it's dripping marinating an emotion you engage
completely different parts of the brain and what that winds up telling you is depending
on the emotional content of things you're using very different parts of the brain to
decide what's the appropriate optimal thing to do so what we see here in this ending is
this is a realm where there's not only not a whole lot of rationality and behavior but
it's even irrational in so far as you're using different parts of the brain to decide what
to do now what all of these lectures have been about these last three is looking at
the evolution of behavior and implicit in that is the evolution of genes that do something
in the nervous system that has something to do with behavior and it's right at this point
that people often get conniptions saying all this theorizing about the evolution of genes
having something to do show me the genes what do genes actually have to do with behavior
and that's now going to be the subject of our next few lectures.
