Lecture 20 Sound Waves and Light Waves
When I think of waves, I tend to think of the ocean.
I visualize the waves in the ocean.
Maybe since I'm a physicist, I might think about a slinky
where you're jiggling one end and you can see these beautiful little waves
traveling down the slinky.
But today, I want to think about waves where it's much harder to see
with the plain eye that what you have is a wave in front of you.
I'm thinking specifically about sound waves and light waves
where you just have to argue scientifically that it's a wave
without being able to see the medium moving.
So we're really going to be addressing the question,
what do you mean when you say sound is a wave or light is a wave?
It's an interesting question.
So we're going to be sort of looking at this argument that you might have
between the wave point of view and a particle point of view.
So let's start off with sound waves and let's think first
on a model in which we have waves propagating in a medium.
What would that medium be?
If I'm speaking to you, then the medium would have to be the air.
And I'm thinking now about the particles of air,
the molecules of air as little super balls.
They're flying around and they travel in straight lines.
They could bounce off of each other.
They could bounce off of the walls.
But that's the way I'm going to visualize the room that I'm in.
And so what would the wave be?
Let's think about an example where we make a nice clear sound.
I clap my hands and what have I done?
I'm now visualizing the air as filled with little, little super balls,
little invisible super balls.
And I've squeezed them.
I've compressed them.
I created a region of high pressure momentarily.
And now think about the consequences.
If you've got a bunch of super balls squeezed together,
they're high pressure.
They're going to push on their neighbors.
They're going to bounce against their neighbors,
making their neighbors go into a little high pressure region.
And then they push on their neighbors
who get squeezed into a high pressure region and so on.
So you push your neighbor who pushes their neighbor
who pushes their neighbor.
That's exactly what you need for some sort of propagating wave.
In this case, the wave is a pressure wave.
It's an oscillation or disturbance of the pressure of the air itself,
which is spreading outwards.
And the individual air molecules bump into their neighbor
and then bounce back again.
So it's not that when I clap my hands,
some air moves from me to you.
That's not how it's working.
It's this disturbance that's traveling from me to you.
So that is my wave model of sound.
The alternative, I suppose, would be a particle model
in which when I clap my hands,
I am spewing out little, whatever it might be,
little particles of sound that go in every direction
and ultimately reach your ear.
And so we're trying to ask, how would we decide
which of these two different models of the world,
the wave model or a particle model for sound, is better?
So when I think about this, I ask myself,
what kinds of experiments could I do to decide?
One thing I might think about is the speed of travel.
So you could measure the speed of sound.
You have to think a little bit about how you would do this.
You could go somewhere where there's a good clear echo.
You clap your hands, the sound travels out,
it bounces off the wall, it comes back again,
and you could measure the amount of time it takes
for the sound to go out and back,
so you know distance and time,
and that would determine the speed.
But that's not really going to decide between a sound model
and a wave model.
In both of those schemes, which seem somehow very different
from one another, you could still imagine
that there would be a finite speed of sound.
The little particles of sound have some speed,
or the waves travel with some speed.
You know, if you go to an outdoor concert,
you will notice that even though you're very far away,
and there might be a noticeable lag.
You see the drummer hit the drum,
and then a fraction of a second later you hear the drum,
so you're certainly far enough away
that the speed of sound might matter.
You will notice that the high sounds and the low sounds
come at the same time to you.
The music still sounds normal,
and so that indicates that high frequencies
and low frequencies are coming towards you
with the same speed.
That makes you think maybe it's a wave,
because we've seen with waves
that high frequency jiggles travel outwards
and low frequency jiggles travel outwards at the same speed.
For light, for electromagnetic waves,
all frequencies travel at exactly the same speed.
Whereas for particles,
if you were thinking about little sound particles,
you might imagine that the high frequencies
could perhaps correspond to higher energy.
Higher energy would probably mean higher speed or velocity,
and so that seems like a possible inconsistency,
but it's not a proof,
and you're hard-pressed to just based on this one argument
decide whether sound is a wave or sound is a bunch of particles.
The game that we're playing today
is to try to fight it out between these two different models.
Let's think about experiments that we could do.
This is really the game of science.
You've got some thing that you're interested in,
some question that you're puzzling over,
and you create multiple hypotheses.
Historically, there were many, many more than just these two.
Once you've got a wave model or a particle model,
you can start to think about how to build that model
in many different ways,
and there might be completely different ways
of imagining what sound is and how it works.
And so what we're really doing is following consequences,
thinking about experiments and thinking about
what would validate or invalidate the hypotheses.
You might also wonder why I would care about this argument.
Who cares whether sound is a wave or sound is a particle?
And it goes beyond just curiosity, our search for the truth.
We have this belief as classical physicists that there is a truth,
a reality out there that we are investigating, we're trying to make sense of.
But it's also a practical thing.
If sound is particles, I'm going to bottle them and sell them.
You can open up a little bottle and hear an orchestra.
That would be cool.
If sound is a wave, then I could take advantage of the fact
that I know, for instance, that waves can interfere and cancel one another,
and I could make a lot of money by building a sound-canceling headphone.
Well, somebody's done that, so you have a good clue
about which is the correct or physically realistic model of sound.
But I want to think through the argument, because people didn't always know.
If sound is a wave, it should be wavy.
Think about how waves behave.
If you look at water waves, when they start at a point,
they tend to spread out in all directions, like dropping the pebble in the pond.
So if sound was a wave, you would expect that it should, for instance,
bend around corners, go through small cracks, and then bend and go in all directions.
And it does precisely do that.
If you're in your house and the door is closed,
and you can barely hear the person in the next room over,
and then they open the door, all of a sudden you can hear them much more clearly.
So what's the physics? How do you understand that?
Well, you can't necessarily see these people and the sounds that they're making,
because they might not be straight through the doorway.
They might not be in a line of sight,
but the waves go through the doorway,
and the doorway acts like a small source of sound,
and the sound spreads out in all directions.
So sound can bend around corners.
That seems like a fairly convincing proof that sound is a wave.
But let me now defend the particle point of view.
Let me try to stick it out.
Scientists do this all the time.
You've got your favorite theory, and people are arguing against you,
and they say, well, maybe sound is still a bunch of particles spewing out,
but they're just bouncing all around.
They're bouncing off the walls, they're bouncing off the door,
they're bouncing off of the air,
and so they sort of make it to your ear in a more complicated path.
So that could explain how sound can go around corners,
and yet still be little sound particles traveling.
Now we could pursue this.
You could imagine making careful measurements of the amount of time it takes
to make a sharp sound to make it through a doorway.
Let me leave that one.
Let me think about other experiments that we might do.
So we just sort of abandon that and say, all right,
let me set up a little microphone.
A microphone is nothing more than a little flap.
It's a little flap of paper or plastic or material,
and when sound waves hit it,
the sound wave in my wave model is alternating high pressure, low pressure,
high pressure, low pressure.
That's the picture that we have in this model.
And if you look at the output of the microphone,
microphones just convert the motion of the little flap into a voltage,
and you can look at the voltage on a screen,
for instance, within a oscilloscope.
It's a device that simply graphs voltage as a function of time,
and you can see a beautiful sinusoidal wave pattern
when a steady tone is reaching that microphone.
So you say, look, it's evidence.
It's proof that sound is a wave because I see a wave on the oscilloscope screen.
And that's pretty compelling until the particle believer says,
well, okay, I've got an alternative idea.
Maybe those little sound particles are hitting the flap,
and then the flap starts to wiggle.
It's like striking a bell with a hard object.
The bell will ring, and then if you're monitoring the bell,
you might falsely conclude that the original source was waved like.
This is an important point, and it happens all the time
when you're having scientific debates,
is that you have to be careful that what you are calling
direct evidence of the thing that you're investigating
might be instead the apparatus that you're using
to detect the thing that you're interested in.
You've got to be careful that you can separate the sound wave
from the waving motion of the microphone.
This is always true in physics. You have to be careful.
And there, again, you could make some arguments and say,
well, look, if it's like a bell, bells have certain tones that they prefer,
but the microphone doesn't have any preferred tones,
so it doesn't seem to be acting like a bell.
Let me leave these arguments and go to one more,
and let's go straight for the convincing argument,
because last time we said, if you've got waves, the real clincher,
the thing that shows you that you've got waves and not particles
is what we called interference, where two waves pass through one another,
and when two waves pass through one another, they interfere,
and that word means that if the two waves are both up waves,
then you get a big up result,
but if one wave is up and the other wave is down,
they tend to cancel, and you get no motion at that spot
and that time at all.
So how could we observe this effect?
This would be a very, very clear evidence that sound is a wave
if we see interference of two different sound waves,
and the simplest experiment that I can think of to do that
would be to set up two speakers.
So we're going to have one speaker in front and to the left,
another speaker in front and to the right,
and let's stand equally distant from both of them
and we'll drive both speakers with the same steady tone.
So they have the exact same loudness
and the same pitch coming out of them.
If you turn one of them off, you hear a nice loud sound.
If you turn the other one off, you hear a nice loud sound.
What happens when they're both going?
And let me play one little trick to make this especially easy.
I'm going to take the wires going to one of the speakers
and just reverse them.
Just flip the wires.
So think about what happens.
If you watch a speaker and you drive it at low frequency,
you can see the cone of the speaker going in and out,
in and out in a lovely sinusoidal pattern.
Now is that proof that you're making a sinusoidal wave in the air?
Not absolutely.
Maybe the speaker cone is jiggling in and out,
whacking into the air and creating little sound particles
that then fly out.
Maybe they fly out in a high density
and then they sort of stop and then they fly out in a high density.
You could imagine that there would be some sort of wave-like observation
from a wave-like source,
even though the thing that's traveling in between might not be a wave.
So I don't think that observation that the speaker is going in and out
is proof that sound itself is a wave.
But the point of flipping the wires was that when one speaker is going out,
the other is going to be going in.
The two speakers are going, we would say, out of phase with one another
or out of sync and they're perfectly out of phase
when one is out, the other is in and vice versa.
So why would I want to do that?
Well, if I believe in the sound is a wave model,
then I'm going to argue that when the speaker is pushing out,
it's making high pressure.
And when it's pulling back, it's making low pressure.
So I'm visualizing the sound wave coming from one speaker
alternating high pressure, low pressure, high pressure, low pressure,
coming towards me.
It's a traveling wave front,
a disturbance of the medium coming towards me.
If I have a particle view, then it might be loud and soft, loud and soft,
or it might just be a steady stream,
but in any case, there's always particles coming towards me from both speakers.
So if I am a firm believer that sound is particles,
I would argue I really don't care whether the two speakers are
in phase, out of phase, or completely random and disconnected from one another.
If they're both equally loud,
then I would expect to get doubly loud sound at my ears
if sound is a bunch of particles.
But what if sound really is a wave?
Then I've got a wave coming from one speaker,
and that means that the pressure at my ear is going high and then low,
and then high and then low.
And coming from the other speaker,
it's going low and then high, low and high,
and I'm superposing these two waves,
and what happens when you superpose two waves
that are exactly out of sync with one another?
They cancel at all times.
When one is high, the other is low,
plus one and minus one adds to zero.
Then, a half a cycle later,
you've got, instead of high plus low,
you've got low plus high, but you're always canceling,
and you will hear silence.
It's a remarkable thing.
It's a very dramatic demonstration.
There's a loud speaker over here and a loud speaker over there,
and yet right where I'm standing, it's quiet.
And if I move a little bit off to one side,
then I'm no longer equally distant from the two,
and if I move just a little bit,
then one of the speakers will have gone through
a little bit more of a cycle by the time it gets to me,
and the two waves will be back in sync,
and now I've got high from one and high from the other,
and then low and low, high and high, low and low.
I'll hear a nice loud sound.
So picture how dramatic this is.
Two speakers blaring.
There are spots where it's totally dead silent,
and other spots where it's loud, then you keep moving,
they'll get back out of sync again,
and now, at a different place in space,
it will be a steady, quiet tone.
We call those nodes, and audio files are quite aware of this.
They set up their speakers in their room,
and they make sure there's some echoing going on,
and they make sure that the speaker wires
are not reversed on one of the speakers.
If the two speakers are in phase,
you can still find a node,
you just have to shift away from the midpoint,
and that's why a good audio file will sit nice and symmetric
between their well-designed speakers.
I can think of another experiment,
which would also be pretty much of a clincher.
Sound is supposed to be a pressure wave in a medium,
so if you get rid of the medium,
then there's no air molecules left to make a high-pressure region,
and so the sound should go away.
Whereas, if I believe in particles,
if I believe that sound is really the flow of particles,
why particles wouldn't care if they were going through a vacuum?
Because there would be nothing to bump into,
they would just cruise along.
This experiment was done back in the 1600s,
where somebody took a noisy object,
and one day you can put a bell inside of a jar,
and you can see the clapper on the bell clapping,
and you can hear it,
and then you pump the air out of the jar,
and the less air there is, the quieter it gets.
And when you have a vacuum,
when there's no medium, then there's no more sound.
You still see the little clapper clapping,
but you can't hear a thing.
And then you let the air back in again,
the sound comes back,
because you've got some medium for the wave to propagate through.
I can never prove a scientific hypothesis.
I can't prove to you that sound is a wave,
but this kind of argumentation certainly goes a long way.
I've proven from two very clear experiments
that it's not particles.
So we've thrown away a whole class of models,
and I've certainly shown that it's consistent
with sound being a wave.
And I've made many predictions,
and every time I make a prediction and I go out and test it,
it comes true, that's the way nature is,
and so this gives us great confidence in this model
that sound is a wave,
even though we don't really physically see with a microscope
anything going up and down.
You don't see the air pressure, air is invisible,
and yet we know now that sound is a wave.
Let me think now about light.
Light is an even more difficult story,
and it's more difficult for a good reason.
A good reason is the following.
Waves have a wavelength.
Sound waves depends on the pitch that you're listening to,
but sound waves have a kind of a typical wavelength
of a human scale, a few centimeters,
kind of the size of your ear,
probably for not by coincidence.
And because of that,
we can notice effects like the one I was describing
with the speakers, as you move from side to side,
you can move your ear a distance
that is comparable with one wavelength of sound.
So if you want to go from a place where the two waves
are out of phase to a place where the two waves are in phase,
you have to move basically one half of a wavelength.
And if the wavelength is human scale, it's easy to do that,
and you hear loud here, soft here, loud here, right?
It's a pattern in the room.
With light, what it's going to turn out
is that the wavelength is very, very small.
It's less than a micrometer.
And because of that, just the slightest wiggle
would move you from one peak to a trough
and then to another peak and another trough.
You would be washing out this effect.
You wouldn't be able to stand at one place
and obviously tell that, well, it wouldn't be silence now,
it would be darkness, darkness and then brightness
and then darkness and then brightness.
And that's not how spectacular and weird that would be
if there were two light bulbs in front of you
and you could move your eyes to some spot
where all of a sudden they canceled out.
That would be an amazing and direct proof
that light is a wave, not light is a bunch of particles
coming out of the light bulbs.
And you've never seen that with your ordinary eyes
with ordinary light bulbs.
And nobody had seen that back in history
and that's why people were arguing so vehemently
about whether light is particle or wave.
Isaac Newton believed that light is made of light particles,
that a light bulb is emanating little light particles
in all directions.
And it's not a bad hypothesis.
I can give you some arguments
why you might want to believe that.
Remember I said that waves bend around corners?
The evidence for that was that you can hear a sound
behind a door even though you can't see the sound maker.
But you can't see a person behind the door
unless they're straight through the door.
So the light is not bending around the corner.
That seems to be evidence that light is a bunch of little particles.
Now it is also true about waves
that the smaller the wavelength,
the less they tend to bend around corners.
You can check this with water waves of different wavelengths.
The bigger the wavelength is compared to the opening,
the more bending you tend to get.
So it's not proof that light is particles,
but there's no evidence back in Newtonian days
that light is a wave.
Isaac Newton was thinking about other properties.
He was thinking about color.
He was thinking about the path of light rays
through optical elements.
And so this argument was kind of a philosophical argument for him.
He didn't have the experimental equipment required
to convincingly show once and for all light is a wave
or light is particles.
And so although he had his beliefs and he articulated them,
they didn't really matter for the science that he was doing.
It was in 1801.
Thomas Young did this dramatic experiment
that I was just describing where you have two light bulbs
and you can find a spot where they cancel one another.
Young showed to the world that light really is a wave.
It's very much like that speaker experiment.
And what he needed was very high precision
and it was a challenging experiment.
Here's the basic idea.
Young took a very, very bright light source
and then he ran it into a black wall
in which he had cut a very, very narrow little slit.
One slit to let the light through.
So when you let light through a very narrow slit,
if it's narrow enough that it's getting down
to the distance scale of the wavelength of light,
then the light will come through the slit
just like water going through a little slit.
It looks like you have a point source
and the light will come out in all directions.
So now you've got the equivalent of a point-like source of light
and if you were to look at that slit,
you would see this little point-like source of light
and the light is going out in all directions.
You can tell because it's illuminating the wall
on the far side of the room uniformly
and it does get a little bit dimmer and dimmer
as you move farther away,
but that makes total sense whether you believe
that light is a bunch of particles or that light is a wave.
So far, no proof either way.
Light could be little particles going through the slit,
bouncing every which way,
like particles just bouncing off of edges
and scattering and going all sorts of directions.
So we haven't yet proven that light is a wave,
but now on the back wall that's uniformly illuminated,
let's put two slits.
It's called Young's Two Slit Experiment for this reason.
So now think about the light that's striking those two slits.
They are symmetrical with respect to the first slit.
So there's wave fronts heading outwards
and where nowadays I would think of the electric field.
When the electric field is high,
at one point, if you go anywhere along a wave front,
the electric field will be high and then low
and then high and then low.
I'm just visualizing this like a water wave
where the high spot is expanding
in a nice beautiful spherical pattern.
Reaching the two slits symmetrically,
so these two slits are now in phase.
When one is up, the other is up.
When one is down, the other is down.
Now Young has no idea what's moving.
He has this sort of vague idea that it might be a wave
and if it's a wave, he's thinking something is moving
and we'll come back to that point.
But whatever it is, waves go positive and negative,
positive and negative.
And so now you've got two light sources,
two slits which are, we call them coherent.
They are in-sync or in-phase.
So now go onto the far side of the two slits.
Turn around and look back at them.
You see two little bright spots.
Let's consider the two hypotheses.
If light is particles,
then you've got little particles spewing out of one hole
and little particles spewing out of the second hole.
And it's back to my argument that
if you've got particles coming at you from two places,
you're going to see them no matter where you are.
As you get farther and farther away,
it'll get dimmer and dimmer,
but basically you expect a pattern that's bright
and brightest right behind a slit
and then fading off smoothly
as you move off towards the edges.
So that's what I would expect
if I believe that light was particles coming through the slits.
But if I believe that light is a wave,
that waves are coming out of the slits
and they're starting off in-sync,
then there should be places back on that back wall
where, for instance, if it's equidistant to the two slits,
then they both started in-sync
and they are waving towards me and they're still in-sync,
and so I'll get a bright spot.
But if I move off to the side a little bit,
if I move just far enough
that one of the waves has gone just a half a wave farther
than the other,
half a wave means when one is up, the other is down,
and vice versa, down, up, up, down, down, up, down.
They are cancelling.
They are destructively interfering with one another,
and I get darkness.
With my eye, I would be looking at these two bright slits,
and then I move over a little bit,
and now I can't see the slits.
Then I move over a little bit further,
I see them again.
It's remarkable.
It's hard to imagine, but it's absolutely correct.
That's what you see in the laboratory.
You can take a piece of film and develop it,
and you'll see a bright band,
then a pitch black band, then a bright band.
It's called an interference pattern,
and it is as clear a signal that you've got a wave
as anything I can think of.
Very, very dramatic.
And the pattern is completely predictable from the wave model.
I can tell you how far apart bright and dark should be.
It's just geometry.
When do the waves add up?
When do the waves cancel?
And I can predict what happens if I change the color of the light,
because that would change the frequency of the light,
and therefore the wavelength.
So I should be able to make a concrete,
mathematical prediction about how the pattern spreads out
or narrows a little bit.
If I change the color,
or if I change the distance between the slips,
or if I change the thickness of the slips,
many, many experiments,
all of which completely compatible with a wave model for light.
So as you think about all these extensions and consequences,
you realize that the wave model of light
is convincing to the point of you accepting it as the truth about nature.
That's what we mean when we talk about scientific truth,
is that experiment after experiment,
new situations, completely novel situations,
they all match up with this one simple hypothesis.
So by 1801, people know that light is a wave.
Newton was wrong.
That's a big deal.
Very exciting for Mr. Young to be able to show
that Isaac Newton is wrong about something.
Physicists love to be able to show
that one of our great heroes has made mistakes,
because this is the way science progresses.
And like every revolution in physics,
this one took a little bit of time.
It was written up by Young in a way
that some people found a little bit hard to understand,
a little bit hard to believe.
It is a pretty dramatic experiment,
and so other people had to repeat the experiment.
These verifications and extensions had to be checked,
but it was a pretty quick revolution,
at which point there was this huge puzzle.
It's clear to everybody, light is a wave,
but what is waving?
And as we talked about over the last few lectures,
that took another 60 years,
till Maxwell in 1860s demonstrated mathematically
that light is indeed a wave.
It's an electromagnetic wave.
So it's not some material substance
that's lifting up and moving back and forth.
It's not a physical motion.
It's electric and magnetic fields
which are waving in strength.
Stronger, weaker, stronger, weaker.
Once you've understood that light is a wave
and sound is a wave,
you can start to take advantage of it.
So I mentioned those sound-canceling headphones.
A lovely little invention.
Once you know that sound is a wave,
here's this clever idea.
Suppose you're in an airplane
and there's a drone from the engines,
a very steady, low-frequency sound wave striking your ear.
So you build a little headphone,
which produces another sound.
Those sound-canceling headphones
are producing a loud sound,
as loud as the jet airplane at the location of your ear.
It's a very loud sound they make.
But they're monitoring the jet sound,
and when the jet pressure wave is high,
they emanate a sound wave
which has a very negative pressure and vice versa.
They cancel the outside noise
by adding their own interfering noise.
And it's quite spectacular that you can do that,
and they work pretty well.
They work best for steady outside drones
that aren't changing with time,
so that you can steadily create your own wave
that cancels exactly with the outside wave.
You can do this with light as well.
Anti-reflective coating is just a simple design.
It takes advantage of the fact that light
comes and bounces off the front or the back,
and either way, you're going to get some sort of interference
of the two different paths,
and you can make that interference cancel out light,
which is in the middle of the visible range,
and so you won't get a strong reflection.
So the wave model allows us to not only understand
the nature of these things, light and sound,
but it also allows us to build devices
and design new experiments,
and in the long run, it's very powerful
to be able to separate a wave from a particle,
and it helps us to understand
how we think about different things in the world
much, much more clearly.
Lecture 21, The Atomic Hypothesis
We spent the first part of this course
thinking about the fundamental underlying principles of physics.
We looked at kinematics, Newton's Laws,
in particular F equals MA,
the principle of conservation of momentum
and conservation of energy.
And then, what armed with that groundwork,
we started looking at particular forces.
We've looked at gravity, we've looked at electricity,
and then magnetism.
And then, we recognized that electricity and magnetism
were unified.
Maxwell helped us to visualize this unification
to think about electricity and magnetism
as flip sides of one fundamental true force of nature.
And we discovered that that force of nature
tells us about light and light waves,
which led us to think about waves
as a general feature of the world,
and that spread of ideas covers a good chunk of classical physics.
There is one key important final story,
which will lead off to many different consequences.
It is perhaps one of the greatest ideas of all of physics,
and I've been alluding to it all along.
It's part of our culture.
Everybody knows that the world is made of atoms,
but nobody knew that.
Even a hundred years ago,
people were arguing about it,
and 300 years ago,
people had no idea about how you would demonstrate
whether the world was made of atoms or not.
And so, today I want to talk about the idea of atoms,
how we could convince ourselves
that the world is made of atoms
during the period of classical physics,
from Newton all the way up till contemporary times.
And so, let's go back a little bit first,
and think about the Greek philosophers.
People have always been wondering
about what the world is made of.
What are the building blocks?
It's a very natural question.
400 years BCE,
Democritus had some writings
in which he postulated that the world is made of fundamental,
little uncuttable objects.
It's the bottom of the line,
and uncuttable in Greek is atomos.
So, that's the idea.
So, that's the origin of the idea of atoms.
Aristotle looked at these ideas and said,
no, I don't think so.
Remember, Aristotle believes in qualities,
and he believed that qualities are not divisible into chunks.
And so, he had a kind of a world view
that the world can be divided and subdivided indefinitely.
In that era, it's a philosophical debate.
People couldn't really think about measurable consequences
of one idea versus the other to test,
so that you could disprove one
and begin to convince yourself of the other.
And let's think about the sort of picture of this.
If I give you a stick of butter,
butter is a material,
and it's got certain defining characteristics.
Aristotle would be perfectly happy to start listing them.
It's yellowish, and it melts at room temperature,
and it has a certain density.
Density is mass per unit volume.
It's not quite how heavy it is.
It's how heavy it is for a given amount.
And if I chop the butter in half,
and now I just investigate a half a stick of butter,
it's still clearly butter.
I haven't changed the character of the material.
I've got less of it, but it's still yellow,
and it still melts at the same temperature,
and it still has the same mass per volume.
Got half as much mass, but I've also got half the volume.
And so you can now start chopping it in half and in half.
Now you've got a pat, now you've got a fraction of a pat,
but it's still butter.
And what Democritus argued was,
it's not butter all the way, there's one final cut.
And he didn't know where it was,
at what point in how big that last piece was.
He assumed that it was extremely tiny,
but not infinitely tiny,
and that's the last piece of butter.
And if you try cutting that, you can't.
If you did, you wouldn't have butter anymore.
Whereas Aristotle argued it's butter all the way.
You could just keep dividing it in half and in half
as many times as you like.
It just depends on the sharpness of your knife.
So how are we going to decide this question?
Well, the nature of science, classical physics,
is to think about consequences.
If you believe that the world is made of atoms,
what measurable consequences can you think up
and then we'll go test those?
So we're trying to create a worldview
that's as simple as possible.
That's one compelling argument
for why you might believe in atoms,
is because it's a nice simple point of view.
But it's got to go beyond that.
We have to be able to test this.
And people spend entire lifetimes
trying to think about the consequence
of the atomic hypothesis.
And I would like to sketch out some of the big ideas.
Nowadays, we can image atoms.
Not with our eyeballs, but with devices
like a scanning electron microscope.
So we can see individual atoms
by the evidence left behind on a computer screen.
So we certainly have very direct evidence today
that atoms exist. They're real.
But even back in the 1700s,
they were already some pretty good clues.
People were beginning to develop the idea
and becoming more and more convinced
that it seemed like a useful
and ultimately correct description of nature.
It turns out that atoms are a lovely topic
in classical physics.
I think they belong squarely
in the field of classical physics.
And yet they also form a kind of a bridge
to modern ideas.
Because classical physicists want to know
what is the world made of?
They're pretending that the world is made of
point-like objects.
And now we're talking about
what are those point-like objects?
And today, we ask
what are the atoms themselves made of?
So you can keep on digging deeper and deeper.
And I would argue that the classical story
ends with the atom.
The atom is the bottom line.
It's the simple, uncuttable, fundamental building block.
And so that's where we're going to zoom in on
for this lecture.
If you don't believe in atoms
or you don't know about atoms,
much of classical physics still continues to work.
You can talk about Newton's law for the moon
or for a baseball.
And we can pretend that the baseball is a point-like object
or that the moon, for that matter, is a point-like object.
And that works just fine.
We've talked about this throughout the course.
That we can simplify our thinking.
But now we're asking,
well, are there true point-like objects?
Is this a physical reality?
And if there was, if there is,
then our philosophical vantage point
that the world is ultimately simple,
our idea of reductionism,
that complex things can be broken down
ultimately into their building blocks,
and determinism that if you know the building blocks
and you know Newton's laws and conservation laws,
you can build up to arbitrarily complex systems.
All of this leads us to believe
that the idea of atoms could be very fruitful.
Let me be a little bit more articulate
about what the atomic hypothesis says.
The world is made of atoms.
Every material object, every object,
solids, liquids, gases,
all objects are made of fundamental atoms.
And there are different kinds of atoms.
We will grant that there might be, for instance,
carbon is one kind of atom,
nitrogen is another,
oxygen, yet another,
hydrogen.
These are all atoms that are present in our bodies.
Those four kinds of atoms
describe a good chunk of your body.
And if you add a few more,
they describe a good chunk of the physical world.
And all you're doing is just combining them in different ways.
In the end, it turns out that we need
about a hundred of these things
to describe everything we've ever observed,
just a little, a few more than a hundred,
fundamental atoms.
And every atom, of course, can be repeated,
so you can have many different carbon atoms,
but they're all identical,
absolutely indistinguishable from one another.
And that's what the world is built up out of.
The periodic table,
that's that graph of the elements
that is always up on the blackboard
or behind, above the blackboard
in chemistry and physics classes.
It's a list of all the known atoms,
hydrogen in the upper left-hand corner,
helium working its way down
through the periodic table,
listing all these hundred.
And if you know them,
and you understand their properties,
you understand what they're made of,
well, all you need to really understand
is that they have a mass
and they have certain interactions with one another,
and then you can build anything else up.
So what's butter?
Butter is made of molecules,
which are built out of carbon and nitrogen
and oxygen and hydrogen.
The idea of atoms is going to help us.
It's going to help us to make sense
of physics and chemistry
and many other branches of physics,
chemistry in particular.
Now at the beginning of this course,
I argued that chemistry is...
I was kind of a little bit dismissive of chemistry.
I argued that it's derivable from physics.
And so physics is the fundamental.
And chemists, of course, would bristle at that thought.
And I got to give them credit where credit is due.
The idea of atoms really arose from chemistry.
Chemistry and physics at one point,
back in the 1700s, were intimately connected.
They were just investigations into the world,
the natural world we live in,
and the way they were investigating it
just evolved until nowadays,
people who mix chemicals and look at the reactions
are doing chemistry,
and we don't call that physics anymore,
but it really still is trying to understand
the building blocks of the world in many respects.
Now back in Newton's era,
there was no chemistry.
Of course, there was no physics either.
He invented it.
But Newton spent time doing what now
we would call the beginnings of physics.
He also spent at least as much time,
maybe more, doing what we call alchemy.
Now it has chemie in it.
It sounds like it's related to chemistry.
It's a kind of a protochemistry.
It's a mix of mysticism and superstition
and kind of a quasi-religious cult approach
to understanding the world.
Nowadays, there are, well, at least as far as I know,
there's nobody seriously pursuing alchemy.
But in Newton's era,
doing alchemy meant what you think of
as kind of some classic chemistry experiments,
mixing things, heating them up,
grinding different materials
and trying to figure out what's going on,
except the alchemists were not behaving
like scientists in the contemporary sense.
So in particular, the culture
that Newton was helping to create in physics
that was so enormously valuable,
what it means to be a physicist,
to do experiments, to try to hypothesize
about an underlying physical explanation,
to then publish this work, to go to meetings
and talk to other knowledgeable but skeptical scientists
who are interested, who can themselves repeat your experiments
and figure out where you're right and where you're wrong
and come up with alternative or improved hypotheses.
That whole culture of science didn't surround alchemy.
Alchemy was done by yourself
and you would write up your results in little cryptic codes
so that other people couldn't steal what you did.
And it was by and large recipes.
It wasn't a fundamental, principle-based scientific study.
And so because of that, Isaac Newton,
although he spent an enormous amount of time doing alchemy,
and he wrote quite a bit about his alchemy,
very little of that has sort of followed through
and contributed to our contemporary scientific development.
It's too bad, but if only he had treated alchemy
in the same way that he had treated optics
or mechanics or astronomy, it's quite possible,
we would have progressed more rapidly in the chemistry field.
I want to talk about how we started developing the atomic model
by following two paths.
And one of the paths was the chemist
and the other path was the physicists.
So I would like to start with some chemists
and think about the early progression
so that we can understand why you might believe
in this crazy idea back in an era
when there's no hope at that time
of being able to see individual atoms.
Antoine Lavoisier, a French chemist,
he's now working roughly 100 years after Isaac Newton.
So chemistry has become something of a science by this point,
and Lavoisier really helped to firm it up.
He makes very, very careful measurements.
And he's looking at what if you mix this material
in some proportion with that material in some proportion.
And he noticed, for example,
if you mix the material number one
and you have a certain mass
and you mix it with material two, you've got a certain mass,
that the total mass of the product that you get,
it might be a new material, it might have new properties.
When you do chemistry, you can change the character
of what you're working with, because a chemical reaction has occurred.
But the mass is conserved.
This is a big idea because it makes you think,
why should that be?
Why don't we make sense of that experimental fact
at the macro level?
And a micro explanation could be
that we are just working with little tinker toys.
You take one piece of mass from this one,
one piece of mass from that one,
you add them together, they hook together,
and you've still got the same amount of mass.
It's just hooked together now.
So that tends to make you think
that the materials you're working with
are built of little building blocks.
If it was continuous properties,
then it's very difficult to make sense
of the observation that mass is conserved.
Soon after Lavoisier comes John Dalton.
He's a British chemist,
and he's called the father of the atomic model.
He really wrote it down and talked about it
and thought about why you might believe
that the world is made of these building blocks.
Not only does he do these kinds of mass measurements,
he does very careful measurements
of many aspects of chemistry.
So let's think about one in particular.
Think about forming water.
So you start with hydrogen gas,
and you have some oxygen gas.
These come in tanks with a measured mass
and a measured volume,
and then you combine them together.
You might need a little spark to make it go,
and then you get some water,
and if everything is hot, you'll get water vapor.
So you've got a different kind of gas coming out.
If you look at the masses involved,
you'll discover that you have 16 parts by mass of oxygen
combining with two parts by mass of hydrogen,
yielding 16 plus 2 conservation of mass
is 18 parts by mass of water vapor.
How could we understand that?
Well, Dalton is saying maybe what's going on
is that we're combining a certain number of hydrogen atoms
and a certain number of oxygen atoms to form water.
Well, you could call them atoms,
but people call them molecules because we've combined atoms.
Now, if you look at the volumes,
you will have two volume elements of hydrogen
combining with one volume element of oxygen
producing one volume element of water vapor.
So that's not like conservation of mass,
2 plus 1 equaled 1.
So in the alchemy days,
that would just be some arcane rule
you may or may not have noticed or cared about,
but it didn't make any sense.
But Dalton says this makes total sense if water is H2O.
If a water molecule has two parts of hydrogen
for every part of oxygen,
then two parts by volume of hydrogen
plus one part by volume of oxygen
will form one water molecule,
so one volume element of H2O.
It's very, very simple,
and all you need is to know some basic properties.
For instance, now we conclude, since water is H2O,
that a single oxygen atom
must weigh 16 times as much as a single hydrogen atom.
That's how you get the masses to add up.
So the hypothesis that the world is made of building blocks,
and now we know the mass of hydrogen and oxygen
relative to one another,
and we can do some more chemistry
and figure out the relative masses of all the different elements,
and then start to learn their properties.
Some of them are reactive in some ways.
We can begin to organize this periodic table of the elements,
and in the end it's a very simple and elegant system
that we've built up
that doesn't have very many fundamental constituents,
and yet all of chemistry,
and for that matter presumably all of alchemy,
if we cared about that anymore,
could be understood just by this basic combination
and recombination of atoms.
So by this period in history, by the say early 1800s,
people are beginning to agree that atoms are a good idea.
They're a useful bookkeeping tool for understanding chemistry.
Chemistry is no longer just rules, it's now principles.
There's an underlying story that we can make sense of.
And you can still argue
whether these little building blocks are real,
are they physical,
or is this just some kind of mathematical bookkeeping game?
People always, when you invent a new idea of something that's invisible,
you worry about that.
And so let's shift from the chemists to the physicists.
They were working in parallel.
Back in the late 1600s,
Robert Boyle was doing both experimental work with gases
and some theoretical work.
Followed up by Jacques Charles in the late 1700s,
and Amadeo Avogadro,
who was sort of pulling the big story together in the early 1800s.
These are following a different path.
Instead of combining things and looking at the ratios and the masses,
they are doing physics.
So physics means you measure masses, you measure forces,
you squeeze on the box of gas,
you've got some piston,
and you make physical measurements.
What's the temperature? What's the volume?
What's the force per unit area?
We call that the pressure.
Pressure is just good old Newtonian force
divided by the area that you've spread that force over.
And these folks were beginning to agree with the chemists.
They said, yes, that hypothesis helps us to make sense
of the properties, the physical properties of gases.
What they were observing was that all gases have certain common behaviors.
Some universal, it's called the universal gas law.
The gas law tells you what happens when you squeeze a piston.
The pressure and the temperature will change depending on the change in volume.
And these relationships between these physical measurable quantities
were very, very regular,
and it worked for carbon dioxide or oxygen or hydrogen gas.
It didn't really matter.
The ideal gas law is how it's referred to today,
refers to an ideal situation,
but most gases that they were working with were very, very close
to doing the same ideal behaviors
and have the same relationships one with the other.
They didn't develop the model that I'm about to describe,
the model that I described last lecture as well,
but it did arise from this atomic hypothesis.
If the world is made of atoms, what's a gas?
Well, the gas is made of atoms.
It's that little super ball story that the gas is really
just a bunch of little super balls flying around.
Oxygen gas is just a bunch of little oxygen molecules,
little tiny compact Newtonian fundamental particles
flying through the air, bumping into one another
and bumping into the walls.
So what would pressure be in this model?
Well, if you've got little super balls bouncing against the wall
and they have a known speed distribution,
you can conclude what would be the force.
It's just Newton's law, F equals MA.
If you've got little objects whacking into the wall,
then you can calculate what the pressure would be
as a function of the energy of those little objects,
which is what temperature is measuring.
This is an extension of the atomic hypothesis.
It's a kind of a branch of physics
that really just takes the atomic hypothesis seriously,
and we now call it statistical mechanics.
Mechanics, because like classical mechanics
that we've been studying all along,
it's the explanation of atomic systems.
Any kind of a gas is really built up out of atoms,
and statistical means, well, we've got an awful lot of them.
These little super balls are extraordinarily tiny,
and so we're not going to worry about the motion of one
or the motion of the other.
We're not going to try to track them.
We're going to look at averages,
and we're going to understand pressure and volume
as just having bulk consequences
arising from this microscopic model.
The atomic hypothesis goes much further
than just the ideal gas law.
This is really the starting point,
and once you've got the building blocks
and it's consistent between the chemistry story
and the ideal gas law story,
you're beginning to ask yourself deep questions about,
okay, if this is the way the world is,
we should be able to understand everything,
every object, not just these ideal gases,
but liquids and solids.
So you could ask questions like,
what makes an object melt?
A solid object gets hotter?
How do we make sense of that?
Well, the atoms, because we're believing in atoms now,
must have been in some sort of lock step.
They were in a solid, so they were bound together
by chemical forces, and as you heat it up,
what are you doing?
Well, you're making the little atoms jitter
more and more rapidly.
It's simple Newtonian physics.
You're giving energy to the little atoms,
kinetic energy, energy of motion,
and as they jitter more and more rapidly,
they will come some point when they break the chemical bond.
There's a certain energy, potential energy,
associated with the little chemical bonds.
It's all straight classical physics,
and at that point, the molecules would be free to roam around,
and that's what a liquid would look like.
Instead of them all being locked together
and having a solid, they are now free to roam around.
They've now separated one from the other,
and that would be a liquid,
where they're still in contact with one another,
but no longer rigidly locked.
If you keep heating it up,
then ultimately they will fly up into the air
because they've got so much energy,
they're like high-energy superballs bouncing around the room,
and that would be evaporation.
So you can understand all sorts of physical events,
evaporation, sublimation, melting,
all in terms of the atomic hypothesis.
You could understand dissolving.
You take some sugar and you put it in water.
Why does the sugar disappear?
Why does the volume of the water not change
in a very dramatic way?
Even though you put a big volume of sugar in there,
you go.
The mass is still there, but the volume didn't change.
Well, the atomic hypothesis helps us to make sense of this,
because the sugar is built up out of little atoms,
and those atoms can react with the water
and begin to migrate in between the water molecules.
There's space in a liquid between the water molecules,
and so the pieces of sugar,
the atoms that are making up the sugar,
or at least little combinations of those atoms,
can fit into the interstitial spots.
And so you can speak qualitatively about all these things,
but you can also start to talk quantitatively
about all these things.
For instance, you could ask, how big are these atoms?
That was a question that democracies had no way of tackling,
but by the 1700s, 1800s,
people are starting to come up with ways
of determining the size of atoms.
Ben Franklin, good old Ben,
came up with a very clever little experiment.
He took a drop of oil.
He knew the volume of that drop, it's just a little sphere,
and he dropped it on top of a pond,
and it spread out, made an oil slick,
spread out, spread out, and then it stopped spreading.
So it had a certain size to it,
and he could easily measure the area.
What he couldn't do was to measure the thickness.
It was so thin that no possible ruler in his era
could measure the thickness.
But by believing that the oil is still oil,
that the oil is made of atoms that have just spread out
until you've only got maybe one or two atoms thick,
because they can't spread any thinner than that,
then you can deduce that the volume that you started with
should be the volume that you ended with,
because it's the same material,
it's just a different physical configuration.
The volume of a liquid doesn't change
as you change the shape of the container.
So Ben Franklin measured the area and deduced the height,
and he came up with one of the first estimates
that turns out to be very good quantitatively
of the typical size of an atom,
about a billionth of a meter.
One billionth of a meter.
And that's how thick that oil slick is.
Well, it's a few billions of a meter.
And then there were many, many, many experiments to come,
and in each of these experiments,
very, very different kinds of experiments,
the size of the atoms always seem to come out
to be about the same size.
So atoms seem to have a characteristic size,
which again leads you to believe
that they are really fundamental objects in the world.
One of the ways that you can determine the size of an atom
would be to shine x-rays at it.
We talked about how x-rays,
like any form of electromagnetic radiation,
can interfere if they're running through slits.
And if the slit size is comparable to the wavelength
of the light, then you'll get a beautiful interference pattern.
And so with the x-rays, people recognized
that the wavelength of the x-rays was comparable
to the size of the atoms themselves.
The atoms were the slits in a crystal.
So many, many different kinds of experiments,
all coming at the same fundamental physical picture.
You think about heat and temperature,
and now this statistical mechanics,
this thinking about the world as being built up
of little atoms is beginning to make more and more sense.
As you heat something up, the atoms jiggle more rapidly,
and now we realize what temperature is,
what is it measuring?
It's measuring the jiggle rate.
It's measuring the energy of those little molecules.
So when you ask questions about what happens
when you heat something up, this atomic world view
will help us to make sense of what's going on.
That's what we're going to want to cover in our next lecture
as we begin to think about further consequences
of the atomic hypothesis.
I recognize that atoms can be a kind of an abstract idea
because they're so small that nobody has ever seen one,
and nobody ever will.
They're so small, they're much, much smaller
than the wavelength of visible light,
and if an object is much smaller than the wavelength of light,
the light won't interact in a useful way with it
such that you can directly image it.
So you cannot and will not see an atom.
You can only see it indirectly
by using something other than visible light,
so you'll need some kind of detector
like an electron microscope detector.
By now, atoms are as well established,
an idea in physics as I can think of,
and there are many, many consequences.
When you look at the world around you
and you think about the properties of ordinary objects,
properties like thermal properties,
like optical properties, like mechanical properties,
the color of the object,
which is really just the interaction of the light
with the electromagnetic radiation,
anything you can think of really can be made sense of
and maybe even calculated on the basis
of this underlying atomic hypothesis.
So far, we've tried to simplify
as much as possible whenever possible.
We treat objects like points.
We considered a spherical cow.
Even an athlete or an automobile,
we've thought of as a point,
focus your attention on the center of mass,
you watch it move, it obeys Newton's laws.
And then, of course, we recognize
that the world is more complicated than that,
and so we started adding in the complexities.
So you don't have to always neglect friction.
Once you understand the underpinning laws of nature,
friction is just another force.
You can calculate it, you can observe it,
and you can add it back into the f side of f equals ma.
If you watch the athlete diving off the high dive,
you can watch the center of mass,
but you can also recognize that they're doing spins and turns,
and you can calculate and understand the rotations
around the center of mass,
or even the change in the shape of their body,
which makes the story even more complicated.
But fundamentally, we've been trying to treat objects
as though they were as simple as we possibly could.
That's why we were able to get along historically for so long
without worrying about whether atoms are real or not,
because we weren't really looking down all the way,
very often, to this microscopic building block level.
This is a very, very successful strategy.
It's been extraordinarily productive throughout history,
and continuing today, an astronomer,
even today, can think about the motion of the Earth
and understand the seasons and the phases of the moon,
and you don't have to worry about things like,
well, there's Mount Everest,
and so it's not really a perfect sphere,
or we have an atmosphere, so it's not really a solid body.
It depends on the questions that you're asking.
If you're worried about global warming,
then you do have to worry about the atmosphere,
and that little thin layer can have many important consequences.
So in this lecture and basically throughout the rest of the course,
I want to make the transition from thinking about the world
as simply as possible to recognizing that the world is built up
out of many little tiny atoms.
Things, even simple things, are enormously complicated,
and even granting this, it turns out that we can make sense
of what's going on.
It's the last piece of the classical physics story
where we accept this reality and look for the consequences,
and again, try to think about the consequences in a simple way
as we possibly can.
This is the field of thermodynamics.
That's the name for the branch of physics that we're talking about.
It belongs to classical physics, thermodynamics.
Thermo makes you think of temperature,
thermometers, and dynamics makes you think about why questions.
Why does an object cool off and another object warms up?
Why does heat flow the way it does?
These are the kinds of questions that we want to investigate.
When you study thermodynamics, you begin to recognize that the microscopic
internal degrees of freedom, the atoms, are very useful.
It's quite fruitful to know about their existence.
Thermodynamics goes beyond just description.
It's not kinematics.
It really is a dynamics, and that means that we're going to need
some fundamental laws to help us to make sense of what's going on.
In a certain sense, we can use the fundamental laws that we've already built up,
the classical Newtonian laws.
We will introduce some laws of thermodynamics.
There are only a few of them, and they are reformulations that help us
to make sense of what's going on in thermal systems.
In thermal systems and thermal physics, it's everywhere.
It's of enormous practical importance.
Just think about your home and insulating it in the wintertime
and cooling it in the summertime, and then you go in the kitchen
and you turn on the stove, and you'd like the food to heat up quickly,
but not too quickly.
You want to measure the fever of a small child.
You want to predict the melting of glaciers on Greenland.
Many, many, many things in the world rely on an understanding of thermodynamics.
It's a lovely field to be wrapping up this course with,
because thermodynamics really does pull together all the big ideas of classical physics.
We will think about F equals NA.
We'll think about energy principles.
We will think about the atomic hypothesis.
We will think about statistical mechanics.
This idea that microscopic motion can explain macroscopic thermodynamics
is a lovely and powerful connection.
You can talk about thermodynamics without ever talking about atoms.
You can put a pot on the stove, heat it up,
make measurements of the temperature as a function of time,
and never talk about the atoms.
But we are going to think about the atoms,
because although it took historically some time
before the atomic hypothesis got folded in
with the pure, straight observations about temperature and heat,
in the end, when you do put them together,
it makes everything make so much more sense.
Thermodynamics begins with energy and energy flow.
That's really one of the languages that helps us to understand heat and heat phenomenon.
When you have a complex real-world system,
any normal object has lots of atoms in it.
A pot of water might have a million, billion, billion water molecules in it.
If you ask, how could I understand what's going on,
the reductionist viewpoint would say,
all right, go all the way down, look at the atoms,
look at the forces between them, track them.
There's an object that starts here,
and now it feels a kick, so now it goes that way.
That might work, but boy, would that be tough.
And nobody has really ever thought about, in a practical way,
understanding a pot of water
by going down to the individual water molecules.
Instead, and this is really the big idea of thermodynamics
and statistical mechanics, is to think about averages.
Think about the behavior of typical atoms.
And don't worry about the details,
so this is the simplification that's going on in thermodynamics.
If you focus on the average,
and thinking about energy will definitely help,
it gives us a nice, concrete, measurable handle,
then we're going to be able to make sense of what's going on.
Think about the insurance company
that can make a living very confidently
by making predictions about when people get married,
when people have babies, when people get sick,
and how often they get sick when they die.
It's not so hard to make these predictions on average.
It's impossible to make these predictions for an individual.
You could never do it.
So insurance companies don't try to make a specific guess about you.
Instead, they ensure many, many people
and they figure everything will wash out.
And that's a very powerful idea and it works very well.
Atoms are much simpler than people,
so it's nice if the insurance company
was dealing with something simpler than a human being,
they would have an easier job.
And it gets easier and easier
as the insurance company ensures more and more people.
My university got into a little bit of trouble a few years ago
because we went with a small local medical insurance company,
and the fact that they were only ensuring the local community
meant that one or two freak accidents,
somebody who has a heart attack much younger
than you would have expected,
and then costs the company a lot of money,
can have a big impact.
But when you're averaging over hundreds of millions,
let alone millions of billions of billions,
then all of a sudden the details of the individuals
hardly matter at all.
Thermodynamics, the study of the understanding of temperature
and heat and work and energy in macroscopic systems
is characterized by three laws.
And then after the third law,
people look back and realize that really there was another law
that should have come first.
So instead of renumbering them,
they added the zeroth law of thermodynamics.
It's kind of silly.
But we're going to start with the zeroth law of thermodynamics.
And in today's lecture we'll talk about the zeroth and the first laws,
and next time we'll talk about the second law.
The third law is kind of a technical and small addition.
We won't really focus too much on it
because the second law contains the essence of that part of the story.
The zeroth and first laws are really about work and energy
and energy flow.
The second and third laws are going to add a new concept,
which we're going to call entropy.
It's a whole new topic,
so we want to leave that until we need it.
Today I want to pull together all the ideas of energy
and see how adding just a little bit of new language
can help us to make sense of much more interesting systems
than we've been looking at before.
This study has gone on for a long time.
You can just imagine when you think about how practically useful it is
to understand thermodynamics,
people must have cared about this back thousands of years ago.
You want to keep warm in the winter.
You would love to be able to keep the ice from the winter into the summer.
There are lots of applications of understanding thermodynamics.
And in the old days, people developed a theory,
and this theory went by the name of the caloric theory.
The caloric theory was quite believable for a long time,
and it's been disproven in the classical physics era.
The caloric theory was discarded.
It was a theory which said that what does it mean to say
something is hot and something else is cold?
The caloric theory says what it means is that the hot thing has some stuff in it.
The caloric is a fluid, a physical material fluid.
Hot things have more of this fluid, cold things have less of this fluid.
It's nice because then when you think about pouring fluids,
that's like flowing heat, so a hot object heating up a cold object
you could think of as the transfer of this mysterious fluid from one to the other.
It's a nice idea.
It meshes with intuitions about flow.
It just turns out not to be correct.
There are consequences of that model which are demonstrably false.
The basis for a contemporary model came from a physicist named James Joule.
We've talked about Joule before.
We named the unit of energy after Joule.
One Joule of energy measures a certain amount of either energy
or if you prefer, work done, force times distance.
James Joule didn't invent this idea.
Other people had thought about it and talked about it,
and he gets most of the credit partly by historical accident,
partly because he was just well situated.
He did some excellent high quality experiments.
He defended them very well.
He wrote it up nicely.
He was part of the physics community.
We think about James Joule as one of the originators of this idea
that when we're thinking about heat, we're really thinking about the flow of energy
and we're talking about thermal energy.
We've mentioned this before.
When I talked about energy, I said it comes in many forms.
One form is kinetic energy.
That was the first form we thought about.
The formula one-half mv squared, mass times the square of velocity,
tells you how much energy you've got.
How much energy was defined as how much work could you do?
How big of a force could you apply over how long of a distance?
That was one form of energy.
Then we talked about gravitational potential energy
and chemical potential energy, and we mentioned thermal energy.
It's another form of energy.
Think about a book sliding across the table.
It starts off with energy of motion, pure, measurable kinetic energy.
It has a certain number of joules of energy.
Then it grinds to a halt and it's stopped now.
You ask, where did the energy go?
It did not go into gravitational potential energy.
It didn't go up.
It's not really stored in an obvious way.
It's not stored like a compressed spring.
You're not going to get it back.
If you let it sit there, there's nothing you can do
to get that energy back in any obvious way.
So has it disappeared from the universe?
No, it's still there, and we call it thermal energy.
And now that we have this atomic hypothesis,
we understand exactly where it is.
The friction, the force between the book and the table,
started making atoms jiggle, molecules jiggle a little bit more rapidly.
If they're jiggling more rapidly, they have more kinetic energy.
But it's hidden from our eye, because the book as a whole
and the table as a whole have no overall kinetic energy.
We can't see that kinetic energy.
It's random.
One molecule is going one way, the other molecule is going another way,
but they all have a little bit more energy each,
and it all adds up, and that's where the thermal energy is stored.
It's stored in the random kinetic energy of the molecules themselves.
And it's quantitative.
You can count that energy.
Thermal energy is measured in joules, just like any other kind of energy.
When you do experiments, you're trying to figure out
whether this is a true statement or not.
Back in the 1700s, after Newton, people are arguing
about whether it's the caloric, and that would be a physical substance,
or whether it's energy that we're talking about,
which is not a physical substance.
It's a property of physical objects.
There's a very different kind of model about what you're talking about
when you're talking about heat and temperature and so on.
These experiments that people did were much tougher than you might think.
It's easy enough to heat a pot of water,
but to keep careful track of how much energy you consumed
and where it went turns out to take about 200 years of steady development.
Isaac Newton thought a little bit about heat and thermodynamics himself,
but it was really 200 years later.
It was the 1800s when people were really beginning
to make a concrete physical science of all this
and realized that there are patterns and regularities.
It doesn't matter what material substance you've got
or what circumstances you have.
If you're trying to measure, why is it so difficult?
Well, think about how difficult it is to isolate a system thermally.
We do this. You go and buy a thermos bottle.
A thermos bottle is designed to keep the coffee warm for a while,
to thermally isolate the hot coffee from the cool outside world,
and you know that that works for, oh, maybe a couple hours,
at which point the coffee is cooled back down again.
And thermoses are a relatively modern invention.
They require good seals and vacuum and high-tech materials.
Back in the 1700s, it's difficult to thermally isolate objects
in order to study their temperature as you add energy to them,
and it's difficult to measure temperature to high accuracy.
Nowadays, we have very small and precise thermometers,
but if you slide a book across a table,
even with a pretty good thermometer,
you're going to have a hard time carefully measuring the change
in temperature of the book and of the table.
And if the thermometer is like it was back in the old days,
some big bulky object with lots of fluids in it,
then it begins to interfere with the experiment that you're doing.
You wanted to measure the thermal properties of the book,
but you're mixing in the thermal properties of the thermometer itself.
And of course, that's a very dangerous thing.
You have to be very careful when doing physics
not to be measuring the device that you're using to measure the physics.
You want to measure the physics itself.
Mr. Joule was one of the first physicists
to really systematically, carefully and quantitatively
come up with very clever experiments,
and it is partly for this reason
that we've named the unit of energy after him.
Many physicists worked on this idea, though.
It took a lot of years,
and how are you going to show that the caloric theory is wrong?
Well, let's think of some very simplistic arguments against it.
Number one argument, if caloric is a physical substance
and you're pouring it from one thing to the other
when you're transferring thermal energy,
then as your coffee cools down, it should get lighter
because it's got less caloric in it.
So you could make some careful measurements,
weigh the coffee before and after it's cooled off,
and you'll discover, nope, there is no measurable change in its mass.
So now you have to start standing on your head.
You say, well, maybe the caloric is really, really light,
and so we can't measure it because it's just not a massive substance.
So then you say, all right,
supposing that I take a block and it's cool,
and I take a drill bit, which is also cool,
and I start drilling into the solid object.
And it's a very, very, very rigid solid material,
so the drill bit is just grinding and grinding and grinding
and barely doing anything, and you know what happens.
It gets very, very hot, very rapidly.
So if you believe in the caloric, where is the caloric coming from?
You're creating it out of nowhere.
You had two cool objects and doing physical mechanical work,
force times distance, frictional force scraping across the object
at the drill bit is creating caloric out of nowhere.
That's an awfully difficult...
If you're a classical physicist, you find that a difficult idea
to create a physical substance out of nothing
doesn't seem like it fits in with Newtonian ideas.
Ultimately, there were many, many experiments,
and this idea of joule that we're talking not about a material substance,
but about energy really became so well verified
that it formed the laws of thermodynamics.
So let's talk about them.
The zeroth law.
The zeroth law of thermodynamics is, in a sense,
defining what thermal equilibrium means.
If you've got two objects, A and B, and you touch them together,
they might be in thermal equilibrium, or they might not be.
If they're not, one of them will change.
It will cool off, and the other will change.
It will warm up until nothing happens anymore.
When nothing more happens, they're in thermal equilibrium.
The zeroth law of thermodynamics says,
if A is in equilibrium with B,
and then you separate them, and you check,
and you discover that B is in equilibrium with C,
so we've done a pair of experiments,
then the zeroth law of thermodynamics argues, or concludes,
A and C will be in equilibrium with one another, guaranteed.
So if A equals B and B equals C, then A equals C.
Now, you might think that's obvious,
but it's not equals like a mathematical function.
It's equals meaning in physical equilibrium.
It's a statement about the world, and it's a very practical statement.
It means that if you take a thermometer and you go to the factory
and you calibrate it, you hold it against an object
which somebody has defined to be 98.6 degrees,
you wait until they're in equilibrium,
and then you draw a little line where the fluid is.
And now you've separated A and B,
and you've moved the thermometer and you put it in your mouth now,
and you wait until the thermometer comes into thermal equilibrium
with your tongue, and you look at the state of the thermometer
and you say, oh, it's in exactly the same state as it was before,
so the temperature of my mouth is the same as the temperature
of this artificially defined definition
of what 98.6 is going to mean.
So this allows us to use thermometers reliably
and understand that they are measuring something physical and repeatable.
Temperature becomes meaningful with the zeroth law of thermodynamics,
and it tells us how you go about measuring it.
You compare things.
So your mouth is the same temperature as the 98.6 standard,
and so we say your mouth is at 98.6.
It's meaningful.
That's what the zeroth law of thermodynamics is telling us,
and notice there's some very subtle ideas in here.
So the thermometer can be big or it can be small.
I don't care.
It will be in equilibrium with my mouth and with the 98.6 standard,
no matter what it's made of, what materials you make it of,
high-tech, low-tech, big, small.
The zeroth law of thermodynamics says temperature is well defined.
It's a property of objects.
Now, if you think about atoms,
you're thinking about statistical mechanics,
which I kind of think of as the underpinnings of the laws of thermodynamics.
What I realize is that, of course, temperature makes sense.
Temperature is measuring the average kinetic energy of the little atoms in the system.
So in my body, there is an average kinetic energy,
and the thermometer gets into equilibrium.
What does that mean?
Well, if the thermometer starts off colder,
that means that the atoms in the thermometer are jiggling more slowly.
So what happens when you bring two solid bodies into contact?
And microscopically, one of them has atoms that are jiggling slowly,
and the other has atoms that are jiggling rapidly.
Well, the rapid ones will whack more frequently into the slow ones and speed them up.
Of course, they'll slow down in the process,
so the fast ones get slower and the slow ones get faster,
and this continues until everybody's going at basically the same average speed,
and that's equilibrium.
So now we have a microscopic picture that helps us to make sense of the zeroth law of thermodynamics.
In the end, when you have equilibrium, all the atoms will have the same average energy.
Now, it doesn't mean that every atom is going at exactly the same speed.
Some are going faster, some are going slower.
They keep bumping into one another all the time,
but when they reach a steady state,
then you have this lovely thermal equilibrium and you have a well-defined temperature.
Temperature has nothing to do with the material object.
It's getting down to simply the average energy of whatever particles you happen to be made of.
Gases, liquids, solids, everything's made of atoms,
so everything has this nicely defined temperature, basically.
The first law of thermodynamics takes this and it now adds in the story of energy flow.
It's really a statement of energy conservation, but in a new way.
So there's some words that we need to keep straight,
and they're subtle words we can easily muck them up
because it's this usual story about physics where the words are defined by physicists,
but they also have common English usage.
We've already used some of them incorrectly because in some cases we tend to be sloppy about thermodynamic words.
The three words are temperature, thermal energy, and heat.
So we've already talked about temperature.
That's what the zeroth law of thermodynamics is helping us make sense of.
So temperature is one thing.
It's the average kinetic energy in a microscopic model.
Thermal energy is not the average, it's the total.
You have a block and it's got some total amount of energy
because every atom or molecule has its own little teeny weeny kinetic energy,
a little teeny weeny mass, and you add all those up.
That gives you the total thermal energy of the object.
When you slide the book across the table,
the original kinetic energy, however much that might be,
is getting spread out over many countless billions of billions of atoms,
and because it's spreading out over so many atoms,
each individual atom doesn't change all that much.
A tiny increase of everybody added up over many millions and billions of objects
can add up to a reasonably large amount of energy.
That's why the book slides across the table.
Its kinetic energy seems to have disappeared and the temperature barely rose
because the average didn't change very much.
The grand total increased by exactly the amount that we started with.
What about heat, the last of those three words?
Well, heat is the trickiest of them all.
If you want to use the word heat correctly like a physicist,
that is to say, if you want to use the word as it's defined,
think of it as a verb, I heat the water.
That's a good usage.
You shouldn't talk about how much heat is there in the water.
That's a noun.
It's treating heat like it was a material substance.
You're going back to the old caloric idea that there's something there in the water.
So it's awfully easy to say and I say it myself.
So I won't fuss on this too much,
but when you have a hot object and you have a cool object,
we say that the cool object gets heated up.
And what do we mean by that?
We mean there is a flow of thermal energy.
So heat really is defined as the flow of energy,
thermal energy from one object to another object.
So when you talk about heating up the water,
you're thinking about a flow of thermal energy.
The total thermal energy of the water is increasing when you heat it up.
The first law of thermodynamics puts this together in a simple statement
that says if you have an object and you can do things to it
in a variety of ways, but energy must be conserved.
The first law of thermodynamics says energy is conserved.
So let's be very clear about this if you have an object
and it's got some total thermal energy and then you do something.
So the thermal energy changes.
How much will it change?
Just conservation of energy.
The thermal energy increase of an object will equal.
How much physical work you did?
That's a transfer of energy force times a distance that's mechanical.
That's one way of adding energy plus the other way of adding energy,
which is heating.
So total change in thermal energy arises from work done plus heat.
So work and heat are two different ways of thinking about the transfer of energy.
That's the first law of thermodynamics.
It says that if you put a cold object on the stove and the stove is hot,
I could turn off the oven, but the stove is still...
I could turn off the switch, but the stove is still hot
and the water is going to get hotter and hotter over time.
Why?
I am putting energy into it.
If you're getting hotter, that means your temperature is going up.
Then if your temperature is going up,
that means your average energy of molecules is going up,
but you still got the same amount of water.
So if the average of each one goes up, then the total must be going up.
We must be adding energy.
Where is it coming from?
How do you add energy?
The old Newtonian way, really it's post-Newtonian,
was to do work force times distance.
You could run a paddle wheel through it and make friction
and do some physical mechanical work.
That would be one way of heating up the water.
But that's not what's happening on the stove.
Instead, we are just doing this heating.
We are transferring the random motion of molecules on the stove top
and converting that into random motion of molecules in the pot of water.
James Joule is arguing that thermal energy can be measured.
It's just all one and the same thing.
It's just energy.
And so heat and mechanical work are really equivalent to one another.
And he measured the mechanical equivalence of heat.
If you do a certain amount of work, a certain force times distance,
you can see how much the object heats up.
You can see it's changing temperature.
And then you can compare that with the amount of heat flowing from some temperature difference.
And so now we can talk about flowing heat with the exact same measurable units, Joules.
We can talk about a flow of heat in Joules in these two different ways, work and heat.
In the end, the first law of thermodynamics is telling us that there is a kind of a bottom line here.
We can understand temperature by thinking about equilibrium.
We can think about, if we want, the internal energies.
We can go down to the level of atoms.
And what we discover is, in a certain sense, there's nothing new here.
There's no new mysterious caloric.
We don't need to hypothesize this mysterious new material substance because you can understand what's going on
just by thinking about energy measured in Joules in these different forms.
So thermodynamics is a kind of a bookkeeping at this point.
We're just keeping track.
How many Joules did I have to start with?
In what different ways did we transfer that energy?
It's very nice.
Physicists love to have simple bookkeeping tools.
And energy is one of those great, useful tools.
There's a number that you add and subtract.
There is more to thermodynamics than just bookkeeping.
And in the next lecture, we will talk about this new concept, the entropy concept,
that will shift a little bit our story and teach us to think about why things happen in a slightly different way.
But for the moment, recognize that just the first law and the zeroth law of thermodynamics
allows us to understand an awful lot of practical things.
Measuring temperatures, measuring properties of objects as you put them on the stove,
what happens when you stir them, what happens when you squeeze them.
All of these are really one in the same fundamental idea.
Thermodynamics.
Lecture 23, Heat and the Second Law of Thermodynamics.
Thermodynamics began as an application and an extension of the basic idea of energy,
an energy conservation.
Thermodynamics really started off by arguing that thermal energy is just another form of energy.
You don't need to hypothesize some mysterious new substance.
You could just think about flow of energy in order to understand objects warming up and cooling down.
And the first law of thermodynamics is just the mathematical statement of conservation of energy.
There are many, many applications of this idea.
And the most significant that I can think of in our lives would be something called a heat engine.
Heat engine is a generic term.
It's the device that drove the industrial revolution.
It's what powers our homes.
It's what runs our cars.
A heat engine is a device that converts thermal energy from one place and moves it and does something with that energy.
So that's really the idea of a heat engine.
And this is what we want to talk about today.
In order to talk about it, we're going to have to introduce a new concept, the concept of entropy.
Let's just begin by thinking about the broad properties of heat engines.
They're made of many, many different kinds of designs.
You can think of lots of different kinds of heat engines.
You always have a hot bath.
There's some place that's hot.
In your automobile, you're burning gasoline and you get a little hot spot.
And there's always a cold spot.
We call it a reservoir.
And this cold reservoir is the exhaust.
And in your automobile engine, the cold spot would probably be the outside world that's at a cool temperature.
And then there will be some working material.
And the idea of the heat engine would be that the working material might be a gas.
It might be a liquid.
There's all sorts of different kinds of designs.
It's going to take some of the energy, the thermal energy from the hot stuff, and do something.
If what you do is convert that thermal energy into work, that is to say into mechanical energy.
I should really use the proper word.
If you convert the thermal energy of the fuel or the hot spot into mechanical energy, kinetic energy,
that would be like an automobile engine taking the hot gases, driving pistons, moving the car.
So you're moving energy from a thermal place into kinetic energy, a different form.
You could, in principle, move the energy, the thermal energy, from a hot place into a cold place,
or from a cold place into a hot place.
Those would also be heat engines.
So when you're thinking about air conditioning your house or running your refrigerator,
all you're doing really is moving thermal energy around.
You're taking it out of the water, turning the water into ice cubes, and dumping that thermal energy into your kitchen.
So that's another kind of heat engine.
In order to understand heat engines, well, in the old days, people were just building these things,
and they didn't really understand them.
So 200 years ago, people were designing steam engines, and there wasn't a good, rigorous physics underpinning for them.
So they were tinkering, there were some engineers, there were some principles that were developing,
and people began to build steam engines.
By 1800, you could heat up water and then use that hot reservoir of water to drive a piston,
that's the working fluid, and that pushes the wheels through some crank system,
and so you could convert the thermal energy into energy of motion.
And people were getting more and more frustrated because they knew that they were putting in a lot of energy,
they were burning a lot of coal, and yet they weren't getting a whole lot of energy out.
They were not climbing big hills, they were not going very fast, so there was this poor efficiency.
And this was a lovely point in the history of science where tinkering and engineering and physics all began to work together.
So there was this kind of practical drive for the physics, but the theory and the development of the laws of thermodynamics
was fueled and then became immediately useful for people to understand how to apply this stuff.
It's a lovely interplay which continues to this day, it's part of, I think it's part of what it means to be doing
certainly classical physics and maybe any kind of physics.
The question that is coming to the forefront with these steam engines is efficiency.
I'm going to define efficiency kind of glibly as what you get divided by what you paid for.
For an engine, what you get means how much mechanical energy did the device provide you?
How high up did the object go or how fast is it going?
So that's the useful energy.
That's the numerator, the denominator is how much energy did you put in,
measured as chemical potential energy of the coal or the gasoline or whatever source of energy you started with.
Now think about conservation of energy.
What you get can never be greater than what you put in.
What you get out can't be greater than what you put in because that would be an obvious violation of the first law of thermodynamics.
So the first law says the efficiency what you get divided by what you paid for will always be equal to 1
or more likely in a practical situation less than 1 because you can always throw away some of that energy.
Imagine if you're burning a coal and it's not really doing a whole lot, it's just heating up the atmosphere.
So that's not useful energy for a steam engine.
Now it might be useful energy for if you're warming up your house
so the efficiency might be different depending on the purpose
but if you're thinking in particular about heat engines as real engines
then you can see that practical engines could be much, much less efficient than 1 or 100%.
If you start off with a small amount of gasoline that's got 1,000 joules of stored chemical potential energy in it,
well defined, if you have a certain amount of gasoline, you know how much energy there is,
you know in principle how much work you could get out of it
and in practice when you burn it in a real life automobile
you won't get anywhere near to that 1,000 joules back out again in the form of kinetic energy of the car.
You might, if you've got a super car, really efficient, you might get a couple hundred joules.
So if you get 200 joules of useful work and you put in 1,000 joules in the form of raw chemical potential energy,
the efficiency of that engine is 200 divided by 1,000, that's 20%.
And that means that 800 joules, by conservation of energy by the first law of thermodynamics,
800 joules must have been thrown away and it doesn't disappear from the universe, where does it go?
It warms up the cold reservoir, which in this case is the atmosphere.
Because the atmosphere, because the cold reservoir is so big, 800 joules of energy isn't very much,
spread out over countless billions of atoms, they don't have to change their temperature very much,
maybe even not noticeably, and yet you still have conservation of energy.
So conservation of energy has put a limit of 100% and back in the early 1800s people were looking at 1% engines at best
and they were really struggling to get efficiency.
And there is a new law of physics, the law that we want to talk about today, in which we discovered that you can't even approach 100%.
It's much, much worse than you might have thought.
The second law of thermodynamics gives us a mathematical formula that tells us what is the maximum efficiency of a perfect, ideal heat engine.
This was discovered originally by Saadi Carnot, he was a French engineer and he was very interested in steam engines.
He also did some good physics and part of the brilliance of Carnot, back in 1800,
was his ability to look at all these different steam engine designs and step back from the details.
Never mind what fluid they use, never mind what fuel they use, never mind the mechanical design of the pistons.
Just think about the broad principles, the laws of nature that are involved.
And what he recognized was so simple that you may not even be able to imagine how this could put a quantitative limit on efficiency.
But what Carnot recognized was that all the engines work in the same fundamental way.
There's thermal energy in a hot reservoir and you are taking that energy, you're doing something with some of it,
you're doing some useful work turning that into kinetic energy.
And you're also throwing away some of it, you always throw some of it away into a cold bath, into the cold reservoir.
And what Carnot recognized was that's the physical underlying principle of any design.
And what always happens in nature, always, is that warm things spontaneously cool off and cold things spontaneously warm up.
When they're brought together but isolated from the rest of the world.
It's a law of nature.
It's the second law of thermodynamics that if you've got an isolated system and one part of it is hot and the other part is cold,
that the hot part will cool while the cold part warms up until you reach equilibrium.
Now think about that for a second.
It's a new law of nature.
It may be obvious, it's obvious from our experience,
but it's useful to recognize the fundamental nature of obvious things.
Why couldn't it happen that you've got two pieces of an object, maybe two buckets of water,
and maybe one of them is a little cooler than the other,
and the cold one just gets colder still,
and the warm one just gets warmer still,
because of their spontaneous contact with one another,
why couldn't you have the cold get colder and the hot get hotter?
It could still conserve energy.
If the total thermal energy of the cold one decreases,
and the total thermal energy of the hot one increases,
you could have the amount balance and preserve the first law of thermodynamics.
The fact that it doesn't happen is the second law.
Carnot recognized this,
and his brilliance was that he recognized there were many concrete consequences of this.
One of the consequences was that if you're running a heat engine
and you've got a certain temperature of the hot bath,
and you've got a certain temperature of the cold bath,
that you will be limited in what fraction of the thermal energy that's stored in that hot bath,
what fraction of that can you get out?
What's the maximum possible efficiency?
And Carnot used logic and mathematics and argued that the maximum depends only on the temperatures.
So that's the best efficiency you could ever come up with.
We're talking about the perfect design, no friction, no internal problems, no poor design,
and even in this ideal case, the maximum possible efficiency will be limited.
And it's a simple formula.
The efficiency basically depends on the ratio of the cold temperature to the hot temperature.
So if you start off with boiling water, that's a steam engine,
so the boiling water is at 100 degrees Celsius,
and it's running some sort of cycle, you're pushing on pistons because they're hot now
and they're expanding gases and they're pushing on iron rods,
and then you've got to cool the gas back down again in order to repeat the cycle.
You've got to always have a cycle if you want the engine to keep running,
and that's part of Carnot's insight.
And if you look at the temperatures involved and you look at Carnot's formula,
which I haven't derived for you, but I'm just sort of stating,
the maximum possible efficiency is very depressing.
For a steam engine dumping to room temperature,
the best efficiency you could get is very roughly rounding 20%.
For every useful jewel, you are throwing away another three.
That seems very painful, and at first the engineers working on steam engines said,
oh, I realize that we're right now only at 2%, but we're not shooting for 25,
we're shooting for 100, or let's be realistic, 99.99.
But they weren't being realistic because they didn't recognize that Carnot's law is not an approximation,
it's another one of those fundamental laws of classical physics.
And in the end, if you think about the world being made of atoms,
we'll talk a little bit about this, the statistical mechanical view of this thermodynamic story
makes the story even more rigorous.
It's really a law of nature that arises from counting how atoms rearrange themselves
and how they're moving, and there's no way around it.
This is not a law of physics that you can struggle out from underneath any more than conservation of energy.
It's a statement about how the world works,
and so not only should you not buy a free energy machine that doesn't take any fuel
but produces some sort of output, but you shouldn't even invest in a machine
that gets 90% efficiency by burning something.
Because if it's burning something, it's a heat engine,
and if it's a heat engine, you can't possibly get 90% efficiency.
Mr. Carnot's formula tells you that to do so,
your hot temperature would have to be so hot that ordinary objects would be vaporizing.
You couldn't make it in any practical sense.
So yes, you can increase the efficiency of engines,
but to do so, you have to make the temperature difference much, much bigger
than you normally would in ordinary working circumstances.
The second law of thermodynamics can be thought of and stated in many different ways.
It's one of the pleasures of studying thermodynamics as a career
to begin to realize how all these different ways of thinking about the second law
all hook together.
Some of them seem intimately related, and some of them seem really different.
The one I'm about to tell you is a new concept that's a way of reframing
the same second law of thermodynamics, and it's the concept of entropy.
So entropy is a number. You can measure it.
If you've got a pot of water, it has some entropy, and you can measure the energy.
Now, energy, of course, is also a fairly abstract concept.
How would you measure the energy?
You would need to really have some deeper understanding of what's going on
and what energy means.
Energy tells you how much work you can do.
So measuring energy is a tricky business,
and measuring entropy is an even trickier business.
Now, what is entropy measuring?
Well, in the early days, in the post-carno days,
people were beginning to recognize that there is this number,
and it's associated with systems, and it changes.
As you do things, you can increase the entropy,
or you might decrease the entropy,
and people came up with some thermodynamic rules
for measuring how much the entropy changed over time.
Ultimately, the statistical mechanical worldview
taught us a way of thinking about entropy microscopically.
It's a measure of randomness.
It's a measure of order.
So the more orderly something is, the less random it is,
the less entropy it has.
And the more random something is, the more entropy it has.
It's a quantitative measure of randomness.
What do I mean by randomness?
Well, microscopically, I kind of mean counting
how many different states are available.
So states would be different configurations.
So one way that you could give a pot of water
more states available to it would be to warm it up,
because now the molecules are jiggling around faster,
so they have more opportunities in their life.
They can go a little bit faster or slower.
There's more options for them.
And, of course, you think about them jiggling around more,
and so this idea of randomness is nice and deeply intuitive.
So adding energy is one way of increasing the entropy,
but it's not the only way.
Energy is not the same as entropy.
If you think of little molecules flying around in a chamber,
if the chamber is bigger and those molecules
have more space to run around in,
they've got more options available to them.
It's more random for them to be spread out
over this bigger space.
So there's alternative ways of thinking
about the microscopic details of entropy.
The second law of thermodynamics
is just a statement of probability.
So it's a counting law of nature.
And let me give you a concrete example
so that we can think about it.
Supposing I've got a deck of cards.
You buy it and it comes highly ordered.
It comes when you buy it from the store,
you take off the plastic, it's ace through king of one suit,
ace through king of the next suit, right, all four suits.
It's ordered.
You can predict by looking at one card
what the next card would be.
That's part of what you mean by an orderly system.
And if you were to now shuffle that deck
or just drop them on the floor and pick them back up again,
if you do something with the deck,
then they will become more random.
That's what spontaneously happens.
And that's the second law of thermodynamics.
The second law of thermodynamics is
if you just let systems evolve
and if they're isolated,
then as time goes by,
either the entropy will stay the same
or it will get bigger.
Things get more random over time,
but they won't spontaneously go and decrease in entropy.
So if you take that deck of cards and you shuffle it once,
so first of all, if you don't shuffle it at all,
then there's no change in the entropy.
You can sit there and hold it.
You can move the deck around without mucking up the order.
It doesn't have to increase.
But as soon as you shuffle it,
the first shuffle, the entropy increases somewhat.
Now it's much more difficult
to predict what the next card will be from the previous one.
If you look carefully after one shuffle,
you'll still see some order,
but there'll be little patterns, 2, 3, 4, and then 8, 9, 10.
And after a couple of shuffles, it's really random.
And from then on, shuffling more and more and more
won't increase the randomness.
It's really fully shuffled.
But what won't happen, this is the second law of thermodynamics,
what will never happen in your lifetime,
is that you take a random deck, shuffle it,
and you look at it and you go,
huh, ace through king of spades,
ace through king of clubs in order.
It's not going to happen.
Why won't it happen?
Well, because it's probabilistically
insanely unlikely to happen.
You can count how many states are available
with 52 distinct cards,
how many different orderings are there.
And that's a fun little statistical calculation.
And you can write it down.
There's a lot of zeros in the answer to that question.
And the probability that a random reordering
will put it into this ordered state
is basically one out of all the many possibilities.
Only one is the ordered state.
And so the probability of that happening
is one out of this astronomically huge number.
It's so improbable as to be practically impossible.
Now that's the case with 52 cards.
If we go to one direction, one extreme,
if we only had a couple of cards,
so suppose you've just got ace 234,
and you shuffle them, it's going to get more random,
maybe ace 4, 3, 2, and then 2, 4, 3, ace.
If you shuffle enough times,
sooner or later you might get back to ace 234,
because there's only 4, so the probability is small,
but it's certainly not zero.
With 52, you could shuffle once per second
for the rest of the lifetime of the universe,
and it's not likely to happen.
But now imagine going not from 4 to 52,
but from 4 to 52 to a million, billion, billion,
because that's a pot of water.
The thermal motion, the random motion of the molecules,
is really shuffling the deck.
It's shuffling the ordering of the molecules in the water,
and now instead of 52, you've got a million, billion, billion,
and the number of states available,
just like the number of states is much more than 52,
there's not 52 configurations of cards,
there's countless billions of billions,
well, it's countable, billions of billions,
and the number of configurations of the molecules
is an insanely large number.
If you tried to write it down,
you start writing zeros, and you can walk all the way
to the edge of the universe, and you're not done yet.
So what's the probability that those will go
into a more ordered state?
The answer is zero.
That's the second law of thermodynamics.
If you think about this, you can go back
and realize that this really is connected
with the way we were thinking
about the second law of thermodynamics,
which was Mr. Carnot's argument.
The first statement that I made was,
thermal systems, when they're isolated,
always tend to go from hotter things get colder
and colder things get hotter.
I can understand that if I'm thinking about entropy,
because spontaneously, if the colder thing
kept getting colder and colder,
it would be getting more and more orderly,
and it would go to perfect order
if this kept on happening.
Now it is true that the hot thing
would be getting more and more random,
and so you might ask, well, could the increase
in entropy of the hot thing
more than balance the decrease in entropy of the colder thing?
So it becomes a technical counting story.
You have to think about how many states are available,
depends on the temperature,
and what you'll discover is that Carnot is absolutely right.
It's pure mathematics.
It's derivable.
Once you think about thermodynamics in this way,
once you think about the second law
as a statement about systems and their natural evolution,
you realize that there's energy and there's energy.
Some energy is more useful than other forms,
and what do I mean by more useful?
Now we can be rigorous.
More useful means less entropy.
If you've got some material and it's got energy stored
and that energy is very orderly,
for instance, an automobile that's traveling down the highway,
every molecule is going in exactly the same direction
with the same average speed, it's the speed of the car.
That's about as orderly as you can get it,
and that's a very low entropy state
for that amount of energy to be configured in.
So what's going to happen over time?
Well, spontaneously that car might crash.
If it crashed, then that energy would be conserved,
but it would now be random thermal energy.
The car is stopped and now all the molecules are jiggling faster.
So you've taken that same amount of energy
and it's spontaneously turned into a more random distribution.
That's the way things go.
What never happens in nature, it's the second law of thermodynamics,
is that a car crash spontaneously undoes itself,
the metal reforms into a more orderly car,
and then all the thermal energy from the crash
gets converted into kinetic energy of the car.
It won't happen.
And that's the second law of thermodynamics.
Once you understand this,
you recognize why there is a maximum efficiency for an engine.
Imagine that you've got the hot gases in your automobile engine.
And so you've got some energy in there.
Every molecule is jittering around fast.
There's plenty of kinetic energy.
If only you could extract that energy
and put it into your car instead of it being random in the molecules,
that would be great.
That would be how you drive your car.
And you can do that.
That's what the car engine is doing.
But you can't take all of the energy
and put it into motion of your car
because that would violate the second law of thermodynamics.
Think about it.
If you started with purely random energy,
it's got a high entropy.
And if you could somehow convert it all into kinetic energy,
then you have just decreased the entropy of your automobile.
Radically, you started off with high entropy.
You spontaneously, through some workings of a magical engine,
converted into a low entropy system.
It's never going to happen.
There is no such magical engine.
So the second law of thermodynamics says,
yes, you've got that energy
and you can't convert it all into mechanical energy.
You have to throw some of it away,
increasing the entropy of the cold bath.
And that will increase in entropy enough
to cancel the small decrease that you get
by also ordering the molecules in the car.
So there's this balancing act,
and the second law of thermodynamics tells you quantitatively
how much is the best balance you can get.
That's Mr. Carnot's original formula,
where he wasn't thinking about randomness.
He was just looking at the steam engines themselves.
There's a very common misconception
about the second law of thermodynamics.
So many people misread it,
and they miss that part, that caveat,
for an isolated system.
If you don't read that, then you say,
oh, the second law of thermodynamics says
that entropy can never decrease.
If you thought that that's what
the second law of thermodynamics says,
then you'd become very confused,
because if I handed you a deck of cards that was random
and you sit down for a few minutes,
you can order them,
you can decrease the entropy of that deck of cards,
and you would say, hmm, is that a violation
of the second law of thermodynamics?
And the answer is no, it took some work.
You are not...
The cards are not an isolated system.
They're not spontaneously reordering.
You need to do some work on them.
You can locally decrease entropy.
Put the water in the fridge, in the freezer,
and they will spontaneously, by themselves,
without you doing anything further,
get colder and colder and then freeze.
As they get colder,
they're getting less entropic, less random.
When they freeze,
the entropy is going down even more,
because now they're very ordered.
There's a crystal. If you know where one molecule is,
you know right where its neighbor is.
It's a very orderly state.
Did that happen spontaneously?
Well, yes and no.
Inside of the freezer, it happens spontaneously,
but look at the system that it's part of.
You have to plug the freezer into the wall
for this to happen.
If you don't plug the freezer in,
it's not going to just happen by itself.
You need some external energy.
Some people have argued that human beings
are low entropy states,
because we're very ordered.
We've got organs that are separated,
and if you think about the chemicals
that made up your body,
if those chemicals were all randomly separated,
they would have much higher entropy.
So that's correct.
When your mom made you,
when her body created you,
she was taking chemicals
and ordering them
into the shape and form of a human body.
So entropy was decreasing
in your location,
but mom was eating.
She was metabolizing.
She was consuming energy from an outside system,
and she was throwing away a lot of waste heat.
In fact, she was warming up the environment
that she lived in,
and the entropy of her surroundings
vastly orders of magnitude more
than the decrease that you represented.
So yes, you can decrease entropy here
as long as you increase it somewhere else
so that the sum total is always going up.
Now this has led some people,
philosophers largely,
to think about time.
Time is connected to this story,
because as time goes by,
entropy is increasing,
and people have often puzzled about why,
what do we,
how do we think about time always moving forward?
The arrow of time points in one direction.
The future is very distinct from the past.
If you look at Newton's laws,
you don't have any clue about why time
should go one way and not the other.
Think about watching a movie of billiards,
perfect Newtonian motion.
They come in, they interact,
they go back out again.
Conservation of energy, conservation of momentum.
You can't tell if they're running forwards in time,
or if you run the movie backwards and they're going backwards.
Both look perfectly reasonable.
So Newton's laws don't seem to tell us
which way time should go,
but if you watch something more complicated,
something where thermodynamics is involved,
like you drop an egg,
and it hits the ground and smashes,
you know very well if you run that movie backwards
that you're looking at nonsense.
Eggs don't spontaneously start
in this random mushed up state,
takes an energy from the floor,
cooling the floor a little bit,
taking that energy to rebuild chemical bonds
to make an egg shell.
Oh, and let's use a little bit of that energy
into the form of kinetic energy
to make it fly up into your hand.
It won't happen.
So the arrow of time points in the direction
of increasing entropy.
Just a fun philosophical point to think about
when you're thinking about this entropy story.
In the end, we've got a new law of thermodynamics.
It's important you can't understand heat engines
if you only think about energy conservation.
You have to think about entropy and randomness,
or the second law of thermodynamics,
in order to fully understand the complexity
and the reality and the ordinary behavior
of any thermodynamic systems.
Buster 24.
The grand picture of classical physics.
Classical physics,
the topic we've been studying throughout this course,
was established,
was introduced by Isaac Newton in 1687
by the publication of the Principia.
And it's been undergoing a pretty continuous development
and evolution ever since then.
I think it's very productive and useful
to step back at this point
and revisit the question of what is classical physics?
How do we define and understand
what it is that we've been studying?
One of the ways that you might choose to define it
is it's the physics that was done
starting with Isaac Newton,
working your way up to, oh, you pick a date, 1900.
I don't think that's the most productive way
of thinking about it,
but classical physics continues to evolve today.
There are people whose career is studying
and thinking about classical physics.
1900 introduced some new ideas,
but it certainly wasn't the end
or even the change of classical physics.
I think it's probably more useful
to try to define it in terms of the topics,
the things that it's thinking about.
Classical physics is about mechanics.
It is the study of motion, force, energy.
It's trying to understand objects,
material objects, particles,
how they move, why they move that way.
As we saw, it goes beyond just particles.
We can study forces of nature,
gravity, electricity, magnetism,
and then we can think about optics
and waves and heat and atoms.
All of these are individual topics,
and that pretty much spans the set of topics
by and large in classical physics.
You could argue, though, that there are other ideas out there,
classical ideas, which we haven't specifically talked about.
What about fluid flow?
Is that classical physics?
Sure.
Fluid flow is something which we didn't specifically address,
in part because we don't need to.
If you want to understand how fluids work,
you have all the building blocks.
It's really about energy and force and particles moving.
And so, even though there have been new specific topics,
the broad underpinning ideas of classical physics
once established form this framework that we're using
to understand a wide set of questions about the world.
And that really is what classical physics is to me.
It's the physics of the world we live in.
You look around you and you try to understand.
You ask questions about anything that's measurable
in the world we live in.
You look at cars or bicycles or rocket ships.
You look at sports.
You think about architecture.
And these are the everyday experiences,
the world that we live in, that classical physics
is helping us to make sense of, to understand.
Even that as a definition, though, is too limited.
Classical physics goes beyond ordinary human experience.
We can understand planet Earth, the entire planet,
and the solar system, and the galaxy,
the Milky Way that we live in,
and even clusters of galaxies all using classical physics.
So it goes far, far beyond ordinary human life experiences.
And it goes in the other direction too.
We can understand tiny particles.
Look at the smallest grain of dust or sand.
We can describe and understand its motion and behavior
by using classical physics.
But you can take out the microscope.
You can start looking smaller and smaller.
You can go down to distance scales of bacteria
or millions of an inch, even smaller still.
And again, classical physics still works just fine.
Newton's laws, energy, conservation of energy, momentum,
these principles determine the properties of the world
that we live in, even down at these distance scales
very far from our everyday experiences.
So you might think about the applications.
Is that what determines classical physics?
Classical physics is still taught and learned
by not just physicists, but all scientists,
biologists, chemists, geophysicists, astrophysicists,
and even people who go into fields
that really don't look like scientific investigations directly.
I'm thinking doctors, archaeologists, architects.
All of these people are using classical physics
and applying it to a variety of experiences
at many, many different scales.
And it's enormously productive.
It's the way we think about the world that we live in.
Isaac Newton commented that if he has seen further,
it's because he stands on the shoulders of giants.
And maybe what I'm arguing is that classical physics
is the giant on whose shoulder we stand today.
As we move into new realms of study,
into modern physics or contemporary biology
or any of a number of modern disciplines,
we are still using classical physics.
We're thinking about it.
It's a vantage point from which we can continue
to explore the world that we live in.
So maybe that's another way of thinking
about what classical physics is.
Rather than looking at what it's studying,
maybe we should think about how we study these things,
because that's part of, it's a deep part of what we mean
when we talk about classical physics.
It's really a philosophical approach.
It's a scientific philosophy for studying things.
If you're a classical physicist, you say,
the world is out there, it's real.
Human beings are learning about the world,
but the world we're learning about exists independent of us.
That's a philosophical idea,
and you can challenge it if you wish.
There are people who make arguments,
very compelling arguments, about this topic.
But it's definitely part of what we mean
when we're thinking as classical physicists.
The earth goes around the sun.
It's a big, solid object, and it's out there,
even if you're not looking at it,
even if for, you know, in some scenario
there was no human beings around,
earth would still be there,
and it's still going around the sun.
Of course, the language that we use,
describing orbits and the formulas and Newton's laws,
that's all definitely about human beings
and our description of this reality.
But classical physics is really arguing
that the real world is out there,
and that's the goal of classical physics,
is to learn something about that real world.
We do it with experiments,
and we do it with a development of theoretical frames.
We're trying to come up with a coherent,
unified understanding
of as much of this external reality as we can.
Classical physics goes beyond
just accepting the reality of planet earth.
It's also a belief
that complicated things can be reduced
to the simpler components.
I think that's a big part.
It's not an absolute necessary part of classical physics,
but it's deeply ingrained in much of the ideas
and approaches of classical physics.
It's a reductionist idea
that if you look at something,
something really complicated.
Look at the sun with its enormous number of constituents,
and it's very hot,
there's a lot going on in there.
You could look at a human development,
like a nuclear reactor or a CD player.
And again, very complicated object,
many parts.
It's got this complex function and behavior,
and you might despair and say,
it's just too complicated.
It's essentially magic, and I give up.
But the philosophical approach of classical physics
is to say, oh, we can understand this,
and we can understand it by breaking it down.
What is it made of?
What are the fundamental underlying ideas,
and how do they fit back together again
so that we can understand this complicated thing?
If you look at a nuclear reactor,
you might say, oh, that's something totally new.
We haven't talked about that in this course,
so that must be beyond the reach of classical physics.
But it's not.
It's very much a classical device.
It's just straight thermodynamics.
Instead of burning coal,
you have some hot uranium.
But other than that little detail,
which really is, in many respects, just a detail,
it's a heat engine.
You've got a hot spot, you've got a cooling pond,
you've got a working fluid,
which is probably just steam in a nuclear power plant,
and you can make sense of all the complicated procedures
and outputs and behaviors
just by understanding the classical thermodynamics
that we've been talking about,
studying, learning about in this course.
There's another aspect of classical physics,
another philosophical idea,
that the world is deterministic.
I'm going to argue that that's related to,
but a little bit different from believing that the world is real
and believing that the world is reducible to simpler ideas.
Once you've done this reduction,
it's the idea that we can make predictions,
quantitative and qualitative.
We have an understanding of the world
if we can make predictions
about what's going to happen in the future.
It's like the universe as a whole is some giant clockwork.
That's part of what it means
to be thinking about classical physics.
Now, I don't mean it literally.
The earth is not ratcheted and connected to the sun
in some mechanical way,
but nevertheless, this belief that the world is deterministic
is there is a sort of a clockwork nature to our behavior.
So I can predict where the earth is going to be
in six months or even 60 years or 6,000 years.
I can predict eclipses.
I can predict just about anything that you want to know
about the planets in the solar system
and a huge variety, of course, of other things
because classical physics says the world is deterministic.
And it works backwards, too.
I can tell you where the planet earth was
and where the planets were long before there was any human beings,
certainly long before there was any Isaac Newton,
by subscribing to the ideas of classical physics.
Ultimately, classical physics postulates a small
and very cohesive set of underlying ideas.
There's this tapestry of ideas that we've constructed,
and we've really looked at the picture that was formed.
We started with the fundamental ideas of position and time.
These are underpinning ideas.
Isaac Newton made it fairly concrete.
He talked about the sorts of operational definitions
that we need to talk about more complex ideas,
like velocity and acceleration,
but they really boil down to understanding just space and time.
And then we talked about some fundamental dynamical ideas,
dynamics being the explanation of motion.
We talked about inertia as a concept and mass,
which is a numerical or quantitative experimental measure of inertia.
We talked about force. It's a primitive concept.
Primitive meaning it underpins our theory of the world.
And once you accept this idea of force,
then Newton's laws follow from experiment and we're often running.
We understand the world based on these basic ideas.
And other ideas that we introduced, like momentum and energy,
are defined in a certain sense in terms of these underlying ideas.
Momentum is mass times velocity.
Kinetic energy, one half mass times velocity squared.
We've already talked about mass. We've already talked about velocity.
Now we're just building up these more complex
and rich classical theories of the world.
We introduced some laws of nature.
We talked about not just Newton's laws, but conservation laws.
Conservation of momentum, conservation of energy.
We didn't talk too much about conservation of angular momentum.
We talked about it briefly.
It's the idea that a spinning bicycle wheel,
which is standing on an upside-down bicycle,
so the bike isn't moving. It's got no momentum.
But the wheel has some kind of momentum.
It's an angular or rotational momentum.
That's another one of those fundamental ideas
which help us to describe and understand the world around us.
Conservation of electric charge,
one of the last conservation laws that we've talked about in this course,
and basically putting those together,
along with Maxwell's equations,
which describe electricity and magnetism,
we really have a description of the world.
That's our classical world view,
along with the philosophical approach, the scientific method.
There were a few other ideas, the idea of atoms,
and then the consequences of there being atoms,
thermodynamics and statistical mechanics.
That pretty much outlines classical physics.
And with these ideas,
we understand a great deal about the world we live in
and beyond in all directions.
Classical physics wasn't the end of the story in a lot of respects.
So first of all, there are people today
who continue to study these ideas and develop them farther.
So there are now new branches of classical physics.
Almost any good high school science fair project
is likely some experiment where a young person
has gotten curious about the world they live in.
They're trying to investigate something about the world,
and they're probably using the methods of
and the ideas of classical physics and extending them.
Sometimes people discover very, very new things,
new applications, new deep ideas,
all built on this framework of classical physics.
It's a nice idea that when you're studying the world,
you are really asking questions, getting curious,
and then in the process of using classical physics,
not only are you answering these questions,
you're posing more.
Any good science fair project asks more questions than it answers.
And that's, I think, one of the ideas behind classical physics
that we retain to this day.
It's part of how we think about science and the world that we live in
is that we're constantly learning,
and we just have this useful framework to help us,
but there's always more, there's always new ideas
even within the realm of classical physics.
We have moved, in some respects, in the 1900s,
in the 20th and now the 21st century,
into a realm of modern physics,
which in some respects is just extensions of classical physics.
In some respects, it's a shift in the way you think about the world.
I'd like to spend some time in this last lecture
looking a little bit at some of the key developments of modern physics,
just an overview so that we can compare and contrast
what's the difference between modern physics and classical physics.
It might help us to understand both the usefulness,
the power of classical physics as it still continues today
by seeing what's different about modern physics.
So for instance, Albert Einstein,
he's really the hero of modern physics,
and Albert Einstein was in many respects throughout his life,
very much a classical physicist.
He looked at these old ideas, respected them,
and in many cases believed them so strongly
that he was able to build them farther.
One of his earliest examples was the theory of special relativity.
Now you might think that's a whole new branch of physics
that has nothing to do with classical physics,
but on the contrary, it was really developed by Galileo.
Galileo invented relativity.
Galileo, or at least he's one of the inventors of relativity.
Galileo, we talked about this.
He argued that there are laws of nature out there,
and we are learning about them,
and those laws of nature are, to a certain extent,
independent of us humans and who is observing them.
This was a very big and important idea in classical physics.
It says that I can observe an experiment
and you, an independent observer,
can be moving past me with a constant speed and straight line.
We're in different reference frames.
We make measurements.
We disagree on many things.
Speed is relative.
Velocity is relative.
Position is relative to the observer.
But the laws of physics are invariant.
They are the same.
We both agree, f equals ma.
Albert Einstein thought about this,
and he believed it.
He understood that this was a statement,
a profound statement about the nature of the world we live in.
So he didn't change that idea.
He also looked at Maxwell's equations
and said these two, these classical ideas,
are very deep, very profound,
and they seem to be matching experiments so well
that they seem to be telling us something, again,
true about the world that we live in.
Now, if you combine Maxwell's equations
with Galileo's principle of relativity,
you discover something very interesting,
very radical at that time.
Maxwell's equations tell us that the speed of light is a constant,
independent of the observer.
So the speed of light is itself a law of nature.
You might just accept that as a fact,
but you've got to think about it.
It's weird.
I just said speed is relative to the observer.
Imagine if you're running away from me at, oh, very, very high speeds.
You're running away from me at three-quarters the speed of light.
And I shine a flashlight beam in your direction.
I measure the speed of that light beam to be the speed of light,
300 million meters per second.
Now, you are running away from the flashlight,
and you turn around and you do some experiment
to measure the speed of light of that beam.
What do you get?
If life was simple and classical,
then you would get a reduced speed.
If you're running away from a thrown ball
and you measure its speed, it seems to be approaching you more slowly.
You measure its speed to be reduced, but not light.
Light is different.
It's a law of nature which Einstein recognized
that the speed of light is the same to all observers.
Now, that's difficult to make sense of.
And ultimately, in order to make sense of it,
what Einstein realized was it's not relativity that's wrong.
It's not Maxwell's equations that are wrong.
It's our kind of intuitive understanding of space and time
that needs to be revisited.
Newton believed that space and time were rigid and fixed
and universal and completely independent of the observer.
It was a natural belief.
It forms a core idea in classical physics.
And that was what Albert Einstein realized needed to be tightened up.
We need to make operational definitions,
obeying the philosophy of classical physics in practice
by trying to define precisely in the laboratory
what do we mean by time and time intervals.
Once you do that, you realize that certain aspects of Newton's laws
have to get fixed up.
We have to rethink our definition of time and space,
which we barely thought about in this course
because we just accepted them as underpinning core principles.
And so modern physics simply questions the definition of those underpinnings.
And so it addresses questions that go not...
It doesn't eliminate Newton's laws.
They're still quite correct.
It's just asking questions about where they come from.
What's the core of classical physics itself?
If you think about relativity,
it changes some of your ideas about space and time.
It requires you to redefine momentum.
It's not just mv anymore.
You have to fix the formula up.
Now, does that make Newton wrong?
Not at all.
When you fix up the formula,
you still have effectively mass times velocity.
You still have absolutely conservation of momentum.
It's just that when objects are going at this extreme speed
near the speed of light,
that you need to make subtle changes
in the way you think about what momentum means,
what kinetic energy means.
So it is very important if you're worrying about objects
that are traveling nearly 300 million meters a second,
which has essentially no bearing on anything in our ordinary lives.
It's nice to know about.
It's a shift in your philosophical underpinnings.
Einstein's theory of general relativity did, again,
a similar thing.
Remember, Isaac Newton has written down the fundamental law of gravity.
And we've talked about it and we've understood how useful it is.
And Einstein didn't throw that away.
He, again, asked, where does it come from?
How do I make sense of gravity itself?
In classical physics,
we just sort of shrug our shoulders and say,
it is what it is.
It's the force that attracts the Earth to the Sun.
Once we accept that, then we're off and running.
We describe the planets and the tides and the eclipses
and on and on, right?
Everything you can think about.
Rockets to Mars.
Albert Einstein says, yes, that's all great.
I'm not going to throw that away.
But what I'd like to know is why?
Why is there gravity?
So it's a delicious question.
In the old days, it might have been a philosophical question.
And Albert Einstein turned it into a measurable question.
And so the general theory of relativity now reimagines space and time
in a geometrical sense and lets us have this new idea
of what gravity really is and where it comes from.
It's fun to learn about.
And I encourage you, after having studied classical physics,
to study some modern physics and learn about the underpinnings.
One of the directions that you might go if you push on classical physics
is to ask, OK, we started classical physics at its core with atoms.
We postulated that they exist.
They're real.
They're out there in the world.
And once we believe in them, we can understand colors of objects
and textures of objects and behavior of objects
and thermodynamics and heat engines, all of that stuff.
And none of that goes away when you ask the question,
what is an atom made of?
It's just a delving deeper.
You're asking, OK, what is that atom?
How do we understand it?
And when we investigated that, and again,
this is the field of quantum physics which began around 1900.
That's why I set 1900 as the shifting point
where we began to ask some new and different questions.
Quantum mechanics asks about the electrons inside of the atom.
And here again, we discover that Newton's laws are insufficient.
They are inadequate to go down to distant scales of billions of a meter or smaller.
And when you start investigating the subatomic structure,
you discover to your surprise that not only do Newton's laws need to be fixed up,
and that's the laws of quantum mechanics.
So Newton's laws are classical mechanics.
And so you reformulate the laws of physics.
You also begin to ask questions about these assumptions that we've been making,
assumptions like determinism.
In classical physics, it's taken as essentially a philosophical article of faith
that if you know enough about the world today,
you can make a quantitative prediction about the world tomorrow.
Now classical physics recognizes that sometimes it might be complicated,
like the weather.
But nevertheless, it's a belief that we can predict the weather
and we do a pretty good job, certainly for tomorrow,
and even two or three days down the pike.
And that's better than we used to do,
and you can easily imagine that 20 years from now
we will have even better long-range forecasts.
But if you're looking at a subatomic particle like a radioactive nucleus,
and we understand that radioactive nuclei decay,
and you ask, when?
When will it decay?
What's going to happen in the future?
It's kind of interesting.
Quantum mechanics says, we don't know.
And it's not that it's just difficult to calculate or complicated.
It's that nature itself doesn't know.
It's not determined today when precisely that nucleus will decay.
It's a wild idea.
And it turns out that we can describe nuclear decays very, very accurately on average.
I can tell you if I've got a million of them,
how many will be left tomorrow and a million years from now.
But I can't tell you specifically which one.
It's like the insurance company,
who can tell you very, very accurately what human life spans are,
when people get married, and so on.
But they can't tell you about you.
And so quantum mechanics has shifted a little bit
the way we think about this fundamental philosophical point.
Maybe the world isn't fully deterministic.
Now again, you can ask the question,
does learning that in the early 1900s,
does that throw away all these wonderful ideas of classical physics?
And my viewpoint is not even remotely.
Classical physics describes the world we live in as accurately today
as it did in the 1600s when Isaac Newton was first making sense of it.
Again, it's the underpinnings down at these levels
that are far, far beyond our ordinary experiences.
And it's useful to know about it.
It opens up new branches of physics when you start asking these questions.
So classical physics will not explain or predict a laser beam.
A laser beam really arises from the quantum mechanics of electrons inside of an atom.
On the other hand, classical physics does a perfect job
of explaining how that laser beam behaves.
It behaves just like Isaac Newton thought.
It goes through lenses.
It focuses.
We understand the color.
We understand how to build a CD player with laser beams
or the scanner at the grocery store with laser beams
because ultimately the macroscopic phenomenon of laser light
is just classical physics again.
Predictable, understandable.
It's only when you ask that deep, deep question
about where did it come from that you might need and want something new.
So in this respect, modern physics has built on classical physics.
It doesn't throw it away.
It enriches it.
It deepens it.
It starts to take that framework, that cathedral,
and it now builds something underneath it,
some even deeper layer still.
And we may not be done.
Classical physics continues to describe aspects of the practical world.
Quantum mechanics continues to be studied and deepened.
It's not necessarily clear that there is an end point.
Classical physics has led us to believe that science is an investigative process.
We're trying to make sense of the world that we live in.
It is absolutely, this is my opinion,
one of the most fruitful, powerful, and productive discoveries,
ideas in the history of human civilization.
It has had enormous impacts.
The world that we live in, the technological world that we live in,
all the aspects of our lives are built upon and center on classical physics.
So we study it not because we're interested in history,
but because it really is telling us something very useful
about the world that we live in and the things that go on around us.
If you want to understand how a baseball behaves in a baseball stadium
or how the electricity for your house is generated
or how your microwave oven works,
these are ideas that can be understood from classical physics.
It's a tool.
If you leave this course and all of a sudden you realize
that there are some important political issues
involving energy in the environment,
you should recognize that the tools of this course are sufficient for you
to understand the scientific questions.
Now, that's only one piece of the story.
You have to think about the politics and the ethical issues involved,
and classical physics may or may not be helpful for you to move in those directions,
but it's enormously helpful for you to be able to separate science
from wishful thinking or from inaccurate thinking.
If you're thinking about limitations,
if you're thinking about environmental consequences,
thinking about energy, conservation of energy,
the definition of power and its relationship to energy,
very, very fruitful.
My hope is that by the end of this course,
the energy that you've put into it has changed a little bit
the way you think about the experiences that you have.
Go out and think again when you're driving your car,
when you're walking, when you're riding your bicycle,
when you're looking at toys,
when you're just thinking about the world around you.
I hope that this inspires you to continue learning.
There's an awful lot more that you can continue to study
in the field of classical physics alone,
and I thank you.
It's wonderful when people take the effort to make sense of the world they live in.
Thank you very much.
