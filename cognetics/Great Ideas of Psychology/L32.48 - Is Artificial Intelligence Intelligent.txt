Lecture 32 is Artificial Intelligence Intelligent
Well, I think it's time to examine the artificial intelligence agenda with a more critical and skeptical eye.
It's an extremely important thesis, the artificial intelligence thesis.
It's important at a variety of levels.
First, it requires us to identify ever more accurately and systematically just what it is about our intellectual and mental and moral lives that seem to make these lives so special and indeed beyond the range of simulation or duplication.
It also gives us a handy way of examining an ever greater detail just what it is we are doing when we are thinking and problem solving and attempting to perceive complex patterns and the like.
It also constitutes something of a test of the materialist thesis.
It would be quite informing to say no more about it.
It would be dazzlingly informing if indeed someone were to be able to come up with a device, no matter how complex, nonetheless a device, a collection of physical entities put together in such a way and functioning in such a way as to be able to handle the kinds of problems we take to be uniquely human, existential, intellectual, rational, political problems.
So this is a very, very important development at the level of theory, at the level of practice, as a kind of heuristic.
It's a very useful heuristic for thinking about cognition and problem solving and the like.
So we have good reasons to examine the thesis and examine the thesis fairly closely to see whether it might be grounded in some misconceptions, mistakes, and the like.
Well, artificial intelligence.
Should we put the stress on the adjective?
Is it utterly artificial when you look at it in the right light?
Is it something far less intelligent than something quite conspicuously and artificially intelligent?
Critics of the AI thesis come in a variety of sizes and shapes and attack the thesis from a variety of perspectives.
I don't think any of the blows delivered to date is fatal, but I do think some of them get very, very close to being fatal.
And this, I suspect, depends finally on your view of reality and of the universe.
In my quiet and entirely private meetings, I regard these blows as fatal, but for public consumption, of course, may the debate go on.
Let's begin with Goethe's theorem, which has been used by some critics of artificial intelligence as an instance of the entire AI program being based on a kind of mistake.
Now, this can be fairly heavy weather, and I don't plan to make the weather any more insilubrious than it is already.
But let's begin with Kurt Goethe, who in the first third of this century made a profoundly important contribution to number theory and to mathematics in general.
Goethe established, beyond dispute, that in any formal system sufficiently complex to generate an arithmetic,
the performance of that system, the achievement of the system itself, would depend upon at least one axiom that could not be established or approved or validated within the system itself.
That is, any formal system sufficiently powerful to generate an arithmetic would have a theorem or axiom that had to be found outside that system.
You could not establish it or validate it within the system, and thus we get what is referred to as Goethe's incompleteness theorem.
The incompleteness theorem, Goethe's theorem, the incompleteness theorem says that any formal system is incomplete in that it will be based on, it will require, it will depend upon a theorem or axiom, the validity of which must be established outside the system itself.
Goethe's argument is a formal argument, and it is true.
Well, what do we say about a computer? And what do we say about any kind of computational device that would qualify as intelligent in the sense in which the artificial intelligence community talks about artificial intelligence devices?
Surely they would have to be powerful enough to generate an arithmetic. They'd have to be formal systems with sufficient power to create functions such as addition, subtraction, multiplication, and division.
Now what that means, among other considerations, is that such devices would be limited by Goethe's theorem, do you see? That is, the operation of such a system, the very resources available to that system, would suffer from the incompleteness theorem.
They'd have to be based on, the operation would have to be based on something supplied from outside the formal system itself.
Now it has been argued that this will be true of all computational systems, all computational systems being formal systems of sufficient power to generate an arithmetic.
So, no artificial intelligence device could possibly be exempted from this.
Nonetheless, it seems that at least Kurt Girdel was exempted from this because Girdel developed his theorem. That is, Girdel was able to discover the incompleteness theorem, meaning there must have been something, I'm being intentionally droll here, there must have been something about Kurt Girdel that is different from a merely formal system with sufficient power to generate an arithmetic.
Which means there must be something about Kurt Girdel's mental life or intelligence, and if we may be so bold and perhaps vain, there must be something about human intelligence that liberates it from the very limitations supplied by Girdel's incompleteness theorem.
Which is to say there must be something about human rationality and intelligence, you see, that can't be captured merely by a formal system with sufficient power to generate an arithmetic.
Now if you accept that as a general proposition, then what you would have to say is that human intelligence cannot be mimicked or modeled on purely computational grounds.
That is, human intelligence is not exhaustively a computational process. Let me rehearse this. If it were exhaustively computational, then indeed it would exhaustively be a complex formal system with sufficient power to generate such algorithms as arithmetic, for example.
But if it were just that kind of system and only that kind of system, then indeed it would suffer from the limitations contained in Girdel's incompleteness theorem.
That would mean, among other things, that it would not discover an incompleteness theorem. That is, it couldn't discover that it was relying on something outside its own resources in much the way that fish will never discover water.
You would simply be so immersed in the process itself. You are constrained by the formalities of the system and could not get outside them.
So one argument against the strong AI thesis is that it's not a matter of time before it succeeds and redeems its promises.
It will never succeed and redeem those promises for the simple reason that the intelligence it seeks to simulate or model or duplicate is, in fact, not a computationally based or not merely a computationally based intelligence.
So this would be one fairly suggestive argument against the strong AI thesis.
Now coming at this from another direction is John Searle.
John Searle raises the interesting question, what is it an artificial intelligence system or device? What is it a computer is doing? What actually is going on?
Not in the sense of electrical discharges or printed circuits or the movement of ions or charge and the like.
But formally, what is it that's going on when an AI device is said to be engaged intelligently in some kind of problem solving?
And to illustrate what John Searle takes to be the utter vacuity of the claim that these operations properly understood are intelligent,
John Searle serves up what is now his very, very famous Chinese room.
Now what's going on in John Searle's Chinese room, John Searle says, is precisely what's going on in any computational system of the sort envisaged by the AI community.
Here's the situation. I'll give it to you in one of its more popular forms.
You go into a room and in that room you find a stack of cards and on each one of those cards is a Chinese ideogram.
Now as it happens, you know absolutely nothing about Chinese at all.
You've never studied it. You've never seen cards like this.
But there is a stack of cards and you also have printed out for you in English on a sheet instructions as to what to do.
Now the instructions take the following form and you can play this out.
You could have a Polish room. You could have a non-linguistic room.
You could have a room where there were various semaphores on the cards.
But we'll stay with the Chinese room.
Now the instruction sheet might say something like this.
Find a card that has two wiggly vertical lines resting on a straight horizontal line below which is around black dot.
Take that card and put it let's say on the extreme left part of the floor in the room.
So you go through this stack now and you turn these cards over and like and sure enough you find a couple of wiggly lines resting on a straight horizontal line.
There's a black dot underneath and you pick up that card and you go over and you put it in that corner.
You put it in the extreme left hand corner of the room as you are facing the room.
Now you go to the next instruction and it says all right now you ought to find a card that has an S like figure on it and a slashing diagonal line below which is a pair of black dots.
Take that and put it next to the first card.
Now you begin to see what you're doing here.
You're going to be thumbing through these things.
You're going to find a card that matches up with what you see on the instruction sheet and you're going to lay these cards out from left to right.
That's it.
You've now gone through all the cards and you've put them in precisely the right order and that's it.
You take a look and you've got all these squiggly funny dotted random forms lining the floor.
This John Searle says is precisely what is going on within a complex computational device.
What's going on here is this instruction sheet that tells you what to do functions very much like what in the old days we used to call a compiler program.
The compiler program is just the thing that lays out how the steps are to be produced in the right sequence.
If you take a look at this array of cards on the floor they mean absolutely nothing to you at all.
The room now comes a native Chinese speaker and looks at the floor and perhaps starts laughing because it tells a very funny joke in Chinese.
Or indeed it's the beginning of a very famous Chinese folk tale.
Or in fact it's the name of three or four members of one's family.
Or it's a description of certain certain birds found only in the north of China etc etc.
In other words nothing of an intelligible nature comes out of this until what has been compiled is now examined by an intelligent entity.
You have functioned we have functioned.
I certainly don't know anything about Chinese ideograms.
If I were to do this task I would be functioning as a device that simply looks for certain patterns and then follows a guidebook or a rule book on
just in what in just what order these things are to be positioned.
There's no element of meaning at all which means the essential feature of intelligence which Searle and other philosophers refer to as intentionality.
Intentionality here is not to be understood merely in the sense of I intend to do something intentionality rather in the sense of meaning something actually means something to me.
And the essential element of intentionality which Searle argues is at the foundation of all intelligent activities all bona fide cognitive thinking conscious activities that element is simply missing.
What you've got instead is a fairly accurate performance by a device you might view it as a kind of perception device capable of spotting patterns once told what to look for picking them out.
And laying them down someplace it's a kind of it's a pattern recognizing device that otherwise is entirely mechanical as I walk over to the room and put down a particular tile.
Now Searle wants to argue generally from this that in fact there is no way of representing intentionality within the program itself.
All you can do is get ever more sophisticated in the compiling aspect ever more sophisticated in the instruction sets so that indeed you can create in the external world every variety of thing.
But it will not become a meaningful thing until it is examined by the kind of entity that has bona fide intelligence and not this artificial compiler type pattern recognizing ability.
So this is another critique and this critique is a powerful one because it claims again as with the girdle's theorem argument not just that the A.I. community has the daunting task of coming up with very very powerful computers and very sophisticated software packages.
No no no that's not the problem the problem is that the one ingredient you must have in order for something correctly to be regarded as intelligent.
You will never have because computational processes are simply not the last word on what it is that intelligent beings do when they are behaving intelligently.
It's not the computational element it's the intentional element and no degree of computation per se yields that intentionality that necessary intentionality.
Now there's also a distinction to be made between following a rule and having a purpose and here we get intentionality in the more conventional sense.
When people tend to when people attempt to work through problems they are not simply functioning as problem solvers they in fact are they are not they are not functioning they are indeed self motivated and self directed.
What is it that makes a problem for Smith may not create a problem for Jones that is in any in any problematical area.
What is a problem for one becomes a problem because it has been selected as a problem.
It's incorporated into a particular form of life.
It has a certain meaningfulness to the one who identifies it as a problem.
There are people all over the world for example who are trying to put together software packages that will be the 46th best chess player in in Spokane.
And now people in the audience might regard this as a worthy enterprise or might regard it almost as a kind of autism that why would anyone devote a life to doing this sort of thing.
And the answer to the question is that one devotes one's life to doing things because in a very very unique way we are self defining creatures that single out as a worthy form of life things that other people might not thus regard as a worthy form of life.
So so just in coming into the room and putting the tiles on the floor we're displaying yet that other sense of intentionality.
There's a kind of striving here trying to do something trying to get it done.
Now this of course quite richly if I can use this term psychologizes the situation there's no reason to believe that an artificial device is doing any such thing.
But suppose this turns out to be one of the invariable marks of human and indeed generally animal intelligent behavior.
A striving the ancient Greek sense of protrepticus really trying to get something done not just getting it done but trying to get it done in the sense of the journey being everything.
And the destination almost being almost being despised because it puts an end to the quest it puts an end to the march itself.
Now this raises a quite general question about meaning about what makes something meaningful what makes a problem intelligible how we go about identifying things worthy of our intelligence.
And whether the A.I. community is is possibly on the right track in attempting to mimic or create something like this.
Where the meanings come from if I stand before a group and say that that I have this pair of inexpensive drugstore glasses which once you pass 40 years old you are inclined to need if you're not going to become illiterate.
Because your arms will never grow long enough for you to get that page far enough away to keep words in clear view.
If I if I say something like that if I say I'm holding a pair of spectacles how is it one possibly knows what I'm referring to.
You certainly not born with the word spectacle somewhere in your brain there are whole regions of the world in which nobody would recognize the word spectacle.
And indeed there are places in the world in which nobody would recognize eyeglasses by any name at all.
So the question becomes this how does an object come to be represented by a word.
And of course the answer to that question is by convention that is language users are the ones who determine indeed what words mean and what they refer to.
Meaning properly understood then is socially constructed not just in the sense that that that mother says to baby ma ma baby that that isn't quite enough in the actual practices in the world as we move around the world.
Something represents the word cup represents something and if it's going to represent something reliably it has to represent a receptacle in which fluid can be put for the purpose of consumption.
It can be rinsed out and used again it's sold in stores et cetera et cetera.
This is something as simple as the word cup when you now go down other kinds of words like playing chess.
What does it mean to play chess.
Is this just a constellation of correct moves or is there something in the very concept of playing that transcends the mere making of moves.
After all you could create a robot that would take things that look like horses carved and move them in a certain pattern on a checkerboard without assuming that the robot was quote playing chess.
Now the philosopher Wittgenstein who's been so influential in philosophy in the second half of the 20th century had developed undefeatable arguments in support of the proposition that meaning itself is not only a socially constructed entity
that that words derive their meaning from a process of social construction.
But that indeed all meaning patterns arise within a broad cultural context in which we find persons living to use Wittgenstein's quite felicitous phrase a common form of life.
That is we become intelligible to each other in virtue of the fact that we share a given form of life.
Meanings are not events that take place privately within the regions of an interiorized mind accessible to no one else.
In fact to defeat that claim Wittgenstein offers the famous example of the beetle in the box and I would want you to think about this for a moment.
Suppose every person in the audience had a box perhaps the other size of a box that a pair of earrings might come in or that a ring might come in.
Everyone had a box and everyone could look only in his or her own box and could not look in anyone else's box.
Now understand what we're trying to create here.
We're trying to create something similar to what is claimed when one says meanings are private.
I have a private language.
The way the way a word comes to mean something to me is I match it up with something going on in my mind in my conscious experience.
And then I might publicize it by speaking to others.
But there is a private language that then becomes that then becomes amenable to others when I simply agree to use terms that they're using the so-called private language argument.
Well I say Wittgenstein wants to illustrate the failure of that argument through this device called the beetle in the box.
So everyone now in attendance has a little box and at a key moment everyone is asked to examine what's in the box and declare what's found.
When you open the box what's inside.
Smith looks.
Remember now Smith can see only what's in his own box.
No one else can see what's in Smith's box and Smith can't see what's in anyone else's box.
Smith says there's a beetle in the box.
Joan says there's a beetle in the box.
Jane says there's a beetle in the box.
Janet says there's a beetle in the box.
Everybody says there's a beetle in the box.
What's in the box.
Now look.
My box is empty.
Maybe something I express via there's a beetle.
I say how would you ever know what it is I'm looking at when I say beetle.
The only way you could know what I'm looking at is if in some prior way by living a shared form of life by communicating with each other by having common experiences.
We agreed that an entity with the following characteristics is one that we will represent with the word beetle.
But if we had never had shared experiences, if we had never participated jointly in a common culture, if we had never engaged in similar practices, cooperative practices, the mere auditory utterance beetle would tell us nothing.
Beetle might mean too fake to one person and it might mean ice cream to another.
It might mean nothing.
It might simply be some random ejaculatory utterance, almost like a coughing beetle, that sort of thing.
Meaning then is inextricably bound up with practices, with actual practices in the world, social interactions, cultural practices.
Again, Wittgenstein's suggestive phrase, shared forms of life, entering into a common form of life.
This is the only basis upon which any symbol system can come to mean anything.
It's the only basis upon which words can come to mean things.
They cannot mean things in some ineliminably private context.
This is not to say that people don't have thoughts and reflections that they can only reach introspectively. Wittgenstein is not doubting that there are thoughts.
What he's doubting is that you can mint meanings privately, that something can come to mean something to the only person who ever existed in the universe.
Even that person, if only one person had ever existed in the universe, found himself looking into a box and said beetle, it couldn't possibly mean anything to him.
Do you see, it takes on its meaning to the extent that it permits patterns of shared activity and interaction and cooperative ventures.
Otherwise, it's just sound in the air.
Well, suppose we grant this much and then we ask, what is it that a computational device is doing when a computational device is allegedly engaged in something intelligent?
Well, if it's manipulating symbols, I mean if it's manipulating, remember now, not manipulating electrical discharges or things moving down printed circuits and the like.
No, no, no, manipulating. It is manipulating symbols, symbols in the sense of things that represent other things, which is to say symbols in the sense of entities having meaning.
How could a computational device be trafficking in meanings? Since a computational device is not part of a cultural world in which there are shared practices with others.
That is to say, unless the entity in question is actually developing and constructing meanings through a set of conventions arising from the social purposes and practices of a community, there can't be any meaning at all.
And since there is no computational device that indeed is itself the member of a culture engaged in shared practices with a community of social beings, then indeed within that device there can't be any meanings at all except such meanings as one puts there from the outside.
And then it's not the device that has the meaning.
Now I want to make this a strong statement. To say that I have memories in my brain might be a statement like this. It might be a statement of the sort, I keep my socks in the top drawer.
That is to say that I have memories in my brain is simply to designate a place where I happen to put certain things. And you all understand that in saying that I keep memories in my brain, it's not the same thing as saying my brain has memories.
Because I could say that I have an invitation to dinner in my pocket. That would not mean that my pocket has an invitation to dinner.
So we want to be fairly precise with the way we use language. If I say that I am putting symbols, I'm coding things in such a way that a computer is going to be able to manipulate these symbols and generate an answer to a question to be or not to be.
We want the computer to tell us should Hamlet die or live. When I say that I'm going to have some kind of input format that will get symbols in there so they can be manipulated.
Understand on the Wittgenstinian critique that the meaning is something that I put in in just the sense that I might put memories or socks in a drawer. It's not to say that at that point the computer qua computer possesses the meanings.
The computer is manipulating things as in Searle's Chinese room. It's then spitting something out in a format which I as a linguistic entity, the member of a culture living a certain form of life and thus able to have meanings and recognize things as meaningful
Now can read out, reassemble, look at and say aha Hamlet should remain forever. Hamletian and shouldn't either do himself in or not do himself in.
Now if you put these three propositions together, Gertl's incompleteness theorem to which the AI community might say well computers don't have to be formal systems. That raises yet another nest of problems.
John Searle's Chinese room, which some have argued is a telling criticism but not not not decisive or fatal. And the Wittgenstinian understanding of meaning, which in my current halibutist state I'm inclined to think is a fatal critique.
If you put it all together you recognize that the AI community has come up with some quite quite daunting proposals but there's no reason for the mentalist and the humanist to fold his cards at least as of this date.
Thanks very much.
