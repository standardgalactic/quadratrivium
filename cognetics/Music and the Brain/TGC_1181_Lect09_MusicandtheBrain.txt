Our last few lectures focused on pitch, melody, and harmony.
Now it's time to turn to a major aspect of music, rhythm.
People often have an intuition that there's something primal about musical rhythm, something
that taps into ancient aspects of how our brains and bodies work.
Darwin himself believed that our sense of musical rhythm built on ancient biological processes.
In his 1871 book, The Descent of Man, he wrote that the perception of musical rhythm
is probably common to all animals and no doubt reflects the common physiological nature of
their nervous systems.
This intuition is understandable because biology is full of rhythms.
Every mammal, reptile, and bird has a heart that beats rhythmically.
When an animal moves by walking, running, or beating its wings, it's creating a rhythm
with its motor system.
Most animals also have circadian rhythms, biological processes tied to the daily cycle
of light and dark.
Within the brain, measurements of ongoing electrical activity often reveal prominent
rhythms.
For example, theta rhythms are bioelectric potentials in the brain that oscillate at
a rate of about 6 to 10 times per second and seem to be important in the function of the
hippocampus, a deep brain structure involved in learning, memory, and spatial navigation.
With our bodies so full of rhythms, it might seem that rhythmic processing should be the
most basic and primal aspect of music.
But in this lecture and the next, I want to show you how mentally complex, even basic
musical rhythmic processing can be.
In fact, as we'll see in the next lecture, some aspects of the way we process basic
musical rhythm may be unique among all primates.
Let's start our discussion of rhythm by making a fundamental distinction between two types
of rhythmic patterns.
This distinction often gets overlooked, but it's crucial for understanding the richness
of musical rhythm.
And as we'll see toward the end of this lecture, it's also crucial for understanding how musical
and linguistic rhythms are related.
This is the distinction between periodic and non-periodic rhythms.
Periodic rhythms are patterns that repeat regularly in time, such as the heartbeat or
footfalls when walking steadily.
Periodic rhythms play a very important role in music.
Much of the world's music has a musical beat, which is a perceived periodic pulse that
listeners use to guide their movements and performers use to coordinate their actions.
Let's listen to a short passage of music with a clear beat.
The music you just heard was a complex acoustic stimulus.
But out of all that complexity, we can extract a steady underlying beat that you could snap
your finger or tap your foot to.
The beat is a very interesting aspect of music cognition.
It's a prototypical example of a periodic rhythm.
But sequences of events can have structure in time without having any periodicity.
For example, think of Morse code, the system of short and long beeps that was so important
to communication in World War II.
Morse code is very structured in time.
Different patterns of short and long beeps are used to represent the different letters
of the alphabet, and the timing between groups of beeps is used to indicate letter boundaries
and word boundaries.
But when you listen to Morse code, it doesn't give rise to a sense of a beat.
There is no steady underlying pulse that we can tap our feet to.
Let's listen to a sequence of Morse code.
Morse code involves temporal structure without periodicity, and it illustrates a fundamental
point about rhythm.
All periodic patterns are rhythmic, but not all rhythmic patterns are periodic.
This point is worth emphasizing because sometimes people use the word rhythm to mean periodicity,
to refer to something repeating regularly in time.
But this definition of rhythm is too narrow.
As we'll see in the rest of this lecture, several important aspects of musical rhythm
don't have anything to do with periodicity.
To do justice to the cognitive richness of musical rhythm, we want to define rhythm in
a way that is broader than just periodicity.
One definition that I've used in my own work is the systematic patterning of events
in terms of timing, accent, and grouping.
That word systematic is important because it's what distinguishes rhythmic patterns
from random patterns of events in time.
According to this definition, Morse code is rhythmic because it's very systematic in
how it's organized in time, even if it's not periodic.
Now if you don't know Morse code, then you might not pick up on any patterns when you
listen to it.
It might just sound like an unpredictable sequence of short and long tones.
This shows that from a cognitive perspective, rhythm is about the systematic patterns of
timing, accent, and grouping that we can detect, either consciously or subconsciously.
Let's discuss a specific example of non-periodic rhythmic structure in music.
This concerns how musicians add expression to musical performances.
If you look at a musical score of a piece of piano music, such as a piece by Chopin,
you'll see visual symbols that represent which pitches should be played at which time.
The melodic, rhythmic, and harmonic structures of the music are all there on the page.
Playing all those notes correctly is hard.
It takes years of training to be a good enough pianist to play through a Chopin piece without
making lots of mistakes.
But a compelling performance involves much more than just accurately playing the notes
from the score.
Let's listen to a part of a Chopin piece played in this technically accurate way, but lacking
in rhythmic expression.
This is from the Mazurka in C-sharp minor, Opus 50, number 3.
Now, let's listen to a rhythmically expressive performance of the same passage by the same
pianist.
Those two performances sounded very different, even though the pianist played the same sequence
of notes.
The difference between the performances concerns the patterns of timing and intensity of individual
notes.
Compared to the more mechanical version, the expressive version lengthens some notes, short
in others, and introduced variations in tempo and dynamics that brought the piece to life.
These changes weren't concerned with maintaining the beat or a sense of periodicity.
In fact, if anything, the changes resulted in making the beat less regular.
In musical terminology, the second performance had much more rubato or local changes in tempo.
I want to focus on this timing aspect of expressive performances because it's been studied empirically.
The pioneering work by scientists like Manfred Klein's and Bruno Repp showed that there
were interesting and systematic patterns in the expressive timing that musicians added
to music.
For example, in 1992, Bruno Repp published a paper in which he analyzed the fine-grain
timing of notes in professional pianists' recording of Robert Schumann's short piece,
Tromeroy.
Part of Repp's study focused on one melodic gesture of six notes.
This gesture is first heard near the beginning of the piece.
Let's listen to the opening of this piece as played mechanically by a computer.
None of the human pianists played it in this mechanical way.
Every pianist played this gesture slightly differently in terms of the fine-grain timing
patterns.
But beneath all of this variation, there was also some commonality.
Many of the pianists showed a pattern where they accelerated smoothly over the first two
notes of this gesture and then slowed down smoothly over the remaining notes.
Let's listen to the music with this kind of timing added.
Repp referred to this pattern of local acceleration and deceleration as a parabolic timing function
because if you plotted the pianist's tempo during this gesture, it went up and down smoothly,
like a parabola.
Building on the ideas of other music psychologists like Neil Todd and Johann Sundberg, Repp suggested
that this kind of timing pattern could be a musical way to allude to the physical motion
of the human body.
How would that work?
Well, think about how humans move.
When we reach for something, like a pen, we don't do it by moving our hand toward it
at a steady rate, like this.
What we do is, as we reach, the speed of our hand rapidly increases to a maximum, then
decreases to zero, all very smoothly.
In the microtiming of the melodic gesture, Repp found a similar pattern, a smooth increase
in speed to a maximum rate, and then a smooth decrease.
Repp suggested that one reason we so often apply metaphors of motion to our perceptual
experience of music is that musical rhythm mirrors some of the temporal dynamics of how
we move.
This is a fascinating idea.
In the example of the Schumann piece, this mirroring happens in a non-periodic aspect
of musical rhythm.
In 2011, Daniel Levitan and colleagues published an interesting study about the relationship
between expressive timing in Chopin's music and people's perception of how strongly the
music expresses emotion.
They used excerpts from Chopin's piano nocturnes and took advantage of the technology provided
by the Yamaha Disclavier piano.
This piano can digitally encode the exact timing and dynamics of a human performance.
Levitan and his colleagues asked a professional pianist to perform the nocturnes on the disclavier
with normal expressivity, the way he would at a concert.
Then they used sound editing software to create different versions of the performance with
less and less expressive variations in timing and intensity.
For example, in the mechanical version, they removed all the expressive timing and intensity
variation, so the piece sounded like it had been produced by a computer that exactly followed
the notes on the page.
But they also had versions that had 25%, 50%, or 75% of the original expressive variation
in timing and intensity.
They presented listeners with these versions and for each version asked them to rate.
How emotional was the performance you just heard?
They told listeners to focus on how strongly the piece expressed emotion, not on what particular
emotion was expressed.
They found a systematic relationship between the amount of expressive variation in timing
and intensity and people's ratings of how strongly the music expressed emotion.
In another experiment, they showed that it was primarily the expressive variations in
timing, not intensity, that were driving people's ratings.
Another interesting finding from this study came from a test where the experimenters used
computers to exaggerate the original expressive patterns.
For example, instead of just playing versions with 25% or 50% of the original expressive
variation in timing and intensity, they also had versions with 125% or 150% of the variation.
When listeners rated these for emotional force, they didn't rate these exaggerated performances
any higher than the original version.
This suggested that the performer had intuitively found an optimum level of expressive variation.
Less variation would reduce the strength of the emotional expression in the performance,
but more variation wouldn't increase it.
The final study of this type I want to discuss looked at how expressive timing and dynamics
influence brain responses to music.
Edward Large and colleagues did an fMRI study of listeners hearing mechanical versus expressive
versions of a Chopin etude and compared brain responses to the two versions.
The expressive versions led to significantly stronger activation in multiple areas of the
brain.
Some of these areas were in the limbic system, including the amygdala and hippocampus, which
suggests that the expressive performances evoked stronger emotion and were more memorable.
Other areas that were more active are known to be involved in cognitive processing.
These included a region in the inferior left frontal lobe of the brain near Broca's area
involved in processing musical structure.
These effects of expression on the emotional and structural processing of music were coming
from the way a performer shaped subtle, non-periodic aspects of a piece's musical rhythm.
Let's turn now to another important aspect of musical rhythm which doesn't concern
periodicity or beat.
This is grouping or phrasing, the perceptual segmentation of events into coherent chunks.
When we hear a coherent melody, we typically hear it as broken into groups of notes or
phrases.
This is crucial for our ability to encode and remember it.
Sometimes those groups are physically separated by short silences, but sometimes the boundaries
we perceive are entirely in our minds.
Let's listen to one simple example of this.
We'll hear a short, simple melody consisting of five phrases.
In that melody, the boundaries of phrases one and two were physical.
There were short musical rests or silences after those phrases.
But listen again to phrases three to five.
You'll hear two phrase boundaries, but there are no pauses between any notes.
In this case, you mentally placed boundaries at points where there was no silence.
One of the things that led you to place those boundaries was the pattern of note timing
just before the boundary.
The notes just before the boundaries of phrases three and four were longer and lower in pitch
than the previous few notes, and research shows that this contributes to the perception
of a boundary.
I want to focus on this tendency to hear a boundary after a note that's longer than
preceding notes.
This is known from speech, too, where a syllable just before a perceived phrase boundary tends
to be longer than that same syllable if it were inside the phrase.
For example, if I say, I went to the store and got some eggs, then drove home and made
dinner.
The word eggs and home are heard as marking the ends of phrases, and this is partly based
on the relative lengthening of those syllables.
This mental boundary placement based on duration is an important aspect of rhythm perception,
which is not about the beat.
The reason I bring up this particular example of non-periodic rhythm processing is that
it's taught us something important about the influence of culture on basic auditory
perception.
It used to be thought that the tendency to hear a boundary after a longer event in a
sequence was a law of auditory perception that applied to music and language.
This idea came from research extending back over a hundred years on how people hear grouping
in simple tone sequences, where the duration of tones is the only thing that varies.
Let's look at a schematic diagram of such a sequence.
In this diagram, the bars represent notes that are all the same pitch.
Short bars represent short notes, and long bars, long notes.
Each note is separated from the next note by the same short amount of silence.
Here's one example of what such a sequence sounds like.
As you heard, there are no obvious phrase boundaries in this sound, no long silences
that break the sequence into chunks.
But research across many years and in multiple cultures showed that if you ask people how
they hear this type of sequence, many people will say they hear it as a repeating short,
long tone pattern.
The mind is grouping the sequence into chunks of two tones, with the longer tone being group
final.
Research showed that people in England, America, Holland, France, and Germany all hear it this
way.
That's why it seemed like hearing longer elements as group final might be a universal
law of auditory perception.
But there were some researchers, including the famous linguist Roman Jakobson, who believed
that basic rhythmic grouping perception in non-linguistic sounds could be shaped by the
rhythms of one's native language.
In 2006, together with John Everson and Kengo Ogushi, I published a paper that supported
this idea.
We looked at how American and Japanese college students perceive sequences like the one you
just heard.
We found that while almost all of our American listeners heard the pattern as a repeating
short, long group, many Japanese listeners heard it as a repeating long, short group.
Both groups were very confident about their perception.
For the Japanese listeners, this sequence was not da-da, da-da, da-da, but da-da, da-da,
da-da.
It was surprising that such a basic aspect of rhythm perception could be so different
in different cultures.
Following Jakobson's suggestion, we looked at how patterns in the native language could
lead to this difference.
It turned out that there was a logical explanation related to differences in the grammar of English
and Japanese.
In English, as in Dutch and French and German, little grammatical words like the, to and
a come before the word they are grammatically related to.
In English, we say the dog, to bed, a book.
That creates a very frequent short, long pattern.
When you say the dog, you're producing a little two-element group with a short, long pattern.
In Japanese, short syllables that serve grammatical functions like ga, ni and wo come after the
word they are grammatically attached to.
For example, hanwo combines han, the word for book, with wo, which indicates that book
is the direct object in a sentence.
So Japanese is full of little, long, short, linguistic chunks, and we think this is why
many Japanese people hear the long duration tone as starting a group, not ending it.
For me, this study serves as a reminder that culture can shape very basic aspects of how
we perceive our world.
Hearing scientists want to know the facts of human auditory perception and cognition,
but we should always ask ourselves, to what extent do the facts that emerge from our studies
depend on the cultural experience of our listeners?
In the rest of this lecture, I want to focus on one final example of non-periodic rhythm
in music.
It provides an example of how thinking of rhythm more broadly than just periodicity can help
drive scientific research forward.
In this case, the research was inspired by an old and provocative idea in musicology.
This is the idea that purely instrumental music reflects the prosody of a composer's
native language.
Prosody refers to the rhythm and melody of speech.
Different languages don't just differ in their words and grammar, they also have different
patterns of rhythm and pitch in their sentences.
If you remember the film My Fair Lady, Professor Henry Higgins had a tremendous ear for language.
He knows where people were raised by their accents.
Part of what makes an accent is the prosody of speech, the rhythm of the syllables and
the way the voice pitch moves up and down during spoken phrases.
For decades, musicologists have suggested that purely instrumental music can reflect the
rhythm and melody of a composer's native tongue.
For example, in the tradition of Western music, Gerald Abraham quotes the harpsichordist
and music scholar Ralph Kirkpatrick as saying, both Couparane and Ramo, like Foray and Debussy,
are thoroughly conditioned by the nuances and inflections of spoken French.
On no Western music has the influence of language been stronger.
This is a provocative idea because instrumental music and spoken language sound so different.
No one would ever confuse the sound of a Debussy piano piece with the sound of spoken French.
Yet Kirkpatrick sensed a connection.
Something about Debussy's music reminded him of the sound of the French language.
Kirkpatrick was not alone.
In 1953, the linguist Robert Hall Jr. published an essay suggesting that the symphonic music
of Sir Edward Elgar reflected the prosody of British English.
Elgar is a Victorian composer who lived around the same time as Debussy.
His music is familiar to almost every U.S. high school graduate because his March pomp
and circumstance is so widely used in graduations.
Let's listen to a short passage of Debussy and Elgar to illustrate the kind of music
that was behind these intuitions.
First, the Debussy.
Now the Elgar.
Clearly neither of those sounded anything like speech.
Yet the idea was that the ghosts of English and French speech were somehow in the sounds
you've just heard.
One thing that made the ideas of scholars like Kirkpatrick and Hall so provocative is
that they weren't presented with any evidence.
They were intuitions, informed by a deep knowledge of music and language.
In the early 2000s, it occurred to me that the ideas might be testable.
For example, focusing on rhythm, one could look for rhythmic differences between languages
like English and French and see if those differences are reflected in the rhythms of instrumental
music like the music of Elgar and Debussy.
The challenge was figuring out what to measure.
Linguists agreed that English and French had very different speech rhythms.
According to an influential theory developed in the mid-1900s by Kenneth Pike and David
Abercrombie, English was a stress-timed language, meaning that stressed syllables were spaced
evenly in time.
Pike used the sentence, the teacher is interested in buying some books, as an example of stress
timing.
In this sentence, some of the syllables are produced with more prominence than others,
even if I say it without emphasizing any particular word.
We can mark the stressed syllables with uppercase letters.
The teacher is interested in buying some books.
If I speak the sentence naturally and knock on each stressed syllable, the knocks are
very regularly spaced in time, almost like a metronome.
The teacher is interested in buying some books.
This metronome-like timing of stresses is what made English a stress-timed language,
along with certain other languages like Russian and Arabic.
Abercrombie argued that French came from an entirely different category, the syllable-timed
languages.
These were languages in which each syllable marked off a roughly equal time interval.
Many romance languages were said to be syllable-timed like Spanish and Italian, but also languages
from other parts of the world, including some African and Indian languages.
Stress timing and syllable timing were ideas about linguistic rhythm that were based on
periodicity.
In fact, in his 1967 book, The Elements of General Phonetics, David Abercrombie equated
rhythm with periodicity.
He wrote that rhythm, in speech as in other human activities, arises out of the periodic
recurrence of some sort of movement.
Abercrombie made enormous contributions to linguistics and phonetics, but in equating
speech rhythm with periodicity, I think he made a serious mistake.
Years of empirical research have failed to support the existence of stable periodicities
in ordinary speech.
Finding measurable differences between the rhythms of English and French had to wait
until around the year 2000 when linguists began measuring non-periodic aspects of rhythm,
and it was precisely these aspects that turned out to be the key for showing that instrumental
music really did reflect speech rhythm.
I'll focus on one non-periodic rhythmic difference between English and French, which was the
first which was first pointed out by the linguist Rebecca Dower.
This has to do with the way the two languages differ in a phenomenon called vowel reduction.
In English, unstressed syllables, like the second syllable and third syllable, in the
word celebrate, sometimes contain a reduced vowel.
This is a very short vowel with a neutral kind of sound, like uh or ih.
In the case of celebrate, we don't say celebrate, we say celebrate.
The vowel in le is reduced.
Since reduced vowels tend to be very short, sometimes less than a twentieth of a second
long, they are often next to syllables with much longer vowels.
For example, the vowel in a stressed syllable might be more than twice as long than the
reduced vowel in the syllable right next door.
This creates a lot of duration contrast between neighboring vowels in sentences of English.
Dower pointed out that many languages that had been traditionally classified as stressed
syllable timed had strong vowel reduction, and many languages that had traditionally
been classified as syllable timed had weak vowel reduction.
This means that in languages like French, you don't see vowels getting squashed down
into little short uh and ih sounds nearly as often as you do in English.
As a result, neighboring vowels in French sentences are likely to have less duration
contrast on average than they do in English.
Around 2000, the linguist Francis Nolan and colleagues published a paper with an equation
that could be used to measure the average degree of duration contrast between adjacent
vowels in sentences.
This equation was called the Normalized Pairwise Variability Index, or NPVI.
Soon thereafter, using the NPVI, other researchers showed that on average, English sentences
had a higher average degree of duration contrast between adjacent vowels in sentences than
did French.
When I read about the NPVI equation, it occurred to me that it could be applied to instrumental
music, measuring note durations instead of vowel durations.
Fortunately, right around that time, I happened to discover a wonderful old resource book
called A Dictionary of Musical Themes.
In this book, two musicologists had compiled musical themes from famous, widely recorded
pieces of western instrumental classical music.
It was meant to be a kind of Bartlett's quotations of classical music, containing the parts of
compositions that were the most identifiable and most likely to stick in your memory.
In 2003, Joseph Danielli and I published a paper where we applied the NPVI to English
and French themes from this dictionary, focusing on composers like Elgar, Debussy, and their
contemporaries.
We analyzed themes from 16 turn-of-the-century composers, including artists like Holst, Van
Williams, Ravel, and Saint-San's.
We showed that on average, the themes of English composers had greater durational contrast
between adjacent notes than the themes of French composers.
This was the first empirical evidence that speech rhythm was reflected in purely instrumental
music.
The key to this discovery was measuring a non-periodic aspect of rhythm, the average
amount of duration contrast between successive events in a sequence.
Let's listen to a theme from a Debussy quartet and from an Elgar symphony to illustrate this
difference in contrast.
You'll hear the themes as solo melodic lines, played on a cello.
We'll also show the music notation, but don't worry if you can't read it.
Just listen for the amount of duration contrast between adjacent notes in the two themes.
First, the Debussy.
Now the Elgar.
The Elgar theme had much more duration contrast between neighboring notes than the Debussy
theme, which had a rhythmically smoother feel.
Our study raised many questions, some of which I have pursued with colleagues and some that
have been pursued by other researchers.
For example, we now know that this relationship between rhythm and speech and instrumental
music isn't just restricted to English and French classical music from around 1900.
In 2011, Rebecca McGowan and Andrea Leavitt used the NPVI to show that the rhythms of
folk fiddling in Ireland, Kentucky and Scotland reflected the speech rhythms of English dialects
in those places.
Also a 2006 paper by myself, John Eberson and our course composer, Jason Rosenberg,
showed that English and French classical music doesn't just reflect the rhythm of speech,
it also reflects the melody of speech, the pattern of speech intonation in British English
and French.
Stepping back from the details of this research, I want to end this lecture by emphasizing
one last time how important it is not to equate rhythm and periodicity.
All of the studies I've described in this lecture teach us interesting things about
musical rhythm, and none have been about periodicity.
As we'll see in the next lecture, periodicity is a fascinating and important topic for music
cognition, but we still have much to learn about music in the mind by studying non-periodic
rhythms.
