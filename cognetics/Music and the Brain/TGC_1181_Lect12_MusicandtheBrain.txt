Today, in 2015, a hot topic in the study of music cognition is the impact of musical training
on other cognitive functions.
For example, researchers want to know if learning to play a musical instrument can
influence the way a child's brain processes language, or math, or visual spatial patterns.
This topic is exciting because it has both theoretical and practical significance.
In terms of theoretical issues, music's relation to other cognitive functions can teach us
about the architecture of cognition, about how different mental processes are related
in the brain.
For example, are musical and mathematical processing related?
In Western culture, there's a long tradition of thinking that music and math are closely
related.
This goes back to ancient Greek philosophers such as Plato and Pythagoras.
The idea can be seen in the way music was taught in the Middle Ages.
Music was taught as part of the medieval quadrivium, a set of four academic subjects that focused
on numbers and proportions.
These were arithmetics, geometry, astronomy, and music.
Music certainly has a rich structure that can be analyzed mathematically, but of the
mental processes involved in music and math related, how about the mental processes involved
in music and language, or music and spatial cognition?
Answering these questions could help address an old debate in cognitive science between
those who view different mental faculties, such as music, language, and math, as largely
independent cognitive domains, and those who argue that there are underlying connections
between different aspects of cognition.
Thinking out if musical training influences other mental abilities is also relevant to
a major theoretical issue in cognitive neuroscience.
This is the impact of experience on brain structure and function.
As we discussed in the last lecture, experience-dependent neural plasticity is now a major theme in
the study of the brain.
We know from extensive research in animal and human neuroscience that the brain's microstructure
and function can be shaped by experience.
But how much can we influence the microstructure of the brain, and the way it processes information
by the activities we choose to engage in?
Musical training provides a great domain for studying this question.
People who pursue music often devote hundreds or thousands of hours to musical training
spread across multiple years.
Does this training result in minor tweaks to human brain microstructure and other cognitive
functions, or are we talking about significant impacts, as measured by brain imaging and
cognitive tests?
This last question is related to practical issues.
If musical training can significantly impact brain functions such as language, attention,
or memory, then this provides a strong motivation for conducting applied research with people
who have deficits in those areas.
For example, there is a lot of interest right now in research that has found relations between
musical rhythmic processing and certain aspects of linguistic processing in children, known
as phonological processing, which is processing the sound structure of words.
Phonological abilities include the ability to segment words into their individual speech
sounds and manipulate those sounds.
This is a key part of learning how to read.
Deficits in phonological processing are often found in dyslexia.
Why would rhythmic and phonological abilities be related?
It seems counterintuitive that non-linguistic rhythmic processing would share anything with
processing the sound structure of words.
Yet multiple studies are pointing to a connection.
One reason might be that the patterns of timing and emphasis play an important role in both
musical rhythm and speech rhythm.
We talked about speech rhythm in a previous lecture.
Research shows that speech rhythm is one of the cues that infants use when they first
learn to segment words and syllables from the continuous stream of speech.
Their segmentation could lead to sharper neural representations of individual speech
sounds.
In 2013, Usha Goswami and her colleagues published a study showing that two months of musical
rhythmic training in six to seven-year-old children led to enhancements in phonological
processing that were comparable to two months of direct training of phonological skills.
The children in their study had been identified by their teachers as children who were struggling
with learning how to read.
These results lead to ideas for applied research.
For example, one could work with preschoolers who are at art risk for dyslexia based on
family history and provide them with early musical rhythmic training in the form of structured
games and activities.
This could be done when they are three or four years old while the brain is still developing
rapidly and before they enter school and begin learning to read.
Later at six or seven years old, one could test these children to see if their incidence
and severity of reading problems is lower than you would expect based on their known
risk factors.
Clearly, studies that look at the impact of musical training on other functions are important
for both theoretical and practical issues.
Fortunately, we know a way to conduct such studies that can give us clear answers about
whether music has an impact and how strong that impact is.
Unfortunately, this way is logistically challenging and expensive, so relatively few studies using
this method have been conducted so far.
This methodology involves doing controlled experiments in which people are randomly
assigned to one of three conditions, musical training, some other form of training that's
demanding but not auditory like painting or martial arts, or no extra training.
In technical terms, you have an experimental group, that's the music training group, an
active control group, that's the group that gets some other form of training, and a passive
control group, that's the group that gets no training.
Once you have these groups, then you do cognitive measurements of each group before training
starts and then after training ends.
These types of studies which measure the same people at multiple points in time are called
longitudinal studies.
For example, you might test language skills or math skills or spatial cognition skills.
You could also test skills that are more closely related to musical training such as basic
auditory and motor skills.
Really you would also take measurements of brain structure and or brain function.
With these before and after measurements, you can determine if musical training caused
changes in non-musical cognitive skills and you can measure how big those changes were.
If you can add before and after brain measurements and relate those measurements to the cognitive
measurements, then you've got a study that's worth its weight in gold.
That's because the study allows you to make strong inferences.
For example, if you're doing research with children and you see a certain aspect of cognition
develop more strongly in the music group than in the group with no extra training, you can
infer that music training caused this enhancement.
You can then use your brain measurements to see which brain areas or connections or processing
mechanisms are involved in this cognitive enhancement.
You can also see if this enhancement is present in the active control group.
If not, then you can infer that the enhancement is not just a result of doing any sort of
extra training but has a more specific relationship to musical training.
You can see now why these kinds of studies are logistically challenging and expensive.
You have to coordinate the cognitive testing of three different groups of participants and
test them at multiple times.
You also have to provide training for two of the groups, the experimental group and the
active control.
Our training has to be long enough to give the brain an opportunity to change with experience.
This could mean hiring trainers like music and art teachers for many months or even a
year or more.
If you want to include brain measures, studies like this can start to get very expensive.
Also there's that issue of random assignment to groups.
Why is that necessary?
For example, if you're studying young children, it would be much easier just to look for children
who are voluntarily starting music lessons versus those that aren't.
The problem with that design is that children who choose to take music lessons or whose
parents choose for them to take music lessons may come from homes that are different from
children who aren't taking lessons.
There could be differences in how motivated the children are to learn and take on new
challenges or how involved the parents are in the children's intellectual development.
These factors could influence cognitive and brain development.
Such factors create what is called a confound in experimental research.
Random assignment to different groups helps reduce the odds of this kind of systematic
bias between the different groups.
Are there studies that have the kind of design that I've been talking about?
Because of the challenges I've mentioned, there are only a few studies like this, though
I hope more will be coming.
I want to tell you about two such studies.
The first was conducted by Silvan Moreno and colleagues and was published in 2009.
They studied eight-year-old children and were interested in the impact of musical training
on language skills.
The researchers randomly assigned children to a music training group and to an active
control group that consisted of painting training.
The groups were matched for socioeconomic status and at the outset of the study, the
children in the two groups didn't differ significantly in IQ.
Each group met twice weekly with a professional teacher.
The music group focused on things like rhythm, melody and harmony.
The painting group focused on things like color, perspective and texture.
After six months of training, the children were brought back for another round of cognitive
testing and also for brain measurements.
The brain measurements used a technique called event-related potentials or ERPs.
Event-related potentials are derived from EEG measurements of brain waves.
These brain waves are ongoing electrical signals from large regions of the brain which can
be recorded using sensors built into a cap that's worn tightly on the head.
Unlike fMRI, EEG can't tell you exactly where in the brain activity is happening, but it
has much better time resolution than fMRI.
It can tell you about large-scale patterns of electrical brain activity with millisecond
temporal resolution.
The researchers found that after training, both the music and painting groups scored higher
on a standardized IQ test.
This made sense because the children were older and had been in school for six months
during the study.
The interesting finding was that the music group improved more than the painting group
on a different test that focused on reading abilities, even though neither group had done
reading training.
Also behavioral and ERP brain measurements showed that children in the music group were
more sensitive to small changes in the pitch pattern of sentences, even though neither
group had been trained on this.
The researchers concluded that music had provided auditory training that had influenced
not only music processing, but also language processing.
In particular, musical training had influenced reading skills and the processing of spoken
pitch patterns.
While reading is obviously an important life skill, better-sentenced pitch processing could
also be helpful in the real world.
In speech, these nuances of spoken pitch patterns can indicate which word a speaker is emphasizing
or the attitude or mood with which the words are spoken.
Since the researchers saw these differences in language processing despite no significant
differences in IQ scores after the two kinds of training, they favored the view that there
were some specific links between music processing and language processing in the brain.
The next study I want to tell you about also showed an impact of musical training on non-musical
cognitive skills, but ended up arguing against specific links between music and language
or any other particular cognitive domain.
This study was conducted by Glenn Schellenberg.
In 2004, he published the first study to use random assignment to examine the impact of
musical training versus other types of training on IQ in a relatively large group of children.
In his study, six-year-olds were randomly assigned to music training, which could be
keyboard or voice, drama training, or no training.
There were about 30 children per group.
The training group studied weekly with professional teachers for about one school year.
More and after training, IQ was measured using standardized tests.
IQ went up significantly in all the groups, which was expected.
We know that attending school raises IQ.
The interesting finding was that the amount of IQ increase was significantly larger in
the music groups than in the other groups.
In the music groups, IQ went up by about seven points.
In the drama group and the group with no training, IQ went up by about four points.
The drama group did show more improvement than other groups in a different area.
This was adaptive social behavior, which refers to things like social skills and leadership.
So drama was having a positive effect on skills beyond just acting.
The striking finding of the study, though, was that music training caused the largest
gains in IQ.
When Schellenberg looked at different components of the IQ tests that he had used, such as
the more mathematical components and the more verbal components, he didn't see any particular
areas where the gains were stronger than other areas.
It seemed like a general boost across a range of cognitive skills.
He felt this argued against a specific link between music cognition and any particular
other aspect of cognition, like math or language.
As you can see from the two studies I've described, this young area of research is
producing some consistent findings and some not-so-consistent findings.
Both studies I described show that music training causes enhancements of non-musical aspects
of cognition, and that's exciting and important.
However, one study didn't see an impact on standardized IQ tests, and one did.
Now these studies were conducted in different countries with different age children, and
the training the children received wasn't the same.
There are multiple factors that could account for the differences in the findings.
What strikes me in these studies is that they provide enough initial evidence to justify
larger-scale randomized control studies.
This is a real need for studies of musical training, which combines structural brain
imaging, like the kind we talked about in the last lecture, with measures of brain activity
and of non-musical cognition.
For the rest of this lecture, I want to focus on relations between musical training and
the brain's processing of speech.
Music and speech both use highly structured sound sequences, and there's a growing body
of research examining how music and speech processing are related in the brain.
As of today, in 2015, there has been much less research on relations between music processing
and math processing, or music processing and spatial cognition.
In terms of math, the psychologist Ellen Winner and her colleagues recently reviewed the literature
on mental relations between these domains.
In a 2011 paper, they argue that the research that's been done so far doesn't really allow
us to draw any firm conclusions.
Hopefully this will change in the coming years.
Going back to music and speech, the work I want to describe next has used an interesting
technique for measuring brain responses to speech sounds in musicians and non-musicians.
This technique focuses on the sensory processing of sound before neural signals reach the cerebral
cortex.
The auditory system has a complex network of subcortical processing regions between the
ear and the cortex.
What is subcortical processing?
After sound is first transduced into neural impulses by the cochlea, it is processed in
multiple regions of the brainstem and midbrain, as shown in this simplified diagram.
These include the cochlear nucleus, the superior olivary nucleus, and the inferior calliculus.
This means that the sound-related neural signals that reach the primary auditory cortex have
already been subject to a lot of processing.
This is different from the other senses.
For example, vision-related neural signals that reach the primary visual cortex have
been through many fewer processing stations than sound-related signals.
It used to be thought that the subcortical auditory processing was hardwired and not
subject to the same kinds of experience-dependent neural plasticity observed in the cortex,
but there is now evidence that the early sensory processing of sound can be modified by experience.
One way that this might occur is via neural connections that project from the cortex down
to these subcortical auditory regions.
That's the opposite direction than we usually think of brain signals traveling in the auditory
system.
The common view is that the auditory system represents a one-way flow of brain signals
from the cochlea to the cortex.
But research in auditory neuroscience has shown that the real story is much more interesting.
Neuroanatomically, the auditory system is a two-way street.
Neural signals not only go from subcortical regions to cortex, they also go the other
way.
The subcortical regions receive many projections from the cortex.
In some parts of the subcortical system, these descending projections from cortex may actually
outnumber the projections going up to the cortex.
In a 2008 study, Nina Krausen colleagues provided compelling evidence for experience-dependent
plasticity in subcortical auditory processing.
I want to tell you about this study because it provides the background for the study of
musical training I'll describe later.
In this study, the researchers made up nonsense words like PESH and gave these words meaning
that depended on the pitch contour of the word.
For example, PESH with a level tone, PESH meant glass.
PESH with a rising tone, PESH meant pencil.
PESH with a falling rising tone, PESH meant table.
Researchers had to learn this vocabulary as if they were learning the words of a foreign
language.
This kind of language with a pitch pattern of a word can completely change its meaning
is called a tone language.
About half of the world's languages are tone languages, including Mandarin Chinese as well
as many other languages in Southeast Asia and Africa.
The participants in this study were native English speakers who didn't know any tone
languages.
English isn't a tone language, so the participants weren't used to learning words where the pitch
pattern can completely change the meaning of a word.
Even so, they did learn this novel vocabulary of nonsense words, which showed they were
capable of learning words where the meaning depended on the pitch pattern.
Before and after this training, Krauss and colleagues measured subcortical auditory responses
to speech using event-related potentials or ERPs.
They focused on subcortical responses to a Mandarin Chinese syllable, Mi, spoken with
the three different pitch contours they'd used in their study, the level pitch contour,
the rising pitch contour, and the falling rising contour.
However, it's important to know that this syllable Mi had not been part of the study.
The researchers didn't teach subjects different meanings with the pitch inflections of this
syllable as they had for the other nonsense words, so different pitches of this syllable
didn't mean different things to the different participants.
Also, the participants didn't pay attention to the syllable Mi while it was being played
and their brain responses were being recorded.
They watched a video while the three different versions of Mi played in the background.
Thus the researchers were focusing on pre-attentive sensory processing of sound and measuring
how accurately subcortical brain activity reflected the detailed pitch contours of
a spoken syllable.
In particular, they wanted to know how the training would influence general subcortical
processing of linguistic pitch contours, not just processing of syllables that had been
explicitly trained.
The striking finding of this study was that after training, the subcortical responses reflected
linguistic pitch contours more accurately than before training.
Early, primitive parts of the auditory system had changed their responses to speech sounds
due to the training the participants had done.
How much training?
Surprisingly little.
Just eight half hour training sessions spread over two weeks.
These results demonstrated rapid neural plasticity in a part of the brain where responses were
once thought to be hardwired and not very modifiable by experience.
They showed that learning a new vocabulary where pitch patterns change the meaning of
words can alter early auditory responses to linguistic pitch contours more generally.
Learning new vocabulary definitely involves the cortex, so we're seeing an influence of
cognitive processing on sensory processing.
This fits with the idea of auditory processing as a two way street in the brain, an idea
that has been supported by research in animal neuroscience.
With this study as background, we can now discuss a study about how musical training
is related to the brain's processing of speech.
In 2007, Patrick Wong, Nina Kraus and colleagues published a study which compared subcortical
brain responses in musicians and non-musicians to Amanda and Syllable with different pitch
contours.
The musicians weren't professional musicians, they were just people with at least six years
of musical training who had started their training at age 12 or before.
All the participants were native English speakers, none of them new Mandarin, so the different
pitch contours on the Syllable they heard didn't change the meaning of the Syllable
for them.
Also once again they didn't pay attention to the Syllable during the experiment, they
just heard it in the background while their brain waves were recorded using event related
potentials.
The idea was to measure subcortical sensory processing of these speech sounds.
The researchers wanted to know if musical training would lead to enhanced sensory processing
of Syllable pitch patterns.
Pitch patterns are important in both music and speech.
In music, pitch patterns are the basis of melody.
In speech they can signal focus on particular words or the emotions or attitude of a speaker.
So both domains use pitch patterns, but would musical training enhance the processing of
linguistic pitch patterns?
The answer wasn't obvious.
These Syllable pitch patterns were short pitch glides that were just a few hundred milliseconds
long, not sequences of several notes with jumps between different distinct pitches like musical
melodies.
The results of the study were clear.
The subcortical auditory responses of musicians did more accurately reflect the pitch contours
of the spoken Syllables.
Kraus and colleagues have now done numerous studies showing that subcortical auditory responses
to speech are enhanced in musicians.
These brains seem to encode speech sounds with greater acoustic detail.
These enhancements don't just concern pitch patterns, they have also been found for other
aspects of phonetic structure, such as the detailed patterns of frequency and timing
that help a listener distinguish between different consonants.
These studies by Kraus and colleagues have not focused on professional musicians.
They focused on people who have been engaging with music regularly for a number of years,
including children, young adults, and older adults.
In a 2014 paper, Kraus and colleagues showed these enhancements in a longitudinal study
of eight-year-old children who were randomly assigned to music training or no music training.
These were low-income children in Los Angeles who were enrolled in a community music program
called the Harmony Project.
This program focuses on instrumental music.
The kids play together in an orchestra after school, modeled on the El Sistema program
from Venezuela.
After two years in the program, the children's subcortical responses to speech sounds were
significantly enhanced.
We need more longitudinal studies of this type.
At the moment, longitudinal studies are much less common than cross-sectional studies,
which compare musically-trained people to musically-untrained people at a single point
in time.
This makes cross-sectional studies much easier to conduct because you only have to measure
people once, and you don't have to do any training.
But the trade-off is that in cross-sectional studies, if you find differences between musicians
and non-musicians, you don't really know what caused those differences.
Are they inborn differences?
Are they the result of musical training?
Do they reflect an interaction between biological predispositions and musical training?
You don't know.
However, in the cross-sectional studies that Kraus and her colleagues have conducted, there
is one thing that suggests that musical training plays a role in causing enhancements in speech
processing.
The researchers often find a relationship between the degree of enhancement in a musician's
brain response to speech and the number of years of musical training that a person has.
If the enhancements were entirely due to inborn differences, you wouldn't expect this pattern.
Also as the researcher Gavin Biddleman has pointed out, cross-sectional studies finding
enhanced brain responses to speech in musicians become even more compelling if those enhancements
can be related to enhanced speech perception.
That is, we want to know if the differences in brain responses translate into differences
in perceptual abilities.
In recent years, the cognitive domain that has been most extensively studied in cross-sectional
studies of musicians and non-musicians is language.
Multiple studies have found that musical training is associated with advantages in several different
aspects of language processing.
These include processing the rhythm and melody of speech, discriminating different emotions
in spoken sentences, and perceiving and producing the sounds of a second language.
But in addition to more empirical work, we also need theoretical frameworks that can
explain why musical training would enhance language processing.
For example, why would learning to play a musical instrument like a cello improve the
brain's processing of speech?
It's always interesting to see that musical training improves things besides musical skills,
but from the standpoint of cognitive neuroscience, we want to understand how this works.
Music and speech both play significant demands on the auditory and motor systems, and they
have a lot of obvious differences.
For example, if we're talking about instrumental music, you're making sounds with your hands,
not with your vocal tract, and the sounds you produce are very different from speech
sounds.
No one would ever confuse the sound of a cello with the sound of someone talking.
In 2011, I proposed a theoretical framework to explain why musical training would enhance
speech processing.
I called it the opera hypothesis.
The basic idea is that musical training can enhance speech processing when several specific
conditions are met.
First, there has to be overlap in the brain circuits that process a certain aspect of
music and speech.
For example, there's evidence that processing the ups and downs of pitch contours in music
and language draws on overlapping brain mechanisms in the right cerebral hemisphere.
The O in opera stands for overlap.
Second, music has to place higher demands on these shared brain circuits than language
does.
For example, we've heard in a previous lecture how the same very small change in pitch can
make a big perceptual difference in music, but not make a big perceptual difference in
the language.
This means that music processing involves more precise processing of pitch patterns than
language.
The P in opera stands for precision.
If music shares brain networks with language and demands more of these networks, then this
sets the stage for music to enhance speech processing.
This is because any improvements in the function of those networks will impact both music and
speech since they both rely on those networks.
I've used pitch processing to illustrate the opera hypothesis, but the hypothesis is
more general.
It's relevant to any aspect of auditory processing that might involve overlapping brain processing
in speech and music, including the processing of temporal patterns.
The remaining three conditions of opera, the ERA, refer to factors that are known to
drive neural plasticity in brain networks.
These are emotion, repetition, and attention.
Research in animal neuroscience has shown that experienced dependent plasticity due
to auditory training is the strongest when the training is associated with strong emotion,
extensive repetition, and focused attention.
Music has all of these factors.
As we've seen in previous lectures, music has strong links to emotion.
Musical training also involves extensive repetition.
You practice passages over and over until you get them right.
And musical training demands focused attention.
So that's opera.
Overlap, precision, emotion, repetition, and attention.
The original opera hypothesis was focused on explaining musician benefits in the sensory
processing of speech sounds.
In a 2014 paper, I expanded the opera hypothesis to consider musician benefits in the cognitive
processing of speech sounds.
I was inspired by the ideas of Nina Krauss and Mireille Bisson concerning how music
training might enhance cognitive factors like short-term memory for sound patterns and selective
attention to one sound when multiple sounds are present.
The opera hypothesis now states that if music and language share sensory or cognitive brain
mechanisms, and music places higher demands on these mechanisms, then music training can
enhance those mechanisms through experience-dependent neuroplasticity, even if the training involves
emotion, repetition, and attention.
In the opera framework, music's strong relationship to emotion is seen as an enabler of neuroplasticity.
It's the combination of this emotional power with the high sensory and cognitive demands
of music that gives music the ability to affect language processing.
The coming few decades will be an exciting time for research on the relationship of music
training to other cognitive skills.
The combination of well-designed experiments, clear theoretical frameworks, and detailed
brain measurements will provide us with information that will shape our understanding of the mind.
This information will also improve our ability to help people with language deficits or other
cognitive disorders using music.
