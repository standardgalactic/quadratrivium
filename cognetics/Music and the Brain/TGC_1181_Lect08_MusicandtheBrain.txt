In this lecture, we continue our explorations of how pitch is organized in music, focusing
on melody and harmony.
Discussing melody and harmony will help illustrate the fundamental role of expectation in music
perception and will also allow us to look deeper into the relationship between music
and language as cognitive systems.
Melody refers to how a single stream of pitches is organized over time.
We can think of this as the horizontal organization of pitch and music, if you think of melody
as a graph of pitch versus time.
Pitch height is on the y-axis and time is on the x-axis.
Harmony refers to how multiple pitches are combined simultaneously or near simultaneously
and how these combinations are sequenced over time.
In western music, the formation of chords is a fundamental part of harmony.
Going back to our graph analogy, we can think of chord formation as the vertical organization
of pitch and music, since it concerns how pitches co-occur in time.
For example, two very common chords in western tonal music are the major chord, which sounds
like this, and the dominant seventh chord, which sounds like this.
We'll talk later about how these chords are built.
Right now, I can use these two types of chords to illustrate the relationship between musical
structure and expectation.
Let's listen to a short chord progression made from two major chords and a dominant
seventh chord.
As you listen, try to judge for yourself if the progression sounds complete or incomplete.
That progression stopped on a dominant seventh chord.
For listeners familiar with western music, it sounded very incomplete.
That chord sequence aroused an expectation for a particular following chord, and if you
hear that chord, then the progression sounds complete like this.
This ability of musical sequences to arouse strong expectations is a fundamental part
of music cognition.
This is because there is a strong link between music's ability to arouse expectation and
its ability to evoke emotion, as we'll discuss later.
In music, expectations arise because melodies and harmonies are made by combining pitches
in principled ways.
As listeners, we implicitly learn those principles, and this implicit knowledge guides our musical
expectations without any conscious effort on our part.
There is an analogy here to language.
In language, sentences are made by combining words in principled ways, and this guides our
linguistic expectations.
That is, as we process the words of a sentence, we often subconsciously use the structural
principles of our language to predict what's coming next.
The particular structural principles differ from language to language, but all languages
have a set of principles for building sentences.
Similarly, all widespread musical traditions have principles by which pitches are combined
to make melodies, though the principles may differ from culture to culture.
We'll focus on western music in this lecture because its principles have been the most
studied from a theoretical standpoint, and because harmony is particularly strongly developed
in this tradition.
Let me give you another example of how our implicit knowledge shapes our perception.
Listen again to this melody, and once again, focus on whether it sounds complete or incomplete.
For many people, that melody sounded incomplete.
That last tone had a feeling of tension associated with it.
Now listen to another melody, and judge whether it sounds complete or incomplete.
For many people, that melody sounded complete.
The last tone had a sense of resolution.
Now let's listen to the last tone of the first melody, and the last tone of the second
melody, back to back.
They are identical.
The sense of tension associated with that pitch in the first melody, and of resolution
in the second melody, were entirely in your mind, and reflected your implicit knowledge
of the principles of tonal music.
We'll discuss these examples again later, once we've had some vocabulary related to
the concept of musical key.
This concept is essential for understanding how melody and harmony are structured in western
music.
But before we dive into the basic facts about musical key, let me give you one more example
of how your implicit knowledge of music shapes perception.
Listen to this melody, and try to judge if any of the notes sound a bit out of place.
For many listeners who grew up hearing western music, there were a few notes in that melody
that popped out.
You might even say they sounded rather dissonant.
But there was nothing inherently dissonant about those notes.
Each one was a perfectly well tuned note.
The notes were contextually dissonant because they were unexpected, given our implicit knowledge
about how pitch patterns tend to be organized in western music.
In real music, this contextual dissonance can be a rich source of expressive power for
composers when these notes are placed artfully at particular points in a melody.
Let's listen to a melody that uses contextual dissonance in a much more musically sophisticated
way to bring energy and expression to a melody.
Our perception of the abstract qualities of musical tones, like tension, resolution,
and contextual dissonance, reflects sophisticated neural processing that depends on more than
just the primary auditory cortex of the brain.
In fact, brain imaging research suggests that processing these aspects of music may involve
mechanisms also involved in processing linguistic grammar.
The idea that there may be deep connections between the way the mind processes musical
and linguistic structure actually predates modern brain studies.
In the 1970s, the composer and conductor Leonard Bernstein gave a set of lectures at Harvard,
which were later published as a book called The Unanswered Question.
In those lectures, Bernstein speculated about connections between musical and linguistic
grammar in the mind.
This was 20 years before the rise of modern brain imaging, so he was relying on his intuition
rather than on any solid evidence.
His ideas were greeted with a lot of skepticism by researchers at the time, but as we'll
see later, he may have been on to something.
In order to talk about relations between the processing of musical and linguistic structure
or about musical expectation and its links to emotion, we need to go into the concept
of a musical key.
What is a musical key and how does it differ from a musical scale?
In our last lecture, we talked about scales, which are sets of pitches and pitch intervals
created by dividing up the octave in a particular way.
For example, the major scale is made by dividing the octave according to a particular pattern
of pitch intervals starting from the lowest pitch of the scale.
If we look back at the piano keyboard, we can see that the C major scale starts on C
and uses seven out of twelve possible pitches in the octave, based on a particular pattern
of intervals between tones.
To make the C major scale, you start on C, then go up two semitones to D, two more to
E, one more to F, two more to G, two more to A, two more to B, and then one more to
get back to the first C of the next octave.
Using this formula, we can make a major scale starting on any pitch in western music.
The key of C major uses the C major scale, but a key is much more than just a set of
pitches from a scale.
It also involves using the scale pitches in a particular way.
For example, the starting note of the scale serves as the structurally most central note
when making melodies.
It's played often, especially at structurally important points in melodies like the ends
of phrases.
This note is called the tonic in music theory, and it comes to serve as a kind of cognitive
reference point so that other pitches are heard in relation to it.
For example, the seventh tone of the scale, which is a B in the key of C major, is structurally
much less central in melodies and often leads back to the tonic.
In fact, this note is called the leading tone in music theory because it so often leads
back to the tonic tone.
Going back to two of our earlier examples, in the melody where the last note sounded
tense and incomplete, that note was the leading tone in the key of the melody.
In the other melody, where the physically identical pitch sounded resolved like a good
ending point, the note was the tonic in the key of the melody.
These different percepts emerge because of your implicit knowledge about the roles different
members of a scale play in a musical key.
In the major scale, notice that the leading tone and the tonic are neighbors, such as
B and C in the C major scale.
They are physically quite close as pitches, yet they are psychologically quite distant.
One is very stable, one is very unstable.
In 1982, Krum Hensel and Kessler published a landmark study in which listeners heard
a short musical context and then heard a single pitch or probe tone.
The listeners were asked to rate how well the probe tone fit with the preceding context.
If the pitch was the tonic, it got a very high rating.
If it was the leading tone, it got a much lower rating.
Here is the entire probe tone profile for pitches played after a passage in C major,
showing ratings for all 12 pitches in the octave.
Notice how jagged this graph is.
The highest rated note is the tonic C.
The closest tone to C in terms of frequency is C sharp, just one semitone above C.
But notice how low the probe tone rating of C sharp is.
This is part of the richness of musical key.
Notes can be very close in terms of physical distance and frequency, but very distant psychologically.
If you remember the melody we heard earlier in which some tones popped out and sounded
a bit dissonant, those are all out of key notes, which we can highlight now on this
probe tone graph.
Notice how they all get very low ratings in the probe tone experiment.
Now let's look at just the in key notes in the probe tone graph.
Notice that their ratings differ a lot.
After C, the next highest rated tone is not the closest scale tone, which is D, it's
G, which is five scale tones above C.
The music theoretic name for the fifth tone in the major scale is the dominant.
It's the second most central pitch in a key after the tonic.
You can see from the graph that different pitches of the major scale have different
degrees of structural centrality to the key.
And this degree is not just a simple function of physical frequency distance from the tonic.
In a musical key, the psychological distance between tones is not the same as physical
distance.
I think this contrast between physical and psychological distance is part of what makes
music in a key so psychologically powerful.
In the rest of this lecture, I'll use the term tonality or tonal melodies or tonal music
to refer to western music that's organized in terms of a musical key.
I've taken some time to explain tonality because it contributes to making melody processing
a little bit like sentence processing.
What do I mean by that?
When people process a tonal melody, they make mental connections between notes that aren't
right next to each other.
When we process sentences, we make mental connections between words that aren't right
next to each other.
After the sentence, the girl who kissed the boy opened the door.
If you speak English, you know that even though this sentence contains the sequence of words,
the boy opened the door, it wasn't the boy who did the opening.
Our mind makes a connection between girl and opened because of our implicit knowledge
of the structural principles of English.
In tonal melodies, we can also make mental connections between events that aren't right
next to each other.
For example, when you hear a jazzy version of a familiar tune like Amazing Grace, your
mind can recognize that original tune by picking out certain important pitches that
form the skeleton of the melody.
To recognize the tune, your mind mentally connects the pitches of the skeleton, even
though they're not right next to each other.
How do we recognize the skeleton of a melody?
In 2005, Edward Large and colleagues studied this by having pianists improvise variations
on simple children's melodies like Hush Little Baby.
They found that to keep the melody recognizable, the musicians kept certain pitches in the original
melody intact.
These were its melodic skeleton.
They altered the less important pitches to create variations of the melody.
One of the main things that predicted which pitches were kept and which were altered was
their structural centrality in the key as quantified by Crum Hansel and Kessler's probe
tone experiments.
In other words, variation in the structural centrality of pitches in a key is part of what
allows us to process melodies hierarchically.
That is not just in terms of relations between immediately adjacent pitches.
Hierarchical processing is also fundamental to language.
It allows us to mentally connect words that aren't right next to each other in a sentence.
In a 1983 landmark book called A Generative Theory of Tonal Music, the music theorist
Fred Lerdahl and the linguist Ray Jackendorf used some of the tools of linguistic theory
to analyze the hierarchical structure of tonal melodies and harmonies.
They recognized that their analysis suggested a certain parallel with language processing,
but they warned that the details of tonal and linguistic hierarchies were actually very
different.
In fact, in the years following their book, evidence began to accumulate that processing
of tonality had nothing in common with the processing of linguistic grammar.
This came from patients with brain damage that impaired their perception of tonality
but left their language processing completely intact.
For example, one patient, nicknamed GL, was studied by the neuropsychologist Isabel Peretz,
a pioneer in the study of brain-based musical disorders.
GL had suffered damage to higher-order processing regions in his auditory cortex on both sides
of his brain due to strokes.
His basic ability to discriminate pitches was fine, reflecting the fact that his primary
auditory cortex was intact.
GL had been a music lover prior to his strokes, but after his strokes he complained that his
music perception had changed dramatically.
Peretz found that he could tell when two melodies were the same or different, so his
short-term memory for pitch patterns was intact.
But when she tested him on the Krumhansl-Kessler probe-tone task, GL showed a very abnormal
pattern.
His ratings of out-of-key notes, like the notes C sharp in the key of C, weren't any
different than his ratings of in-key notes.
Also, he didn't find novel tonal melodies any easier to remember than atonal melodies,
melodies that weren't structured in terms of a musical key.
Imagine if the following two melodies sounded equally challenging to you, equally hard to
learn.
The first is tonal, and the second, atonal.
GL had lost his sensitivity to tonality.
But he had no language deficits.
This was a classic case of amusia without aphasia after brain damage, a deficit in music perception
without any deficit in language processing.
Cases like GL made it seem like musical tonality and language processing had nothing in common.
Then around the turn of the millennium, something new happened.
Brain imaging methods, like EEG and FMRI, began to be used to compare the processing
of tonality to the processing of linguistic grammar.
I was involved in this work, and a paper I published with others in 1998 provided the
first evidence that in healthy normal people, there appeared to be overlap in the brain
mechanisms involved in processing tonality and linguistic structure.
Soon thereafter, studies by Stefan Kolsch, Barbara Tillmann and their colleagues showed
that processing musical tonality appeared to engage Broca's area, a region of the left
inferior frontal cortex known to be involved in processing linguistic grammatical structure.
These brain imaging studies used chord sequences instead of melodies, so I should take a moment
to discuss some basic facts about chords and how they're built within a major key.
Chords are made by combining scale tones in particular ways.
In the musical key, each tone of the scale can serve as the basis for a chord, which
is a collection of simultaneous or near-simultaneous pitches.
For example, in the key of C major, the C major chord is C, E, and G, and the G major
chord is G, B, and D. Just as with the individual tones of the scale, different chords vary
in how structurally central they are to the key.
Chords built on the first, fourth, and fifth scale tones are the most structurally central
chords and are called the tonic chord, the subdominant chord, and the dominant chord respectively.
Let's listen to these chords in C major, where the tonic, subdominant, and dominant chords
are built on C, F, and G.
As every beginning guitar player learns, many simple pop and folk songs can be played with
just these three chords, and a prototypical chord progression in these songs goes from
tonic to subdominant to dominant and back to the tonic.
Many different melodies can be harmonized with this same progression.
Let's hear two different melodies, first alone and then harmonized with this same progression.
Before we go on, I want to take a moment to say something about the dominant seventh
chord, which we heard at the beginning of the lecture.
It sounded like this.
That chord is made by taking the dominant chord and adding another note to it, a note
that adds a feeling of tension or urgency to the chord.
Let's listen to a chord progression first ending on a plain dominant chord and then
on a dominant seventh chord.
That dominant seventh chord really heightens our expectations.
We expect the sequence to resolve back to the tonic chord like this.
In the last lecture, we discussed the interval of the tritone, which was once shunned as
sounding dissonant and diabolical in western music.
In fact, it's the tritone that helps give the dominant seventh chord its urgent quality.
That extra tone added to the dominant to make it a dominant seventh creates a tritone interval
within the chord.
Dissonance is often thought of as bad in western music, but the dominant seventh shows how
important it is for driving harmonic progressions forward.
Going back to the brain imaging studies of musical tonality, these studies examined what
happened in the brain when a listener heard an unexpected chord.
This could be a chord from a different key, in which case it sounded contextually dissonant,
or a chord from the same key, but which was unexpected at a particular point in the music.
The finding that processing these chords engaged language areas in the front of the brain created
a paradox in the neuroscience of music.
As we discussed earlier, there was evidence from brain damage patients like GL that one
could lose sensitivity to tonality without having any language problems.
In other words, evidence from brain imaging suggested overlap in the processing of tonality
in language, and evidence from brain damage suggested no overlap.
In a paper in 2003, I suggested a possible resolution to this paradox.
I agreed that tonality and linguistic grammar involve very different kinds of knowledge.
Our knowledge of words and how they are combined is very different from our knowledge of musical
tones and chords and how they are combined.
Music doesn't have nouns and verbs, and language doesn't have tonic and dominant chord functions.
This domain-specific knowledge could be stored in distinct parts of the brain in the temporal
lobes, and could be damaged independently, leading to a selective deficit in one domain
or the other.
On the other hand, when we process musical language, we need to access this domain-specific
knowledge and use it to integrate each word or chord into the unfolding sequence.
We need to mentally connect each incoming element to other events which may not be right
next to it in the sequence.
Connecting this integration might require brain regions in the front of the brain that
are partly shared by language and music, and which communicate with the language-specific
and music-specific regions in the temporal lobes during the integration process.
So the basic hypothesis was that language and music involved domain-specific knowledge,
but shared mechanisms which acted on that knowledge as part of sequence processing.
An analogy can help make this clear.
If you know how to play chess and checkers, you have knowledge that is specific to each
game.
For example, you know what kind of move a knight can make in chess, and you know what
it means to king a piece in checkers.
Your knowledge about the rules of each game is domain-specific because the rules of chess
and checkers are quite different.
But when you play chess or checkers, some of the cognitive processes you use are similar.
For example, both involve thinking about what your opponent is thinking, which is called
theory of mind.
We use theory of mind in many situations, not just in chess and checkers.
My argument about language and music was similar.
They have unique knowledge bases, but when you process language or music, mentally connecting
the different elements in a sequence may engage shared cognitive mechanisms.
This hypothesis generated specific predictions, which have been tested by myself and others.
So far, the hypothesis is still viable and still a topic of research and debate.
If it holds up, it would have both theoretical and practical implications for the study of
grammatical processing in the brain.
Let's turn now to the issue of how our implicit knowledge of musical structure gives rise
to expectations, and how these expectations are related to the emotional power of music.
I'll focus this discussion around one particular type of harmonic structure, which we've already
heard in the lecture, in the context of discussing musical expectations, a progression from
a dominant or dominant seventh chord to a tonic chord.
In music theory, this is a fundamental structure called an authentic cadence.
It helps establish the musical key and acts as a kind of musical punctuation mark or point
of rest.
In 1956, the music theorist Leonard Meyer published a book called Emotion and Meaning
in Music, in which he argued that there was a close relationship between expectations
aroused by music in our emotional response to music.
He especially focused on cases where expectations were aroused, but then not fulfilled, and
said that these moments triggered subtle emotional responses in us.
Meyer was decades ahead of his time.
In modern cognitive neuroscience, there's a lot of interest in the brain mechanisms
of expectation or prediction, and in the brain processes that ensue when our predictions
are not fulfilled.
It's thought that prediction is a fundamental function of the brain.
We're constantly awash in a sea of sensory input.
The more we can accurately predict what's going to happen next, the faster we can respond
to events and process them efficiently.
The brain rewards itself for making accurate predictions and triggers specific internal
processes when predictions are wrong, processes which can help trigger learning.
Modern philosophers of mind, like Daniel Dennett and modern neuroscientists like Carl
Friston, have emphasized the fundamental role that prediction plays in human cognition.
Let's use the cadence to explore the link between expectancy and emotion in music.
Internal music, one way of setting up an expectation and then thwarting it, is with a harmonic progression
that seems like it's going to end on an authentic cadence, but it actually ends another way.
For example, after a tonic subdominant dominant seventh progression, instead of delivering
a tonic, a composer may deliver a chord built on the sixth note of the scale, which would
be the note A in C major.
This is called a deceptive cadence, because an authentic cadence was expected, but something
different was delivered.
Let's listen to a chord progression that has an authentic cadence, versus one that has
a deceptive cadence, then continues to an authentic cadence.
The deceptive cadence has a characteristic sound.
Research that measures subtle physiological signals of emotional arousal, like measures
of skin conductance, has shown that people do react emotionally to unexpected chords.
Brain imaging by Stefan Kulshen, colleagues, has revealed that regions of the limbic system,
including the amygdala, respondent to the sound of the chord progression, have a characteristic
sound.
The deceptive cadence has a characteristic sound.
Brain imaging by Stefan Kulshen, colleagues, has revealed that regions of the limbic system,
including the amygdala, respond to unexpected chords.
One very interesting thing about the deceptive cadence is that it can produce an emotional
effect, even in a piece that you know well.
But when you know a piece of music well, how can a chord progression deliver anything
unexpected?
In 1994, the music psychologist, Jamshed Bharucha, made an important conceptual distinction
between two types of expectations in music.
One type is called schematic, and reflects your implicit knowledge of how western music
is generally patterned.
This type of knowledge leads you to expect that after you hear a dominant seventh chord,
you'll get a tonic chord.
Veridical knowledge reflects your memory for a specific piece of music.
Bharucha's point was that schematic knowledge operates automatically.
Even if your veridical knowledge knows about an upcoming deceptive cadence, it can't suppress
a surprise reaction triggered by your schematic knowledge.
The link between expectation and emotion in music has been a major idea in music cognition,
and has been updated and put into a modern cognitive science framework by David Huron
in his 2006 book, Sweet Anticipation, Music and the Psychology of Expectation.
Huron has argued that musical expectations and our responses to them don't just reflect
a single brain process, but the interplay of several processes which he lays out in detail
in his ITPRA theory, which stands for imagination, tension, prediction, reaction, and appraisal.
Here once again we see the idea that even a single facet of music cognition, namely expectation,
engages multiple simultaneous or near simultaneous mechanisms in the brain.
Today we recognize that emotions can be aroused not just when expectations are thwarted,
but also when they are strongly fulfilled.
One of the most important points about emotions aroused by musical expectations is that they are dynamic
and fleeting because they concern events that happen at specific moments in time.
This is a fundamentally different way of creating emotion in a listener than simply creating a static mood
through music like joy or sadness.
The emotions related to expectation in music are often subtle,
but the rich and intricate way they unfold over time makes them a critical part of our experience of music.
