Welcome back.
We began this course by exploring similarities that a wide variety of addictions share.
We looked at some of the major behavioral symptoms of addiction, including tolerance,
withdrawal, and dependence.
We also briefly mentioned that all addictions work in the same basic way by hijacking the
brain's reward circuit and reinforcing harmful behaviors.
Today, I want to go inside the brain itself to show you how that reward circuit actually
works and how its inner workings shed light on addiction.
Along the way, we'll answer a question that's fundamental to the course.
How do we actually learn from experiencing rewards or pleasures?
And how does that experience reinforce the behaviors that led to that reward?
We're going to approach this question from two angles.
First, we'll talk about the psychology of reward processing, starting with Pavlov's
famous dogs.
Then we'll turn to the neuroscience of reward processing and discuss some controversial experiments
that attempted to locate the brain's pleasure centers.
As we'll see, these two lines of research have recently converged in remarkable ways
to provide new insights into the connections between how our minds and our brains process
rewards.
And in the next lecture, we'll see how those insights have dramatically changed our understanding
of addiction.
So let's begin by talking about the psychology of reward processing.
I want to start with a famous experiment that you may be familiar with.
Ivan Pavlov's study on classical conditioning in dogs.
It turns out Pavlov was not a psychologist.
He was actually a physiologist.
In fact, he received the 1904 Nobel Prize in Physiology for his research on digestion.
Pavlov actually discovered classical conditioning by accident when he was studying salivation.
He had developed an experimental setup in which he could collect saliva directly from
the salivary glands of dogs when they smelled or saw food.
But Pavlov noticed something unusual when he measured the output of saliva.
The dogs were salivating as soon as he or his assistants entered the room, even though
they couldn't see or smell any food yet.
Pavlov got interested in this phenomenon and he began studying it directly.
For example, he tried ringing a bell before the food was presented.
Now of course, the first time he rang the bell, the dogs didn't salivate.
That makes sense because they didn't associate the bell with food.
But what he found was that if he consistently rang the bell before the food was presented,
then pretty soon the dogs would associate that bell with food.
And so now he could just ring the bell and the dog would salivate even if there wasn't
any food present.
That's the phenomenon of classical conditioning.
Pavlov had conditioned the dogs to associate the bell with the food and that led to the
conditioned response of salivation.
Now one early explanation of Pavlov's conditioning results was based simply on co-occurrence.
Maybe what's happening is that the bell and the food co-occur in time and whenever stimuli
co-occur like that, they become associated.
Furthermore, the more frequently they're presented together, the stronger the association
gets.
And sure enough, Pavlov found that if you present the bell with the food more frequently,
then the association does get stronger.
However, subsequent experiments posed problems for this simple co-occurrence explanation.
Suppose you've conditioned a dog to strongly associate a bell with food.
But now imagine that you start presenting another stimulus at the same time as the bell.
For example, maybe you ring the bell and then you make a salute.
And only then do you present the dog with food.
And you do this every time.
Ring the bell, make a salute, and then give the dog food.
Now the question is this.
Will the salute become associated with the food too?
Specifically, after enough training, will the act of saluting lead to salivation even
without the sound of the bell?
Well, according to a simple co-occurrence explanation, the prediction would be yes.
After all, the salute is co-occurring with the food.
And so if that co-occurrence occurs frequently enough, then the dog should learn an association
between saluting and food.
It turns out that prediction is wrong.
Once the dogs learned a strong association between the bell and the food, other stimuli
that occurred with the bell did not become associated with the food.
Saluting did not lead to salivation if the bell was missing.
This phenomenon is referred to as blocking.
And simple co-occurrence theories have a hard time explaining it.
So if a co-occurrence explanation is wrong, then what is going on?
Well, in 1972, psychologists Bob Rascorla and Alan Wagner proposed an answer that has become
one of the most influential theories in all of psychology.
Their so-called Rascorla-Wagner model was based on the idea that learning isn't based
on co-occurrence, but is instead based on prediction error.
So what is prediction error?
Here's the idea.
If you can already predict that the food is coming, then there's no real reason to change.
You already know what you need to know, so there's not really an opportunity to learn
something new.
On the other hand, if the food shows up when you weren't predicting it, in other words,
if you made a prediction error, then that's a situation where you really want to learn
so that you can make better predictions in the future.
So in the Rascorla-Wagner model, when an unexpected reward like food shows up, the stimuli that
were presented before its arrival begin to get more strongly associated with that reward.
For example, the first few times the food shows up after the bell, the dogs probably
weren't expecting it, and so the association between the bell and the food gets strengthened.
But pretty soon, the dogs have developed a strong association between the bell and the
food.
They know what's coming, and so there's no need to change the association anymore, because
their prediction is accurate.
Now consider what the model would predict in the situation that led to blocking.
So the dogs had already been conditioned to expect the food after hearing the bell.
But now an additional stimulus, like a salute, gets paired with the bell.
But of course the dogs are still predicting that the food will appear after the bell,
and that prediction is accurate.
So there isn't any prediction error, and there's no need for additional learning.
As a result, they don't learn an association between saluting and food.
That association is blocked.
Another way of describing the Rascala Wagner model is to say that it's a learning model
based on surprise.
That is, when you can already predict that a stimulus is coming, then it isn't surprising,
and learning doesn't occur.
But if you get surprised by a stimulus, like if the food arrives unexpectedly, then you'll
strengthen associations with cues that preceded the surprise, in the hopes of making better
predictions in the future.
Now this idea may seem simple, but it turns out that it's extremely powerful.
In fact, learning from prediction error plays a major role in modern artificial intelligence
in a field called reinforcement learning.
First simply, reinforcement learning refers to figuring out how to behave in order to
maximize long-term reward.
What action should I take right now in order to get the most reward I possibly can, both
now and in the future?
That's the problem that reinforcement learning is trying to solve.
And learning from prediction errors has proved to be an extremely effective approach.
But researchers in AI have added one more key idea that makes prediction error learning
even more powerful.
And that's the idea of backing up the prediction error.
Here's what I mean.
In most situations, a sequence of multiple events precedes a reward.
Take a board game like chess or checkers.
The reward of winning requires a long sequence of actions that slowly move you closer to
the goal.
So how can prediction error learning solve problems like that?
After all, most of the actions don't lead to reward themselves.
So how can those earlier actions get reinforced?
Let's suppose you're learning chess, and you stumble upon a sequence of moves that
lead to an unexpected victory.
Well, the unexpected success constitutes a prediction error, and so learning will occur,
and the final action that led to that success will get reinforced.
Furthermore, that prediction error can also be backed up and associated with earlier actions
that you took that led you to the final action.
And now those actions can also get reinforced more than they already were.
So this is still prediction error learning, but it's prediction error learning over time,
or what's sometimes called temporal difference learning.
In temporal difference learning, the word difference refers to the prediction error,
and the word temporal means that those prediction errors are getting backed up to actions that
occurred earlier in time.
Now to illustrate the power of temporal difference learning, consider the example of the Artificial
Intelligence Program, TD Gammon.
This was a computer program that played backgammon, and it was developed by Gerald Tesoro at IBM
back in the early 90s.
The TD in the name refers to temporal difference learning.
Basically, this program used temporal difference learning to try to get better at playing backgammon.
Tesoro programmed the system to know the rules of backgammon, but then he used temporal difference
learning to try to figure out how best to play.
At first, the program tried legal moves randomly, and naturally, it played terribly, but the
program was playing itself, so its opponent wasn't any better.
Eventually, the program would win a game, even though it was guessing.
Now of course, it didn't realize that sequence of moves would produce success, and so winning
actually constituted an error in prediction, and such prediction errors lead to learning.
So the actions that led to that reward were reinforced, and it wasn't just the final move
that got reinforced.
The moves that led up to the final move also got a little bump of reinforcement.
Now Tesoro never taught his program any of the well-known strategies explicitly.
He just let the program play against itself over and over, using temporal difference learning
to try to improve its play, and improve it did.
After playing roughly one and a half million games against itself, the program had become
extremely good at backgammon.
In fact, it was almost as good as the best human backgammon players in the world.
Furthermore, it actually came up with strategies that expert humans hadn't considered, or
had even ruled out.
Some backgammon experts actually felt that the game's judgment about the value of different
positions was better than human expert judgments.
And the success of prediction error learning hasn't been limited to backgammon.
In fact, this approach has now been used by countless AI systems to learn a wide variety
of complex behaviors, ranging from cognitive skills like checkers and chess, to complex
motor skills like robotic walking, controlling a remote control helicopter, and even playing
air hockey.
So we've gone all the way from Pavlov's dogs and simple classical conditioning to learning
backgammon and air hockey based on backing up prediction errors.
Now I want to come at the question of reward processing from the angle of neuroscience,
and talk about the neural mechanisms involved.
As we'll see, recent evidence suggests that prediction error learning isn't restricted
to AI systems.
It may actually be the way reinforcement learning works in the human brain.
To explore this idea, let's go back to the 1950s, when Robert Heath performed some controversial
experiments on psychiatric patients.
Heath actually implanted electrodes in their brains, and the electrodes were connected
to a box with buttons that would stimulate the particular regions of the patient's brain.
So the patients could actually stimulate their own brains by pressing a button.
When the electrodes were implanted in a deep midline part of the brain called the septal
region, the patients reported feeling pleasure and even excitement whenever the electrode
was stimulated.
In fact, they would repeatedly press the button over a thousand times and even complained
when the box was taken away, asking for just a few more button presses.
Now if you're thinking this sounds eerily like the behavior of a drug addict, you're
not alone.
And as we'll see, the same areas of the brain are involved.
The original self-stimulation studies were performed in rats by James Olds and Peter
Milner at McGill University in 1954.
They implanted an electrode in the rat's septal region, and they used a wire to connect that
electrode to a lever that the rat itself could press.
So whenever the rat's paw hit that lever, it sent electricity to the electrode and stimulated
its own brain.
Olds and Milner found that the rats kept hitting that lever over and over again in order to
stimulate their brain.
In fact, the rats would press the lever thousands of times an hour for days on end.
There was no evidence that they ever got satisfied or had had enough.
In another experiment, scientists even placed an electric grid between the rat and the lever.
The grid delivered a painful shock, so if the rat wanted to press the lever, it had to
be willing to cross the grid and withstand significant pain.
But the rats were more than willing to endure those shocks to get to the lever and self-stimulate
their septal region.
And what's even more surprising, when scientists performed the same experiment using food instead
of brain stimulation, the rats would not cross the grid.
They would rather starve than get shocked.
The bottom line is that the rats would rather press that lever than do virtually anything
else.
If they had to choose between eating and pressing the lever, they would choose the lever.
This experiment meant starving to death.
Stimulating the septal region has also been found to be chosen over sleep, over taking
care of children, and over sex.
Once again, this sounds a lot like addiction, don't you think?
Now brain structures in and near the septal region are actually part of a neural circuit
that plays a crucial role in processing reward and in motivating us to pursue reward.
Think of this reward circuit as a primitive animal part of the brain that's important
for survival and that underlies our desires and drives like hunger and sex.
And in addition to playing a central role in motivating normal behaviors like eating,
the brain's reward circuit also plays a crucial role in the pathological behavior of addiction.
So let's dive down and take a look at the major parts of the brain that are involved
in processing reward.
There are three main components that I want to mention.
Number one, the nucleus accumbens.
Number two, the prefrontal cortex.
And number three, the ventral tegmental area, or VTA.
Let's start with the nucleus accumbens.
The nucleus accumbens is often referred to as the brain's pleasure center.
This is one of the regions that both rats and humans repeatedly stimulated to the exclusion
of everything else in the self-stimulation studies that we discussed.
It's above and just behind your sinuses, near the midline of the brain.
This region has been associated with a wide range of pleasures.
For example, it's been found to be active when rats taste something sweet.
And men see pictures of attractive women, and even when mothers are with their babies.
The nucleus accumbens is also a major site of action for all drugs of abuse.
And it's thought to play a central role in the euphoric effects that they produce.
Now let's turn to the second major brain area involved in processing reward, the prefrontal
cortex.
The brain is covered by layers of neurons, kind of like bark on the outside of a tree.
That bark is your cerebral cortex.
If you look at a cross-section through a brain, you'll see a distinction between gray
matter on the outside and white matter inside that.
The gray matter on the outside is the cerebral cortex, and it's often divided into four lobes.
The occipital lobe at the back of the brain, the temporal lobe behind the ears, the parietal
lobe at the top of the brain, and the frontal lobe at the front of the brain.
And the front part of the frontal lobe is called the prefrontal cortex, and it plays
a central role in processing reward, sort of.
Actually the prefrontal cortex doesn't process reward itself, but it's critical in controlling
the urges coming from the reward circuit.
Whereas the reward circuit is more primitive and impulsive, the prefrontal cortex is more
sophisticated and deliberate.
We use the prefrontal cortex to plan and think about future consequences, so that we can
make thoughtful decisions, rather than just following whatever urge we currently feel.
The prefrontal cortex is often described as the CEO of the brain.
So like the CEO of a company, the prefrontal cortex doesn't perform the lower-level operations
of the brain, like vision or motor control, rather it oversees and manages the big picture
operations of the whole company.
It sets goals that need to be accomplished, and then makes sure that those goals are actually
carried out.
Now patients who have damage to the prefrontal cortex will typically exhibit the symptoms
of a so-called dis-executive syndrome.
These patients might be able to perform single actions easily enough, so they might be able
to cook green beans, or roast a chicken, or make mashed potatoes.
But they'll have significant difficulty planning and coordinating a sequence of actions.
So preparing these dishes in a sequence and having them done around the same time would
pose a significant challenge.
Most important for our purposes, patients with damage to their prefrontal cortex exhibit
symptoms of dis-inhibition, meaning they have difficulty inhibiting their impulses.
They often have a hard time controlling their emotions, and they'll suddenly blow up in
an angry outburst for no good reason.
They'll also have trouble in social settings because they have a hard time inhibiting their
impulses.
They might become aggressive or exhibit inappropriate sexual behavior.
And the prefrontal cortex also plays a central role in controlling addictive behavior.
The primitive reward circuit doesn't think about consequences, and it's constantly urging
us to do whatever gives us pleasure or reward.
It's the prefrontal cortex that can consider consequences and exert self-control.
Okay, so now we've talked about two of the major brain regions involved in processing
reward.
First, the nucleus accumbens, which we can think of as the brain's pleasure center.
And second, the prefrontal cortex, which is the CEO that allows us to control ourselves
and inhibit undesirable behavior.
Now let's turn to the third component, the ventral tegmental area, or VTA.
The VTA is in what's called the midbrain.
At the top of the brainstem, the most primitive part of the brain.
The VTA is located very near the middle of the head, just slightly above the ears, a
couple of inches behind, and a little below the nucleus accumbens.
Now brain cells in the VTA project to both the nucleus accumbens and the prefrontal cortex,
and can therefore influence both pleasure and self-control.
But what's most interesting about these cells is when they fire, and when they don't.
Wolfram Schultz and his colleagues performed some of the most influential studies of these
cells at the University of Freiburg in Switzerland.
What they did was to implant electrodes in the VTA of monkeys so that they could record
neural activity while the monkey was performing different tasks.
In one task, the monkeys had to press a lever whenever a light was flashed, and whenever
they did so, they would get a squirt of good tasting juice.
And the juice served as the monkey's reward.
If the monkey didn't press the lever after the light flashed, then it wouldn't get the
juice.
So the monkey had to learn by trial and error how to obtain the reward.
Now when the monkey first started performing the task, it obviously didn't know that pressing
the lever was associated with reward.
Nevertheless, it would occasionally hit the lever after the light appeared, and it would
get some juice.
And Schultz found that neurons in the VTA fired a lot when the monkey got that juice.
Now a natural explanation is that the VTA neurons fire whenever a reward is presented.
And for a long time, scientists assumed that that's what these neurons did.
However, something interesting happened as the monkey learned the task.
Pretty soon, the VTA neurons didn't fire when the monkey got the juice.
So what's going on?
Why would the neurons fire initially when the monkey gets the juice, but then stop firing
later on in learning?
Well, Schultz proposed that the activity of the VTA neurons actually reflects a surprising
reward, not just any reward.
Here's the idea.
When the monkey is first performing the task, he has no idea that hitting the lever after
the light appears will lead to a juice reward.
So when he gets the first squirt of juice, he's surprised.
And the VTA neurons fire a lot.
However, pretty soon, the monkey realizes that hitting the lever after the light always
leads to the juice reward.
And he can therefore accurately predict that the reward is coming.
And since the reward is no longer surprising, the VTA neurons don't fire when it happens.
They only fire for unexpected rewards.
But Schultz also found something else that was very interesting.
After learning, the VTA neurons fired when the light flashed, as if the reward had backed
up from the juice to the light that preceded the juice.
Now, can you think of why that would be?
A flashing light certainly isn't rewarding by itself, so why would the VTA neurons fire
in response to the light?
Well, Schultz's explanation was that even though the light isn't a reward itself, it
does predict future reward.
Furthermore, the timing of the light isn't predictable.
It's a surprise.
Schultz therefore proposed that activity in the VTA neurons reflects reward prediction
error.
Whenever the light is off, the prediction is that there will be no reward.
But as soon as the unpredictable light comes on, the VTA neurons fire, signaling that that
prediction was wrong.
Now, does the behavior of these neurons ring any bells?
When these neurons fire, that doesn't necessarily mean that a reward has arisen, rather it
means there's been a reward prediction error.
And recall that prediction error is just what the Rascala-Wagner model used to signal a
need for new learning.
So we see a convergence between the psychology of reward processing and the neuroscience
of reward processing.
But the convergence goes even further than that.
Remember the key idea behind temporal difference learning, the algorithm used in AI research?
In addition to learning based on prediction errors, temporal difference learning backs
up that prediction error to previous stimuli, like earlier moves in chess or backgammon that's
set up the final winning move.
And that's exactly what Schultz observed in these VTA neurons.
After learning, the prediction error signal backed up from the juice to the light.
So we have this very close correspondence between the temporal difference algorithm and
the activity of neurons in the VTA.
And don't forget, AI has demonstrated that temporal difference learning is extremely
general, powerful, and effective.
So it makes sense that the brain would be using a very similar approach.
This convergence of brain evidence with the psychological evidence has led many neuroscientists
to conclude that prediction error learning and specifically temporal difference learning
may be one of the fundamental mechanisms of learning in the brain.
Furthermore, this type of reinforcement learning from prediction error is now widely believed
to play a central role in the neuroscience of addiction.
In our next lecture, we'll bring all this together.
We'll combine what we've learned so far about addiction with what we've learned about
the brain's reward system to see how our reward system can get hijacked and actually
encourage addictive behaviors.
To do that, we'll dig a little deeper into the brain and explore how drugs of abuse affect
the reward circuit and ultimately change it.
See you then.
Thank you.
