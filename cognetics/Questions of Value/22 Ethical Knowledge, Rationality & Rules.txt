Lecture 22 Ethical Knowledge, Rationality, and Rules
Many of the lectures in this series have focused on issues in ethics, and many people tend
to think of ethics in terms of following rules, a list of do's and don'ts, much in
the style of the Ten Commandments, honor thy father and mother, thou shalt not kill, thou
shalt not commit adultery, thou shalt not steal.
Is ethics essentially a matter of rules, or is it something else?
That's what I want to talk about over the next 30 minutes or so, the role of rules in
our ethical thinking.
An interesting place to start the inquiry is with a glance at some of the classic psychological
work on moral development.
Jean Piaget worked originally as a biologist, and in fact published 20 scientific papers
on mollusks by the time he was 21.
But he's best known for his studies in the cognitive and conceptual development of children
starting in the 1930s.
One group of Piaget's studies focused on moral development.
Based on these studies, Piaget claimed that there's a change in the character of children's
moral thinking in their early teens.
Before that age, they are moral rule followers.
They think of rules as laid down in stone and incapable of change.
As they enter their teens, Piaget says, they begin to see moral rules as something more,
as reflecting conventional devices that facilitate social cooperation.
In a sense, their thinking has moved from the letter of the law to the purpose of the
law.
For Piaget then, there's an essential development in moral thinking that moves beyond the rules.
There are other developmental changes in moral thinking too, though less important for our
purposes here.
Piaget claims that younger children are more likely to judge things in terms of consequences.
They're little utilitarians.
Younger children are more likely to judge actions in terms of intentions.
If that's right, the move is toward a more Kantian approach.
That move toward the more Kantian approach is a mark of moral development.
In the early 1970s, Lawrence Colberg completed a more extensive and systematic series of
studies on the development of moral reasoning in children.
What he proposes are three major levels of moral development, and each of the three levels
is divided into two stages.
Colberg's first level is called pre-conventional, and includes a first stage that corresponds
roughly to Piaget, a stage of moral thinking geared to obedience and structured in terms
of rules.
Colberg calls his second level of moral reasoning conventional.
Here morality is conceived in terms of social relationships.
Just as in the pre-conventional level, the conventional level includes a stage in which
moral thinking functions primarily in terms of obedience to rules.
In this stage, however, the rules are not understood as having a force in themselves,
but rather as conventional tools for the maintenance of social order.
Colberg labels his third level of moral development post-conventional.
This final level of moral reasoning includes critical reflection on social conventions
themselves.
For example, moral reasoning now includes a recognition of rights against the majority.
Colberg's final stage involves thinking in terms of justice along Rawlsian lines, and
in terms of universal principles such as cons.
So in Piaget's theory, it's tempting to characterize moral development as moving beyond mere rules.
In Colberg, that's more difficult, because it depends on what we mean by rules.
Rules appear as somehow authoritative in their own right in Colberg's earliest stage.
They appear in Colberg's second level of development as conventional rules necessary for social
order, but something like rules also appear at the final stage in Colberg, this time in
the guise of universal principles.
So is ethics properly conceived of in terms of moral principles, moral rules, or not?
If it's not to be conceived of in terms of principles, how is it to be conceived?
A different picture, not phrased in terms of rules, appears in W. D. Rawls's book The
Right in the Good, a seminal work in 20th century ethics first published in 1930.
Rawls starts with a critique of utilitarianism, the idea that right actions are those that
produce the greatest good for the greatest number.
Rawls doesn't think that's correct.
He doesn't think that considerations of consequences, essential in defining of utilitarianism,
he doesn't think that considerations of consequences are enough.
Here it helps to introduce a technical term.
The definition of deontological ethics is this, a theory counts as deontological if
it holds that there's something of ethical importance beyond consequences, something
beyond the greatest good for the greatest number, and Rawls is therefore a deontologist.
The philosopher who's usually put forward as an example of deontological ethics is
Immanuel Kant, and we used Kant as the prime example of a pure right-based theory of ethics
in an early lecture in the series, you'll remember.
But in this regard, Kant is a bit of an extreme in the deontological spectrum because Kant
thinks consequences don't matter at all.
Rawls doesn't think that.
Of course consequences matter, Rawls says.
It's just that they are not all that matter.
So Rawls remains a card carrying deontologists because he thinks there are also other things
that matter, things beyond consequences, that there are things of ethical importance
beyond consequences.
Rawls's then is a deontological theory.
There is something important for ethics beyond consequences.
What Rawls says is that utilitarianism is too simple to be right.
Our ethical thinking regarding other people is really a very complex tangle of ethical
relationships, and one problem with utilitarianism is that it portrays that complex set of relationships
as something all too simple.
Here's a quote from Rawls in critique of utilitarianism.
Utilitarianism says in effect that the only morally significant relation in which my neighbors
stand to me is that of being possible beneficiaries of my action.
They do stand in this relation to me, and this relation is morally significant.
But they may also stand to me in the relation of promisee to promiser, of creditor to debtor,
of wife to husband, of child to parent, of friend to friend, of fellow countrymen to fellow
countrymen and the like.
And each of these relations is the foundation of a prima facie duty, which is more or less
incumbent on me according to the circumstances of the case.
Rawls's outline of ethics is written in terms of these basic prima facie duties.
In any situation, there are a number of demands on you.
That's the idea.
Different demands, ethical pulls in different directions.
Those are your prima facie duties.
Those are the ethical things you have to consider in deciding what to do.
In any situation, there's also something you should do all things considered.
That's what Rawls calls your obligation sans frasse.
So here's Rawls again.
When I'm in a situation, as perhaps I always am, in which more than one of these prima
facie duties is incumbent on me, what I have to do is to study the situation as fully as
I can until I form the considered opinion, it's never more, that in the circumstances
one of them is more incumbent than any other.
Then I am bound to think that to do this prima facie duty is my duty sans frasse, sans frasse
in the situation.
Now there's something that seems very right about that picture of ethical reflection.
That does seem to be the way we think about things.
Kant thought that lying was always wrong, whatever the circumstances.
Despite the fact that he's identified with Colberg's highest level of moral development
in terms of universal principles, Kant's insistence that lying is always wrong sounds
very much like the rule bound first stage in both Colberg and Piaget.
By contrast, Rawls does not think that lying is always wrong, but it is always prima facie
wrong.
That means that lying always comes with a black mark against it.
Nothing is ever right just because it's a lie.
Those cases in which lying is not wrong will be cases in which there's something else
important at stake.
If you have to lie to save someone's life, for example, the prima facie rightness of
saving a life outweighs the prima facie wrongness of the lie.
In that case, the right thing to do all things considered, your obligation sans frasse, is
to save a life by lying.
Ethics for Rawls is a matter of competing prima facie obligations.
We have an obligation to prevent harm.
That's one prima facie obligation.
We have an obligation to fulfill our promises.
That's another prima facie obligation.
We have an obligation to pursue justice.
That's another prima facie obligation.
We have prima facie duties to each other in our roles of promisey to promiser, of creditor
to debtor, of wife to husband, of child to parent, of friend to friend, of fellow countrymen
to fellow countrymen.
The inevitable metaphor for decisions in these kinds of cases is weighing.
We weigh the competing values at issue, deciding what the best thing to do is overall.
The idea is that we balance values as if they were physical objects in each hand, comparing
their inertial gravity, feeling the weight of each, and trying to take that into account
in our ultimate judgment.
So how do we do this weighing?
Rawls is clear that there really is something that is the right thing to do, the right thing
to do in a particular situation.
We don't just make it up.
We have to figure it out.
Rawls is clear that in a given situation, there really is an obligation that we have.
All things considered.
What we have to do is to figure out what it is.
What guides us in doing that?
Doesn't that mean that there's a right way and a wrong way to balance competing prima
facie obligation?
And how do we know that we're doing it the right way?
How do we know that we're doing it correctly?
There's an implicit argument here that there must be something like rules.
Rules for balancing prima facie obligations.
If one prima facie obligation outweighs another in a particular situation, there has to be
a reason that it does so.
In the case of lying in order to save a life, the reason that that's the right decision
is because lives are more important than lies.
But if there are reasons for these decisions, shouldn't we be able to formulate them as
rules?
Rawls himself is pessimistic about there being that kind of rules.
But he's not entirely dogmatic about it.
Here's what he says, quote, Every act therefore viewed in some aspects will be prima facie
right and viewed in others prima facie wrong.
And right acts can be distinguished from wrong acts only as being those which, for all those
possible for the agent in the circumstances, have the greatest balance of prima facie
rightness over their prima facie wrongness.
For the estimation of the comparative stringency of these prima facie obligations, no general
rules can, so far as I can see, be laid down.
We can only say that a great deal of stringency belongs to the duties of perfect obligation,
the duties of keeping our promises, of repairing wrongs we've done, and of returning the equivalent
of services we've received.
For the rest, entay, I say, hey, crisis.
The decision rests with perception from Aristotle's Nicobakean ethics.
The sense of our particular duty in particular circumstances preceded and informed by the
fullest reflection we can bestow on the act in all its bearings is highly fallible, but
it's the only guide we have to our duty, end quote.
So regarding comparative stringency, no general rules can be laid down, Ross says, as far
as I can see.
W.D.
Ross was important as an ethical thinker in his own right and also as a translator of
Aristotle.
It's probably no accident that he did both of these things because his view, his ethical
view, has a lot in common with what Aristotle says about ethical decision.
In the Nicobakean ethics, Aristotle speaks of intellectual virtues as well as moral virtues.
In doing so, he distinguishes different applications of intelligence.
So science for Aristotle is a reasoned capacity for acquiring knowledge.
Art for Aristotle is the reasoned capacity to make things.
The intellectual virtue that's appropriate to ethics is practical wisdom, a reasoned
capacity for action, and a knowledge of how to secure the real goals of human life.
Aristotle says that practical wisdom is the quality of mind concerned with things just
and noble and good for man.
It's a reasoned capacity, but it's neither science nor art.
What we use in weighing our prima facie duties, Ross would say, is this Aristotelian practical
wisdom.
It's not a matter of rules.
The idea of having to balance the importance of competing considerations appears in other
people as well as Ross.
For example, in the work of Thomas Nagel, who we've also talked about in a previous
lecture.
In an essay on the fragmentation of value, Nagel lists several different types of values
that come into conflict.
This is a quote from Nagel.
First, there are specific obligations to other people or institutions, obligations to patients,
to one's family, to the hospital or university at which one works, to one's community or
one's country.
The next category says, Nagel, is that of constraints on action deriving from general
rights that everyone has, either to do certain things or not to be treated in certain ways.
The third category is that which is technically called utility.
This is the consideration that takes into account the effects of what one does on everyone's
welfare, whether or not the components of that welfare are connected to special obligations
or general rights.
The fourth category is that of perfectionist ends or values.
By this I mean, the intrinsic value of certain achievements or creations, apart from their
value to individuals who experience or use them.
Examples are provided by the intrinsic value of scientific discovery, of artistic creation,
of space exploration perhaps, end quote.
In a particular case, we have to balance these competing values.
But there are going to be right and wrong ways of doing that.
There will be cases in which there are clear reasons for compromising one value in order
to fulfill another.
So Nagel, like Aristotle and like Ross, thinks there is a reasoned capacity for balancing
these values.
But if we're talking about reasons, shouldn't we be able to make the reasons explicit?
Shouldn't we be able to prioritize values in terms of what outweighs what?
Here Nagel is pessimistic just as Ross is.
He says obligations, rights, utility, perfectionist ends, those different categories, these values
enter into our decisions constantly and conflict among them and within them arise in medical
research, in politics, in personal life, or wherever the grounds of action are not artificially
restricted.
What would it mean to give a system of priorities among them?
Nagel concludes that there is no overarching rule for resolution between these competing
values.
Quote, to look for a single general theory of how to decide the right thing to do is
like looking for a single theory of how to decide what to believe.
So Nagel thinks there are no rules, but that doesn't make decision irrational.
Well, we are familiar with this fragmentation of understanding and method when it comes
to belief, but we tend to resist it in the case of decision, yet it is as irrational
to despair of systematic ethics because one cannot find a completely general account of
what should be done as it would be to give up scientific research because there is no
general method of arriving at true beliefs.
So our sources, Aristotle and Nagel and Ross, all seem to agree.
In ethics we have to deal with competing considerations of value.
Given a particular situation, there are reasons by one thing, one prima facie obligation or
one category of value outweighs another.
We weigh reasons rationally and we come to a conclusion.
Reasons and rationality, but all our sources agree we have no rules.
Are there rules capable of formulation in principle?
If you stress the in principle hard enough, it seems like the answer has to be yes.
The argument that there must be implicit rules capable of formulation in principle is this.
There are reasons why ethical situations have the ethical values they do and we are able
to see or distinguish the value of ethical situations in at least many clear cases.
If that second point is true, it must be because we are able to see what it is about the situation
that makes it have a particular value and if that is true, it seems it must be possible
in principle to formulate what it is that we see in terms of explicit rules.
Now that's a promise of possible formulation in principle, but that promise of possible
formulation in principle is pretty thin.
Aristotle and Ross and Nagel are surely right that we can't formulate our ethical knowledge
in terms of rules now.
Are there other cases in which we know things that we can't formulate in rules?
Interestingly, the answer is yes.
Consider our knowledge of our own language.
We can tell in clear cases when a particular grammatical construction is misapplied or
when a particular word is improperly used.
We're confident, moreover, that when either of these is the case, there's something about
the sentence or something about its context, its context of application that makes that
a misapplication or an improper usage.
So we have an implicit knowledge of our own language.
It must, therefore, be possible to make that knowledge explicit.
The case of linguistic knowledge is a good comparison for a couple of reasons.
First of all, it's clear that our language is a social construction in some sense.
We don't read the true names of things off the things themselves.
We've evolved a cultural coordination of linguistic behaviors and expectations.
There are many who want to say much the same about our ethical values.
But to say that our values are in some sense a social construction need not entail that
situations somehow don't really have genuine ethical values.
That doesn't follow any more than to say that our language is a social construction need
entail that certain linguistic constructions aren't genuinely misapplied in certain cases.
Now is it possible to formulate a set of rules that match our linguistic judgment?
We don't have such a set of rules now, but it may be possible to formulate them.
It's clear that such a task would be an enormously complex one.
That's the task of all contemporary linguistics, and it's a task that can boast notable successes
but also has to admit wide areas of ignorance.
Somehow mastery of our language, the mastery we've acquired, somehow that mastery of language
is easy.
Somehow judgment in a variety of cases is clear.
A child picks all that up by the age of three or four.
Hundreds of linguists have been working decades trying to formulate in rules what a single
child knows about language without rules by the age of three or four.
They don't think it's impossible to formulate knowledge of language in terms of rules, but
they haven't yet succeeded.
That's precisely the situation that seems to confront us in ethics.
Mastery of ethical judgment, at least in many cases, seems clear and easy.
That makes us think that we've incorporated a set of implicit rules for ethical evaluation.
Shouldn't we be able, with a little effort, to make those implicit rules explicit?
Here as in linguistics, it may be the characterization of our knowledge in terms of implicit rules
that's part of the problem that may be at fault.
As long as we regard all cases of learned decision as cases of muttering rules to ourselves
under our breaths, applying implicit rules under our consciousness, so to speak, it will
appear that we should be able to spit out the rules that we are employing all along.
The simple truth seems rather to be that we can pick up wide areas of consistent skill
in ways that are radically disassociated from explicit rule following.
The truth in linguistics, in practical geometry, in blacksmithing, in music, and in ethics
is that we can master large bodies of consistent practice.
Because we're familiar with rule following, we're tempted to think that consistency of
practice in these areas entails at least possible formulation in terms of rules.
The truth in each case is that we haven't learned these areas in terms of rules.
We don't actually navigate them in terms of rules, and we can't generally expect to
be able to reconstruct them in terms of rules.
Contemporary computer science offers another intriguing formal analogy.
There are two very different ways in which machines can be taught to handle a particular
task.
The traditional format is serial programming.
One example is the attempt to build what are called expert systems.
These are computer programs designed to match the diagnostic skill of medical experts, for
example.
The idea is to figure out what rules the human experts are following, and then to incorporate
those rules within a standard computer program.
Unfortunately, the attempt at building expert systems of that type has met with very mixed
results.
An alternative that has shown a great deal of promise in computer science is the training
of neural nets.
This uses a very different kind of computer program that is explicitly patterned on the
way neurons function in the brain.
Programmers write bits of code that work roughly like neurons do, and they string those together
into what's called a neural net.
Neural nets do not come with all their information hardwired at the beginning.
Neural nets have to be trained through experience, just like brains do.
If we get lucky in the original architecture of the net, and here, too, there are only
rules of thumb, we'll be able to train the net on a set of examples for which we also
provide expert opinions.
We have, say, a real doctor deciding whether the symptoms in this case indicate pneumonia
or not.
What about this case?
What about this case?
As the real doctor makes every successive decision, the neural net is being trained up at his
elbow to match those judgments.
After an initial period of training, the computerized neural net will be tested on a new set of cases.
The human experts give an independent evaluation, and in at least some areas, the ability of
neural nets to generalize to new cases is astounding.
In a number of cases, neural nets have proven far more successful in giving the right answer
to a broad range of cases than has serial programming via explicit rules.
It's also interesting how little we know about a neural net when it has successfully been
trained up.
We do know that any neural net behavior can be matched, in principle, by an appropriate
set of rules in the old form of serial programming.
Neural nets can go nowhere that simple rule-following machines can't, in principle, go as well.
But when you've trained a neural net, when you open up a neural net to see what rules
it is following, when you look at the code, you don't find any rules at all.
You just find a bunch of computational neurons hooked together and adjusted in certain ways.
If you opened up your brain, you wouldn't be able to see any rules either.
And neural nets end up being a lot like that.
The fact that we can successfully train up neural nets, and the fact that they are in
principle equivalent to some conceivable rule-following program, doesn't give us any easy and effective
way of transferring neural net abilities over to serial programs.
So in linguistics, in practical geometry, in blacksmithing, and in music, we have a model
of a consistent pattern of practice for which formulation, in terms of rules, is beyond
our reach.
That appears to be precisely the case with regard to our ethical knowledge as well.
It's as if we were trained up as neural nets, consistent, effective, capable of distinguishing
clear cases.
We're also like neural nets in that the transfer of this kind of capability to a structure
in terms of explicit rules, even if it's possible in principle, is beyond our reach
in practice.
In the next lecture, I want to turn to the idea of moralities as complete moral codes.
The topic is Moralities in Conflict and Moralities in Change.
