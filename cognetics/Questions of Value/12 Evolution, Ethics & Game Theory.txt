Lecture 12 Evolution, Ethics, and Game Theory
In the previous two lectures, I talked about cultural anthropology and the issues of cultural
relativism.
In this lecture, I want to talk about two other fascinating areas of recent work.
First of all, evolutionary theory applied to social organization.
Here, E. O. Wilson's work in sociobiology is preeminent.
And secondly, the development of game theory and its application to issues of social organization.
Here the founding father is John von Neumann.
Both of these areas of study are fascinating in their own right.
What I want to focus on in this lecture is their application to ethics.
Both evolutionary theory and game theory may do something to help us understand both social
organization and social ethics.
Alternatively, as handled by E. O. Wilson himself, for example, they offer not a promise
of understanding, but a kind of intellectual threat to ethics.
Let me explain the kind of threat that's an issue.
One of the motivations that drives theoretical ethics is the idea of understanding the conceptual
structure that underlies our moral judgments.
As we've seen in earlier lectures, ethical thinkers like Plato and Aristotle and Kant,
Bentham, Mill, and more are all trying to get a grasp on an underlying structure that
explains the whole range of our ethical attitudes.
This is where what's called evolutionary ethics seems to pose a potential threat.
What if we look for an underlying structure of ethics, a structure that explains our moral
judgments, but what if we find the wrong kind of explanation?
How could an explanation be the wrong kind?
The theoretical explanations for ethical beliefs that philosophers have traditionally looked
for have two basic characteristics.
What they have sawed is an underlying structure that would explain why we hold certain ethical
intuitions and would do so, at least in part, in large part by justifying major aspects
of our ethical belief.
It is in principle possible, however, that those two aspects of the traditional search
explanation and justification might come unglued.
The possibility that evolutionary ethics raises is that we might get an explanation
of why we hold the ethical beliefs we do, but an explanation that doesn't seem to justify
our ethical attitudes.
It might explain attitudes all right, but in terms of justification might simultaneously
undercut them.
Both sociobiology and contemporary game theory can be read as posing that kind of threat.
Let me start with sociobiology.
Sociobiology was introduced in the work of E.O. Wilson.
It's further developed in the eloquent writings of Richard Dawkins.
The basic scientific question is this one.
Group cooperation is evident in many animal species.
Ants work cooperatively.
Wilson was an expert on ants.
Bees work cooperatively.
Wolves hunt cooperatively.
And of course, people often work cooperatively.
Altruism is also evident in many animal species.
Bees will sacrifice themselves for the hive.
Antelope jump when they see a predator, alerting the herd, but putting themselves in greater
danger.
And of course, people act altruistically, sacrificing themselves for other people.
The scientific question is how cooperation and altruistic behavior are to be explained.
The whole idea of evolution is that those animals that have more successful traits will
pass those on through their genes.
We'll therefore get more animals with those traits and fewer animals without them.
But in cooperation, you're benefiting someone else.
That's helping them to perpetuate their genes.
If you sacrifice yourself for someone else, you are thereby decreasing your chances to
pass on your genes and increasing their chances to pass on theirs.
That makes altruism and cooperation look like losing evolutionary strategies.
The altruistic and cooperative genes should be weeded out.
How then do we explain the fact that we do find altruistic and cooperative behavior in
a number of species?
The answer that sociobiology gives is still an evolutionary answer.
All genetics works in terms of evolutionary advantage.
And so somehow, altruistic and cooperative behavior must be evolutionarily advantageous
after all.
That's why they exist, where they not have evolutionary advantage, they wouldn't exist.
One explanatory strategy that's been used in sociobiology is kin selection.
The idea is that even if an animal is sacrificing itself, it may be furthering the spread of
its genes by making it possible for its close kin to reproduce.
Another explanatory strategy is group selection.
The idea that cooperative groups may be more likely to survive.
Now ours too is an animal species, of course, and we can expect the same answer to apply
to us, that the explanation for attitudes favoring cooperation and altruism is going
to be an evolutionary explanation, but that means that the ultimate explanation for ethics
is going to be an evolutionary explanation.
The sociobiological thesis is far from established as an explanation for all cases of cooperation
and altruism.
What I want to get across is just the kind of threat it seems to pose.
For the sake of argument, then, suppose that the theory were established.
Suppose we could prove that the ultimate explanation for cooperation and altruism lies in genetic
advantage and natural selection.
Without contradicting a single one of our ethical inclinations, a successful sociobiology
of that type might change our attitudes toward our ethical feelings quite radically.
I see, I had thought that ethical inclinations were somehow valuable.
I had thought that the attempt to act ethically was somehow worthy.
My mistake, it's now clear that ethical belief is a mere epiphenomenon of evolution, a predictable
but hardly a precious result of the inexorable mechanisms of biological adaptation.
The philosopher Michael Roos makes this approach perfectly clear.
Here's a quote from Roos.
Is it not the case that sometimes, when one is given a causal explanation of certain beliefs,
one can see that the beliefs themselves neither have a foundation nor could ever have such
a foundation.
Once we see that our moral beliefs are simply an adaptation put in place by natural selection,
in order to further our reproductive ends, that's an end to it.
Morality is no more than a collective illusion fobbed off on us by our genes for reproductive
ends."
Okay, let me leave evolutionary biology for a moment to introduce another body of contemporary
work that can be read as going in the same direction.
Game theory.
This is a branch of contemporary economics rather than biology, but in fact evolutionary
ethics and game theory have often gone hand in hand.
Game theory grows quite directly out of a work by John Von Neumann and Oscar Morgenstern
in 1944, the theory of games and economic behavior.
A second important figure in the development of game theory was John Nash, who you may
know from the movie A Beautiful Mind.
Here's the basic idea.
For different kinds of games, there are different strategies of play, and some of these strategies
of play may be more or less rational.
Now if the games we were talking about were just monopoly and checkers, this might seem
little more than recreational mathematics, but games in the sense of game theory include
all situations involving cooperation or competition.
The topic of game theory isn't just games then.
The topic of game theory is the question of rationality in competition and cooperation
in general.
There's one aspect of game theory that's very important for ethics.
Ethics is often conceived in an individualistic fashion.
You're looking for principles that guide your action as an individual.
But of course social life is a matter of multiple agents interacting, not just one agent acting
against the world.
That focus on interaction is something important about game theory that has the potential to
further our understanding of social values.
The focus of almost all game theory to date has been on two-person games, on games, situations
of competition and cooperation involving just two people.
Why the concentration on two-person games?
Well because those are more or less manageable.
When you step up to three-person game theory or four-person game theory, the complexity,
just the mathematical complexity, increases exponentially.
So the predominant concentration of game theory has been on two-person game theory.
The predominant concentration has also been on non-zero-sum games.
In a zero-sum game, every point that one player wins, the other player has to lose.
Winnings and losings sum to zero.
That's why it's a zero-sum game.
In a non-zero-sum game, that's not the case.
Both sides might win, both sides might lose.
It's no accident that von Neumann and Morgenstern wrote their masterpiece on game theory in
1944.
World War II was coming to an end, and von Neumann was wondering what the world was going
to be like after the war.
Von Neumann figured rightly that there would be two major players, the United States and
the Soviet Union, just like in a two-person game.
He also realized that this would be a non-zero-sum game.
The Cold War wasn't going to be a question of how the winnings would be split.
In nuclear competition between two superpowers, it was quite clear that both sides could lose
big.
Here I want to talk about just one setup regarding competition and cooperation.
One particular game in game theory.
It's called the Prisoner's Dilemma, and this is by far the most widely applied game
theoretic model.
The story behind the game, the reason it's called the Prisoner's Dilemma is this.
Lefty and Scarface are arrested for bank robbery.
They're isolated in separate cells so they can't communicate, and the district attorney
comes to Lefty, and he says, Look, Lefty, I have to admit that the evidence we've got
against you guys is pretty thin.
I'll be honest, if you both stonewall, you'll probably get off with only two years each.
On the other hand, if your friend Scarface confesses and you keep mum, we'll let him
turn state's evidence and we'll nail you for five years.
But I'll make you the same deal.
If you confess and Scarface stonewalls, we'll let you turn state's evidence and we'll
nail him for the five years.
And what if you both confess?
Great, we'll use the two confessions to get you both with a little discount for a guilty
plea.
In that case, you're each looking at four years in the slammer.
Now the district attorney is entirely honest with both Lefty and Scarface.
He outlines the same scenario for each.
Each prisoner then has to decide what to do.
So suppose you're Lefty.
What are you going to do?
Well you could keep mum.
And if Scarface keeps mum too, that's a good choice.
You'll both get only two years.
But if you keep mum and Scarface confesses, keeping mum is a terrible choice.
You'll take the rap and you'll do a full five years.
Scarface will walk.
The other possibility is you could turn state's evidence.
If Scarface keeps mum, you've aced it.
He'll take the rap and you're out of here tomorrow.
But if you confess and Scarface confesses too, things aren't so good.
In that case, you're both looking at four years.
So what's best for Lefty in this situation doesn't depend solely on Lefty.
It depends on what Scarface does too.
What's best for each player depends on what the other one is going to do.
The reason people have concentrated on this particular game is that it's characteristic
of so much of human life.
We'll all do better if we cooperate.
But if everybody else is being cooperative, you can turn a quick individual profit by
exploiting that situation.
The prisoner's dilemma has gained a lot of attention in economics, in biology, and
in social and political philosophy because it seems to capture an essential tension between
collective good and individual advantage, just like in real life.
There are two different ways that people have thought about the prisoner's dilemma.
The easiest way is as a one-shot game.
You make your decision just once and you get the payoffs just once.
The one-shot game is the way we just talked through the Lefty-Scarface situation.
The prisoner's dilemma.
They only have one time at which they're making this choice.
But the more interesting way to think of the prisoner's dilemma is to think of it as an
iterated game.
In an iterated game, we have two players playing again and again.
They play against each other 20 times or 200 times.
Or maybe they don't know how many times they're going to play.
Each time they decide whether to cooperate with the other player or to defect against
the other player.
This is a more realistic way of thinking of the prisoner's dilemma.
In most of our exchanges, we don't deal with each other just once.
We deal with each other repeatedly.
And if someone has burned you on one occasion, you're very likely to deal with them very
differently on the next occasion.
In an iterated prisoner's dilemma, where you have these payoffs, but people are playing
with each other again and again, there are certain strategies that can emerge.
Here are a few of the really simple strategies.
One strategy in an iterated prisoner's dilemma game is all defect.
The idea is, no matter what the other guy does, burn him.
You start off defecting.
If he cooperates, defect.
If he defects, defect.
No matter what he does, defect.
That's why it's all defect.
Another very simple strategy is all cooperate.
No matter what the other guy does, continue to cooperate.
This is sometimes called quaker.
The idea is you're turning the other cheek because you're cooperating even if he defects
against you.
Sometimes it's called doormat.
The strategy that's going to be most important for what we're talking about, however, is
this one.
It's called tit for tat.
In the tit for tat strategy, you start off being cooperative.
If the other guy cooperated last time, you cooperate this time.
If the other guy defected last time, you defect against him this time.
On this strategy, you're starting out being cooperative.
If he cooperates with you, you cooperate with him.
If he defects against you, you defect against him.
On this strategy, you are returning like with like.
You are giving him tit for tat.
This is where the convergence with evolutionary ethics comes in.
In 1980, the political scientist Robert Axelrod announced a computer tournament in the iterated
prisoner's dilemma.
Anyone who wanted to was invited to submit a strategy no matter how complicated that
strategy was, as long as it could be written as a computer program.
Submissions for the tournament came in from game theorists in economics, in psychology,
in sociology, in political science, in applied mathematics.
In Axelrod's tournament, each strategy that was submitted to the competition was pitted
against every other strategy that was submitted in a round robin.
Each strategy played every other strategy in the group.
Each strategy also played itself as a strategy in the group, and the winner in that round
robin tournament was the strategy that got the most points that did the best overall.
Okay, there were a number of complicated strategies that were submitted in that first 1980 Axelrod
tournament.
Axelrod played them all against each other.
He tallied up the points, and when the dust cleared, the winner was one of the very simplest
strategies.
The winner in Axelrod's 1980 tournament was Tit for Tat.
So then Axelrod put together a second tournament.
That was interesting.
He thought he'd do it again.
The results of the first tournament were announced and circulated.
Everybody knew that Axelrod had taken all of these submitted strategies, had competed
him against each other, and that Tit for Tat had won.
The invitation then went out for submissions to a second tournament.
Now it was clear at that point that the strategy to beat was the winner the first time.
It was clear at that point that the strategy to beat was Tit for Tat.
So he took the second group of submissions.
He played them all against each other, counted up the points of each strategy playing all
the others.
And when the dust cleared, the winner the second time was Tit for Tat.
In 1981, Axelrod teamed up with William Hamilton.
And together they made a tournament that changed the rules.
In the Axelrod Hamilton model, the competing strategies are first pitted in a round robin
tournament just as before, but then they reproduce in the population in accord with their rate
of success.
So you play a bunch of strategies against each other.
The ones that are more successful then are a larger portion of the population.
So you are playing more of them when you play in the second time around.
So you then have a different proportion of strategies in the population.
You play again and the proportions of the population change again.
So in that form of an iterated game, less successful strategies will die out.
Successful strategies have a higher chance of being passed on to later generations just
like in evolution.
So this is a game theoretic setup that converges to the evolutionary model.
So here then we have a formal model that's working just like evolution.
We have this convergence between game theory and evolutionary theory.
In Axelrod and Hamilton's evolutionary model, when the dust cleared, the winner of that
tournament was Tit for Tat.
Now there are two important points about this result.
The first is, Tit for Tat wins in the evolutionary competition, just as it wins in the others
just because it gets the most points overall.
These aren't built so that there's a reward for ethical or good strategies.
There are only rewards for successful strategies that get more points playing against the opposition.
This is game theory and Tit for Tat wins just because it gets a large number of points,
the largest number of points in play with a whole range of other strategies.
The other important point about this is this.
Despite the fact that it wins just because it gets the most points overall, still when
you look at it, Tit for Tat looks very familiar ethically.
What Tit for Tat does is return like with like.
It reacts to defection with defection.
It reacts to cooperation with cooperation.
And that looks a lot like the Golden Rule.
Do unto others as you would have them do unto you.
The first documented form of the Golden Rule comes from Egypt, about 2000 BC.
It shows up in the literature of the Zoroastrians in 700 BC in this form.
That nature only is good when it shall not do unto another whatever is not good for its
own self.
The Golden Rule appears in Confucius in 500 BC, and in Hinduism in 200 BC, and of course
in both the Jewish and Christian traditions.
Okay, sociobiology and game theory are fascinating in their own right, but both can be read as
making an unsettling suggestion regarding ethics.
Phrased in terms of game theory, the unsettling suggestion is this, that maybe following the
Golden Rule is nothing more than a successful interactive strategy.
Maybe its ethical value is no deeper than that.
Maybe that kind of strategy just pays.
The suggestion put in terms of sociobiology is this, maybe what we call altruism is just
an evolutionarily effective strategy.
Groups of organisms that cooperate do better than groups of organisms that don't.
And so they pass on their genes, and maybe that's all there is to ethics.
I've tried to put both the game theoretic version of this suggestion and the evolutionary
version of the suggestion in the strongest form I can.
Both sociobiology and game theory pursue interesting and important questions, but despite my attempt
to put it in the strongest form, I don't think that in the end they really undercut our ethical
intuitions.
Personally I think, although they both have forms of promise for understanding ethics,
I don't think either of them is any real threat to ethics.
There are occasionally theories that say that people act only selfishly.
There's a form of psychological egoism, for example, that says that we are all just out
for our own gain.
And some of the language that sociobiologists in particular use suggests that kind of theory.
The Richard Dawkins book is called The Selfish Gene.
And he starts out, for example, by claiming that people are born selfish.
Now if that were true, it would be true that no one ever sacrificed themselves out of genuine
concern for other people.
And that would be a threat to ethics.
It would mean that genuine altruism never happens.
And of course it happens.
People make sacrifices for other people every day.
They give of themselves and they sometimes sacrifice their lives for people they love
or sometimes for people they barely know.
The more dramatic stories of rescuers running into burning buildings show up daily on the
news and they don't appear to do it for their five seconds of fame.
Sometimes the rescuers disappear unrecognized into the crowd.
Of course altruism happens.
You know plenty of less dramatic cases from your own life, plenty of cases in which people
have sacrificed for you.
Now in the face of those facts, the only way to support this kind of theory is to trivialize
it.
People sometimes do this by saying, oh, but they were just doing it so that they would
feel good.
And I'm quite sure that that isn't true either.
The theory that says acts of altruism never happen, that's not going to be a threat to
ethics because it's not true.
The more plausible way of reading the sociobiological theory is not that people always act selfishly,
they don't.
It's not that they never act altruistically, they do.
The more plausible way of reading the sociobiological theory is that those kinds of feelings have
an evolutionary history.
It's because on this reading it's because we're the kind of organisms we are evolving
in the way we did that we have these kinds of feelings.
And that may be true, but it's no threat to ethics.
It doesn't deny that we really have these feelings about altruism and cooperation, and
it can't deny that those feelings might not be ethically right.
If it did, it would have managed what we know since Hume to be impossible, it would have
derived ethical conclusions from purely factual premises.
Here's a very simple point that I think puts all of this in perspective.
People have choices.
There are cases in which people can decide to do something that benefits themselves or
can decide to do something that benefits others.
Different people make different choices.
Some people take the course that benefits only them.
Some people do something different.
They behave in genuinely altruistic ways out of concern for other people.
They perform random acts of kindness.
Of course, it's also true that the same people make different choices on different occasions.
Now what does that say about a theory of ethical feelings that's based on evolutionary history?
Well, what would such a theory explain?
It can't be explaining why we always act selfishly because we don't always act selfishly.
And it can't be explaining why we always act altruistically because we don't always
act altruistically.
As the philosopher of science Philip Kitcher concludes, whatever is written in our genes
isn't making the difference between what choices we make.
Our choices are something above and beyond our evolutionary inheritance.
Different people make different choices, but we all have the same evolutionary history.
So it can't be evolution that's explaining the different choices.
I make different choices on different occasions, but my genetic makeup remains the same.
It can't then be my genetic makeup that explains the differences in those choices.
So the choices are still ours, and they are still genuine choices.
The most that evolution might explain is the fact that we have certain emotional and conceptual
resources at our disposal.
Let me offer an analogy.
Evolution may be able to explain, at least in outline, how we got to be so good at color
vision.
The reason is that we evolved from primates who had to distinguish ripe fruit from green
fruit far away in the treetops.
But look right now at what's in front of you.
Look at what you see.
Evolution can explain how you can see, but it can't explain that you see that.
Why you see what you see right now depends crucially on just what happens to be in front
of you right now.
In a similar way, evolution may be able to explain, at least in outline, why our brains
developed as they did, and therefore how it is that we have an ability to reason.
But it isn't going to explain how we use that reason, and it's not going to explain
what conclusions we come to.
It explains the ability, but not its application.
Game theory, by the same token, may be able to explain the effectiveness of strategies
of reciprocation, like tit for tat or the golden rule.
In building on that, evolution may be able to explain why we are capable of feelings
of sympathy for other people, concern for other people, why we may seek cooperation
as of mutual advantage.
So those theories may thus be able to explain the conceptual and emotional resources we
have at our disposal when we make the choices we make.
But no such theory is going to explain why we all make the same choices because we don't
all make the same choices.
No such theory is going to explain why some of our choices are right and some of them
are wrong.
That's something that we have to work out, given the various resources, the conceptual,
the rational, the emotional resources that we have inherited.
We've talked about aspects of subjectivity and objectivity in at least in at least indirect
ways in a number of lectures.
In the next lecture, I want to emphasize the objective side of value.
