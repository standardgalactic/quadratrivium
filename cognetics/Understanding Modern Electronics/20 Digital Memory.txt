Welcome. Here's a lecture I hope you're going to remember, because this lecture is about
memory. Digital devices need to remember. And we've already talked about flip-flops
as devices that can sort of remember, but here we want to build them into real substantial
memories such as we'd be using computers and all kinds of other devices. What do these
things need to remember? Well, your smartphone needs to remember your contacts. It needs
to remember your pictures that you've taken with it. It needs to remember the apps you've
downloaded to it. It needs to remember all these things. Your camera needs to remember
photos, instruments that scientists might use or that you might use need to remember.
You take a temperature with your digital fever thermometer and it beeps when it's done and
it locks that temperature into its memory and it remembers it. Have you got a maximum
minimum thermometer? If you've got a maximum minimum thermometer, it has memory in it that
remembers the maximum temperature and remembers the minimum temperature and only replaces them
if the temperatures exceed those limits. Computers need to remember lots of things.
You're typing at your computer or maybe texting on your phone. And every keystroke you make,
every letter you enter in is converted into one of those 8-bit strings, those strings
of digits, binary digits that use the ASCII code to represent a particular character.
And then those things are stored in memory, maybe temporarily with the text message before
it gets sent out by some kind of parallel to serial conversion scheme like we've already
talked about. Or maybe more permanently, maybe you're typing your term paper or an email message
that you're going to save and it gets saved for long term to memory.
And finally, one thing we tend not to think about as much, but this is really important
to computers in particular and was important in the development of digital computers. Not
only did they store information, that is data. The data they're going to work with, the information
on the pixels that make up the photograph, the information on the phone numbers, whatever
it is, but they also store the instructions that are used to tell the computer what to
do. So when a programmer writes a computer program, that program is also a set of binary
numbers. In fact, think of your favorite spreadsheet or word processing program. In some sense,
that program is really nothing but an enormous long binary number. And pieces of that program
are loaded into memory and executed as the computer does what it's supposed to be doing.
So in this lecture, we want to look in some detail at memories and different kinds of
memory and how memory is used and how memory is made using flip flops. And then we'll talk
about some other kinds of memory that aren't made using flip flops. So there's several
distinctions, many, many different kinds of memory. And later in this lecture, I'm going
to show you a table of many of these kinds. But I want to make two big important distinctions
right at the outset. Memory can be either volatile or non-volatile. What does that mean? Volatile
memory is memory that forgets if the power is lost. When you're working on some important
project like that term paper, you're always told, hey, you ought to save frequently. Because
if the power fails and it's not a laptop with its own battery built in, that every keystroke
you typed in is gone. Now, most modern programs have auto-save every 10 minutes or every five
minutes or maybe you can set the timing. They will save what's in there to a more permanent,
non-volatile memory. But the basic memory that's there in your computer when you're
first working on it, that is volatile. And if you quit a program and it's given you the
option to save or not save and you say don't save, gone. Because that was in volatile memory.
Volatile memory forgets if the power is lost. Why would we ever want volatile memory? Well,
it's fast and it's cheap, but it's volatile. Non-volatile memory, non-volatile, is memory
that remembers without needing power. It doesn't need the power to keep on. You turn the power
off, the non-volatile memory is still there, it remembers. Many, many, many devices have
non-volatile memory. Why can you turn your smartphone off and then summon up the picture
you took three months ago because it's in non-volatile memory? How does your computer
know to get itself started in the first place to execute the few basic instructions that
get the computer booted up to the term we use? Bring it up by its own bootstraps is what
that term comes from. Well, it's got some non-volatile memory in there that can remember. And there
are actually gradations between volatile and non-volatile. There is, for instance, memory
that can be read-only memory, it's called, but it can be rewritten, but it's non-volatile
and so on. We're not going to go into those details. But the distinction between volatile
and non-volatile is just a little bit less clear than you might think. But volatile memory
is basically memory that forgets if the power is lost and non-volatile memory remains there
even when the power is gone off. That's one distinction. The second distinction is what's
called random versus sequential memory, random access versus sequential. Random access memory,
RAM. And you probably have thought about your computer and maybe you paid an extra few hundred
dollars to get more RAM in your memory, more read-only, more random access memory. What's
random access memory? Random access memory is memory that can be accessed equally quickly,
no matter which piece of memory it is. It's like looking on a bookshelf and saying I can
grab that book or I can grab that book with equal ease. That's what random access memory
is. Most semiconductor-based memory made of flip-flops or other devices on integrated
circuits is random access memory. And the virtue of random access is it's fast. You
want a piece of memory, you want a piece of information that's in memory, you can go grab
it. And it doesn't matter which piece it is, it's equally easy to grab. On the other hand,
there's sequential memory. Suppose you could not pull the book off the shelf without first
pulling out all the books to the left of it. That would be a pain. That's sequential memory,
memory that you need to pass through a sequence of memory items before you can access the
one you desire. And there are some memory devices that are inherently sequential. A very old
fashioned one is magnetic tape. You have to run the magnetic tape past the read head and
the tape machine. And if the information you want is a mile down the tape, you've got
to spin a mile of tape past there before you can read it. Maybe that's not a problem,
but maybe it is if you want to get that information quickly. Hard disks, spinning hard disks,
which are still widely used in computers, although by the end of this lecture you'll
appreciate that they aren't going to be widely used for much longer, are sequential devices.
There's a spinning magnetic disk with information stored on it and that magnetic disk spins
past a so-called read head and you have to wait until the information you want comes
by. CDs and DVDs are other examples of sequential access devices. They're spinning disks and
you have to wait until the part you want to read from gets under the laser that reads
the CD or DVD. These are called optical disks. And so those are sequential memory. Why would
you want sequential memory? Because it's cheap and it can store a lot. The disadvantage is
it's slower to access. So two important distinctions, volatile versus non-volatile memory, random
access versus sequential memory. Another issue for memory is how we address it. Think of
a single item of memory storing a bit, a 1 or a 0, as like a house. It's got an address.
Where is it? And if you want that bit and find out whether that bit is a 1 or a 0, you've
got to know where that memory is. You've got to know its address. You've got to be able
to tell the circuitry to go to that bit. And maybe you want to put some new information
in that particular memory location. You've got to know the address to put it in. So there's
an issue about addressing memory. And the address of a memory, the amount of memory that you
have available is usually expressed in bytes. One byte again being 8 bits. And the question
arises then, what's the maximum amount of memory that you can address? And that is ultimately
set by the word length. Remember we talked about word length, computer word lengths have
grown from typically 1 byte to 8 bits to 64 bits. So what determines the amount of memory
you can address? Well, the answer is it's 2 to the power the number of bytes. And so
if you have a certain number of bits. And so if you have a 32-bit word, you can address
up to 2 to the 32 individual memory locations. And those locations might each involve 1 byte
of information. 2 to the 32, by the way, comes out to be about 4 billion. It sounds like
a huge number. But when we had 32-bit computers, which wasn't that many years ago, they could
address at most 4 gigabytes of memory. Today, 4 gigabytes is a pretty puny amount of memory
in a computer. With 64-bit words, we can address 2 to the 64 different bytes of memory. And
that's about 20 trillion gigabytes, 20 trillion billion bytes. And we're not likely to exceed
that limit anytime soon. Another issue when we talk about addressing memory is, are we
addressing that memory because we want to find out what's in it? Or because we want
to write some new information to it? So we're going to use the words read and write. Read
means extract the information from memory. Write means put new information into that
memory. So the question is about addressing memory. And again, the amount of memory we
can address depends on how big the words are in our computer or other device. Okay, so
let's go to our big screen and look at a particular implementation of memory. This
is after all, of course, in electronics, not just in computers and memory. And we want
to know how you'd build a memory. So we're going to build a single memory cell that stores
one bit, and we're going to base it on the kinds of flip-flops we've been studying already.
But we're going to really embellish it to do everything you need to do with memory.
So I'm going to start my one-bit memory cell with that SR, that set-reset-flip-flop. By
the way, I'm not going to show the clock just to avoid complication. This memory cell would
certainly be clocked so it would change in synchronism with the other memory cells in
whatever device it's in. So here's going to be a one-bit memory cell. The basic essence
of it, place where the bit is stored, is in this flip-flop, and the stored bit exists
as a one or zero at the Q of that flip-flop. Okay, I'm going to add different circuitry
to this. I'm going to start with an input circuit, and the input circuit consists of
an input, that's where you send information into this memory cell, two, three input AND
gates. We haven't built three input gates, although you had a project where you could
have built some multiple input gates if you had wanted. They work just like two input gates.
The only way the output of an AND gate can be one is if all its inputs are one. So these
are three input AND gates. You've got to have a one at all three inputs to get a one at
the output, and this is the input circuitry. Notice the only difference here is the input
comes in and it goes to the middle input of the upper AND gate, and it goes to the middle
input of the lower AND gate, the one that's connected to the flip-flops reset, but it
does so through an inversion. So whatever you put in here appears at the input to that
gate, and the opposite of it appears at the input to the lower gate. So that's the input
circuitry. Here's the output circuitry. The output circuitry consists of a three input
gate connected to the Q. Not surprisingly, the Q is what contains the information that's
in the flip-flop, and after we go through that output gate we have what we call the output
terminal of the memory. Whether or not that output tells us what's in Q depends, and we'll
see what it depends on. Here's the select line. The select line says if you want that
to be a, if you make that a one, you have selected this memory cell to work on, to read
from or to write information to it. If that one, if that select line is a zero, this memory
cell is inert, nothing happens, because you can't get information in and you can't find
out what the information is that's in it. And finally we have a read-write line which
tells us whether we want to read the information that's in the memory or write new information
to the memory. So that complete system is a one-bit memory cell and let's now see how
that one-bit memory cell operates. So suppose I have a zero on the select input. I've argued
that one is needed to select this memory cell. Well, if there's a zero there, there's a zero
at the input to those two gates and a zero at the input to those gates. Those are all
AND gates, and an AND gate's output is one only if all its inputs are one. So if you
put a zero to the input, any input of any of those gates, the outputs are zero. So we
got a zero on S and a zero on R and a zero on out. Now what does a zero do on S and R?
We spent a lot of time with S bar, R bar, flip flops where a one and a one kept it from
changing a zero and a zero once we got the S and the R keeps it from changing. So Q can't
change in this configuration. Furthermore, the output is zero regardless of what Q is
because this gate has a zero to one of its inputs. And also, by the way, regardless of
what the read-write is doing. And these gates have zero, so the outputs of those gates are
zeros and that keeps the Q from changing. So nothing happens. If you don't select this
particular memory cell, you can't read it, you can't write it, you can't do anything
with it. What if we select it? Well, then we got a one up here. That's encouraging.
And what if, in fact, in addition to selecting it, we set the read-write to zero? Now read-write
of zero is read in this particular setup. You could have made it the other way, but
the way I've designed this circuit is zero means I want to read it. So let's see what
that means. If I have a zero here, I've got this inverter. So then I've got a one here.
So I've got a one at the lower input of that gate, a one at the upper input of that gate.
Now this thing is behaving like a gate and it's an open gate because if Q is one, we're
going to get one at the output because all three inputs will be one. If Q is zero, we'll
get zero at the output. What's that mean? It means the output is Q. And so here I'm reading
the memory cell I get on the output whatever Q is. I can't change Q in this configuration,
but I can find out what Q is. So there is the one bit memory cell selected. So we're
going to operate on this particular memory cell and we're going to read from it. On the
other hand, if the select is one, still selected, but the read-write is one, let's see what happens.
So then I have a zero at this output gate. And I don't care what Q is, I don't care about
that one, zero is at the output. So when I've got the right selected, I can't tell what's
in the memory cell, I just get a zero. But I've got a one coming in here, that one is
going to all, to these two gates. By the way, in drawing this rather complicated circuit
where two wires cross and they're different colors, they're not connected. So I'm not
drawing that little curvy thing for non-connection. So there's no connection, there's a connection
here, there's no connection here, and there's a connection here. So this one is going to
the bottom of that gate and going to the bottom of that gate. There's a one at the tops of
these two gates because they've been selected with the one at the select input. And what
that means is what we get at the input here is coming through this gate. So in that case,
S becomes the input and R becomes not the input because it's been taken through this
inversion and Q therefore goes to the input because that's what happens in this SR flip
flop. It loads when Q is equal to, it loads Q from the input to the S connection there.
And so this is how I write to the flip flop. And when the clock comes along, I will have
loaded whenever it's at the input into Q. So that is a complete one bit memory cell.
It took an SR flip flop, three input gates and gates and a couple of inversions and we
have a complete memory cell that we can address, we can select, we can do whatever we want
with. And now let's take that thing and shrink it down and make a one bit memory cell, just
give it its own symbol, it's got to read, write, and out, select, and in. And that is
our one bit memory cell. So there's how we make a one bit memory cell. Well that's great,
one bit. We can store it, we can address it, we can read it, we can write it. What do we
need to do to go further? Well we need to store lots of words of memory. So here's an example
of a two word memory, each word in this case is four bits. This is a pretty simple memory.
Each one has a select line. Each of the words has a select line. The select lines of all
the memory cells are tied together. The each one has an output line. The output lines of
each corresponding bit are tied together through or gates. So whichever cell is selected I
can read the output from that one only. There's the read rights. The read rights are all connected
together so we know whether we want to read them or write them. And again the one crucial
thing is the so-called address decoder. The thing that says which of these two do I want
to select to work on? And the address decoder in this case is very simple. If you do the
project you'll have a more challenging address case. But here the address case requires just
a one or a zero because there are two input words. And if it's a one we address the upper
word because the address line is connected directly to all the selects of that one. And
if it's a zero we go through that inverter on the left there and that inverter causes
us to select word two. So there is a four bit word, two four bit word memory. Very simple
but you could scale this up billions of times and make the memories in modern computers.
Well that's an example of a memory built from flip-flops. And there's plenty of memory out
there built from flip-flops and it's good memory. It's very fast. It tends to be a bit
expensive. It tends to be a bit power consumptive but it's good. It's a little too expensive
and a little too bulky to use in our everyday computers. And so we go to a different kind
of memory and I want to distinguish now another distinction between two types of memory. We
have static memory in which the basic information storage unit is as in the memory cell I just
described a flip-flop. It's a volatile memory but it remembers as long as power is applied.
It's energy efficient and it's fast but it takes up space and it's also a bit expensive.
The memory used in your computer is much simpler. It's called dynamic memory DRAM. It's basic
information storage unit is the capacitor. Now we talked about capacitors a lot in this
course. They store charge and energy and they have a voltage across them because they have
two conductors separated by an insulator. The insulator is never perfect so the capacitor
always discharges. And dynamic memory would forget rather quickly in a fraction of a second
typically if it weren't periodically refreshed. It's somewhat slower and it uses some power
but it is very high density and it is ubiquitous in personal computers. Here's how DRAM works.
Dynamic random access memory. DRAM has two lines, a select line and a read-write line.
It has a capacitor. This is not a capacitor you go by from some store. It's a capacitor
that's built on to the integrated circuit chips and modern memory cell memory systems
have literally billions of these capacitor transistor combinations on them. Remember
that a metal oxide semiconductor field effect transistor turns on or off in this case depending
on whether you put a one or a zero on the gate. So if you put a zero on the select line
which is connected to the gate, then that capacitor is isolated from the rest of the
world because of that transistor being off. If you put a one on the select line you have
selected and then you can either write to the capacitor, put charge on it through the
transistor or read what charges on it through the MOSFET and you can read or write as you
wish. So the transistor is not conducting when the select is zero. It's conducting when
the select is one and you can read or write. Now there is one caveat with this. I mentioned
that those capacitors tend to lose their charge fairly regularly. So typical DRAM, typical
dynamic random access memory is refreshed and the rate at which it's refreshed varies
but it could be as slow as about 15 times a second. So in your computer's memory, my
laptop has 16 gigabytes of memory, every one of those 16 billion memory cells about 15
times a second, you read what's on it and if it's a one you pump a little more charge
on it to keep it at that one and if it's a zero you don't pump more charge on it and
that is called refreshing and it's happening 15 times a second. You say, whoa, 15 times
a second I got to do this? Well, 15 times a second is awfully slow compared to the roughly
three gigahertz, three billion times a second that the clock is going in my computer and
refreshing things. So it's actually a very rare thing in the context of the computer's
internal timing that you actually refresh the memory but it is dynamic random access
memory has to be refreshed. I've got an old dynamic random access memory card out of
an older computer. It consists of eight integrated circuits each of which is a chip containing
a several hundred million, probably 256 million in this case, individual memory cells of the
type I just showed you with the capacitor and there's eight of them so we represent one
byte with this or rather 256 million bytes with this. Out of a modern computer this thing
would have billions of transistors in it. So that is a memory. Well, let's take a closer
look at memory, different kinds of memory. I mentioned there were all kinds of different
times so let's just run down some of them. There is what's called SRAM, static RAM, that's
the fastest, that's what we talked about. It's volatile, it's random access and it's
small and it's high speed. By the way, if you're an electronics experimenter you probably
want to use SRAM even though it's a little more expensive because you don't have to mess
with the circuitry to dynamically refresh the DRAM. DRAM is also fast, it's volatile,
it's random and it's the main memory in most everyday personal computers. There's memory
called ROM, read-only memory, PROM, programmable read-only memory which you can only read except
you can program it and EEPROM, EEPROM which is erasable programmable read-only memory.
They're slower, they're not volatile, they're used for things like the boot up sequence
in computers, pieces of, simple pieces of computer code that you have to store permanently.
They're random access and you store permanent information, start-up segments and so on.
There's flash memory which I'll talk more about in a moment, that's the memory in most
of our modern electronic devices other than computers in terms of, it's not the random
access memory but it's the long-term storage memory in many devices. There are magnetic
disks, these are the spinning hard disks, there are optical disks, CDs and DVDs, there's
magnetic tape and finally there's something called magnetic core which is long since obsolete
to consist of crossed wires that were woven together by hand and a little ferrous core
was put on them and it was either magnetized or not. We can look at ancient memory of memory
items and magnetic core memory is here and it looks like this woven cross-hatch pattern,
magnetic tape used in these giant tape machines. Magnetic tape is still in use but you probably
don't see it. We have floppy disks and we have hard disks and I have some of these things
here which I'd like to show you. So this is an example of magnetic tape and this is like
the tape you used in old-fashioned tape recorders, it's a predecessor of the little cassette
tapes you may have used or the VHS tapes, all of them now obsolete although this is
still used in long-term computer storage. This happens to have my Ph.D. thesis on it
from a long time ago as the backup storage. We developed computers that had floppy disks
spinning magnetizable media and this is truly a floppy disk, that's why they're called floppy.
If you came into the computer era a little later you probably used floppy disks that
looked like this, they didn't look floppy but if you got inside them they had a floppy
magnetic component. We are still using optical disks for storage, some of them rewritable,
some of them not and here is the hard drive such as might still be in your computer and
there is the rapidly spinning disk and here is the head which moves over that disk and
picks off or writes to bits of magnetic information from that disk. So there are some examples
of different memory types. Now having talked about those types and that's kind of a museum
everything here is either still in use but fading out, when was the last time you got
a DVD instead of streaming something for example, even optical disks are fading out, hard drives
are going fast, they're going in favor of something else called flash memory and let
me just show you some examples of flash memory and then we'll talk about how it works. So
what's in a camera? Well no film, film is long gone. What's in here is a sensor, probably
a CMOS based sensor, complimentary metal oxide semiconductor and the camera information,
the pixels, whether it's red, how much red, how much green, how much blue and what intensity
for every one of the 18 million pixels that this camera can take is stored in flash memory.
Here's a flash drive, flash, USB stick, whatever you want to call it, it's got a USB connection,
serial connection to whatever you plug it into and it stores information, several 32
gigabytes of information in that case. Here's a smartphone, the smartphone's memory is all,
the non volatile storage memory is flash memory and here is a tablet reader which also stores
the books or whatever you're doing in flash memory. So let me take a few minutes and talk
about flash memory and then we'll wrap up. Flash memory uses transistors that look a
lot like the transistors we've talked about before, the metal oxide semiconductor field
effect transistors except they've got an extra gate and it's called a floating gate and it's
sandwiched between two layers of insulation and this has to be really good insulation
because your term paper, your photograph collection, your contact information and your phone, all
that stuff is stored in the form of electrons sitting on that floating gate and if they could
jump off and this is why early flash memory, there were worries about would it last 20
years for example, I think the jury's still a little bit out on that but it lasts a long
time those electrons on that gate are carrying the information. How does it work? Well if
you want to write information to this floating gate transistor and you want to write a one,
you charge that control gate very highly positive, you put a big voltage on it and that big voltage
works just like any MOSFET transistor, it pulls electrons into the channel, the channel
becomes n type and conducts and some of the electrons that are conducted through that
channel undergo a process called quantum tunneling and they leap onto that insulated gate and
the presence of those electrons tells you that you have a one stored in that transistor
in that memory unit and those electrons will remain there for years. You want to write
a zero, you charge the control gate highly negative and if there are any electrons on
that floating gate they're pushed out through the insulation and into the substrate. You
want to read the thing, you put a modest charge on the control gate and you ask yourself does
the transistor conduct or doesn't it conduct? Now if the transistor doesn't conduct, the
reason it doesn't conduct is because the floating gate has some electrons on it, it's repelled
electrons out of the channel and the transistor doesn't conduct and then you know that the
floating gate holds electrons and you interpret that as a logic one. On the other hand, if
the transistor does conduct then the floating gate doesn't have any electrons because the
charge on the control gate has pulled electrons into the channel and made it conduct. So that's
how flash memory works. Flash memory is with us today, it's almost everywhere, it is slower
than RAM, it is not volatile, it's there when the power is turned off, it will last for years
and years. So let me end by looking at your future in memory. Two examples of memory,
here's a mechanical hard drive like I just showed you, these are still in use in computers,
you can still buy computers with them and you will be able to for a number of years.
I priced a 500 gigabyte hard drive in 2013 and it was $50 and the same unit in 2014 was
$50. By the way, that's remarkable because that's half a terabyte and when I started
in as a PhD, newly admitted PhD, we were talking about the world's best computers having terabytes
of memory storage. Now you can buy $50, half a terabyte and pay a little more, you get
a whole terabyte. The other device shown here is a solid state drive in SSD, also 500
gigabytes, by the way when we talk about numbers of gigabytes, it's really always a power of
two because of the addressability of the device, how we have to address it with a power of two
and so 500 really means 512 which it says on this picture and in 2013 that thing cost
$350, seven times as much as the hard drive in 2014, it was down to 250, five times as
much. They're on a collision course, nobody knows exactly when, you can talk to the electronics
economists, but probably before 2020 they'll be at parody. I paid some extra dollars for
my current laptop, it has solid state drive entirely, it has 768 gigabytes worth of solid
state drive because it's much faster to load computer, load programs or load lots of data
into memory from the much faster solid state drive and there's nothing to break when if
it drops. Okay well that's a lot about memory, I hope you remember that and if you want to
know a little bit more about memory do the project, you can simulate the one bit memory
cell described in this lecture, if you're going to do it in circuit lab which doesn't
have three input AND gates you'll need to make them yourself, but you can do that. If
you go to do circuits you'll find when you select a gate you can choose how many inputs
it's got so you can use three input gates right there and I suggest to watch your memory
work, you clock the read write input and the data input at a slower rate, maybe a quarter
of the rate of the read write input, you can do that with the built in clocks both these
simulators have and you'll be able to see your memory go through its paces and then
consider a four word by four bit memory so I showed you in the lecture a two word four
bit memory so it's similar to that one and design, but don't build it please, an address
decoder for that memory and you're going to need two address lines because you've got
to address four different words and two lines give you four possible combinations that is
for addresses.
So lecture 20 was about memory circuits and so what I'd like you to do for the project
first of all is to simulate the one bit memory cell I described in this lecture. Now if you
use circuit lab you'll have to make your own three input AND gates, that was a part of
an earlier project because it doesn't have three input AND gates, do circuits actually
allows you to select the number of inputs once you load a gate onto your circuit you
can double click it and change the number of inputs so you can get three input gates
there and here's my suggestion for studying how this circuit works. Clock the read write
input and then clock the data input also at perhaps a slower rate of maybe a quarter
as fast and that's what I'll show you in my simulations. Then part two is a simple pure
design project which I don't want you to simulate because it's a bit complicated and that is
consider a four word by four bit memory that is there are four four bit words in this memory
similar to the two word memory I described in this lecture and I'd like you to design
but not build an address decoder for this memory and you'll need two address lines as
opposed to the one we had for the two word memory because you need to address a total
of four words and it takes two lines with their possible one and zero states to do that.
Okay so let's look at the solution to the first part in both circuit lab and do circuits.
So here is the solution in circuit lab you'll notice a lot of AND gates on the left there
that's because I had to make those three input gates I'm going to send the data in as a 250
hertz square wave and the read write is going to flip faster at one kilohertz that's four
times faster and I'm going to look at the out. Now I've taken the select line and just
set it to one because I'm interested in seeing how this memory cell works remember if I set
the select line to zero the memory cell wouldn't do anything it wouldn't allow input it wouldn't
do anything. So let's run a time domain simulation for this one. And start time I'm going to
go to six milliseconds I'm going at one kilohertz so that'll give us sufficient coverage and
there we go and let's look a little bit at what's happening here. Okay so on the top
I have the read write flipping from write to read to write to read read is zero write
is one and let's see what happens. And then I have the input data so initially the input
data is zero so when the write input goes high I write a zero and the next time the
read input goes high I get a zero as the output for I get a zero at the at the output. Now
I'm not showing you the output on here I'd have to scroll down to see it and the only
time I get a one on the output is when the input is a one so I'm saying let's load a
one in here and the write input goes high. Now you can't see this very well in the circuit
lab simulation because I can't show you all those graphs at once I could put them on one
graph but then they would get confusing. So instead I'm going to take a look at the result
of this when I did this and then grabbed all those graphs and put them together. And now
you can see what happens. The output which is what I can see when the read is low when
the read write is low you'll see the output goes high when the read is low and after the
input was one for a while and the write input had gone to one. So think about that for a
while. It takes the write input going to a one in order that the input one gets loaded
into the memory and then it takes going to the read write input being low to read that
output and see it at the output. So I see it twice at the output and then it goes back
to zero again as the input goes low. So take a look at that think about it a while and
you can convince yourself that the circuit is working. Let's go over and see the same
thing in do circuits. So here is the same situation in do circuits. I'm going to run
the digital simulation again for 100 nanoseconds and nanosecond is do circuits basic timing
unit so I didn't bother to change that. You'll notice it's a little simpler here because
I now have three input and gates which I can get again by double clicking the gates and
setting their parameters to how many inputs they have. And I've got these digital logic
probes at the read write and at the data input. I'm doing the same thing running the data
input and then the read write running about four times as fast. I'm going to look at Q
which is what's actually stored in memory but is not necessarily available at the output
unless the read write is low for read. So I'll run the simulation. Again we got some
open ports so we'll say no to that. We'll let it simulate a while. It's going to take
it a little bit of time. There it is and we have basically the same thing we had before.
When do we get a one at the output? We get a one at the output only after we've written
we've had a read write with a one at the input. And rather than continue to look at these graphs
let's go to look at the lights. So the read write is flipping on and off. The data in
is flipping between one and zero. There goes Q to one and the output goes to one when the
read write goes to zero. So the only time we see the output is when the read write is
at zero that says to read. Q may be one sometimes but we don't see that on the output until
the read write goes to read. So here is that complete memory cell, that simple memory cell
involving a single bit. Let's look finally at the second part of the project which was
to decode for a four word memory containing four bit words and here is the decoding logic
that it would take. So word one has an address of zero zero. Well I've chosen a NOR gate
for the driver of the select lines of word one because a NOR gate has a one output only
when both inputs are zero. And then if you think about the other two word two is supposed
to be addressed at zero one. So when a one address line one is a one we get a one into
that AND gate going into word two and when a two is a zero the inverter gives us a one
into the AND gate that's heading toward word two and so only under those conditions do
we get a one and then we select word two. Word three is a one zero for its address so
we just do the opposite. We put the inverter on a one rather than a two and finally when
do we address word four? Word four's address is one one four possible addresses with four
with two input address lines and we want that one to be selected only when both inputs
are one and so we simply use an AND gate on the two inputs. So there is an address decoder
for a four word four bit memory and your computer which may have a billion words of storage or
something has just more complicated decoding logic to decide which memory cell gets addressed.
