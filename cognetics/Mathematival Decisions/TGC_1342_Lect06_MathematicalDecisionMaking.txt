The last lecture was an introduction to data mining.
We looked at how to use mathematical algorithms to extend our native ideas of pattern recognition.
We can then use computers to implement those algorithms on large, multi-dimensional sets
of data, letting us find relationships and patterns that the data contains.
So far we've focused on classification and prediction.
In a nutshell, these algorithms proceed like this.
First, I show the algorithm a large set of training data.
This data contains both input and output values for a large collection of individuals.
It's like showing the algorithm a bunch of questions along with their answers.
The algorithm then generates a way of predicting the output to new individuals based on their
input values.
Any such activity can be called prediction, although if the output classifies a category,
it's usually called classification.
All these procedures are called supervised learning, which merely means that you have
to supply both the input and the output values for each individual in the training data.
The techniques that we explored last time included the construction of a classification
tree, which we could picture as repeatedly subdividing rectangles into smaller and smaller
rectangles until all of the given points in any given rectangle fall into the same category
of output.
When we applied this to our 50-point dataset on spam emails, we ended up with this picture.
Given the picture, the rule that it implies for new data is the obvious one.
Any new data point falls into one of these rectangles.
We find the new point to be in the same class as the others in that rectangle, like this.
If you're in a blue dot square in which all the emails are good, your email is classified
as good.
If you're in a red dot square in which all the emails are spam, your email is classed
as spam.
Classification trees aren't the only way of making such an assignment, of course.
For example, there's the nearest neighbor classifier, which is pretty much what it sounds
like.
First, normalize all the variables so that they have the same mean and standard deviation
This puts them on the same even ground.
Next, given a new data point, find its nearest neighbor in the training set.
Assign the new point to the category of that nearest neighbor.
For our spam training data, the result would look like this.
If your email ends up in a blue region, it's classed as good.
In a red region, it's classed as spam.
The picture is similar to the classification tree of a moment ago, but far from identical.
This simple nearest neighbor rule can do a surprisingly good job when given lots of
data, as can a slightly more complicated version of it called k nearest neighbors.
If we decided to use a five nearest neighbor rule, for example, then given a new point,
we'd find the five nearest points to it in the training data.
We then check the output categories of those five neighbors and let majority rule in assigning
a category to the new point.
There are other classification algorithms as well, such as discriminant analysis.
In our graphical interpretation, discriminant analysis finds a flat surface, in our two-dimensional
example, a line that tries to separate one category from the other.
For our data, it would look like this.
Each has its own advantages and disadvantages.
A quick glance at our three examples, though, might make you think that discriminant analysis
has to be the loser of this competition.
Yes, a single line gives a simple rule, but discriminant analysis misclassified some points.
The other two methods got everything right, and that point actually deserves quite a bit
more attention, because the regions that we see in these three examples were created by
using the training data.
The first two techniques got grinding away until they had gotten rid of all the misclassifications.
For those techniques, every region in the picture consists of dots of a single color.
But what's wrong with that?
Well, it would be pretty naive to think that these super classifiers are going to do as
well on new data as they did in the training data.
For example, let's take a look at the nearest neighbor picture again.
See that large blue patch on the right-hand side?
If an email falls anywhere in that range, it's classified as being good, non-spam.
And that's all because of the single blue dot on the right-hand side of the field.
Obviously, it's possible for a point in that area to represent a good email, the blue dot
that's shown is just such a point.
But is it sensible to expect that that blue patch is really dominated by good emails?
One cause of the problem, an obvious one, is that I restricted us to only 50 emails for
the training data, nearest neighbor approaches in particular, like lots of data.
And this chart shows you why, but it's not the only problem.
Look at the border between red and blue in the lower left-hand corner.
Do you really think that the best demarcation of span from non-spam follows such a tortured
contour?
No.
The problem is one that we discussed before when we used polynomials to fit a set of data
points in regression, the problem of overfitting the data.
You can think of it this way.
Some of the observed relations between inputs and outputs in the training data are due to
genuine relations between the two.
But there are random fluctuations in observations, too, noise, if you like.
And if we go too far in our classification algorithm, we can end up fitting the noise
as well as the true relation.
And that's bad, because noise is random, and new observations are unlikely to follow the
quirky pattern that let us correctly classify training points based on its noise.
So how do we avoid this?
The key idea is validation.
When we get our data, and remember in data mining we all often have a lot of it, we don't
use all of it for training.
Instead, we partition it randomly into two or three groups.
The first, and almost always the largest, is used for training in the way that we've
already described.
But the second set is called the validation set, and it's not used until after the data
mining algorithm has been applied to the training data to generate the rules.
The performance of the model on the validation data is a fair test of the model's quality,
because that data had nothing to do with the model's creation.
This gives us a way to benchmark the different data mining algorithms and to evaluate which
one looks best, the one that does best on the validation data, except that if you think
about it, selecting the model this way still doesn't tell you how good you can expect it
to be.
Yes, the model that you pick is the one that did best on the validation set, but what it
do is, well, on other new data, face it, you pick the winning model because it was the
best on the validation set, maybe it just got lucky with that set.
And here's where the third set comes in, the testing set.
If we've used the validation data to pick the best model, we've more or less poisoned
it for usefulness in assessing how that model does on new data, so the final step is to
evaluate how well the selected model does on brand new data, the testing data.
Given a decently sized testing set, we should be able to use our model's performance on
it as a fairly reliable measure of its overall performance.
So how does all of this apply to a technique like classification trees?
Well, we begin by following the procedure that we discussed last time, a procedure that
continues by adding one node after another to the classification tree, in other words,
by subdividing rectangles again and again until every point is correctly classified.
But we keep track of the order in which we add the nodes to our tree.
To see how this works, let's take the training wheels off of our spam filter.
My actual data file had 57 input variables and over 4,600 records.
I used half of these for the training set, 30% more for validation, and the last 20%
for screening.
The validation set was 41% spam, so if you know nothing about an email, classifying
it as good email has a 41% error rate.
But the first division ordered by the classification tree algorithm was to split email based on
the density of exclamation points, dividing it at a line of about 0.03%.
If we assign non-spam to email below this threshold, and spam to emails above the threshold,
we misclassify 22% of the validation set.
The next split in the tree lowers the genie index but leaves the error rate at 22%.
The one after that lowers it to 16%, and we keep going.
There are some special rules that can alter the order in which we add nodes to our growing
tree, but they're not important here.
What is important is the result.
Not surprisingly, as we add additional splits, we fine-tune our algorithm and get less and
less errors on the validation set.
Up to a point.
After 66 nodes, the misclassification rate on the validation set is down to 9.05%, but
at that point, the error rate levels off, and adding more nodes leaves it at 9.05%.
Then adding the 70th node actually increases the misclassification rate on the validation
set.
Beyond that point, additional nodes just make it worse.
We've gotten to the point where the original algorithm was no longer training to the real
relationships between inputs and outputs.
It was trained to fit the noise that was in the training set.
Because we're deciding where to stop based on the validation set, we actually could be
guilty of overfitting it.
With 66 nodes, we may have gone further than new data would warrant.
To account for this, we usually prune the tree more than the best fit to the validation
data.
The amount of pruning is computed based on a statistical formula for how much we can
trust the validation set error rate.
And for our data, this sums up pruning the tree quite extensively, leaving a 30 node
tree that has a 9.56% error rate on the validation data.
It turns out that to use this prune tree, you only have to know 10 things about your
email, not 57.
The character densities of exclamation point, hashtag and dollar sign, the total number of
capital letters, the longest string of caps, and average length of cap string, and what
fraction of the words in the email were edu, hp, money, and your first name.
You never have to answer more than eight questions about your email to get a classification.
And I need to hardly point out that with eight questions, that could take you a bit of time,
but it's an eye blink for a computer, as is collecting the data required by scanning
your email to begin with.
Predictably, the filter doesn't do as well on test data as it did on validation data,
but it's not bad.
The overall error rate was just over 11%.
Our filter messed up on 6.4% of the good emails, and almost 19% of the spam.
Filters built with more care could do better.
Of course, spam writers learn what the filters are looking for, and the spam work continues.
But let's stick with our filter, which is messing up on three times as many spam emails
as it does on good emails.
That lopsided balance results in the minimum overall error rate of 11%, but is it necessarily
what we want?
Well, in general, no.
We've been assuming so far that the errors of both kinds are equally costly, and yet
that isn't necessarily true.
Which would you consider a more serious issue?
Finding a spam email in your inbox, or never receiving a real email because it was re-rooted
to your junk folder?
I'm betting that losing a real email is much worse.
You'd be willing to see a bit more spam to significantly reduce the chance of losing
a real message.
Can data mining handle this preference?
It certainly can.
When the prune tree looks at the training data, it gets some things wrong.
Suppose, for example, that after asking all of its question, the prune tree lumps your
email in with 25 training emails.
25 emails for which the answers to the questions were the same as they were for your new email.
I'll call that your email's bin.
Maybe you'd check those 25 emails from the training set and find that 14 of the 25 were
spam.
The bin is 64% spam, so it's identified as a spam bin, majority rules.
So your new email is classed as spam too.
The algorithm could go on, in fact, to estimate that there's a 64% chance that your email
is spam.
But you can change the bin's assignment rule in light of the fact that misclassifying
a real email is more serious than misclassifying spam.
Suppose, for example, that you decided that having a real email classed as spam was twice
as serious as having a spam classed as real email.
Then you could set a threshold for your bin to be classed as spam at two-thirds, not 50%.
If a bin isn't at least two-thirds spam on the training data, it's classed as good mail.
So by the new rule, your email would not be classed as good since the bin was only 64%
spam.
Most classification and prediction techniques have an equivalent ability to set a slider
for the threshold.
This answers one of the questions we discussed last time about screening for cancer.
We want to set our threshold so that the bin that has any sizeable number of cancer
victims in it is classified as potential cancer rather than as clear.
There's a second way to handle this issue, and it can be used alone or in conjunction
with adjusting the classification threshold.
It's called oversampling, and it's used in particular when the classes of interest are
especially uncommon in the data, like the rare cancer example we were discussing, or
for that matter, like picking terrorists out of a crowd of airline passengers.
The training set might include so few examples of the rare cases of interest that a straightforward
application of data mining techniques would fail.
The best rule for the technique might be to classify everyone as belonging to the largest
class, and no one to the class of interest.
Oversampling does exactly what it sounds like it does.
It includes a number of observations that are, in the rare class, disproportionate to
its actual frequency in the population.
You can do this either by taking random samples, or the replacement if necessary, of individuals
that are in the rare category and adding them to the training data, or by duplicating
each rare entry in the training set a fixed number of times.
In our cancer example, suppose missing a cancer victim is 20 times more costly than treating
a healthy person as potentially having cancer.
Then in the training set, each cancer victim could be replicated 20 times.
The model could then be validated on a sample reflecting real-life proportions, alternatively
it could be validated with an oversampled validation set, and then the results could
be rebalanced to reflect the actual population proportions.
These examples have been focusing on classification problems rather than prediction ones, mainly
for clarity of presentation.
The prediction problems are basically the same, but the quantity to be predicted is
a continuous variable like income, rather than a category like spam.
We use the same kind of techniques, but different measures of error.
In classification trees, the purity of a rectangle depended on what fraction of its contents
belong to each output class.
In the continuous case, the regression trees, I might be based on the variance of the output
values in a rectangle.
In classification trees, the class is assigned to the bin based on the vote of the training
data in that bin.
In regression trees, the prediction is the mean of the trading data of the bin, or some
similar measure of central tendency.
Happily, most of the software that can handle one kind of problem can handle the other as
well.
But there are other interesting pattern recognition problems besides classification and prediction.
Data mining also deals with problems of affinity analysis, or put simply, what goes with what?
The recommender systems at online stores are an example.
They use association rules.
The system recognizes that you were looking at A and B, then points you to people, points
out that people who like them tend to be interested in C and D too.
In retail, association rules are sometimes called market basket analysis.
If you know that people who buy these two products also tend to buy that one, you can
use that information for product placement, for promotional sales, and so on.
The strategy behind a basic association rule generator is actually pretty simple.
Let's discuss it in terms of retail sales.
You have, for each customer in your database, a list of all of the items that they bought
from you.
You're going to be looking for rules of the form, if they buy all of these, then guess
that they want this too.
The game is probabilistic in nature, of course.
You won't be right every time, but you want your rules to give you useful guesses as to
what might interest your customer.
Well, what do we need to have a useful rule?
First, there's no point in considering an item or group of items that appears very rarely,
at least not unless the combination is very important, like one that leads to you selling
a pro football team.
So you want to consider the support of a collection of items, which is merely the number of items
or fraction of your customers who bought everything in that collection.
Believe it or not, if you're doing this in every possible way for a store with more than
a small number of items, you're going to run into trouble.
For example, if there are a hundred items in your catalog, there are two to the one
hundredth minus one different possible collections of items that could be purchased.
If you consider a trillion such possible orders every second and you got an early start, namely
you, when the universe was created, you'd be a bit over one tenth of the way now through
the job.
A common workaround is to use the a priori algorithm to decide what your support threshold
is going to be like two percent of your customers will call any item or collection of items
that were purchased by at least two percent of your customers transactions frequent.
So look through your database and identify each frequent item.
Now any subset of two items that appears in at least two percent of your transactions
will consist of two frequent items.
Check each pair and keep the frequent ones, the ones that appear in at least two percent
of the orders.
Now a frequent triple of items will have a single item paired with a frequent pair.
Generate all such triples and identify the frequent ones.
You keep going until you can make no more frequent sets.
But knowing, for example, that A and B are frequently bought together, isn't enough to
make a good prediction rule.
Imagine that 40 percent of people who shop in a store by bread and 35 percent by butter.
Then you can show that 14 percent of people will buy both, even if purchasing one has
absolutely nothing to do with purchasing the other.
No.
If we're going to propose a rule like bread implies butter, we want more than bread and
butter showing up together.
We want to know how much that bread bumps the probability of butter showing up.
To find this, we first compute the confidence of the inference.
That is, if you know that they did buy bread, what's the probability that they also bought
butter?
Then we divide this value by the chance of buying butter at all to get the lift ratio
that the bread implies butter rule.
A value close to one means that the rule is not pinpointing a very strong association.
A large value means that if the customers mean the if conditions, the antecedent, then
the customer has much more than likely than a typical customer to also meet the then part,
the consequent.
Let's check out an example.
Suppose that our grocery store database has the records for 1,000 transactions, 400 bought
bread, 350 bought butter, and 340 bought both.
340 of the 400 bread buyers also bought butter, that's 85 percent.
That's the confidence of the bread implies butter rule, 85 percent.
But without knowing that the customer bought bread, the chance of butter is 350 over 1,000
or 35 percent.
The lift ratio is the ratio of these two, 85 percent over 35 percent, 2.42.
That means that finding out that a person bought bread increases their chance of buying
butter by 142 percent.
That's a good rule.
This example had the antecedent and the consequent, both being single items, but they can just
as easily be sets of items.
Association rule software will scan the data and create frequent sets of items, then generate
association rules for them and compute the confidence and lift indices for each.
Those with high values are good candidates for rules that the company may want to note
and use.
There's definitely a place for human beings in this process.
Since the procedure I've just described can result in some rules that can either be discarded
or merged with others.
For example, people buying lemons also tend to buy citrus, undeniably true, but useless
as a prediction.
Association rules are looking at information in a very specific way, that this set of
attributes gives you reason to anticipate that set of attributes as well.
But there are other techniques for looking at things going together that are based on
the idea of clustering the data, of creating categories, then sensibly sorting the individual
data points into them.
Sometimes this is a first step into further analysis that will be based on these clusters.
Two of the questions, two questions of course arise.
What should the categories be and how should we decide which category an individual should
be sorted into?
And the key idea to answering both of these questions is the same.
Distance.
We've already seen how distance played a role in our k-nearest-neighbors approach to
classifying data, and the issues that be raised there are relevant here as well.
The most straightforward way to define distance between two individuals is Euclidean distance,
the straight line distance separating the points in an n-dimensional scatter plot.
Yeah, that's hard to visualize in more than three dimensions, but it's no harder to compute
with a souped-up version of the Pythagorean theorem that you learned in Geometry.
To find the distance from me to you, given that we have information about six characteristics
A through F, we find out how much we differ on each of the six traits.
Square those differences, add them up, and take the square root.
So if my scores on A through F are 3, 4, 5, 6, 7, 8, and your scores for the five traits
are 5 on all six traits, then the differences in the six scores are negative 2, negative
1, 0, 1, 2, 3.
And we square all of these and add them.
Then take the square root to get a distance of 4.36 or so.
So this is the most obvious way to define the distance between two observations.
There are some problems with it.
One is the issue that we discussed before, scale.
Imagine that we're measuring distance from Washington DC, but for some reason we're
measuring north-south separation in miles and east-west separation in degrees of longitude.
A change of 40 in the north-south direction would take us 40 miles north, a bit beyond
Baltimore.
A change in 40 in the west direction, it would be 40 degrees west, somewhere in Nevada.
It's often the case that there's no possible or sensible common unit of measure for each
variable, and to level the playing field, we can normalize our variables in the way
that we discussed for PCA and the nearest neighbor algorithms.
Normalizing may not be enough if some of the variables are strongly correlated.
As an extreme example, if I recorded city locations by the miles east-west of DC, their
miles north-south of DC and their longitude, the first and last variables are perfectly
correlated.
The result of the computations will be a distance measure that considers east-west separation
as being twice as important as north-south separation.
Nobody wants that.
There are some nice ways of addressing the problem by using different definitions of
distance, such as the statistical distance that both standardizes and accounts for correlation.
Pandora Internet Radio uses this kind of distance when picking out songs to play for you.
Their database contains the result of the Music Genome Project.
The project has hundreds of humans analyze 400,000 songs by 20,000 contemporary artists.
They track hundreds of different characteristics from the gender of the lead singer to whether
the piece of court includes an accordion.
Each of these characteristics, the variables, is scored on a scale of 0 to 5.
When you tell Pandora that you like a song, it looks for another song near to the one
that you liked by its distance measure.
It plays that.
If you say you like it too, it adds it to your cluster of liked songs and looks for
another near that cluster, and so on.
Not surprisingly, it gets a little more complicated than that, like how to handle songs that you
say that you dislike, but that's the essential idea.
To explore how this works, I'm going to work through an example small enough that we can
rely on your visual intuition to help us along.
Here's a two-dimensional example of a small data set, and I'll use our usual meaning of
distance in this example, but it'll give you a good intuitive feel for how our clustering
algorithms work with any data set.
So, here's my data set from the students in one of my classes, showing students final
exam scores and their average on quizzes that were given throughout the semester.
Your eyes probably going to pick out some clusters, but we want to see how we can do
this mathematically, so that it can be done in a much larger data set with many more variables.
Multiple techniques to do this do exist, including K-means clustering, where we specify the number
of clusters that we want in advance.
The approach I'm going to show you today, though, is called agglomerative hierarchical clustering,
and it starts with each point being its own cluster.
The next step is rather obvious, really.
Check the distances between every pair of points, and put the two closest together into a cluster.
Since I'm using the normal Euclidean distance as my distance measure, you can do this by
eye, like this.
And we continue this way, looking at the things that are closest together.
But before we do that, we have to answer a question.
What do we mean by the distance between two clusters?
Well, it's up to us, and that definition that we choose depends on what we know about the
data.
Suppose we're trying to track pollution levels.
If a river is contaminated, we'd expect to find pollution close to the river.
We don't have to be close to all points on the river, just close to some point on the
river.
Your distance from the river then would be measured as the distance of the closest point
on the river.
That's one definition of distance between two clusters.
It's called the minimum distance, or single linkage distance.
If you use it, you get clusters that tend to be ropey.
A second possibility goes to the opposite extreme.
You could define distance between two clusters as the largest distance between any two points
in the clusters.
So the distance from California to Washington State is the distance from the southern tip
of California to the Canadian border of Washington State.
This method usually gives small clusters to begin with, and the clusters tend to be spherical.
There are other methods, too.
We can define the distance between two clusters as the average distance, computed for each
pair of points with one in each cluster.
With large clusters, that's a lot of calculation.
Or we can find the middle of each cluster, the so-called centroid.
And by finding the average value of the variables in the points in the cluster, and then define
the distance as the distance between the two centroids.
And other more complicated methods seek to minimize the amount of information lost by
creating new clusters from old ones.
For demonstration, let's stick with the minimum distance definition, as it's perhaps the
easiest to see by eye.
Here's the evolution of the clusters of our students.
When you watch this progression, from each cluster being a single point, until eventually
all the cluster points being in a single cluster, it prompts a rather natural question.
When do you stop?
How many clusters do you want?
Well there's no universal answer to this.
Sometimes your domain knowledge will inform you of how many clusters there should be in
the data, but more often it comes down to examining the groupings resulting from different
levels of clustering.
Ideally, the clusters pass three checks.
First, they're interpretable in some sensible way.
The Nielsen prism segmentation of American consumers divides them into 66 demographic
groups such as young digerati and pools and patios, and each group can be described roughly
in a sentence or two.
Second, clusters should be stable to minor changes in the data used to create them.
One way to test this is to partition the data before starting into training and validation
sets.
Begin by using all the stated to define clusters.
Set this result aside.
Well build the clusters built based on the training data alone, then assign each point
in the validation data to the nearest of these training-based clusters.
Compare the result to the clusters you got from starting with all the data.
You don't want to see dramatic differences between the two approaches.
Finally, you'd like the variation between clusters to be significantly greater than
the variation within a cluster.
You can examine this last point by a dendrogram, a chart that's essentially the script for
the movie that we just saw.
As you move from the bottom to the top of the dendrogram, you see what clusters are
being merged next to form a new cluster.
The vertical dimension is recorded in the distance between the clusters, so merging
toward the top indicates that clusters are far apart.
I'm stopping at the red line before joining clusters at a distance of 18 or more from
one another.
That gives a reasonably small number of clusters that are fairly apart.
This one.
And the groups are sensible.
The rectangle holds the high achievers, the triangle the low achievers, the ellipse the
students who can't be bothered to study for daily quizzes, but do well on tests, and some
outliers.
Actually, it probably makes sense to group the small rectangle below the ellipse with
it, although the distance rule for the nearest point between clusters would not do so next.
If I'd used the average distance measure, which is actually more sensible in this application,
the result would be identical, except the ellipse and the small rectangle would be merged.
A better clustering, given the nature of the data.
There are only two variables in a small data set, the creation of clusters could have been
done by hand, although it's time consuming.
For real-world scale data sets with tens or hundreds of variables, you need the mathematics
of our algorithm and the power of a computer to implement them.
But with those algorithms and that power, data mining confers upon us an ability that
could be seen almost magical, the ability to find patterns in a wilderness of data that
by all rights should defy our conception.
