In the first lecture in this series, I said that applying operations research to a problem
was often like taking a long trip by car.
If your goal is getting there rather than sightseeing, you use superhighways.
They're fast and reliable and let you cover a lot of distance.
In OR, the superhighways are the mathematical algorithms that we apply to the formulation
of a problem.
Just like with real road systems, mathematical superhighways are something relatively new.
It wasn't that many years ago that to have any hope of applying those useful algorithms
you had to know a lot of mathematics.
Being bright and industrious lets you get to the point where you understood the algorithms.
But even then, applying them to real road problems was often a formidable task.
The calculations required were often just too wild and wooly.
Modern computing, cheap and readily available, changed all that.
The result is that I didn't need to teach you to grind through the simplex method or
gradient descent or genetic algorithms or equations for finding multiple regression coefficients
and confidence intervals.
With nothing more than a PC and a spreadsheet.
You can handle all that and more.
And with spreadsheet add-ins, you can go much further still.
It's really quite remarkable to be honest.
But of course, there's a catch.
The only things allowed on the operations research superhighway are mathematical formulations
and the only things coming off of that system are information about those formulations.
So maybe you can chill out while you're on the highway, but every trip begins with formulation.
Every trip ends with interpretation.
And to date, that's work that no computer can do.
That's why we've focused on these aspects of solving problems throughout this series.
We've assembled quite a toolkit, with each tool suited to its own kind of challenge.
Problems that won't yield to one technique may yield to another, or perhaps to a combination
of several.
We learn analytics one technique at a time, but when applying it in the field, the solution
often involves attacking it on more than one front.
So in this final lecture, I'd like to demonstrate what it looks like to bring our entire toolkit
to bear.
And in addition to using some techniques we've already discussed, I'm going to introduce
a powerful hybrid of our stochastic work of the most recent sections of the course and
our optimization work from the middle lectures.
This combination is called, sensibly enough, stochastic optimization.
And it really is the best of both worlds.
We get started.
Let's go back to the example from last lecture.
You were a building contractor who was going to be submitting a sealed bid on a construction
job.
Low bid wins the job.
The first step was figuring out how much the job was going to cost.
The problem was stochastic, because the materials cost, the weeks of labor required, and the
weeks of delay in construction were all random variables.
As a result, we ran our simulation thousands of times, with different values for the random
variables, and compiled the results.
That gave us the frequency distribution for the quantities of interest.
Then we played some what-if games.
We saw, for example, that you could lower your average cost by increasing your number
of workers from four to six or eight.
Six was a little cheaper than eight on average, but eight greatly reduced the risk of running
up huge light fees on a job that took more than 12 weeks.
When you compare the eight-week worker peak, there in blue, to the original four worker
peak, you can see that it's further to the right, indicating that it has a higher modal
cost.
But remember that the mean of the distribution is the balance point of the pile of sand in
the picture.
The blue eight-worker curve balances out at about 94,500, while the red four-worker curve
balances to the right about $2,000 higher due to that long tail on the right.
But getting a handle on cost is only part of the problem.
You still have to make your bid.
And you have two competitors who are hoping to get the job themselves.
Let's call them Fred and Wilma.
Up to this point, the three of you have each used teams of four laborers and the same suppliers,
so you face the same cost curves.
Giving your understanding of the economics and the kind of work that you generally do,
though, you now am going to ace up your sleeve.
Assuming you get the job, you're going to use eight workers.
They'll stick with four.
To make a bid, each of you is going to estimate what you think the job will cost you, then
add to that a certain percent markup, which hopefully will be your profit.
For now, let's assume that each of you routinely decides on a 40% markup.
You might be happy with that, or you might want to change it.
In fact, the real question that we're trying to answer is how much you should bid.
Your golden bidding will be to maximize your average profit, but you may want to have a
constraint limiting the chance of losing too much money, a constraint, and an objective,
and a decision variable of what markup percentage you want.
That all sounds quite a lot like an optimization problem.
Let's see how far this optimization approach can take us.
The size of your markup determines your bid and indirectly your profit.
Is this relationship linear between markup and profit?
Well, if you get the job, each 1% increase in your markup adds the same number of dollars
to your bid and hence to your profit.
So yes, linear.
Or rather, no.
Because that increase in markup also reduces the chance that you get the job in the first
place.
From our work with expected values, we know that finding your average profit is going
to involve multiplying the job's profit by the probability that you get the job at all.
Since both the profit and the probability depend on the markup, we've got a non-linear
objective.
Too bad.
So we'll need a non-linear technique.
Which one?
We need to survey the mathematical landscape of our profit function in terms of our variable
percent markup.
What's it look like?
If it's all gentle rolling hills, we can use the steepest ascent algorithm.
We'll have to watch out for local minima, but steepest ascent algorithms tend to be
fast.
We can tell the spreadsheet to use a bunch of different start points for its search and
then take the best one.
But the landscape isn't all rolling hills.
Look at what happens when your bid is very close to the lower bid of your two competitors.
You need one cent less than anybody else, and you get the job, and make what you make
in the construction business.
Two cents more than that, though, and you lose the job, getting nothing but the bill
for the $5,000 prep fee.
So you're not going to be able to use a walk down hill approach because there's a cliff
in the profit landscape, right where your bid's tied for the lowest bid.
Which means that we need to use an evolutionary algorithm.
They don't like a lot of constraints, but we don't have many.
Just a limit on losing a lot of money.
Evolutionary algorithms are going to want us to put upper and lower limits on our variables,
but that should be easy enough.
Fantastic.
Ah, except that we've gotten so caught up with this optimization analysis that we've
been ignoring the elephant in the room.
This problem is stochastic, stochastic all over the place.
We want to solve an optimization problem, but we've never done that when dealing with
randomness at the same time.
There's no chance that we can come up with a strategy that's guaranteed to give you the
best profit here.
The best we can hope for is to maximize, say, expected profit or median profit.
Similarly, we can't absolutely assure that we don't lose more than a certain amount of
money if we make a bad bid.
We can only assure that such a screw up doesn't happen more often than we're willing to accept.
So what can we do?
Well, in rough terms, we need to run a simulation with a set of input values, look at the results,
use that information to make changes in the input values, examine the new results to see
whether the changes were in the direction desired, and continue this way as we narrow
in an optimal solution.
In two words, stochastic optimization.
If that sounds like a heck of a lot of computation to you, you're right.
Amazingly though, it's now possible to do problems like this in a spreadsheet environment,
with a surprisingly small amount of work, on the part of the human at least.
But to do so, we've got to cross the border that separates what current day spreadsheets
can do on their own, from what they can do with third party add-ins.
The kind of work that I'm going to demonstrate today can be accomplished by augmenting Excel
with add-ins like analytics solver, crystal ball, or at risk.
I'm not going to demonstrate those products explicitly, that's not my focus.
But I'll show you the kinds of results that they allow you to obtain, and give you a sense
of what it takes to use them.
I will say that these products improve each year, and that I'm sometimes flabbergasted
by how fast they can grind on hard problems using only a PC.
Once that is, that you formulated the problem, you still have to get to the superhighway.
So let's take the simulation that we wrote for costing the construction job and modify
it for our new needs.
First you'll recall we had a certain section for parameters, the constants of the problem.
Here it is now.
Here we arrange things a bit, but the same old information is there.
The distribution of materials costs, the late penalty information, the probability distributions
per delay time, and labor requirements, and information about your workers and their cost.
I've added similar labor parameters for Fred and Wilma who are only using four workers
each.
In the lower right corner I've added a section about how each of you price a job based on
your estimate of what you think it would cost you to complete it.
You'll see that I let you specify not only what markup you want on the job, but also
to set a minimum bid on any job in order to limit your downside risk.
Right now only Wilma is using that option.
So I added a bit to the parameters section.
I've also taken away a bit.
When we first encountered this problem I had to supplement the tables that you see in the
upper right hand corner with two more columns specifying the ranges of the random numbers
that corresponded to each output.
It looked like this.
Those RAND columns are no longer necessary, nor is using RAND or VLOOK up at all for that
matter.
Any simulation add in will include pull down menus for all of the common distributions.
Tables like our weeks of delay can be handled by a custom discrete choice from the pull
down menu and all you have to do is to fill in the boxes saying here's where the values
are and here are the probabilities that go with them.
Easy peasy.
For our normal distributed materials cost we click on the cell where the cost is to
appear, we choose normal from a pull down menu, then we enter the mean and standard deviation
that we want and we are done.
There are dozens of distributions to choose from, exponential, beta, uniform, Poisson,
binomial, triangular, some that even I haven't heard of.
The software will show you a PDF of whatever distribution that you've specified, then allow
you to modify it in various ways to better match your real world variable.
Okay, how about the cost simulation itself?
Well it looks very much like what we had before, only in triplicate.
I've made three copies of the original simulation, one for each contractor.
That way I can change the numbers of laborers, weekly labor costs, and prep fees of one contractor
without affecting the others.
Fred and Wilma currently have identical values of these parameters.
All three of you face the same size job, the same delay times, and the same materials
cost so I generated one random value for each of these and applied it to all three contractors.
Some of your values look different because of your eight worker decision, a shorter job
completion time, but a higher labor cost, since you may recall that you had to guarantee
your workers at least five weeks' pay.
Note that on this particular job that you see, Fred and Wilma's four worker cost is
actually cheaper than your eight worker one.
But now comes the new stuff.
We want to simulate the bidding, but a person's bids based on his or her estimate of the actual
job cost.
We have to model the accuracy of those estimates.
How do we get a handle on that?
Well, we could guess, of course.
That's more or less what we did earlier when we modeled material prices on the normal curve
using the mean and standard deviation from historical data.
Normal curves are easy and a lot of real-world quantities follow a normal distribution.
But maybe we can do better.
With an add-in, we in fact can.
We can take our historic data and fit a distribution to it.
Suppose we look at the percent error in the last hundred jobs that you bid and find that
the distribution looks like this.
As you can see, you've been close to right on average.
The balance point of these bars is very close to zero percent error, but there's some spread
high and low.
This set of a hundred is only a sample, though.
It's unlikely, for example, that you never underestimate a job by around 24 percent where
there's a gap in the histogram off on the left.
But the spreadsheet with an add-in can automatically compare this data to a host of different theoretical
probability distributions and assess how well each distribution fits the data.
We simply highlight the data, say, fit, and instantly get a set of distributions to choose
from ranked from best to worst according to the application of a statistical goodness-of-fit
test.
For our data, the best looks like this.
The software picked a shifted Weibull distribution, shown here in red.
It provides the graph and a variety of statistical charts and measures to assess the goodness-of-fit
between the distribution and the data.
In this case, everything checks out.
Once I approve this choice for the distribution, I click the cell in my simulation that's going
to contain the estimate error.
Thereafter, each time the sheet updates, it will automatically compute a new error for
your estimate, drawn from this Weibull distribution.
Since I don't have any data on your competitors, I'll run with the assumption that their errors
are distributed the same way that yours are.
You'll have about the same amount of experience in the business, after all.
But your estimates are independent, so each contractor is going to get his or her own
random variable.
So here's what the new part of the model is going to look like.
The first three columns contain those shifted Weibull distributions that capture the errors
in pricing.
In the trial shown, you have estimated the cost of the project while Fred and Wilma priced
it below the actual cost.
Now we can compute what each person actually bids.
Let's walk through the logic for Wilma, to let the same logic will apply for you and Fred.
For the current trial, Wilma's actual cost ends up being about $87,500, but of course
she doesn't know that.
What happens?
Well, her error shows that she underpriced the job by about 13%, 100% minus 13% is 87%,
so her estimate is only 87% of the actual job cost, which is about $76,300.
Now she applies her markup.
She wants a 40% markup according to our parameters section, so she multiplies her estimate by
140% or 1.4.
This gives her her bid of about $107,000.
Well, almost.
Because remember that Wilma had a minimum bid of $120,000 as a hedge against downside
risk.
She'll either bid what she just computed, or $120,000, whichever is larger.
In this case, since $107,000 is less than $120,000, she bids $120,000, as shown in the
table.
If you follow the logic we just laid out generally, you'll see that Wilma's bid will be this.
Her intended bid is the job cost, adjusted for her error, then increased by her markup
percentage.
And she'll bid that or her minimum bid value, whichever one's higher.
Okay, done.
And an equivalent formula is entered for you and Fred.
What happens in the trial shown?
Fred was the low bidder at about $115,000, so he gets the job, and actually makes about
$27,000 in the process.
By the way, both the winner's sell and the profit sell are just if statements.
For example, the profit sell says, if the winner is you, then write bid minus actual
cost, which would be your profit, otherwise write loss of prep fee.
So good for Fred.
Not so good for you.
Since you didn't get the job, you're out the $5,000 prep fee.
This time, remember, that's only one trial.
Every time we hit the F9 key, we get another simulation, like this.
This time, you overestimated your cost by almost 11%, but Wilma overestimated by even
more.
Underestimated is true cost, but this was a long job, so he accrued late penalties that
you didn't.
So your bid still comes in low.
You make a profit of over $57,000.
Sweet.
Well, you know what comes next.
We simulate the situation thousands of times, then collect the summary, summarize all of
those results.
Except with an add-in, we don't need to.
We can just tell the spreadsheet what sells you want to keep track of, and then click
on them automatically, provides us with interactive graphics.
It also computes a lot of summary statistics on this distribution, like the mean cost of
the job, which is about $97,000, and your mean profit, which is about $8,300.
Which is, wait a minute, you're going for a 40% markup, right?
40% on a job that costs nearly $100,000 is $40,000 profit, but your average profit isn't
$40,000.
It's only about 8,000, 9%, what gives?
Well, let's check out the profit graph and see what we can see.
Oh, and there it is.
See that huge spike at negative 5,000?
That's your payoff when you don't get the job.
What you can see is happening about 61% of the time.
Your average profit includes all of those times when you lose $5,000.
Interestingly, though, that's not all that's going on.
To see the rest of it, let's temporarily reduce everyone's prep fee to zero.
I'm also going to make this the most cutthroat market imaginable, so that people are going
to use a 0% markup.
They're going to bid exactly what they think the job is worth.
Now, admittedly, this isn't smart, but what do you think is going to happen?
Well, if people never made a mistake in estimating the job's cost, the whole thing would be a
break-even proposition.
If you don't get the job, you get zero, since we're waiving the prep fee.
If you do get the job, you bid at cost, and so your profit's still zero.
Of course, people do make mistakes in estimating job value, but we saw on average that their
estimate's right, overbidding and underbidding balance out almost perfectly.
So get the job or not, we should break even on average.
But if so, then what's the deal with this?
When I change the sheet's parameters to give a prep fee of zero and a 0% markup for each
contractor, the power of what if analysis again, the sheet reports that you don't break even.
On average, you lose over $2,000.
If you check Fred and Romy, you'll see similar results.
Why?
It's called the winner's curse.
And it arises anytime that people are bidding on something whose true value they don't quite
know.
Yeah, it's true that your bid, while sometimes high and sometimes low, is right on average.
In a trial on the screen, you got the job since you had the low bid.
But you had the low bid because you estimated the job at almost 14% under its actual cost.
As a result, you won.
And in winning, you lost over $15,000, hardly cause for celebration.
And if you think about it, the amount that you bid doesn't matter at all unless no one
bids below you, which means that you only get the job if nobody else believes that they
can do the job for what you think you can do it for.
And if your evaluation of the cost of the job is below both of the other two bidders,
there's a pretty good chance that you've underestimated the actual cost of the job and
in winning, you're going to lose money.
So even if you aren't looking to make a profit, you have to shade your bid, accounting for
the winner's curse.
How much each bidder should shade is an interesting and complicated question in game theory.
But we're not doing charity work.
Let's put back in the prep fee and restore our 40% markups for each bidder.
Your current problem is that you aren't making enough money because you aren't getting the
job often enough.
What if you slash your markup?
And while we're at it, I'm going to allow you to play with your minimum bid value too.
Given the other contractors are behaving as we described, what's the best thing for you
to do?
Well, one approach would be to plug in a bunch of different values for that 40% markup and
see what happens to our average profit as a result.
We could do this manually as we did earlier in the course, but all the adding programs
that we mentioned make it possible to do this very easily.
You specify the values that you want and the spreadsheet resports the results, including
some very nice charts like this.
I love this.
This is a trend chart showing what happens to profit as we vary the percentage markup
by 5% increments from 0% to 45%.
The blue dots in the middle are the mean profits.
On the left, with a 0% markup, you get the job about 97% of the time, but when you do,
you lose about $100 on average due to the winner's curse.
You also lose the prep fee and the 3% of the jobs where you don't clinch the contract,
so you lose on average about $250.
But the expected profits increase as we increase our markup from 0% to somewhere around 25%.
Then average profit starts to drop again as we price ourselves out of the market.
So for a best mean profit, we want a markup of about 25%.
Now the colored bands, which give us a sense of risk.
Imagine this as a single piece of green felt, or the piece of gray felt laying over top
of it and partially concealing it.
The gray band is the 75% confidence band, meaning that 75% of our trial runs gave profit
within it.
So the profits for a 0% markup were 75% likely to lie between negative $11,000 and positive
$11,000.
The corresponding green band lies between the gray one, so with a markup of 0%, it runs
from about negative $17,000 to positive $16,000.
This is the 90% confidence band.
That is 90% of the time the profit at 0% markup lies in this range.
5% of the time it may be above $16,000, 5% of the time it may be below negative $17,000,
but the rest of the time it remains in the confidence band.
Once you understand this, you can see that increasing the markup decreases the downside
risk, but that once you reach 20% markup, there's a limited effect.
At most 5% of the case is losing more than $5,000, your prep fee.
So any markup of 20% or more is pretty good insurance against losing your shirt.
On the other hand, the bands at the top of the chart continue to climb.
Why?
Well, the higher your markup, the higher your profit, if you get the job.
And every once in a while, both of your competitors are going to grossly overestimate the cost
of the job, and you can swoop in with your high, unprofitable bid.
But that doesn't mean you'll want to use such a markup.
For example, even though with a 45% markup, you have a 5% chance of making more than 45
thousand dollars or so, your average profit has dropped.
Such a high markup costs you more in lost jobs than it's worth on average.
Well, what we just did is very nice.
But what if we have more than one variable to look at?
It's quite a bit harder to implement.
What's the best combination of variable values for our contracting business?
We can find out with stochastic optimization.
Using the add-in, we specify that our objective is to maximize our expected profit.
We tell it that our decision variables are the markup percentage and the minimum bid.
We add a constraint specifying that we want to lose no more than five thousand dollars
at most one percent of the time.
What the heck?
Let's go for broke, and also let the optimization decide on how many workers we should use.
That will be an integer variable.
We really should go further, like specifying generous upper and lower bounds on each of
the variables and specifying that we want to use an evolutionary solver.
But the software has gotten pretty smart these days, and even if we don't specify these
things, it will probably make those decisions on its own.
I set our original values for our variables to exactly match Fred's, in which case our
average profit is just under seven thousand dollars.
Then I told solver to run, and he got a really good feel for the range of possibilities I
had to conduct each simulation five thousand times.
Evolutionary solvers trade away speed for flexibility, but in less than thirty seconds,
the solver had set the problem and gotten to work.
Within a minute, it had improved the original average profit.
In less than three minutes, it was done, with a suggestion that you use a markup of twenty-eight
percent, never bid under twenty-eight thousand five hundred dollars, and surprisingly, use
six workers, not eight.
Since you'd never want to bid under twenty-eight thousand five hundred anyway, the spreadsheet
setting that as a cutoff for a minimum bid was its way of saying that a minimum bid is
a stupid idea in this problem.
This keeps the chance of losing more than five thousand to about point seven percent
and gives an average profit of just under thirteen thousand dollars, and you win over
two-thirds of the bids.
That means of course that you're still losing five thousand bucks one-third of the time,
but that's the price of doing business.
The only way to reduce that is to lower your markup and win more jobs, and that reduces
average profit.
The kind of production possibilities frontier work that we did in multi-objective programming
can let us explore that possibility.
It shows, for example, that lowering your markup to about twenty-two-and-a-half percent
will get you an extra five percent of the contracts, but it reduces your average profit
by about five hundred dollars.
You can decide if it's worth it, and that's our solution.
In recent years, the kind of interface offered by the add-ins I've mentioned has been gaining
increasing prominence, and that's good, because that kind of intuitive, interactive, drill-down
environment is an ideal place to do analysis and data and analytics.
The software does the cranking, you do the interpretation.
The last part of your mathematical cross-country trip from the superhighway to your final destination.
For example, now that we've optimized our workforce size, markup, and minimum bid, we
can see what random factors have the most impact on our bottom line.
The software will compute the correlation of your profit to each of the random variables
over the thousands of trial runs, then present the result to you in a tornado diagram.
You can see why it's called a tornado diagram.
The bars are always listed from the longest to the shortest, making a rough tornado.
In general, some of the bars are pointing left, indicating there's a negative correlation.
Here all the bars show positive correlations, meaning that an increase in any of these factors
tends to increase your profit.
Number one on the list is Fred's error.
The more he overestimates the job cost, the more you like it.
The more he underestimates it, the less you like it.
And for obvious reasons.
A low bid from Fred's never good news for you, since it makes it easier for him to
steal your job.
Interestingly, William's WOMA's error is far less important.
Since she never bids below $120,000, her mistakes are less likely to result in you losing a
job.
If you look at the two bars between Fred and WOMA, they show that you actually like long
jobs with substantial delays.
Why?
Comparative advantage.
With your larger workforce, you're less affected by late penalties so you can undercut the competition.
So keep an eye out for this kind of job.
Materials cost helps you far less, since all three of you face the same cost here.
A big materials cost means that your markup gives you more money for the job at 28%, but
that's a weak effect.
And finally, there's virtually no correlation between whether your estimate's high or low
and how much you make.
Screwing up your estimate is bad if you estimate too high or too low.
So the net effect is a wash.
In a similar way, I could specify parameters of interest like cost of labor per week or
the size of a weekly penalty and could obtain tornado charts showing how your bottom line
depends on these factors.
Worth doing if the factor is something you might be able to do something about.
Of course, in the end, the problems that we've been considering are about real-life applications,
a fact that we always need to keep in mind.
If you implement the strategy we just developed for your construction business, you should
do quite well for a time, but if you model Fred's situation, you'll find that his mean
profit is now negative.
You know that situation isn't going to last.
He might get out of the business or he might slash his markup margin to make himself competitive
with you.
If he comes sniffing around your job site, he might also get the idea that hiring more
labor more than pays for itself.
And now you'll have a new problem of how to avoid a price war.
The field of game theory, a subject which I developed in another course, might be able
to help us here, but the tools and analysis we developed in this course are what can clue
us into the fact that the issue is going to come up.
In the long run, the decisions that we make can change what our competitors do.
Like so many aspects of life, business is an ever-evolving situation, but analytics
can help you to stay one step ahead of the competition.
And I hope that now that you've reached the end of this course, you feel you have a better
idea of how you can make predictive analytics, optimization, and computational power work
for you.
I've had a great time in this course.
I hope I've managed to convey not only the usefulness of analytics, but the beauty and
elegance of some of its techniques.
We've opened some doors to amazing mathematical worlds, and you can walk through those doors
anytime you like and take a closer look at whatever you've seen that happens to have
captured your fancy.
And me?
I keep doing that too.
But for the last 24 lectures, I've been the kid in the candy shop.
For someone who loves to teach, there are a few satisfactions as rich as the company
of someone who loves to learn, and that clearly describes you.
So thank you.
Thanks for sharing the trip.
