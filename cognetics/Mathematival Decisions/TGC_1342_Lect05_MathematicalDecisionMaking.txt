Statistical techniques like regression and time series forecasting were the strongholds
of mathematical prediction for much of the 20th century.
But they've been supplemented in recent decades by additional techniques from the exciting
and fast-growing area known as data mining.
Data mining focuses on large datasets and its job is to find what useful patterns and
connections they contain, connections and patterns that might otherwise be missed.
Some of the techniques used in data mining have been around for many years, including
regression and time series analysis, but others are quite new.
But data mining is especially important in the 21st century because of two advances.
When I was a kid, stores had to close from time to time to take inventory, to physically
examine the goods on their shelves and in their warehouses so that they knew what they had.
Those days are largely gone.
Now, when any item is purchased, a barcode scanner records its sale and all of that purchase
information is stored in a database, along with a lot of other information like what
else you bought and what it cost and whether you used coupons and, assuming that you have
a discount card, what other items you bought in the past.
It adds up to a lot of data.
According to the economist, Walmart adds about a million transactions every hour to its 2.5
petabyte database.
A petabyte is a million gigabytes or a thousand million megabytes.
To put such a large number into context, that's about 167 times as big as all the books in
the Library of Congress put together.
And that's just one company.
And retailers like Walmart aren't the only ones.
In today's world, an amazing amount of information is collected every day by many types of organizations
from satellite photos, internet traffic, GPS devices, telephone conversations, medical
tests, and on and on.
The trouble, of course, is how to make sense of it all.
And that's where data mining comes in.
One of the key differences between a lot of classical statistical techniques and data
mining is the quantity of the data available.
Throughout much of history, data was scarce and hard to come by.
And that meant that all the data that was available had to be used in the analysis and used in
double duty, both to create the model and to test its accuracy.
In order for that analysis to get very far, some assumptions had to be made.
For example, the confidence intervals that we've built with our regression analysis assumed
that our errors were normally distributed.
That means that if, for example, you're applying linear regression to predict a man's weight
based on his height, that there's a straight line relationship between height and average
weight for that height.
But if you measure the weight of a lot of men of that same height, their variation from
that average height, the ups and down random bumps that we talked about, would be distributed
as a bell-shaped curve, a normal distribution.
The spread of that curve isn't supposed to depend upon what height you focus on.
In technical terms, it is supposed to be homoscedastic.
The work also assumes that one observation being above or below the line a certain amount
doesn't imply anything about whether another observation will be above or below the line.
That is, the errors are assumed to be independent from one another.
Some divergence from these assumptions don't cause too much trouble.
But if they're seriously violated, you can't trust your confidence intervals.
Which means that you don't know how much you can trust your predictions.
In data mining, we often have an embarrassment of riches when it comes to data.
If anything, we have too much.
The first thing that means is that we can use part of the data to come up with our model
and a completely different set of data to test the reliability of that model.
This also means we aren't bound by a lot of assumptions, such as the distribution of errors.
In other techniques, such as regression, we can actually spend a lot of time transforming
the data so that we don't have heteroscedasticity and we don't have autocorrelation and don't
have error terms that are distributed, for example, like lottery payoffs.
Lots and lots of small negative values sprinkled with occasional big positive values.
Distributions like that are said to be highly skewed.
Values on one side of the average clump together while on the other side they're spread out.
A long way from the assumption of normally distributed errors.
Fixing these problems can be quite a headache.
With data mining, the headache largely goes away.
Practitioners are sometimes, sometimes debate exactly what the margins of data mining are,
but there are three objectives central to it.
The first, data visualization and exploration.
The second, classification and prediction.
And the third, association among the cases.
Time series analysis, which we've already discussed, is sometimes considered part of
data mining too.
Let's start with visualization.
When we're exploring data in data mining, we're probably going to start by looking
at one variable at a time.
If the variable's categorical, bar charts are a good choice.
We can see at a glance here that heart disease and cancer, malignant neoplasms, are overwhelmingly
the most common causes of death in the U.S.
And seeing that in a chart conveys this information much more quickly and memorably than it says
just a table of numbers.
But increasingly, software supports the ability to drill down to look at interesting aspects
of the data in greater detail, like this.
From our last graph, we can break things down by age.
In my age group, the greatest threats to health may be heart disease and the big C, but we
can see from the green bars that for teens and young adults, the list is quite different.
Unintentional injury, suicide, and homicide.
Excel's pivot tables and pivot chart can give you some functionality in this area.
For a single numeric variable, the most common choice is the histogram, which is like a bar
chart in that each bar represents a range of values.
Here you can see a histogram of the distribution for how many Facebook friends the students
in my university classes have.
The relationship between two numeric variables, as we've seen, we generally use scatter plots.
This initial exploration often uncovers connections between the variables, connections that we
can exploit in a further analysis.
We can push it one more dimension to look at relationships among three variables in
a three-dimensional scatter plot.
That's less useful when the software renders such an image as a static plot in two-dimensional
screens like on a screen, a piece of paper, but there are still options.
We can code additional information with color or size.
A lot of weather maps do this, where a temperature at each point in the country is given a color
corresponding to it, the hotter the color, the hotter the location.
Such representations are often called heat maps, even when the color coding is something
quite different from temperature, like ozone concentration, or when the thing being colored
isn't a map at all.
For example, there are heat maps of the stock market, arranged by market sectors, sized
by company value, and colored by a change in stock price.
We can push the envelope still further by using the time dimension, combining multiple images
into a movie.
Swedish academician Hans RÃ¶lling created an astounding video that you can find on
BBSoar 4's website, where he tracks the wealth and longevity of people in 200 countries over
200 years of history.
He uses a scatter plot with an axis for lifespan, a logarithmic axis for wealth, color to indicate
continent, size to indicate population, and time to indicate the passage of time.
The film lasts only four minutes and is stunningly effective at communicating all of that data.
Processing visual information in two or three dimensions, recognizing structures and patterns
there, is something that humans are uncannily good at, which is why charts and graphs exist
to begin with.
It's also why whenever possible, I'm going to use pictures to get across new ideas in
this course.
But as the number of variables increases, obvious ways of representing data graphically
fail.
Each variable, each category of information needs its own dimension of space, and for
visualization three dimensions of physical space is our limit.
So what can we do?
Well for starters, we can conduct a kind of dimensional reduction by using mathematical
procedures.
It's like using a student's GPA or SAT score in place of every detail of his or her academic
history.
The trick is to find ways to summarize multiple pieces of data in a way that loses as little
information as possible.
The computations required may not be simple, but since data mining by definition deals
with large datasets, no one would perform the calculations by hand anyway.
Statistical software such as SAS or SPSS or the free package known as R are perfectly
capable of this kind of dimensional reduction.
In fact, it can even be done in Excel with an add-in package such as Excel Miner.
Because the tools always do it for you, I don't want to focus on the computational details
of the techniques we'll discuss.
Instead, my intent is to give you an understanding of when the techniques are useful and some
intuitions about how they work, and some examples of the tools in action.
So let's take a look at an example.
Suppose I'm engaged in a project to predict whether a person owns a computer tablet.
I have a large dataset with information about thousands of people with dozens of pieces
of information about each person.
Suppose that among them are a person's age and the number of Facebook friends that they
have.
For demonstration purposes, let me focus on only these two variables and only on a hundred
people.
The restrictions won't alter the idea of what's going on.
Since the dataset's fairly small, I'd prefer to display it as a dot plot than a histogram
so that I can see every person and yet still perceive the pattern.
As you can see, there's considerable variation in the ages in our data, and also in their
number of Facebook friends.
But if we make a scatter plot of these two variables, we see that they actually have
quite a strong correlation with one another, with an r-squared value of .76.
So while either of these variables might be quite important in predicting whether someone
owns a computer tablet, a lot of their variation is actually co-variation.
Young users tend to have more Facebook friends than older ones.
That means that useful information contained in one variable is, to a significant degree,
repeated in the other.
It's for situations like this that a technique called principal components analysis, or PCA,
exists.
The idea is to create new variables that partition the variation in a better way, with the first
variable capturing as much of that variation as possible.
I've chosen this simple two-variable example because it allows your natural visual processing
to be brought to bear.
Looking at this picture, it's clear that most of the variation is occurring along a
diagonal line from upper left to lower right.
How do we measure variation?
By using the most common statistical measure of dispersion, the variance.
The calculation of the variance bears a great deal of similarity to our calculation of MSE.
Remember that average the squared error's calculation from regression?
For variance, the only difference is that the error is taken to be the distance of each
point from the mean of the data.
If you take the square root of the variance, you get the standard deviation, something that
comes up in most statistical discussions.
So PCA begins by finding the line along which that variation, that variance, is maximized.
If you're a glutton for punishment and are good with matrix algebra, you could do this
yourself, by finding the eigenvector of the covariance matrix of the data that has the
largest eigenvalue.
Yeah.
Personally, I'd let the computer do the math, which gives us this line, which would normally
be called the Z1 axis when we're doing PCA.
Impressively, this line captures over 97% of the total variance in the two variables.
That's great.
In an important sense, we're already done our work.
But if you want to capture that remaining 2.6%, visible here is the spread of the points
around the Z1 axis.
PCA we construct a second axis, perpendicular to the first, called the Z2 axis.
I've shown it here in green.
Converting data from the original age and Facebook friends values to the Z1 and Z2 values
is just basic algebra.
The Z1 value, for a point, can be computed as a specific, linear combination of age and
Facebook friends, and the same is true for Z2.
And part of the PCA analysis is finding and reporting these combinations.
For our example, to find Z1, you take 94.8% of a person's Facebook friends value, and
subtract 31.8% of his or her age, and then subtract 66.1.
For Z2, it's 94.8% of the person's age, plus 31.8% of his or her Facebook friends count,
then subtract 60.4.
By the way, you may have noticed the similarity of the coefficients in the two expressions.
It isn't coincidence, but it's a consequence of the two axes being perpendicular, so their
slopes are negative reciprocals.
If we were combining more than two original variables, we generally wouldn't see such
an obvious pattern.
Okay, let's try this out.
For example, the red point in our scatter plot represents a person who's 70 years old
and has 51 Facebook friends.
We'll use our two magic equations to find Z1 and Z2 for this person, like this.
You can see what the new coordinates mean, as is shown on the graph, 40 units down on
the Z1 axis, and 22 units over on the Z2.
Well, swell.
But what did we really gain here?
We replaced two variables that we understood with two new variables that are hybrids of
the original two.
And who knows intuitively what Z1 equals negative 40 even means?
This is progress?
Well, if we keep both variables, no.
But given that over 97% of the variation in the original two variables is captured by
one new variable, Z1, it becomes a possible approximation just to genison Z2, and use
Z1 in place of the original age Facebook friends pair.
And remember, it's in more complicated examples with more than a handful of variables that
this becomes particularly useful.
In those cases, PCA finds the direction in multidimensional space that captures the largest
possible fraction of the variation in the data and calls that the Z1 axis.
Then it looks for a direction perpendicular to that one that captures the maximum amount
of remaining variation.
That's the Z2 axis.
And with more than two dimensions in the original problem, there are more perpendicular directions
to choose among.
PCA continues like this.
Next it finds a direction for Z3 that's perpendicular to both Z1 and Z2 and captures as much of
the remaining variation as possible, and so on.
We often find that after only a handful of principal components are found, the vast majority
of variation is accounted for.
If that's the case, the original variables, however many there were, can be discarded
and replaced with this handful, losing almost nothing about the original variation in the
data and greatly simplifying future work.
But there are some disadvantages too.
Principal components analysis won't work well for data whose underlying pattern is not
linear, but that can be overcome by modeling a non-linear pattern using local patterns
that are linear, analogous to what we learned in multiple regression.
A more fundamental disadvantage is difficulty and interpretation.
We understand what an age of 70 is, but what is the Z1 score of negative 40?
Then we can make some progress on this by looking at the size and sign of the coefficients
in the equations for the various Z sub i.
For example, suppose that we're looking at airline passengers and we find that Z1 has
a large positive coefficient for on time and short weight on check-in and a large negative
weight on baggage lost and bumped.
These are all service convenience time issues.
Z2 may have a large positive weight on free upgrades and complimentary drink and a large
negative weight on ticket price.
Then Z2 could be summarized in terms of cost-value issues.
One other problem with PCA, at least as I presented it, is it has to deal with the units
of measurement of my variables.
In my age, Facebook friends example, if I'd measured people's ages in months rather than
years, I would have gotten very different principal components.
Now all the age numbers are 12 times as large as they were, and so the variation in ages
has grown correspondingly.
The Z1 component is now much more concerned with capturing that increased variation.
Whether this is a good thing or not depends on the particular problem and brings up a
question that needs to be considered before applying most data-mining techniques to normalize
or not to normalize.
Normalizing data means standardizing it.
We do this by first subtracting the mean of the variable from the values of that variable
and then dividing each of those differences by the standard deviation of that variable.
No matter what data you start with, after you've normalized it, the mean of the data is zero
and the standard deviation is one.
In that sense then, each variable is now on an equal footing.
A person of age 51 with 120 Facebook friends is equally remarkable for being one standard
deviation above the average age of 36.3 and being one standard deviation above the average
number of Facebook friends, 81.9.
He or she would have a normalized score of one for each of the two variables.
Generally, if the two variables are measured in units that aren't comparable or on dramatically
different scales, normalization is a good idea.
Take hourly wage and outstanding mortgage value.
Both are in dollars, yes, but a $10 change in wage is huge while a $10 change in mortgage
is insignificant.
That's the data.
With age and Facebook friends, I don't even have the same units, so normalization is probably
what should have been appropriate for my example.
I didn't normalize because with only two variables, the resulting PCA is always rather boring.
The two principal components are essentially just the sum and the difference of the two
normalized variables.
There's another very human factor that can be useful in reducing the dimension of the
data, domain knowledge.
One familiar with a field of investigation can often narrow our focus before we even
start with the math.
They can tell us what quantities are almost certainly relevant or irrelevant to the task
at hand.
They can also let us know when one of our variables is useless as a predictor because
we won't be able to gather it in time to make use of it.
Power outages may be a wonderful predictor of home generator sales, but by the time that
you know about the outage, it's too late to stock more generators.
Any knowledge can also allow us to inspect the final answer and see if it makes sense.
Okay, so far we've discussed visualization, exploration, and dimensional reduction of
data, but in a way that just gets us ready for the real work.
Perhaps the most common kind of task for data mining is using data about an individual
to predict some unknown characteristics about them.
An individual here could be a person, or a river, or a bottle of wine, or anything else.
The thing I'm trying to predict might be a categorical variable such as brand of car
a person owns or whether they would accept a particular offer to refinance their home
or whether they committed fraud or their income tax.
When that's the case, prediction is often called classification, sorting individuals
into the classes where we think they belong.
On the other hand, we may be trying to predict a continuous variable like a person's life
expectancy or the probability that a firm will fail within the next five years.
Some people use the term prediction only for continuous variables, while some use prediction
for sorting individuals into classes as well.
So let's start with a classification example and one of my favorite techniques from data
mining, classification trees.
If you're working with continuous variables rather than classes, they're usually called
regression trees, but the idea is the same.
I like the technique because it's a great off the shelf approach.
It can be used on a lot of different kinds of problems.
It's easy to explain to non-technical people.
The results are directly usable by the non-expert, and it doesn't care if the data is normalized
or not.
If you want to use categorical variables that have more than two categories like kind of
pet, you have to create dummy variables to replace it, dog, yes or no, cat, yes or no,
hamster, yes or no, and so on, but that's not really a problem.
For my example, I'm going to use a data set obtained from the UCI machine learning repository
at the University of California at Irvine website.
It lists 58 characteristics of over 4,600 email messages.
Included in the list of characteristics is whether or not the email was spam.
We're going to use classification trees to see how well we can do in building our own
little spam filter.
To get an idea of how classification trees work, I'm going to focus our attention on
only two of the variables in the problem and look at only a small number of the 4,600 cases.
My reason is the same as with PCA, to let you use your visual intuition to understand
the process.
Mathematically, the procedure in higher dimensions and larger sets is a perfect analog of what
we're going to be doing here.
So the variables I'm going to focus on are x, the percentage of characters in the email
that are exclamation points, and caps, the average length of a string of capital letters
in the email.
Note how domain knowledge is already at work here.
I've seen enough emails to suspect that these might be useful flags for spam.
Let's start with a scatter plot.
Exclamation point percentage on the horizontal axis and average caplants on the vertical.
The red dots are spam, the blues are non-spam.
You undoubtedly see some patterns.
Spam tends to be to the right and top of the picture, non-spam tends to be on the bottom
and left.
But for classification trees, we're going to pay attention to only one variable at a
time.
By focusing on x, the percentage of exclamation points in the email, I want to draw a vertical
line that so much as possible separates the spam and non-spam.
To my eye, it belongs around here.
So how well did I do?
Well on the left side of the line, there are 24 points and 23 of them are real emails.
So by classifying those points as emails, I'd have a 23 out of 24 or 96% success rating.
On the other side though, I don't do as well.
There are 26 points on the right of my line and only 19 of them are spam.
So by classifying the right side of my graph as spam, my success rate is only 73%.
Overall success rate, 84%.
You might think that overall success rate is the right way to proceed here, but not
so fast.
There are actually several problems with that.
First off, how about if what you're looking for is either very common or very rare, like
say an unusual cancer?
When so few people have it, there'd be very few red dots in my scatter plot.
The lowest overall error rate might just be to classify everyone as not having cancer.
If only one person in a thousand has the disease, my error rate for doing so is 0.1%.
Of course, what I'm trying to do is to detect cancer for treatment, this is worthless.
Instead of overall success rate, we often talk about sensitivity and specificity.
If what we're looking for is people with cancer, the sensitivity is the probability
that we correctly identify someone with cancer as having it.
100% sensitivity would mean that we found everybody who has cancer, though maybe we
worried a bunch of people with false scares too.
The specificity is the probability that we correctly identify people free of cancer so
that we don't give a healthy person cancer therapy.
So if we classed everyone as being free of cancer, we'd have 100% specificity, we got
all the healthy people right, but 0% sensitivity, we got all the cancer victims wrong, hardly
a satisfying screening procedure.
The best classification procedure usually involves a trade off between sensitivity and
specificity.
The cancer example brings up a second point too.
It's not just the fraction of the population having the characteristic that you're looking
for, it's also how important it is if you find them.
Not finding a person who has cancer might be a dense sentence for that person.
On the other hand, identifying a person who's free as possibly having it will lead to some
more tests, not pleasant, but much less severe a consequence than death.
So when deciding our best classification rule, we're going to need to take into account
the relative costs of making mistakes in different categories.
We'll come back to this idea later.
For the moment, let's just treat spam and not spam as being equally important so a screw
up in either direction is equally bad.
We've just divided the original rectangle into two by drawing a line.
The idea behind classification trees is to keep doing this in a way so that each line
drawn results in new rectangles that maximally increase the purity of the partitioning that
we've done so far.
There are a couple common measures of this purity, one called the entropy of a rectangle
and one called the genie index.
The genie index is a little easier to understand, so that's what I'll use.
To find the genie index of a rectangle, take the fraction of points in that rectangle that
are classed as each of the possible output classes, square these proportions, add them
up and subtract the total from 1.
Let's see how that works.
In our original picture, before we drew our line, there were 20 spam points, 40%, and
30 non-spam points, 60% of the points.
To find the genie index, take 40% and 60%, square them to get 0.16 and 0.36, add those
up to get 0.52, and the last step, subtract it from 1 to get 0.48.
So the original genie index is 0.48.
Smaller is better.
A perfectly pure rectangle with only one color dot would have a genie index of 0.
Okay, so after I drew my vertical line, what's the genie index of each of the two sub rectangles
I made?
The one on the left is 96% blue and 4% red, for a genie index of 1-0.962, I'm sorry,
0.96 squared minus 0.04 squared equals 0.08, nice.
The right hand rectangle is 70% red and 27% blue for a genie index that works out to
0.39.
The rule for combining these is to weight them by the number of observations in each
rectangle.
48% were in the left rectangle and 52% were in the right rectangle, so the genie index
for the two together is 0.48 times 0.08 plus 0.52 times 0.39 is 0.24.
Cut into the chase.
By drawing a vertical line where we did, we dropped the genie index from 0.48 to 0.24,
a reduction of 0.24.
It turns out that no different horizontal or vertical line would have reduced the genie
index by more, and it's for that reason that the classification tree's algorithm would
draw the first line that I did by eye.
The algorithm just keeps on repeating the process, checking each possible horizontal
or vertical line in each new rectangle, and choosing the dissection that causes the greatest
drop in the genie index.
The best split for the left-hand rectangle would be to draw a horizontal line, splitting
the top two points off from the rest, but this line drops the overall genie index by
a paltry 0.008.
How about the right rectangle?
Well, it's not hard to believe that for it, the best choice is a horizontal line a bit
above 3, like this.
The upper rectangle has 10 points all red for a genie index of 0.
The lower rectangles are fairly a mixed bag, but when you find its genie index and compute
the weighted average for the pair, it turns out that splitting the right-hand rectangle
this way drops the genie index by 0.047, much bigger than the 0.008 we saw for the left-hand
rectangle.
So this horizontal line is our next dissection.
Next comes another vertical line for the lower rectangle, and we can keep doing this
in this way until we have classified everything, like this.
We've done the work with only two variables, but the procedure's identical in higher dimensions.
Instead of two-dimensional rectangles, we get n-dimensional ones, but you don't need
to be able to see them to apply the procedure.
At each step, you're looking for a cutoff value of one variable applied to one rectangle
that cuts that rectangle in two.
For each possible division, compute the genie index of the partition and choose the one
that gives the smallest genie index.
You're going to find the rectangle, the variable, the division point that drops the genie index
as much as possible, and then repeat the whole process again.
When you're done, every box will have dots of only one color in it.
Note too that there's nothing in the procedure that says that you have to classify into only
two classes, like spam and non-spam.
We could have dots of more than two colors and classify into more than two categories,
and the technique still works fine.
All of these lines that we drew can easily be turned into a series of binary decisions,
and those decisions can be arranged in a tree that captures the decision rules we've created,
a classification tree.
For the work we just finished, here's the tree.
Let's zoom in for a closer look and see how we'd use it to classify an email that was
.07% exclamation points, and in which the average length of a string of caps was four.
Here's the first question.
It's about x, the percentage of exclamation points.
If it's less than .0865, go left.
More than .0865, go right.
At .07% exclamation points, I'm left of the cutoff, so I go left.
The next node is about caps.
If the average cap's length of a string is less than .3.6, go left, otherwise go right.
Mine was four, so I go right.
And we just keep going.
The next node is also about caps, and since four is less than .544, the threshold, I go
left.
My message is classified as spam.
This is all very nice, but it's not enough.
What we just built was a classifier that's perfectly accurate in separating spam from
non-spam, but only for the set of 50 emails that we started with.
Look at the last node that we worked with, and you'll see something that looks fishy.
It says that if my emails had contained longer strings of capitals, it would have been classed
as good.
Our tree says this because our original data included one good email with a string of seven
caps and no exclamation points, and our tree generalized from that.
That rule happened to work for our limited emails that we used to create the tree, but
we want to build a classifier that will do well with new data, not just be flawless in
the data that's already seen.
The answer is going to be more data, not just more training data that we can use in creating
the tree, but more new data that we can use to assess how well our classifier will do
in the field.
This approach applies not only to classification trees, but to most data mining applications.
We'll be looking into this in our next lecture, along with the other important task of data
mining association.
For example, how does Amazon decide what offers to make you?
How does Pandora Internet Radio create custom stations to reflect each listener's musical
tastes?
Data mining has raised the business of anticipating customer preferences to a high art.
Next time, we'll look into how it's done.
See you then.
