Sometimes, as the old saying goes, history is destiny.
The trajectory of future events can be strongly influenced by what came before.
In fact, sometimes that future seems almost foreordained.
Is this a good thing?
Well, it depends on what you mean by good.
It can certainly be useful if you're in the forecasting business.
You'll remember from our discussion of time series that the whole analysis hinges on one
crucial assumption, that what happened in the past is a good guide to what will happen
in the future.
And we were making that assumption in a very strong sense that the observed trends and
seasonality seen in the past could be expected to continue into the future.
But you might remember another topic discussed in that lecture, a random walk.
We came across it in the context of an investment whose value each day changed by an amount
and in a direction that was determined by the flip of a coin.
By chance.
I mentioned that some people think the short-term variation in the Dow Jones Index is essentially
such a random walk, and a case like that is to pass the good model for the future.
Well, yes.
And no.
No, in that history is not destiny, the only thing that matters about your history is the
very last part of it, your situation right now.
How you got there doesn't count for beans.
On the other hand, if you do know your situation now, then I can assign probabilities to what
might happen next.
Probabilities that depend only on that current situation.
And that allows a mathematical analysis of an entirely different kind.
That's what this current lecture is about, and it goes by the name of Markov Analysis,
named after the Russian mathematician Andrei Andreevich Markov, who did a lot of the early
work in the field.
In business and organizational decision making, it's useful for tasks like anticipating bottlenecks,
whether on the factory floor or an telecommunications network, predicting demands on hospital resources,
characterizing browsing behavior, designing maintenance schedules, and predicting failure
times, assessing the behavior of waiting lines, and evaluating whether the admissions to a
university program today will result in overloading the program in years to come.
It's no less useful in the sciences, for example, modeling diffusion across a membrane, or
drugs in a body.
Here's the mathematical skeleton of a Markov process.
You've got a system in which you're interested.
At any given point in time, this system can exist in one of a variety of states.
In a lot of systems, the list of possible states is finite, but it doesn't have to be.
As far as the management of a restaurant's concern, for example, a table might have eight
possible states.
Table reserved, table available, guests selecting orders, guests ordering, guests waiting for
food, guests eating, guests waiting for check, table unoccupied, but needing cleaning.
A system generally stays in one state for some amount of time, then transitions to another.
Each such transition has a specific probability, and in a Markov process, it's one that only
depends on the current state.
So for example, a table that's currently unoccupied but needing cleaning may stay that way for
the next minute, or it may become a table that's available, or it may become a table
that's reserved.
If we analyze the system from the point of view of a diner, it would have different states.
The diner would probably finally reach the state of paid bill and leave, although other
possibilities like leave without eating might exist too.
What will be the long-term behavior of such systems?
How will they evolve over time?
And if a system ever does reach an unchanging final state like paid bill and leave, how
long will it take to get there?
These are the kind of questions we're going to look at today.
We're going to see how to represent these states and transition probabilities in a picture
called a Markov diagram, a graphical structural model of how the pieces fit together.
We'll then convert that picture into a compact mathematical representation of the situation
using what are called transition matrices.
Finally, we'll use the insights of Markov analysis to extract a wealth of information
from the result.
Let's introduce these new tools with a second look at a now familiar problem, the downed
plain example from last time.
Markov analysis will give us a much easier way to envision some aspects of that problem.
Then we'll apply our new Markov toolbox to something a bit more ambitious and see how
they help the German direct marketing firm avoid financial disaster.
So first tool, the Markov diagram.
In some ways you could think of it as an odd cousin of the decision tree.
Each node is a state, it's a situation you could find yourself in.
All of these nodes are like the chance nodes in a decision tree.
There aren't any decisions for you to make, and so each of these nodes has a certain probability
of transitioning to another state, another node.
But unlike a decision tree, there may be more than one way of getting to the same state.
In spite of these differences though, the path through a Markov diagram, like the path through
a decision tree, tells a story.
Here's the Markov diagram for our plain search.
What's going on here?
Begin at the node labeled start at the top.
From there we follow an arrow making a transition to a node specifying where the wreck is, zone
one through zone six.
Each edge leading to a possible site of a wreck is numbered, giving a probability of
following that edge, so 25% of the time we arrive at the node for zone three, which means
that the wreck is in zone three.
Whatever node we reach, we then continue to the bottom of the diagram, ending at either
the node where the search found the wreck or where it didn't.
For example, if we follow this path, it means that the wreck was in zone three and then
it was found.
The first edge has a 25% chance of being followed, since originally we said there was a 25% chance
of the wreck being in zone three.
The second edge has a 90% probability since if the wreck is in zone three, then our search
is 90% likely to find it.
Our rule for joint probability thus tells us that the probability of the sequence of
events is 0.25 times 0.9 or 0.225.
On the other hand, take the route from start to zone one to found.
This one never happens.
There's a 10% chance that we originally transitioned to node one, that is that the wreck is in
zone one, but if we get there, there's a 0% chance of making it to the found node.
The search of zones two, three, and four didn't include zone one, so the search, this search
is guaranteed to fail.
And multiplying the probabilities on this path show this, 0.1 times 0 equals 0.
This is how Markov diagrams work.
If you want to know the probability of moving around the diagram following a certain path,
just take all the probabilities on that path and multiply them together.
The method gives the correct answer, so long as the probability of ending up at a particular
node after a transition depends only on what node you're leaving in that transition.
That is, we've got a memoryless process where how you got to a particular state doesn't
matter.
It matters in terms of what happens next is where you are now.
There are actually ways of introducing memory into a Markov process, but they're clumsy,
and I'm not going to focus on them here.
Let's use our Markov diagrams to find out how likely it is that the search fails.
That was an important probability in the last lecture.
Well, there can be six different routes through the Markov diagram that ends in this result,
as shown by the six different colored paths here.
The probability of any one path is obtained by multiplying the probabilities along it.
Do it for each path, and then add the results.
That's the probability of reaching the failed node.
That is the chance that the search failed.
It's actually the same calculation that we did in the last lecture.
Well, the picture is nice, but it'd also be nice to have a more compact way to do this
calculation itself, and there is one.
One whose utility goes far beyond what we're using it for today.
It's matrix arithmetic, or more specifically, matrix multiplication.
So let me give you a quick primer on that.
If you already know it, it'll be review, and if not, it'll get you up to speed for
what we need.
In mathematics, a matrix has nothing more than a rectangle full of numbers, so many rows,
so many columns.
If it's only got one row, it's also called a row vector.
We usually put brackets around the matrix.
The dimensions of a matrix are the number of rows and columns.
So here's a six by two matrix, six rows, and two columns.
You can add or subtract any two matrices that have the same dimensions, and you do it in
the obvious way, just by adding or subtracting the corresponding locations.
So if matrix A has a six in the upper left-hand corner, and matrix B has a two there, then
the upper left-hand corner of A minus B is just six minus two.
You play the same game in each location in the matrices, which is why they have to have
the same dimensions.
But the tricky operation with matrices is how to multiply one by another.
A natural choice would be to play the same game you did with addition and subtraction,
upper left corner of A times upper left corner of B and so on, and you can do that operation,
of course, if the matrices happen to have the same dimensions, but it turns out not
to be very useful.
There's a different way to define matrix multiplication that ends up being much more
useful and is exactly what we want for a Markov process.
In fact, I'm going to introduce matrix multiplication in general by thinking about the Markov diagrams
in particular.
I want you to imagine that our matrices have rows and columns labeled with states, and
that the row of the matrix tells you where you're coming from, while the column of the
matrix tells you where you're going to.
In our airplane search Markov diagram, we begin at start, but we can go to any of six
nodes labeled one to six.
I can represent this by a row vector.
One doesn't normally put labels on the rows and columns of a matrix, but including in
here should make things a lot clearer in our current discussion.
This one row matrix, this vector, lists the possible transitions from start to a zone
state, and it gives the probability of ending up at each zone after one transition.
Those entries add up to one, since there's a hundred percent chance that you end up
somewhere after a transition.
It's not true that every row of an arbitrary matrix will add to one, but every row of a
Markov transition matrix will.
This row vector can also be called the initial state vector.
That's because it gives you the probabilities that the system is initially in one of the
possible states.
Here are the probabilities that the wreck is in each possible location.
Okay, how about the second transition?
The transition from the zone nodes to the final result.
That'll look like this.
Again, the numbers are just transition probabilities, with the probability that the plane was found
in each row of the left column, and the probability that it failed in the right column.
For example, the .9 in the zone 2 row and found column means that given that you start
in the zone 2 node, there's a 90% chance you'll transition to the found node.
That is, if the wreck's in zone 2, you're 90% likely to find it.
So each matrix is, if you like, a table of conditional probabilities, given that you
begin a transition in a particular row state.
How likely is it that you'll transition to a particular column state?
Here's the thing to keep in mind.
Matrix multiplication is so defined that it corresponds to the English phrase, and then,
from Markov matrices.
We want to say, from start, we transition to a zone, and then, from there, we transition
to finding the results of the search.
To write this mathematically, we just multiply these two matrices.
I've suppressed the labels here, but keep thinking about them.
They're quite useful.
Remember that for each matrix, the row tells you where you're transitioning from, and the
column tells you where you're transitioning to.
So again, the first matrix, the row vector, takes us from start to a zone node, and the
second matrix takes us from a zone node to a result node, either finding the wreck or
failing to.
Now, I want you to see that in order for this to make sense, the number of columns in the
first matrix, where the first matrix drops you off, has to be the same as the number
of rows in the second matrix, where the second matrix picks you up.
And a little more thought should convince you that the two together tell a whole story,
which begins at start and ends with either found or failed.
That means that the answer to this computation should be a one by two matrix, one row corresponding
to start, and two columns corresponding to found or failed.
This is a general truth of matrix multiplication.
When you multiply two or more matrices together, the final answer will have the same number
of rows as the first matrix, and it'll have the same number of columns as the last matrix.
So a two by three matrix times a three by four times a four by five will give you a two
by five matrix.
And each matrix in the product represents one transition, one step through the Markov
diagram.
So how do we actually do matrix multiplication?
Like this, take the row vector and stand it up on its end, line it up with the first
column of the second matrix.
Now we multiply the two numbers in each row, and then add the six results like this.
Just go right down the column, point one times zero is zero, point two times point
nine is point one eight, point two five times point nine is point two two five, and so on.
Add up all six of these numbers, and you'll get point five eight five.
Now take the row vector, stand it up next to the second column of the second matrix,
and do the same thing again.
Multiply each entry in the row vector by the corresponding number in the second column,
and then add the results.
Point one times one is point one, point two times point one is point zero two, point two
five times point one is point zero two five, and so on.
The sum of all of these numbers is point four one five.
So the result of our matrix multiplication is this.
Which means that the search is 58.5% likely to result in finding the rack, and 41.5% likely
to fail.
A couple of important observations on what's been going on here.
This thing, arithmetic, there was a lot of it, and multiplying matrices always involves
a lot of it.
Which is why as an undergraduate I didn't give matrices the attention that they really
deserve.
But multiplying matrices by hand is no more necessary or informative than multiplying
normal numbers by hand.
A lot of scientific calculators can do matrix multiplication these days, and almost any spreadsheet
can.
Here's how you'd use calc to do the math that we just did.
The calculation is done by highlighting both the cells that I filled here in green, and
then typing in this command.
Which says to matrix multiply, M molt, the A box, and the B box.
As always if working in Excel instead of using a comma, we use a comma instead of a semicolon
as a separator.
But there is one freaky thing.
Since this formula fills more than one cell, the green cells in the spreadsheet, you don't
just hit enter.
When you enter it, use control, shift, enter.
This tells the spreadsheet that you're entering what's called an array formula.
Just be sure you highlight the right size rectangle before you start typing in the formula.
For example, a six times five, six by five times a five by seven matrix is going to require
you to highlight a six by seven rectangle to hold the answer.
But there's another observation that I don't want you to overlook, and it's this.
The definition of matrix multiplication corresponds exactly to the words and then when looking
at a Markov diagram.
In other words, as soon as I could say the plane went down in a zone and then from that
zone it was either found or not?
Well, I knew I only had to multiply the what zone vector by the found or not matrix to
get the probability of the search being successful.
I could do the needed calculations almost on autopilot, or have a spreadsheet do them
for me.
In spite of its odd definition, in most ways matrix multiplication behaves like the ordinary
multiplication you're used to.
It's associative, it distributes over addition and subtraction and so on.
But there's one oddity that might surprise you.
It's not commutative.
A times B and B times A aren't the same.
In fact, one of them may not even exist.
If you think about the Markov interpretation, it's not hard to see why.
If we reverse the order of our Markov matrices for the plane rack, putting the second one
first, like this, then the first matrix is taking this from one of the six zones to either
found or fail.
Logically, the subsequent and then would have to start in found or fail.
But it doesn't.
It starts with only one entry, start.
In this order, the multiplication can't be done.
But let's kick our work up a notch.
Where things really start getting interesting with Markov analysis is when the Markov diagram
bends back on itself, making a cycle.
Inceptionally, this means that it's possible to revisit a state that you may have been in
before.
The restaurant table can return to ready.
Or being on hold this minute can turn transition to being on hold in the next minute, and it
usually does.
A patient discharged from intensive care may wind up there again before leaving the hospital.
A customer who bought something from direct marketing catalogs in the past may do so again.
Or may not.
And it's this last situation that was getting the German direct marketing company, Rennania,
into hot water.
It was losing customer base, losing market share, and losing profitability.
And it was essentially Markov analysis that helped it recover, and in fact, to become
the second largest direct mailing firm in Germany.
Let's shrink the problem down to manageable size without losing the essentials.
We'll have you run the direct marketing company.
You send out catalogs once per year.
I'm a new and promising customer.
You consider me to be in segment one.
If I don't buy in the first year, though, I slip to segment two.
If I don't buy during the second year either, I slip to segment three, and you send me a
reactivation package.
If I still don't buy, you write me off as not worth it and stop sending me catalogs.
But anytime I buy, I'm back in segment one.
Now, of course, you don't know whether I'm going to buy or not.
But you can get probabilities for my actions from historical relative frequency data.
For example, suppose that 30% of your segment one customers make an order when they receive
a new annual catalog.
Then you could say there's a 30% probability that a segment one customer reorders and remains
segment one.
Similarly, perhaps 1% of your customers are lost to death each year or disconnection.
So you'd probably take that as the probability of involuntarily losing a customer.
But we can put all of this into a Markov diagram that describes my possible future relations
with your company.
Like this.
This is quite a bit more interesting than the downed playing chart.
Again, we have two terminal nodes where things can wind up, this time lost and dropped.
In Markov analysis, we'd call these absorbing states.
Since once you get into one of them, you never get out.
But what goes on with the rest of the diagrams, quite a bit more complicated, the segment
one, two, and three nodes each have a chance of transitioning back to segment one.
It happens whenever a customer makes a purchase.
As you can see, when the annual catalog is received, there's a 30% chance of a purchase
from segment one, a 12% chance that a segment two customer will buy, and a 6% chance for
a segment three one.
Each one also has a 1% chance of transitioning to lost, since customers in any of the segments
are equally likely to die or become disconnected.
The remaining arrows capture the rest of the probability, and since the total probability
out of any one will have to equal one, it's easy to figure out.
Well, it's pretty clear that sooner or later, a customer is going to reach an absorbing
state that they'll go three annual catalogs in a row without ordering, or they'll become
lost.
But what's not clear is how long that's going to take.
In the plain example, by the time you performed two transitions, you were guaranteed to have
reached the end of the diagram, and the wreck was either found or it wasn't.
Not so here.
You can go round and round and round.
As a marketer, you're going to want to get some sense of how long you can expect to keep
me as a customer, as well as how many purchases you can expect me to make in that time.
And with Markov analysis, we can do just this.
Let me show you how.
First, turn the Markov diagram into a Markov matrix, like this.
It's common to call the Markov matrix M. Remember, the row indicates the state from which you're
transitioning from, and the column indicates the state that you are transitioning to.
Each row has its total probability adding to one, so the first row shows what happens
to a customer in segment one, a 30% chance of buying from the annual catalog, and so
staying in segment one, a 69% chance of not buying, and so sliding into segment two, and
a 1% chance of being lost.
Look at the last two rows, too.
They show that once you're dropped, you're dropped.
And that once you're lost, you're lost.
That is, if before the transition, you're in the dropped state, then after the transition,
you'll still be there, 100% probability, and so on, forever.
That's why it's called an absorbing state.
To find out what happens in two years, we just multiply the transition matrix by itself.
A year goes by, and then another year goes by.
M times M, by the rules of matrix multiplication.
You get this.
Again, look at the top row.
It says that a customer that is currently in segment one is 60% likely to be in segment
three, two years from now.
And to find out about three years from now, we multiply this new M squared matrix by M
to get M cubed, which looks like this.
That is, about 59% of the customers are gone by year three, with about 56% being dropped
for not buying catalogs from catalogs, and 3% lost.
We could continue this way year by year to watch how the top row of this matrix evolves.
To get the next year's matrix, we just multiply the previous year's matrix by M.
Here's a graphical representation of that evolution.
As you can see, almost no customers last 10 years in this model.
There's over a 95% chance of being dropped from the mailing list for failing to make
a purchase in three years.
There's a 4% chance of being lost, and there's less than a 1% chance of still being active.
There's another way of interpreting Markov transitions that's conceptually different,
but mathematically identical.
We could imagine that we're speaking not of one individual, but of a large population,
and the transition probabilities indicate the fraction of that population currently in
a given state that transitions to another state.
For example, rather than saying that a segment one customer has a 30% chance of making a
purchase, I could say that 30% of segment one customers will make a purchase.
From this perspective, our graph shows that in 10 years, we lose about 99% of our customers,
and 95% of our customers are dropped for not making an order in three consecutive years.
In the real world, Ranania was seeing just this kind of drop off on its customer base.
They followed the industry accepted model of only mailing to people whose recent purchases
more than covered the cost of selling to them.
The result was a smaller and smaller volume of profitable customers.
Of course, one way to address this is to acquire new customers, for example, from mailing lists
to replace the customers who were dropped or lost.
To some extent, Ranania did this.
We can modify our Markov diagram to replace all lost and dropped customers by changing
our lost and dropped states from absorbing states into ones that transition back to segment
one.
That is, each person who's lost or dropped this year is replaced in the following year
by a new segment one customer, like this.
The long-term behavior here is going to be very different from what we just saw.
In a system with one or more absorbing states, any non-absorbing state that can ever reach
an absorbing state is going to eventually be empty.
Such eventually empty states are called transient states.
Imagine a fountain with many basins draining into each other, maybe some pumps to send
water back up to the top, but which includes a basin from which no water escapes.
Then any basin that eventually feeds into this no-exit one will be empty in the long
run.
That was the case for dropped and lost in our original diagram, no exits.
But in our new diagram, there are no absorbing states.
In fact, we can say more.
Notice that following the arrows, you can take a tour of this Markov diagram that will
start in one, go through every single state in the system, and eventually return back
to one.
For example, we could go one to two to three to drop, then back to one, then to lost, then
back to one again.
A Markov system with this property is called irreducible.
In an irreducible system, no state ever runs completely dry.
The system also has a second important property.
Because of that loop at state one, we could take a trip from anywhere to anywhere in this
diagram in exactly 20 steps.
For example, suppose you wanted to go from lost to dropped in exactly 20 steps, then
start in lost, transition to state one, hang in state one using that little loop like you're
in a waiting room as long as necessary, and then at the appropriate time, leave it to
make your way from one to two to three to dropped on the 20th move.
There's nothing magic about the number 20 here.
As long as you can always get from anywhere to anywhere in some fixed number of transitions,
the system is called ergodic.
Ergodic systems will eventually stabilize into an equilibrium.
The equilibrium of an ergodic system is given by a string of numbers, a vector called a
steady state vector.
And knowing what you already know, you can let you find it.
A state vector in this problem, steady or not, is going to look like this.
And there's a probability for each of the five states of one, two, three, dropped and
lost.
These have to add to one as any row in any Markov matrix does.
And in equilibrium, how things look after the transition is going to be exactly the
same as how they look before the transition, steady state.
So v times m equals v.
Start with v and then do a transition with the Markov matrix.
And you're right back at the v where you started.
Using the rules of matrix multiplication, this equation is really just a shorthand for
five linear equations in five unknowns.
And that system can be solved to find the unique steady state vector.
It takes less than a minute to set this up and solve it on a spreadsheet using Solver.
Just enter those equations as the constraints of a linear program, pick any objective that
you want and solve it.
When you do, we get this.
This is the long run behavior that we'd expect to see.
At the end of any year we expect to see about 35% of our customers in segment one, 24% in
segment two, 21% in segment three, and about 20% being lost or dropped.
This 20% will have to be replaced by new customers for the upcoming year.
We can verify this calculation by starting with everyone in segment one, like this, and
then watching the evolution of this system by multiplying this vector by our Markov matrix
repeatedly.
The results look like this.
And you can see that we're converging to the long-term steady state behavior that our
equations predicted.
Segment one has about 35% and so on.
We can do a lot more with Markov matrices, especially those including absorbing states,
but I'll just hint at some of the possibilities here.
We can find the fundamental matrix, F, for our original one-customer model.
It tells you the expected number of transitions that the customer spends in a column state
before eventually being absorbed, given that they started in a row state.
Here it is.
So for example, look at the top row.
It says that a person who starts in segment one spends 1.72 years on average in segment
one.
Since a customer starts in segment one and returns there only when making a purchase,
we know that segment one customers on average make 0.72 purchases during their time with
you.
Similarly, a customer who's originally in segment one will spend an average of 1.19
years in segment two and 1.03 years in segment three for a total time of 3.94 years as your
customer.
So you send on average 3.94 catalogs and 1.03 reactivation packages and I give you, on
average, 0.72 purchases.
If catalogs cost you $1 each and activation packages are $3 each, that means that you're
spending about $7 on 0.72 sales or a little under $10 per sale made.
You better keep this in mind when you're working on your business model.
You can play fancier tricks with the F matrix to find what fraction of customers are eventually
dropped or eventually lost or even the average time that a new customer who's eventually
lost to death will spend as your customer.
The existence of matrix algebra makes Markov analysis a powerful set of tools.
The real-life mail order firm that I mentioned earlier, Ranania, learned this when they
used these techniques or one similar to ours to take a look at their mail order business.
The transition diagram of their three-segment customer based is very much like ours.
A similar although somewhat more sophisticated version of our analysis was used to fine-tune
their operations.
For example, they allowed the possibility of more than one mailing per year.
If they mailed monthly, for example, a customer would have 12 chances to buy before being
demoted to a segment 2 customer.
But their basic approach was the same as ours.
Use the transition probabilities to see how the system evolves through time.
Find what fraction of customers fall into each segment and use this information and other
costs in purchase information to determine how often mailing should be sent and to whom.
Here's a little spreadsheet that I cooked up to do just this.
You can see how as the number of catalogs per year increases, the size of the customer
based grows.
People who get more catalogs per year have more chances to buy and so are retained longer.
For a while this translates into increased profits.
The height of the green bar shows profits.
But eventually the additional sales for more catalogs is no longer enough to warrant more
catalogs.
The profit, the green bar on the right of the graph, declines.
In my model, using as many of the real life figures from Inania as I could get my hands
on, this happens with 26 catalogs a year.
One every two weeks.
That's just the answer that Inania found too.
They also found, by the way, that it was important to have the catalogs arrive on Saturday.
German mail is reliable enough that they could make this happen.
The turnaround for Inania as these changes were implemented was dramatic.
Before the implementation, Inania lost 30% of its customer base in four years.
It had experienced declining market share and declining profits.
Now the base started to grow.
The sales increased.
They managed to acquire two of their competitors and they moved from the fifth largest direct
marketing firm in Germany to the second largest.
Funny, the more you know about how chance works, the less you have to leave your fate
to chance.
