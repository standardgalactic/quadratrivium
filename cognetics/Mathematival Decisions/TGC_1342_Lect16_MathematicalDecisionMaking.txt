The past nine lectures have been built around the idea that one mathematical technique can
be applied to a lot of different kinds of problems.
Look at the variety of problems that we've approached by linear programming.
The reason that we've lumped all these programs together is because they all yield mathematically
to the same kind of analysis.
In a sense, once a problem becomes a linear program, it doesn't matter what the original
context of the problem was.
At least it doesn't until you find the answer and have to interpret it in practical terms.
Sometimes the intermediate steps between the formulation and an answer are sufficiently
obstruous that it's tough to keep track of any meaning along the way.
And that's bad, because almost anyone who's good at mathematics, whether it's applying
existing results to end techniques or discovering new ones, is guided by a powerful intuition.
Figuring out the best path to take, the path that gets you from the start to the finish
most efficiently, is a creative act that relies heavily on intuitive insight.
And that observation too has guided the presentation of the materials in this course.
We've been performing the nuts and bolts work needed to find the answer, yes, but we've
also been building an intuition about what the math is doing.
And the one thing that we've relied on again and again is how good our natural intuition
is when looking at patterns of objects in 2 or 3D space.
Using that strength can help us bridge the gap between real-world problems and pure mathematical
calculation.
We can use our intuition about visual space to guide our work on a myriad of problems
that from a real-world context seem completely different from one another.
Look, if you're solving an optimization problem with no auxiliary variables, that is, no equality
constraints, then you're stuck with one dimension for each decision variable contributing to
the problem.
And that means that if you want to visualize it, you're going to be visualizing a problem
with at most three variables, and that's pretty restrictive.
But a lot of aspects of what are going on generally can be seen in these dimensionally
low-level examples.
First linear programs.
One variable.
One dimension.
The problem lives in a line.
Constraints are points or everything on one side of a point.
The feasible region, if it doesn't go on forever in some direction, is a line segment.
The objective tries to go as far to the right or to the left as possible without leaving
the feasible region.
At the end of the feasible region, one constraint is binding.
Two variables.
Two dimensions.
The problem lives in a plane.
Constraints are lines or everything on one side of a line.
The feasible region, if it doesn't go on forever, is a convex polygon and its interior.
In any case, all of the sides are straight lines.
The objective tries to go as far as it can in some specified direction in the plane like
north northeast without leaving the feasible region.
An extreme point of the feasible region has two constraints meeting.
Three variables.
Three dimensions.
The problem lives in our normal three-dimensional space.
Constraints are planes or everything on one side of them.
The feasible region, if it doesn't go on forever, is a convex polyhedron with its interior,
a crystal with flat sides.
In any case, all of its sides are flat.
The objective tries to go as far as possible in some specified direction like that way
without leaving the feasible region.
An extreme point of the feasible region has three constraints that meet like the corner
of a room where a flat floor meets two flat walls.
These patterns hold for n dimensions as well, although very few people can see them.
In linear programs, the constraints are always flat, resulting in a feasible region with
flat sides and no dense, what's called a convex polytope.
As always, flat means that there are no kinks or bends or jumps.
The objective always tries to go as far in some specified direction as it can without
leaving the feasible region.
And at an extreme point, at a corner, n constraints will meet.
Anything by analogy can be dangerous, of course, but the analogy can guide an intuitive
guess and then you can use rigorous techniques to prove that the guess is correct.
And once you do, it's there for your future use.
For example, suppose that you have a linear program with six decision variables, but only
five constraints, all in equalities.
Then it can't have an optimal solution.
Any linear program with an optimal solution has an optimal solution at a corner.
That was the extreme point theorem of linear programming that we mentioned when we were
solving these things graphically.
And in six dimensions, any corner has six five-dimensional walls meeting.
I can't see it, but I can count.
Our program only has five constraints, five walls, so the feasible region doesn't have
any corners, and so it doesn't have any optimal points.
When reasoning about dimensions, intuition can lead you astray, too.
Imagine you lived in a plane like a tabletop, and you're examining what a loop of string
could look like.
You could simulate this by taking a loop of string, setting it down on the table, and
pushing it around, making sure that every point stays on the tabletop.
But you'd pretty quickly figure out that every loop looks pretty much like every other one,
like a deformed circle.
But take that string in three dimensions, and you can have the same circular loop or
a loop with a knot in it.
In fact, there are an unlimited number of different knots that you could make.
This is a new behavior for three dimensions.
Take that knot into four and higher dimensions, and any such knot that we tied can simply
be shaken out.
The loop of string again has only one configuration.
The point that's relevant to this course is twofold.
First, develop a reliable intuition whenever you can.
It'll greatly aid your understanding and ability to solve problems.
Second, be careful, because sometimes new possibilities open up in higher dimensions.
And with that in mind, we're ready to leave linear programming and to start exploring
a landscape that's much more wild and wooly, non-linear programming.
Because when you get rid of the requirement that everything's linear, a lot of stuff
changes.
To see why, let's see what's going on when there are only two decision variables.
The visual intuition for this is a landscape in the normal sense.
You can move around the country side as you please, north, south, east, west.
We'll use the third dimension, height, to represent the value of the objective function
for each point in the landscape.
So for a maximized problem, you'd want the highest point.
For a minimized problem, you'd want the lowest.
To fix ideas today, I'm going to assume that we're looking for the lowest point.
We've used this imagery before for linear programs.
The landscape there is like a tilted plane of glass, and the constraints run in straight
lines on the surface of that glass, and you don't need to be able to see the horizon
to be able to reach your lowest point.
You only need to see the ground beneath your feet.
When you do walk, you walk straight downhill, and when you hit a fence, you keep going downhill.
And when you reach a corner, if the new fence keeps going downhill, follow it.
When you can't follow it downhill anymore, stop.
You've reached the lowest point.
Instructions are simple and could follow.
But how about a non-linear program?
How do things change?
Well, again, let's keep two decision variables so that we can visualize our mathematical
landscape as a landscape in the traditional sense.
But now that landscape is much more interesting.
Rolling hills and valleys, maybe ravines, mesas, ridges, maybe even pits.
The topography is much more complex and made more so by the fact that the constraints can
be non-linear too.
That means that the fences that bound our zones of exploration, our feasible region,
can be curvy, or they can zigzag in and out.
But our goal is the same as it was before, to find the lowest point.
How much of a difference do these new possibilities really make?
Well, for a linear program, if the feasible region has a lowest point, it's going to be
on a fence line.
In fact, in such a region, you can always reach minimum altitude just by checking all
the corners and then picking the lowest one.
But for non-linear programs, that's not true anymore.
The fence could run along a ring of mountains surrounding a valley, and the lowest point
would be in the middle somewhere.
As a real-world example, your store might have 10 checkout lanes so that you can handle
your busiest times, open them all on a typical day, and you're paying a lot of employees
to stand at the registers.
Open only one, and you're incurring costs from lost sales and customer dissatisfaction.
Those are the two fences, and the sweet spot is somewhere in the middle.
Another difference in a non-linear world is something that might look like a lowest
point, but not be a lowest point.
It might only be the lowest point in your area, a local minimum, not the global minimum.
Want a real-world example?
Let's look at the great HD DVD and Blu-ray war, and how it might have turned out differently.
Purely for argument's sake, let's imagine that HD DVD is actually twice as good as
Blu-ray.
Some fraction of customer, the customer and consumer market buys the player for one format,
the rest buy the other.
Some fraction of the disc's manufacturer, the one type, the rest are the other.
Imagine we start off in a world where equal quantities of discs of each format are available.
Then the logical choice for each customer would be to buy HD DVD, we said it was twice
as good, and if everybody then ends up owning an HD DVD player, producers would want to
make only HD DVD, which would be fine with their customers.
So we'd end up in an HD DVD world, which is pretty much what we've got.
But imagine that by chance, we started in a world where the product was 90% Blu-ray.
The higher quality of HD DVD wouldn't compensate for the lousy selection of compatible discs,
so intelligent consumers would buy Blu-ray players.
And if consumers had Blu-ray players, the best choice for producers would be to abandon
the HD DVD and make only Blu-ray discs.
We end up in a Blu-ray world.
And it would stay that way.
Neither producers nor consumers would have a reason to change, even though the resulting
world is only half as good as the other alternative.
That's a local optimum, but not a global one.
Think about the ridiculous arrangement of keys on our keyboards, or the fact that in
America we still use inches and miles rather than the easier and more logical metric system.
You'll see that this idea isn't just abstract mathematics.
But this was a solution that was in a corner of the feasible region, all HD DVD players
and all HD DVD discs.
Let's focus on the other possibility when the lowest point is in the middle of a rolling
landscape somewhere.
How can we find it?
Well, one answer is magic.
Or so it must have seemed in the 1600s, when the approach was invented.
It's calculus.
Particularly the derivative.
The mechanics of differential calculus belong in a different course.
But here's the magic of it.
If you have a function that gives you the contours of the landscape, then its derivative
gives you the slope of that landscape at every point.
That is how tilted the land is and in what direction.
And we've already seen one way, whether that information is valuable, since it tells you
what direction you should go for downhill from where you are.
But there's a second way that it's useful too.
And one that you undoubtedly saw if you ever took a calculus.
It lets you narrow down where the lowest point could possibly be.
With the intuition that comes from our visualization, this becomes really clear.
Imagine that the landscape is rolling in nature, no ridges, cliffs and the like.
And again, no fences near where you are.
Then if you're at a lowest point, the ground beneath your feet is flat or level.
I don't mean flat, you're probably standing at the bottom of a bowl actually.
But you could imagine that a flat plane is buried under the ground and would reach the
surface at the one spot where you're standing and never rise above the ground anywhere in
your immediate vicinity.
That's called the tangent plane and where you're standing, it's dead level.
In calculus, the derivative, or to use more proper language, the gradient, tells you how
that plane is tilted or not tilted.
So when you're at the lowest point, that's going to be zero.
And that means that if you can calculate the derivative with calculus, and you can, you
can then use algebra to find out when the slope is zero, at least that's the hope.
Sometimes solving the derivative equals zero equations can be rather a bear.
But even if you do solve them, you're not out of the woods.
Yes, the ground will be flat in that if you're at the lowest point, you'll be flat if you're
at the lowest point and you're not up against the fence.
It will also be flat if you're only at a local minimum or, think about it, a local maximum
or a terrace point where the ground goes up, levels off, and then goes up some more, like
terrace farming.
While those are the only possibilities with one decision variable, which is what you generally
learn about in your calculus course, in a higher dimension you can end up with stranger
things like this.
It's a saddle shape.
The technical name is a hyperbolic paraboloid, and it happens to be one of my personal favorite
shapes.
In fact, this landscape is the landscape for our HDDVD blu-ray problem.
On the left, it's an all HDDVD world, which you can see as a local maximum.
On the right, it's an all blu-ray world, which is a global maximum.
But if you look at the place between them where the ground is level, that's the lowest
point on the path between the two maxima, but that point isn't a minimum.
Because if you traverse the landscape from front to back, instead of left to right, that
same point would look like a maximum.
The more you move toward the front or the back of this picture, the more the consumers
and producers are out of sync with one another in terms of their format choices.
In this direction, the mix is the best that you can do.
That point in the middle with zero grading is called a saddle point, for obvious reasons.
If you take the derivative of the objective, and that gives you the landscape, then even
if you can solve the resulting equations to find the points that are level, you still
have some work to do to figure out whether these points are minima or not, and whether
they're global minima or just local ones.
In higher dimensions, the math to throw out the possibilities can get a bit hairy.
Solve a family of simultaneous non-linear equations to find the eigenvalues of the Hessian matrix
of the partial second derivatives and hope it's non-singular.
That sort of thing.
And that only handles the case when the minimum is out in the middle somewhere.
If it's a long offense line, then the ground at the minimum doesn't have to be level.
In fact, it probably wouldn't be.
You can see that with our HDDVD problem.
It has two local maxima and two local minima, and the ground slope quite steeply at all
four of them.
And there's another problem.
We've been assuming rolling hills.
In mathematical terms, we've been assuming that the function's differentiable, that
the derivative exists.
We even need more than this, such as the function also having continuous second derivatives.
If the landscape includes cliffs, ridges, mesas, and the like, you can see where things can
go wrong.
At a sharp ridge, like a roof ridge, the derivative isn't defined.
It doesn't have a single tangent, so you won't find it by looking for where the derivative
is zero.
It's not defined at the edge of a cliff either.
As a simple example, imagine that we're making a product with a fixed demand.
Each unit we make and sell gives us the same profit, so we have a nice linear relationship
until we saturate the market.
Thereafter, each additional unit reduces profit by a fixed amount, since we paid to have the
unit made, but we can't sell it.
That's a linear decrease in profits beyond that point.
The optimal production is obviously where we just meet demand, but you'd never find
it by calculus.
That's a roof peak, and there's no derivative there.
So calculus isn't as useful for some practical problems as you might guess.
In contrast, the nonlinear programming approaches that we'll be developing will allow multiple
variables, they'll allow complicated boundary conditions, which usually don't appear into
a second calculus course, and nonlinear programming can allow the characterization of multivariate
critical points.
So how do we do problems where the landscape's not smooth and continuous?
Well, there are a number of approaches, and what you should use depends, not surprisingly,
on your landscape.
So let's start again with rolling hills and valleys.
That means that at any point, we can figure out what direction is downhill.
In mathematical terms, we can find the gradient of the objective.
So here's a strategy that can take advantage of computer power.
Start at a random point in the landscape.
Find out what direction is downhill at that point, just like you would for a linear program.
Then walk a little ways in that direction, then stop, and repeat the process.
Keep repeating it.
This is the basic idea behind a collection of techniques called steepest descent or gradient
methods.
Excel's GRG nonlinear method is an implementation of one of these.
The variants differ exactly in what downhill direction you pick, and how far you go before
you check again.
For our example, imagine that we have a landscape that looks like this in an aerial view.
Here, we're in the south end of the landscape, and we're in the south end of the landscape.
Here, we're in the southwest corner of an oddly shaped field, and darker colors indicate
lower altitude.
Straight down hill from where we stand is to the east, but you can see that we couldn't
go very far that way before hitting a wall.
So an algorithm like Excel might choose a direction which has more anticipated room
for improvement, maybe something like this.
Of course, if the ground goes back uphill on the other side of the field, then that might
not have been a great choice.
So there's some finesse in deciding what direction you should go and how far you should
go before you recheck, but that's the basic idea of gradient descent.
Can such a technique ever fail to find the lowest point in a rolling landscape?
Well, sure.
If it hits the bottom of a little dip valley or follows a slope downhill to a point along
a fence, then it's just going to stay there, even though there might be much lower points
somewhere else, the old local minimum problem.
And that is a problem for any gradient descent method.
One way to try to avoid this is to begin at many points rather than one.
It's kind of like having a Boy Scout troop out with you.
Each scout starts in a different spot.
When you're done, you all compare notes and you see who did the best.
The more starting points, the better your chances.
Of course, you can't imagine a landscape like one of those annoying holes in miniature golf
where the hole is surrounded by a volcano.
The deepest point on the hole may well be at the bottom of the hole itself, but how likely
is it that you're going to find it with the steepest descent approach?
Every time that you check your bearings when you're standing on the volcano, you're going
to end up going exactly the wrong direction.
It's good that this kind of thing doesn't happen too often in mathematical landscapes
of real-life problems, because in general, there's no general algorithm that's guaranteed
to find the global optimum in any reasonable amount of time.
But things often work well.
And there are large classes of problems where the gradient descent approach is guaranteed
to work.
A key issue is the idea of convexity.
Now, this lecture is about developing an intuition on these problems, so I'm not going to get
bogged down with a formal definition of convexity.
But here's the heart of it.
First, convexity of the feasible region.
Imagine that the fences defining our feasible region are opaque walls.
You and I are standing at points inside the region.
Then the region is convex.
If no matter where we stand, we can always look at each other in the eye.
No walls in the way.
So a square region is convex, but a star-shaped region is not.
There's no hiding from one another in a room with a square floor plan.
But see those two little red dots in my star-shaped room?
If you were standing at one of them, and I was standing at the other, we couldn't see
each other unless we had x-ray vision.
We can also talk about the convexity of a function in our intuitive terms, a landscape.
It's a closely related idea.
Again, informally, the landscape is convexed.
If no matter where you and I stand on it, we can still make eye contact.
So if we're standing inside of a bowl-like depression, no problem.
If we're standing around a mountain, or in the vicinity of a mountain pass, then there
could be places where we could hide from each other's view.
The function giving the landscape is not convex in those cases.
Why does convexity matter in finding optimal solutions?
Well, go back to you and me in our star-shaped room for a minute.
And imagine the goal was to move as far south in this room as possible.
South is down in my picture.
Again, look at the two red dots representing you and me.
If we were following gradient descent, neither of us could move any further south, could
we?
We'd both be at local minima.
But only the person on the right would be at the actual global minimum.
So even if the objective is linear, a non-convex feasible region can cause problems.
But how about if we have a convex feasible region and a convex objective 2?
Your intuition may be telling you that now you're going to find the global minimum, and
you'd be right.
Usually, you do so quite quickly.
And that's good news because a lot of important problems fall under this category.
I'll give you some simple rules for identifying some functions as convex without doing any
work in my next lecture, but let me mention just one today.
Summs of squares, of linear expressions in the decision variables, sums of squares, are
always convex.
And that means that variance is convex.
And risk is often measured in terms of variance.
So the objective of minimizing risk is usually a pretty straightforward one to handle.
But not everything behaves.
We've been dealing with smooth landscapes where derivatives always exist, but the absolute
value function, which we used in computing MAD and MAPE, for example, has a graph shaped
like a V. At the point of the V, there's no derivative.
And even worse, how about a cliff or a mesa?
Mathematically, now we're talking about a function that is discontinuous.
It has a jump.
Again, at the edge of the cliff, the derivative is not defined.
These come up a lot in real life, too.
For example, the post office charges the same amount of money for heavier and heavier packages
until a certain weight is reached, and then wham.
They jump to a new price.
Most if conditions will also do this, as in, the cost is $4 per hour, but if you use ten
or more hours, the cost is $360 per hour.
Again, we have a jump discontinuity, a cliff.
Ten hours costs less than nine hours in one minute.
If you use your topographic intuition, you can see the troubles that you can have.
If you're at the top of a flat mesa, the ground around you looks level, so you have no reason
to go in any given direction.
If you decide to move to a new position, near your current one, the ground stays flat.
In fact, it may even stay flat at the new positions at the bottom of the cliff.
So you went from one height to another and never went downhill.
And remember, the more dimensions in your landscape, the more possible directions you
could choose to go.
Imagine for a moment finding a treasure in a series of rooms where each room has only
two doors.
Not too bad.
You go in one, you go out the other.
Now imagine that you had to do this with each room having a thousand doors.
Well, there are ways to handle even this kind of problem.
And the good news for us is that many of them have been implemented in computer software
that you can run on a PC.
One family of techniques is referred to as genetic algorithms.
They're really neat.
They're also called evolutionary algorithms.
They take their paradigm from evolution and biology.
I'll be honest.
I'm still rather amazed that they work and work as well as they do.
Let's see if I can give you a feel for them.
Some of the harder problems that we'll be looking at are going to need them.
In fact, if you write a program in Excel that uses many of its built-in functions like if
or choose or vlookup or the like, your program is probably going to need a genetic algorithm.
To get started, we have an initial population of possible solutions, maybe 500 of them,
that are usually randomly generated.
Like other optimization techniques that we've been discussing, genetic algorithms are stochastic.
They involve randomness.
Among other things, that means that if you apply genetic algorithms to the same problem
twice, you probably won't get the same answer.
As an example, suppose that my problem involves three variables, x, y, and z.
Then each solution would have the values of x, y, and z, of course.
But here's the new twist.
We'd represent them as coded into binary, strings of ones and zeros.
This echoes a strand of DNA which is built of a sequence of four possible nucleotides.
We only have two, but that's enough.
Let's say that each variable is represented by a 16-digit string of ones and zeros.
Now we have the possibility of any pair of solutions breeding.
They do this by a process similar to recombination of genes, the procedure is called crossover.
In a genetic algorithm, we cut each 16-digit sequence in a particular location.
Here we cut the x-sequence after the fifth digit, the y-sequence after the twelfth digit,
and the z-sequence after the tenth digit.
These cut points are chosen randomly for each pair.
Now we're going to make two new x-values from the original ones.
Let's see what that would look like.
With these two x-values and a crossover point being after the fifth digit, first cut the
sequence into two after the fifth digit, and make two new x-values by doing a swap.
Make the head of the first old x-value and stitch it to the tail of the second old x-value.
That gives you one new x.
And make another by taking the head of the second old x-value and stitching it onto the
tail of the first old x-value, like this.
We'll cross-breed the y's and z's in the same way, and so from the original two solutions
we've created, a pair of children.
We're going to evaluate the parent solutions and their children by seeing whether they
satisfy all of the constraints, and seeing how well they do in the objective.
These factors are used to determine how fit each potential solution is.
We're going to pair down these candidates into a new population of the same size as
the original pool of solutions.
The more fit a solution is, the more likely it is to breed and appear in this new pool.
One way this can be done is something like a spinner in an old children's game, one
that you might use to decide how many squares to move.
Each candidate gets a wedge on the spinner, but a candidate that's twice as fit gets
twice as big a wedge.
Here's what it might look like with four candidate solutions, where the sea green candidate
was the most fit, and the dark blue candidate was the least fit.
To decide who gets to fill each available slot in the new generation, spin the spinner.
Whichever solution it stops on gets that slot.
So the new population might contain multiple copies of a really good solution.
One problem with doing this is the same as the problem seen in modern ecology, loss of
biodiversity.
In this culling process, solutions that would have been great with a little tweaking might
get lost, and surviving solutions might fixate on a local minimum.
To prevent this, each member in each generation has a small chance of mutation, that is, of
one of its digits randomly changing from a zero to a one, or from a one to a zero.
Mathematically, this can teleport a solution completely across the solution of the landscape
to a new position.
The vast majority of the time, the new solution will be less fit than the rest of the population
and will die out.
But every once in a while, we'll stumble on a better solution, quite different from the
other ones being considered.
And then the search can proceed in this new area as well.
Because of the random aspect of the solution procedure, the technique will eventually
find the optimal solution eventually.
But the bad news is it could take a very, very, very long time to do so.
I've had small but hard problems run for hours before their solutions were found, and large
hard problems could be very time consuming indeed.
In theory, you could have problems that would take longer than the life of the universe so
far to solve.
Fingers crossed on that one.
And of course, genetic algorithms create a new generation of solutions by comparing fitness
to those solutions that it's already found.
There isn't really any sure way of telling when you should stop the thing, because there's
nothing that tells you when the optimum really has been found.
From a practical standpoint, one usually lets it run until either no appreciable improvement
has been seen over a specified number of generations, or until you just can't stand it anymore.
And the stopping point isn't the only thing you have to decide.
How big a population do you want?
What mutation rate do you decide on?
And so on.
Your software will use default values, but you might get better performance if you tweak
them.
But if you need genetic algorithms, you're probably going to be willing to put up with
these shortcomings because they can handle functions that none of our other techniques
can deal with very well.
They pretty much don't care what the functions are.
Even something weird like, how many times does this digit appear in the answer?
You can make a set of variables that are all integers from 1 to 10, insist that each number
be used only once, and make a rank ordering.
Evolutionary solvers don't care.
But you could believe from the way I described the fitness function that they hate any of
the most simple constraints.
They like to have an upper bound and a lower bound on each variable like x less than or
equal to 10 and greater than or equal to 0.
But anything more complicated usually slows them dramatically.
We avoid this unpleasantness by using soft constraints and penalties like the ones we
used in the last lecture.
They're easier to write for an evolutionary solver since you can use things like if statements.
Well that's an overview of the nonlinear wilderness, the lay of the land so to speak.
Now you have an idea of how nonlinear techniques work and why we use multiple techniques.
And what to watch out for in a nonlinear world.
In the next lecture we'll be getting more specific about exactly how we do all of this.
We'll gather together our equipment, square our shoulders, and set out over a landscape
defined by a variety of practical examples.
And because it's operations research we'll have a better reason for reaching that peak
than the mountaineer's explanation because it's there.
