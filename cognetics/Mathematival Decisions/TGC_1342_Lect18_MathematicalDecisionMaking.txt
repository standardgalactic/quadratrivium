Randomness, uncertainty, it's a part of life and in this lecture randomness takes center
stage where it will remain with us to the end.
So let's start by refining our intuitions about randomness, drawing on ideas from probability.
We often have a pretty good intuition about random events that we deal with all the time,
but when it comes to things that are unfamiliar or that occur only rarely, people often are
terrible at estimating the probabilities of events, or what average results they might
expect if facing the same situation many times.
Take a roulette table.
When a streak of five or six black numbers shows up, a number of people will scramble
to bet on black, which they believe to be hot, counterbalanced by a second crowd of people
that will scramble to bet on red, based on the fact that they believe it is overdue.
These opposite conclusions are sparked by a common belief that the streak that just occurred
is unnaturally long, too long to be random chance.
Well, to a pretty good approximation you can figure out how long the streak you'd expect
in a certain number of spins by a literal rule of thumb.
I'll show you.
First, specify how many spins you're talking about.
If the wheel spins every two minutes over an eight hour day, that's 240 spins.
Now put up your thumb and say two.
Then double the number that you say every time you raise another finger.
So four, eight, sixteen, thirty-two, sixty-four, one, twenty-eight, two, fifty-six.
Stop when you get close to your number of spins.
The average length of the longest streak is quite close to the number of digits that
you just had raised.
So in just eight spins, I'd expect a run of about three in a row, two, four, eight.
In the case of 240, we'd expect a streak of about eight reds or eight blacks sometime
during the day by simple random chance.
We'd expect many more shorter streaks, too, of course.
But these streaks that are sending gamblers to the table aren't surprising at all.
In fact, it would be surprising if they didn't occur.
A similar analysis applies to an athlete on a hot streak.
In almost every case, there's no evidence that the streak is anything more than random
chance.
Suppose you put 36 strangers in a room.
How likely is it that any two of them have the same birthday?
Well, you might figure, since there are 365 days in the year, the answer is about one in
ten for 36 people.
It's actually over 83%.
You can ignore the calculations or work through them, but notice the graph.
With only 57 people, the probability of a shared birthday rises to over 99%.
Here's another one.
Take those same 36 people, take their keys, scramble them up, and then redistribute them
randomly.
How likely is it that someone got their own keys back?
And if you played this game with more people, would it be more or less likely that at least
one person gets his or her own keys?
Well, the answer does depend, not surprisingly, on the number of people, but that dependence
quickly becomes incredibly weak.
Once you get to about five people, the probability is almost exactly one minus one over E. That's
the same E that we used in our transformation and variables work.
That's a bit over 63%.
And that figure is right if you're returning keys to five people, or 36 people, or 5,000.
We can check the answer by brute force for five people.
120 possible orders for key returns, and in 44 of them, shown in red, no one gets his
or her own keys.
In the remaining 76 cases out of 120, someone does, 76 out of 120 is 63.3%, as we said.
These arrangements in which no number of stays in its own place are called derangements,
and the formula that gives their probability is very beautiful and not obvious.
The brute force verification for only five people was already quite enough.
I had to look at 120 orders.
For 36 people, it would be over 10,000 trillion, trillion, trillion.
Another example, suppose you pick one of these people, and they tell you that they have two
siblings.
You ask if they have a brother, and they say yes.
How likely is it that they have a sister, too?
If you think the answer is one-half, guess again.
It's two-thirds.
Here's the math for that one, using tools that we'll discuss today.
You're more likely to be killed by a toaster than a shark.
In fact, more people are killed by collapsing sand castles than by sharks.
And sometimes an impressive sounding statistic tells you almost nothing at all.
50% of doctors graduated in the bottom half of their class.
100% of their patients will die within six months of their birthdays.
It can be surprising, a lot of statements like that can upset people.
Well, these weaknesses in probabilistic thinking aren't just so much cocktail conversation.
They can undermine predictions and decision-making of all sorts.
And occasionally, people who understand probability can use that to their advantage.
One way that auditors catch people who are cooking the books is by looking at the relative
frequencies of the leading digits and the numbers that they report.
People who make up the numbers usually think that each digit is equally likely to start
a number, but real data spread over several orders of magnitude tends to follow something
called Benford's Law.
Amazingly, about 30% of those numbers tend to start with the single digit 1, while numbers
starting with 8 or 9 appear only about 5% of the time each.
Smaller first digits are more likely than bigger first digits.
When an auditor sees a big deviation from that law, they take a closer look.
And taking a closer look is just what we'd better do, too, if we don't want to be blindsided
by randomness.
In regression in time series and data mining, we dealt with some randomness, but it was
confined to the characterization of error.
Do a statistical analysis of the variation and use it to construct confidence intervals
on how much you can trust the prediction.
But in many situations, random events play a much more central role in how things will
turn out.
World War II veterans, like my father, will never forget the date of June 6, 1944, D-Day,
the invasion of Normandy, and the beginning of the liberation of France by the Allies.
But D-Day was tentatively scheduled for June 5.
On June 4, General Eisenhower pushed it back a day due to uncertainties in weather about
the 5th.
The weather predicted for the 6th was better, but not good.
But the next window for a desirable tide was about two weeks later, and during that window,
history now shows that a major storm would have made landing impossible.
Turning on the role of a meteorological die.
More prosaically, when banks issue loans, the successor failure can hinge on how accurately
the banks can assess the risk of not being paid back, and how well they can make decisions
in light of that risk.
Most of our work in optimization has been deterministic, so randomness hasn't played
a role beyond the controlled deviations that we explored with sensitivity analysis.
But if we want to go further with optimization, to optimize in the face of uncertainty, then
that's going to have to change, too.
Well, these are the kinds of things that the next section of the course is about, analyzing
and evaluating situations in which we have only partial control, public relations problems,
research opportunities, waiting lines, market share, homeland security, bidding on jobs,
and we can't do any of this until we get a handle on uncertainty, on randomness, on balancing
risk and reward.
And that, in turn, means getting a handle on three things.
Basic probability, worst case to best case scenarios, and what we could reasonably expect
to have happen if we face the same situation many times.
That's what I want to focus on now, along and along the way, I want to give you a clearer
sense of why we need these tools and why intuition is not enough.
Probability.
It's a measure of the chance that an event occurs.
Events that never occur get a probability of zero, events that always occur get a probability
of one, or 100%, and everything else fits inside that range, with more likely events
getting higher probabilities.
Going further than that, isn't as easy as you might think.
The first people we know about to seriously think about probability were Pierre de Thermont
and Blaise Pascal in the 1600s.
Both were prodigious mathematical thinkers, both were very interested in gambling, and
in their letters to one another, they talked about this, and that's generally taken as
the beginning of the study of probability.
Probability theory today is a formal discipline of remarkable complexity and depth, but you
can go pretty far with just a good grip on the basics.
Let's consider the very simplest example.
Flipping a coin.
50% chance of coming up heads, yep.
Yep.
The probability of getting a head is 0.5, but that doesn't really mean that it comes
up heads half the time, does it?
I mean look, if I flip a coin twice, I won't be surprised if they both come up the same,
two heads or two tails.
In fact, half of the time, in two flips, you will get two of the same.
So where does this probability of 0.5 head really come from?
It's the classical probability of the event.
With a coin, there are two equally likely outcomes, heads and tails.
Heads is one of those two, probably of one in two, or one half.
If you draw a card from a deck of 52 cards, and 13 of those of 52 are spades, then the
chance of drawing a spade is 13 out of 52, 1350 seconds, or one fourth.
That's the whole deal with so-called classical probability.
If all the outcomes are equally likely, then divide the number of successful outcomes by
the total number of possible outcomes, and you have your probability of success.
Except that in a lot of real-world situations, this definition is practically useless.
You often can't break down all of the possible outcomes into a collection of equally likely
possibilities.
And when you can't, we usually adopt a frequentist approach, finding the empirical probability
based on historical data and relative frequencies.
It's the same idea as a batting average.
If a player has been a bat 400 times, and has gotten a hit in 100 of those 400 attempts,
his batting average is 250.250.
Barring other information would take the probability of this player getting a hit when he steps
up next time as one fourth.
There can be problems with this approach, of course.
If one can conduct identical experiments in a large number of cases, fantastic.
But in our baseball player example, how about the effect of professional improvement over
time, or aging?
Still, empirical probabilities are often the best that we have, and they're often accurate
enough to help us make good decisions.
Unfortunately, sometimes we can't even get empirical probabilities.
For example, what's the probability that AIDS will be cured in the next 10 years?
Obviously, we can't use either classical or empirical probability for a question like
this.
We can't look at how many times AIDS was cured in previous decades.
And it may not really be sensible to use data for the cure rates of other diseases in the
prediction and eradication of AIDS.
Here the best we have is subjective probability, a hopefully educated guess.
How reliable such a probability is depends on how much you can trust the person making
the estimate.
But whatever our probabilities come from, wherever they are, there are three things
that you can do with them once you have them.
First, we can characterize the variation of important quantities in the problem.
How much on average will this risky investment pay?
What's the worst that could happen?
What's the chance of losing money?
Second, we can use probabilities to simulate situations.
This lets us get a grip on the random factors in the situation and how they influence the
final outcome of events.
For example, what happens to traffic flow when we open up this new lane on a highway?
And underlying both of these uses is the third use.
Probabilities of fundamental events can be used to figure out probabilities of more complicated
compound events.
For example, how likely is it that Bank A or Bank B or both approve our loan?
If we get the loan amount, how likely is it that our bid is accepted and that we can pay
off the loan within five years?
There are rules in probability that let us figure out the probabilities of combinations
like this and we're going to cover just a few, the ones that we need in our later lectures.
The most important one is joint probability, joint in the sense of joint checking account.
Joint probability is the probability that happens, that this happens and that happens.
More generally, we'd like to get an idea of how the probability of a whole collection
of events occur.
The answer is actually pretty simple.
The word and in probability always goes with the operation of multiplication.
To get the formula, I always think of a person trying to make it through an obstacle course.
Every obstacle was an event from our collection and making it through the course means clearing
all of the obstacles.
So line up the events in any order you like and run the course.
In order to complete it, first you have to clear obstacle A. The probability of doing
that is just P of A, which we write on as probability, that's how we write probability
of A. No surprises.
Now you go on to event B and it's natural to think that the chance of clearing the second
obstacle is just P of E, B, the probability of B. But that's overlooking an important
piece of information because by the time that you get to this point in the course, we know
something about you.
We know that you cleared obstacle A. So what we really want at this point isn't just the
probability that a random someone can clear obstacle B.
It's the probability that someone who cleared A will also be able to handle B. From my little
runner after A, B looks like pretty easy sailing.
And so we go on this way.
The probability that we need for the third obstacle was the chance of clearing C given
that you already cleared A and B. Get it?
Once more, the word and in probability calculations always goes with the operation of multiplication.
So if we want the probability that all the events occur, we just take this whole string
of probabilities and multiply them together.
That's it.
That's joint probability.
Let me give you an example.
And given my obstacle course metaphor, I'll choose one from sports.
In 2007, three English teams made it to the quarterfinals in the European Champions League
in soccer.
Three English teams, five non-English teams.
The pairing for teams in the quarterfinals was going to be decided by a random draw,
but there was a rumor flying around that the draw was going to be rigged so that no
English team played another one in the quarterfinals.
If that happened, it would obviously be good for England.
And sure enough, when the pairings were made, no English team was put it against another.
Does this suggest some sort of hanky-panky?
Let's work it out.
The English teams were Chelsea, Liverpool, and Manchester United, which I'm showing
as C, L, and M.
Clearly for convenience, I'll mark the five non-English teams as E for European, even
though the English teams are clearly Europeans in this competition too.
I'd mark them C for continental, but Chelsea already has dibs on that.
So to say that no English team plays against another means that Chelsea gets a European
opponent, and Liverpool gets a European opponent, and Manchester gets a European opponent.
And the word and, as we said, always means times and probability.
So let's run our obstacle course.
First, Chelsea has to get a European opponent.
Well, Chelsea could get any of the seven opponents in the first round, and five of those seven
opponents are European.
So the chance of Chelsea avoiding an English opponent is five out of seven, or five-sevenths.
That was easy.
Now, stage two.
We've cleared the first hurdle, and Chelsea is playing a European team.
Cross out Chelsea and its opponent.
Our picture now looks like this.
Six teams, four of which are European.
OK, now the second hurdle.
Liverpool has to get a European opponent.
Well, there are five remaining teams that Liverpool could play, and four of them are
European.
So the chance that Liverpool gets a European opponent, given that Chelsea got one, is four
out of five, or four-fifths.
The chance that Chelsea and Liverpool both got European opponents is the product of these.
Five over seven times four over five is four-sevenths.
OK, last hurdle.
Manchester United.
Given all that we've already said, Chelsea and Liverpool have European opponents.
Cross them out.
I really don't know which European team to cross out, but it doesn't matter for this
argument, so I'll just pick one.
There are only three teams left that could play Manchester United, and all of them are
European.
So given that Chelsea and Liverpool have continental opponents, it's guaranteed that Manchester
does too.
The probability is one.
So the chance of the whole sequence occurring, that is no English team facing another one
in the quarterfinals, is five-sevenths times four-fifths times one, four-sevenths, which
is about 57%.
So in fact, the fact that no English team ended up facing one another didn't imply foul
play.
It's actually the most likely outcome of a fair draw.
The talk of the fix was fed by mistaken ideas about probabilities.
On the other hand, this idea of the probability of A being true, given that B is true, is
terribly important in a lot of computations.
So it's given a special name.
It's called conditional probability.
In this case, the probability of A given B. The word given is represented in symbols
as a vertical bar, like this.
So what we just said in symbols is, and this kind of formula always works.
In fact, it gets even easier when the events happen to be independent.
A collection of events is independent if knowing whether some of them are true doesn't
change the probabilities that others are true.
For example, this is always the case when the events have no connection at all with
one another.
If we each pick a person randomly, whether my person is married is independent of whether
your person is of legal age.
That is, knowing my choice's marital status neither increases nor decreases the chance
that your arbitrarily chosen person is a minor.
On the other hand, if we agreed to choose the same person, the marital status and whether
or not a person is a minor would be dependent events.
Miners are less likely to be married than those of legal age.
Any time you're looking at joint probability, you always want to ask yourself if the events
are independent.
If they are, it's a piece of cake to figure out the probability that all of the events
happen.
Just find the probability of each event individually, then multiply all of those together, like
this.
This is part of the idea behind building backups for vital systems lies in a spacecraft.
A vital component may only have a 1 in 100 chance of failing, not because of a design
flaw, but just from mischance.
That's small, but maybe you wouldn't like to risk your life on its continued operation.
On the other hand, if we have two such units, then you're only in trouble if both of them
fail.
The failure of one is independent from the failure of the other, so the chance that they
both fail is 1 in 100 times 1 in 100, or 1 in 10,000.
Add another backup and you're down to 1 in a million, and so on.
Do we really need to understand this?
I've been suggesting that if you ask a person, even a bright person, to make decisions in
a situation in which they have only limited probabilistic ideas of what may happen, that
their proposals are often questionable and reflect fundamental gaps in their understanding
of chance.
Well, let me give you an example and see how you would handle it.
I'm going to choose a topic that doesn't require any special expertise to keep the playing
field level.
Take a look at my lineup of 9 randomly chosen Americans.
Now, according to a study of the American Pet Products Association, about 1 in 3 Americans
have at least one cat.
Let's take that figure as being correct.
I'm asking you to divide these people into two groups, those that you think own a cat
and those that you think don't.
Thumbs up or thumbs down on cat ownership for each person.
It's my new game show, Cat or No Cat.
Each one of these people that you get right, you'll get $100, $0 if you guessed them wrong.
Get them all right and you'll earn a big $900.
So make your guesses as to who's what.
Your goal is to make as much money as you can.
If you want, pause the video for a second if you need more time.
The result should be educational.
Okay, I'm going to show you the correct answers of which people in this randomly selected
group of Americans are cat owners.
You ready?
Here they are.
Give yourself $100 for each person you got right, a cat owner that you said owned a cat
or a non-owner that you said didn't.
If you got six or more right earning at least 600 bucks, you're in a very small minority.
But if you played this game according to probability, you should have made even more than that.
Let's walk it through.
Imagine for starters, and I think that this is actually true, that people's faces don't
really tell you much information about their cat ownership.
We will revisit this point later, but let's assume it's true for the original analysis.
Whether you ignored faces or not, you may have picked three people as cat owners and
six as non-cat owners, since cat owners make up about a third of the population.
That's the most common choice, and in the past I've had more than a few people challenge
me indignantly on the fact that only two of our photos are cat owners.
They're one third of the population, so why aren't there three?
Funny question.
Remember how obvious it was that with a coin, a 50% chance of flipping heads might flip
the same way twice in a row?
Given the size of the population, whether any one of these nine people is a cat owner
has virtually nothing to do with the cat ownership of any other.
They're independent, just like coin flips.
So for whatever reason, suppose you chose three cat owners and six as non-cat owners.
How likely is it that you get them all right?
That's a joint probability.
And we just got done saying that the events are independent.
That means you can just multiply together the probabilities of the nine individual events
that you're looking for.
Each time you said a person had a cat, you had a one third chance of being right.
Each time you said that a person didn't have a cat, you had a two thirds chance of being
right.
So, if you guessed three cat owners and six non-cat owners and weren't able to sharpen
your guesses by looking at the pictures, your chances of getting them all right is the product
of three one thirds and six two thirds, which comes out to be about one third of one percent,
bit worse than one in three hundred.
On the other hand, if you guessed that there were no cat owners in this sample, you're
probably getting them all correct is about two, it actually is two thirds to the ninth,
which is 2.6 percent, about one in 38.
Still long odds, but much better than one in three hundred.
In fact, the more people that you guessed as cat owners, the less likely you were to
get everyone right.
Of course, that's assuming that you're not learning much from the pictures.
Maybe you think that you could read a face pretty well.
Obviously, if your inspection of a face results in more than a 50 percent probability that
the person's a cat owner, then guessing cat owner for that person makes sense.
But how much can you trust your subjective probability?
There's a well-established psychological bias, you see, the overconfidence effect.
People consistently rate their subjective confidence in their answers higher than their
objective accuracy.
In the spelling test, for example, respondents correctly spelled only 80 percent of the words
which they said they were 100 percent certain about.
And the pattern continues for all levels of confidence.
When people claimed to be 80 percent certain, they were right far less than 80 percent of
the time.
A 1981 study showed that 93 percent of American drivers rated themselves as being better than
the median.
By definition, the median is the halfway point in the population, so only 50 percent of the
drivers are better than half of all other drivers.
Interestingly, a number of studies have shown that in a situation like our cat game, it's
common for people to completely ignore the provided information on how frequently the
target group appears in the population and instead to base their guesses on details of
people that have almost no predictive power for the question at hand.
Given any concrete information, people often seem to disregard the relevant probabilities.
It's called ignoring the base rate, and it leads people to believe that a short slender
Englishman is more likely to be a jockey than a welder, even though there are over 100 times
as many welders in the population as jockeys.
It's a tempting error, isn't it?
We'll come back and look at this kind of thing more closely when we discuss Bayesian probability
because the error in reasoning is terrifically common.
Yes, it's true that if you are a jockey, your chances of being slender are very high, but
that does not mean that if you're slender, your chances of being a jockey are very high.
Every great pianist has two hands, but that doesn't mean that a person like me with two
hands is likely to be a great pianist, does it?
Since we're often dealing with chance, we'd better figure out how to judge whether one
strategy really is better than another because a strategy that would work out brilliantly
under one set of circumstances may be the worst choice for another.
In our cat game, I assess the quality of a strategy in terms of how likely it was to
correctly identify the cat ownership of all nine people.
This one, and you might have, well, missed them all, but in a lot of circumstances, that's
not right.
A more important question may be, how well does the strategy work on average?
A bank making loans is going to sometimes reject a person who would have paid up, and it will
sometimes accept a person who defaults, but they're interested in their long-term average.
The average payoff is also called the mean payoff.
When dealing with matters involving chance, the mathematician is usually called up the
expected value, brought in name for everyone else really, since it isn't the value that
anyone would actually expect.
If 10 raffle tickets for a $50 prize are sold to 10 people, each person expects to win nothing,
but the average winnings is $5.
Ten people played, and the total winnings were $50, so the average winnings per person
was $50 over $10.
And that's what we call the expected value, even though it's impossible to win $5.
But once you get past the possible confusions about the name, computing expected values,
averages, in many cases, is pretty straightforward.
Suppose you have a quantity that can only take on a finite number of values.
You know these values, and the fraction of the time that each one occurs.
That's the probability.
Then to find the expected value, you take each possible value, multiply it by the probability
that it occurs, and then add all these products up.
That's it.
Let's look at an example.
I can make a $50,000 year, $50,000 year-long investment in a company doing genetic research
up by a particular disease.
I've estimated, and obviously this is a subjective probability, that there's a 10% chance that
they'll succeed this year, in which case I'll be paid $500,000.
There's also a 50% chance that they'll go bankrupt, in which case I lose my $50,000
investment entirely.
The rest of the time, I just break even, getting my $50,000 back.
In terms of expected payoff, is this investment attractive?
Well, let's find the expected value.
First, list all possible payoffs, taking into account the $50,000 that I had to invest.
Then the probabilities of each of these, multiply each payoff by its probability, and finally
add all the results.
Expected value, $20,000.
On a $50,000 investment, $20,000 is a 40% return, not too shabby for one year.
What this means is if you had the opportunity to make this investment again and again and
again, then on average, it would return 40% at the end of the year.
But of course, there's also risk.
Half the time, you lose your $50,000 outright.
So even though there's a reasonable 10% chance of exceptional gain, there's a much larger
50% chance of total loss.
This variation in the outcome is the hallmark of a risky investment.
Great variability in the possible payoffs.
Yes, you can make a bundle, you can also lose your shirt.
If it were possible to repeat this investment many times, this might not matter.
In the long run, the average should be trustworthy.
It's a result from statistics called the law of large numbers.
But if we're talking about a one-shot or an opportunity that occurs only very few times,
then that variability is going to be a serious concern.
Some of the techniques I'm going to be showing you in the next few lectures focus on expected
value, the average payoffs.
But even when we're looking at them, it's important to keep in mind that unless a situation
is being faced many, many times, knowing the average is never enough.
The average temperature on the surface of the moon?
It's about 5 degrees Fahrenheit, decidedly nippy, but tolerable in winter clothes.
But that 5 degrees average comes about by averaging temperatures from positive 253 degrees in
daytime with negative 243 degrees at night.
So we'll need to keep an eye on variability too, and we'll do so increasingly as we move
through the upcoming lectures.
Still, the average is an important place to start, and happily expected values show a
lot of useful and important mathematical properties that will often let us find them easily.
One such property is that they behave very well under addition.
Take any two activities, A and B, and average the results from doing both A and B. That's
going to be the same as the average from doing A plus the average from doing B, regardless
of whether A and B are independent or intimately connected.
This isn't nearly as obvious as it might at first sound.
For example, suppose that a person's paid $10 for correctly calling a coin flip, but
gets nothing for calling it incorrectly.
Then the expected payoff for that person is $5, 0.5 times $10 plus 0.5 times 0.
So our new addition law says that if you and I both play this game, then our expected
total winnings will be 5 plus 5, or 10.
That's true regardless of whether we share a flip and call the same result, or share
a flip and call the opposite results, or each flip our own coin.
So how about the cat game?
Earlier we saw that if you wanted to maximize your chance of getting them all right, you
should guess no cat owners at all, but we also said that seemed to make better sense to try
to maximize your average winnings.
What's your best choice now?
Well, let's think about guessing one person only.
Probabilistically, two-thirds of the time they're a non-cat owner, one-third of the
time they're a cat owner.
If a correct guess gets you $100, then guessing non-cat owner will get you two-thirds times
$100 plus one-third times 0, or about 67 bucks on average.
Guessing cat owner only gets you $100 one-third of the time, so its expected payoff is one-third
times 100 plus two-thirds times 0, or about 33, only half as much.
But a moment ago, we said that expected values from different activities always add.
That means that if in nine pictures you make an average of 67 bucks every time you say
no cat, and an average of 33 bucks every time you say cat.
So your optimal strategy is still to guess that none of them have cats.
On average, you'll make about $67 on each picture, or a total of 600 bucks with this
strategy.
With the nine people in this lecture, you actually make 700.
Using their common sense, very few people fare this well.
So we've put together some essential tools and probability, calculations of probabilities
of compound events, and expected value, and we'll need them.
We also know to watch out for variability in the data, especially if we're doing a risk
analysis for an event that happens only once or a few times.
Okay, we're now ready to see how probability helps us to navigate the interplay of decisions
and chance that characterizes an unfolding situation.
That's our next stop, Decisions in the Face of Uncertainty.
