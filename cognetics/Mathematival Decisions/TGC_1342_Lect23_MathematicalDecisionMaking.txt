We've been addressing randomness more directly in our recent lectures, including stochastic
processes to help us to predict what to expect.
For a lot of techniques, we've considered the best solution to be the one that gives
the best expected payoff, the best average result.
In some cases, that's as it should be, when you face the same situation over and over
and all that you really care about is the total payoff at the end, the expected payoff
is exactly what you want.
A realtor doesn't care how many houses he or she sells on a particular day, the important
thing is the total for the month or the year.
In cases like this, statistics says that the long run average payoff that you actually
collect is likely to be very close to the expected payoff, never mind the ups and downs
along the way, but when a situation is faced only once or only a handful of times, now
you have to worry about downside risk, and so far this is something that we've given
limited attention to.
In queuing theory, our big time equations computed expected values like average time
spent in line, but there's also variability to consider, it may be fine if a restaurant
serves you on average in six minutes, but not if most people get lightning fast service
while one person in twenty actually waits two hours.
That's why we touched on the probabilities of having to wait over the line being longer
than a given size.
We actually did a bit better than this in decision theory, we talked about how to adjust
the payoffs on the tree to reflect feelings of risk love and risk aversion, and we could
also look at the possible outcomes of our strategy and find the probabilities of each.
Still, even in decision theory, our strategy was chosen based on what was best on average.
There's another issue too.
While models like the queuing and markup models that we've recently studied can provide us
with good insights on how some systems behave, they also tend to have strong requirements
in their assumptions.
It's hard to use markup analysis on processes that aren't memoryless.
Most queuing models are even more restrictive, requiring that the number of servers is constant
over time, and that all servers work at the same mean rate, and so on.
Small deviations from these assumptions won't invalidate the model, but gross deviations
will, and real-life situations often show such gross deviations.
So what can we do?
We can use simulation.
In simulation, we numerically model the relationships among the important parts of the problem,
the decisions that we control, and the random events that we don't.
It's often called Monte Carlo simulation, a name that comes from the famous casino in
Monte Carlo, Monaco.
We generate random variables for the values for the random variables in our problem, and
then see how a proposed strategy works when faced with these random events.
The technique is remarkably powerful and versatile, making it, like linear programming, one of
the most widely used techniques in operations research.
Ever have a flight delay due to the weather?
The capacity of the U.S. air transport system is hard pressed by demand to begin with.
From a queuing theory's perspective, we know that means long waiting lines and frequent
late flights, but throw in a major weather event that disrupts the system, and things
can get much, much worse.
In 2005, the FAA had only a vague idea of the cost of such disruptions, and no real
strategy at all for handling the problem.
So they decided to take a closer look, in conjunction with Metron Aviation and the Volpe
Transportation Center.
They designed simulation models for the air traffic system, including the impact of storms.
Based on what they found, they came up with a new approach to managing air traffic, and
built it into an expert system.
This system could make recommendations in real time, as traffic blockages occurred.
Computerized expert systems may suggest changes in flight paths, but that doesn't mean that
the air traffic controllers are going to pay any attention to them.
If the success of a project required buy-in from the controllers, and given that the error
on their part can result in the loss of hundreds of lives, they were rightly skeptical.
So the research team went on to develop an interactive simulation that allowed controllers
to work with the expert system in a simulated environment.
Simulations wonderful for handling situations that are too dangerous or too costly to do
live.
The interaction of controllers with expert systems allowed refinements to the system,
resulted in rules for its use in deployment, and generated buy-in from the controllers.
Bottom line, in the first two years of the deployment, the savings to the aircraft companies
and the flying public totaled almost $190 million.
Compare this to the cost of the study, which was about $5 million.
The projected savings are estimated to be about $2.8 billion over a 10-year period.
It's a considerable chunk of change in anyone's ledger.
Now, it's doubtful you face a problem on that scale, but the versatility of simulation
and the increasing availability of simulation software make it a powerful tool for any operations
researcher.
So how do you do a simulation?
Well, as always, the better you understand a problem, the better you can model it.
We need a structural model.
What factors in the situation are key, and how do they interact?
Once we understand how the pieces fit together, we need to understand the nature of the random
events in the problem.
This usually requires data, lots of data.
For the FAA study, how often do storms occur?
How large are they?
How long do they last?
How long does a storm like this delay a flight like that?
And what's the distribution of flight delay times when there aren't storms present?
And so on.
All of that data goes into mathematically describing the possible outcomes of random
events and how likely each outcome is.
That gives us a probability distribution for each random event.
Of course, we can't just generate one set of random events and see what happens.
That'd be like visiting Times Square on New Year's Eve at 11 p.m. and figuring you had
the place pretty much sussed.
If you visited on a Tuesday afternoon in June, you're going to get a very different impression.
Look, it's the basic lesson of statistics.
To draw a decent conclusion about a variable quantity, you have to look at a sufficiently
large sample.
So we want to run our simulation over and over, each run having its own randomly generated
values for its chance events.
This kind of repetitive work is heaven for a computer.
And assuming that we've built a good simulation, the compiled output gives us a pretty clear
idea of the range of outcomes that we might expect, not only what's possible, but also
how likely.
And that can lead to better strategies for handling the situation.
And then we can implement those strategies in our model, rerun it, and see if our new
ideas do, in fact, work.
Simulation isn't an optimization technique.
It doesn't automatically figure out what the best strategy is.
But in spite of that, a well-designed model can allow you to play what-if games.
To modify the model in order to see how that modification affects the possible results.
This glimpse into worlds other than our own can be incredibly useful in identifying both
risks and opportunities in managing uncertainties.
Let me highlight this with a business application, and at the same time demonstrate how simulation
is actually done.
I'm going to continue to work in the spreadsheet environment.
It's easily accessible, and it allows you to see all the pieces fit together as we build
the model.
But I'll be honest, the bare-bones spreadsheet environment is not the perfect place to do
simulation.
When our simulation is built, we'll want to run it many times and summarize the results
in useful ways, like charts and tables.
Ideally, you'd like to be able to interact with these charts and tables, drilling down
the specific information of interest.
These tools are possible on a spreadsheet, but for this lecture, we'll create them.
Most aren't built in.
That said, there are also third-party add-ins that give Excel this functionality.
Three well-known ones are analytic solver, crystal ball, and at-risk.
And they greatly simplify the variable generation, data gathering, and data reporting steps.
If you're interested in this material, give them a free trial and decide whether they
might be worth purchasing.
If you're doing more advanced simulation, you may even want to learn one of the dozens
of simulation programming languages that exist, such as Simscript or ACSL.
Anyway, a basic spreadsheet program has enough to give us the basic feel for what's going
on.
So, here's our problem.
You are a construction contractor and you're bidding on a construction job.
It's a competitive bid, so your success is going to be dependent on you're being able
to determine with reasonable accuracy while completing the job is going to cost you.
You have enough experience with jobs like this that you can model its different activities
pretty well.
Preparing the bid will cost you 5,000 bucks, win or lose.
If you actually land the job, you'll have other costs, materials, labor, and possible
late fees if your work gets behind schedule.
But of course, with the winning bid, you'll get paid for your efforts.
So what should you bid?
Bid too high and you have a little chance of getting the job by buying a $5,000 prep
fee.
Bid too low and you'll get the job, but you'll be sorry you did.
Your profit's going to be whatever your bid is minus your expenses.
We're going to use simulation to get a handle on what those expenses might be.
To begin, I'm not going to worry about how to handle the randomness.
I'll just put in average values and then we'll make it stochastic on the second path
through the model.
So let's work out the cost of completing the job.
Prep cost, fixed $5,000.
Materials cost?
Well that's actually going to vary from job to job, but for now I'll use the average
figure for similar jobs in the past, $60,000.
Next comes labor cost and late penalty, but the labor cost depends on how many weeks of
labor we need and the penalty depends on how long the whole job takes.
Again, let's stick in some fixed values for now and we'll make them random later.
Nine weeks for labor, two weeks for delay.
You can see I've created a few extra columns too, things that we need to figure out what
the total cost of the job will be.
Okay, your laborers get paid by the week, $700 for each week that they work.
You currently have four workers, so your labor cost will be given by the formula $2,800 times
the number of weeks of labor needed, like this.
Four weeks needed was in cell C4, so the formula for labor cost is $2,800 times C4.
As you know, every formula in the spreadsheet starts with an equal sign, the asterisk means
multiplication.
When I enter this into the spreadsheet, it computes the value $25,200.
Okay.
When's the job done?
Well, take the labor time and add the delay time, and that's the completion time.
Like this.
With our current numbers, that turns out to be 11.
And now the completion penalty.
Part of the contract for this job is that it needs to be finished in 12 weeks or less.
Finish it late, and you owe a penalty of $12,000 for every week that you're over the limit.
Again, we need a formula.
One way to get it is an if statement, like this.
If statements always say, if the first thing is true, then write the second thing, otherwise
write the third thing.
So here it says this.
If the cell F4, the job completion time, is less than or equal to 12, then write a zero.
No penalty.
Otherwise, write the penalty of 12,000 times the quantity F4 minus 12, because every week
over 12 is going to cost you 12,000 bucks.
By the way, the separator that you're seeing here is the semicolon, which is what Calc
always uses.
If you're working in Excel, type exactly the same thing, but use commas instead of semicolons.
Calc always uses commas.
Okay, we enter this formula, and we get a late penalty of zero this time because our work
was done in week 11.
All that's left is to add up the cost, like this, which comes out to be $90,200.
And that's our model for how much the job will actually cost.
Well, almost.
We still have to put in the randomness for materials costs, for labor time needed, and
for delay time.
I'm not for it.
Let's get to work.
The materials for the construction on the job averaged $60,000.
We said, but there's a fair variation in this.
Looking at similar jobs in the past, you decide that the cost of materials is approximately
like this.
The picture is called a probability density function, or PDF, of this random variable.
To interpret a PDF, imagine that it's a silhouette of a pile of sand.
The grain of sand in that pile represents the cost of one job.
Lots of grains are piled up on or close to $60,000 in this PDF.
That means that costs that are close to $60,000 are common.
The further you get from $60,000, the less common that cost is.
In statistics, a bell-shaped curve like this is called a normal distribution.
This one has a mean of $60,000, which works out for any distribution to be the balance
point of the pile or sand.
You can imagine it sitting on a teeter-totter.
Its spread, on the other hand, is measured by its standard deviation, and here it's
$4,000.
For normal curves, the standard deviation is easy to find.
Take a look at how I shaded the picture.
In the shaded area, the curve looks like an umbrella.
In that region, the curve is concave.
Now look at the two light-colored tails.
There the curve is convex, like a bowl.
The place where we switch from umbrella to bowl is called the point of inflection.
For a normal curve, the point of inflection is always one standard deviation away from
the mean.
Here it happens that $60,000 plus or minus $4,000, so the standard deviation is $4,000.
Ah, fair warning, this only works for normal curves.
But for curves that are close to normal, about two-thirds of all of the observations, all
of the grains of sand, are within one standard deviation of the mean.
About one-third of the sand in our picture would be out of the umbrella's shadow and
would still get wet in a rainstorm.
Anyway, we need to generate random materials costs for our model.
If you're using simulation add-ins to your spreadsheet, there are straightforward ways
to get this, but for Excel or Calc, I'm going to introduce a formula that gets the job done,
although I'm not going to go into why it works.
For our normally distributed materials costs, with a mean of $60,000 and a standard deviation
of $4,000, we can generate values by using this.
The RAND generates a random number between zero and one, with all values in that range
being equally likely.
The NORM INV then uses that random number to generate a random variable for a normally
distributed variable with a specified mean and standard deviation.
It essentially picks out one grain of sand.
Every time you enter something new anywhere in the spreadsheet, the sheet recalculates
the value of this RAND in this cell, and that means you're going to get a new value for
your materials costs, a new grain of sand.
And again, if you're in Excel rather than Calc remember, all semicolons become commas
in Excel.
Okay, that's materials.
How about labor?
Labor costs are a fixed $2,800 per week, but the number of weeks required to complete the
job is uncertain.
Here's a table of possible times and their probabilities.
Unlike our pile of sand PDF, this table talks about the discrete random variable.
It can take on only a finite set of values.
How do we get this work done in our spreadsheet?
Well, I'm not going to use an add-in, so I need to enhance this table first a bit.
I'll paste in, paste it into the spreadsheet, and then to its left, I'll add a range of
values for each row, like this.
Here's the idea.
The RAND function that I told you about a minute ago always returns a value between 0
and 1, and all the values in that range are equally likely.
That means that 10% of the time, it'll give you a value between 0 and 0.1, because that's
10% of the interval from 0 to 1.
By the same logic, 30% of the time, it will give you a value between 0.1 and 0.4.
The range from 0.1 to 0.4 is 30% of the total distance from 0 to 1, so numbers in that range
come up 30% of the time.
Here's another way of looking at it.
RAND picks a random point from 0 to 1 on the line.
You can see that range in the black numbers.
Since the length between 0.1 and 0.4 is 30% of the total, the RAND value falls in this
range 30% of the time, and we've assigned this range the meaning 8 weeks of labor.
You can see that in red.
So 8 weeks of labor happens 30% of the time, just as our table required.
To get Excel or Calc to follow this rule, we use the command VLOOKUP, vertical lookup.
It uses the table we just made.
Here's how the command would look.
The first argument of VLOOKUP is the number to lookup, which is going to be random.
The second argument tells us where in the spreadsheet this four-column table is by specifying
its upper left and lower right corners.
We don't want to include the heading, so our table starts in row 16.
The final argument specifies which column of that table contains the information that
we want.
For us, it's the third column of the table, which contains the number of weeks of labor.
This is the formula that goes into our weeks of labor cell in our spreadsheet, and in
parallel fashion, I'll set the table up to tell us how many weeks of delay we have.
And here's our simulation.
Every time the spreadsheet recalculates, the cost will change because we'll have different
random numbers.
We can force the spreadsheet to do this by pressing the F9 key.
Let's see it work.
And we can see some things already, like the fact that it's not rare to have a late penalty
and that it can be expensive.
We're also getting some idea of what variation we can expect in job cost.
Let's be a little more systematic about this.
What we want the spreadsheet to do is to repeat the simulation many times, just as we've been
doing by hitting the F9 key, but to record the result of each trial.
Once again, simulation add-ins will do this automatically, but even without them, it can
be done quite easily in Excel.
And Calcut can be done, but it's more than a bit of a pain.
Rather than get into the detail of it here, I'll provide a link to a sensible set of
instructions in the bibliography.
Anyway, here's the result, or the beginning of it.
We're looking at the values for three quantities for the first 15 simulation runs.
I actually told the spreadsheet to repeat our situation 1,000 times and to record the
values of these three quantities in each trial.
Now no one wants to look at all of those values individually, so let's get some summary statistics.
We can use the functions max, average, and min to find the biggest, average, and smallest
values for each variable.
To find the maximum value of the cells from L5 to L1004, for example, you can type equals
max L5 colon L1004.
Pretty straightforward.
Here's what we get for the three variables that I was tracking.
These numbers will fluctuate a little bit every time you recompute the sheet.
Their recomputation is based on new random numbers, but the results over 1,000 runs are
pretty stable.
And it looks like our total average cost is about $98,000, but the range is quite wide
from around $75,000 to over $190,000.
I can get a better sense of this by creating a frequency table and histogram.
The frequency function is built from making such a table.
Let's look at the total cost in $10,000 increments starting at $70,000.
We start by putting the lower and upper cutoffs for each category in the spreadsheet, row
by row, like this.
Now the frequency column will contain how many observations fall in each category.
We can get all of this with a single command.
First, we select all of the blank cells in the frequency column, then type this, equals
frequency, and then select all of the 1,000 values of total cost, then a semicolon, or
comma if you're working in Excel.
And now we highlight the upper bounds of each category, then in parentheses.
And here's the odd part.
Like last time, don't hit enter.
Hit control, shift, enter, all at the same time.
It's an array formula, since it fills in an entire column.
Anyway, we get this.
And now we can use Excel or Calc's chart capability to draw a bar chart.
The materials costs were normally distributed, but the total cost definitely isn't.
Look at that long tail on the upper side of the graph.
It's something that you ignore at your peril.
Even though the average cost of the job is just over $98,000, over 5% of the time the
cost is $130,000 or more, and costs of $120,000 or more occur about one time in eight.
If cash flows an issue for your company, this is something to keep in mind, because
a bit of $120,000 on a job like this, even if accepted, will actually lose you money
one time in eight.
Why is this happening?
Well, let's take a look at the time required to complete the job.
We'll make a bar chart.
And a real part of our problem begins to become clearer.
Anything over 12 weeks starts the late fee clock ticking, and at $12,000 a week, those
fees can quickly eat up your profits, and we can see from the graph that we're paying
late fees about 26% of the time.
If you prefer, we can graph how much we're paying in late fees instead.
And as we found before, we're paying an average of almost $7,000 in late fee, but it can go
as high as a whopping $72,000.
So we're a little wiser about what we can expect in costs, and we've identified a source
of expense at first sight may not have seemed so important.
But the question, of course, is, what can we do about it?
This brings us to one of the most wonderful things in the analytic approach in general,
and a simulation in particular.
What if analysis?
A simulation can allow you to explore the impacts of changes in your situation.
In our case, it's going to help us get a grip on our bottom line.
In order to prepare for this, I'm going to modify our simulation model in a way that
I should have done from the start.
One of the precepts of good modeling, particularly in a spreadsheet environment, is, thou shalt
not hard code thy parameters.
Putting it simply, if there's a constant in your problem, a number, then it deserves
its own cell in your spreadsheet.
Then whenever you want to use that number in a formula, the formula should refer to
that cell rather than containing the number itself.
Why this extra complication?
Well, one reason is that if the number happens to be an ugly one, like 25020598, then it's
faster to refer to the cell than to type the number, and easier to avoid making a typo
when one of the times that you enter it.
But the big reason is that it allows you to change one number in your spreadsheet and
then have the effects of that one change percolate through everything else in the sheet.
So let me do a little revamp of our sheet, creating an area in the top for the parameters.
It's essentially the same simulation we had before, but now everywhere that we had a number,
like 12, where we place it with the name of the cell that contains that 12 in our parameters
section.
We ended up with six parameters.
There's one change to be made in our simulation, actually.
Now that I've made the number of workers an explicit parameter in the problem, well the
table for labor time was written with the assumption of four laborers.
How long does the job take now when I change the number of laborers?
We can answer this by looking at the labor requirement a little differently.
Not in weeks, but in man weeks.
A man week is the amount of work that one man, or woman, can do in one week.
So four people working on a job that takes five weeks requires four times five or twenty
man weeks.
It could have been done by five people working for four weeks instead, or ten people working
for two weeks.
We should be a little careful here, since there could be synergies.
A single worker can't accomplish some building test, like laying the ridge beam of a roof,
and a swarm of workers would just get in each other's way actually slowing progress rather
than speeding it.
It's also possible that workers are specialized in their abilities or work at different speeds
and so on.
One often runs into this kind of thing when modeling real-world situations.
What do we do about it?
Well, the idea that additional workers add the same amount of oomph to the building process
probably isn't perfectly true.
But is it close enough to true?
A model tries to extract the important aspects of the situation and to ignore the insignificant
ones.
Here, we want to seek domain knowledge.
We want to talk to someone who knows the ins and outs of building projects.
Since you are our experienced contractor in this scenario, you'd probably already know
that this model is pretty accurate as long as you have, say, at least three workers but
no more than twenty.
But you may also know that you're not going to be able to get workers if you can't promise
them at least five weeks' work.
With these caveats in mind, I'm going to compute the time to complete the job.
I'll first find the number of man hours implied by our four-worker table.
That just means multiplying the time given in the table by four.
Then I'll divide that by our actual number of workers to find out how many weeks they'll
need for the job.
Finally, I'll round that number up to the next whole week because the workers get paid
by the week or fractions thereof.
The old formula for labor, labor time, was this.
And the new one looks like this.
Start with the old formula for how many weeks it takes for four workers to finish the job.
Multiply that by four to get the man weeks for the job.
Then divide that by how many workers we have.
That's in cell I4.
Finally, round the result up to the next whole number.
The zero at the end tells Calc to round to zero decimal places, that is, to a whole number.
And remember, for Excel, all of the semicolons become commas.
We'll also modify the labor cost to say that even if the job is done in less than five
weeks, the workers still have to be paid for the five weeks of labor, like this.
I4 is the number of laborers, I5 is what we pay each per week, and C11 is the number of
weeks of labor that we need.
If it's less than five, we still pay for five.
By the way, when you give a spreadsheet model to someone else, an end user, it's important
that they know what quantities in the sheet they could enter new values for, and what
quantities are being computed from those values.
I follow the convention that any cell that you're allowed to change is blue bordered
with a gray interior.
You'll see this in our parameters section, but also in the probability tables for job
delay and labor time needed.
That means you could modify these distributions, too, if you wanted.
So, okay, let's play with our new improved sheet.
How can we get those costs down?
Do we can negotiate with the customer on the deadline of 12 weeks?
If I'd extend that to 13 weeks, how much would it help us?
Well, change the 12 and the Weeks Allowed parameter box to 13, and find out.
Huh.
Well, it helps.
Some.
The mean cost is now about $95,000, or before it was about $98,000.
If the customer would accept a one-week extension before levying fees, it would save you about
$3,000 on average.
If you could offer them $1,000 off your quoted price for each such extension.
Of course, they might not go for it.
Of course, still, it really hasn't addressed a lot of your downside risk.
In the 1,000 simulation runs, you still could end up paying up to $60,000 in penalties.
Any better ideas?
Materials?
Don't look promising.
You'd either need to find cheaper materials or a more accommodating supplier.
Yeah, same for delay times, whether supplier delays or acts of God.
But how about workers?
More workers should be able to get the job done faster, cutting down on late penalties.
On the other hand, you have to pay them.
But since the job is completed faster, you don't have to pay them as long.
Here's what happens to total cost as we change the number of workers.
The red dot shows the average cost of the job with a specified number of workers, and
we see that we can save some money on average by hiring more.
6 seems to be about the best number, saving us around $4,000 on average, even though we
need to pay more workers.
But perhaps more importantly, look at the vertical bars that show the maximum and minimum
costs in the 1,000 simulated trials.
With eight workers, twice your original complement, the average price is about $2,000 cheaper
than the original value, but the job price never rose above $115,000.
The job is literally 99.9% likely to be completed on time, a fact that you can use in negotiating
with a customer.
In fact, the job is usually done by week six.
So we can cut our original average cost, virtually guarantee the job will be done on time, be
able to bid even if the customer raises the late penalty, and greatly reduce the uncertainty
in our own return, allowing us to bid more intelligently and to predict our profit margin
more accurately.
And that is the power of simulation.
In the next and final lecture to this series, we'll return to the simulation, coupling the
stochastic work that we've done in recent lectures with the optimization work from the
middle part of the course.
The result will be stochastic optimization, with the goal of finding best answers to problems
that include random elements, where you get to define yourself what constitutes best.
I think you'll be amazed by the power and flexibility of the machinery that we can now
bring to bear.
