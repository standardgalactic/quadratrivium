Knowing what's going to happen, or even having a pretty reliable estimate of it, is power.
If you know the future, you can capitalize off it.
I'm often amazed how much of the behavior of the physical universe can be captured by
relatively simple mathematical models.
And of course, they are models.
A good model is one that shows close agreement with the facts so that we can use it to make
predictions that might not otherwise be possible.
I want to talk with you today about a forecasting technique with considerable power in describing
connections between related quantities in many disciplines.
Linear regression.
It's underlying ideas, easy to grasp and easy to communicate to others.
The technique is important because it can, and does, yield useful results in an astounding
number of applications.
But it's also worth understanding how it works, because if applied carelessly, linear regression
can give you a crisp mathematical prediction that has nothing whatsoever to deal with reality.
To develop these ideas, it's best to start easy.
In fact, let's all take it easy.
Let's go on vacation to Wyoming, to Yellowstone National Park.
Impressive place, Yellowstone.
Beneath the park is an active supervolcano, the largest on the continent, which is the
reason the park contains half of the world's geothermal features, also more than half of
its geysers.
The most famous of these, of course, is Old Faithful.
It's not the biggest geyser, nor the most regular, but it's the biggest regular geyser
in the park.
Or is it?
There's a popular belief that the geyser erupts once an hour like clockwork.
Let's look at some data and see.
This graph is a dot plot, tracking the rest time between one eruption and the next for
a series of 112 eruptions.
Each rest period is shown as one dot.
Rests of the same length are stacked up on top of one another.
And what this plot tells us is, you shouldn't set your watch by Old Faithful.
The shortest rest time that we saw over on the left is just over 45 minutes.
The longest one, way over on the right, is almost 110 minutes.
If you wanted to be generous, you could call the geyser Old Semi-Faithful, since there
seems to be a cluster of short rest times, around 55 minutes, and another cluster of
long rest times in the 92-minute range.
So based on what we've got right now, when tourists ask about the next eruption, the
best a park service can say is, folks, it'll probably be somewhere between 45 minutes
to two hours after the last eruption, which isn't very satisfactory.
Can we take a predictive modeling approach to do this better?
Let's see.
We want to do a better job of predicting Old Faithful's next eruption time.
We might be able to do that if we could find something that we already know that could
be used to predict the rest periods.
But what?
We might reason like this.
I don't know the details of how a geyser works, but my rough guess is that water fills
a chamber in the earth and heats up.
When it gets hot enough, it boils out to the surface, and then the geyser needs to rest
a while while more water enters the chamber and is heated to boiling.
It doesn't sound like a bad guess, but an analyst would also want to take advantage
of domain knowledge if possible, talking to people who know and understand geysers.
It's an important rule of thumb for any project.
When you can, pick the brains of the experts.
Still, if our model of a geyser is roughly right, then we could imagine that a long eruption
uses up more water in the chamber, and then the next refill, reheat, erupt cycle would
take longer.
We can look into this, making a scatter plot with eruption duration on the horizontal axis
and the length of the following rest period on the vertical.
Here's what we get.
When you're looking at bivariate data, two variables, and they're both quantitative,
numerical, then a scatter plot is usually the first thing you're going to want to look
at.
Wonderful tool for exploratory data analysis.
Each eruption gets one dot, but that dot tells you two things.
The x-coordinate, the left-right position, tells you how long the eruption lasted.
The y-coordinate, the up-and-down position, on that same dot tells you the duration of
the subsequent rest period.
For example, look at this point.
It represents an eruption that lasted about 190 seconds, followed by a rest of about 86
minutes.
So we have short eruptions, followed by short rest periods, clustered in the lower left,
and then we have a group of long eruptions, followed by long rest periods in the upper
right.
Looks like we're on to something.
There does indeed seem to be a relationship between the eruption duration and the length
of the subsequent rest.
Okay, we have a prediction.
Can we capture this relationship quantitatively?
Well, we can get a reasonable approximation to what we're seeing here by drawing a straight
line that passes through the middle of the data, like this.
If I asked you to sketch a straight line through the middle of this data, it's likely that
the line you'd have drawn would have been close to the one that I'm showing here.
So for an eruption of 180 seconds, this line would let us predict a rest period of about
72 minutes.
Great.
We've predicted to within a minute how long to wait for the next eruption.
But how reliable is that prediction?
Before we can answer that, we have to talk a bit more about the line itself.
You see, the line that I drew really isn't just one that I eyeballed from the graph.
It's chosen according to a specific mathematical prescription.
Not surprisingly, we want the line to be a good fit to the data.
We want to minimize the distance of the dots from the line.
If you've got a line like this and a dot like that, you might think that the way you'd
measure the distance is along the diagonal like that, but we don't.
We measure it vertically like that.
That distance tells us how much our prediction of rest time was off for that particular case.
It's also called the residual of that point.
A residual is basically an error.
So this dot has a positive residual of about positive 13.
It's 13 minutes above the line.
This one has a residual of about negative 5.
It's 5 minutes below the line.
Remember that for this problem, the vertical axis is rest time in minutes.
So our graph has 112 points and we could therefore find 112 residuals, how well the line predicts
each data point.
We want to combine these residuals into a single number that gives us a sense of how
tightly the dotted data is clustered around the line to give us a sense of how well the
line predicts all of the points.
You might just think about averaging the distances between the dots in the line, but for the
predictive work that we're doing here, it's more useful to combine these errors in a different
way by squaring each residual before we average them together.
The result's called the mean squared error, or MSE.
The idea is that the residual tells you how much of an error the line makes in predicting
the height of that particular point, and then we're going to square each of those errors
and then average all those squares.
A small mean squared error means that the points are clustering tightly around the line,
which in turn means that the line is a decent approximation to what the data is really doing.
The straight line that I drew on the old faithful scatter plot is the one that has the lowest
MSE of any straight line that you can possibly draw.
The proper name for the prediction line is the regression line, or the least squares
line.
The decision to square all of the residuals before averaging them seems an odd thing to
do, so it's probably worth justifying this decision a bit.
In several senses, it's really a matter of practicality.
One aspect of practicality is simplicity, and that seems to favor just averaging the
residuals directly, but a residual can be positive or negative depending on whether
the points above or below the line.
Average plus five and minus five, and you get zero.
But the error for those two points obviously not zero.
If you shoot two arrows at an archery target, one five feet to the left and one five feet
to the right, you can't claim two bull's eyes.
Okay then.
We don't use the residuals directly, how about if we use their absolute values?
Whether a number is positive five or negative five, it's still an error of five, but this
system is impractical for its own reasons.
First, we're going to want to be able to use statistics, calculus, and algebra to be able
to draw useful conclusions about this line, what it tells us, and how much we can trust
it.
The average the squares calculation of MSE has very nice properties for this work, while
the absolute value function does not.
For example, the absolute value function doesn't have a derivative at zero, which makes it
a pain if you're doing calculus.
There's another problem with the average the absolute values approach to.
It doesn't necessarily give you the line you want.
In fact, it may not specify a single line at all.
Look at this example.
If I asked you to draw the best straight line to match this data, what would you pick?
I'm betting it's this.
One choice, right through the middle.
Each point has a residual of either plus five or minus five.
Each one's either five above or five below the line.
So the average distance of the residuals from the line is five, two.
But look at this.
The horizontal line doesn't do nearly as well at capturing this trend of the trend of points,
but look at the four residuals.
Two of the points are on the line, residual, zero.
The other two points are 10 away from the line, one above, one below.
The average distance from this line is the average of zero, zero, ten, and ten, which
is also five.
In fact, any line that passes through the goalposts on the left hand side of the graph
and also passes through the goalposts on the right will have an average residual of five.
But most of us would say that these lines are not all equally good at characterizing
the scatter plot.
On the other hand, the MSE approach of combining the residuals, square them, then average,
it's rid of the pesky negative residuals, a square is always non-negative, it behaves
very nicely with calculus and statistics, and the line it prescribes is a close match
to what our native intuition says is the right line to draw.
For example, it goes right down the middle, splitting both sets of goalposts.
Oh, for the sake of completeness, I'll mention that for technical reasons, MSE is often computed
by adding the squared residuals and then dividing by the number of points minus two,
rather than the number of points itself, as you would for a normal average.
This definition is more useful for advanced statistical calculations, and it doesn't change
what line minimizes the MSE.
So we've defined our best straight line to fit the data, the least squares regression
line, and finding and using this line is called linear regression.
More precisely, it's simple linear regression.
The simple means that we only have one input in our model.
Here that's the duration of the last eruption.
If you know some calculus, you can use the definition of the regression line, the line
that minimizes MSE, to work out the equation of the regression line, but the work is time-consuming
and tedious.
Fortunately, any statistical software package or any decent spreadsheet, like Excel or Open
Offices Calc, can find it for you.
In those spreadsheets, the easiest way is to right-click a point on your scatter plot,
click on Add Trend Line, and for a couple of more clicks, it'll tell you the equation
of the line.
For the eruption data, the equation of the line is about y equals 0.21x plus 34.5.
X is our eruption duration, and y is the subsequent rest.
The equation says that if you want to know how long of a rest to expect on average after
an eruption, start with 34 and a half minutes, then add an extra 0.21 minutes for every additional
second of eruption.
So if the last eruption lasts 4.5 minutes, 270 seconds, you'd plug 270 in for x, crank
out y, and you'd conclude that you'd expect to wait about 90.8 minutes for the next one.
You can also get this right off of the scatter plot, like this.
Any software package will also give you another useful number, the r-squared value.
Technically, the r-squared value is called the coefficient of determination, since it
tells you how much of the line determines or explains the data.
Most people just call it r-squared.
For our old faithful data, the spreadsheet reports that the r-squared value is about
0.87.
Roughly, that means 87% of the variation in the heights of the dots can be explained
in terms of the line.
So given that we don't really know beans about how geysers work, our predictions are actually
pretty good.
We've got a lot of variation in rest times, from 45 minutes up to 110 minutes, but the
up and down bouncing around the line is much less.
Most dots are above or below the line by no more than 10 minutes, and many are even closer
than this.
If you take the variation of jiggles above and below the line, it's only 13% as large
as the total variation seen in the rest period data.
So the linear model is accounting for the remaining 87% of the variation.
The linear model explains 87% of the variations in rest times in terms of the length of the
previous eruption.
Compare that to a graph like this one, based on a random sample of my students, it's looking
for a link between height and grade point average.
Here the r-squared is 0.02.
You can see that there is a trend in the data.
It does tend to drop slightly as you move from left to right, but the vertical spread
around this line when compared to the drop is extreme.
The line explains only about 2% of the variation in GPA, small enough to be attributed to random
chance, where we're not seeing any meaningful relationship between height and grades.
Which is not as much of a no-brainer as you might expect.
Research studies have shown that height and salary show a positive correlation with an
r-squared of somewhere between 0.08 and 0.15, even after correcting for gender, age and
weight.
While those values are too small to make height alone a good predictor of salary, a value
of r-squared in that range, when based on a large enough sample, can still indicate
that there is some real connection.
The highest r-squared value that any line could achieve is 1, meaning that 100% of the
variance of the output is predicted by the line.
That could only be the case if every data point fell exactly on the line.
In real life data, whatever it is, that's pretty unlikely.
r-squared of 0.87 is nothing to sneeze at.
Now, people who do regression should always tell you what the r-squared value for the
regression line is, and they usually do.
A small r-squared, unless the sample is huge, means that there isn't much evidence of a
linear relationship between the variables and the data.
It's worth noting though that there may be a very strong relationship between the variables,
just not linear or a straight line one.
Here's an example.
You throw a ball up in the air and measure its height every fifth of a second until it
lands.
You get a graph like this.
Obviously, there's a strong relationship between time and height, the fact that allows you
to actually catch the ball to begin with.
But if you apply linear regression to this data, here's what you get.
The red line, which corresponds to the ball hovering at an altitude of 35 feet.
The model doesn't explain the actual variation in the ball's height at all, so the r-squared
is zero.
An extreme example, but it drives home an important point.
Linear regression assumes that your data is following a straight line, apart from errors
that randomly bump the data point up or down around that line.
If the model's not close to true, then linear regression is going to give you nonsense.
We'll expect the data to follow a straight line when a unit change in the input variable
can be expected to cause a uniform change in the output variable.
For old faithful, we saw that each additional second of eruption added about 12 seconds
of rest.
So okay, if the r-squared is low, we're on shaky ground.
And that's one thing that everybody learns quite early about linear regression.
But linear regression is so easy to do, at least with a statistical calculator or a computer,
that you'll often see people getting quite cocky with it, and getting themselves into
some pretty bad trouble.
I feel like giving a 12-year-old a chainsaw and turning them loose.
I don't want you to be in that all too common situation.
So let's take a look under the hood and see what's going on.
For that, let's look at how the populations of U.S. cities compare to their land area.
I went to the U.S. Census Bureau's site on the Internet.
I chose 10 cities and recorded their populations and their areas.
Let's see what we can do with that.
Intuition would lead us to think that cities with larger populations would also be larger
in extent, and we could hope for a linear fit.
That would mean that each additional person adds about the same amount of area to the
city.
Is that intuition borne out in the sample?
Well, here's the scatter plot with the trend line, equation, and r-squared value.
Things are looking pretty good.
Our r-squared value is even higher than it was for old faithful, with about an 88% of
the variation in our city's areas being explained by their populations.
The regression equation says that for each additional 1,000 people, we'd expect the area
to increase by about 3.055 square miles.
That's the coefficient of x, which gives the slope of the line.
The intercept tells you that for a city of zero people, x equals zero, we'd expect an
area of about 95 square miles.
Huh?
Zero people in a city of 95 square miles?
That's a bit extravagant.
Yeah, well.
But we really shouldn't be too surprised.
We know that the line doesn't fit the data perfectly.
So it's a little off on the intercept, too.
The important point is that we've got a reasonably reliable relationship between city population
and city area.
Our r-squared was 0.88 after all.
So we have an equation that we can use to estimate the area of any U.S. city.
At least that's what the conventional wisdom is of people who dabble with regression lines.
Like, unfortunately, some business people.
How about if the graphs of the lines were sales volume as a function of advertising,
like this?
Or sales as a function of price, like this?
Knowing the relationship between those variables could be gold for your business.
So given a set of bivariate data, one of the first things that MBA students usually do
with it is to perform a linear regression.
The spreadsheet's only too happy to do the calculation of the straight line in the r-squared.
And so for very little effort, they end up with a nice, crisp equation backed by a computer
that they can take to the next planning meeting.
What's wrong with that?
Well, nothing, maybe.
But linear regressions aren't always as trustworthy as they seem.
Let's see why.
We were assuming that the connection between city area and population can be thought of
this way.
There's an underlying linear relationship, an expected city size for each city population.
Then each real city is bumped off of this line above or below it by some random amount.
The amount depending on things other than population.
Our job is to collect data in order to find the equation of the line that the cities are
jiggling around.
That's pretty much all that you know when you set out in linear regression.
Here's what you don't know.
The truth is that the line is really the area of the city is three times the population
in thousands of people.
But the city is going to jiggle above or below that line by up to, yeah, say 500 square miles.
Let's see what happens if I gather data on only two cities rather than ten.
Now, using such a small data set would be a very poor way to make predictions, of course.
But I'm going to use this extreme example to make it easy for us to analyze the situation.
So, maybe the first city has a population of one million people, a thousand thousand people.
That means the true regression lines is that the area is about three times a thousand or
three thousand square miles.
But that's just the straight line itself, the average.
The up-down jiggle for the city might be as big as 500 square miles.
So your million person city takes up in realities somewhere between 2,500 and 3,500 square miles.
Or it is, write it down.
Now, go to a second city.
Let's say that this one has a population that's twice as big, two million people, two thousand thousand.
Then the correct straight line relationship between population and city size so that the
area should be about two thousand times three, or six thousand square miles.
But again, real cities don't all fall right on the line.
We said they could be off by as much as five hundred square miles.
So the population that you measure for city two is somewhere between 5,500 and 6,500 square miles.
Let's look at this on a scatter plot.
If we lived in a perfect world where there was no jiggle, the two cities would be the
two blue dots on the plot that be right on the line.
Then you'd just draw the straight line between the dots and voila, you'd have the true trend
line shown in black.
But due to random variation in city size, that's not how it works.
Your million person city could be anywhere on the left hand red bar, and your two million
person city could be anywhere on the right hand red bar.
So when you draw the straight line between the two cities that you found, you could get
a lot of different lines.
Maybe by chance, you picked two particularly large cities for their populations.
You'd get this purple line above the blue one, the true one.
Or maybe you picked two cities that were very small for their populations, getting
a regression line below the true one, like this blue line.
But it gets worse.
What if one of your cities had a large size for its population while the other had a small
one?
You could end up with one of these lines.
The rust-colored one says that 1,000 people need four square miles, while the hot pink
one says that 1,000 people only need two square miles.
So even though you could draw a straight line between your two cities and get an r-squared
of one perfect straight line fit, the line that you find might be a long way from the
true line that you want, the one that gives the true underlying relationship between the
population and city size.
Once you have this picture in mind, you can see that things are even worse if your two
cities had more similar populations, like this.
The error bars for each city are as tall as before, but those bars are separated by a
smaller horizontal distance, so your two cities could lead to guesses of the line slopes
that have nothing to do with reality.
The hot pink line would lead us to think that cities with larger populations take less room,
and still the r-squared would be one, because both points lie exactly on your line.
There's one more thing I want to point out here.
Note that not only might the line that you find differ significantly from the true line,
but the further to the right or the left you get from the middle of your data, the larger
the gap between the true line and your line can be.
This is true for linear regression in general, and echoes the intuitive idea that the further
you are from the observed data, the less you can trust your prediction.
Now some of you may feel like I've stacked the deck here, and I have.
I just showed you what would happen if you based your regression on only two points of
data.
It's a general principle of statistics that you get better answers from more data, and
that principle applies to regression too.
But if so, how much data is enough?
How much can we trust our answers?
Well, any software that can find a regression equation for you can probably also give you
some insights into the answer of that question.
In Excel, it can be done by using the program's regression report generator, part of its data
analysis add-in.
Put in your x and y values, and it generates an extensive report.
Let's look at just one part of that information for our city data.
The information in the coefficients column just reiterates the regression equation that
we found earlier today.
Area equals 94.96 plus 3.06 times population in thousands.
But look at the last columns.
Their job is to give you an idea of how much you can trust that equation.
For example, we already talked about our regression equation having a constant term of about 0.95
rather, meaning that a city of zero people should take up on average 95 square miles.
The right value, obviously, is zero square miles.
But the numbers in the last two columns in the first row are telling us that we shouldn't
take that 95 square miles too seriously.
Given the small data set and the variations from linear that we see in it, it's safer
to say that the constant term is somewhere between about negative 1500 and positive 1700.
And note that zero, which we know to be the correct value, lies in that range.
Actually, the software isn't even guaranteeing that the real intercept lies in that range.
But it's making these confidence interval predictions based on some often reasonable
assumptions about how the residuals are distributed.
It's giving a range that is 95% likely to contain the real intercept.
And in this case, it does contain it.
We have the same kind of information about for the population coefficient, which is the
slope of the line.
Based on our 10 selected cities, the line slope was 3.06.
But that's just the best line for those 10 cities.
The best line based on city data for all cities would almost certainly be different from this.
How different?
The 95% confidence limits are about 2.14 and 3.97.
That is, the 10 data points told us that on average you get 3.06 more square miles for
a thousand people, but that's just the best guess from the data.
We're 95% confident that the actual average for all cities is somewhere between 2.14 square
miles and 3.97 square miles per thousand.
That's a lot more variation than the picture might have led you to believe.
And if you look at all 363 of the largest metropolitan areas in the U.S., the slope
of the true best straight line is actually quite a bit outside this range, something
like only 0.7 square miles per thousand people.
So do we just happen to be in the 5% of cases where we randomly fall outside the 95% confidence
interval?
No, for two reasons.
The first is that I didn't pick my cities randomly.
I wanted them to fit comfortably on a graph.
I wanted to make a point.
So I focused on cities that weren't too large and weren't too small.
I took a convenient sample, bad idea, because all of our analysis and regression assumes
that the sample is drawn randomly from the population.
In fact, even if we used a moderately more extensive dataset drawn randomly, we'd have
seen that cities don't actually fit into a straight line that well at all.
Here's our original 10-city data in blue, expanded to a random list of 30 U.S. cities.
By the way, you might notice that our R-squared has dropped now to less than 20%.
So let's see how we put what we learned to work on a new problem.
The value of the house depends on a lot of factors, but one of them is probably the square
footage of the house.
I'm going to try to model this in a relationship for U.S. homes based on a random sample of
1,728 homes.
First, the scatter plot, along with its trend line, equation, and R-squared.
Okay, we definitely see a relation with a positive correlation, and the R-squared value
is about 50%, which is frankly higher than I would have expected.
That means we can account for about 50% of the variation in house price from square footage
alone.
The fact that we don't see any pattern in the residuals as we move from left to right
is good, too.
It suggests a linear fit to the data doesn't sound like a bad model.
There are a lot more houses on the left side of the graph, but that's not a problem.
Importantly, we can also see that the residuals aren't spreading out as we move from one side
to the other.
They suggest more of a cigar shape than a trumpet.
That means that we should be able to trust our confidence intervals.
Trumpet-like distributions of the residuals mean that the data is heteroscedastic, and
that messes with the confidence intervals that we compute.
The regression line itself says that every square foot of space in the house is worth
on average $113, but we want to look at the regression report to see how much we can trust
that.
Here it is.
Again, the highlighted numbers on the left are giving us the equation of the straight
line of our graph.
Price in thousands equals 13.44 plus 0.11 times square footage.
The confidence interval on the right shows that 13.44 is a little dodgy, where 95% confident
the actual values between 3.6 and 23.23.
The more important coefficient, though, is the slope.
And here we're seeing that we're 95% confident that an additional square foot increases the
average house price by between $108 and $118, a nice narrow range.
The uncertainties in slope and intercept translate into uncertainties in what the correct line
would predict.
As we saw before, any inaccuracy in our line gets magnified as we move further away from
the center of the data.
The calculations for this are actually a bit messy, but if your data set is large and you
don't go too far from the majority of your sample, the divergence isn't going to be too
much.
You can see by the red lines that for our data, the 95% interval for the right line, what
the right line predicts, is in a pretty narrow range.
For a 3,000 square foot home, the margin of error is about plus or minus $7,300.
Not bad.
But that's the accuracy on what the true line would say for a 3,000 square foot home.
And the true line is only reporting the average price for homes of that size.
We know that the actual value of the homes bounce up and down around this average.
So suppose I want to be 95% confident about the price of a specific home given only at
square footage.
There's a complicated formula for this prediction interval, but if your data sets large, there's
a rule of thumb that will give you quite a good working approximation.
Look back at your regression report and find one number.
It's usually called either the standard error or the standard error of the regression.
In our current example, the software says its value is about $69,000.
Take that number and double it to get about $138,000.
About 95% of the time, the value of a randomly selected home is going to be within $138,000
of what our regression line said, like this.
That's a pretty wide band, but facts are facts, and we see from the graph that there's a lot
of variation around the line.
So there's something else to watch out for if a colleague starts waving regression models around.
If you're talking about what happens on average, the regression line is what you want.
If you're talking about an individual case, you want the prediction interval.
