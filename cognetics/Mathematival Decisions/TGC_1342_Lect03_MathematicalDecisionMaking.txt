Last time, we introduced simple linear regression, a way to quantify the relationship between
two numeric variables.
But there are two important limitations to simple linear regression, both of which we're
going to address today.
One is that linear regression is fussy about the kind of relationships that connect the
two variables.
It's got to be linear, with output values bumped up and down from that straight line
relationship by random amounts.
For lots of practical problems, the scatterpot of inputs versus outputs looks nothing like
a straight line.
We'll see that this can often be handled by a technique called transformation of variables.
The second problem is that simple linear regression ties together one input with the output.
In many situations, the values of multiple inputs are relevant to the value of the output.
A motel room's cost depends on location, number of beds, season, and so on.
So we'll turn to the topic of multiple linear regression, which allows multiple inputs.
Once we have those tools in place, we can see what we can do with non-linear dependencies
on multiple inputs, a pretty impressive challenge.
But to begin, let's start with a single input variable in relation to the output that's
decidedly non-linear.
Just like these.
The one on the left is what the graph of your bank balance might look like over time under
compound interest, or the number of people suffering from a new plague, or the number
of watchers of a new video on YouTube.
The one on the right is what a graph might look like of the charge left on your car battery
in hours after you accidentally left your lights on, or the amount of a drug still in
your system over time, or the new people that your current ad campaign is reaching date
for day.
What ties these things together is the idea of exponential growth and decay.
Exponential growth is going to show up any time that the rate at which something is growing
is proportional to the amount of that something that's present.
If you have twice as much money in the bank at the beginning of the year, you were in
twice as much interest during that year.
Exponential decay shows up when the rate at which something is shrinking is proportional
to the amount present.
If there are only half as many customers left for you to reach, your ads are only reaching
half as many new customers.
It turns out that this characterization is equivalent to saying this.
For exponential growth, the time taken for the quantity to double is a constant.
So for example, Moore's Law describes exponential growth.
It says that the number of transistors on a microchip doubles every two years.
Of course, there's nothing magic about doubling.
I could have equivalently said that the time required for the number of transistors to
quadruple is four years, or to the increased eight-fold is six years.
You can describe the process in terms of any base increase that you want.
For exponential decay, the amount of time required for something to be cut in half
is constant.
Remember half-life from radioactivity?
That's exponential decay.
Carbon 11 has a half-life of about 20 minutes.
No matter how much you start with, in 20 minutes, half of it will be gone.
But both of these ways about thinking about exponential growth and decay are equivalent
to the one that's going to be especially important to us today.
Namely, anything undergoing exponential growth through decay can be expressed mathematically
as y equals c to the ax plus b.
Here, y is the output, the quantity that's growing or shrinking, x is the input, and
a lot of models, that's time, and a, b, and c are constants.
You can pick a value for c, anything bigger than one is a good workable choice.
For example, look at y equals two to the x plus two.
When x equals zero, y equals two to the two, or four.
When x equals one, y equals two to the three, or eight.
When x equals two, y equals sixteen.
Every time x goes up by one, y goes up by a factor of two, and that's exponential growth.
And you can see why it's called exponential.
That variable stuff is up there in the exponent.
So many things follow this kind of hockey stick curve that you see in exponential growth
and decay.
We really want to be able to predict them.
Unfortunately, at the moment, our only prediction technique is restricted to things that graph
is straight lines, linear expressions.
In algebra, y equals ax plus b.
Oh, but wait, write that down.
What was the exponential?
Ah, so close.
That linear ax plus b is stuck up there in the exponent, darn it.
Ah, but we can fix that.
Any time you do algebra and want to solve for a variable, you always have to use inverse
functions, functions that undo the thing you're trying to get rid of.
If you have three x equals 12, you can divide both sides by three to get x equals four.
Multiplying by three is the inverse of multiplying by three.
It undoes the multiplication.
If x squared equals 100, and you want to get rid of the square, you can undo it by taking
the square root of both sides to get x equals 10.
In a similar way, you can undo exponentiation, such as two to the x.
Its inverse goes by the odd name of logarithm, or log, for short.
If you're working with a number c to some power, you undo the c to the whatever by taking
its logarithm, base c.
So if y equals two to the x, you can take the log base two of both sides and get y equals
two to the x, and therefore log two, log of, I'm sorry, log of two y equals x, okay?
So how does this help us?
Well, go back to the exponential equation.
Y equals c to the ax plus b.
Take the log base c of both sides.
But log base c undoes c to the something.
And so we've got ax plus b out of the exponent.
Yes.
You might be less excited.
Yes, I've got a nice linear expression on the right, but now I don't have y on the left
anymore.
I've got log of y.
Well, true.
But if y is a number that I know, and c is a number that I know, then log base c of y
is just a number two, one that I can find easily on a spreadsheet or a calculator.
So if you give me a bunch of values of x and y that are following an exponential distribution,
I can add a new column to your table, one that contains values of log y.
X versus y will graph as an exponential, but x versus log y will graph as a straight line.
And that means that if you start with x and y values that are close to an exponential
relationship, then x and log y will have close to a linear relationship, and that means we
can use simple linear regression to explore that relationship.
It turns out that this works for any reasonable c that you pick, anything bigger than one
will work, for example.
But most people who do this kind of work use a base that at first blush seems stupid.
It's not two or 10 or some other sensible seeming number.
It's a bit bigger than 2.71828.
It's a number called e.
You don't have to use e if you don't want to, but that base makes a lot more of more
advanced work, like work involving calculus, a lot easier.
So I'm going to follow that convention, and it really won't cause you any extra trouble.
No matter what base we use, you're going to need a calculator or a spreadsheet to find
powers and logarithms, and calculators and spreadsheets know all about e.
Your calculator probably has an e to the x key.
It probably also has a key for the log base e, which is also called the natural logarithm,
and usually just written ln.
We usually pronounce it lin, so log base e of x, or natural log of x, or the lin of x,
they all mean the same thing.
The only thing you need to remember is that lin and e to the something are inverses.
They undo one another.
Okay, let's see how all of this theory works in practice.
Let's look at the up and down, up and coming technology, photovoltaic cells.
In 2009, the European Union declared a target of having 20% of its energy production come
from renewable resources by 2020.
Let's take a look at the historical data from around that time and see if we can predict
what happens next.
Here's a graph of the data from 2005 to 2010.
It looks rather exponential, but if our data fell on an exponential curve, the relationship
would be y equals e to the ax plus b, where y is photovoltaic capacity, x is the year,
and a and b are constants to be determined to fit the data.
It's equivalent to lin y equals ax plus b.
It's the same thing we wrote a minute ago, but since I've committed myself to using base
e, the log on the left hand side is written as lin.
I'm going to let my spreadsheet compute the lin of all of the megawatt capacities that
we had in our last table.
Here's what the results look like.
I'm plotting columns in a salmon color this time.
That is on the vertical axis, I'm no longer plotting the actual capacity, but the lin
of the capacity.
I made another change too.
To make the numbers we'll be dealing with a bit more friendly, I've changed my horizontal
axis to record both the year and how much time has passed since 2005.
So year one is 2006, year two is 2007, and so on.
I'm going to use these smaller numbers as my x values in the work to come and look at
what happened to my graph.
Just as hope, the hockey stick is gone, replaced by points that lie quite close to a straight
line.
This transformation of variables working with lin y rather than y itself has moved the original
problem to a place where linear regression applies.
So let's do it.
Well, the r squared value is quite good, but the data sets small as well.
Let's look more closely examining the regression report.
The 95% confidence levels are fairly narrow.
In fact, the 95% confidence level for the true straight line looks like this.
And you can take this graph back to the original one by undoing the lin with e to the x inverse.
Here's the 95% confidence interval for prediction.
So for any year x, calculate 0.5271x plus 7.6068, then raise e to that power.
That gives y.
The prediction for photovoltaic capacity in that year.
Take 2011, for example, 2011 is six years after our base year of 2005.
So we need to use x equals six.
Our exponent expression works out to be about 10.77.
So punch e to the 10.77 into the calculator or spreadsheet, and you get about 47,500,
the red dot on our graph.
The actual European photovoltaic capacity in 2011 was a little higher, the purple cross,
about 53,000 megawatts.
Conversely, according to some sources in 2012 and 2013, total capacity was below what we
would have predicted, only 70,000 megawatts in 2012, only 82,000 in 2013.
The 2012 prediction is still quite close to the trend line, and the 2013 was just outside
the 95% confidence prediction band.
By the way, if you're using the spreadsheet, you can almost certainly get it to do this
transformation of variables work for you.
In Excel or Calc, for example, you can just ask for the exponential trend line directly
on your original graph.
It'll give you this.
Note the same r squared value that we just found before.
The equation looks different from ours, but mathematically it's equivalent, using laws
of exponents.
If you suspect that something may be showing exponential growth, you can do a quick exploratory
data analysis just by taking lin of the output variable and graphing it against the input
variable.
Doing so will turn exponential growth into a linear relation.
And it happens a lot.
Here's the US GDP shown on a log graph.
GDP is measured in billions of dollars.
A remarkable agreement with exponential growth.
So we've found a whole family of nonlinear relationships, exponential growth and decay,
that can be analyzed with linear regression by a simple transformation of the output variable,
taking its logarithm.
But there's another family of relationships, perhaps even more common, that will yield to
an extended application of the same idea.
Suppose we took the log of both the input and the output variable.
We'd be able to apply linear regression to the result if lin x and lin y actually do
have a linear relationship.
That is if lin y equals a times lin x plus b, or a and b are constants.
Then using laws of exponents and the fact that e to the x undoes lin, we can recover
the original relationship between x and y like this.
Raise e to the power of both sides, then simplify.
This is nothing more than laws of exponents, but when the smoke clears, we end up with
a relationship between y and x themselves, namely y equals e to the b times x to the
a.
e to the b is just a positive constant, so we're saying that y is proportional to some
fixed power of x.
A relationship where one variable is directly proportional to a power of another is called
a power law, and such relationships are remarkably common in sociology, neuroscience, linguistics,
physics, computer science, geophysics, economics, even biology.
The physical sciences are loaded with power laws.
Universal gravitation, gravitational force varies as the distance to the negative two.
Electromagnetic attraction shows the same variation.
In parallel motion, the distance an object drops near the earth varies with time as its
power to the plus two.
Tidal force, the force of the tide varies inversely as the distance to the power of
three.
The luminosity of a star varies as the fourth power of its temperature, and you can look
to see whether a power law is a decent description of your data just by taking the log rhythm
of both variables and plotting the results.
Here's a graph of our solar system on a log-log plot.
Log of planetary distance from the sun on the horizontal axis and log of the orbital
period on the vertical are squared equals one, an essentially perfect fit.
And see that slope of 1.5 on the regression line?
That means that orbital period is proportional to the 1.5 power of distance from the sun.
So Saturn is 9.53 times as far from the sun as earth, and so would predict a year for
Saturn to be 9.35 to the power of 1.5 years long.
That's about 29.4 years.
The actual figure?
29.5.
We've just discovered Kepler's third law of planetary motion without breaking the sweat.
There are even good fits to things that involve those most unpredictable of creatures, human
beings.
Here's one that blows my mind.
Zip's law.
Rank the words in English by frequency of use.
Number one is the, which makes up about 5.6% of the words that we write.
Of comes in second at 3.4%, followed by and, and so on.
Believe it or not, by the time that you get to 177 words, you've covered about half of
all of the words that we write.
But look at what happens when you plot the lin of the frequency of these words on the
vertical axis and the lin of their rank on this list on the horizontal.
The first several words on the list are off a bit, but overall the fit is incredible.
This is a graph of the frequencies of the 3,000 most common words in English.
The slope of the line is given by the regression equation is almost exactly negative one, meaning
that a word's frequency is inversely proportional to its rank in the list.
So the 200th most common word, find, is used about twice as often as the 400th most common
word, mean, and so on.
Our language actually works this way.
Even more amazing, it seems to apply to most other languages too, and many other kinds
of rankings as well, whether income rankings, the sizes of corporations, or the populations
of cities.
Recently, researchers have been looking into how power laws reflect the dynamics of cities.
Quite good power relationships can be uncovered linking crime rate, energy consumption, even
the number of gas stations to a city's area.
Similar relationships hold when describing statistics for various businesses as well.
So many relationships seem to follow a rough power relation, so many that researchers
being done as to why these kinds of connections should appear so often.
But whenever they do, a log-log plot can tip you off to it, and linear regression can
let you find the equation that fits.
Okay.
What about allowing more than one input?
Let's turn to the other topic of today, and in doing that, go back to linear relationships.
Each additional variable adds one dimension of space to our picture, so our best straight
line through the data picture needs to change a bit.
Let's imagine that we have two inputs, X and Y.
An easy way to imagine this is to imagine a tabletop.
Each point on the table can be identified with an X and Y coordinate, the inputs.
Now imagine a flat plane of glass hovering over top of the table at some angle in mid-air.
That glass records a linear relationship between the X and Y inputs with the height as the output.
A plane, like our flat plane of glass, is the two-dimensional analog of a straight line.
If even more inputs, the resulting graph would need more than three dimensions, and the plane
becomes an unbending hyperplane.
But the idea remains the same.
Let's stick with what we can see.
Okay.
Now paint some dots on the pane of glass.
Then take each of those dots and bump them vertically a bit, raising them or lowering
them off of the plane to hover in mid-air.
These vertical adjustments are the random fluctuations that real-date it takes on the perfectly linear
relationship.
Finally, take away the pane of glass, leaving only the dots in space.
The result might look like this.
The challenge of multiple regression is given only the dots come as close as you can to
figuring out where the pane of glass was.
Find the best linear fit to the data.
The mathematics of this plays the same game that we used in simple linear regression.
When you take a guess as to where the glass should be, each floating data point is going
to have a residual, a distance that lies above or below that guessed plane.
The best guess for the plane is the one that makes the mean squared error of those residuals
as small as possible.
Actually doing the math for this gets quite tedious.
The good news is that, again, statistical software or spreadsheets can do that work
for you easily.
If you're using the spreadsheet, Excel report has historically been more complete and easier
to read than open office calcs, but both can do the job.
And statistical software like R, which is free online, can do an even more thorough
job.
To see how this works, let's look at some data from a study of households in 1966 in
which the head of households had a low annual income, under 15,000 bucks.
The data sets old, but it demonstrates a number of important points so well that I think we
can overlook that.
The question was how the average hours worked during the year was affected by variations
in hourly wages.
To make my charts clear, I'm going to look at a set of 36 observations.
For each observation, we have the head of household annual income and the hourly wage.
But we also know the head of households age, earnings of the spouse and other family members,
the number of dependents, and the family's assets, bank accounts and so on.
We can start out with a simple linear regression looking at the connection between wage and
hours worked annually.
It looks like this.
A positive correlation with wage explaining about a third of the variation in hours worked.
Each additional $1 of wage adds on average just under 80 hours to the annual work time.
Although the regression report indicates quite a bit of uncertainty on that figure, the 95%
confidence interval runs from 39 bucks to 117.
But let's take a look at what happens if we create a model in which all of what we know
about a household is used to predict the hours worked by the head of house.
Now we're looking for a best linear fit between all of our input variables and hours
worked.
Doing this in a spreadsheet is hardly harder than doing a simple linear regression.
In Excel, we use the regression tool in the data analysis toolkit, specify where the data
for your input variables are, where the data for your output is, and it will generate a
report.
Regardless of what statistical software you use, you're going to get similar information.
Here's part of the standard report.
The value of R squared is being reported, and the interpretation that you're used to
is the same from linear regression.
When the fraction of the, while what fraction of the variation in the output, the work hours,
is explained by the linear relations we're proposing with the inputs.
It's 80% or so, which is quite respectable.
When working with multiple regression though, the second figure, the adjusted R squared,
is actually more important.
Here's why.
Suppose you make a prediction based on a bunch of variables that you have available.
Now, suppose I ask you to do the same thing again.
But this time, I provide you with one additional input variable, one additional piece of information
on each individual in the sample.
What would happen to the quality of your prediction?
Well, it might go up.
If the new information was an important piece of the puzzle, knowing it would let you make
a better prediction.
On the other hand, your best prediction couldn't possibly get worse.
If the new information was completely useless, you could just ignore it and get as good a
prediction as you got before.
On the other hand, suppose that the new information had some coincidental connection to the output
for the values in your data set.
Maybe it turns out that for your sample, people with phone numbers that ended in nine happened
to work a few more hours.
And including the phone numbers last digit among your inputs can make your model look
a little better.
In other words, even if you throw in the kitchen sink as a variable, the only effect it can
have on R squared is to make it go up.
And this is what the adjusted R squared adjusts for.
The more variables you throw in to the inputs without substantially improving the R squared,
the smaller the adjusted R squared gets.
So getting a high adjusted R squared is a target in multiple regression.
Well, we just made our regression by using all of the input variables that we had.
Was this a good idea?
To answer that, we need to look more carefully at the regression report.
Here's some more of it.
Using this part before in simple regression, the coefficients column tells you the equation
of the regression.
Here it's hours worked equals 2299 minus 4.46 age plus 0.02 spouse and so on.
But now I want to look at a new column, the one labeled p-value.
I'm not going to go into a formal definition of p-value, but the larger the p-value for
an input, the less evidence there is that the variable belongs in the regression.
A common threshold is 0.05.
We can see that four of the variables exceed that threshold, with the largest p-value belonging
to spouse, the annual income of the spouse.
That is, with the other inputs that we have, spouse doesn't seem to add enough to be kept.
Either spouse or income doesn't affect hours worked significantly, or other input variables
of the contribution covered.
So we're going to do our regression by stepwise elimination, that is, delete the entry with
the highest p-value and rerun the regression without it to see where we stand.
Okay, we take out spouse and rerun.
We get this.
The report also says that the adjusted r-squared is now 0.769, up a tiny bit from our old 0.764.
Dependence has the highest p-value well over our 0.05 threshold, remove it from the model
and see what happens.
The adjusted r-squared held at 0.769.
Our ages over our 0.05 threshold for p-value, get rid of it and rerun.
Before we do that though, I'd like you to notice the coefficient of wage.
It's about negative 54.
I'll come back to this point in a few minutes.
For now, let's rerun.
Okay, adjusted r-squared has dropped to 0.747, a little bit of a drop.
The wage now has a p-value above our threshold indicating that it is not statistically significant
to use the technical term.
We can delete it from our model and get this.
Again, the adjusted r-squared drops by a little bit.
It's now 0.731.
All the p-values are now extremely tiny, so we stop.
This last model says that to predict how many hours the head of household will work, begin
with 2120.5, subtract off a third of an hour for each dollar that a non-spouse member of
the household earns, and then add 0.019 hours for each dollar that the household has in
assets.
We can look at the graph showing how close to accurate the predictions of this model
are.
Here I've made a scatter plot with predicted hours on the horizontal axis and actual hours
on the vertical.
A perfect prediction would have the actual and predicted hours identical, so all the
dots would be on the diagonal line.
Our worst error was about 70 hours, or about 3.5%.
If we use the model tied for the highest adjusted r-squared and four inputs, we get a slightly
better fit, but not much.
We can choose the simplicity of the model with two inputs, or a little more accuracy
in the prediction with four inputs, at least as far as the sample data goes.
But there remains the question of what all this means.
Our original graph, simple linear regression of hours worked on wage, showed a positive
connection, with the line suggesting that each additional $1 an hourly wage adds about
78 hours in work.
The four input model had the wage being significant with a negative coefficient, which means that
higher wages result in less hours worked, and in the final model, wage doesn't appear
at all.
It has no direct impact on hours worked at all.
So which one is it?
Well, that's a good question.
The first graph was right, knowing nothing else, higher wages correlate with more hours.
That doesn't necessarily mean that higher wages cause people to work more hours.
For all we know, people who work more hours are more likely to be paid well.
But they do go together.
Then why did the second model say that increased wages lead to fewer hours?
Well, you have to remember what the other variables are in the model.
For example, assets shows a high correlation with wages, an r-squared of about .63.
Again, causality is not specified by regression, but it seems reasonable to me that people
who make more per hour can save more.
In any case, if I know what your assets are, I know a fair amount about your wage.
So even if you're working more hours because of wages, I might be able to explain it in
terms of your assets, which are strongly tied to your wages.
In a model including assets, wages might be completely unnecessary, as was the case in
the last model, or if assets gets a larger coefficient, wages might actually show a negative
modifier, as it did in our four-input model.
The coefficient of a variable in a model is intended to capture the effects of that variable
if all other inputs are held fixed.
You can start to see why when two variables measure almost the same thing, it's often
a good idea not to include both in your model, which one gets credit, can be an issue.
This is a special case of the problem of multicollinearity.
Okay, we've got time left for one more clever and useful variant of linear regression.
Suppose you've got bivariate data that suggests a nonlinear relationship from your scatter
plot and that your take-the-log transformations can't tame it into a straight line.
It turns out that multiple regression gives you a way of fitting a polynomial to the data.
This is sometimes called polynomial regression.
Let's take a look at an example.
We've tried a number of different prices for our product and the revenues from the resulting
sales were recorded for a number of months.
It's pretty clear this isn't a linear relationship, but it might fit on a parabola, which is a
function that has not only x in its formula, but also x squared.
y equals ax squared plus bx plus c, if we can find the right values of a, b, and c.
Well, we can trick multiple regression into doing exactly that.
We tell it that we don't have just one input variable x, but we tell it we have two, x
and x squared.
Yes, we know that if you know x, you know x squared, but multiple regression doesn't.
It just treats x squared as a new variable that just happens to have the name of x squared.
So if our first data point is x equals 15, y equals 126,000, we tell multiple regression
that the inputs are x equals 15, x squared equals 225, 15 times 15, and y equals 126,000.
We feed the multiple regression algorithm with this data and it comes back with the coefficients.
In this example, the equation is approximately 10,000x minus 100x squared with a very impressive
r squared of over 0.97, and based on this, we could estimate that we should be charging
around 50 bucks if we want to maximize revenue.
You might be attempted to include higher powers of x's inputs too, x cubed, x to the fourth,
and so on, but be careful, don't use more than you need.
Remember the kitchen sink effect that we talked about with adjusted r squared?
Here it can show up with a vengeance in the guise of overfitting the data.
Look at this example where x is hours worked and y is units produced.
We can fit a linear model of this and get a pretty good fit.
The line says we'd expect an additional 16.5 units per hour of work on average, and then
r squared is a very respectable 0.95.
Now let's do it again, but not only with x number of hours worked as the input, but
also the powers of x up through x to the fifth.
Here's the result.
The wiggles in a fifth degree polynomial make it possible to fit the six data points perfectly.
R squared now equals one.
But does anyone actually believe that this snake is a better explanation of the data
than the straight line?
For example, it suggests that beyond 5.8 hours of work or so, total units produced plummets,
while with less than one hour of work, it's skyrockets.
With zero hours of work, the model predicts 416 units produced.
There's a lot going on with multiple regression and some pretty sophisticated math that supports
it.
The topic of linear models can and does fill a graduate level course in statistics.
But with what we've covered today, you have the tools to do a great deal of powerful and
potentially useful analysis and prediction.
