Lecture 29 â€“ Values and Objectivity
We saw last time that philosophers who adopt a naturalistic approach are willing to allow
results of science to help settle questions about science.
In this respect, philosophers are following a path that was suggested by Kuhn and followed
by others, away from the idea of philosophy as a radically unempirical discipline, primarily
as a conceptual discipline.
This also brings philosophy closer to the approach of the sociologists of science that
we saw back in Lecture 17.
They rejected conceptual analysis, the idea of a logic of science, and the focus on normative
questions construed as sort of abstract or unrealistic in their view, and ungrounded
in empirical work.
So the sociologists put themselves forward as replacing conceptual analysis as the way
to approach normative questions about science.
Naturalistically inclined philosophers have generally thought that the sociologists overreacted
against the stringency of logical positivism.
They generally held that the sociologists were wrong to deny the relevance of notions
like truth or objectivity to a naturalistic explanation of science and its success.
In this lecture we examine naturalistic approaches to the social structure of science and their
consequences, approaches that at least stand some chance of vindicating some of science's
epistemic ambitions.
But we should realize at the outset that it seems undeniable that social factors, money,
prestige, political and economic interests, have often loomed large in the actual practice
of science.
So the sociologists weren't wrong to insist on that.
It's often been implicitly assumed that these social aspects compete with norms of rationality
and objectivity that also figure in explanations of scientific conduct.
Few people come out and claim that an explanation in terms of social factors precludes one
in terms of epistemic factors or vice versa.
But both those who favor the epistemic explanations of scientific behavior and those who favor
social explanations have tended to act as if their kind of explanation is the only one
that really matters.
So as we saw the positivists tended to think that social factors distort the objectivity
that would otherwise result from the application in a disinterested way of the scientific method
at least within the context of justification.
Psychological and social factors can operate within the context of discovery.
This can count as one extreme for our purposes in this lecture.
For many of the sociologists of science, on the other hand, appeals to evidence and logic
function mainly to mask the operation of non-evidential interests and of biases.
And it's these interests and biases that constitute the real explanation of scientific
conduct.
This can serve as the other extreme.
We've also seen a position in between these two views in the work of Thomas Kuhn for whom
social aspects of the organization of science can aid rather than impede the rationality
of science.
Even held, for instance, that the relatively permissive norms that govern during scientific
crises combined with social and psychological idiosyncrasies of individual scientists leads
to a beneficial distribution of workers and resources.
It's a mechanism by which science manages to have some people pursuing the old paradigm
and others trying to develop new ones, which is exactly what you'd want during a time of
crisis.
That work in naturalized epistemology and philosophy of science has more or less followed
Kuhn in developing a position according to which the social and epistemic norms can cooperate
rather than compete.
But they followed the sociologists of science in thinking that even normal science is significantly
governed by non-epistemic factors, while also following the logical positivists and some
of their fellow travelers in thinking that science is nevertheless, for the most part,
epistemically distinctive.
It's a peculiar combination of views, and our task is to try to see how they hang together.
Everyone will admit that it can be disastrous for science to be driven by ideology.
Lysenkoist biology under Stalin is perhaps the clearest example.
Even somebody who thinks that all science is driven by ideology can grant that some
ways in which science is driven by ideology can be particularly harmful.
It's much less clear that ideology is necessarily epistemically harmful to science.
Let's consider an argument due to a philosopher of science named Peter Railton, who I should
admit was one of my teachers, so perhaps I'll be too easy on this argument.
Suppose Railton says that an old-fashioned Marxist critique of science was entirely on
the money.
Science relentlessly serves the interests of industrial capitalism.
And what could be more ideologically driven than that?
Nevertheless plausible that such ideology-driven science would be highly reliable.
Why?
Industrial capitalism highly values accurate information about the empirical world.
Capitalism provides strong incentives for individuals to formulate new hypotheses about
how the natural world is structured and how objects will behave.
It provides strong monetary incentives for testing these ideas and, under conditions
of copyright and intellectual property and stuff like that, for propagating those ideas.
Now the term like objectivity is a notoriously tricky term.
It often means something like disinterestedness.
And on this Marxist hypothesis, natural science would be anything but disinterested.
But a process is also sometimes called objective when it is mainly determined by the objects
it says it's about, whether or not agendas or biases play a role in that determination.
And it's plausible that the biases and incentives of industrial capitalism motivate beliefs
that are determined by their objects.
One is biased to get the world right, not out of some disinterested desire for knowledge,
but nevertheless the bias serves to help one get the world right.
As Railton would be the first to insist, the story is not as simple as I've made it sound.
It's only going to be plausible where there's the right sort of feedback mechanisms with
nature so that if my hypothesis is wrong, somebody else will have the opportunity and
plausibly the incentive to correct it.
And one might well suspect that such feedback mechanisms are going to be lacking, at least
to some extent, in a discipline like economics.
It's not clear that there's a causal interaction between us and the objects of economic inquiry.
One might further expect, at least if we continue assuming a Marxist perspective for the sake
of argument, that ideology will provide less incentive to have our ideas produced or caused
by the objects of inquiry in economics.
According to the Marxist anyway, capitalism has reasons to hide economic truths from itself,
though it doesn't have reason to hide, say, geological truths from itself.
So this case will work quite differently from cases in the natural sciences.
Similarly, to some extent anyway, Railton's invisible hand argument about the epistemic
status of science will be subject to the same kind of limitations as more classic versions,
like Adam Smith's famous invisible hand argument.
Too much power concentrated in the wrong hands can interfere with the desirable mechanisms
of the system.
If a company or individual can effectively squelch competition, the incentive structure
starts to wobble, which is why government regulation against monopolies is provided
in classic capitalism.
And perhaps such examples as corporate sponsored science aimed to cast doubt on global warming
suggest that, after all, ideology does lead to distortion in the long run, and we could
multiply examples by looking at, say, biomedical research.
So it would be too sanguine to claim that ideology and objectivity are never in tension
with each other when science takes place within a broadly capitalist economy.
But the weaker claims the really noteworthy one here, namely that it's not automatically
the case that there's a tension between ideology and objectivity.
And of course, this point doesn't depend on facts about capitalism and Marxism.
For a naturalistically inclined philosopher, it's an empirical question to what extent
a given kind of scientific bias distorts, and some biases that are not at all epistemic
can be epistemically beneficial.
Similarly, ideology is not the only non-epistemic source of potentially beneficial epistemic
effects.
One could argue that the reward structure of science, on the whole, has epistemically
welcome results.
Scientists get rewarded with things like prestige, for having their ideas cited and used.
This encourages finding original results so that you can publish them and get prestige,
and also it includes making one's ideas available to others.
Those seem like good things.
Since others use one's ideas by relying on them in their own work, it creates some,
albeit quite imperfect, pressure towards testing and replicating the results of others.
Ideas can get tested both through a kind of cooperation.
Those who want to rely on one's ideas might test them either before or during their own
work, and also through a kind of competition.
Those whose work is threatened by one's work, in various ways, have an incentive to see
whether the result was properly arrived at, or whether, say, your experiment was badly
constructed in some way.
We don't want to sound polyanna-ish about this, the reward system also tends to encourage
things like plagiarism and fraud.
It also encourages protecting oneself against such abuses, so it's plausible to think that
such problems only occur occasionally.
The reward system of science also has a tendency to promote a reasonably healthy distribution
of scientific labor.
If a great many people are pursuing the most developed and the most promising research
project, it can be rational for other scientists to pursue alternative projects.
This is because rewards and credits are, at least to a first approximation, inversely
proportional to the number of people working on a project.
The more people who produce a successful result, the more the credit has to be shared.
So it might be reasonable, for purely selfish reasons, to devote part of one's career to
a long-shot project because the payoff is bigger, though perhaps less likely.
This is an application to normal science of a point Kuhn had made with respect to revolutionary
science.
Here we're not relying as Kuhn had on randomizing factors, as Kuhn appealed to individual idiosyncrasies
to explain distribution of scientific labor across revolutions.
The explanation here is more structural, it's more built into the reward system.
But the picture is similar in that choices at the individual level are supposed to make
for a good distribution of labor at the level of the science as a whole.
To some extent, anyway, self-interest at the level of individuals can thus lead to a community
that functions in a more or less disinterested, inquiring manner.
The desire to have one's work shared encourages sharing of information, but competitive factors
and a desire not to have one's work dismissed because it relied on shoddy work by other
people encourages criticism.
Once again, this may be too comfortable a picture, at least in certain respects.
Money and similar incentives are starting to play a much larger role in science than
they have in the past.
Issues like the corporate sponsorship of research complicate the reward system model considerably.
It's commonly claimed, for instance, that 10% of biomedical resources go towards diseases
that account for about 90% of human suffering.
A lot more money is spent on hair loss and erectile dysfunction than on tuberculosis.
Note that there is no way of distributing resources that's going to count as free of
some kind of interest or agenda.
The humanitarian agenda in biomedicine isn't a purely scientific one, whatever exactly
we might have in mind, with a phrase like a purely scientific agenda.
There are theoretical questions that matter to science without having any clear practical
payoff, but we shouldn't confuse that with the idea that it's something is value-free.
The talk of mattering suggests that even when science is free of practical agendas, it's
not a value-free enterprise.
We should, I suggest, try to clarify and defend the values that animate our research, not
pretend that our research is animated by no values at all.
This is a common, though not an uncontroversial opinion.
So it's plausible to argue that there's something epistemically benign about the reward structure
of science, but on the other hand, it hardly seems unimprovable.
This is another way in which a naturalistic approach to philosophy of science can raise
normative questions.
We could, perhaps, set up a reward system that makes science more objective, more efficient.
So far, we've looked at non-epistemic interests built into the reward structure of science,
but this is not the kind of non-epistemic interest that gets people's attention, that
gets them exercised.
The high level of volume attained during the science wars of the 90s concerns such matters
as the role of political, in a broad sense of political, values in science.
So let's return to the issues of ideology.
It's these kinds of non-epistemic factors that get people worked up about whether science
is objective or not.
Just about anybody will have to admit that prominent scientific work has sometimes been
rather embarrassingly influenced by political or religious ideas and by conceptions of
gender and race.
To take just one example, some really impressively tortured arguments were invoked about skull
and brain size during the 19th century to make sure that it turned out that males of
European origin came out ahead on whatever measurement would most matter, because it
had to be correlated with intelligence somehow, and what the physical basis of intelligence
was could vary as long as it ended up favoring the right people.
It started off being absolute mass of the brain, that it was body ratio, because it
couldn't end up that women or people of not-of-European origin would finish ahead
on some ratio that mattered.
Along with the political kinds of considerations that sometimes affect the judgment of scientists,
we should note that things like birth order, at least according to an influential recent
book, shape the intellectual personalities of scientists in striking ways.
It suggested that first-born children prefer sort of order, and second-born children are
much more adventurous, attracted to different hypotheses than their older siblings are.
These sorts of factors presumably figure differently in different sciences.
Certain masculinist assumptions are widely cited in the biology literature, perhaps
most famously in the Sleeping Beauty conception of conception, according to which the sperm
cell is struggling heroically upstream, looking for an egg to fertilize, while the egg is
just sort of hanging out waiting for something to happen.
This does not accurately represent the biological facts, but it's a way that at least I learned
how human reproduction worked back in 10th grade.
Similarly within primatology, some feminists have worried that the alleged sexual passivity
of female primates is much exaggerated by some observers.
In physics, one would expect to find less directly political cases of influence, though
the case has been made that women tend to see things as more interconnected than men
do, and so take different hypotheses more seriously than men would.
Some feminist scholars, on the other hand, think that this is little more than a stereotype
of women and women scientists in particular that oughtn't to be dignified by a place
in the discussion of scientific objectivity.
One also might expect to find a fairly large role played in physics by aesthetic considerations
of simplicity and explanatory elegance.
So matters of intellectual style of a sort emphasized by the birth order hypothesis about
scientific personalities might matter a good bit more in physics than say in biology.
The nature and significance of these cases needs to get examined on an individual basis.
Our interest in this lecture is in the strengths and weaknesses of the social structure of
science at handling potentially distorting factors.
We don't have time to look into, in a given case, what sorts of factors there are and
how distorting they are.
Individual scientists are often capable of great objectivity, but few of us are aware
of all of our major biases.
Such protection from distortion as science possesses really rests less on finding impartial
judges of evidence than on bringing a range of critical perspectives to bear on how people
process the evidence.
Ideally, we'd want as wide a range of perspectives as we can able to bring relevant criticism
to work on how evidence gets interpreted.
This is a point that had a while ago been emphasized by Fireobbent, though most people
since Fireobbent approach it less dramatically than he did.
You can't approach things more dramatically than Fireobbent had.
The idea is that the use and give credit system can help sort out idiosyncratic interpretations
of data, misleading descriptions of experimental results in the service of political agenda
or some unexamined bias or something like that.
This immediately raises questions about the diversity in terms of gender, in terms of
age, birth order, politics, style of intellectual training that characterizes the scientists
in a given field.
Again, ideally it seems you'd want as much variety as you can get, at least with respect
to any features that seem relevant to the field in question.
So gender might loom larger if you're trying to put together a research team investigating
the sexual behavior of chimpanzees than it might if you're doing string theory.
Again, even here you have to wonder whether women as such are supposed to have anything
deep in common or whether this is a kind of stereotype.
These are hard questions.
The point of this approach, though, is not so much about head counting.
It's about how effectively criticism can be brought to bear in a field.
This is an important issue about scientific objectivity.
It crops up in many ways, and the consensus seems to be that a naturalistic approach shows
that science does pretty well, but it could certainly do better.
Science could be more objective than it, in fact, seems to be.
The questions to ask about objectivity within a field tend to have a kind of paparian flavor.
They're about the ways in which a field genuinely, not just nominally, opens itself up to criticism.
Does the field have good conferences and journals, where good here means that ideas
are really exchanged rather than self-congratulatory speeches given?
Are established people taken too seriously and young scientists not seriously enough?
Or perhaps vice versa, as sometimes happens when a field gets taken over by a new trend?
Is criticism really valued within the scientific community?
It's often not prestigious to be a good editor of a journal or a good referee of journal
papers.
But some have claimed that that's as important as original research is to the health of a
field, though that's not necessarily what deans look at when they're looking to hand
out promotions and raises.
We can also worry about whether there's a good ol' boy or good ol' girl network,
the members of which manage, and I'm not suggesting that this often happens intentionally, to
kind of credential each other by publishing articles, writing letters of support, etc.
While again unintentionally insulating their shared views from criticism, they are appealing
to their own standards of good work and not listening to other people and they have enough
institutional heft to make that a self-sustaining kind of program.
That sometimes happens.
Do the practitioners in a field generally conduct themselves in a way that discourages
those with different training or different approaches from joining or remaining in the
field?
One can also worry about whether the standards of rigor within a field are, in fact, deeply
connected to producing good work, or are some of them gratuitous, as we sometimes find in
philosophy, where you have to show how much logic you know, even if it's sort of incidental
to making your point clearly.
You don't want standards of rigor that are just designed to show that you're smart
or tough.
That excludes people who might have something valuable to contribute.
Given all of these issues, along with the more straightforwardly political ones, almost
any field or subfield will fall short of some ideal of perfect objectivity.
But there's also a major worry from the other side, more as it were from the Kunian wing
of philosophy of science than from the Paparian wing.
As we saw, Kun emphasized the importance of shared belief, of a shared paradigm, to the
success of normal science.
And by trying too hard to increase objectivity, we raise a kind of white noise problem.
Diversity of background and opinion is great in terms of bringing criticism to bear on
views, but it has costs as well as benefits.
It gets in the way of actually doing research.
How much attention are evolutionary biologists really supposed to pay to creation scientists?
We saw that even if we can't settle the demarcation problem, there are limited resources available
for scientific research and discussion.
A given science has rough and ready gatekeeping criteria, but there are legitimate worries
that these can be too permissive, that they can let in fringe views that are a distraction
from getting on with the business of science, and that they can be too restrictive, that
they decrease scientific objectivity needlessly by silencing voices that could be added to
the conversation.
So it's worth asking to what extent science can have the best of both worlds.
Can we have Kunian normal science with its shared vocabulary, shared approaches to how
questions should be asked, along with a Paparian openness to criticism?
Might there be a way of setting up the incentive structure of science so that it works more
effectively?
Often people who are political conservatives, who have no sympathy with socialism in politics,
have sometimes envisioned science as a kind of socialist paradise about intellectual matters,
with everyone contributing to the common good.
That's a more plausible picture about science, some have thought, than it is about economics.
One could try to make that case by appealing, for instance, to a naturalistic investigation
of the motives that draw people into science, but the case needs to be made, not assumed.
So far we've asked questions about non-epistemic factors that, in some sense, are internal
to science, how science is governed from within.
But questions about the values and the social structure of science loom even larger when
we turn our attention to science's role in society at large.
For simplicity's sake, let's focus on publicly funded science.
Privately funded science seems to have some legitimacy for serving the narrower interests
by which it's funded, but it comes to figure in the public sector when a privately financed
proposal gets put forward as some kind of scientific truth or is appealed to to guide
public policy.
Recent years, this is a major development in how universities are organized, have seen
a sharp rise in public-private partnerships where it's not clear whose interest the scientific
research is supposed to be serving.
Sometimes that's a good thing, sometimes a bad thing one might think.
Now it's sometimes claimed that science itself is morally neutral, that it's a pure kind
of inquiry, and it's only technological applications of science that raise moral questions.
This seems at least somewhat too simple.
There seem to be some cases of scientific experiments, like those performed by Nazi
doctors on victims in the Holocaust, where it's pretty clear that there's no gain remotely
proportional in scientific terms to the suffering that is to be expected, and it seems quite
cavalier and dishonest to appeal to some notion of pure inquiry in such cases.
That's a particularly grotesque example that doesn't falsify perhaps a more modest claim
of scientific disinterestedness and objectivity.
But even in the less grotesque cases, we might not want to distinguish basic science too
sharply from technology.
To what extent do scientists have an obligation to reflect on the likely uses of their research?
It's one thing to make an argument that it was morally appropriate to work on atomic weapons
for the United States during World War II.
That's a complicated issue about what Germany was up to in saving lives that would otherwise
be lost in various places.
Difficult complex matter.
That's quite different from trying to make the argument that the pursuit of knowledge
itself was justified and that the moral consequences can be left entirely in the hands of those
who will apply one's research.
Maybe someone can make that case, but we generally hold people who play a crucial role in a questionable
process to some standard of reflecting on whether it's appropriate to play the crucial
role in that process.
We expect people to at least ask, we're prepared to accept their answers, but they need to
at least ask whether it's morally appropriate for them to be involved in the research they're
involved in.
Now the case can be made from the other side that certain kinds of knowledge anyway are
valuable for their own sake.
It's kind of crazy to think that knowledge as such is always valuable for its own sake.
Knowledge of the number of hairs on my head doesn't seem to be valuable at all.
But arguably some knowledge is valuable for its own sake, and in addition, lots of knowledge
has unexpected applications, both for better and for worse.
When scientists want huge sums of money for superconducting supercolliders and things like
that, they often don't have a clear sense of any practical benefits besides the knowledge
that they think is itself scientifically valuable that's going to arise from the work.
But nobody expected computers and the internet to arise from some really quite abstract work
in the logic of computation either.
So there's a case to be made that the open-ended pursuit of knowledge tends to have good consequences.
If consequences of scientific work are generally unpredictable, the traditional view shields
scientists for moral responsibility.
Their job is just to get knowledge and to leave it to others to decide what is to be
done with the knowledge.
That's the traditional view.
There are lots of interesting test cases for views in this neighborhood.
There's some decent evidence, for instance, about how data from the Human Genome Project
might well get used.
It doesn't seem right to hold biologists responsible if people in 20 years are denied
jobs or health insurance on the basis of a genetic profile.
That's not the biologist's fault, but nor does it seem entirely irrelevant to raising
questions about to what extent one wants to be involved in research that has a decent
chance of being used that way.
They're also important issues about how scientists obtain their data.
In our country, if people participate in a medical study you're running, it's generally
accepted that you owe them the highest standard of medical care.
To some extent research is done in other countries for purposes of avoiding this expensive burden.
So one might worry, on the one hand, that we're using economically disadvantaged people
as guinea pigs, taking advantage of already huge inequalities in order to make our own
medical research cheaper.
On the other hand, there's a case to be made that we're giving these people better medical
care than they would otherwise get if the study hadn't been conducted in their country.
Students of science generally leave these issues in the capable hands of ethicists.
Philosophy of science, as such, does not have the resources to bring the right theories
to bear on these questions.
Finally, we can note some difficulties about scientific decision-making.
We might recall that Fireobin thought science was becoming a threat to democracy.
Why?
Because non-scientists have to rely on scientists in order to ascertain the scientific significance
of a proposal like the superconducting supercollider.
But the scientists have an interest in, as it were, overselling the importance because
they want to build the superconducting supercollider.
Scientists don't seem similarly beholden to non-scientists to determine social significance.
We don't have experts on what's socially important.
We let each citizen have a full say in those matters.
But scientists claim, we think rightly, more of a say about scientific significance.
So how are we to balance these norms?
Who should decide whether the superconducting supercollider gets built, and how are citizens
supposed to weigh in on the social significance of such a project given that they lack the
scientific expertise that seems required for doing so?
The challenge is to set up a social structure that has the best chance of generating suitably
informed judgments simultaneously of scientific and of social significance.
This is a challenge that's really only recently been taken up in a kind of theoretically serious
way about what kind of social structures might lead to the right kinds of decisions.
Next time we turn to a new topic, that's really an old topic.
Probability has glancingly figured in some of our earlier discussions.
But in order to see what's currently central in philosophy of science, we need to make
probability a theme rather than a side issue in our discussions.
