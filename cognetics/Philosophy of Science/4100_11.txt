Lecture 11. Some solutions and a new riddle.
We left off last time in a bind. Induction looks hard to justify, and it looks hard to
do without, both in science and in everyday life. Today, we see whether we can wriggle
out of that bind.
One suggestion is that Hume's skepticism about induction runs afoul of a straightforward
mathematical result. This would be nice because it would be nice to draw on something as solid
as mathematics to help convince ourselves that it's okay to think the sun will come
up tomorrow, that we have decent evidence for thinking that the sun will come up tomorrow.
For simplicity's sake, we've been focusing on traits that, at least as far as we know,
are uniformly present when they're present in a population. We think that 100 percent
of copper conducts electricity.
But this is a special case. The general issue about induction asks what reason we have for
holding that any pattern will continue to hold up an experience. And such patterns can
concern the frequency of a trait in a sample being anything between 0 percent and 100 percent.
So we need to generalize the kind of on-off discussion of a trait to the statistical notion
of a trait, as we did in our discussion of Mill's methods. So we could ask what reason
we have for thinking that a certain sample of people's opinions about, say, the president
accurately represents public opinion. Why think that a 30 percent approval rating in
the sample indicates a 30 percent approval rating in the population? So now we don't
have to talk in terms of just 0 percent and 100 percent representation of a trait in our
sample.
And this helps bring about an often overlooked point. Inductive inference is from examined
to unexamined cases. It needn't be from past to future cases, though it's often convenient
to characterize it that way. It's an inductive inference to go from a 30 percent representation
in a sample to the representation in the whole population at a time. We're not talking about
the future, we're talking about unexamined cases. Now we need this more general form
of induction so that we can state a mathematical theorem called the law of large numbers.
Uniformly presented, as it will have to be because I'm no mathematician, it states that
by taking a large enough random sample of a population, we can attain as high a probability
as we would like of coming as close as we would like to knowing the frequency of a trait
in the population. This will become clearer as we proceed. But let's warn against a common
misunderstanding of the law of large numbers. The law does not require that we sample a
high proportion of the population. It's the absolute size of the sample that matters.
And this is how things like polling actually work. Now what do we mean by a random sample?
That's one in which each member of the population has the same probability of being chosen to
be in the sample. Elementary statistics books are full of examples of biased sampling, which
just means non-random sampling. It doesn't mean somebody is intentionally spoiling the
sample. It just means the sample is not chosen in such a way that all members of the population
have the same chance of appearing in the sample. So to take a classic example, if you were
taking a political survey during the Depression, using the telephone is not a random way to
reach people, even if you choose numbers at random. Too many people didn't have telephones,
and so poor people, people from certain geographic regions of the country, would be underrepresented
in the sample. So it's a biased rather than a random sample.
With that clarification behind us, let's notice that this looks like a great way out
of Hume's problem. Because the law of large numbers suggests that induction is pretty much
guaranteed to work. Just keep making your sample bigger, and you'll get as close as
you like, with as high a probability as you like, of getting the frequency of the trait
in the whole population. It might be expensive, it might be inconvenient to get the right
sort of sample, but we're not looking for a cheap way out of Hume's problem. We will
settle for some reason for thinking the sun is going to rise tomorrow.
Unfortunately there's a catch. The law tells us that if we have a random sample, induction
will work. But there are two problems. First, we have no particular reason to believe that
our sample is random. We do in fact believe that we have taken a suitably large and varied
sample of copper. But that belief is itself supported by a kind of background trust in
our inductive practices, what counts as variety in places that copper shows up, for instance.
But that background knowledge is precisely what's at issue when we're trying to figure
out whether we can legitimately make inductive inferences. So the analogy here would be to
try to convince yourself that you're taking a random sample of ordinary Americans without
relying on any information at all about where Americans live, how they spend their time,
whether they go to the mall or have computers or telephones. If you knew nothing about the
people from whom you were trying to get a random sample, what method could you possibly choose
that you'd have reason to believe was random? In addition, we have some reasons to think
that our sample is non-random. We saw that Russell's chicken thought it had, presumably
a suitably large and varied sample of situations in which the farmer and chicken feed were
correlated. From our perspective, though, we can see the limitations of the chicken's
worldview. The farmer has a much broader perspective of where the chicken's life figures in the
great scheme of things. Similarly, when we make a scientific claim about copper and electrical
conductivity, we're not making a claim about early 21st century earth copper. We're making
a claim about copper everywhere in the universe, and at all times throughout history and into
the future. Looked at that way, our samples of copper look an awful lot like the chicken's
samples of farmer food correlations. Our samples look tiny and non-random when looked at from
the point of view, as it were, of the universe. So the law of large numbers doesn't look
like it's going to provide an adequate solution to Hume's problem, since we have no good
reason to think we have a random sample, and some pretty good reasons to think we don't
have a random sample. So our next solution is what's called the ordinary language solution
to Hume's problem. This, we don't need any mathematics for, it just says that accepting
some inductive arguments in favorable conditions is part of what it means to be rational. Our
notion of a reason, according to the proponent of the ordinary language defense, is such
that the sun coming up every morning of my life, and never failing to come up, just is
a reason for thinking it's going to come up tomorrow. This is what we mean by a reason.
As a result, asking why it's rational to think that the sun will come up tomorrow is
like asking why it's rational to be rational. So the argument says that in asking whether
it's rational to trust induction, we're asking whether it's rational to be rational.
So Hume's challenge, which seems so difficult to meet, is just a matter of learning how
to say yes. Is it rational to be rational? Why yes it is.
So developing this response in more detail, the idea is that the fact that we can't defend
induction without, as it were, cheating by appealing to induction shouldn't bother us
anymore than the fact that we can't defend deduction without using deduction. But we
don't think that casts any doubt on the legitimacy of deductive reasoning. It's not fair for
somebody to ask us to show, without using any reasoning at all, that reasoning is OK.
That's not a challenge we're under any obligation to meet.
So what Hume has shown, according to the proponent of the ordinary language solution, is that
induction, like deduction, is basic to our notion of rationality.
This means that there's nothing more fundamental to which we can appeal in order to defend
induction. But that doesn't mean that induction isn't rational. It means it's baseline or
fundamental rational. There's nothing deeper to make it rational. It just is.
Now I've tried to do right by this objection, in part because I think it articulates the
frustration many non-philosophers have with Hume's challenge. Hume seems like he's asking
for something that it's not legitimate to ask for. But I think that feeling is mistaken.
And in order to see that, we need to distinguish two different senses of the word rational.
Let's grant that the ordinary language solution shows that there is nothing more fundamental
than induction, in terms of which induction can be justified.
What this means is that induction can't receive a certain kind of backward-looking justification.
It can't be derived from any more basic or more fundamental principle. Philosophers
sometimes express this by saying induction can't be validated. It's basic or fundamental.
But the objection leaves intact the question of whether induction can get a kind of forward-looking
justification. An explanation of what induction is good for, rather than a derivation of it
from something more fundamental. So we might still be able to ask, why is induction a good
way of getting at the truth? Why is it a good way of making predictions? And this is a different
question than asking, in terms of what more fundamental principle can induction be established.
Philosophers sometimes call this a vindication of a principle, rather than the earlier validation
of a principle. And on this issue, there does seem to be an impressive difference between
induction and deduction. We can show to our own satisfaction how deductive arguments do
what they're supposed to do. We can show how they preserve truth and clarify thoughts.
Why? Because we can make explicit tables showing how the logical connectives work and show
that if you mean by these terms what we mean by them, they won't let you go from true inputs
to false outputs. We can make absolutely explicit how our thinking about deductive reasoning
works and why it works. We don't, however, have an analogous understanding of why induction
does what it's supposed to do, namely extend our knowledge to unobserved cases. And when
we, as it were, step out of our skins and look at it, it can seem kind of miraculous.
They were able to go from such a small sample to such a grand conclusion.
Hume had emphasized that every event is separate from every other event. Which raises the question,
why are we entitled to link events as we do? Do we think that past pieces of copper constrain
what future or unobserved pieces of copper can do? How is it that we're latching on to
something that lets us go enormously beyond our current experience? So I think it's an
injustice to Hume to think of this as just a case of philosophers trying to figure out
what they can doubt next. Doubts about induction are motivated in a way that maybe doubts about
deduction are not. So we're in trouble. Induction is hard to defend. Our next solution, the
pragmatic vindication of induction, lowers its sights a little bit, perhaps wisely. It
does not try to show that induction will work. It settles for something weaker. It says induction
will work if anything will work. That's an important weakening.
The argument defends the generalized version of induction that we started with today, this
statistical notion of induction we talked about with the law of large numbers. It's sometimes
called simple enumerative induction and it's also sometimes called the straight rule. It
just says that you should infer that the entire population has a trait in whatever proportion
your sample shows the trait to be. So we're just making explicit this statistical notion
of induction we've been relying on. Essentially what this does is directs us to assume without
argument that our sample is random. Well, it's okay to assume it as long as you're not claiming
to be justified in the assumption. And the idea is induction works if by using this method,
by projecting the characteristics of our sample onto the whole population, we can get close
to the right answer. Now the argument does not assume so much as that there is a proportion
of the trait in the population as a whole. This is a tricky but important point. There
may be no fact of the matter about what proportion of copper in the universe conducts electricity.
How could that be? It may be that the proportion of copper that conducts electricity fluctuates
widely without ever settling on a value. Maybe for this millennium all copper is going to
conduct electricity and then for the next 3,000 years none of it will and it will wildly
oscillate without ever converging on a particular value. That's a possibility. If that possibility
turns out to be actual, no method could possibly get you the right answer because there is
no right answer. There's no fact about how much copper conducts electricity. The longer
you sampled, the more fluctuation you'd see in your results. It would never get you anywhere.
But if there is a correct answer, a correct value for what percentage of copper conducts
electricity, then the argument goes an infinite application of the straight rule of simple
enumerative induction is guaranteed eventually to settle on the right answer. And that can't
be said, the argument goes, of any other method, which is why induction is special.
Let's unpack this. The idea is that infinite sampling would eventually have to generate
a random and hence a representative sample. That's controversial. You get into deep waters
about statistics and metaphysics here that we're not going to go into. Let's grant
that assumption. If you were to sample forever, you'd eventually have to get a random sample.
Now note some significant limitations here. John Maynard Keynes, the great early 20th
century English economist, said in the long run, we're all dead. And this is a defensive
induction in the long run. It does not tell you that you and I have good reason for thinking
the sun is going to come up tomorrow, just that induction could eventually provide good
reason for thinking the sun is going to come up tomorrow. So we're not defending finite
applications of induction, but only infinite ones. But at this point, we are desperate
enough, or some kind of answer to Hume, that we will settle for an answer in the indefinite
long run. That's a start anyway.
So the main part of this pragmatic vindication of induction concludes with the realization
that if any method will work, induction will. Why? Well, if reading tea leaves turns out
to be a good way to find out whether copper conducts electricity, then when we inductively
test the skills of tea leaf readers, we'll find out that they're reliable metallurgists.
So if any method can get the right answer, induction eventually will. If you went straight
to tea leaf reading, you might get the right answer faster, but you don't get the guarantee.
What this argument says is special about the inductive method is, eventually you're guaranteed
to get the right answer. And we're willing to waive the eventually. We've got forever.
So we know that induction will work if any method will. And if induction is the only
method that has the property that it will work if any method will, then induction seems
special. It has a property that no other method has. It's only the beginnings of an answer
to Hume. It's limited by the long run and other factors, but it's a start. It's a good
start. We're not out of the woods yet, though. We can show that there are still infinitely
many rules that are guaranteed to work if any method will. All of these rules resemble
induction in an important respect, but differ in having an a priori component, a role for
a background belief about what your sample is like that is not derived from the features
of your sample. And this often seems quite reasonable to us. In these cases, it's useful
to talk about marbles in jars. So if you've got an opaque jar and you know there are three
colors of marbles in there, you might start before you've taken any samples at all from
the idea that each color will appear one-third of the time. And you might stick with that
assumption for the first round or two of samples, because if you just used the straight rule,
you draw a red marble the first time, what does it direct you to infer? It directs you
to infer that all of the marbles in the urn are red, and that seems premature. We have
background beliefs about distributions of these things, and a method that provides a
role for these background beliefs seems reasonable to us. And such methods share with the straight
rule the feature that they will converge on the right answer if there is a right answer,
provided that these background beliefs get less prominent as the sample size gets bigger.
So you might start with the idea that red, blue, and green will each appear in roughly
the same proportion. After you've drawn a few million marbles, you're going to get
over your prior beliefs about it and go with the sample. So just as induction is guaranteed
to converge on the right answer, this modified version of induction that allows a role for
prior beliefs is also guaranteed to converge as long as the prior beliefs lose their force
as the sample gets bigger. But this means that we really haven't gotten anywhere, because
there's an infinite number of possible sets of prior beliefs. Just think about all the
different ways you could have a prior expectation about red, green, and blue marbles. Vary the
numbers as much as you want. And so you've got an infinite number of competing inductive
inferences. I could start by giving red a higher probability than green and blue, because
I like red. And I could vary that in all kinds of proportions. Each of those rules is going
to give a different result. So I've still got an infinite number of rules, each of which
is guaranteed to converge on the right answer if there is a right answer. Each of these
rules gives a different answer, and so I've gotten no advice about how to make an inference
on the basis of any sample. We need a reason to favor the straight rule over its infinite
number of competitors. But the pragmatic vindication is not dead yet, because there
does seem to be a basis for preferring the straight rule. Maybe not in the short term,
but in the long term. The other methods allow for a different result without any change
in the observation. Suppose I decide to draw a new distinction. I want to call light green
a different color from dark green. So now I think there are four colors of marbles in
the earth. Now I've got a method, if I'm say distributing prior probabilities equally
across all the colors, I now want to give each of them a prior probability of one fourth
rather than one third. But the earn hasn't changed, the marbles are still as they were.
It's my language that has changed, not the data. So the defender of the pragmatic vindication
if that person can appeal to the principal that our choices about language are not allowed
to determine our beliefs about marbles, then the straight rule stands out as special once
again. It's the one that's driven only by data. The other rules are all driven by the
language that you choose to speak, and that can seem arbitrary.
So we still maybe have a defense against you that has some legs here. While you and I might
not use the straight rule in the short term, I wouldn't infer that all the marbles are
red the first time I draw a red marble. It does seem to have a special status in the
long term. And so perhaps we have a significant if limited defense against you.
But things are about to get ugly. The best way to appreciate the fact that even this
defense won't work is to look at a new version of the problem of induction.
Nelson Goodman put forward what he calls his new riddle of induction in 1955. Goodman turns
Hume's problem on its head. Hume had given us reason to think that we can't latch on
to any uniformities in nature. Goodman says we can latch on to plenty of uniformities
in nature. The problem is there are too many of them, not too few. So it's a different
problem of induction that shows the same kind of paralysis in the light of evidence.
So Goodman shows, or another way to appreciate what Goodman shows, is that even the straight
rule allows for different results depending on the language in which you formulate your
hypothesis. The example Goodman gives is going to seem weird at first, but it is by no means
as ridiculous or dismissible as it might initially seem to you. So I'm going to introduce a new
term. Let's call an object grew if it is first observed before January 1, 3000 and is green.
Or if it is first observed after January 1, 3000 and is blue. I can introduce terms
if I want as long as I explain the meaning. There are lots of grew things in the world.
Grass is grew. All grass that's ever been observed has been observed before January
1, 3000 and is green. Similarly, all emeralds that have ever been observed have been grew.
And so if we're using the straight rule, we should project that regularity into the future.
So we should expect all emeralds first observed after January 1, 3000 to be grew and hence
to be blue. We're projecting an observed regularity of 100% into the future population. All observed
emeralds have been grew, so all emeralds are grew. So observed emeralds after January 1,
3000 will be blue. It looks like our evidence for the grueness of emeralds is every bit
as good as our evidence for their grueness. Now, Goodman is like Hume in the following
respect. He's not telling you that you should expect emeralds in the next millennium to
be blue. But Hume wasn't telling you to stop believing the sun would come up. Both of them
are trying to show you something about the reasons behind your beliefs. They're not urging
you to give up the beliefs. Now one is tempted of course to claim that grew is somehow an
illegitimate term. Green is a real color term and grew is not. But it's not easy to get
rid of these terms which philosophers of course call gruesome. Now it certainly seems weird
to talk grew talk to us, in part because it builds in a color change at a particular time
and that seems random and unmotivated and weird. Goodman's point is that's because
of the language we speak. If we had started out speaking a language that involves grew
and bling, bling of course means first observed before January 1st 3000 and blue or first
observed after January 1st 3000 and green. If we started off speaking grew and bling,
then green and blue would seem like weird terms that have objects arbitrarily switching
their colors at particular times. For a grew speaker, green means first observed before
January 1st 3000 and grew or first observed after January 1st 3000 and bling. So for the
grew speaker, if you say all observed emeralds are green, so all emeralds are green, you're
telling the grew speaker that emeralds are going to start off grew and end up bling. And
the grew speaker is going to want to know why you expect their color to change. The grew
speaker thinks they're going to stay grew and thinks you're thinking they're going
to change to be being bling. So this is what I meant when I said that Goodman argues that
even the straight rule depends on the language one speaks. Goodman spent the second half
of his career as a colleague of Quines at Harvard and his new riddle sits pretty well
with Quine's approach. The positivist had wanted the strength of inductive arguments
to depend only on the facts, only on logical relations between the premises and the conclusion.
What Goodman's trying to show is that questions of fact can't be separated from questions
of language in the way that the positivist had hoped. So his approach sits very well
with the reading of Quine's two dogmas of empiricism which suggests that language and
fact are irreversibly intertwined. Let's continue trying to respond to Goodman's
annoying problem. A different response doesn't worry about the term grew but about the property
it picks out. The idea here is that greenness is a real property of things while grueness
is an unnatural, gerrymandered, made up pseudo property. Now we're getting into some pretty
deep philosophical waters here. There's a lot of discussion in metaphysics about what
counts as a real property, a real distinction in nature. So there do seem to be ridiculous
properties. I could decide that something is a broccosaxidyle if it is either a piece
of broccoli, a saxophone, or a crocodile. In one sense it's clear that there are broccosaxidiles
because there are pieces of broccoli and saxophones and such. But they don't seem to have enough
in common to really get lumped together. There's a sense in which there aren't any broccosaxidiles.
But this raises some issues about how confident we should be that we can carve nature at its
joints, that we can speak nature's preferred language, draw distinctions that are out there
rather than in our language. Intuitively, something like the periodic table, the chemical
elements seem to get at deep differences that are out there. But there are solid reasons
for some skepticism here. Prior to Einstein, people had thought that space and time were
genuinely and deeply distinct things, that that was a distinction that carved nature
at its joints. But after Einstein, there is reason to think that there are no such things
as space and time as traditionally understood, there's only space-time. Lots of categories
that we use in science encompass pretty diverse members. Think of the category of a predator.
Is that a real category or not? I mean, how much do predators have in common? They don't
have anything really physically in common. Why is that a better category than broccosaxidyle?
It's a more convenient category. Does it get at a real distinction in nature? That's
a tough question. We'll approach that later in the course. For now, what we need to notice
is there's no philosophical consensus on how you can make sense of the idea of a real
property, such that greenness counts as real, but grueness does not. And in any case, similar
problems are going to arise with clearly unproblematic terms and properties. What's the matter
with the inference from all observed emeralds have been observed to all emeralds are observed?
That's not using grew, that's using observed, which looks like a perfectly normal term. But
that's a very problematic inductive inference. It says there are no unobserved emeralds anywhere
in the universe. What Goodman takes this to show is that the whole idea of an inductive
logic is misguided. Inductive logic, like its deductive counterpart, was supposed to
depend only on the forms of the arguments used. But what Goodman's trying to show us
is that arguments of the same form can be, on the one hand, very good, and on the other
hand, terrible. Goodman thinks all emeralds are green is a much better inference than
all emeralds are grew, but they have the same form. Goodman's problem, weird though it
is, illuminates real problems in science. There's a problem called the curve fitting
problem. If you have a few points on a graph, intuitively you want to connect those points
with a nice smooth curve. But there are an infinite number of curves that would connect
all of those points. Using something like grew is the equivalent of drawing a kind of
elaborate curve and an elaborate mathematical function describing the curve rather than
a simple one. But it fits all of the data points. It might fit the data points better
than a simpler curve. What's the matter with complex curves? Do we think nature has an
obligation to seem simple to our minds? Do we think nature has an obligation to answer
to our language that grew speak would somehow be unnatural and wrong? To what extent are
our judgments of simplicity and naturalness shaped by our language, our culture, our evolutionary
history? And to what extent are they shaped by the way the world is?
So Hume had us thinking that we couldn't find any real connections in nature. All events
are distinct. Goodman comes at it from the other side. Connections between events are
too cheap to be valuable. Any set of objects has an infinite number of properties in common.
The challenge is to figure out which connections, which uniformities, matter for predictive
purposes. Connections like evidence for popper are cheap. We have to pick out the meaningful
or important connections. So we haven't really solved the problem of induction. And next
time we're going to drop induction and drive ourselves crazy with some different puzzles
about confirmation.
