Butcher 30, probability.
Probability and some closely allied notions have cropped up a couple of times in this
course and to a certain extent we've treated them as distractions from the main business
at hand.
This is a situation for which I am fixed and to apologize.
We've already seen some indications that probability is more fundamental than this
usual way of treating it as a kind of complicating factor would indicate.
So in our discussion of induction and evidence we saw that probability provides the way
to understand evidence statements in general.
All copper conducts electricity and no particle travels faster than light are limiting cases,
special cases in which the frequency of a trait in a sample is 100% in the case of
copper conducting electricity or 0% in a case of particles that travel faster than light.
A lot of important stuff happens in between those extremes.
Some smokers don't get cancer and some non-smokers do.
So for lots of the work science is called upon to do we need to deal with frequencies
that are in between these extremes.
What exactly probability statements have to do with these frequencies is an issue that
will occupy us later in this lecture.
In our discussion of scientific explanation we also saw that explanation works surprisingly
differently when statistical laws rather than deterministic ones are in the picture.
In Carl Hempel's model anyway, a scientific explanation has to get relativized to background
information when statistical laws are in place in a way that deterministic laws don't require
such relativization.
Why?
Because new information can render a previously strong probabilistic explanation extremely
weak.
If we learn that our patient is allergic to penicillin, our prediction that he will
recover from his infection goes from very probable to very improbable, with the addition
of new information.
These complications involving probabilistic explanation are especially unfortunate given
that quantum mechanics gives us some reason to believe that the most fundamental laws
governing the entire universe are probabilistic rather than deterministic.
According to quantum mechanics, there is no cause that explains why one uranium atom decays
and an identical one doesn't.
There are just brute probabilistic laws that a certain percentage decay within a certain
time.
And if you insist on some explanation, empiricists will tell you you're pushing the demand for
explanation too far.
There is no deeper explanation than a brute probabilistic law.
So probability and statistics, we won't need to carefully distinguish these notions
for our purposes.
More or less represent the normal case.
The deterministic case is the falling off from the probabilistic norm.
We tend to focus on non-probabilistic cases because in many respects they're simpler.
But doing so can distort important features of the general case of scientific reasoning.
So feminists complain sometimes that male doctors used to treat distinctively female
organs as distractions from what a normal body is like.
That's a bad way to practice medicine, and similarly treating probability statements
as a falling off from a norm of determinism gets philosophy of science in some important
respects backwards.
Now, it's perhaps forgivable since philosophy of science is hard and we wanted to avail
ourselves of every simplifying opportunity that we could find.
But nevertheless, we've got to confront the fact that the general cases are probabilistic
rather than deterministic.
So in this lecture and the next two, we start to rectify this situation.
We'll focus mainly on issues of confirmation and evidence, and we'll see that probability
makes an enormous difference there.
That starts next time.
This time we start to get clear about the basic notions in the field of probability.
As the English philosopher Bishop Joseph Butler famously said, probability is the very guide
to life.
probability has a fascinating history, and the idea that it could be a guide to life
was rather slow in coming.
The basic mathematical theory of probability did not really arise until around 1660.
Now this doesn't sound shocking by itself, but it is shocking when you realize that gambling
had been going strong for thousands and thousands of years by this time.
Ian Hacking, perhaps the foremost philosopher concerned with the history of probability,
tracks that a gambler with basic knowledge of the mathematics of probability could have
won all of Gaul within a week.
So given how useful the basic mathematics of probability would have been, it's perhaps
striking that it didn't develop until so much later in Western history.
Now absences are usually rather difficult to explain, and as far as I know anyway, there
isn't a generally accepted explanation of why something as useful as a basic understanding
of probability took so long to emerge.
We are venturing a bit into intellectual history here, and I have to rely on standard
sources because I'm not an intellectual historian, I have no special expertise.
It's widely believed anyway that part of the reason that no theory of probability arose
goes pretty deeply into Western culture.
The idea is that chance didn't seem like the sort of thing that was amenable to being understood.
You're not going to have a theory of something that seems beyond comprehension.
Joseph Bertrand, an important mathematical theorist of probability, was able to write
as late as 1888, how dare we speak of the laws of chance, is not chance the antithesis
of all law.
So if chance is a name for ignorance, for limits to knowledge, that suggests, though
it by no means establishes, that it's not itself the sort of thing that can be known,
and that's going to be a discouraging factor when you try to sort of get your head around
probability.
And as we briefly noted back at the beginning of the course, for much of Western intellectual
history, knowledge was of that which could be demonstrated, that which had to be the
case.
The model of real knowledge is geometry.
If that's your picture of knowledge, demonstrations of things that have to be the case, that flow
from some kind of self-evident principle, then chance will, by its very nature, seem
to be unknowable.
In addition, some scholars have speculated that the Christian notion that everything
that happens is a manifestation of God's will, and so is determined, though in ways
that we can't begin to understand, might also have discouraged systematic reflection
on chance phenomena.
Whatever the explanation, sophisticated probabilistic reasoning seems to have caught on much more
quickly in India than in Europe.
As hacking emphasizes, however, we may be better off asking how we came to have our
concepts of probability, rather than why these concepts didn't show up sooner.
This suggestion is that it is by no means inevitable that the phenomena around the notion
of probability would get divided up as they did, that it's kind of a gerrymandered notion.
This will become at least slightly clearer as we proceed.
The study of probability really got going when a nobleman and gambler asked the French
mathematician and philosopher Blaise Pascal to solve some problems about how you divide
up a gambling stake fairly when the game is called off and everybody goes home.
Who gets how much of the pot and why?
Once the mathematics started to develop, probability caught on very quickly, if somewhat haphazardly,
in business, especially in businesses like insurance.
But it really was haphazard.
You found governments selling annuities for a hundred years after the rise of probability
theory that didn't take the age of the people buying essentially life insurance into account.
They were trying to use the mathematics of probability but didn't seem to have a deep
understanding of how it was supposed to work.
Our concern, however, is with probability's role in science.
Arguably, probability is central to the modern conception of evidence.
The word probability starts off being associated with testimony.
An opinion was probable if it was grounded in reputable authorities.
Given that meaning of the term probability, it was not uncommon to hear it said that an
opinion was probable but false.
What could that mean?
It meant that the authorities were wrong in this case.
So the opinion was supported but not in a way that made it likely to be true.
That's not what probability meant.
It meant the authorities as it were stood with the opinion.
Probability eventually morphs enough to allow the causes of high sciences like physics and
astronomy, which reasonably aspire to this geometrical model, to get assimilated to the
signs of low sciences like medicine which could not approach real demonstrative knowledge.
High sciences, as we've seen, aspire to be systematic, while low sciences, which included
geology and alchemy, as well as medicine, had to settle for much less.
They could deal only in opinion, which was a radically different kind of thing than knowledge.
And so they had to rely almost entirely on testimony, at least in their original incarnations.
It's only in the Renaissance that the notion of diagnosis of a thing serving as an indication
of another thing came to be clearly distinguished from notions like authority and testimony
on the one hand versus direct dissections in medicine and deductive proof on the other.
It occupied this weird nether region between real evidence and relying on somebody else's
evidence.
Probability becomes what we now regard as evidence when it becomes the world's testimony,
as it were.
A symptom testifies, in an all but literal sense, to the presence of the disease.
It's a report, but it's not a report from another person, it's a report from the world.
And it's through that morphing of the concept of probability, from testimony to the world's
testimony, that we get what we now regard as our notion of evidence.
Now, as the idea that physics, for instance, could be demonstrative, like geometry, starts
to fade, and we've looked at that a little bit in this course, we end up with an idea
that anything worth calling evidence derives from science and symptoms, that it's one
bit of the world indicating inconclusively, but importantly, what another bit of the world
is like.
So the notion of evidence derives from second-class sciences taking over, and the first-class
sciences modeling themselves on the second-class sciences.
In the 19th century, the spread of probabilistic and statistical thinking gradually undermined
the assumption that the world was deterministic.
And the history of this is rather surprising as well.
As governments kept better records of births, deaths, crimes, suicides, it emerged that
the general patterns could be predicted had regularities were lawful in a way that individual
events could not be predicted, did not seem to be lawful.
We could know approximately how many people would die violently in France in a given year,
even though we couldn't predict which people they were, or how exactly it would happen.
The assumption all along was that we settled for statistical laws because we couldn't understand
the presumably fully-determinate processes leading to each individual homicide.
But as the statistical laws became increasingly useful, the assumption that they reflected
underlying but more or less unknowable deterministic laws became increasingly irrelevant.
All of the predictive juice is contained in the statistical laws.
It's just a kind of metaphysical assumption that there's determinism backing those laws.
The statistical laws started to seem to be the stuff of science, not a substitute for
real science.
And so in the social or human sciences, the idea of a human nature shared by all people
starts to morph into the idea of the average person, or the normal person, with predictable
deviations from this statistical norm.
The normal person was sometimes thought to be the best specimen of the species, was sometimes
thought to be a distinctively mediocre specimen of the species.
But it was thought in either case that the numbers could, if we could just compile them
in the right way, simultaneously settle is questions about society and ought questions
about society.
Because the notion of a norm is descriptive and normative.
It's the way things in some sense should be.
Now it was something of a two-way street, but it is fair to say that important parts
of statistical thinking migrated from disciplines like sociology to disciplines like physics.
And then deterministic explanations started to seem increasingly irrelevant in physics.
So again, we have innovation flowing in this case from a supposedly soft science to a supposedly
hard science.
Again, it flowed in both directions, but innovations don't always track sort of prestige and scientific
success.
And as we've seen at least glancingly, with the arrival of quantum mechanics early in
the 20th century, we start to encounter powerful scientific arguments to the effect that our
world is governed by statistical laws that are not, and maybe cannot be, backed by deterministic
laws.
As we'll see in a few lectures, you don't actually have to get as far as quantum mechanics
to start to see a more or less irreducible role for probability in physics.
The reduction of thermodynamic phenomena, things like heat and entropy, to the motion
of molecules involves treating this part of physics as inescapably, if not perhaps irreducibly
probabilistic.
We'll talk about that in a couple of lectures.
Now notice that we did all of this potted intellectual history without clarifying the
notion of probability.
That was intentional, because the notion of probability eluded clarification throughout
most of its history, and to some extent it still does so today.
The notion of probability has received a ton of attention in mathematics and philosophy
in the 20th century, but there is no agreement about the basic interpretation of probability.
There's a range of basic interpretations, but which one is most useful for scientific
purposes is a very vexed question.
There is, however, a common mathematical core that all probability theorists agree about.
We need to characterize this, though we'll do it somewhat loosely.
Mathematical rigor is beyond our needs in this lecture.
All probabilities are between zero and one.
That's a definitional claim.
You can think of it as probabilities being between zero percent and a hundred percent,
if you prefer.
That's, of course, equivalent.
Any necessary truth gets assigned a probability of one.
You might agree with Quine, back in lecture eight, that no statement should be considered
necessary or unrevisable.
Even the laws of logic, Quine thought, might get revised in the course of inquiry.
It's okay if you think that.
The point is just that if any statement were to express a necessary truth, if you thought
the statement, there are no square circles, is a necessary truth, then it should get a
probability of one.
That's built into the notion of probability.
In addition, we need a rule for adding probabilities, and everything else will more or less flow
from these axioms.
If A and B are mutually exclusive, then the probability that one or the other will happen
is equal to the sum of their individual probabilities.
If there's a 30 percent chance that you will order the ranch dressing, and a 40 percent
chance that you will order the vinaigrette, then there's a 70 percent chance that you'll
have either the ranch or the vinaigrette.
I'm helping myself to the assumption that you would not be so disgusting as to put both
of them on your salad at once.
It gets more complicated if the probabilities are not mutually exclusive.
The chance that I'll have the cake or the pie, given that I might have both, is more
complicated, right?
It's the chance that I'll have the cake plus the chance that I'll have the pie minus the
chance that I'll have the cake and the pie, because we essentially have to avoid double
counting.
The other rules for calculating probabilities can be built out of these axioms.
You actually only need the rule for adding mutually exclusive outcomes.
You can build everything else that you need mathematically out of those three principles.
But so far, this is just a mathematical formalism with some kind of silly examples thrown in.
It just gives the laws anything will have to obey in order to count as a probability
function.
But lots of mathematical functions that aren't intuitively probability functions obey this.
Length can be made to obey a kind of probability function.
We only refer to this kind of mathematical entity as a probability function when we give
it an interpretation in certain respects.
So the issue is what gets modeled by the mathematics?
What are probability statements about?
And that's a philosophical question about probability.
We will consider three major interpretations.
There are a few others, but these will suffice for what we need.
The first are frequency theories.
These place probabilities out there in the world.
A probability statement is about facts, as it were.
This is the most commonly used conception of probability, especially in statistical contexts.
As gamblers and actuaries, or at least good gamblers and actuaries, know, probabilities
had better have something to do with frequencies.
If the probability of drawing a straight flush in a certain poker game had nothing
to do with how frequently it happened, we would have no use for the statement about
the probability of drawing a straight flush.
So the frequency theorist makes a simple identification.
We know probabilities must have something to do with relative frequencies.
The frequency theorist identifies probabilities with relative frequencies.
There's more than one way to do that, however, so there's more than one frequency interpretation
of probability.
Straightforwardly, probabilities could be construed as actual relative frequencies.
This very directly makes probability statements factual statements.
The probability of getting lung cancer, if you smoke, is the ratio of smokers with lung
cancer to total smokers.
Now, that's oversimplified, of course, in the real scientific case.
We need to know how much one smokes, or how long one has smoked, we'd have to add a few
other things.
But the point is that it's an actual relative frequency of a trait in a population.
And this has certain very impressive virtues, it's clear, and it has a clear connection
to empirical evidence.
We know what we're saying when we make a probability statement, and we know what backs the statement.
Now we'll face some issues about why such probability statements are scientifically interesting.
As we saw when we looked at statistical explanation, we have to place a given individual in a scientifically
relevant population in order to make their relative frequencies scientifically interesting.
I have one probability of getting cancer as an on smoker, a different one as a 40 year
old male, perhaps a different one as a coffee addict, and if you just focus on me, the probability
that I will get a given kind of cancer is either 1 or 0, if we assume that the laws
governing that kind of phenomenon are deterministic.
So what do we mean when we talk about my probability for getting a certain kind of cancer?
Well, generally, unless we specify an error context, what we mean is we want to use all
of the information that's statistically relevant, so we don't need to know the rate of cancer
for people who have my exact number of freckles, for instance.
The point is that despite the actual relative frequency interpretation making probability
statements about things out there in the world, their usefulness nevertheless depends on a
state of information.
There is no such thing other than perhaps the 1 or 0 probability, which is my probability
of getting cancer.
The only meaningful statement here is how I'm described or categorized, what reference
class I am placed in, that's what gives meaning to the actual relative frequency.
Now, as stated, this actual relative frequency interpretation of probability faces a very
serious problem.
Speaking kind of loosely, but drawing on the background we've developed in this course,
it's too empiricist.
Like an operational definition or a reduction sentence back in our positivistic days, it
links a scientific result too closely to experience.
A coin that has been tossed an odd number of times cannot, on an actual relative frequency
view, have a probability of .5 of coming up heads, because it's been tossed an odd number
of times.
Similarly, a coin that's been tossed once has a probability either of 1 or 0 of landing
on heads.
And we don't think that captures the facts about probability.
It reduces the probability statement too thoroughly to the evidence for the probability
statement.
Now, we should note that single case probabilities, talking about the probability of an event
that is not repeatable, those present problems to every interpretation of probability.
But nevertheless, it's a particularly grotesque problem for the actual relative frequency view,
since that seems seriously to misstate what we want to say about the probability of the
coin landing on heads.
So one might go with a hypothetical limit frequency.
We might say, for instance, that the probability of rolling a 7 using two standard six-sided
dice is the relative frequency that would be found if the dice were rolled forever,
assuming that there's an answer.
We saw when we looked at the pragmatic vindication of induction, that induction can be shown to
work if there's a long-term limit frequency for a trait in the population.
So assuming that there's an answer, that answer is your probability of getting a 7 when you
roll two dice.
But this version doesn't seem to be empiricist enough.
We've seen that empiricists raise serious worries about what we call counterfactual
conditionals, or contrary to fact conditionals.
These are if-then statements about how the world would be if things were different than
they in fact are.
But the dice, and maybe the whole world, would have to be pretty different if the dice were
to be rolled forever.
For instance, people would have to have much longer attention spans than they do, if people
are going to roll these dice forever and write down what happens.
The sun would have to last indefinitely if these dice were to be rolled forever.
So for this interpretation to work, we need to find a way to give clear meaning to the
counterfactual conditional what it would take for these dice to be rolled forever.
We need some answer to that question.
So it's obvious and important that probabilities have something to do with frequencies.
Not to identify them with actual relative frequencies seems too reductively empiricist,
and to get away from the actual relative frequencies builds in idealizing conditions that get pretty
far away from the evidence.
That's a problem.
Logical theories treat probability statements as statements about evidential relationships.
This is a pretty different approach than the frequency approach.
On this interpretation, probability statements can be thought of as the judgments of an ideal
agent or as evidential relationships as it were in logical space.
The idea is that probability gives a kind of logic of partial belief or inconclusive
evidence modeled on, but generalizing from, deductive logic.
Deductive logic tells us how different statements bear on one another when the statements are
regarded as having probabilities of one or zero when they're either true or false.
So we generalize deductive logic to cover all of the cases in which the evidence neither
conclusively establishes nor conclusively refutes the statement.
This is the inductive logic that the logical positivists, especially Rudolph Karnapp, pursued
with such energy.
So we know that it didn't work, we've gotten to that point in our course.
The first component of this approach is probabilistic coherence.
Just as our full beliefs shouldn't contradict one another, our partial beliefs should cohere
with one another.
Incoherent beliefs is not sufficient for getting the world right, but having incoherent beliefs
is sufficient for having gotten at least part of the world wrong, so we'd like to avoid
having either deterministically or probabilistically incoherent beliefs.
Now contradictions are harder to come by with partial belief.
My belief that the Red Sox might win doesn't contradict my belief that the Yankees might
win, as it would if I took out the might, assuming the teams are playing each other.
So the notion of a contradiction doesn't carry over in quite the same way to the probabilistic case.
Probabilistic coherence is instead a matter of how well an agent's partial beliefs hang
together.
If your evidence assigns a probability of 0.8 to a statement, then it had better assign
a probability of 0.2 to the negation of the statement.
And similarly, you'd better not think it's 60% likely that the Red Sox will win, and
60% likely that the Yankees will win when they're playing each other.
The probabilities for mutually exclusive outcomes should sum to one.
There's something incoherent, in a way we'll explore more deeply next time, about having
probability assignments that don't meet this condition of coherence.
But logical theories of probability impose conditions of rational belief that go beyond
mere coherence.
In particular, they impose what's called the principle of indifference.
If your evidence does not give you a reason to prefer one outcome to another, you should
regard them as equally probable.
So if you know absolutely nothing about baseball, you should assign a probability of 0.5 to
each team winning.
That's the probability assignment that properly reflects your evidential situation, and so
it's the rational probability estimate to adopt, doing anything else would reflect a
kind of irrational bias.
Note that no principle of indifference was built into the mathematics of probability.
This is an independent condition of rationality that logical theories of probability impose.
And it turns out to be very troublesome, as Joseph Bertrand, from whom we heard earlier
in this lecture, showed.
Let's say that I have a vase, and I don't know how much water is about to be poured
into it.
So I'm supposed to distribute my probability assignments in a way that shows my ignorance.
But am I supposed to be indifferent between, because I'm ignorant of, various heights
of water that could be poured into the vase, or say, various volumes of water that could
be poured into the vase?
Unless the vase has a linear shape, those are different ways of distributing these prior
probabilities, different ways of reflecting my ignorance.
And the idea that one of them is rational, and the other irrational, seems arbitrary
and ungrounded.
If I know that the pizza I'm about to order will be within a certain size range, should
I distribute my ignorance over possible pizza diameters, or over possible pizza areas?
In the absence of any evidence, is one of these supposed to be the rational way to reflect
my ignorance, and the other not?
This is very like the lesson we learned from Nelson Goodman's New Riddle of Induction.
Goodman's riddle showed that there are too many regularities out in the world.
There are too many properties that could count as the color of emeralds.
They could be green, they could be grue, they could be gred.
Bertrand shows that there are too many outcomes we might be indifferent between when we're
in a state of ignorance.
You could be indifferent between diameters, or areas, or volumes.
In neither case, Goodman's or Bertrand's will reason or language tell us which properties
or which possibilities matter.
So to say it's a rule of rationality that you use a certain language to divide up the
space of possibilities seems arbitrary and under motivated, which suggests something
like the logical approach to probability that drops the principle of indifference.
These are often called subjective theories of probability.
They treat probabilities as degrees of belief, but of something more like actual agents than
ideal agents.
So we saw that the logical theory of probability is easiest to model as degrees of partial
belief of a perfectly rational or ideal agent, because the evidential relationships, the
kind of logic, gets messy.
The subjective theory just says we're modeling the beliefs of actual agents.
These statements concern the believing agent rather than facts out in the world as the
frequency theory has it.
And these probabilities are subject to objective but rather minimal criteria of rationality.
So what do we mean by a degree of belief or a degree of partial belief?
The standard way of measuring it, and this is a kind of operational definition, is by
one's notion of a fair bet.
The odds at which you think, we're talking about your degrees of belief here, that it
would be reasonable for someone to bet that a Democrat will win the next presidential
election, tells you the extent to which you believe that a Democrat will win.
The more unlikely you think it is, the higher a payoff you would demand before taking the
bet.
Notice that even here we're implicitly linking degree of belief, this fact that somehow in
here, with some notion out there of frequency and repetition.
What makes the betting behavior a good measure of belief is the idea that by your lights,
you would expect to break even if you took a large number of such bets.
You'd lose some of the time, but win other times, and if the payoff is enough for an
unlikely bet, you'd break even, that's what makes it a fair bet, that's what makes it
a manifestation of your degree of belief.
So we're constantly tempted to use frequencies, to use repetition as tests for correctness
of degree of belief.
But since we've dropped the principle of indifference, this approach does not explain
probabilities in terms of frequencies or rules of rationality.
It relies only on the notion of probabilistic coherence.
At a time and across time, this will be central in our discussion next time.
That's all it takes to make probability assignments as correct as they get.
And the problem here is, it seems way too easy to have probabilistically coherent beliefs.
Paranoid delusions are strikingly coherent, everybody's out to get me and I will fit everything
into that framework.
A paranoid delusion is probabilistically coherent, I assign a high probability to their
out to get me, and a low probability to their not out to get me.
But it seems like a fatally flawed web of belief.
So our task next time is to see how an effective approach to scientific reasoning can be built
out of such modest resources, requiring only that beliefs be probabilistically coherent
at a time and across time gives us an extremely influential approach to scientific problems
of confirmation and evidence.
