Lecture 21 Laws and Regularities
We have seen that the notion of laws of nature figures centrally in Hemple's approach to
explanation in his covering law model.
Whenever we think of the merits of the covering law approach to explanation, the notion of
laws of nature is itself one that cries out for philosophical investigation.
It's important for explanation, but it's important in and of itself, so we'll approach
this issue directly now rather than through the philosophy of explanation.
It is generally, though by no means unanimously agreed, that science seeks to uncover laws
of nature.
The reason it's not unanimously agreed is it is occasionally thought that an emphasis
on laws of nature reveals a kind of physics bias, and surely laws do figure more prominently
in physics than in, say, biology or psychology.
Arguably, that's a matter of the complexity of those sciences.
The laws are harder to state, harder to empirically test, harder to use, doesn't necessarily show
that those sciences, in principle, work differently.
But some people think that they do, that biology explains narratively, and that biology and
psychology both appeal to goal-directed explanations, rather than law explanations.
That's a complicated question, whether you can have sciences that don't make any interesting
or important use of laws of nature, it requires more detailed examination of particular sciences
than we're going to be able to go into.
But it's worth noting that in biology, you do have laws, or at least decent approximations
of laws, like Gregor Mendel's laws of classical genetics.
And in the human sciences, we find quite a lot of laws, mainly in something like economics,
and say highly law-governed, at least alleged science, the law of supply and demand being
the best known case.
Economics, while we're on that subject, raises a whole bunch of issues about what makes
science science, highly law-governed, highly quantitative.
But the empirical interpretation of economic laws is itself a vexed question.
They come up with very precise models, but do they interpret their models in terms of
predictions that are tested against the world?
That's a more complicated issue than we can quite get into.
But you certainly can have laws in the human sciences.
Other issues crop up, of course.
Now the notion of a law of nature, like that of explanation, and that of causation, these
are closely intertwined, has seemed worthy of suspicion to empiricist philosophers.
The very terms in which it's expressed have conjured up associations with divine decrees
and other metaphysical pictures, if they're laws of nature who laid them down.
The picture is that laws aren't just facts about how things are, which are supposed to
be observable and testable and all that good stuff.
A law seems to be modeled on this idea that there's a necessity out there in the world.
There's this T-shirt with a picture of Einstein describing the speed of light, 186,000 miles
per second.
It's not just a good idea, it's the law.
That's a connection between laws in the positive sense, the sort of things lawyers study, and
laws in the physical sense.
The suggestion is that laws impose requirements on nature.
Empiricists want to know what the observable output of a requirement on nature is.
For an empiricist like AJ Air, this feeling that laws require things of empirical phenomena
is a kind of metaphysical hangover.
It's back to this idea, which might be a fine idea, but is not obviously scientific,
that God laid down the laws and makes things follow them.
And that has seemed metaphysical and hence worthy of suspicion.
So laws of nature must be distinguished from positive laws, which is what you study if
you go to law school, and also from the laws of logic, which for positivists and many other
people are analytic and hence necessarily true.
It's an analytic truth, or law, that there are no square circles, for instance.
But the laws of nature are synthetic, they're not true by virtue of meaning alone, and they're
contingent.
They could be otherwise than they are.
So even if you accept this idea that laws impose a kind of necessity on things or events,
the law prevents anything from going faster than light.
The decree that would back them up, this idea that God laid down the law, is itself supposed
to be contingent, though it makes certain facts necessary.
What do I mean by that?
Well, God could have set our universe up, with gravity being inversely proportional
to the cube of the distance between two objects, rather than to the square.
So the law itself is contingent, even if it makes particular events obey it, given that
it's a law.
So it's a contingent, necessitating kind of statement.
And empiricists like air are really bothered by this idea that laws make things happen
or prevent things from happening.
So they want to, as we've seen, focus on logic and language to explicate the notion
of a law.
Most if not all laws are of what philosophers call universal conditional form.
We've seen that quite a lot.
The form is all a's or b's, or take anything in the universe, if it's an a, then it's
a b.
All copper conducts electricity has been our main example of this.
That's got the right logical form to be a law.
But logical form by itself is not going to guarantee that something is a law.
And some laws, like the law of supply and demand, which asserts a kind of inverse proportion
relationship, don't appear to have this form, but can be reproduced in that form.
All cases in which supply goes down, and relating price and supply and demand, that serve stuff.
They can be assimilated to the all a's or b's form.
Singular facts can be crucial to scientific explanation or understanding, but they can't
themselves be laws.
The location of the center of mass of our galaxy might be very important for understanding
a bunch of things, but it's not a law, it's a particular fact.
Laws are general, not particular.
Now, it is generally, though not as we'll see next time, unanimously agreed that a
statement isn't a law of nature unless it's true.
Names reasonable enough.
But names can mislead a bit here, because there are lots of statements that are called
laws and, at least strictly speaking, are not really true.
Newton's gravitational law gets corrected by Einstein's general relativity, but sort
of by tradition and courtesy it gets called a law, and it can be used in lots of contexts
as if it were true.
So strictly speaking, a lot of laws aren't laws because they're not true.
And a lot of things that, strictly speaking, are laws get called equations instead of laws.
But an equation can have that all A's or B's form, so that needn't be a problem.
For empiricists, the most common approach to laws of nature is a regularity account.
A regularity account of laws treats them as statements about what always happens.
Laws of nature report or describe patterns in experience rather than something above,
beyond, or behind the world, which explains these patterns.
These are empiricist-friendly theories, because they say laws of nature just describe what
always happens.
But a simple version of an empiricist or regularity theory of laws faces devastating problems.
The simplest version would say that any true statement that has the right logical form,
any true statement of universal conditional form, counts as a law of nature.
This is a disaster.
The main problem with such an approach is it can't distinguish laws from accidental generalizations.
That's the usual contrast.
That's a true statement of universal conditional form that just happens to be true rather than
being a law of nature.
So even if it's true, the statement, all the beer in my refrigerator is American-made
does not seem to express a law of nature.
It says take everything in the universe.
If it's a beer in my refrigerator, it's American-made.
It's got the right logical form.
And it needn't be perfectly accidental, I might, have a habit or a principle that says
I will always buy American beer, but it still doesn't seem like a law of nature, even if
it always turns out to be true, and even though it has the right logical form.
Well, empiricists then need to explain why it's not a law of nature.
One suggestion is we can disqualify the law on the basis of its being restricted to just
one place.
Everything isn't a law if it's just about something as small as my refrigerator.
That might be too restrictive, though.
Remember, whenever we try to restrict bad cases, we have to worry about whether we're
going to prevent good cases.
And we want to allow for the possibility that some genuine laws of nature hold at just one
place.
For instance, on the earth, some biological laws might be laws, but might be as it were
local laws.
Galileo's Law of Free Fall, which turns out not quite to be true, would be a law, if
it were true, but it's just about the behavior of objects near the surface of the earth.
So the fact that my refrigerator law is about objects near my refrigerator doesn't automatically
disqualify it from counting as a law.
Vacuous laws, laws that don't have instances, present a similar kind of problem for a regularity
account.
So let's take the following law.
All particles that travel faster than the speed of light are pink.
Now that doesn't seem true intuitively, but remember that it's equivalent to no particle
traveling faster than the speed of light is non-pink, and that does seem true.
So once you realize that a statement like, all dragons like jazz is logically equivalent
to there are no jazz-hating dragons, you'll see that it's not so hard to admit that all
dragons like jazz turns out to be true, courtesy of there being no dragons.
So our statement that all particles traveling faster than the speed of light are pink turns
out to be true, courtesy of there being no particles traveling faster than the speed
of light.
Well, the simple way to get around this problem would be to require that there be at least
one instance of something for it to count as a law.
You would rule out vacuous laws.
You require that any statement to be a law of nature must have at least one actual instance.
That would get you out of the problem of vacuous laws.
But again, as soon as we try to exclude bad cases, we have to worry about whether we'll
be excluding good cases.
Not all laws that lack instances are illegitimate, or seem to be anyway.
The fact that there are no bodies on which there are no forces anywhere in the universe,
as some gravitational, maybe electromagnetic force acting on any given body, does not prevent
Newton's first law, which describes what would how a body that on which no forces were
acting would behave.
That still seems like a law and an important law, even though it doesn't directly apply
to any objects.
There are no instances of bodies on which there are zero forces acting.
So we're going to need to buttress our regularity account.
We're going to need to impose some more conditions to try to avoid these problems, according to
which things that don't seem like laws end up counting, and if we exclude them, things
that do seem like laws get excluded.
The most common way to do this involves an epistemic regularity account, and this was
AJ Air's approach.
It distinguishes laws from other generalizations on the basis of how we treat them.
And this approach can handle the problems we noted above, but it's going to face some
major problems of its own.
So how do we treat laws differently than we treat accidental generalizations?
Well, we let laws support counterfactuals.
A counterfactual conditional is a conditional that describes what would happen if things
were different.
So we can talk about what would happen if the people in the studio here were to pull
a certain switch, the lights would go off.
That's a true statement about how the world would be if things were different.
And so we can say that if my pen were made of copper, it would conduct electricity.
That's a mark that we think all copper conducts electricity is a law.
Well we don't want to say if a German beer was placed in my refrigerator, it would become
an American beer.
The difference between a law of nature and an accidental generalization is we treat
laws as supporting counterfactuals.
Similarly, laws, as we saw with Hempel, hold a special place in our explanatory practices.
We can at least begin to explain why an object conducts electricity by saying it's made
of copper.
Maybe you don't buy the covering law model, we're only beginning to explain, but at
least we're beginning to explain.
But arguably we don't begin to explain why a given beer was made in America by saying
that it comes out of my refrigerator.
True generalizations of universal conditional form can explain things if they're laws of
nature, but if they're accidental generalizations.
Saying something came out of my refrigerator does not explain, even though it's true that
all beers coming out of my refrigerator are American, that we can appeal to that statement
to explain why the beer is American.
If it were a law we could.
Laws are relatively central to our webs of belief to use the Quinean metaphor.
They are not easily undermined by new information or falsified by apparent counter-examples.
So if I'm told that this piece of copper was freshly mined and I've never seen a freshly
mined piece of copper before, that's not going to threaten my confidence that it conducts
electricity.
And that's a mark that I think the statement, all copper conducts electricity, is a law.
But if I'm told that I had some friends over and that they brought beer and there's a brand
of beer in my refrigerator that's never been in there before, I'm going to have my confidence
undermined that that beer is American.
New information threatens accidental generalizations in a way that it doesn't threaten statements
we treat as laws of nature.
In addition, we treat laws as more readily confirmed or supported by their instances
than accidental generalizations are.
I don't have to sample that many instances of copper in order to become convinced that
all copper conducts electricity.
But if you've been to say a meeting of the American Philosophical Association, you might
have encountered a bunch of weird philosophers, but you're not going to be perfectly confident
that all philosophers are weird.
You might think there's a normal one out there somewhere.
And that suggests that you don't think that all philosophers are weird is a law.
All copper conducts electricity is a law.
You think we could easily find an exception, even though it's generally true that philosophers
are weird.
You don't think that gets support from the particular cases in the same way that all
copper conducts electricity does.
And this actually is how Nelson Goodman originally set up his New Riddle of Induction, or the
Gru problem.
His idea was that the Gru hypothesis has the right form to be a law.
All emeralds are Gru has the same form as all emeralds are green.
But the mark of it's not being a law is that it's not confirmed by its instances.
No matter how many Gru emeralds we observe, we don't confirm the hypothesis that all emeralds
are Gru.
If all emeralds are green is a law, it does get confirmation from its instances.
And so we increase our confidence with each green emerald that all emeralds are green.
So on this epistemic regularity approach to laws of nature, it's a matter of how we
treat laws, what work they do for us that distinguishes laws from accidental generalizations.
The major problem this approach faces is it doesn't look like it can make room for undiscovered
laws.
What makes something a law of nature is how we handle it.
And so something as it were becomes or gets transformed into a law of nature, courtesy
of being treated in a certain way in our theories.
And that doesn't seem to do justice to the idea that the laws are in some sense, admittedly
this is metaphorical, but in some sense the laws are out there waiting to be discovered
by us, not made or manufactured by us.
We think we're tracking something when we get a hold of the laws of nature, not manufacturing
it, or at least many people think that.
Which brings us to the most sophisticated version of the regularity theory, or this
broadly empiricist approach to laws of nature.
Remember on a broadly empiricist approach, laws are going to have to be nothing more
than patterns in experience.
Laws are going to have to be built up out of statements about the ways in which things
happen.
So on the systems theory, the laws of nature flow from the deep structural patterns in
actual events.
We identify the patterns by figuring out the best way of describing the world in a deductive
system.
This fits the empiricist approach, according to which we can use observation and logic
rather than metaphysics to solve philosophical problems.
So we have no resources other than our observable evidence and deductive relations.
We're not appealing to some relationship out there of a law preventing anything from
going faster than the speed of light.
All events are separate, was the Humian motto.
So any connections between events or statements have got to be, as it were, built up by us
rather than found out there by us.
Connections between facts or events are not present in observation for people in the Humian
tradition.
And it's not kosher to claim that we can just reason our way to these things.
That would be metaphysics, that would be claiming synthetic a priori knowledge.
But the systems approach is going to come as close as it can while retaining these empiricist
scruples to putting laws out there.
So let's examine this in a little bit of detail.
What do we mean when we talk about the best way of systematizing experience?
Best here means the simplest and strongest deductive system.
This notion of a deductive system is very like the positivist conception of theories
we saw way back in lecture seven.
Theory is modeled on geometry, it's a set of statements that bear logical relations
to each other.
It's also even more closely modeled on the unificationist approaches to explanation
that we saw the lecture before last deployed for a somewhat different purpose in the philosophy
of explanation.
So what do we mean when we say a deductive system has the virtue of simplicity?
Equity is inversely proportional to the number and complexity of the axioms of the system.
So it's a nice feature of say a geometry if we don't need very many axioms, very many
independently accepted statements, statements for which we're not providing a reason.
But we want to do this with experiential content as we don't do it in the mathematics of geometry.
So here's a nice simple theorem, everything happens according to the will of Elvis.
Now we're only positing one object, so far we only have one theorem.
It's a pretty special object, we've only got one argument scheme, which also is good.
So everything is to be explained in terms of the argument from Elvis's intentions.
Now you might need a few more independently acceptable sentences in order to flesh out
the king's will.
You might need to look at song lyrics pretty carefully, try to get some evidence about
what Elvis's will is.
But there aren't going to be very many independently acceptable sentences, and you've just got
this one argument pattern, so that's a nice simple deductive system.
At the other extreme, a list of everything that happens is a very unsimple, a very complex
deductive systematization, because it imposes no order, it has no tendency to try to do
the most with the least.
Strength is the other main virtue a deductive system can have.
Strength is a matter roughly of how informative the theorems are, and that's measured by
how many statements about the world can be derived from the statements that make up the
theory.
So a list of everything that happens is a very strong, though not a very simple, deductive
system.
Why?
Because if you've got a list of everything that happens, it includes everything that
happens.
You can derive everything that happens from that list.
The system, according to which everything happens according to the will of Elvis, scores
well on simplicity, but badly on strength.
It's hard to derive particular predictions from it, at least not without adding a whole
bunch of other statements, in which case it loses its simplicity.
So as we see, the simplicity and the strength of a deductive system tend to work against
one another.
It's easy to have a simple theory that doesn't say much, and it's easy to have a strong
theory that isn't simple.
The laws of nature, according to systems theorists, the most sophisticated of the regularity
theorists, consist of all of the true contingent generalizations, all the statements that
have the right logical form, and are true, that figure in all the best deductive systems.
That's the rule.
That's a little elaborate.
It's easier to see with examples.
So the following statement has a good chance of being a law of nature, according to the
systems view.
There are no uranium spheres that are a mile or more in diameter.
Why?
Well, our best physical theories, let's assume for the sake of argument, are highly likely
to belong to the best deductive systems.
Why?
Because we can do a lot with a little.
To oversimplify it a little bit, Newton's laws of physics unify a lot of phenomena.
With a few independently acceptable statements and argument patterns, we can derive a lot
of predictions.
So if our best science will figure in the best deductive systemization, then we can
derive directly with no other statements that there are no uranium spheres more than a mile
in diameter, because those theories by themselves imply that if you put that much uranium together
in one place, it would go boom.
On the other hand, there are no gold spheres a mile or more in diameter anywhere in the
universe is not a law of nature.
Let's assume it's true, pretty plausible that it's true, but who knows?
It's got the right logical form, and it's perfectly true.
It looks a lot like the statement about uranium.
The difference is it doesn't follow from, it doesn't flow out of the best deductive
system.
You would have to add it independently as an axiom to get it into the best deductive
system.
And if you add it, you're lessening the simplicity of the system without getting a compensatory
payoff in strength.
So if it's perfectly true, it nevertheless ends up being something that just happens
to be true, and the difference between things that need in some sense to be true and things
that just happen to be true is the difference between laws of nature and accidental generalizations
that we were trying to get at in the first place.
So this is what the systems theorists have in mind when they say that to call something
a law of nature is to say that it is a generalization that is true and figures in or flows out of,
two metaphors that amount to the same thing, all of our best deductive systematizations
of the world.
This looks like a promising way of handling the problems that plague simpler versions
of the empiricist approach to laws of nature.
Because laws that don't have instances are allowed, but they have to pay their way in
terms of the currency of simplicity and strength.
So there's no ban on laws that don't have any instances, but all particles traveling
faster than the speed of light are pink does not pay its way in terms of allowing us to
derive a lot of predictions we couldn't otherwise have derived.
While Newton's law, saying that all bodies on which no forces are acting will continue
in inertial motion, arguably does pay its way, and so figures in all the best deductive
systematizations.
Similarly, laws can be restricted in space or time and still count as laws if they pay
off in terms of simplicity or strength.
So if we get a large payoff in terms of unifying our experience and our theories by talking
about laws that hold only on earth, if they let us derive a lot of predictions using a
modest number of hypotheses, then they count as laws.
But it's highly unlikely that there are going to be laws governing the behavior of objects
in my refrigerator.
We would need to posit them independently, and we derive relatively few observational
predictions from those independent posits.
So even though it might be true that all the beers in my refrigerator are American, it
won't be a law because it would have to be added to any deductive system, and it detracts
from the product of simplicity and strength in the system.
Arguably, this system's approach explains why the laws of nature have the role they
have in explanation, why they support counterfactuals.
So for instance, if unification provides the right account of explanation, then the depth
of a statement in our best theories is a measure of how unifying its force is.
And similarly, the right way to evaluate counterfactuals for an empiricist is going to be given by how
much havoc it would wreak with our theory if we evaluated the counterfactual differently.
So how do we decide what would happen had Oswald not shot Kennedy?
Well, the way to evaluate counterfactuals is first, we try to keep the laws of nature
constant.
Why?
Because if we change them, huge changes ramify throughout our theory.
So we minimize large widespread violations in laws of nature.
Next we try to make the situation as spatio-temporally continuous with our world as possible.
So we could say, had Oswald shot Kennedy, there would have been a black hole emerging
in Dalit, had Oswald not shot Kennedy, excuse me, a black hole would have emerged on the
grassy knoll, and all of matter would have gotten sucked into some other dimension or
something like that.
Experience doesn't directly tell us what would have happened had things gone differently.
But there are more and less reasonable answers to these things, and the measure of reasonableness
is a matter of how deeply ingrained in our best theories of the world a given statement
is.
So even though that was an important particular fact, we don't need to change much about how
the universe would have gone in order to talk about how things would have been differently
in those cases.
What we preserve are laws, not facts, even when they're important facts.
Because changing facts changes to resort to the Quinean metaphor, things at the periphery
of the web of belief, not things at the center of the web of belief.
So when we evaluate counterfactual conditionals, we're asking which possible worlds are close
to our possible worlds, to our actual world, sorry.
And we do that by asking whether, according to our best theories, the world works fundamentally
differently from our world.
And so this systems approach solves the problem with the epistemic regularity view, the big
problem with the epistemic regularity view, because what makes something a law is not
that you and I believe it.
We don't turn things into laws by believing them or using them in a particular way.
The idea is our best theories, structured in the best possible way, determine which things
the laws are.
It doesn't matter whether you or I believe that things are that way.
What matters is the deductive relationships that hold among them.
So it's got a kind of objectivity.
What's nifty about the systems view is it makes laws depend on the best patterns in
experience.
But best here doesn't mean best as judged by you or I.
It means best according to a standard of maximizing simplicity and strength.
So it's as objective an account of the laws of nature as an empiricist view is going to
be able to get.
Because all the empiricist has to go on is patterns in experience and deductive relations
that hold among them.
So though we're far from having a best deductive systemization of our view of our theories
and it's not clear what it would look like if we did.
This approach can build laws and hence explanation out of mere deductive relations on this approach
to say that the laws of nature forbid something from happening is not to say there's some
metaphysical relation of forbidding out there.
It's to say the best version of our best theories forbids this thing from happening.
And so this builds in as much mind independence for the laws of nature as you're going to
get if you're going to insist that laws have to be built out of mere patterns in experience
and deductive relations.
So remember Hume had made causation in here in our heads rather than out there.
There's no necessity of the cue ball moving the eight ball.
The necessity is in our thinking about cue balls and eight balls.
Similarly, the systems approach treats laws and causes as in here in the sense of in our
theories rather than out in the world.
But it's in here in the sense of an idealized version of our theories.
And so it's independently of what you and I think and our current best theories.
And so the regularity or systems approach uses all of the resources available to empiricists
to prevent laws from having to be about some kind of cement of the universe, some kind
of stop sign that the world gives to particles that try traveling faster than the speed of
light and yet it makes laws found rather than made.
We'll look at a competing approach to laws of nature in our next lecture.
