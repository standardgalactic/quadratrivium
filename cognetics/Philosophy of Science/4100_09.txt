Music
Lecture 9 Discovery and Justification
Applause
Notions of evidence and of confirmation, and I'm going to use those terms more or less interchangeably,
along with a notion like empirical justification.
We'll get fancy when we need to.
But those notions have loomed large in the background of our discussions of demarcation and scientific meaningfulness.
So for instance, we've been saying things like,
a statement or a theory is testable, just in case evidence bears on it either positively or negatively.
We turn now for the next several lectures to issues of evidence and justification, directly.
This lecture will sketch the birth and transformation of the modern idea of a scientific method.
This idea arose in the 17th and 18th centuries,
and when it did, the central problem of scientific method was the discovery of correct scientific theories or explanations.
In our period, both popper and the positivists are going to deny that scientific method has anything to do with discovering the truth at all.
We'll look at their view next time, or starting next time.
In the most general sense, the study of scientific method is the study of whatever it is that helps scientists...
whatever it is scientists do that helps account for the distinctive epistemic status of science.
You can offer a descriptive theory of scientific method.
Here's what they do that seems somehow connected to why science works,
but almost any theory of scientific method is going to be at least in part normative.
It's going to describe methods that are supposed to work.
It's going to give advice about what you should do, not just say what scientists do do,
and it's going to explain why this is good advice.
If a notion of scientific method can be defended,
it provides some constraints beyond those of deductive logic and observation
that can tighten up and give form to our web of belief,
that can tell us which auxiliary hypotheses are well supported, which ones are poorly supported,
and can help us distribute praise and blame across our theory.
Now, the originators of the modern idea of a scientific method thought of it as a kind of recipe,
a systematic procedure for attaining new knowledge.
This idea fit with the boldness of the times that we were emerging from superstition and ignorance,
that there was lots of new knowledge to be found,
if only we could overcome timidity and tradition and use our intellectual ideas properly.
This was an idea very much in the air in the 17th century.
So Francis Bacon, Ren√© Descartes, and later Isaac Newton,
these are probably the three most important methodologists,
had different ideas of what recipe should get followed,
but their general notion of a recipe or method combines two key aspects.
First, the method tells the inquirer how to discover and formulate the right answer,
or at least a right set of small set of candidate answers.
Often the method is going to tell the inquirer how to ask the right questions
so that you get the right answers.
This is the discovery part, and it's the key to gaining new knowledge.
The answers that you settle on by using the proper method are justified precisely
because they result from an application of the correct method.
This is the justification rather than discovery part,
and it's the key to getting new knowledge.
It makes the new discoveries count as knowledge.
These two projects from our vantage point seem more distinct to us
than they did to the classic methodologists.
The fact that they combined the projects of discovery and justification
makes classical methodology, in retrospect, very ambitious.
Let's see more clearly why.
The method is supposed to be as close to mechanical as possible.
Each step in the method is supposed to be very simple to perform.
Why? Well, much of the point of a method is to eliminate the need for luck or genius.
You can give advice like be really smart, study hard,
get lucky, stumble into promising suggestions,
but that advice is hard to follow.
It's hard to do anything useful with.
So a method is supposed to reduce complicated inquiry to relatively simple steps
so that inquirers can generate from observational inputs
the right hypothesis to account for the observations.
Now the term hypothesis is a little bit dangerous here.
In the 19th century, later than the classical methodology period,
the idea caught on that hypotheses are first formed and then tested.
This is a conception of scientific method.
It's sometimes called the hypothetical deductive method,
and it will concern us later.
But it's not the classic notion of a scientific method.
The classic notion has inquirers read the right explanation or theory out of the data,
not bring it to the data,
which is why the word hypothesis is a little bit dangerous here.
In classical methodology, it's not as it were a mere hypothesis.
It's not a guess or a proposal independent of the data.
It itself comes out of the data.
We'll see this in more detail as we move forward.
The 19th century method of hypothesis, the hypothetical deductive method,
is a retreat from classical methodology in the face of criticism,
both from within science and from within the arts and literary world,
that valuable novelty, that real creativity,
is not the sort of thing for which a method can be provided.
It needs insight, luck, inspiration, genius.
So culturally, in the early 19th century,
you get the romantic reaction to the Enlightenment
and the classical idea of method, to some extent,
goes by the wayside in the 19th century.
Nevertheless, we're going to look at a 19th century methodologist, John Stuart Mill,
who's writing after this romantic reaction,
but is a defender of something like the classic conception of method.
In his 1843 system of logic, Mill formulated and brought together
a whole bunch of classic empiricist methodological principles.
They're not original with him,
but the formulations were enormously influential.
And they've come to be known as Mill's methods.
Mill is very much an empiricist methodologist,
which is one of the reasons he's especially interesting for our purposes.
Somebody like Descartes favored a much more rationalist conception of scientific method,
relying on truths that could be detected by reason alone.
And this worked pretty well.
He's a great philosopher and an important mathematician.
He invented analytical geometry.
But just reasoning out the way things must be turns out
not to be a really good way to do science.
Descartes' physics was wrong in almost every particular.
Mill is at the other extreme from Descartes.
Everything, even the statements of mathematics,
is supposed to be testable by experience.
Not very many people have followed Mill
in thinking that mathematical statements should be subjected to the test of experience.
So we won't discuss mathematics here,
but we can use Mill's very empiricist conception of a method
to appreciate the strengths and weaknesses of a very pure empiricism
about how to inquire into the natural world.
Mill's classical leanings in methodology
emerge in his conception of empirical inquiry
as the operation of discovering and proving general propositions.
Now, he verbally distinguishes discovering and proving,
but they are part of the very same process for Mill.
His methods are supposed to show us
that we can arrive at a proposition like smoking causes cancer,
and the way we arrive at the proposition
is itself supposed to give us excellent reason
to think the proposition is true.
A proposition is just for our purposes a statement.
That's philosopher speak I will sometimes slip into.
Mill's official view is that most effects depend on more than one cause,
but his methods are nevertheless designed
to help us determine the cause of any given phenomenon.
And by a phenomenon I roughly just mean an item or pattern in experience.
Phenomena are observable for Mill.
His first method is the method of agreement.
This applies when two or more instances of the phenomenon under investigation
share only one circumstance in common.
And the method then tells us to infer a causal connection
between the circumstance they have in common and the phenomenon.
So let's say we examine a number of patients,
each of whom has cirrhosis of the liver.
The patients vary in age, they vary in sex, diet, etc.
But they share the property of being heavy drinkers.
The method of agreement says to infer that cirrhosis is due to heavy drinking.
Notice that we don't go in with a hypothesis about the cause of cirrhosis of the liver.
The hypothesis comes out of our examination of the data.
That's what makes this a classical conception of method.
Now the idea behind the method of agreement is relatively intuitive.
If eating meat caused cirrhosis of liver,
we'd have no way of understanding how the vegetarians among our patients got it.
So if our patients have just one property in common, that's the cause.
Or not so fast, maybe it's the effect.
Because what we observe is a correlation, not causation.
Remember, we never observe something cause something else.
The way we can determine from a correlation which one is the cause
and which one is the effect is by peeling to time, causes proceed effects.
But this brings us to the first of several limitations of the method of agreement.
The method won't always reveal what kind of causal connection we have among our phenomena.
Thunder doesn't cause lightning, nor does lightning cause thunder.
They are products of a common cause, but they agree,
wherever you have thunder you have lightning, wherever you have lightning you have thunder.
So the method of agreement by itself won't sort out what causes what.
Causation can get too complicated for the method.
Similarly, the method can mistake coincidences for causes.
The Russian physiologist Ivan Pavlov famously conditioned some dogs
by using something like the method of agreement.
He would give them food and ring a bell, give them food and ring a bell over and over and over.
And the dogs eventually infer a causal connection between the bell and food.
But of course, the real causal model is much more complicated.
The dogs have no idea that there's some perverse experimenter
and that the whole causal network is enormously more elaborate than they realize.
The method of agreement also assumes that the effect is always produced by the same cause.
But that's not always the case.
One person's cirrhosis of liver could be the result of too much drinking
while the very same condition could be produced in somebody else by too much cauliflower.
Clearly, I'm making this up. I just don't like cauliflower.
What the method of agreement really establishes is that any condition that isn't always present
when a phenomenon occurs can't be necessary for the phenomenon.
He has an established causation, he's established a necessary condition.
If heavy drinking is necessary for cirrhosis, it will invariably accompany cirrhosis.
The method of difference, Mill's second method, applies in cases where the phenomenon occurs
and cases in which it doesn't occur share all circumstances except for one.
The method has us infer that the circumstance in which the two cases differ
is causally connected to the phenomenon under investigation.
That's a little abstract.
The most noteworthy advantage the method of difference has over the method of agreement
is that the method of difference can make use of negative as well as positive instances.
We don't just look at cases in which the phenomenon occurs,
we contrast cases in which it occurs with cases in which it doesn't,
and so that's in advance.
If two lab mice are alike in all respects except that one has been given massive doses of vitamin C
and gets cancer, again a fanciful example, and the other hasn't and doesn't,
then the method of difference suggests that the presence of vitamin C is causally connected to cancer.
Now strictly speaking, you only need two cases here.
Then you can establish your causal conclusion.
But that's a little misleading because it's very hard to find two cases that seem to fit the bill.
How confident are we that we ever have two instances that are entirely alike in all relevant respects except one?
This is part of why there are lab mice in the world.
Scientists want to make their experimental subjects as similar as possible
so that they can apply something like the method of difference.
The method of difference faces problems similar to those that plague the method of agreement.
The causes can be more complicated than the method can handle.
Even if vitamin C is the only difference between these two mice,
the method of difference does not show that massive doses of vitamin C cause cancer and mice.
The cause might be more complicated than vitamin C.
It might be an interaction between a gene that these mice have and vitamin C,
so that other mice who don't have this gene could safely take massive doses of vitamin C.
So again, what the method shows is something less than Mill thinks it shows.
It shows something less than a causal connection.
It shows that if a condition occurs both where our phenomenon does and where it doesn't,
that condition can't be sufficient for our phenomenon because it occurs and the phenomenon doesn't.
The joint method of agreement and difference, Mill's third method,
combines the power of the preceding methods.
We use the method of agreement to figure out what can't be necessary for our phenomenon.
We use the method of difference to find out what can't be sufficient,
and hopefully we're left with a condition that's necessary and sufficient, hence a cause, in Mill's view anyway.
So if we want to know the cause of food poisoning in a group of people, say on a cruise ship,
we eliminate anything that sick people haven't eaten, and we eliminate anything that non-sick people have eaten,
but similar problems are going to arise from before.
This method, because it combines the other two methods, is doubly tough to apply.
What you ideally want is cases such that some people are different in all relevant respects except one.
You want two people, both of whom got sick, ideally they're different in age, health, how they spent their day,
and they ate nothing in common except, let's say, the gumbo.
And you also want cases that are alike in all relevant respects except one.
You'd like two identical twins who spent the day the same way.
They ate exactly the same things except for the gumbo, and one of them got sick and the other one didn't.
Cases that fit the joint method of agreement and difference can be hard to find.
Even though this method touches more bases, it can still run into problems with complex cases of causality.
One of your apparent cases of food poisoning might be seasickness instead,
and so your whole theory is going to be messed up because you've got a false positive.
Mill's fourth method is the method of concomitant variations,
and it's really a generalization of the joint method of agreement and difference.
It comes into play when two or more phenomena co-vary, either positively or negatively.
The joint method of agreement and difference only applies to cases in which the phenomenon is, as it were, on or off.
You either get sick or you don't.
The method of concomitant variations is more flexible.
It can apply to cases where the values for variables can take on a range.
So the pressure of a gas goes up when the gas is heated, if you keep volume constant.
Volume and temperature aren't on or off properties.
It's not that you either have a volume and temperature or you don't.
A statistical notion like that, things that can vary within a range,
these kinds of notions will crop up several times in the course.
So when two phenomena vary across a range with each other,
the method of concomitant variations has us infer a causal connection between the phenomena.
Now this correlation does not by itself tell you what causes what.
It just tells you that there's some kind of causation involved.
Now this is a vague version of something enormously important and enormously common in science.
It shows really that the method of agreement and the method of difference are widely deployed in actual science
because they are limiting cases of the method of concomitant variations.
The whole discipline of statistics has essentially made the method of concomitant variations much more powerful and precise.
But even as supplemented, it's still vulnerable to causal complications like its simpler predecessors.
Some years ago, there was a study showing a positive correlation between coffee drinking and certain kinds of cancer.
That caused great consternation in people like me.
But it turns out that a higher proportion of coffee drinkers than non-coffee drinkers smoke cigarettes.
So the correlation between coffee drinking and cancer is the term used here usually is screened off by the correlation between smoking and cancer.
If you remove that correlation, then there's nothing yet scary for coffee drinkers like me.
We'll look back at statistical methodology and talk late in the course about why so many conflicting medical studies come out.
We turn now the last of Mills' methods, the method of residues, which applies when we know what part of a phenomenon is due to the effect of certain causes.
And then we infer that the rest of the phenomenon is due to the causes that remain.
So when my veterinarian wants to weigh my dog, she first gets on the scale, weighs herself, and then picks up the dog and weighs the dog-vet combination,
then all you have to do is use a little subtraction and she's got the dog's weight.
Like the other methods, there's something commonsensical and appropriate here, it certainly has its uses.
But causation is once again more complicated than the method will allow.
This method assumes that causes are additive.
But that's not always the case.
Cream gravy makes biscuits taste better.
And jelly makes biscuits taste better.
But cream gravy and jelly together make biscuits disgusting. Their causal powers are not additive.
Perhaps more seriously, though I insist that biscuits are themselves a serious matter, chemical effects are typically not additive.
The properties of sodium, which explodes when you place it in water, and chlorine, a poisonous gas at room temperatures, don't add together to make table salt.
So in general, there's no denying that Mills' methods are on to something.
They figure in good scientific practice even today.
But the reason we're focusing on them is because their limitations are especially illuminating to us.
They're the limitations of a relentless empiricism.
And that causes the methods to try to establish too much on the basis of too little.
We've so far focused on the fact that the methods have a hard time with causal complexity.
Causation is not always additive.
Two phenomena can be products of a common cause.
The same phenomenon can be the product of different causes, and the method has a hard time sorting through all that.
We've seen that the observable data won't yield the theoretical conclusions the method wants.
The method wants to zero in on the cause of a phenomenon.
And the methods don't suffice to establish the uniqueness of the phenomenon.
There's too much causal complication that's built in to the notion of a method.
And so Mills can't use the process of elimination that he wants to on the relevant causes to find the right one.
The relations can just be more complex.
Nevertheless, the real problem here is that elimination has happened earlier in the process than Mills realizes.
The methods can be applied only if we have a list, as it were, of all the circumstances that might be relevant to the phenomenon in question.
But any two cases are similar in limitless ways, and different from one another in limitless ways.
The whole idea of eliminating causal possibilities wouldn't tempt us if we didn't think we could narrow the causal possibilities down to a manageable list in the first place.
That's what it takes to apply the process of elimination that Mills' methods involve.
But when you think about it, just about anything might be causally relevant.
Maybe gumbo only makes you sick if you've been listening to bad jazz before dinner.
Critics of Mills' empiricism insist that we have to bring some kind of category scheme or theory or hypothesis to experience before we are in a position to learn from observations.
The great German philosopher of the 18th century, Immanuel Kant, roughly says,
theory without observation is empty, but observation without theory is blind.
Mills tries, according to this criticism, to make observation do the work of theory.
So Kantian critics will say that without some theory or hypothesis, we couldn't so much as gather data that bears on our questions at all.
There's an interesting dispute between Mills and his neocontian contemporary, William Huell,
about Johannes Kepler's discovery of the elliptical orbits of the planets early in the 17th century.
Mills, being a good empiricist, insisted that Kepler discovered the explanation in the data,
while Huell insisted that Kepler brought the hypothesis to the data and imposed it therein.
It was a creative act of scientific unification, not something read off the data.
Mills doesn't want hypotheses to be taken seriously until they're tested by something like his methods,
while Huell thinks hypotheses have to already be taken seriously before you're in a position to do any testing.
There's an additional empiricist constraint built into Mills' methods that's worth noting.
The methods don't allow any role for hypotheses that make reference to unobservable objects.
The methods always talk about a causal correlation between something observable and something else that's observable, patterns in experience.
But this is a pretty significant limitation.
Mills' methods by themselves won't let us infer that there's some unobservable organism in the water that causes an outbreak of disease,
because that's not part of the observationally given phenomenon.
This is science done within the parameters of Locke and Modesty back from Lecture 5.
Experience constrains what we're in a position to say.
Now, turning back to our period, Popper and the positivists essentially side with Huell and the Neocontians in the dispute about the status of hypotheses.
They drew a distinction that's going to crop up a few times in the course between the context of discovery and the context of justification.
In the context of discovery, they say, there is nothing worth calling a rational method.
Worthwhile scientific hypotheses can be generated by luck, by hard work, by genius, but not by anything like a logic or a method.
Creativity just won't get regimented that way.
Now, there may be valuable things to learn about how to think creatively or how to get ideas from tables of data,
but that's an empirical inquiry. It belongs to psychologists, to historians of science, not philosophers or logicians.
And we see here this idea common to the positivists and Popper, that philosophy is an a priori discipline that doesn't have its own empirical turf.
When we turn to justification, however, the philosophers think they have a role.
There can be a logic or method for testing hypotheses once they have been generated from whatever source they've been generated.
So in mathematics, we can't provide anything like a recipe for generating a possibly provable theorem.
But once one has been suggested, we can use deductive logic to check the proof to see whether it's a good one or not.
Similarly, the structure of the benzene molecule allegedly appeared to its discoverer in a dream of a snake eating its own tail.
Now, that's quite interesting. Popper and the positivists would say it tells us something about human psychology,
but it tells us nothing about what justifies the hypothesis that that's the right picture of the benzene molecule.
Where the hypothesis comes from has nothing to do with its justification.
There's a logic or method of empirical testing, and that justifies hypotheses.
The origin of hypotheses does not depend on any method.
So Popper and the positivists not only distinguish between discovery and justification,
they identify discovery with history and psychology, not epistemology.
This is in part because they think of epistemology and methodology as modeled on deductive logic,
and it's seen clear to them that there can be no logic of suggesting creative, interesting hypotheses.
If you loosen this conception of method, though, discovery has been making a bit of a comeback recently,
especially in artificial intelligence circles.
Most people will grant that there's no logic of discovery or methodology in the classical sense,
in the sense of an approach that will guarantee that you get true or reasonable hypotheses no matter what field you're studying.
There's nothing that general or that reliable.
But the details of this belong more to philosophy of mind and artificial intelligence than they belong to us.
It's worth noting that this old-fashioned classical conception of discovery is really back on the scientific agenda in surprising ways.
In part, this is due to the rise of the computer, because a computer has to be given simple steps to perform,
and you can try to teach a computer how to generate or discover from the data using simple steps,
hypotheses that have a reasonable chance of succeeding.
The rise of computational power has made a logic of discovery seem possible in a way that it never had before.
The results so far are limited, but they're very, very far from trivial.
So the idea of making at least part of scientific creativity rule-governed, in a loose sense of rule,
but a rule here roughly means something you can program a computer to do,
has come back since 1960 after being dead for a century or more.
So to take one example, you can run a kind of Darwinian model of artificial intelligence.
You have blind variation of hypotheses, and you retain ones that have certain desirable features.
And then you repeat the process, retaining features of the hypotheses that fit the dataset pretty well.
If you run this for 30 to 45 generations, you actually get really remarkable results.
There's no sort of creativity in the romantic sense going on here.
This is a methodology that generates interesting plausible hypotheses by application of a kind of mindless recipe.
There are lots of strategies that aren't just the Darwinian ones,
and there's some reason to think that human beings do some of our discovery this way,
that it's not a matter of formulating hypotheses and then testing them,
that the data sort of pushes our buttons and suggests hypotheses in certain ways.
So the classical conception of methodology seems old fashioned to a lot of people,
but there may be more to it late in the 20th century than our main characters,
writing early in the 20th century, would have given it credit for.
So next time we're going to return to the beginnings of our period, early in the 20th century,
with our empiricists chastened about the logic of discovery.
They've realized the problems with Mill's methods,
that it's not going to zero in on the right explanation for the data.
But they are ambitious about the logic of justification, if not about discovery.
They want to provide a methodology that is objective and will tell us
to what extent we've got justified scientific hypotheses.
