I feel obliged to begin this lecture with a kind of modest warning, because we are
fixin' to commit some mathematics in here.
I believe it will be intelligible for audio customers without too much difficulty, and
I would encourage people not to pay exceptionally close attention to the mathematics if they're
finding it difficult or distracting.
The points that are needed will get stated non-mathematically either right before or
right after the mathematics, but I do want to get for the record the formulations that
we're officially talking about before you.
Nevertheless, I would like to encourage anybody driving in big city traffic to just let the
math go if it's requiring too much concentration, I don't want to be responsible for any accidents.
I'd also like to assure people that I do not perpetrate mathematics lightly.
I think this is going to be a very useful avenue into some problems of real human heft
and significance, and I'll try intermittently in this lecture and quite substantially at
the end of the next lecture to indicate why I think the mathematics is an avenue into
something that is by no means dry and technical, but really quite immediately important.
Starting from very modest resources, the Bayesian approach to probability has rejuvenated
in recent decades philosophical thinking about confirmation and evidence.
I'll explain why it's called Bayesianism in a little while.
Bayesianism begins with what we called last time the subjective interpretation of probability
statements.
A probability statement reports or characterizes personal degrees of belief.
These degrees of belief are more or less measured by betting behavior.
The more unlikely you think a statement is, the higher the payoff on which you'd insist
in order to accept a bet on the truth of a statement.
Orthodox Bayesians don't require your beliefs to line up with any observed frequencies,
any supposed hypothetical relative frequencies, and they don't impose a kind of logical principle
of indifference as the logical interpretation of probability did last time.
So subject to a few additional constraints which will impose shortly, your degrees of
belief for an Orthodox Bayesian are your own business.
If for whatever reason you decide that it's unlikely that copper conducts electricity,
Bayesianism so far anyway has no gripe with that estimate as long as it coheres with your
other beliefs.
So you'd better not then assign a high probability to metals conducting electricity, unless you
then assign a low probability to copper being a metal.
However you want to maintain a coherent belief set is okay with the Bayesians.
Now deductive incoherence means that your beliefs contradict one another.
Probabilistic incoherence is a bit trickier.
And the idea that this kind of coherence is a requirement of rationality is generally
established by what is called the Dutch book argument.
The origin of this term is shrouded in mystery.
I anyway do not intend any offense to our Netherlandish friends, but it may well have
been a slur back in the day.
To say that a Dutch book can be made against you is to say that if you were to put your
degrees of belief into practice, you could be turned into a money pump.
I'll explain what a money pump is forthwith.
If I assign say a 0.6 probability to the proposition that it will rain today, and also
a 0.6 probability to the proposition that it won't rain today, I haven't straightforwardly
contradicted myself.
It's not clear whether the world would have to do to make either of these statements false.
The problem emerges when I realize that by my own lights I should be willing to pay $6
for a bet that pays $10 if it rains, and nothing if it doesn't, and I should be willing
to pay $6 for a bet that pays $10 if it doesn't rain, and nothing if it does.
At the end of the day, if I've taken both of these bets, I'll have spent $12, and
I've gotten back $10, no matter what happens out in the world.
And it does seem like a kind of failing of rationality if acting on my beliefs would
guarantee that I would lose money no matter what happens to happen in the world.
So that's the Dutch book, that's what it is to be money pumped.
It's to be assured of losing money no matter how contingent events go.
This is supposed to be a theoretical problem, not a practical one.
The idea is not that there's a Dutch bookie on every corner waiting to take your money.
You might be morally opposed to gambling, you might just get suspicious, something might
sound funny if this sequence of bets is offered to you.
So it's not a worry about your financial future here, it's the fact that your degrees
of belief are such that if you were to act on them, you'd be assured of losing money
that's supposed to convince you that you're behaving irrationally.
Now it can be shown mathematically that if your degrees of belief obey the probability
calculus, that's what follows from the axioms of probability we discussed last time, then
no Dutch book can be made against you.
It can also be shown that any violation of the probability calculus makes you vulnerable
to a Dutch book.
So obeying your degrees of belief obey the probability calculus is a necessary and a
sufficient condition for avoiding a Dutch book.
Now so far this is just a slightly more detailed version from a different angle of the stuff
with which we ended last time.
It is swell to have probabilistically coherent beliefs, but that requirement still permits
intuitively crazy beliefs like paranoid delusions, and none of this seems to have very much to
do with science so far.
Well let's start fixing that.
We want to make this a theory of scientific rationality, not mere coherence.
We're going to pay special attention to how we get a theory of scientific rationality
by imposing really very modest additional constraints beyond mere probabilistic coherence.
Back in lecture 12, which concerns such matters as the Raven paradox, we saw Carl Hempel struggling
to characterize a qualitative notion of confirmation.
He wanted to talk about what it is for a piece of evidence to confirm a hypothesis.
The question of how much confirmation the evidence provided for the hypothesis is supposed
to get answered later.
We're just tackling the basic question, Hempel thinks, of whether something is evidence for
something else.
The Bayesians think this is a mistake.
For a Bayesian, the notion of confirmation or evidence is quantitative right from the
start.
We can't ask whether a given bit of evidence confirms a hypothesis unless we know how probable
the hypothesis started out being.
So in other words, in order to confirm a hypothesis, it has to start off with a prior probability.
A piece of evidence confirms a hypothesis just in case the evidence raises the prior
probability of the hypothesis.
And this means that the probability of the hypothesis, given the evidence, is higher
than the probability of the hypothesis had been without the evidence or before the evidence
came in.
Similarly, a piece of evidence disconfirms a hypothesis if the probability of the hypothesis
given the evidence is less than the prior probability of the hypothesis had been.
Now all of this is done within this subjectivist or personal interpretation of probability.
We're talking about what counts as evidence for you.
So a big dark cloud on an otherwise clear horizon counts as evidence of rain for me
just in case my subjective degree of belief that it will rain, given the new information
that there's a big dark cloud on the horizon, is higher than my prior probability that it
would rain had been.
Now in saying this, I've tacitly invoked an ocean called conditional probability.
The probability of a hypothesis conditional on, or as I'll often say, given the evidence.
Now here comes a mathematical definition of conditional probability.
I'm stating it for the record because we're going to derive Bayes' theorem from it.
We won't do the derivation, but it's important for deriving Bayes' theorem.
It's not important that you memorize it or get the whole thing as it's being stated.
So the conditional probability of a hypothesis, given the evidence, is the probability of
the hypothesis and the evidence together divided by the probability of the evidence.
Provided the evidence has a non-zero probability, we don't want to be dividing by zero.
The probability of the hypothesis and the evidence together is the intersection of those
two sets, as it were.
It's the overlap of cloudy days and rainy days.
And the definition of conditional probability says that the higher the percentage of cloudy
days which are rainy, the higher the conditional probability of the hypothesis that it will
rain on the evidence that there's a dark cloud on the horizon.
So this is just a mathematicization, I think that's a word, of a pretty intuitive notion.
Now let's connect this to the raised probability conception of confirmation.
Had I already been convinced that it would rain because of the weather report or because
the voices in my head told me so, then this high conditional probability of rain, given
the cloud on the horizon, would not raise my prior probability and so wouldn't count
as evidence for me of rain.
It's on the assumption that I had been relatively neutral, that the new information that there's
a big dark cloud on the horizon would confirm by raising the prior probability of the hypothesis
that it would rain.
Now let's briefly note that the idea that whatever raises the probability of a hypothesis
confirms that hypothesis has some counter-intuitive consequences.
If I see Robert De Niro on the street, that probably raises, slightly, my subjective probability
that he and I will make a movie together, but it hardly seems to count as evidence that
we're going to make that movie.
Maybe it's a teeny, tiny bit of evidence, but we're going to set such objections aside.
It's quite common to think that for a bit of evidence to bear on a hypothesis positively
is to confirm it.
So the raised probability conception of evidence is quite common and I'm going to assume it,
but I want you to note it's not completely uncontroversial or unproblematic.
So that's the first thing we add to the notion of probabilistic coherence, as a theory of
evidence as probability razors.
The second thing we need to add is that beliefs should be updated in accordance with something
called Bayes' Theorem.
Thomas Bayes was an English clergyman who proved this theorem in the 18th century.
The theorem is a straightforward consequence of the definition of conditional probability
along with the definitions that make up the probability calculus that we talked about
last time.
It's an uncontroversial mathematical result.
Non-Basians accept Bayes' Theorem just as surely as Bayesians do.
You can apply it using a frequency theory of probability.
The truth of Bayes' Theorem is not at issue.
It's the use of Bayes' Theorem that makes a Bayesian a Bayesian.
Under the Bayesian interpretation, the theorem tells us how we are to update our beliefs
in the light of experience.
And it's this idea that's going to turn Bayesianism from a kind of unassuming theory
of mere probabilistic coherence into a somewhat promising theory of scientific reasoning.
These thin constraints on our subjective probability assignments that they just have to cohere
across time get some teeth as we impose a requirement of probabilistic coherence not
just at a time, but across time.
I'll explain that momentarily.
First, let's just get the classic statement of the theorem.
You don't have to take this in all at once.
I'll be drawing on this repeatedly throughout the lecture.
So the probability of a hypothesis given the evidence, that's just the left-hand side.
That's the conditional probability of the hypothesis on the evidence.
We're defining that notion.
It's equal to the probability of the evidence given the hypothesis, that's in the numerator
of the right-hand side of the equation, times the prior probability of the hypothesis,
and those two things are divided by the probability of the evidence.
That's the theorem.
If you've got your booklet, it'll be lovely to have in front of you.
If you're driving your car, don't worry about what I just said.
The left-hand side of the equation we've already talked about, that's just a conditional probability.
But it can be given two different readings depending on the time at which we're considering
things.
Depends on whether the evidence counts as already in or not.
If the evidence isn't yet in, we interpret the left-hand side, the probability of the
hypothesis given the evidence, as the prior conditional probability of the hypothesis
on or given the evidence.
And the agent's background beliefs count in here too.
I'm sort of oversimplifying by leaving those out.
So it's what your current beliefs are about how probable the hypothesis would be if a
certain piece of evidence were to come in.
So if I were a physicist in 1915 or so, I might have assigned a low probability, subjective
probability, to Einstein's hypothesis of general relativity.
But I also might have thought to myself, you know, if it turns out that light rays are
bent by the sun, I will end up assigning quite a high probability to Einstein's hypothesis
of general relativity.
We don't generally go around declaring our prior conditional probabilities like that,
but that's the kind of probability we're talking about if the evidence is not yet in.
If the evidence is in, then the number on the left-hand side is called the posterior
probability of the hypothesis.
It's the probability that you assign to the hypothesis now that the evidence has come
in.
Once you discover that light rays are bent by the gravitational field of the sun, what
probability do you in fact assign to the hypothesis?
That's the simple part, the left-hand side.
We turn now to the right-hand side of the equation.
The probability of the evidence, given the hypothesis, is a measure of how unsurprising
the evidence is, if it's in, or would be, if it's not yet in, given the hypothesis.
So given Einstein's hypothesis of general relativity, the probability that light rays
are bent by the sun's gravitational field is quite high.
The probability of the evidence on the hypothesis is quite high.
That gets multiplied in the right-hand side of the equation by the prior probability of
the hypothesis.
However probable a given person thought Einstein's theory of general relativity was.
Since these numbers are in the numerator of the fraction, the posterior probability,
it could also be the prior conditional probability, there are two interpretations of that, I'm
just focusing on one for convenience's sake.
The posterior probability goes up when these go up.
The more expected the evidence would be given the hypothesis, and the more likely you thought
the hypothesis beforehand, the more likely you're going to find the hypothesis afterwards.
The prior probability of the evidence, apart from any hypothesis, is in the denominator
of the fraction.
This reflects the fact that all other things equal, unexpected evidence raises posterior
probabilities a lot more than expected evidence does.
Apart from Einstein's theory, nobody would have thought the probability of light being
bent by the gravitational field of the sun was very high.
It's because Einstein's prediction is so unexpected, except in the light of Einstein's
theory, that the evidence has so much power to confirm Einstein's theory.
That's the heart of Bayes' theorem right there.
The more unexpected a given bit of evidence is, by one's own subjective lights of course,
apart from a given hypothesis, and the more expected it is according to the hypothesis,
the more confirmation the evidence confers on the hypothesis.
So your hypothesis is doing well if it makes something otherwise really quite surprising,
quite unsurprising.
That's the heart of Bayes' theorem.
And this is just a mathematical representation of that fact that turns out to have some other
uses as well, but it's not that hard.
The controversial part arises when the Bayesian proposes, as a rule of rationality, that once
the evidence comes in, the agent's posterior probability for the hypothesis, now that the
evidence is in, should be equal to the agent's prior conditional probability for the hypothesis
were the evidence to come in.
That's a mouthful.
Let me English it up.
It sounds like it should be uncontroversial.
These are just the two different ways of interpreting the left-hand side of the equation.
If the evidence isn't yet in, it's the prior conditional probability.
If the evidence is in, it's the posterior probability.
And the Bayesian says your posterior probability, once the evidence comes in, should be the
same as your prior conditional probability were the evidence to come in.
What's controversial about that?
Well, the math won't get you this result.
This is an additional requirement because there are other ways I could maintain probabilistic
coherence.
Once I get the results of the Eddington Eclipse experiment that light is, in fact, bent by
the rays of the sun, I could maintain my probabilistic coherence by altering some of my other subjective
probabilities.
I could decide I don't like that Einstein guy.
Let me tweak some different numbers on the right-hand side of the equation.
I could decide, well, this evidence wasn't so surprising after all.
So I can make my posterior probability, now that the evidence is in, different from my
prior conditional probability, what I had said I would think were the evidence to come
in.
It's a substantive claim that those two things have to be identified.
It's a substantive claim that today's priors should be the same as tomorrow's posteriors
if the evidence comes in.
You don't get that for free.
The Bayesian has an argument, however.
It's usually called the diachronic.
That just means overtime rather than at a time, Dutch book argument.
Our first Dutch book argument was synchronic, it was at a time.
If your degrees of belief are incoherent at a time, you can be offered a series of bets
that will guarantee that you'll lose money.
A similar argument can be run across time.
If you use any rule for updating your beliefs, it's got to be a rule, not a whole bunch of
incoherent principles.
If you use a single rule to update your beliefs, then a bookie who knows what rule you use
can offer you a series of bets across time.
Some of these bets will depend on your future degrees of belief.
If you act on your present probability assessments and your present conditional probability assessments,
then you'll be guaranteed to lose money.
It's a Dutch book argument that considers you at various times.
If you update your beliefs in any way other than by Bayesian conditionalization, you'll
be guaranteed to lose money.
This is not generally thought to be as airtight a Dutch book argument as the synchronic one,
but it's non-trivial.
It does suggest there's a kind of rationality across time that Bayesian conditionalization
gets you.
Now, if we pull all this together, we get an interesting approach to scientific reasoning.
The Bayesian approach allows for an impressive range of subjectivity.
There are very few constraints on prior probabilities.
They have to cohere with your other degrees of belief.
But also impressive objectivity.
There is exactly one correct way of updating your beliefs in the light of new evidence.
So this is a view that has the potential for incorporating a great deal of the literature
on confirmation that we've seen earlier in this course.
So for instance, it can capture a central part of what the logical positivist had wanted,
as well as a central part of what Kuhn, the great critic of the positivist, had wanted.
Even in cases of revolutions, it could be the case that all of these scientists are
following the same scientific procedure, namely Bayesian updating of prior probabilities.
But the deep disagreements about whether we should be Newtonians or Einsteinians or whatever
are explained by plugging in different probability judgments into the very same equation.
So Bayesianism has room for the rules of the positivist, a sort of precise logical mathematical
rule about what rationality involves, but also for Kuhn's values, which are much more
permissive, permit much more disagreement, while allowing everybody to meet standards
of scientific rationality.
So there's a sense in which Bayesianism offers a bit of commensurability across scientific
revolutions, though perhaps not as much as the positivist would have wanted.
It offers the prospect that scientific disagreement can be rational and discussable, because we
just plug the right values into Bayes' theorem, while also making room for lots of idiosyncratic
views about what the world is like and how a given bit of evidence bears on a given theory.
You and I have the same conception of rationality, but we start from different places and have
different conceptions of how the evidence bears on our theories.
Bayes' theorem also seems kind of anti-metaphysical in a way that would have pleased both the
positivist and Kuhn.
The kind of rationality and scientific norm defended is coherence.
There's no direct argument that a coherent set of beliefs will get the world right.
This notion of the way the world is seemed metaphysical to Kuhn and the positivist.
But there does seem to be something scientifically valuable about updating your beliefs in a coherent
way as new evidence comes in.
And if you do that, arguably you've lived up to all of the scientific norms that really
matter.
So, we've really got parts, at least, of the best of both worlds going here in this
pretty simple mathematical setup.
For reasons like these, Bayesianism helped rekindle issues about, sorry, interest in
issues about evidence and justification.
After the demise of the positivists and their theory of inductive logic, people had kind
of shied away from these issues.
We need to delve a bit more deeply, though you'll be pleased to know no more mathematically
into Bayesianism in order to see how Bayesianism helps us get going on these problems about
evidence and justification.
While allowing that there's a sense in which any set of beliefs that maintains probabilistic
coherence across time, not just at each time, you can do that without Bayesian conditionalization.
The idea is you have to maintain coherence across time.
That's the Diachronic Dutch book.
Nobody who does that, the Orthodox Bayesian says, is in an important sense beyond epistemic
reproach.
Such a person is doing everything that rationality requires.
Nevertheless, despite all that subjectivity, Bayesians think they can account for scientific
agreement.
They argue that the initial subjectivity starts to disappear when enough good evidence comes
in.
This is called the washing out or swamping of prior probabilities.
It can be established that no matter how great the disagreement between two people is, there
is some amount of evidence that will bring their posterior probabilities as close together
as you would like.
So no matter how much you and I start off by disagreeing, if the right quality evidence
comes in, it can be mathematically shown that we will come very, very close to agreeing.
That's an impressive result, but it's subject to some significant limitations.
For one thing, if either of us assigns a prior probability of zero to a hypothesis, no evidence
will ever increase that probability.
It's in the numerator of the fraction, you multiply by zero, you're going to get zero.
So Bayesians add this pretty small and controversial additional requirement that it's not okay
to assign a probability of zero to any contingent proposition, any proposition that could possibly
be true.
It's a modest additional restriction on rationality that they impose.
We won't worry about that one.
We should also note that there is no assurance that this convergence will happen in a reasonable
amount of time.
In the long run, we're all dead.
The amount and kind of evidence it would take to overcome some degrees of subjective
disagreement is really quite epic, so we don't want to oversell the washing out of the priors.
More importantly, the washing out results require that the agents agree about the probabilities
of all the various pieces of evidence given the various hypotheses in question.
So they have to agree in all of their judgments about how unsurprising each hypothesis renders
each bit of evidence, and that's a pretty impressive requirement of agreement to impose
on two people who disagree fairly massively.
It suggests, unrealistically, that people can really disagree about the probability of
hypotheses while agreeing about the probability of evidence bearing in particular ways on
the hypotheses, but it might not be as bad as it sounds.
In many cases, it's really quite clear how probable a given hypothesis renders a given
bit of evidence.
Typically, one might think, a hypothesis, along with auxiliary hypotheses, we never
escaped this Quinean problem of Holism we saw way back when, often a hypothesis entails
the evidence.
If my hypothesis is true, a given bit of evidence must be true.
If my hypothesis is, all copper conducts electricity, and my evidence is, this is a bit of copper,
the probability of that evidence on my hypothesis, connecting electricity, is one.
Those are often really well-behaved probabilities, so you can get more agreement than you might
think about how different hypotheses are borne on by evidence.
We can further examine Bayesianism's credentials as a theory of scientific inference by applying
it to a couple of our old friends.
The new riddle of induction and the Raven paradox, a couple of our brain breakers from
the first third of the course.
Goodman's new riddle seems like much less of a threat to the Bayesian, since at least
for most of us, the prior probability that the next emerald observed will be green is
much higher than it is for the hypothesis that the next emerald observed will be grew.
So, at any time before January 1st, 3000, the other parts of the equation will function
identically.
Each hypothesis confers a probability of one on finding a green or a grew emerald.
Before January 1st, 3000, the emeralds look the same.
And the denominator is the probability independently of either hypothesis, of the hypothesis that
all emeralds are green, and of the hypothesis that all emeralds are grew, of the next emerald
being green or grew, which again is identical up until January 1st, 3000.
So every number in Bayes' theorem is going to be the same except for the prior probability
of each hypothesis.
And since you and I attach a much higher prior probability to the hypothesis that all emeralds
are green than we do to the hypothesis that all emeralds are grew, we will continue as
new evidence comes in to attach a much higher probability to the hypothesis that all emeralds
are green than that all emeralds are grew, and we do so quite rationally.
Since the probability of getting a green or a grew emerald is pretty high, once we've
gotten emerald, we both expect that it's going to look a certain way, there's not much
confirming power.
That's the number in the denominator.
If the probability of getting the evidence by my lights is reasonably high, it doesn't
confirm my hypothesis very much.
But it doesn't confirm either hypothesis very much, and so it's only the prior probability
of the hypotheses that are going to do any work here.
So the Bayesian is going to say that there's nothing to matter with either inductive argument.
It's fine to infer from the grudeness of emeralds to their continued grudeness or from the greenness
of emeralds to their continued greenness.
But whichever hypothesis you think had been more probable going in will remain more probable
going out, and so we can preserve our confidence, our rational confidence in the hypothesis
that all emeralds are green.
If you're a grew speaker or a very peculiar green speaker, you might have gone in with
a higher probability that all emeralds are grew than green, in which case you'll come
out with that probability.
But it's not clear that that's a mistake.
Bayesians can handle the Raven paradox more or less equally straightforwardly.
As we noted back in the initial discussion of the paradox, it's easier to swallow the
idea that a white shirt confirms the statement, all ravens are black, if we make room for
a quantitative, rather than a qualitative, notion of confirmation.
Hemple didn't do that when he first formulated the paradox.
That allows us to say that a white shirt does confirm all ravens are black, there was some
convoluted logic behind that, that I won't march us through again.
But we might be able to live with that if the evidence of a white shirt confirms the
hypothesis that all ravens are black much less than a black raven confirms all ravens
are black.
It doesn't seem like such a difficult pill to swallow at that point.
Now if we look at Bayes' theorem, we'll see that the greater the ratio of the probability
of the evidence given the hypothesis to the probability of the evidence, the greater the
power of the evidence to confirm the hypothesis.
Once more with feeling, that just means the more expected our hypothesis makes the evidence,
and the more unexpected the evidence, the better things look for our hypothesis.
And that's the source of the difference in the confirming power of white shirts and black
ravens to confirm all ravens are black.
The probability that the next raven I see will be black given that all ravens are black
is one.
That's the probability of the evidence given the hypothesis.
That's much higher than the probability that the next shirt I see will be white given that
all ravens are black.
That hypothesis does not make that evidence particularly likely.
It's pretty much just my prior probability that the next shirt I see will be white.
That's the heart of the matter right there.
That's the difference between the confirming power of a black raven with respect to the
hypothesis that all ravens are black versus that of a white shirt with respect to all
ravens are black.
So these are just some small illustrations of how Bayesianism has some real potential,
starting from really quite modest resources to become a plausible theory of scientific
reasoning.
It captures much of what Kuhn had wanted, much of what the positivists had wanted, and
it looks like it can be applied to cases in a way that's simultaneously illuminating
and somewhat plausible.
Next time we'll start looking at some criticisms of the Bayesian approach.
