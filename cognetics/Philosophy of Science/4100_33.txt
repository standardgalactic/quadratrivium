Lecture 33, Entropy and Explanation
I'd like to begin winding down the course by looking at some examples from within the philosophy of particular sciences.
There are a couple of reasons for doing this.
First, it's just plain useful to see a more detailed and hence more realistic treatment of how some of these general issues in the philosophy of science play out in closer contact with scientific practice.
In addition, most philosophy of science these days is philosophy of a particular science.
This reflects the influence of Kuhn and of naturalized philosophy, according to which it's important to pay serious attention to actual scientific practice
because the izzes of science determine to a great extent the aughts of science.
Nevertheless, a kind of reciprocal illumination is also to be expected.
We can understand some of the issues in general philosophy of science more clearly when we see them deployed less abstractly with more factual content and with more context.
So it's not the case that general philosophy of science simply gets applied unproblematically within particular sciences.
The road between general philosophy of science and the philosophy of the more particular sciences runs both ways.
New problems about explanation, about confirmation, about the meaning of scientific terms emerge from our reflection on particular sciences, not just on science as such.
To some extent applications sometimes do and sometimes should drive theory rather than theory dictating how the applications should go.
The illustration I've chosen for this lecture is from the philosophy of physics and will begin rather innocuously, but will end up raising some really quite surprising questions about explanation, about laws, and about scientific reduction, or so I hope.
The issue can be stated briefly if rather peculiarly. Why can't I stir milk out of my coffee?
This may not sound like the kind of why question that needs an answer, but it's less ridiculous than it sounds.
In one sense it turns out I can stir milk out of my coffee, or I could if I actually put milk in my coffee.
The basic laws of nature, both classical and quantum mechanical laws, permit milk to be stirred out of coffee.
We will assume a nice simple Newtonian mechanics that'll do no harm in our context.
The basic laws of motion permit all the gas in a container to cluster in one corner of the container.
And they permit heat to flow from a metal bar that's been kept in the freezer to a metal bar that's been kept in a hot oven.
There is nothing in the basic laws of motion specifying in which direction molecules are supposed to move.
For any actual motion of molecules, a reverse motion is permitted by the basic mechanics.
Just change all the velocities that are positive to negative and all the velocities that are negative to positive.
You violated no law of nature, you get a physically permissible motion.
But though those things are possible, they never happen.
They're permitted by the basic laws, in this case roughly statistical mechanics, the science of molecules in motion.
But the laws that in some sense reduce to, or at least are thought to reduce to these more basic laws,
the laws of thermodynamics tell us that air never leaks into a punctured tire
and a hot bowl of soup does not spontaneously heat up when brought to the dinner table.
So the underlying laws tell us that these things can happen while the overlaying laws tell us that they never do happen.
The second law of thermodynamics, as we saw some lectures ago,
thermodynamics concerns such phenomena, directly observable phenomena in a loose sense, as temperature and volume.
It's a phenomenological science.
And the second law of that science says, speaking loosely, that energy tends to spread out to dissipate.
Like some of my graduating seniors, concentrations of energy become dissipated and increasingly unavailable for work.
That's the second law of thermodynamics. Again, loosely stated.
Another way of saying this is that entropy, this is a term that we'll figure frequently in this lecture,
it's a measure of this dissipative tendency.
Second law of thermodynamics says that entropy will tend to increase.
We don't need a rigorous understanding of it, we just need a basic understanding.
A system that is energetically isolated, a system from which energy is neither added nor removed,
will tend to move toward an equilibrium state, one which is unchanging at the phenomenological or macroscopic level.
In other words, something like heat will spread out and it will stay spread out.
Question is, why does the second law hold, since there's nothing about the underlying laws of motion
taken just by themselves to make the second law hold?
This question started to receive a lot of attention in the middle of the 19th century,
mainly occasioned by the steam engine, which is about turning heat into work and things like that.
Starting in the 1870s, Ludwig Boltzmann, an Austrian physicist and mathematician,
worked out the two most influential answers to this question of why the second law of thermodynamics holds,
though it seems to have taken him a while to appreciate that he had worked out two quite distinct answers to the question.
Boltzmann's first answer is that the effect of collisions between rapidly moving gas molecules
will tend to bring it about that entropy will increase until it reaches a maximum value.
It was soon pointed out, though, that since the underlying laws of mechanics permit motions
in the opposite direction of the motions that actually happened,
this explanation will be statistical, not deterministic.
And this touches on some issues we've raised earlier in this course.
Well before the rise of quantum mechanics, we have the idea broached that the second law of thermodynamics
is probabilistic, not deterministic, and that this is a basic law.
There are possible motions by which entropy would decrease, even though it seems always to increase.
Boltzmann's second answer builds on this realization about his first answer.
Rather than claiming that collisions among molecules cause entropy to increase,
he suggests in the second answer that there are just more ways for particles to be spread out
than there are for particles to be concentrated.
It's a matter of different degrees of what we have called multiple-realizability
in our discussion of scientific reduction.
So the idea here is that you don't need an interesting causal story
about why you generally get dealt a bad hand at poker.
Assuming a random distribution of cards, there are just a lot more bad hands
than there are hands that make, say, a straight flush.
Given just random changes in motion, running the analog here,
if an isolated state is in a low entropy state, a not very dissipated state,
it's highly likely to move to a higher entropy state,
simply because there are so many more higher entropy states than lower entropy states.
So on this interpretation, the second law again becomes statistical,
and it stems from the fact that almost all of the possible distributions of particles at the micro level
together realize high entropy states, high dissipation states at the macro level.
So far, so good.
Isolated systems tend toward equilibrium states either because of the laws of mechanics
or because most of the possible states are themselves equilibrium states.
The first explanation has the second law of thermodynamics serve as a causal law
in a pretty robust sense.
Collisions cause increases of entropy.
There is a real tendency in nature that operates to bring about increasing dissipation.
This is most naturally construed as a version of the necessitarian conception of a law
that we saw back in lecture 22.
It makes entropy increase.
The other explanation looks friendlier to an empiricist or regularity conception of law.
On this view, the tendency towards entropy is just the sort of thing that always happens.
Nothing is posited to make it happen.
The tendency towards entropy is just what's to be expected given the laws of nature,
but there's no making, there's no mechanism invoked.
Either way, though, we're going to run into a peculiar puzzle.
Let's take the more empiricist explanation first.
This explanation of why entropy increases into the future should equally well explain why it increases into the past.
That's a weird formulation. Let me unpack that.
Just as almost all of the states a closed system can move to are high entropy states
because of the way that probabilistic distributions work,
the explanation here is more or less mathematical.
But the same explanation indicates that just about all the states,
it could have moved from our high entropy states.
So the same explanation should work in both temporal directions.
For the same reason that non-equilibrium states move towards higher entropy,
they should themselves have come from higher entropy states
because there are so many more higher entropy states than there are lower entropy states.
So entropy should increase as we move towards the past,
just as it does as we move towards the future.
But that never happens.
That would be things concentrating as they move towards the present.
There's a temporal asymmetry at the observable thermodynamic level
that this explanation doesn't seem to account for.
Now the other, the more causal explanation, might seem more promising in this respect.
There's still a worry about a time asymmetry here
because, though I didn't put it this way initially,
our earlier point about the physical possibility of the motion of particles being reversed
shows that the basic laws of motion are time symmetric.
Every motion that can happen forward in time can happen backwards in time.
You can, as it were, run the universe's movie backwards
without violating any laws of nature.
This is a kind of peculiarly underappreciated fact about the laws of mechanics.
Nevertheless, if we can appeal to facts about collisions
to provide a mechanism for entropy to increase,
then we finally have a time asymmetry built into our system.
It ends up, though, being very complicated trying to formulate
what molecules and their collisions must be like
in order for these collisions to bring about increased entropy.
There are other mechanisms, for instance, from quantum mechanics that have been proposed,
but there is no generally accepted mechanism that would provide this time asymmetry.
This is remarkable because we're looking for a law on which to base the direction of time.
Without such a law, without such a causal mechanism,
we don't seem to have any reason for thinking that heat and gases, that sort of stuff,
should be moving in the direction of dissipation rather than concentration.
If we don't have a mechanism to account for it,
and if the probabilistic distributions seem neutral between those directions,
we have a problem because the laws of motion don't have a built-in temporal direction.
So this would lead one to expect that entropy would be increasing in the direction of the past,
just as it seems, in fact, to increase in the direction of the future.
The problem is not merely that the laws of nature say that decreasing entropy
in a closed system, to be fancy about it, should be possible,
and that this possible thing doesn't happen.
That needn't be a problem at all.
All sorts of things that the laws of nature say are possible fail to happen.
That's not an objection to a theory.
The point is that thermodynamics, in particular the second law of thermodynamics,
seems to be in conflict with what the underlying laws of statistical mechanics would lead us
to expect to actually happen, not just to be possible.
Here's another way of putting this problem.
If the thermodynamic equilibrium states are overwhelmingly the most probable ones,
why is the world we observe so full of situations that are so far from equilibrium?
If those are the dominant probabilistically favored states, why don't they happen more often?
Boltzmann in the 1890s, about 20 years after first raising these issues,
suggested that we inhabit a very peculiar corner of the universe,
where the thermodynamic equilibrium states that in fact hold sway in most parts of the universe
do not actually hold.
So we're just in a weird neighborhood from the standpoint of the universe.
Why would that be?
Well, Boltzmann has something of an explanation.
Only very peculiar combinations of circumstances will give rise to organisms that can think and observe.
Critters like us require concentrations of energy and other conditions that are associated with low entropy states.
So the reason that we happen to see entropy increasing all the time is because we inhabit a corner of the universe
in which entropy is abnormally low, and so the main direction for it to go is up.
So it's not a universal truth, it's a local truth.
Furthermore, what we mean by the future is tied to the direction of local entropy increase.
In some parts of the universe, on Boltzmann's view,
there is no objective distinction between past and future, because there is no direction of entropy increase,
because things are at something like an equilibrium, so there's no direction of dissipation or concentration.
Now we're getting into some pretty deep waters here,
but think about a part of space where there is no local gravitational field acting,
just far enough away from all objects that there's no gravitational pull.
Which direction is down?
It's a question that doesn't have a good answer.
Boltzmann thinks the same thing can be said about past and future in big chunks of the universe.
There's just no answer to what counts as past and future in those places.
He even says that the direction of time that counts as future in some parts of the universe
could be opposite to the direction of time that counts as a future in our corner of the universe.
Future could run in the direction of concentration in parts of the universe.
Now there turn out to be some problems with Boltzmann's explanation of temporal asymmetry as a kind of local phenomenon.
We have other fish to fry, we won't be able to go into those, highly interesting though they are.
The most influential answer to this overall time asymmetry problem is actually a kind of generalization of Boltzmann's idea
that locally we see entropy on their eyes, because around here anyway it started so low in the first place
and doesn't have really any place else to go.
Many physicists and philosophers posited a very low starting point, a very low entropy starting point for the entire universe.
On this view nothing makes entropy increase.
There is no deterministic basis for the second law of thermodynamics.
It's just overwhelmingly likely given the starting point.
And it's worth noting that a posit like that seems to be required not just according to Boltzmann's purely statistical explanation of the second law
but also according to his causal explanation of increasing entropy.
Why? Because if entropy had started high, as in some sense it should given the number of possible high entropy states
compared to the number of possible low entropy states, then the mechanical explanation would only help keep it there.
It won't bring it about.
So whether we appeal to an entropy increasing mechanism or not, we need entropy to have started low for it to be able to increase this much.
And when we say we need entropy to have started low, we mean really low.
That means, according to some standard interpretations of this, that the starting point will have to have been very, very, very, add a few thousand varies, improbable.
One calculation has it that the entropy starting point of the universe had a one chance out of 10 to the 10 to the 23rd power of being chosen randomly
according to the measure that's standard in statistical mechanics.
It's hard to explain that in more detail.
The point is just the low entropy starting point of the universe is immensely improbable on standard measures.
Why? If the universe is pretty much always moving toward more probable states, then the initial state will need to have been mighty improbable.
That's the basic point.
There's some interesting work here to be done about the various senses of probability that figure in this.
If you're not careful, you'll move from one of the interpretations of probability we've talked about, probabilities as frequencies to probabilities as degrees of belief.
But we won't go into enough detail to have to labor this point.
But now some pretty deep questions about explanation, about evidence, and about the business of science start to loom.
To what extent does the past hypothesis, which is the name usually given to this claim about the low entropy origin of the universe,
to what extent does that hypothesis call out for explanation?
Here's one way of understanding what the past hypothesis says.
Matter seems to have been weirdly uniformly distributed about 100,000 years after the Big Bang, which is no time at all, cosmologically speaking.
Now a uniform distribution sounds like a high entropy state to us. It sounds like a dissipated state.
But that's because we're used to thinking of entropy in terms of things like gas molecules and the main forces there are pressure.
That's a repulsive force.
When you're dealing with objects that are primarily governed by an attractive force like gravity,
a uniform distribution of matter is highly unusual, because it's highly unstable.
Matter will tend to clump together.
So this is the surprising fact, the uniformity of the distribution of matter in the universe relatively soon after the Big Bang.
Q Price, a philosopher of science who does a lot of work on the direction of time,
compares the past hypothesis to the idea of throwing trillions of foam pellets into a tornado,
having them shake down into a uniform sheet, one pellet thick, over every square centimeter of Kansas.
According to the standard measures, there's some quibbles here about exactly how they should be run,
but according to the standard measures, the past hypothesis is immensely more unlikely than the tornado hypothesis.
So, says Price, the past hypothesis is a very weird initial condition,
and it turns out to be the only weird initial condition that we need in order to account for all of the low entropy systems in the universe.
This weird initial smoothness leads to the formation of stars and galaxies given some nice well-behaved laws,
and it's these sorts of things, stars and galaxies, that are responsible for the temporally asymmetric phenomena we encounter.
So the deep facts about our universe seem to turn on a distinctively and enormously improbable fact,
namely the incredibly low entropy state of the universe at a certain point relatively soon after the Big Bang.
And in some sense, given the laws, they turn only on that fact.
So some philosophers want to say, what could call out for explanation more than such a jaw-droppingly surprising and important fact?
And so it seems obvious, given those considerations, that an explanation of why the past hypothesis is true, assuming it is,
would be a prime task for physics and for cosmology.
On the other hand, however, there are powerful reasons for wondering what could possibly explain such a fact,
and for wondering whether such an explanation could ultimately be scientific.
The worries here are broadly empiricist in flavor.
The past hypothesis can be compared to the God hypothesis as put forward in what is usually called the cosmological or first-cause argument for God's existence.
This argument was made famous primarily by Thomas Aquinas.
Over simplifying somewhat brazenly, but I hope at least a bit helpfully, the argument says,
everything needs an explanation, and so the universe needs an explanation, and so this explanation is God.
That is far from a full-dress presentation of the argument, but you get the idea.
And that puts you in a position to appreciate the basic worries that arise,
and I don't by any means mean to suggest that a smart guy like Aquinas wouldn't have some replies to these worries.
The major worry is, why do you stop at God?
If absolutely everything needs a cause, needs an explanation, then it looks like God should need one too.
If choosing the other horn of the dilemma, some things can be self-explanatory,
or can be posited as just brute facts, stuff happens.
Why can't the universe itself, or the Big Bang, get that status that is reserved for God?
I don't mean there's no answer to these questions from the standpoint of the cosmological argument,
I just mean they're serious questions.
The point is that empiricists are suspicious of the demand for explanation being pushed too far.
When we start to run out of possibilities for getting evidence that bears reasonably directly on a hypothesis,
the project of explanation, if one is an empiricist, needs to stop.
So the parallel worry about the past hypothesis goes something like this.
It's not going to help to explain a past state of a surprisingly uniform distribution of matter
by positing an even more improbable and surprisingly uniform distribution of matter before that.
That would be a good explanation of the state, since the tendency for states is to move towards equilibrium,
and so a more improbable state will explain the slightly less improbable state.
But there's no overall explanatory gain since the explaining state would seem to be even more in need of explanation
than the explained state was.
So the empiricist says, why not stop before we start?
Just admit that our universe started from some unbelievably improbable state.
There's a sense in which most things that happen are pretty improbable.
So it's not obviously a demerit of our theory if we have to admit that the universe began from an improbable state.
Another way of pressing this point is to say that the past hypothesis is more or less an initial condition,
not literally an initial condition, it's after the Big Bang, but it's close to an initial condition.
It's an initial condition of the only universe we know.
As Charles Perce said, this is the philosopher on whom I wrote my dissertation, he's an American pragmatist,
if universes were as plentiful as blackberries, we'd be able to study them.
We could run some tests.
We could get observation to bear on questions like how these critters called universes arise and develop.
This is the problem of single case probabilities.
It's hard to place the origin of the universe in a reference class,
from which we can draw samples and get data about how these things work.
And so the empiricist says, without being able to do that,
it's not clear how we're going to get evidence to bear on the question of how these things get the initial states they get.
We can't run tests, we can't look at a sample.
Empiricists also tend to mistrust our intuitions about what calls out for explanation and why.
The classic case here is Newton on gravity.
Newtonian gravity seemed to his critics, especially the Cartesians, to need a mechanical explanation,
some kind of contact force, a push or a pull, that can't happen instantaneously at a distance.
But attempts to explain how there could be one ended up positing untestable metaphysical objects and properties,
ethers and things like that pushing objects around, but that couldn't independently get detected.
And so for centuries until Einstein came along,
physicists learned to accept gravity as a brute fact, not intrinsically in need of explanation.
That's a basic piece of the furniture of the universe.
The intuition that contact forces don't require explanation,
but that forces that act at a distance do require explanation, had been powerful, but it got rejected for centuries.
Einstein arguably is a kind of a reversion to a more Cartesian or Aristotelian picture, but let's put that aside for right now.
Another interesting issue that arises in this context concerns whether the past hypothesis counts as a law of nature or not.
It's a prime example of something that only happens once, but that nevertheless might count as a law.
It's both an initial condition and law-like in an important respect.
This is going back to Lectures 21 and 22 for those of you with good memories.
It doesn't have the logical form we associate with laws of nature.
It's not a universal conditional form or all A's or B's.
But it functions crucially in explanations of lots of different phenomena.
And so might, for instance, on a best systems conception of laws,
which identifies laws with the axioms of the true deductive systems that best combine strength and simplicity.
Maybe that sounds familiar.
The past hypothesis might well be an axiom.
Why? Because when we add it, we can derive lots and lots of correct predictions that we couldn't have derived without it.
Generally, a statement's going to be law-like in order to do that.
You won't be able to plug in a particular fact and get all of that sort of observation-deriving juice out of it.
But the past hypothesis is special.
It's a particular fact that is very explanatorily rich.
Does calling the past hypothesis a law rather than a fact make it any less needful of being explained?
Well, some think so.
Because the laws determine what counts as physically possible in our universe.
So on a view like that, we can say it's physically impossible for entropy to have been higher than it was in the past.
Why? The laws of nature include the past hypothesis.
So given the laws of nature, the past hypothesis not holding is impossible.
Now that's certainly not to explain the past hypothesis,
but some philosophers think it's a legitimate reason to reject the demand that the past hypothesis get explained.
We have only scratched the surface of this debate.
I don't know enough physics to dig too deeply into this debate.
Much is going to turn, at least for somebody who's sympathetic towards philosophical naturalism,
but arguably for almost anybody, on what room our current best science leaves for possible explanations here.
What might, according to science, not some philosophical theory of explanation,
count as a possible explanation of the past hypothesis?
That's going to tell you a lot about the extent to which you think it should be explained.
We don't want to assume naturalism to be true.
One might, with some plausibility, I think, claim that abandoning the search for an explanation of such a remarkable fact or law
could constitute a betrayal of scientific and intellectual standards,
no matter whether our current science says anything about how to explain this hypothesis or not.
You might be willing to defend a kind of a priori demand that amazing and important stuff should receive an explanation
even if we have no idea how to provide one.
That's to reject the kind of empiricist constraints that the demand for explanation tends to push towards metaphysics,
and metaphysics is a temptation that should by and large be resisted.
So this debate between empiricists and realists about the conditions under which explanatory inferences are possible,
and the conditions under which they're desirable, comes to a head rather nicely around this issue.
And I don't think there's an easy answer.
So one question is, how bad is it to leave astonishing facts unexplained?
On the other hand, how bad is it to posit untestable explanations for astonishing facts?
Each of these seems to run afoul of something deep within our scientific norms.
So the case illustrates rather nicely a pretty profound tension between the evidential boundaries that seem built into science,
and the explanatory ambitions that seem equally built into science.
Next time we'll continue illustrating the philosophical questions that arise within particular sciences
by looking at the status of species within biology.
Is this a real classification system, or a convenient classification system, or is there no difference between those two questions?
Thank you.
