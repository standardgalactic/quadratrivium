Lecture 8 â€“ Holism
In this lecture, we confront an elephant that's been in the room for most of the course.
It presents significant problems for Popper and for the logical positivists.
Now, it's not as if they didn't see the elephant.
They in fact did quite a lot to point out the problem, and it had even been noticed
as far back as the 19th century.
But there's some evidence that they underestimated the philosophical significance of this problem.
On the other hand, the philosopher who pointed out just how big the elephant is, William
Van Orman Quine, arguably overestimated its significance.
Quine was an American who had gone to Vienna to study with the positivists, and who went
on to a very distinguished career at Harvard.
Let's point out the problem before we get to Quine's solution to it.
A hypothesis like all copper conducts electricity does not, in fact, have any observational
implications by itself.
Taken on its own, that hypothesis is neither verifiable nor falsifiable.
We need some straightforward, at least to some extent, additional premises like this
object is made of copper, and this machine is built in such a way that the arrow will
move to the right if an electric current is passing through it.
With those sorts of hypotheses in place, we can derive an observable consequence like
the arrow will move to the right.
In lecture six, we called these auxiliary hypotheses.
Now note that last time we were worrying about how auxiliary hypotheses can be built up out
of experience, how they can be made meaningful.
We're going to waive this problem for now.
We're going to assume that we've got meaningful auxiliary hypotheses.
We need to see now how hypotheses have to work together in order to connect up to observation.
Now, strictly speaking, straightforward auxiliary hypotheses won't do the trick.
You also need some pretty peculiar ones, like electrical conductivity does not vary with
the days of the week, nor does electrical conductivity vary with the color of the experimenter's
shirt.
Note that you don't need to have explicitly stated either the weird auxiliary hypotheses
or the normal auxiliary hypotheses.
Their relevance to deriving observable conclusions is logical, not psychological.
Let me explain in more detail what I mean by that.
Suppose the arrow does not in fact move to the right, as our theory predicts.
What does that show?
It shows that something has gone wrong somewhere in our little theory.
We have a bunch of statements.
The hypothesis we're testing, copper conducts electricity, plus the auxiliary hypotheses
we need in order to link our original hypothesis to experience or observation.
Now, since our little theory implies a false statement, we know that at least one statement
in our little theory must be false.
This is the logic of, if then, the horseshoe that we saw recently.
It says that a group of true statements can't imply a false statement.
You can't go from truth to falsehood.
Since our little theory, put together, implies a falsehood, at least one statement in our
little theory must be false.
So logic tells us what?
It tells us either that not all copper conducts electricity, or that this device isn't good
at measuring electric currents, or that this isn't a piece of copper, or that electricity
gets lazy on the weekend and won't let itself get conducted.
We know that something has gone wrong somewhere.
But logic won't by itself tell us which one or more of these hypotheses is false.
Logic won't tell us how to distribute praise or blame for our theory's successes or failures.
And this allows us to reopen the question of Popper's falsification and see that there
are some problems that were there that he realized but didn't fully realize as it were.
Popper suggested that science is distinguished by the way in which it subjects its hypotheses
to falsification.
Now we're seeing that experience without some help from auxiliary hypotheses, without
some help from us, won't falsify any hypothesis at all.
Popper was at least somewhat aware of this problem.
He maintains that proper scientific behavior involves an eagerness to put forward bold,
easy to falsify hypotheses, and then to try to falsify them.
So he tells us that ideally we're to specify in advance which hypothesis is the one being
tested, which one we're going to blame if the experiment gives a different result than
our theory predicts.
And this, in Popper's view, is the difference between an Einstein, who supposedly put his
whole theory up for grabs in that Eddington Eclipse experiment we talked about back in
lecture two.
You might raise some questions about Popper's view here.
Suppose the experiment hadn't gone as predicted.
Do you think Einstein would have dropped his entire theory?
I think that's an interesting supposition, an interesting historical question.
But Popper thinks Einstein put his theory up for grabs in a way that the Marxists did
not put their theory of history up for grabs when the Russian Revolution apparently falsified
it.
They blamed auxiliary hypotheses rather than the central hypothesis that in some sense
should have been under test.
But Popper himself permits blaming, as it were, an auxiliary hypothesis rather than
the hypothesis under test in at least some cases.
So we saw that not even Popper thinks that when Uranus had a different orbit than Newtonian
physics predicted, the thing to do was to jettison all of Newtonian physics.
An auxiliary hypothesis was modified, namely that there are no planets beyond Uranus that
might be exerting a gravitational force and deforming its orbit from the predicted orbit.
The main requirement that Popper imposes if you're going to blame an auxiliary hypothesis
is that the auxiliary hypothesis must be independently testable.
I'm only going to give a kind of loose characterization of that.
A hypothesis is independently testable to the extent that it can be subjected to experimental
test in ways quite different from those that motivated the refutation in the first place.
So when Marxism apparently was falsified by the Russian Revolution happening in a country
that wasn't industrialized, Marxism could have remained a science of history in Popper's
view had its proponents given us a clear way of telling who the promising revolutionaries
are, how to find them, how to make definite predictions about where the next revolution
will happen.
Now notice, that's a very demanding standard.
So how will that work if imposed on Einstein?
If the Eddington Eclipse experiment had gone badly, it would not have been okay for Einstein
to just say the eclipse must not have been total or your equipment must have malfunctioned.
That kind of placing the blame on an auxiliary hypothesis is illegitimate.
What Popper says is you have to put forward a proposal that leads to testable implications.
And the more testable, the more falsifiable the implications are, the better a job you've
done of being scientific.
So it can be okay to allege some kind of problem with the experimental apparatus in the Eddington
Eclipse experiment, as long as the allegation is itself fairly readily and fairly independently
testable.
If you have to wait 80 years for the next total eclipse, if you have to assume general
relativity true in order to understand the complaint, that's unscientific behavior on
Einstein's part.
Okay, so that's roughly how Popper tries to face this problem of the need for auxiliary
hypotheses.
As you've perhaps come to expect by now, any such proposal has to face the porridge test.
Is it too hot?
That is, will it exclude what look like valuable instances of scientific behavior?
And also, is it too cold?
Will it fail to exclude misguided or pseudoscientific behavior?
And there is a case to be made that Popper's criterion, interesting as it is, is too hot.
Copernicus and his followers faced a big problem when they wanted to defend the sun-centered
conception of the solar system.
If the Earth orbits the Sun, it covers a lot of distance.
The orbit is pretty big, the Earth on one side of the orbit is pretty far from the Earth
on the other side of the orbit.
So we should expect to be seeing the stars from quite a different angle.
When we're on one side of the sun, then we do six months later when we're on the other
side of the sun.
But there was no such observable change in the angle, it's called a parallax.
For centuries after the Copernican system was proposed, astronomers were trying to measure
the parallax, and they couldn't find one.
The Copernicans, all along, modified an auxiliary hypothesis rather than the main hypothesis
under testing, they insisted, and it turns out they were right, that the stars are almost
unimaginably farther away than people had thought, so that the change in angle was just
too small to measure.
But that wasn't independently testable for centuries after the Copernican hypothesis
was proposed.
So if you want to allow that that was scientifically legitimate, you're loosening up the criterion
of independent testability to the point where it might well be too cold.
To avoid the porridge being too hot, we're going to make it too cold.
Maybe you could formulate a standard that would permit the Copernicans to blame the
distance of the stars for their observational failure, and that somehow excludes the Marxists
blaming Lenin's charisma for the failure of their theory.
But it's not going to be easy to do.
Very hard to formulate a criterion that gets the cases right.
So we've seen Popper wrestling with the role of auxiliary hypotheses.
He has some valuable proposals about how to distribute praise and blame, but it's far
from clear that they're going to work.
The most striking thing for today's purposes, though, is that Popper still writes as if
the auxiliary hypotheses are supposed to be independently testable, as if those hypotheses
are testable in isolation.
But as he knows, the issue we're talking about today is that no hypothesis is testable
in isolation.
The auxiliary hypothesis then becomes the hypothesis under test, which has no observational
implications without different auxiliary hypotheses.
This is what I mean when I say Popper was aware of the problem, but not aware as it
were of the depth of the problem.
Similar things can be said about the logical positivists by whom Quine was enormously influenced.
They write as if hypotheses can be tested in isolation.
In lecture six, we looked at the verification criterion of meaningfulness due to AJ Air,
which talks about the observational consequences of a statement, what difference the statement
makes to observation.
But statements by themselves don't have any observational consequences, right?
Only with auxiliary hypotheses do they imply anything about the observable world.
And last time, we talked about a statement by itself being fully or partially definable
in observational terms, which suggests that a statement has its own observational meaning.
In isolation, that's not strictly speaking the case.
We talked as if an object's fragility can be tested on its own, but it can't.
You need auxiliary hypotheses about the standard whack with the standard hammer, for instance.
Like Popper, the positivists at least dimly realized the problem.
They realized that all copper conducts electricity connects to observation only through other
statements of various kinds, and this was explicit as well as tacit in Carnap's Received
View of Theories, which we talked about last time.
That had to allow sideways links within the theory from one theoretical statement to another.
They move empirical content around without it flowing up into the theory.
So statements by themselves don't connect to observation, but only through other statements,
which are not themselves observational statements.
So Quine is not giving Popper and the positivists any brand new news, but he's making central
something that had seemed peripheral to them.
They didn't see a big problem here.
They knew that strictly speaking, there was a problem, but it didn't seem major to them.
They would admit that it's not true that each hypothesis gets its own observations on
its own.
It has to have a little help from its friends.
But they didn't think it was a dangerous oversimplification, as it were, to leave the background assumptions
in the background and talk as if we're just testing this hypothesis.
And then if something goes wrong, you bring in the more complicated picture.
Quine gives some reasons for thinking that picture is dangerously oversimple.
Quine's seminal paper on this topic is called Two Dogmas of Empiricism.
It was published in 1953, and it is often considered the most important philosophical
article of the 20th century, although with any claim like that, of course, you're going
to get some controversy.
Quine has been called by some people the last logical positivist, and by others, in fact
by some of the same people, the leading destroyer of logical positivism.
Both claims were a little bit exaggerated, but one of the things that's going to strike
us as we work through Quine's article is how much he shares with the logical positivists,
and yet how different a position he comes to on the basis of largely shared premises.
Quine combines the point we've been discussing about the role of auxiliary hypotheses in
testing.
We can call this wholism about theory testing.
Our theories face experience only as groups, not as single statements.
He combines that with the positivist's notions about empirical meaning and cognitive significance.
Since testing proceeds holistically, and since meaning for the positivists is a matter of
testability, that was the verification principle from Lecture 6, meaning is holistic.
Wholism about testing for Quine implies wholism about meaning.
This means that statements don't have empirical significance in isolation.
They don't say anything in isolation.
Since the mark of the empirically meaningful is the ability to imply observational consequences,
and since virtually no statement can do this on its own, virtually no statement has any
empirical meaning on its own.
Not individual statements are the bearers of scientific meaning.
This looks like it makes mincemeat of the positivist project of distinguishing metaphysical statements
from non-metaphysical statements, because statements are not the bearers of meaning.
In fact, Quine suggests in some of his more radical moments that even a whole theory,
like say population biology, may not have any observational consequences on its own.
You need background assumptions, auxiliary hypotheses, from other theories, plus general
philosophical background assumptions, like we can more or less trust our senses.
So Quine at least flirted with the idea that nothing short of science as a whole really
bears meaning.
So, though no statement has empirical meaning in isolation, a given statement's meaning
in the context of the theory in which it figures may not have that much to do with observation.
We may be able to know the meaning of a scientific statement about genes or quarks, something
pretty removed from observation, without having any clear idea of which observations would
bear positively or negatively on it.
Remember, that had been the idea behind the verification principle.
How observations bear on a theoretical statement is that statement's meaning.
So, we see here Quine drawing out the implications from the positivist conception of theories.
They realized that if statements about quarks or genes were going to be meaningful, they
were going to be meaningful only at a great remove from observation through various connections
to other statements.
But they didn't quite want to come out and say that you could understand what a statement
like that means without understanding anything about observation, because that threatened
to allow metaphysics to count as meaningful.
So, Quine, when he draws this implication from the positivist's own theory of meaning
is showing that he's less afraid of metaphysics than the positivists were.
Quine is so close and yet so far from his positivist mentors.
Quine's most striking departure from the positivist is his claim that there is no interesting
distinction between analytic statements, statements true by virtue of meaning, and synthetic
statements true by virtue of fact.
Now, this should make one sit up and take notice.
Wasn't the distinction between a paradigmatically analytic sentence, like all bachelors are
unmarried, and a paradigmatically synthetic statement, like the height of the average
American bachelor is 5'10", isn't that a pretty clear and impressive difference?
One is a question of meaning, the other is a question of fact.
Quine does not see a clear or an impressive difference here.
There are a lot of arguments that we won't go into in this seminal two dogmas paper.
For our purposes, his main argument is that the analytic-synthetic distinction does no
valuable philosophical or scientific work.
Nothing turns on whether force equals mass times acceleration, famous Newtonian law,
whether that's a definition of force, or a deep empirical fact about force.
It still does whatever work it does in the theory.
If we look at how statements function in science, we'll see that there's just no way to tell
what's a definition and what's a deep empirical fact.
Philosophers looking in from outside raise this question, but from the standpoint of
somebody using the theory, it doesn't matter.
What matters is just f equals ma.
You can call it a definition, you can call it an empirical fact.
Who cares?
So why not just drop the distinction, says Quine, and say that all sentences of a theory
work together simultaneously to be as it were about language and about the world.
Any sentence is simultaneously in the business of spreading meaning around and describing
the world.
Well, the main reason if you're a positivist not to do this is it would demolish the positivist
conception of philosophy as concerned with conceptual matters, with the realm of the analytic.
The boundary between philosophy and science starts to get undermined if you drop the analytic
synthetic distinction, as Quine does.
But there's significant pressures towards dropping this apparently clear and intuitive
distinction.
One of them was noticed by Carl Hempel, one of the positivists, though again he didn't
draw the dramatic conclusions from this problem that Quine did.
Not all of our partial definitions of theoretical terms, this is that stuff from last time,
again, despite their names, be analytic.
We were giving connections, stipulating connections between a term and experience, saying things
like, by fragility I mean what happens to certain objects when they get struck the standard
whack.
Those are supposed to be definitional stipulations, and stipulations are cheap.
As long as I'm clear, I can tell you what I mean by various terms, right?
So those were supposed to be straightforwardly analytic.
But suppose I have two partial definitions for a theoretical term, like X is entirely
composed of copper, that's my theoretical term, and I want to link that to experience
in more than one way, that's important for doing science.
So my first test says, if you do certain experiments and a certain value is reached for the density
of the material, then the stuff is made of copper, that's my first test for the presence
of copper.
So if you have another test, saying run some experiments and if you get a certain value
for the melting point, then the stuff is copper.
Those are definitions connecting the term copper to experience.
But if you put these two definitions, these two analytic statements together, you get
a synthetic empirical statement, namely that you won't find anything that has this density
without having that melting point.
Our definitions, when put together, have empirical consequences.
We thought we were just talking about our language, but you put two of those together
and you're actually making claims about how the world is.
So this distinction, central to the positivist conception of science and philosophy, between
discourse about language and discourse about the world, ends up being surprisingly hard
to take on.
So for these reasons, Quine thinks that we should be unimpressed by the supposed difference
between assigning meanings to our terms and describing the way the world is.
What does this cost us?
This is a more radical move than it might sound, because analyticity was supposed to
ensure us that certain statements were knowable a priori, knowable independently of experience,
and could be known to hold with necessity.
We could know for sure that every effect is caused.
That's part of what it means to be an effect.
Quine urges that we treat all of our beliefs as contingent, all of them as knowable only
on the basis of experience.
So one of the radical conclusions that comes out of Quine's two dogmas is that any belief
can be revised in the course of experience.
Experience could convince you to give up the idea that all bachelors are unmarried.
That's not just a conceptual claim, that could be falsified by experience.
So we might, for instance, have solid empirical reasons for changing the meaning of our terms.
Arguably, for instance, it used to be part of the meaning of water that it was liquid.
People didn't realize the same stuff could be in solid form, for instance.
Or that it was an element, not a compound.
Used to be, perhaps, part of the meaning of water that it was a simple substance.
But empirical discoveries changed the very meaning of the term, says Quine.
And Quine's view has a momentous consequence.
Theory, he says, is always under-determined by data.
Observation never forces particular changes to a theory.
So we've just seen that no beliefs are insulated from the possibility of revision.
Quine suggests that even some of the laws of logic could be rejected on the basis of
experience if that looked like the most effective way of making predictions, of doing good science.
And some of the weird stuff that comes out in quantum mechanics in the 20th century has
tempted some philosophers to do just that.
The positivists would not have dreamed of an experiential refutation of the laws of logic.
Quine thinks that's possible.
Similarly, we've at least glanced at major beliefs that were thought to be knowable a priori,
knowable independently of experience about, say, the nature of space, Euclidean geometry.
And these beliefs about the nature of space were given up for empirical reasons.
So conversely, any statement can be maintained in the teeth of experience.
No matter what experience shows me, there's still going to be a way to preserve any statement
I happen to want to.
If we are willing to make enough modifications to other parts of our theory, we will always
be able to preserve a commitment to the truth of any given statement.
So someone who, for whatever reasons, is committed to the Earth being only 6,000 years old can
maintain a perfectly coherent set of beliefs by taking advantage of Quine's wholism.
And none of the evidence behind relativity forces anybody to give up good old-fashioned
Newtonian physics and its nice comforting conceptions of space and time that we messed
with in lecture four.
If you want to accept, for instance, their undetectable forces that shrink meter sticks
and slow down clocks without having anything to do with speed, you can maintain absolute
space, absolute time.
You just have to make enough changes elsewhere in your theory.
Now this is a bit of a shocker.
As far as logic goes anyway, Quine says any new evidence, no matter how bad it looks for
your theory, can be rendered compatible by making enough changes elsewhere in the theory.
Quine's famous metaphor is that of a web of belief.
Experience he says impinges on the edges, but there are always lots of ways of distributing
that impinging force throughout the web.
And so it's possible to keep any local belief in place if you're willing to move enough
stuff around it.
Of course that can be a big job.
So a notion like the analytic-synthetic distinction gets replaced by the metaphor of relative
centrality in the web.
We're reluctant to give up the laws of logic because we're reluctant to make big changes
unless we really have to.
So the issue is not whether you're changing the meaning of your terms or whether you're
changing some factual statement, it's a matter of how deeply embedded in your theory the
statements you change are and how much reverberation throughout the theory is going to result.
So in some respects Quine's other metaphor, the web of belief is the famous one, but the
force field metaphor that he also uses in this paper, is more appropriate because it
emphasizes continuity within the web of belief.
The web suggests they're distinct nodes, but our theories are just densely packed, they're
like a force field, he says.
So having done away with the analytic-synthetic distinction, there are no sharp divisions
within the force field, like that between philosophy and science, or between science
and metaphysics.
Adams, says Quine, have the same kind of status as the Greek gods once did.
What does he mean by that?
They're posits we use to systematize experience, and the only question is, do they do a good
job of it, not do they do it scientifically or not?
So Zeus was an explanation of lightning.
Quine has a fairly specific idea of what's involved in a hypothesis doing a good job.
Changes in the web of belief are to be evaluated in terms of their simplicity, we want to minimize
the number of basic laws we have to posit, and the number of basic kinds of objects we
have to posit.
And there to be evaluated in terms of conservatism, you want to preserve as much of your old
web of belief as you can, don't make big changes you don't need to.
Why are these the right criteria for modifying the web?
Well, they're pragmatic criteria.
They make the theory, or your web, easier to use.
So why keep track of Zeus and electricity?
As explanations for lightning, if you got one you don't need the other, so try to figure
out which one is going to be easiest to use.
Now it's an open question whether these pragmatic criteria are supposed to have any connection
to truth.
Quine himself seems to have kind of waffled on this front, and it's a tricky problem.
Many people have thought it's obviously rational that simpler theories, waving some worries
about measuring simplicity, are better than more complex theories.
And they thought it's equally clear that better theories are likelier to be true.
But that argument moves too fast, I mean how much reason is there to think that the world
is more likely to be simple than complex?
Don't we need an argument for that assumption?
And similarly, what reason do we have to think that theories that more closely resemble
our current theories are more likely to be true than theories that don't?
So Quine surely says, these rules for updating the web will make it easier to use, it's not
clear whether he thinks they'll make your web more likely to get the world right.
However we read Quine's own approach to this problem of under determination, the issue
is enormously important and controversial, it's going to crop up at various times in
our course.
Some think that Quine has shown that a very permissive, even a relativistic approach to
science is in order.
Relativism in this case means that there's no standard from outside a given web of belief
from which that web can reasonably be evaluated.
That's the moral many people have taken from Quine's discussion, it's a pretty strong
claim and it's not Quine's view.
Quine has shown that no matter how much data comes in, it doesn't force us to a unique
theory, that's the point of under determination.
But Quine's not a relativist because he thinks that one should be constrained by simplicity
and conservatism.
He's far from thinking that all theories or webs of belief are equal, and he doesn't
think the Zeus web is okay because it fails of simplicity and conservatism, so he's
no relativist.
Now there's an interesting puzzle about under determination.
A mild version of it is pretty common in science.
The data plus scientific norms or logic don't decisively settle the choice between the
earth-centered and sun-centered pictures of the solar system, because of problems like
the parallax.
So some cases we have under determination of theory by all the actual data.
Quine goes beyond that to talk about under determination by all the possible data.
There will always be more than one theory that fits the data no matter how much evidence
comes in.
That's a striking result.
But note that in actual science, the problem more often consists of finding one theory
that fits the data reasonably well, not choosing among an infinite range of theories that all
fit the data fine.
Now one way to explain this would be if there are additional constraints on the web, besides
deductive logic, besides pragmatic constraints.
And next time we'll start looking at the idea that there's some kind of inductive logic
or inductive method that can further help constrain legitimate webs of belief.
If there's a scientific method that tells you how to update the web, that would explain
why our choices are more limited than Quine's argument, which only uses deductive logic,
can show.
But what's the status of those principles going to be?
For Quine, they're themselves just part of the web of belief, and are up for discussion
and revision.
And that's where we'll turn next time.
