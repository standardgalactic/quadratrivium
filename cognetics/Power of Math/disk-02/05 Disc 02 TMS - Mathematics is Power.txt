So Taylor's theorem says that if we keep taking derivatives of functions and plug in a number
for each derivative, then with some certain specific divisions as well, we end up with a
polynomial that will give us the exact behavior of that function for most cases. For almost any
case of anything that we care about, it will give us a marvelous approximation of it. And
polynomials are easy to program into computers because, again, it's just adding and multiplying
and subtracting. We can add and subtract and multiply and divide. We all learn how to do
that in elementary school. Math starts to get a lot stranger when we start throwing in things like
sine functions, cosines, tangents, logarithms, exponentials, compilations of these. But what
Taylor's theorem says is we can restrict to a finite polynomial and we have an error bound on
how far off that will be from the function that we're interested in. Here are just a couple
examples. The square root, if we were interested in taking square roots close to one, I'll say this
a lot. If we look at minus x minus one squared divided by eight plus x minus one divided by two
plus one. Now there's something you could compute by hand if you put in like 1.3 or 1.7,
0.9, whatever. If you plug those numbers in for x, that will give you a very good approximation
of the square root. If you needed to know the sine of an angle, and it wasn't an angle that you
could come up with a nice clever trick to find, if you looked at the polynomial x to the fifth
divided by 120 minus x cubed divided by six plus x, you put numbers in for x, that will give you a
very good approximation of what the sine of the angle is. And finally, if you're interested in
computing compound interest and you need to know the exponential value of something, the polynomial
x cubed divided by six plus x squared divided by two plus x plus one will give you a remarkably
good approximation. The more terms you add in the polynomial, the better the approximation gets.
Just recapping that for a second, because I think I threw a lot of ideas out at you.
If we take a function that we need to find a value for to solve a problem, we can approximate it by
a polynomial, and with Taylor's theorem, had a very specific notion of how big the error will be.
If we take a few more terms in the polynomial, the error will go down. And this is how our calculators
and computers are able to work. They take polynomials, which are just multiplication and addition,
a little bit of division and subtraction, all that can be done by pencil and paper, and produce
instead these numbers within as close accuracy as we like to exactly what we want.
