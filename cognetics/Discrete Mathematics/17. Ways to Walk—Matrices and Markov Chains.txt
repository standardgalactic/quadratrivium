In our last lecture, we talked about various different kinds of walks on a graph.
Paths, trails, Eulerian tours, and so on.
In this lecture, instead of describing new types of walks, we address a combinatorial
question that involves counting walks.
How many ways can you walk from one vertex to another?
This will lead to a discussion of random walks on graphs and basic concepts of how many search
engines rank web pages.
But before turning to the World Wide Web, we begin with a much smaller example.
In this graph, how many walks are there from vertex one to vertex three that have a given
length L?
So for instance, if L is one, I'm asking how many walks are there from vertex one to vertex
three of length one?
Well, you can see in the graph, there's just one walk that goes from one to three.
Okay, how about when L is two?
How many walks of length two?
You can either get there by going one to two to three, or you could go one, four, three.
Okay, there were two walks.
How about how many walks of length three?
There are five walks.
You can go one, two, one, three, or one, three, one, three, or one, four, one, three, or one,
three, two, three, or one, three, four, three.
I won't count them all out, but there are 14 walks of length four, 33 walks of length
five, 90 walks of length six, and so on.
The question is, how would we, or a computer, determine these numbers?
But first, a more fundamental question.
Before a computer can count walks in a graph, how does a computer actually see a graph?
How does it represent it?
Well, there are a few ways of doing it.
One is just by using the definition of a graph.
It says, well, I know the vertex set is one, two, three, four.
I know the edge set consists of these five edges, and that's enough to represent the
graph, or it might represent it as a list.
It might say, well, vertex one is adjacent to vertices two, three, and four, and vertex
two is adjacent to one and three, vertex three is adjacent to one, two, and four, vertex
four is adjacent to one and three.
That would be another way that the computer could represent the graph.
But another way of representing the graph, the one that we're going to focus on in this
lecture, is using something called an adjacency matrix.
So the computer could represent the graph using this adjacency matrix.
Now what is this, I hear you cry.
Let's get acquainted with some basic properties of matrices, and all will be made clear.
So what's a matrix?
Essentially, you can think of a matrix as a box of numbers.
Here's a typical matrix.
You might call it the pi matrix, because it has the digits 3, 1, 4, 1, 5, 9.
This matrix, which I'll call A, has two rows and three columns.
So we call it a two by three matrix, two rows, three column.
We say that the element aij refers to the number that's in row i and column j.
So here, A13 is four, because the number four appears in row one column three.
Or we can say that the one three entry of the matrix A is four.
So the adjacency matrix for our graph is defined as follows.
It consists of zeros and ones, where we put a one in the ij position if vertex i is adjacent
to vertex j, and otherwise we put a zero.
Another way of defining it, equivalently, is that aij is simply the number of edges
that go between i and j.
And then a graph, the number of edges between i and j, is either going to be one if they're
adjacent or zero if they're not adjacent.
So for instance, if you look at our graph here, you see that vertex one is adjacent
to vertices two, three, and four.
And that's why row one has a one in positions two, three, and four in columns two, three,
and four.
And it has a zero in the one, one position because the vertex is not considered to be
adjacent to itself because it doesn't have an edge that goes back to itself.
Whereas vertex two is only adjacent to vertices one and three.
That's why we have a one in the first column, and the third column of row two, and the rest
are zeros.
Okay, so check to make sure that you see that this matrix represents this graph.
Adjacency matrices can also represent multi-graphs using that same definition of aij being the
number of edges that connect vertex i and vertex j.
And we see that this multi-graph would have this adjacency matrix.
So for example, you see we have two edges that connect vertex one and vertex two.
And that's why the one, two entry is two.
We also have a loop that goes from four back to four.
And that's why the four, four entry is one.
Again, verify that this multi-graph has this adjacency matrix.
Another thing, adjacency matrices can also represent directed graphs.
Those are graphs where each edge is given an orientation, kind of like one-way streets.
So for example, here we have an edge that goes from one to two.
And we'll let aij be the number of edges that go from i to j.
So for instance, the entry of a12, that would be one, whereas a21 is zero.
There is no edge that goes from two back to one.
By the way, I should point out that with graphs and multi-graphs, if there is no orientation
of the edges, then your graph is symmetric.
That means the number of edges that go from i to j is the same as the number of edges that
go from j to i.
That means your adjacency matrix is going to have the same number in the ij position
as it will in the ji position.
But with a directed graph, you're generally not going to be symmetric.
Like here, a12 is one, but a21 is zero.
Using adjacency matrices, we can calculate the number of walks of any length for a graph
or multi-graph or directed graph.
All we need to be able to do is to multiply matrices.
Now, so let me tell you a little bit, all you need to know about matrix arithmetic for
this lecture.
First of all, matrix addition is easy.
You add them just as you'd expect to add them.
So for instance, here's the matrix 1, 2, 3, 4 plus the matrix 5, 6, 7, 8.
When you add them, how would you want to add them?
Component-wise, right?
1 plus 5 is 6, 2 plus 6 is 8, 3 plus 7 is 10, 4 plus 8 is 12, and that would be correct.
So matrix addition, piece of cake.
Matrix multiplication, that's a little trickier.
In fact, let me tell you right off the bat, 1, 2, 3, 4 times 5, 6, 7, 8 does not equal
5, 12, 21, 32.
We don't multiply components together.
Okay.
How do we multiply matrices?
Using something called a dot product.
Let me explain it through this example.
So I'm going to take the 2 by 2 matrix, 1, 2, 3, 4 times the 2 by 2 matrix, 5, 6, 7,
8.
I'm going to create a 2 by 2 matrix.
Let's first calculate the number that's in the first row and the first column.
I do that using the first row of the first matrix times the first column of the second
matrix.
Now when I say times, I really mean using something called the dot product, and here's
how it goes.
To do the row 1, 2, dotted with the column 5, 7, I take 1 times 5 plus 2 times 7, that's
5 plus 14, giving me 19.
Let's do a few more examples.
To get the 1, 2 entry of the product, I'm going to take row 1 from the first matrix
and dot it with column 2 of the second matrix.
And when I take 1, 2 dotted with 6, 8, I get 6 plus 16, which is 22.
To get the next entry, to get the 2, 1 entry of the product, this time I'm going to take
row 2 of the first matrix dotted with column 1 of the second matrix and giving us 43 in
this case.
And finally, for the last entry, for the 2, 2 entry, I'll take the second row of the
first matrix dotted with the second column of the second matrix, giving me 50.
In general, if A and B are n by n matrices, then their product, A times B, is also an
n by n matrix.
And the number that you get in row i and column j is obtained by taking the i-th row of A
and dotting it with the j-th column of B.
Now one important word of caution, if there's anything you remember about matrix multiplication,
it should be this.
Matrix multiplication is not usually commutative.
That is, A times B is generally not equal to B times A. As this example shows you, 1,
2, 3, 4 times 5, 6, 7, 8 does not give you the same matrix as 5, 6, 7, 8 times 1, 2,
3, 4.
Matrix multiplication is associative.
What associative means is you don't have to worry about parentheses.
That is, A times Bc is the same as AB times C. I haven't changed the order of the matrices
A, B, and C. I just changed how I parenthesize things.
I won't prove to you that matrix multiplication is associative, but that's definitely worth
knowing.
So for example, A cubed, you can think of that as A times A times A, which is either
A times A squared or A squared times A. So you can calculate A cubed either way.
Because matrix multiplication is defined this way, we can solve our original combinatorial
question, the question of how many length L walks exist from vertex i to vertex j very
easily.
Here's the theorem.
Let G be a graph or a multigraph or a directed graph with adjacency matrix A. Then the number
of length L walks from vertex i to vertex j is simply the ij entry of the matrix A to
the elf power.
Let me say that again.
In other words, to find the number of length L walks from i to j, you take the matrix A,
raise it to the elf power, then look at the number that appears in the ith row and jth
column, and that's your answer.
So let's do some examples with the original matrix that we had.
So here's our graph G, and there's its adjacency matrix A. And now how many length 1 walks
are there from 1 to 3?
We just look at the 1, 3 entry of A, and there it is.
We knew it would be there.
That was part of the definition.
So how about the number of length 2 walks?
We know there are 2 walks of length 2.
Let's see if the matrix can find it.
We take the matrix A squared.
So we take A times A, we calculate A squared, and we look at the 1, 3 entry of our matrix,
and we see that the 1, 3 entry of our matrix A squared is 2.
So the theorem says there are 2 walks of length 2 from 1 to 3.
How many walks of length 3?
Well that's going to be the 1, 3 entry of the matrix A cubed.
I can get that by taking the first row of A times the third column of A squared, right?
Because A cubed is A times A squared.
And when I do that dot product, I get 5 as the number, as the 1, 3 entry of A cubed,
and therefore there are 5 walks of length 3 from 1 to 3.
How many walks of length 4?
This will be the 1, 3 entry of A to the fourth power.
Now A to the fourth power, that's A squared times A squared.
So I could take row 1 of A squared dotted with column 3 of A squared, and when I do
that dot product, I get 14 as promised.
This theorem can be proved by induction on L, and I'll sketch it for you here.
When L is equal to 1, the theorem is true by the definition of the adjacency matrix
A.
Well, Aij is the number of length 1 walks from i to j.
It's the number of edges from i to j.
Now to prove this inductively, our induction hypothesis is, assume that for every i and
j, the number of length k walks from i to j is the ij entry of A to the k.
So the question is, how many walks from i to j are there of length k plus 1?
I'm going to do that by first considering where the first step goes as I go from i to
j.
Let's say how many of them go from i to the vertex m, and then continue on to j.
So such a walk is going to look like this.
I take one step from i to m, and then a k step walk from m to j.
Now how many ways can that be done?
Well, I know the number of length 1 walks from i to m is just the i m entry of the matrix
A.
And how many length k walks are there from m to j?
By the induction hypothesis, that's the mj entry of A to the k.
When I sum over all values of m, this gives us the ij entry of A times A to the k, which
is equal to A to the k plus 1.
Now suppose you take random steps in your graph so that when you're at vertex i, you
move to vertex j with some probability pij.
I want to talk about the subject of random walks now.
For example, if we move in our original graph to adjacent vertices with equal probability,
what I mean by equal probability is if you're at vertex 1, you could go to 2 or 3 or 4 each
with probability a third.
But if you're at vertex 2, you can't go to vertex 4, but you can go to vertex 1 or 3
with equal probability a half.
We represent this with our transition probability matrix.
So here's the transition probability matrix p, sometimes I'll just call that the p matrix
for this graph.
So the ij entry of our p matrix is the probability that we go from i to j when we're at vertex
i.
So question, given that we start in state 1, I'm asking what's the probability that
l moves from now, we are in state 3.
For example, let's start simple.
When l is 1, what's the probability of going from vertex 1 to vertex 3 in one move?
If you start at vertex 1, there's a 1 in 3 chance that you end up at vertex 3 on your
next move.
That happens to be the 1, 3 entry of our p matrix.
When l is equal to 2, what's the probability that if we start at vertex 1, that we end
up at vertex 3 in two moves?
Well there are two ways of getting to vertex 3, as we saw before.
You can either go 1, 2, 3 or 1, 4, 3.
The probability of taking walk 1, 2, 3 is a third times a half, which is a sixth.
The reason for that is that once you're at vertex 1, you have a one third chance of moving
to vertex 2, and from vertex 2, you have a one half chance of going to 3.
On the other hand, the walk 1, 4, 3, that also has a probability of 1, 6, because there's
a 1 in 3 chance of going from 1 to 4, and a 1 in 2 chance of going from 4 to 3.
Therefore the total probability is 1, 6th plus 1, 6th, which equals one third.
For walks of length 3, there are five walks.
I won't go through all of them, but their probabilities range from 1, 18th to 1, 27th.
Let's just look at two of them.
So for example, the walk 1, 2, 1, 3, what's the probability of taking that?
It's the probability of going from 1 to 2, then from 2 to 1, then from 1 to 3, and when
we multiply those probabilities, we have a third times a half times a third, and that's
equal to 1 over 18.
On the other hand, the walk 1, 3, back to 1, back to 3, that's going to have probability
one third times one third times one third, which is 1 over 27.
Taking up all those probabilities gives us a total of 7 over 27 as the probability of
going from 1 to 3 in three moves.
By essentially the same reason as in the last theorem, we have the following theorem.
For a random walk on graph G with probability matrix P, when starting at vertex i, the probability
that we're in state J in L steps is the i, j entry of P to the lth power.
This process of randomly walking on a graph is called a Markov chain.
Markov chains can be used to model any random process where things move randomly from one
state to another, where the probability of moving from i to j only depends on your current
state, i, and not where you were previously.
For instance, here's a rather simplified model of weather prediction.
Let's imagine we consider there to be two states of weather.
It's either sunny or cloudy, and you've decided, you're the weatherman, and you've decided
that if it's sunny today, then there's an 80% chance that it will be sunny tomorrow,
and therefore a 20% chance that it will be cloudy tomorrow.
On the other hand, if it's cloudy today, then there's a 60% chance that it will be cloudy
tomorrow, and therefore a 40% chance that it will be sunny tomorrow.
We represent that by our P matrix in front of us, 0.8, 0.2, 0.4, 0.6, representing the
probability of going from a sunny or cloudy day to another sunny or cloudy day the next
day.
If we look at the matrix P, if we want to predict the weather two days from now, we
look at the matrix P squared.
So we know how to multiply matrices.
We multiply P, P times P, we get P squared.
That means, and then for P cubed, we can take P times P squared, multiply those together,
give us the matrix P cubed.
Let's take a look at that matrix P cubed.
It says, if it's sunny today, then the chance that it's cloudy in three days is going to
be this entry, 0.312.
That means that if it's sunny today, there's a 31.2% chance that it will be cloudy in three
days.
Here are the matrices P to the fourth, P to the fifth, P to the sixth, and P to the seventh.
Take a look at those entries.
Do you see anything interesting about them?
Even though these are representing a random process, these numbers aren't looking very
random, are they?
Take a look at the last number, P to the seventh.
That first column, the numbers are like 0.667, and another number that's like 0.666.
The second column, the numbers are, both of them are one of them's like 0.333, the others
are like 0.334.
It appears that as L gets larger and larger, that P to the l-th power is getting closer
and closer to the first column looking like two-thirds, two-thirds, and the second column
looking like one-third, one-third.
What that says is that if you look far into the future, the chance of being sunny under
this weather model is 66.7% regardless of whether today is sunny or cloudy.
This kind of makes sense.
By the way, if you take a look at what would happen to the matrix two-thirds, one-thirds,
two-thirds, one-third, you can think of that as like P raised to the infinity power.
Look at what happens if you hit it with one more application of our P matrix.
When you multiply 0.8.2, 0.6.4 times the matrix two-thirds, one-third, two-thirds, one-third,
do the math, you'll see you still get two-thirds, one-third, two-thirds, one-third.
This makes sense since our equilibrium matrix doesn't change when we multiply it by P. Looking
into the future infinity days from now is going to look just like looking into the future
infinity plus one days from now.
This relationship allows us to find the equilibrium matrix without actually having to raise P
to a large power.
All you have to do is solve a system of equations.
Incidentally, suppose you had a more accurate model that predicted tomorrow's weather based
on the weather today and yesterday.
Could you model that with a Markov chain?
Remember Markov chains are only supposed to be based on what's going on now.
The answer is you actually could, but instead of it being a two-by-two matrix modeling the
situation, it would be a four-by-four matrix, and it would have this kind of structure.
We would have four states, not just two representing sunny or cloudy, but all possibilities for
the last two days.
We have SS, CS, CC, and SC.
A state like SC on the bottom would mean that it was sunny yesterday and cloudy today.
Let's go to the top row and look at what happens.
If it was sunny yesterday and sunny today, then what's this possible situation for the
next day?
The next day, it's either going to be sunny or cloudy.
If it's sunny, we'll still be in the SS situation, but if the next day is cloudy, that's going
to put us in an SC situation because the next day is going to be cloudy.
The day before it, where we are now, is sunny, that puts us in an SC situation, but there's
no way of going from an SS situation to a CC situation in one day, and there's no way,
even of getting from an SS situation to a CS situation in one day.
That's the structure of these matrices.
Obviously, the more days you want it to be based on, the more complicated, the more states
are going to be required in your adjacency matrix.
By the way, if we think of the worldwide web as one giant graph, it's a directed graph
with an edge directed from web page i to web page j, if there's a web link from page
i to page j, then if we wander the web at random, just like we were doing in our earlier
example, then we're going to spend more time at web pages that are popular than web pages
that are unpopular.
By calculating equilibrium probabilities in a certain way, a search engine can measure
a web page's importance.
When I say in a certain way, the main difference from the usual equilibrium calculation is
that when it reaches a web page with no escaping links, it moves to another web page just anywhere
in the world entirely at random.
This is one of the key components behind the page rank algorithm used by Google and other
search engines.
Other applications of Markov chains can be found that model population dynamics, genetics,
stock prices, and games people play like Blackjack.
In each of these situations, your prediction of where you'll be in the next moment in time
is based on where you are now.
So for instance, if you're playing Blackjack and you see that the dealer right now has
a six showing, then based on what your prediction on what he's going to have in the end is going
to be is going to take advantage of the fact that he has a six showing now.
And then when he gets another card, let's say a four, then he's got a 10, that's going
to revise your prediction of what his total is going to be at the end.
And speaking of games people play, stick around for our next lecture when we apply graph theory
to the topic of social networks.
