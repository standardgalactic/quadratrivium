J. David Bolter wrote a very nice book quite a while ago when computers were still novel,
in which he referred to the computer as the defining technology of our age, presciently
since this was written in the late 1970s. And it is, although perhaps it would be more
accurate to say that the defining technology of our age is semiconductor-based microelectronics
technologies. And that's a mouthful and very antiseptic sounding. It is more dramatic and
not incorrect to say it's the computer, because the computer is the most potent and dramatic
manifestation of semiconductor-based microelectronics technologies insofar as we're talking about
a technology that has really changed the world. And there's a relationship here, I think, between
an earlier lecture in which I talked about the industrial revolution of the 12th century,
which was based on gear-train technologies, harnessing wind and water power through gear
trains that allowed that power to be applied to machinery of all kinds and industrial processes
of all kinds. And, nevertheless, historians treat the clock, which is only one instance of gear-train
technology, as the defining technology of the late Middle Ages and the Renaissance and early
modern period. So, analogously, I think we can say that the computer is the defining technology
of our age, understanding that power of the computer comes from the extraordinary continuing
increase in the power of semiconductor-based microelectronics technologies. But what is
the computer? What is the computer, and especially from our purposes in this lecture, what is
the idea underlying the machine that gives the machine its power to be such a force in
reshaping life? I would say, through World War II, the term computer was a job description
term. I mentioned in the last lecture that Henrietta Swan-Levitt was a computer. She
was one of a number of computers, all of them women, I believe, who worked at Harvard Astronomical
Observatory and at Astronomical Observatories around the country, which were well enough
supported to afford computers. That's who did the computing. But throughout society,
there were people who made their living doing calculations, because, especially in the 19th
and early 20th centuries, the needs of scientists, mathematicians, the military, and increasingly
in the 19th century industry, for the results of huge numbers of calculations continued
to grow. And so one use of the term computer, which we no longer use, and we've lost track
of the fact that there was, once, a subculture within society of people who made their living
making calculations, the computer is a job description. But having said what I just said,
it is more accurate to now focus on a phenomenon that emerged in the 19th century pulled by the
demand for calculations that I just referred to, and that is mechanical or automated calculation.
It was in the 19th century that we start seeing an industry, a very successful industry,
that disseminated mechanical calculators into society with a point to routineizing and of
course saving money over human computers and increasing the efficiency of making all of the
kinds of calculations that businesses, the new business models, the tremendous increase in the
scale of business, the new sophistication of engineering, which requires huge numbers of
calculations, and I mentioned scientific researchers, and the military, which needs
tremendous numbers of calculations for things like calculating the trajectory of
of cannon shells and naval guns, etc. So mechanical calculators became an industry.
The first generation of these mechanical calculators in the early 1800s actually picked up on the
design of Leibniz for a four function calculator. The four functions are the arithmetic functions of
of addition, subtraction, multiplication, and division. And this sort of is the basic,
is the basic mechanical calculator, sometimes called an adding machine of the 19th and early
20th centuries. And in the course of the century Leibniz's design was commercialized in a number
of different forms. And so Leibniz's design was based on entering numbers by rotating gear
combinations, tooth gears that were designed and interconnected in various ways to represent
the entering numbers. And then when you rotated the dials, you got the result of the addition or
the multiplication. There were other designs that were implemented commercially for patent
reasons, for example, and because people thought they were better, by the end of the 19th century,
after the typewriter was invented, adding machines with keyboards. Initially, they didn't have
keyboards. You had to actually physically manipulate the gears or pins and pin driven
calculators. But by the end of the 19th century, you had electrically operated mechanical
calculators, but with keyboard entry. And this was very helpful, but it was still limited to
arithmetic calculation. Earlier in the 19th century, Charles Babbage, a British polymath,
but a mathematician, I guess primarily, although a very important figure in British science
in the middle of the 19th century, Babbage had this grand vision for an automated calculating
machine that would reduce, in fact, he thought it would eliminate the very large number of errors
that crept into the calculations done by human beings and the printed versions of those
calculations. Now, this is something that the average citizen who is not an engineer or a scientist
does not often get to see. But there are, especially in the 19th century, there continued to be,
as there had been for 100 or so years before that, gigantic volumes which were nothing more
than hundreds and hundreds of pages of logarithms, values of logarithms. This is crucial,
logarithms which were invented in the 17th century, becomes a crucial way of speeding up
mathematical calculations and by reducing multiplication to addition and division to
subtraction. And in order to use logarithms, then you have to know what the value of the
logarithm is for every particular, to multiple decimal places, for every particular value of
the numbers that you're using in your own calculations. We don't need to get any further
into it than that. Sufficient to say that if you were a scientist, an engineer, or in the
military, you needed access to these tables of logarithms and there were a tremendous number
of errors. There were errors in the human side, there were also errors in the printing, entering
those values into the printing presses so that these were published together with volumes
that corrected the values that had been published either in the current version or in earlier
versions. So Babbage said, this is ridiculous, and everybody agreed it was ridiculous, what are
you going to do about it? Babbage envisioned an engine, which he called a difference engine,
that would be able to make complicated arithmetic or simple algebraic calculations
repetitively automatically with a printer attached to it so that it would be printed out
automatically and there would be no more errors. The calculations would be much quicker and they
would be error free. And he designed this machine in meticulous detail, convinced the British
government to fund its construction. But it turned out this was in fact something that would be
today, that is called by almost all historians of computers, the first computer, at least in
principle, because it turned out that the building of it, since it was mechanical and based on
gear trains, it could not be made using the machinery technology, a machining technology
of the 19th century and the materials necessary in order to have the gears that had very peculiar
curves to them in order to make the calculations that he promised to make. So after spending
probably the equivalent of millions of dollars today, the British government lost patience,
this machine was never going to get built and cancelled the contract and what we have left
are the drawings. By that time, Babbage had lost interest in that machine anyway because he had
come up with a vision of an even more powerful automatic calculating machine, which is the,
he called the analytical engine, meaning the algebraic engine. This could solve, forget just
arithmetic problems, this could solve a certain class of algebraic equations and this was a machine
that had a memory that was programmable and that had a printer attached to it so that it would all,
you would plug in the calculations, the equations that you wanted and the values for the variables
and then it would give you the, it would print out the solutions to those equations.
Again, this machine was never built. However, the meticulous drawings, which are in themselves
beautiful, do exist and over the last 20 or 30 years, various groups of enthusiasts have built
parts of the machine to see if they would work and they do and they're quite lovely in examples
of 19th century technology, which in fact could only be made using late 20th century
technology but they look awesome and you realize that this machine would have worked
if in fact it had been constructed. This is always called the beginning of computing but
I don't agree. I think what we see here is another step in calculators. This is an automatic
calculator. It computes in the sense that that's what calculation was called then. That's why
human calculators were called computers. So this is a computer in the sense of being a calculator
but computers as the defining technology of late 20th century, early 21st century life
are not calculators. They're much more than calculators. The history of increasingly sophisticated
calculators really picks up in the 1920s and 30s in particular in the 1930s when without any
reference, without any specific reference to Babbage's machine in Germany and the United States,
very sophisticated calculators were designed in 1936. A young German engineer named Conrad
applied for a patent for an electromechanical computer that was essentially a very sophisticated
calculator that used binary arithmetic and used what's called floating point arithmetic in order
to deal with very large numbers in a compact way and it had the capacity for a stored program in
the patent application. In fact the machines that he built didn't have all of these features,
especially not the stored memory part but they were programmable. They were programmable.
They did use binary bits and they did have floating point arithmetic and he built,
he went to work for the Heinkel engine plane manufacturing company in Germany during World War
II and his initial version which he built in his parents home, the Z1, he scaled up to something
called the Z3 machine which he used during the war to speed up the calculations he was working
as an aeronautical engineer and then during the war he designed and built a Z4 machine which was
actually much more sophisticated but still electromechanical and that machine was completed,
snuck out of Germany and wound up in Switzerland and until about 1960 worked in a Swiss bank doing
routine calculations in the bank. So Zeus has a firm place in the history of computers
at the level of calculation. Concurrently and quite independently in the United States,
Howard Aitken at Harvard University and supported very substantially by IBM in the 1930s built
the first of a series of giant electromechanical calculators and George Stibitz who was an engineer
at Bell Labs also built a very sophisticated calculator based on telephone switching equipment
that was also electromechanical. Electromechanical calculators are not gone anywhere. They're much
too slow and they are enormous. These machines weighed many tons and occupied a tremendous
amount of floor space. They were in a certain sense hopelessly, much too hopelessly complicated to use
intensively on a routine basis but they worked. Much more interesting from the point of view of the
history of the computer as a defining technology are the following events. In the 1930s, late
1930s, second half of the 1930s, there are two physicists at Iowa State University,
Barry, two physicists really, John Atanasoff and Clifford Barry collaborated on building
a vacuum tube, 300 vacuum tube computer, electronic calculator that solved a certain class of
algebraic equations and that had a stored memory. It had a memory using condensers.
It had an automatic printer attached to it and it was programmable. So here we have all of the
features of a computer but it is in fact a calculator. They built this machine in the late 1930s.
They used it a bit at the University of Iowa in their laboratory to solve these equations.
It could solve a series of what's called linear algebraic equations. So if you have up to 10
equations with 10 variables and they are so-called linear equations, relatively simple,
but very important for engineering calculations and many scientific calculations,
this machine could actually solve it and it worked. They were inspired to build this machine
by an analog computer that Vannevar Bush had built at MIT and which John Atanasoff was familiar with,
which solved a very sophisticated problem in electrical engineering having to do with
electrical circuit network, complex electrical circuits, especially linking multiple utilities
together so that electricity could flow from one utility to another. It's called an interconnect
and inspired by that analog computer which had no vacuum tubes. Atanasoff and Barry built their
vacuum tube computer which was studied by John Mauchly and John Eckert in the early 1940s when
they got a contract to build for the U.S. Army the machine that we came to know and love as ENIAC,
which was an electronic calculator of a formidable sort. This time with something on the order of
18,000 vacuum tubes, imagine the power and the electrical power that it used and the heat
that it generated, not to mention the failure rate of the tubes, they went to at least one of them,
I believe it was Mauchly, went to Barry's laboratory and saw the ABC, the Atanasoff-Barry
computer and then came back to the University of Pennsylvania where they had this contract to build
an ENIAC for the Army and designed and built ENIAC because Atanasoff did not patent his computer.
In fact, when the war started and he was redirected on to war time related research, seems to have
lost interest in it. Subsequently, when Mauchly and Eckert founded a computer company after World
War II and wanted to patent the computer after a bitter and protracted lawsuit, which was fought
out by the Berry Corporation, which had sort of gobbled up various companies that Mauchly and
Eckert had had founded, a very bitter lawsuit, they were denied a patent on the ground that
the computer technology that was in ENIAC and its successors was already in the public domain
because Atanasoff and Berry had shared their knowledge with Mauchly and Eckert and they had
not patented it. Not clear how much Mauchly and Eckert learned from Atanasoff and Berry, but
they certainly learned something, especially the idea that you could actually use an electronic
computer with memory, with a programmable that was binary that used floating point arithmetic
and to solve algebraic equations. ENIAC was a colossal calculator. ENIAC was a gigantic calculator,
physically gigantic, but while it was much faster than an electromechanical relay, a relay-operated
computer calculator, it was still used as a calculator. What's interesting about ENIAC is,
as a watershed machine in the history of what I believe we really mean by the computer,
is, again, a sort of an instance of circumstance. And here we begin to get at the idea of the
computer, completely independently of all of this stuff about calculators, of the building
of more and more sophisticated calculators, which are often called computers because calculating
is called computing, but it's not what we mean by the term computer. In the mid-1930s,
a British mathematician named Alan Turing addressed a deep foundational problem in mathematics
that had been posed at the turn of the century by one of the greatest mathematicians of the 20th
century. Hilbert, at a conference of mathematicians in Paris in 1900, had posed a set of problems
which became sort of the gold prize of mathematics, was to solve these problems of Hilbert's,
and one of them had to do with showing that there was a decision procedure that would guarantee
the solution of any mathematical problem, that from a logical point of view, you could guarantee,
you might not know the solution, but you could guarantee that every mathematical problem,
because it was really a problem in logic, which was Hilbert's view of mathematics,
that mathematics could be formalized in a logical way. He was not exactly a logist,
he was a formalist. He believed that mathematical reasoning, that same sort of deductive mathematical
reasoning whose influence we've been tracking since ancient Greece, was an example of formal
reasoning from various kinds of principles, and given the principles that we use in arithmetic
and geometry, that all problems in arithmetic, for starters, have a decision procedure. Well,
what Turing proved was that he proved that this was not the case, that there was no such decision
procedure, that he proved that it was not possible to have a decision procedure, that if you stepped
through that procedure, would guarantee the solution of any problem in arithmetic.
In the course of generating that proof, Turing imagined a very simple machine that would embody
such a decision procedure, and he showed that in the case of arithmetic, the machine would have to be
infinite in order to work. A byproduct of his solution, so to speak, a secondary consequence
was that he said, well, but of course, for any problem for which you can specify a decision
procedure. If you have a problem, and you know that the problem has a decision procedure, that if
you stepped through all of these steps, the problem would be solved, could be solved by this idealized
machine, which is extremely simple in Turing's formulation of it, and of course would take
forever to do it. It would take so long to do it that it would be ridiculous, but the idea is
that it's possible to design a machine that would sequentially step through a series of
instructions and would solve any problem that you could provide this decision procedure for,
and we now call that an algorithm, a set of instructions that if you follow the instructions
will guarantee that the problem will be solved, sort of like instructions for buying something
from IKEA, and then if you follow these instructions, it will stay together when you erect it.
The algorithm is a very important concept here. Now, Turing went to the United States and spent
some time studying at Princeton University where he met and interacted with and shared ideas with
a number of leading American logicians, but in particular, the director of the newly created
Institute for Advanced Study at Princeton, John von Neumann, a brilliant Hungarian mathematician,
mathematical physicist, mathematical logician, von Neumann was brilliant at just about everything
that he touched, and von Neumann became a very important advisor to the U.S. government on
virtually every problem involving science during World War II, including the atomic bomb project,
and on one of his trips to Washington, he met Mockley, who was going back to Pennsylvania
from Washington, and Mockley told him about the ENIAC project. von Neumann recognized
that while these people were building ENIAC, which was functioning as a gigantic calculator,
such an electronic machine could be an incarnation of Turing's idea for a machine that would solve
any problem for which a decision procedure could be specified, and so this was in 1943.
In 1943, while ENIAC was under construction, eventually it was built and it worked, but by
the time that it was operational, the war was over, and so it did not actually provide the
calculations that the military was looking for during the war. He designed a machine that was
called EDVAC, EDVAC, that would be an example, would be an incarnation of Turing's idea for a
universal problem-solving machine as long as the problem has an algorithm for its solution,
and in 1948, at Princeton, von Neumann's design for EDVAC was realized and EDVAC was built and
became the prototype of the next 40-some-odd years of computers, except for those machines that are
based on what's called a neural network principle. Computers have what's called von Neumann architecture.
They follow the rules that von Neumann laid down for sequential processing because electronics
allows much more rapid calculations because the individual steps that the machine has to step
through in order to run through the algorithm, the instructions that it's given, that's what made
the EDVAC-style machine capable of solving the problems that von Neumann sketched out
because of Turing's machine in its conceptual form was hopelessly slow. It would have taken
forever to make a simple calculation, whereas with electronics technologies, it becomes speedy
enough that you can actually use the machine. Now, what happens with the development of
microelectronics technologies since 1948 was, first of all, the invention of the transistor
quite independently of the computer. It had nothing whatsoever to do with the computer. It was
invented in the course of exploring certain consequences of quantum mechanics, having to
do with the behavior of electrons in these weird substances called semiconductors. The invention
of the transistor at Bell Labs in the late 1940s, the decision of Bell Labs not to restrict the
dissemination of transistor technology to the public by patenting it and demanding royalty,
sort of allowing it out in the public domain, although the people who invented it subsequently
got the Nobel Prize for it, and at least one of them founded a company to manufacture
transistors commercially, but the transistor technology was given essentially to the public,
and the development of transistor technology from the early 1950s, especially when there was a
switch to silicon as the basis for the transistors, that is what generated, so to speak, that provided
the material engine for the increasing speed with which the algorithms, the instruction set
that computers are given can be stepped through, and the complexity of the machine was increased
with the shrinking of the size of transistors and their integration into logic circuits that allowed
the machine to emulate what we would call the basic rules of human reasoning, logical human
reasoning, electrically. So the first integrated circuit was invented more or less independently
in 1958, and by 1965 the number of transistors in a single integrated circuit was increasing
at such a rapid rate that an engineer named Gordon Moore predicted that it looks to him as
though the number of transistors on a single integrated circuit is doubling every year,
which means that by the mid-1970s there'd be about 64,000 transistors on a single integrated
circuit, what we now call a chip, silicon chip, and that turned out to be correct. Now in fact,
today we're up to about a billion transistors on a single chip, and that's pretty much still
in keeping with Moore's law as it's called, it's not a law of course, it's just an empirical
observation, but we're running into atomic dimensions that make it look as though that
increase cannot continue for very much longer, maybe another 10 years or so, that rate of increase.
So what's wonderful about the technology is that it speeds up the process of following the
instruction set that's fed into the computer. It was this conceptualization of the computer
that is what gives the computer its power, but let me point out what is this conceptualization.
I think that the power of the computer lies in that it is a universal simulator, it is a simulator,
it's not a calculator, there is nothing that goes on in the computer that corresponds to anything
that we're interested in. What goes on in the computer is that the chip, the processing chip
of the computer is matching high and low voltage inputs, high input, low input, and combining
them in various ways as the output of the chip and then transmitting or shipping the result of
manipulating a high input and a low input in various ways that have been built into the hardware
to equipment that then does something with that. Now what it does with it depends on the instructions
that you give the computer, it could be a word processor, it could be a spreadsheet device,
it could be a video game, there's in a certain as long as you have an algorithm then the computer
will do anything, but everything the computer does is to simulate the solution to your problem.
You have to convert your problem into a form that the computer chip can process
and then it gives you back the output in the form that you instructed it to simulate.
So it is powerful because it is a simulating machine and it is universal because it can
resolve, it can solve any problem that you can give an algorithm a rule for solving
and that can be telling it to design animations for a movie or asking it to calculate numbers,
but that is just using the computer as a calculator. Now the circuitry of the computer,
the logic circuits in the computer are incarnations of another powerful idea,
a powerful idea that has also appeared in 1948 and that is the idea of information
and we will be addressing that idea in the next lecture.
