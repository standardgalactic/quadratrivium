Thanks in large part to the computer, we live in an information age and the term information
is somewhat like the term time as we discussed it with respect to Augustine. Augustine remember
said about time that if no one asks him what it is, of course he knows what it is, everybody
knows what time is but as soon as someone asks him to be specific and tell me what is
time then it turns out that what he thought he knew is extraordinarily puzzling and isn't
what time is after all. And it's analogous with information. Everybody knows what information
is but if you try to be specific about it and say well what do you mean by information
then it turns out that the sense in which we live in an information age is not at all
obvious. Let's begin by sort of bounding the term information and by distinguishing
it from data on the one side and knowledge on the other side without getting into complicated
definitions of data and knowledge. Information seems to me to be usefully understood as organized
data. So when data are organized then the data become information. Knowledge is interpreted
information. We interpret information in ways that we call knowledge. What's interesting
for the moment is to note that the terms organized and interpret information is organized data,
knowledge is interpreted information, that the terms organization and interpretation
are loaded terms. They are value laden terms. They are all kinds of assumptions that underlie
any attempt to organize, any attempt to interpret. For example it brings the taxonomy problem
to bear. Is there a natural way of organizing data or can data be organized in multiple
different ways all of which are valid depending on what your intentions are with respect to
that data. Which organization is going to be useful for example. If you have just unorganized
lists of telephone numbers, names and addresses that's much less valuable than having organizing
that data alphabetically so that the telephone, so you have a telephone book in which is organized
by persons last name. But there are many more valuable ways of organizing the data in a telephone
book and thanks to the computer we've been able to explore many of those for better and
for worse. So the terms organization and interpret, organized and interpret are remind us of
a problem that we encountered very early in this course, the problem of how you're supposed
to define terms. There is no natural definition I said at that time of terms like knowledge,
truth and reality and there is no natural definition of information. The term is defined
in various ways depending on what uses we want to make of that term and the most common
use that we make of it is tied to content. We certainly think that we live in an information
age because more information in the sense of content is available to more people than
ever before in spite of the fact that we continually think that while we're drowning information
we are not being given the information that we really would want in order to make intelligent
decisions politically, socially and as consumers. We are overwhelmed by information and in that
sense we live in an information age, the information as content but this sense of information is
not really the fault of the computer. The dissemination of books in the 19th century
especially mass circulation newspapers and magazines of the near universal literacy so
that there's a huge market for newspapers, magazines and then the telegraph and the telephone
and radio and television all flooded us with information to the point that even scholars
who spend their lives studying any increasingly narrow fields of specialization find it literally
impossible to keep up with all the publications in their field let alone in all of the fields
that they would like to be able to read in or reading generally you cannot it's literally
impossible for example for I shouldn't say literally but it's effectively impossible
for anyone to read all of the books in the philosophy of science all of the articles
published in relevant journals in the philosophy of science in the course of any given year
you have to exercise some discrimination in terms of what you're going to read. Now the
computer intensified this the computer intensified what was already a centuries long process
of what you might call a flood tide of information and it brought it to the level of maybe dysfunctionality
in which we feel that there's no possibility of even attempting to experience all the different
kinds of information that are available especially through the internet so that the introduction
of the internet and the right from the file transfer protocol which in an early phase of
the internet before it had that name which allowed you to search the hard drives of other
computers that you had access to so you could all of a sudden discover in lots of information
that was relevant to something you were studying that had not yet reached print or maybe would
never reach print the introduction of the great search engines from initially mosaic
and all the way down through Netscape to Yahoo and Google and the creation of the worldwide
web by Tim Berners-Lee all together have provided us with have made staggering quantities of
textual visual and auditory information available to us and we're drowning in that but that's
one sense in which we live in an information age but if that were the only sense it would
be relatively trivial and the computer would be an accessory after the fact it would not
be the real cause it is much in a much deeper sense of the term information that people
I think may find initially quite counterintuitive we live in an information age in the sense
of we live in an age in which information theory plays a an increasingly powerful prominent
role in technological applications and also intellectually in science the information
theory can be attributed to Claude Shannon and a man who was working at the time for
Bell Labs and in 1948 published an essay called a mathematical theory of communication which
later that year appeared as a book co-authored with Warren Weaver who had been a very important
director of the natural science division of the Rockefeller Foundation in the 1930s and
this is in some sense the founding document of information theory as it became a prominent
factor in technology and science since 1948 Shannon didn't invent this out of whole cloth
there were people who had been exploring a mathematical theory of communication and
information is the content of communication channels after all so a mathematical theory
of communication before Shannon but Shannon's work turned out to be epical he had been a
student at MIT and he got his initial degree in electrical engineering he did his master's
degree on a symbolic analysis of relay and switching circuits which was relevant to the
work of AT&T of course using relays and switching circuits in order to direct telephone calls
as people dialed them which is what people did in the days when you had to dial a telephone
number and as the dial rotated it selectively closed various electrical circuits that activated
relays that eventually settled on one particular circuit that was the number that you had dialed
and what Shannon showed was that using a mathematical model he developed he could dramatically increase
the efficiency of the use of relays by the telephone company but part of his model was
using was based on a familiarity that he had with George Bull's laws of thought you remember
that we I mentioned the lecture on the attributing reality to relationships the importance of
symbolic logic the rise of symbolic logic in the 19th century and one of the figures
I mentioned was the Englishman George Bull who formulated a logical version of the laws
of thought and even then Charles Sanders first the American philosopher and scientist
and logician noted that these laws of thought these laws of human reasoning the way we
use the logical connectives that were built into early symbolic logic could be implemented
in electrical circuits and so you could have electrical circuits that reasoned that you
could enter in the appropriate the signals on the input side and what would come out
would be the solution to problems of logical inference what without being influenced by
purse whose work I mentioned before is what was barely published in his own lifetime the
Shannon went back to Bull's laws of thought and showed that you could use this relays
telephone switches as to build circuits that simulated the laws of thought that you would
set the the input signals on the relays and out would come inferences based on and or
not and if then relation logical relationship so you could you could plug in a series of
of logical premises and out would come the the the calculation the logical inference
to be drawn from those premises of course without the contents totally irrelevant because
as I've mentioned many times the validity of a deductive argument is a function of its
form not of its not of its content if an argument if a deductive argument is valid it's because
of the relationship between among the premises and between the premises in the conclusion
not because of what the argument happens to be about that was something that Aristotle
already had already realized and became increasingly important when a symbolic notation for logic
was invented in in the 19th century Shannon when he was at MIT worked on Vannevar Bush's
differential analyzer and he was influenced by Bush who knew what that Shannon was interested
in a mathematical theory of of communication and information and he suggested to him that
he applied these ideas to the emerging field of population genetics which sounds odd but
that's what Bush had in mind and and Shannon responded to that and he does doctoral rotation
doctoral dissertation had the title an algebra for theoretical genetics applying these ideas
to to the to the field of population genetics which was looking at the distribution the
rates at which genes flowed through populations over time so there is a kind of analogy there
you can see to circuit to circuit design it was through that work in population genetics
that he became initially contacted Warren Weaver who as I said he was a very good scientist
and he contacted Warren Weaver who as I said in the 1930s was an extremely prominent figure
in American science himself a mathematician who worked as the director of the natural
science division of the Rockefeller Foundation and was very instrumental in in funding sort
of a leading edge science and making expensive instruments available to American researchers
so that they could catch up to the level of European science at the time now in in 1940
and 1948 Shannon published this mathematical theory of communication which has the startling
characteristic that he strips away the content information theory has nothing to do with
information content which should strike you as bizarre that whole first opening part of
this lecture in which we talk about being a washing content has nothing whatsoever to
do with the mathematical theory of communication that Shannon formulated and in his own words
in that paper of 1948 what he wrote was the fundamental problem of communication is that
of reproducing at one point either exactly or approximately a message selected at another
point frequently the messages have meaning that is they refer to or are correlated according
to some system with certain physical or conceptual entities these semantic aspects of communication
are irrelevant to the engineering problem wow that's dismissing the entire the meaning
of what we say over the telephone the meaning of what we write in our books and magazine
articles that's that's irrelevant to the engineering problem what is the engineering problem the
significant aspect of the engineering problem is that the actual message is one selected
from a set of possible messages the system must be designed to operate for each possible
selection not just the one which will actually be chosen since this is unknown at the time
of the design of the system if you're designing a communication system you have to design
it in such a way that it is capable of carrying any one of a large number of possible messages
and it has to be designed in such a way that it could transmit any one of those messages
whether it's a cry for help or a poem if the number of messages in the set is finite then
this number can be regarded as a message of the information produced one when one message
is chosen from the set all choices being equally likely doesn't matter what the message is
about we can attract we can attach a number to a communication channel which reflects
the total set of possible messages that can be sent through that channel and what what
Shannon did was he developed a mathematical theory for the design of of communication
channels that separated the the problem into a source problem a channel problem and a receiver
problem and then showed that you could formulate mathematically what is the optimal what is
the what is the optimal means optimal means of accurately recognizing and reproducing
a signal given any communication channel depending on the the the the physical character of the
channel we're not interested in that there is a mathematical model of an information channel
which treats information as essentially a stochastic phenomenon he applies statistics
some of which the specific form of which he actually adapted from the work of Norbert
Wiener an MIT professor who was one of the founder of the science called cybernetics
which intensively used positive feedback loops which have occurred in a number of elect of
lectures the the idea of positive feedback loops what what Shannon did was to analyze
mathematically what it would take for an automated receiver to recognize that an input was a
signal a meaningful signal rather than random noise and then to accurately reproduce that
signal either for transmission or for storage regardless of what it was about that we don't
care what it's about and in fact the statistical uns the statistics are have to have to do with
uncertainty so from his point of view information is designed in such a way that the more uncertain
the content of a message is the the the the the more information it carries if you have
a message if you you receive a message that you already knew the information content is
zero on his math in his mathematical model the uncertainty getting a signal whose whose
a form is uncertain to you that is highly informative but you have to be able to distinguish
between a a signal that is meaningful something an input that is a meaningful signal as opposed
to random background noise how can you do that given any particular information channel
so from our point of view the idea that you can have a mathematical theory of information
in which content and meaning are stripped away is startling the idea of treating information
as a stochastic process come treating the communication channel and the community and
the and the information that flows through that channel using statistics is a strong
reinforcement of the whole phenomenon intellectual phenomenon that we've seen over the last hundred
fifty or so years in which in which we have applied statistics stochastic character to
fundamental natural processes right we saw this in the in the 19th century with regard
to the kinetic theory of gas and we saw it with regard to the concept of statistical laws
and the and eval the role of chance and random mutations in in evolutionary theory in radio
activity we saw it in a deeply embedded in quantum theory and here we have here we have
the same character same stochastic mathematical modeling applied to information which might
seem odd because how can that be applied to meaning but take the meaning out and now we're
treating we're treating information as a signal that has a certain form and we don't care
what the form stands for what meaning a person puts on that form now this has to have has
to sound hopelessly abstract to you it has it seems to me that it has to be the case
that this seems that there's no way that that that this can have any practical applications
but it turns out that that's quite wrong on the contrary the techno science built on
Shannon's theory of mathematical theory of information are blockbusters for example in
in no particular order the the fiber optic networks that are now being installed all
over the world are based on Shannon's mathematical model of information and the communication
channel that messages are distributed from it was Shannon who showed that the closest
you can come to the most efficient means of transferring information accurately with error
correction incorporated is a bits binary a binary bit stream so the internet is based
on Sharon on Shannon's mathematical model of information all your cell phones are based
on math on Shannon's mathematical models of information the so-called CDMA co-delay
multiple access TDMA time delay multiple access technologies which underlie the overwhelming
majority if not all of the cell phones and in some form or other all the cell phones in
the world all all rely on algorithms that were that are founded ultimately in Shannon's
in Shannon's information theory music CDs DVDs are are reflect Shannon's information
theory the the the theory of of deconstructing the message and storing it as a series of
pits to be read by a laser on a music CD or on a DVD is is based on Shannon's algorithms
the cryptography the whole idea of encoding and decoding messages nowadays utilizes Shannon
algorithms for the efficiency with which messages can be coded and and decoded and and this
doesn't exhaust the applications for example storage drives hard disk storage drives also
use Shannon's information theory again because from his point of view detecting a signal
accurately and then reproducing it accurately can either be for purposes of transmission
to another through another channel or for the purposes of storage which is transmission
with a time delay or for the purposes of reproduction in the just as let's say through a speaker
so it doesn't matter the of the whether we're talking about music CDs video DVDs whether
we're talking about cell phones whether we're talking about hard disks for computers whether
we're talking about fiber optic systems all of these utilize utilize Shannon's information
theory furthermore as if this weren't enough remember what I said about Shannon back when
he was a young man and did his master's thesis on the switching network in the telephone
company Shannon showed that you could use electrical switches in order to to to incarnate
so to speak you could make models of human reasoning logical reasoning using these electrical
switches Shannon in the late 1940s laid down the certain the fundamental electrical circuits
that became the logic circuits of computer chips all of the computer chips in the von
Neumann architecture family of computers which is the overwhelming majority of computers
in the world are still using logic chips which at their core incorporate variations on well
they always incorporate Shannon's logic circuits which are a sort of electrical implementations
of George bull's laws of thought that's why Boolean algebra is frequently taught to people
who study computer science and and variations and extensions of those so that his work is
embedded embedded in the logic chips of the the processing units of every single computer
chip following the von Neumann architecture so I excluded the so-called neural network
chips which have a different logic now I don't know if it's cute or not but it's interesting
that Shannon together with a colleague at Bell Labs and a and a friend at MIT who may
or may not have participated in the adventure but certainly was intellectually responsible
of decided to adapt his his use of probabilities statistics mathematical model from information
to gambling and in the 1950s and 60s certainly in the 1950s Shannon and his Bell Labs and
colleague went to Las Vegas where supposedly they made a very great deal of money but nothing
compared to how much money they are reputed to have made when they applied this to these
these this model to Wall Street and and and benefited from seeing from being able to identify
statistical patterns that that that paid off handsomely for them at least that's the story
and Shannon was was canny enough not to to make an issue out of this and and so he and
his colleague were never banned from the gambling casino is the way card counters were subsequently
and it is the case today that the overwhelming majority of trades going on in the stock markets
and let's say the the and wall on Wall Street or the New York Stock Exchange for example
the London Stock Exchange are computer trades that are based not on the content of the stocks
being traded the companies of the the nature of the company but on statistical patterns
that complex algorithms are looking for in stock trading data noticing for example I'm
just making this up that that on that whenever IBM stock moves up in a certain way Texas
instrument stocks move down in a certain way and then using that information to buy and
sell selectively buy and sell IBM a Texas instrument stock so a very to a very considerable
extent actually it seems to be the overwhelming majority of stock trades today really have
nothing to do with the character of the company stock of being traded like well they just
invented something or they haven't invented anything in a year and this is analogous to
Shannon stripping away the content from from information and treating information as a
statistical pattern so now we have two senses of the term information and I want to add a
third the first of course we live in an information age because we're awash in content the second
and I think much more profoundly we live in an age in which information theory is of is
that the underpinning of many of the technologies that have shaped and are shaping our our daily
lives in a wide spectrum of ways the third sense in which we live in an information age
is sounds like it's right out of science fiction and and that is the sense in which information
is attributed physical reality that information structures have physical reality that information
structures are physical reality that would be the most extreme claim now there's one one sense
in which information structures were on what began to be understood as real is in the decoding
of DNA and we'll be talking more about that in the lecture on molecular biology but we all we
all know about the DNA code the the power of DNA to regulate cell metabolism is contained in the
sequence of the bases that make up the DNA molecule everybody's every order every organisms DNA
has the same bases it's the sequence that makes the difference so and sequence is a pattern it's a
relationship it's not the base whose properties make the give the instruction to the cell to
manufacture some protein at a particular time or to stop manufacturing it another time it's the
particular sequence of these same bases and it was the fact that it was recognized in the 1930s
that everybody's DNA whether you're a plant or a fly or a human being has the same bases that
made people think that this must be an unimportant molecule because how how could it be it doesn't
make it doesn't make a difference whether you're a fly a plant or a human being but it turned out
that it's the sequence so that is the sense in which information by itself it's a bit information
is a particular kind of relationship that's the way Shannon treated it it's a statistical pattern
the information is a relationship that has physical reality this notion is extended much more
dramatically in physics for example the the physics of black holes uses information theory
mathematics which itself draws on the concept of the same mathematical rules that guide that
that define entropy and thermodynamics information theory mathematics to analyze black holes and
in fact now the the most current thinking in the early 21st century which Stephen Hawking
recently accepted although he had been opposed to it for years and made a bet that it was wrong
but he now paid off on the bet because he had to acknowledge that it was right is that the that
and all of the information content associated with black holes sucking in energy and matter is
contained in the black hole and is contained on the surface of the black hole nothing is lost
inside the black hole a complete description of what got sucked into the black hole is is part of
the the structure of the black hole we don't have to read that but it's part of the structure of
the black hole and curiously is a function only of the surface area of the hole not of the of the
volume and that that seems to be a real puzzle but that has led to what's called the holographic
principle which is that in some sense the universe is an information structure you know the difference
in a hologram and a photograph is that the whole of that that the photograph is strictly
two-dimensional you can't look behind the photograph to see what the objects in the photograph look
like from behind but in a hologram which is also two-dimensional all of the three-dimensional
information is contained so that appropriately stimulated you can walk around a hologram and
see the three-dimensional character of the objects that superficially look like they're only two
dimensional in the hologram that three-dimensional objects are in fact information structures as
in some in some physically real sense and this idea of of pursuing information structures as
as physically real entities and that material entities are and certainly energy structures are
information structures ultimately is explored in a subset of physics called algorithmic
information theory which which is indebted to Andrei Kolmogorov a Russian mathematician who
did a lot of the deep mathematical thinking as well as deep American thinkers such as Gregory
Chaitin and Ray Salomonov this algorithmic information theory is used as a means of defining
three-dimensional objects as information structures which certainly seems dramatic and and maybe
even maybe even revolutionary but it turns out to have explanatory power and in some cases to
also translate into techno science applications.
