Robots on the road are one of the most transformative, emerging technology.
Many of us already have elements of robotic autonomy built into our cars, and we have had for years.
That's right, your car is a robot.
Creating robotic cars involves incredible challenges.
Chief among them is this trade-off, speed versus safety.
As drivers, we know this trade-off firsthand.
Nothing is more frustrating than being in a traffic jam that slows us down,
and nothing is more scary than being in an ice storm and losing control of your vehicle.
To drive safely, slow down, but if you slow down, you take longer to reach your destination.
To enhance both speed and safety at the same time is the goal of the modern automotive industry.
Nearly every manufacturer has already embedded robotic elements into your car to assist you.
Let's take cruise control.
That's automatic speed control, and it's a robotic system because it involves sensor-guided movements.
You let your car take over the gas pedal, allow it to adjust the throttle, keep your speed steady as you travel up and down hills.
Now, interestingly, automatic speed control was invented in 1788 by James Watt and Matthew Bolton for steam engines.
As a mechanical controller called a governor, it was available as a working technology almost from the moment that cars were invented in the late 1800s.
But it took until 1958 for the first commercially available speed control in a car where it was initially called an autopilot.
Cruise control is an example of what works by negative feedback control.
Now, the word feedback tells us that a sensor is feeding information about the real world back into the system.
For cruise control, the sensor is the speedometer.
The controller compares the actual speed to the desired speed, and the difference between those two is called an error.
Any system that uses negative feedback control is self-regulating and embodies the kind of self-control we see in autonomous robots.
The MO of the automobile industry has been to add one-by-one different autonomous systems to vehicles.
In response to cruise control's lack of braking, in 1995 Mitsubishi introduced a laser-based adaptive cruise control.
And the idea behind the adaptive cruise control, which goes by a variety of different trade names, is what in robotics we call object avoidance.
Don't hit the object, the car, in front of you.
Most adaptive cruise control systems use radar as their primary sensor.
Radar is an active sensor system, and emitter sends out radio waves at a particular frequency, and then a receiver collects the echo, the return of that signal.
And using radar, you can measure the distance from you to the nearest object.
You can also tell the relative speed of that object, whether it's heading away from you or towards you.
And with adaptive cruise control, you set a target speed, like good old-fashioned cruise control,
but you don't hit the vehicle in front of you because of that object avoidance function.
You end up following at a set distance behind the vehicle in front of you, and that distance is adjusted based on the speed you're traveling.
Now, to avoid collisions, you need to add an actuator to your toolkit.
In addition to throttle control, you need control of the brakes.
You'll often find emergency braking sold as a feature on cars, and it really is a part of the whole object avoidance system.
Even though adaptive cruise control is meant for the high-speed movements of freeway driving,
some companies like Bosch have a related system called stop-and-go that is built for slow speeds.
In heavy traffic, stop-and-go will bring the car to a complete standstill if needed,
and then initiate movement when the car in front moves.
And this makes traffic jams far less frustrating, right, if you're behind the wheel.
So, what would we like to add next in terms of partial autonomy?
One common cause of accidents is when you change lanes and you don't see another car in your blind spot.
Bosch offers side-view assist.
Using four ultrasonic sensors, two on each side of the car, the side-view assist signals the driver when an object is detected in the blind spot.
In cases where the car is simply drifting out of the lane like when the driver is sleepy,
some companies offer lane departure warning to signal the driver and lane-keeping assist to actually take over the steering.
Video is a common sensor for lane-keeping, since most paved roads have clear lines that mark the edges of the road.
Many simple robots do the equivalent of autonomous lane-keeping by following lines.
Let's do this with Sparky, the robot from ArcBotics.
Sparky has three IR sensors on the bottom and up front here.
Now, these are active sensors with an emitter and a receiver.
The emitter pulses an IR signal that is then reflected off nearby objects.
The intensity of the signal received by the detector can tell the distance of the object.
To use these sensors in line following, the trick is to take advantage of the fact that different colors positioned at the same distance have different IR reflectivities.
So if we put a white line as we've done here down on this dark surface, we hope that Sparky can detect that difference.
And so I've just turned Sparky on. Let's see what's going to happen here.
And Sparky is going to take off right now.
So we'll see if there's any change in movement.
Yes, Sparky has detected the line. It's turning towards the line here.
And as it moves along, you can see that it wiggles, makes little adjustments as it moves along.
And I've got to keep it from going off here.
The end, we've just seen line following.
So what's cool about this is what's happening is that the computer code in Sparky is coordinating the three sensor signals with its motor movements
and commanding that Sparky turn in the direction of the sensor that first detects a weaker signal.
Once all three signals detect that weaker signal, then the trick is to move in the direction opposite to the sensor that detects a strong signal from the white background.
Now, this is not how cars are either keeping to their lane or navigating longer distances.
But line following illustrates two principles of robotics.
First, we can use sensors to detect regular features of the world that allow for local navigation.
And second, we can modify the environment to help vehicles stay the course.
In fact, we already have lane markers on the side of the road.
And as we just saw with lane keeping, some vision systems in cars make use of these regularities in the world.
The regular position of objects in the world can likewise serve as information that our robotic cars can sense and use.
Take parking.
Park assist can sense other cars and be used when you want the car to park itself.
The same sensors that are used for side view can help detect the open parking spot
and then commands are issued to the steering and gas to maneuver the car carefully into position.
Now, because twice the number of accidents happen at night compared to the daytime,
one of the most exciting driver assist features is enhanced night vision.
This system works by combining infrared light and video analysis.
IR emitters up front in the car send out signals that are read by the IR video camera.
A special screen on the dashboard shows the scene ahead.
Using software that also recognizes pedestrians, the night vision system can brightly illuminate people so that they can be seen and avoided.
With the exception of old-fashioned cruise control, all of these behaviors rely on vision
or a sensory capability that functions like vision, such as radar or infrared imaging.
Engineer Ernst Dickmans was among the first to build a robust working vision system for robotic cars.
Beginning in the 1970s, Dickmans realized that trying to make sense of a visual scene,
which was complicated, right, in itself was complicated by the fact that we create all kinds of flow as we move.
You can get a sense of visual flow if you just walk forward and pay attention to the whole field of view that's expanding around you.
The flow of the visual field is what I experienced as the sides of my world streamed past me.
The corollary of visual flow is that the one object I'm heading towards most directly does not flow past me, but it stays put.
So Dickmans recognized when he was building this vision system the speed-safety trade-off,
and he and his team were working at a time when computers weren't very fast.
Yet here was all this self-generated motion and motion of other objects that had to be sensed, disentangled, and computed.
So his approach was to have the computer use a model of its motion and the motion of other objects.
Now, here's the great thing. Since motion follows the laws of physics,
the robot's model can easily build a set of reasonable expectations about how the world is going to behave.
You can't break the laws of physics.
So the trick then is to use your attention to relate different frames of the visual field to one another over time
to identify new objects, track them, and then predict where they'll be next.
Corrections to the visual inputs were made using inertial sensors for velocity and acceleration.
In 1986, Dickmans and his team built a fully autonomous robot car that drove in tests on empty streets in Germany.
With participation from the automobile industry, by 1995 vehicles equipped with Dickmans' dynamic vision system
were traveling safely for up to 1,000 miles at a time at speeds as high as 100 miles per hour on public highways.
While dynamic vision solves many of the immediate problems of autonomous driving,
it was not meant to solve the longer-term problems of driving from point A to point B.
So combining navigation with autonomous driving in robotic cars was the primary challenge put forward in 2004
by the Defense Advanced Research Projects Agency of the U.S. Department of Defense, which is commonly known as DARPA.
DARPA ran a robotic challenge that they called the DARPA Grand Challenge.
The DARPA challenge was for fully autonomous robotic ground vehicles.
To succeed, a robot had to travel 150 miles off and on road between Las Vegas and Los Angeles.
Now DARPA had this requirement, quote,
Entries must be unmanned autonomous ground vehicles capable of completing the entire route without external communication or human control, end quote.
What made the DARPA Grand Challenge more difficult than the work already done by Dickmans' team was that most of that driving was actually off-road.
No more lane markers, few regularities, no signs, no cars to follow. Off-road is less structured than on-road.
DARPA defined the route using a series of GPS waypoints.
Waypoints and navigation are intermediate places on the map that you have to hit.
There are the dots you connect to create the path of your journey.
Now the off-road portions included mountain passes strewn with rocks and flat dry lake areas.
Roads would include some bridges and underpasses.
GPS waypoints were provided to teams only a short time before the race began,
so the robots had to be able to quickly read and plan out a course.
At first glance, the Grand Challenge of 2004 was a failure, a complete failure.
Even though 15 robots started, not a single robotic vehicle finished the 150 mile course.
Some, like team Enesco's David, were turned turtle by the local challenges of the terrain, and it was worse than that.
Even the best vehicle went only 7 miles before it went off course and got stuck on an obstacle.
That was a modified Hummer called Sandstorm, built and fielded by red team robot racing from Carnegie Mellon University.
But roboticists can always learn from mistakes. We often fail our way to success.
So what did we learn? Some vehicles were able to navigate to the GPS waypoints,
but those robots tended to do a poor job detecting objects along the path.
Other vehicles were better at sensing those objects, but they weren't good at navigating to the waypoints.
So it was clear that a short range system like Dickman's Dynamic Vision
needed to be combined with a longer range navigational system.
Now, to DARPA's credit, instead of folding up their tent and running,
they decided to hold a similar Grand Challenge the next year, 2005.
Given all that had been learned in 2004, this turned out to be a brilliant decision.
In 2005, 23 robots started the 132-mile completely off-road course in the desert of Nevada.
Five robotic vehicles finished the entire course.
Because of this huge and positive turnaround, many consider the DARPA Challenge of 2005 to be a watershed moment in robotics.
Perhaps the most revealing result in 2005 was that the winning robot Stanley,
a modified VW Torregg, hadn't even competed the year before.
Stanley was the brainchild of the Stanford Racing Team, which was led by Sebastian Throon,
then director of the Stanford Artificial Intelligence Laboratory, abbreviated as SAIL.
So how did Stanley's team do it? Here's their approach in a nutshell.
Treat autonomous navigation as a software problem.
The strategy of the Stanford Racing Team was that all the failures of 2004 could be solved by building a better control architecture.
DARPA had already told everyone that a stock four-wheel drive pickup truck could be physically capable of traversing the entire course,
so no need to redesign the body.
In terms of sensors, they used three types for short-term navigation, staying on the path and avoiding objects.
Radar was the longer-distance system.
We've talked about how radar is used for adaptive cruise control,
so it's good for keeping track of other vehicles and is insensitive to things like shadows that can throw off a video system.
For finer resolution at short distances, Stanley used lidars, which can handle resolutions down to 1 centimeter at distances of up to 25 meters.
The word lidar comes from a smushing of light and radar.
The light of a lidar is laser light, which the sensor shines out into the world and then reads the radar-like rebound of that light to measure distance.
Stanley used an array of five lidars mounted on its roof.
The second-place robot, Sandstorm, used both radar and lidar too,
and the third sensor system that Stanley used was a video camera for optical vision that could sense out past the range of the lidar but closer than the radar.
Three different navigational sensor systems were used because each involves different trade-offs between range and accuracy.
The lidar system provided good resolution out to about 25 meters.
The camera sees farther and is able to collect more data than an individual laser.
The radar senses out to 200 meters, but it provides sparser information about small features than either the camera or the laser.
For long-term, long-range navigation, Stanley used a GPS and a compass.
Stanley also had an initial measurement unit with accelerometers akin to those the Dickman's team had pioneered for road vehicles
and gyroscopes that helped Stanley know when it has turned and if it's tilting dangerously.
Odometer sensors also kept track of the number of rotations of Stanley's wheels to help it estimate position on the path.
Their software controller consisted of three separate modules.
One, a data acquisition module.
Two, a planning module.
And three, a world model module.
So this was 100,000 lines of code running on a 1.6 gigahertz Pentium M-based computers, which by the way, they had six of them in the trunk.
Stanley's controller was innovative for two reasons.
First, the controller took into account that all the information from the sensors about the world and about the model,
and it also took into account information about the actuators, and all of that information contained uncertainty.
So that was the big idea, right? Getting uncertainty.
So they modeled that uncertainty explicitly.
Second, that controller was built to be teachable, and it was extensively trained by humans during hundreds of hours of off-road work.
The idea of modeling uncertainty has helped create a new subfield in robotics called probabilistic robotics.
One of the things that is certain about sensors is that they are uncertain.
They provide imperfect information.
For example, let's say that we're using a camera to detect edges of the road for lane keeping.
And remember, cameras are very good at finding edges because most of the time, edges in the world create sharp contrast of light.
And it's that contrast line that video cameras love to see and sense very well.
But one problem for a robot is that sometimes high contrast lines can come from the border of a shadow.
They might not be physical edges at all, in other words.
Now, you and I know because of our experience that we needn't be afraid of our own shadow,
but robotic light sensors cannot always tell the difference.
For our robots, we have to understand how and when shadows occur
and build that knowledge into our uncertainty about the information coming from the camera.
Since radar is sending out radio waves and reading the reflections, radar isn't fooled by shadows.
The controller's world module can then combine this information from multiple sensors
to make a probabilistic guess about whether the edge in the camera is an actual edge or a shadow.
If it's a real edge, then a quick change in plan needs to be enacted, if not, carry on.
In the dynamic, ongoing process of collecting sensor information and creating plans and signals for the actuators,
each of the controller's modules does a different job.
The job of data acquisition is to collect and then convert the raw data from the sensors
into the first approximation of meaningful information.
For example, raw data from the five lidars is just a blur of numbers. What do they mean?
Well, answering that question involves using all five sensors to create a 3D model of the road ahead,
searching that 3D model for specific patterns rapidly, and then reporting the patterns to the planning module.
The planning module, in turn, collects data continuously from all the sensor systems,
adjudicates differences in sensor opinions based on the probabilities derived from using and comparing those sensors,
like we just talked about, and then plots a safe course to the next waypoint.
Because Stanley is in constant motion, both data acquisition and path planning have to happen continuously
and very rapidly, and it's the third module, the world model, that knows how to drive safely.
For robots operating off-road, the world model relies on a map that Stanley has created in real time,
just in time, as it drives.
For example, if a turn has to be made, the world model understands how Stanley handles under the conditions at hand,
and then adjusts speed and steering so that Stanley drives under control.
This map of conditions just in front of Stanley is called the driveability map.
The driveability map is linked to a global reference frame through GPS, but then Stanley's position on it is plotted.
What is also plotted are the probabilistic guesses of where it's safe to drive, and that's driveability.
By classifying different parts of the road or terrain ahead as safe, dangerous, or don't know, unknown,
this information can be used by the planning module to chart a safe course and speed the way to the next waypoint.
So when we look at these driveability maps, the purple line extending out of Stanley's nose
is the best path that the planner has plotted.
So the shaded purple area on one of these maps represents a whole host of other possible paths.
Now in practice, Stanley had to be taught how to drive.
Training Stanley is an example of what we call machine learning.
What and how Stanley learned reminds me a lot about driver's education.
The goal of driver's education is to learn to drive safely, right?
So Stanley is looking ahead and trying to answer this question as it drives, is it safe to drive in this area?
So in the training phase, Stanley has only two ways to answer that question, yes or no.
And here's where our human teacher comes in, the driver education.
The human drives while Stanley is on and observes and works on answering the question about driveability.
The human is deciding to drive only where he thinks the terrain is drivable.
So if Stanley agrees that the path ahead is drivable, then the decision-making process that Stanley used to make that decision is rewarded.
No reward is given if the human and Stanley disagree.
For Stanley, the reward is a mathematical treat, increased probability.
So the next time that Stanley encounters a similar pattern of sensor readings,
then the probability increases that it will classify the area ahead as drivable.
Stanley learns.
So now you understand how the Stanford Racing Team was able to treat the autonomous navigation of a vehicle in the desert as a software problem.
The design of the three-module controller, combined with estimating uncertainty and machine learning,
were the keys to the game that allowed Stanley to finish the 132-mile off-road course and win the race.
Throne and many members of the Stanford Racing Team took what they had learned with Stanley to Google to build the Google driverless car.
Now, nearly all automobile manufacturers have begun work on autonomous driverless cars,
and they've been helped immensely by components manufacturers like Bosch,
who are making robotic systems that can be deployed on any vehicle.
Like Google, Bosch's driverless system uses lidar, video, and radar to create a dynamic map of the world.
Part of the technology transferred from Stanley to the Google driverless car was this drivability map.
Google's testing engineers would ride in the passenger seat essentially looking at the Google car's version of Stanley's drivability map.
Now, one of the benefits of driving on roads, as opposed to off-road,
is that roads offer more regularities in the world and more structure from lines to signs and curbs.
Google has also mapped the roads, so the robotic car doesn't have to start from scratch the way Stanley had to in the desert.
What's more challenging, though, what makes the world of roads more unstructured,
is that in suburban and urban settings, roads can be packed with irregular traffic.
Not only cars and trucks of all shapes stopping and starting in unexpected times and places,
but dogs, pedestrians, skateboarders, bicyclists, and their positions are constantly changing.
States in the United States began to offer driverless cars the right to operate in 2012,
and testing began in the United Kingdom, Singapore, and other countries.
One of the great things about these vehicular robots is that this technology can be applied to trains, trucks, and buses as well.
So the potential here is to completely overhaul our transportation networks that carry people and goods.
For all vehicles, an immediate benefit that we see, even with driver assist functions in a semi-autonomous vehicle, is safety.
While the number of deaths from automobile accidents continues to trend downward as we've added safety features like seatbelts, airbags, and anti-lock brakes,
tens of thousands of people still die every year from vehicle-related accidents. Millions more are injured.
The promise of driverless cars, trucks, and buses with even more automation is that they would reduce further those deaths and injuries.
And in 2014, the United Nations Convention on Road Traffic was rewritten to allow self-driving cars as long as, quote,
the system can be overwritten or switched off by the driver.
But what about cases where some sort of collision remains unavoidable?
For those cases, rules about how to function in a collision can also be programmed into the world model of the vehicle.
For example, imagine you're being driven autonomously in a small car on the freeway.
Using its radar, your robotic car detects that the tractor-trailer truck just in front of you is losing control and predicts, using laws of physics,
that collision is imminent and unavoidable.
Your car runs an accident model in its planning module and determines that you, sitting in the front seat, are likely to be severely injured
unless action is taken. So your car runs several different plans to ascertain which course of action is most likely to reduce your injury.
It selects the plan that initiates a 180-degree spin of your car so that the rear end of your car collides first with the trailer,
giving you five more feet of distance between the impact point and the collapsing frame of the vehicle.
You're injured, but to a lesser degree than what would have happened had you not had that action taken.
In addition to safety, driverless cars could increase the independence and mobility of people who, for a variety of reasons,
are unable to drive a car themselves.
For example, as we age, what often makes assisted living imperative is reduced mobility, not being able to get to the grocery store or to a doctor's appointment.
One of the unexpected consequences of having driverless cars is that we'll be able to put many more cars on the road as well.
By some estimates, only 5% of a crowded road is occupied by vehicles.
Lanes are much wider than the width of the vehicles, and following distances are kept larger than physically required to compensate for the slow reaction time of human drivers.
If you take a defensive driving course, the first thing they tell you is, keep a safe following distance.
A Columbia study from 2012 found that each vehicle with robotic sensing capabilities can contribute to a steady increase in the capacity of a highway to allow more vehicles on it.
Communication among robotic vehicles is ultimately expected to be even more powerful.
Once about 80% or more of the vehicles gain the ability to communicate automatically with each other, highway capacity rises even more sharply by compressing that traffic, allowing a smaller following distance.
So, in short, by using robotic sensors and communication to pack those moving cars together more efficiently, we could eventually double or triple the capacity of our existing roads.
As population expands, this would save countries billions on unneeded road expansion and keep land available for farming, housing, wildlife and other purposes.
But with robotic vehicles, the opportunities go far beyond traffic lights or compressing traffic.
Stop-and-go traffic of all kinds is the worst for mileage for two reasons.
First, if you're stopped, you aren't going anywhere, right? Gas mileage is zero miles per hour.
Second, when you accelerate, you use more gas than you do when you can simply cruise at a constant velocity.
If the transportation network is tracking all the vehicles on the roads and pedestrian traffic as well,
that information could be used to calculate and send electronic signals to each vehicle about optimum speed and position to minimize the time that traffic on average is stopped.
Congestion levels that currently bring traffic to a stop could be managed more efficiently so that traffic keeps moving.
The transition to autonomous cars and intelligent transportation systems will take time,
but the technology is already available to make robotic cars not only feasible but safer and more efficient,
an improvement that will probably be rewarded with reduced insurance costs.
Robotic cars may overcome the tradeoff of safety versus speed.
As the infrastructure for robotic cars to communicate with each other through intelligent network continues to develop,
it's even possible that some roadways will be restricted to robotic cars only, offering passage that's fast and safe.
In any case, robotic vehicles will continue to make driving safer, more efficient, and yes, for those who want it even faster than ever before.
