What happens to a robot when you get rid of its sensors and you don't let it have an
internal world map? I want to show you using the Hexbug Nano. It's designed on purpose
not to have sensors. Instead, the Nano has a wonderfully simple actuator based on Vibrobots,
an idea developed out of Mark Tilden's beam robotics movements. So I'm going to turn it
on here and then put it right down here on the floor and we're going to watch what happens.
So these things are quick, right? And they're kind of freaky because they do remind us of
bugs. And they tip over, they get back, grab it, toss it back here. They're really cool.
But there's no sensor involved, right? It's not steering itself. It's just moving around
in this wonderful way. Okay, so if you look close up at the legs of a Nano, you can see
they don't actually move. They're like little bristles. They aren't articulated with joints
and muscles. Instead, they're curved and flexible. So that's all that this actuator does. It just
vibrates up and down. That vibration bends the legs when it's doing it and the legs when they
unbend are what pushes Nano forward. So Nano is not an autonomous robot. It's just an actuator
and a body. Now, clearly, since the Nano will zoom right along the edge of the floor, it's
not like it's detecting anything important about its world. Now, let's see what happens
if we ask the Nano to navigate its way through a simple obstacle course. I'm going to come
back here and we'll do that. All I'm going to do now is put my foot in the way. And let's
see what happens to Nano. That's pretty cool, huh? It actually moved up and around my foot.
And the trick here in its design is that it's built to move in a counterclockwise fashion.
So you can see that it fights its way around obstacles, but it gets fooled because it doesn't
really have sensors. So it doesn't have any way to navigate towards a goal, a beacon,
or a landmark. In principle, building an internal world model is a great idea if you
have sensors and you have the computing power. You sense, you plan, and then you act. Sense,
plan, and act. That's how we thoughtful creatures work, right? That may be true in part, but
the real trick is that we have to constantly build our model. And at the most, say, walking
in a room, you have to be able to create and update our own map while we're operating in
the room because we need to map ourselves as it's happening. No one hands you that kind
of a map, though, when you walk into a new room. And this is very zen. There is no map.
Save the one that you are continually creating. And that map is always changing. Think about
entering a cluttered room that you've never seen before. You have to get to the other
side, but you do not have a map, which path do you pick? What's in your way? Can you go
around it? This is a serious challenge. What would you do? Well, if you're like most of
us, you might take a couple of steps forward. You look around from a different perspective.
Now you can see things you hadn't seen before. Maybe you spot some low-lying boxes that block
your original path. No big deal. You see a new way forward. Or maybe you can still not
see a path. So you move your head a bit to get a new perspective. Tricky, right? But
you can imagine making this work. What would make your navigation task even more complicated
now is if the world suddenly changed right in front of you. Imagine this. You're in
a cluttered room, and that room is a warehouse, with workers moving stacks of crates and boxes
all around. You not only need to keep track of the stationary objects like before, but
you also have to keep track of the moving objects. The people and the crates in the
boxes that are pushing around and to make matters worse. You have to be able to distinguish
between real objects like the boxes in the people and false objects like shadows or stains
on the floor, sort of illusions, right? Posters on the wall. So here's the bottom line. Live
by the map and die by the map. You have to rapidly, constantly, and accurately update
your internal world model. You have to make and remake your map continuously as you learn
about the constantly changing world you're in. If you don't, you could be in trouble.
For example, imagine if you were allowed just one peek inside the warehouse, and then you
had to close your eyes and move across the room. Could you do it? Well, back in the
early 1970s, robot Shaky's answer to this problem was twofold. First, Shaky moved short
distances, stopped, and then used its sensors to look for changes in the world. Second, if
Shaky found objects that weren't on its map, it would update the map. Then, and only then,
would Shaky calculate a new navigational plan. And this calculation took time, and looking
back, it's easy to criticize Shaky for being slow. But let's keep in mind that that was
1970, before small, fast, personal computers were even invented. So, a practical question.
How do we make Shaky faster, smarter, with more intelligent behavior, for responding quickly
to changes in the world? Most AI researchers thought they knew the answer. Shaky needed
a better brain, a faster computer with more memory, bigger, faster, stronger. That way
Shaky could update its world model faster, make it more accurate, map and remap moving
objects as it kept track of its own movements. Well, they were right in part. Faster computers
with more memory do help. But back in the 1980s, this make the brain smarter and use
maps approach had largely failed, and not just for the reason that the computers weren't
big enough and fast enough. The fields of artificial intelligence and AI robotics were
stuck in a methodological rut. Mobile, map-making robots were still very slow to move and worked
only under conditions where the world was structured, like inside. That's a world that's predictable
and not changing much. Throw most real-world situations at these robots, and they simply
couldn't move or accomplish their tasks. Frustrated engineers, some of them at least, said, enough.
One such AI researcher, Rodney Brooks, a professor at the Massachusetts Institute of Technology,
began to question the whole sense, plan, act, approach of that model-based robotics. Instead
of thinking about how humans worked, Brooks was inspired by the intelligent behavior of
non-human animals. He watched insects move, fly, navigate, sting and pollinate, and he
observed that insects behaved intelligently, doing things no robot at the time could do,
even without much brain at all. So this got Brooks thinking that the problem with robots
like shaky was that humans were trying to build them in their own image, with human-like
intelligence. So Brooks figured that by focusing on human intelligence, abstract reasoning tasks
like playing jeopardy or chess, solving math problems and building internal world models,
we'd really started the field of mobile robotics backwards, evolutionarily speaking. So why
not work the way evolution had? Start with the simpler animals and build up from there.
Pushing that idea to its limits, Brooks wondered if you could still get intelligent behavior
if you removed planning altogether from the sense, plan, act, architecture of the robotic
controller. Don't create an internal world model at all, but instead rely on sensing
and acting without planning. And since the planner is the brain, just remove it altogether
no brain, no pain. So let's see this no brain approach in action with the original hex bug,
which I have over here. So this one is really built to look like a cockroach. And I'm going
to turn it on with a switch back here. We're going to watch it move here. So what we are
after here is not to see whether we've got the basics of intelligent behavior, but I
want to look at hex bug and see how it does here if I put my foot in the way and give
it a little bit of an obstacle course. Compare this to what we just saw in nano. So it's
marching straight ahead here, but you can see it hit my shoe, it turned, I'm going to
put my shoe in the way again. It backs up, turns, give you a better view of it here.
It backs up, turns, moves a little way, and then you can see it sort of hits my shoe and
it's going to work steadily around that shoe. So what's happening here, if I pick this up,
what's happening is it hits my shoe, is its antenna right here is touching my shoe. And
look what happens if I hit the left antenna, the left leg stop moving, and the right leg
is back up. I hit this antenna, and so what's going on is each time it's doing the same
maneuver, these are touch sensors, and it just moves on back. So one of its wire-like
antennae hits the wall or my shoe, triggers the reverse and turn command again and again.
That's the wall, and that's the response to it. So what we're seeing here, and you can
really see it back again here along this wall, is what roboticists call wall following, and
it's a basic task. This doesn't look like much of a behavior, I know, but it's an important
one. It's a kind of navigation, if you will. Now, and just for comparison, let me get nano,
and we'll put nano down and see how it does against the wall. It just doesn't want to
stay along the wall here. It's going out. It's not really navigating. We're going to
let the nano go there. So what the original does is it really shows you that you have
to have a combination of sensors and actuators to have purposeful movement that includes
connecting up the triggering of that touch sensor with the actuation and the way the
legs are working to get that. So nano can't follow the wall because it doesn't have a
sensor. So hex bug works and has a very simple kind of intelligence that lets it respond
to where it is in the world. Let me turn it off here. That interaction with the world
creates the behavior that you just saw. I should mention that real insects like cockroaches
use their antennae in similar ways to guide them along surfaces. Cockroaches are also
really hard to catch if you've ever tried, because they have a pair of sensors that can
detect disturbances in the air if you try to sneak up on them. Let's see if hex bug has
any of these sensors. I'm going to disturb the air right behind its dairy air by clapping
here. So again we're going to put it on the floor. Pretty cool. It works. So notice that
it backed up and turned around. It has some kind of sensor here. Let's check out what
that is. This is a sensor all right. It's a microphone. Hex bug is detecting air disturbances
by sensing the pressure wave that we call sound. Very cool. Now this brings up a really
important question. How do animals that don't have much of a brain who don't do much planning
manage to live, to behave, and to be successful? The answer to this is, well we're going to
have to dig a little deeper into biology to find out and think about brains and intelligence.
We humans are animals with a large brain that seems extraordinarily big compared to our
body size. In fact there's a ratio in biology that's calculated across all kinds of mammals
and other vertebrates called the encephalization quotient. And that encephalization quotient
suggests that our brains are over four times larger than would be expected from our body
size alone. However, our big brain pride tends to blind us when we consider the cognitive
workings of non-human animals and their implications for robots. If you're like most of us you
probably think that no other animal or robot is as smart or as intelligent or as adaptable
as humans and depending on the situation you might even be right. A human can certainly
be at a chimpanzee at chess, baseball, or the math olympics, but intelligence depends
on the context, the environment in which your species evolved. The implication that
matters most for building robots is that every species has its own special set of sensors
and actuators that are adapted for working at a particular job in a particular environment.
Blue Marlin are visual predators using their large heated eyes to find elusive prey as
they prowl the cold waters of the open ocean. Barn owls are auditory predators using their
sound scooping facial feathers and exquisite hearing to pounce on mice in the dark. And
female mosquitoes, huh, whole factory and thermal predators using an array of chemical
sensors on their antennae to fly up streams of carbon dioxide that we mammals make as
we live and breathe. When thinking about insects Rodney Brooks realized the following
important lesson from biology. Sensor guided movement is central to intelligent behavior.
Like we saw with the little nano robot, movement without sensors may be cool, but it doesn't
let you navigate in any purposeful goal directed way. So you can't move towards any goal, you
can't navigate or get pre-specified tasks done. So what does sensor guided movement
mean? Well, let's do this thought experiment. Let's be a mosquito. Whether or not you think
a mosquito is intelligent, you have to appreciate their pesky hunting abilities. How do they
find us? To answer that question and explore sensor guided movement, let's just focus on
one of the mosquitoes senses, that ability to detect carbon dioxide also known as CO2.
So I know this isn't polite to say so, but CO2 is a waste product that we animals make
as we turn sugar into energy for our cells. Every breath we exhale has a hundred times
the concentration of CO2 compared to the air that we inhaled. We huff and puff creating
a cloud of CO2 and a halo all around us. If the air is still, like on one of those peaceful
summer nights when we want to sit outside, then our cloud of CO2 makes a multi-directional
pathway that's called a concentration gradient. And mosquitoes can follow that concentration
gradient back to the source, and the source is us. Ah, imagine strapping on a pair of
antennae that are about three times as long as the diameter of your head. Why? Why because
we're pretending to be that mosquito, okay? So imagine that you're wearing your antennae
and like prongs on a crazy spaceman Halloween hat, those antennae stick out in front of
you. On each antenna you have a sensor that can detect the local concentration of carbon
dioxide in the air. Now imagine that each antenna puts out a signal on a screen, okay,
that ranges from red to green. Red means that antenna is not detecting carbon dioxide. Green
means, hey, this antenna is detecting a lot of carbon dioxide. Colors in between red and
green could signal for us intermediate levels of CO2. Now, mosquitoes obviously don't have
screens with color-coded patterns, but they do have nervous systems that create signals
carrying the same kind of information. So let's keep thinking about being the bug. Now
you've got to get to work, okay? Find a human or a mammal. Can you think of how you do it?
You could use a simple rule. Turn towards the green screen, right? That's a great start.
So if this screen is green, then I turn in this direction, right? Must be more carbon
dioxide that this antenna is sensing. Sounds good. But what happens if the screens show
the same color? Well, then you might want to add a default rule. If the screens are
the same color, walk in a spiral of increasing radius. Keep moving until you see a color
difference between your two screens, always turning towards the greener screen. Well,
then what happens? If the wind is still, which keeps the CO2 concentration pathway stable,
then you'll eventually run into a mammal. Bam! Welcome to dinner. Alright, let's not
take this too far. We don't need to visualize further the vampire antics of mosquitoes we've
all been there. But let's think about what we've learned by being the bug. We found
a mammal. We tracked it down. Being the bug, we've given ourselves at least one answer
to this question. How do animals without much brain behave intelligently? And in the case
of mosquitoes, they can track down mammals with a simple system of sensors connected
to a simple nervous system that creates a continuous turning signal for the wings. And
that, my friends, is what we mean by sensor-guided movements. And I'm going to admit it, it's
brilliant if you think about it. Nature has engineered hundreds of different species of
miniature blood-thirsty mammal hunters. Mosquitoes are incredibly successful survivors. And
here's the really brilliant part. Mosquitoes win the game of life without an internal map.
They don't need one. All they need is to navigate to food using sensor-guided movement. In the
language of artificial intelligence, mosquitoes behave intelligently because they don't waste
processing power making maps that they don't need. Instead, simple reflex-like connections
between sensors and motors get the job done. So instead of sense, plan, and act, the mosquitoes
do sense and act, sense and act without planning. Behave without a brain. Now, these were exactly
the kinds of bio-inspired insights that Rodney Brooks was after when he was thinking about
the slow progress of the sense, plan, act approach of classical AI robotics. If we wanted
intelligent machines that could actually do something in the real world, Brooks' reason
that we needed to stop focusing on the computationally intensive problem of building an internal
world map. Stop making maps. Instead, we needed to think about what happens out there in the
world. Behavior happens. Animals move around and they get stuff done. So how could robots
do that? And how could they behave intelligently without planning and without much of a brain?
Back to the bugs. Once you understand the intelligent behavior of insects, you can use the simple
sense and act rule to program a robot. And that was Brooks' flash of brilliance. But
a practical question remained. Will those simple insect-inspired, reflex-like rules
really work on a robot? So to find out, Brooks and his graduate students at MIT built a six-legged
insect-like robot named Genghis. Like a mosquito, Genghis had onboard sensors to detect mammals.
But while we talked about mosquitoes sensing CO2, Brooks focused on another insect sense,
heat detection. So Brooks and his team mounted six pyroelectric heat-detecting sensors on
the front of Genghis. Genghis used these sensors, and a simple rule like the one we
just developed in our thought experiment, to follow warm mammals around the lab. And
it worked beautifully. Here, finally, in the form of the mobile autonomous robot Genghis
was a mobile robot that behaved intelligently. Genghis could move around in the world and
do something quite impressive for the time, track and find mammals. Now, no other robot
in the AI community had come close to this kind of ability, this kind of intelligent
behavior, and best of all, Genghis could track mammals in a dynamically changing world without
a map. No planning needed, no brain required either. So Brooks moved robotics from the
classic sense-plan-act paradigm, what we call model-based robotics, to what is now called
behavior-based robotics. Behavior-based robotics might not seem like quite such a big deal
to us now, and we've just seen it in some little toys, but in 1988, when Genghis was
built, it was a real breakthrough in the world of robotics, and Brooks became internationally
famous. Brooks and his students went on to use behavior-based robotics to create the
first consumer robot to have widespread success, Roomba. Now, just to have some fun, let me
put Roomba out in the middle of the lab and put my foot in front of Roomba, and let's
see what happens without a wall in front of us. There we go. So I'm just going to do
this thing of putting my foot out, and it's not trying to follow a wall along my foot,
it's trying to do what they call an escape behavior, which is figuring it's run into
the wall or a leg, and it's just looking for a new pathway to go. Roomba has this great
reflex, by the way, when you pick it up, when those wheels pop out, it immediately stops
trying to move forward. So the bumper, as we saw, is the sensor that initiates a set
pattern of movements, either an escape or it helps initiate the wall following. So you're
seeing here with Roomba one of the hallmarks of behavior-based robotics, modularity, and
by that I mean you've got an escape behavior and a wall following behavior that are set
up as different modules. So the whole repertoire of Roomba is divided up into these discrete
behavioral modules. Let me introduce you now to an insect-inspired robot that we use in
the interdisciplinary robotics research laboratory at Vassar College, where I'm a professor.
There is a robot that's called BotBallBot, and I'll explain why in a second. On top is
the digital controller, which you can see is perched on top of this body here. The controller
is built by the KISS Institute for Practical Robotics, and this version, which is the CBC2,
used to be used in their robot soccer competitions. It's been superseded by a more recent version,
but we still find it very, very useful. I actually am going to turn it on because it
takes a second for this to warm up. So we call these robots BotBallBots because they
were built for playing in robot soccer. So even though we're just using the controller
and our BotBallBots don't have actuators to play soccer, we call them that. My colleagues,
Ken Livingston, Nick Livingston, and Larry Doe, have built the BotBallBots to teach
our students in cognitive science how you can use simple insect-like behavioral modules
like Escape and something called Seek Light to create more complex behaviors, like a robot
that will search for light and avoid objects at the same time. Now, of course, if we wanted,
we could just as easily insert a Seek Dark behavior instead of Seek Light, but usually
Seeking Light is something we do a lot in robots. The programming trick for behavior-based
robotics by the way is a really, really important logical structure that you're going to see
throughout robotics, and we call it the if-then-else structure. If-then-else is the logic that we
can program into our robots, and we do that using analog electronic or digital controllers
so that they can make decisions on their own. Example, if you are in danger, then escape.
Else, explore and seek light. Do you see how that logic works? So let's translate that
into instructions for BotBallBot. Danger is when you hit an immovable object just like
we saw with our Roomba. So the BotBallBot knows it's in trouble in a sense because the
front bumper sensors here are turned on. It will then escape. Now, if the front bumper
is not on, then the BotBallBot will explore, in this case, by seeking light. So let's put
that all together and see this in action here. And I'm going to put it down. I'm going to
hit the run button here. So it's seeking light over here. But it hits front bumper.
Oh, and you can actually see it deviating. See, it didn't even hit my foot there, and
that's a neat thing that's going on here. I'm going to turn it off. In addition to the
photo resistors right here that are seeking light, the fact that it was actually swerving
away from my leg was showing you that these sensors here, which are infrared proximity
detectors, were sending out a signal that was bouncing off my legs and telling them there
was something in the way. Okay? So here's how we code the set of priorities into BotBallBot.
And you can see this in computer language that we use for BotBallBot, which is a version
of C, just like what you would use to program an Arduino microcontroller or other robots.
And there are two of the words that we've already used that's in the code in the actual
computer program, if and else. Now, we have to ask, where's then? John, you said it was
if then, if, if then else? Well, the then is implied. It's silent, hidden, but its meaning
is still there. Let me explain by starting on the top of the screen here at the code.
Here's how to decode this code. If the escape behavior associated with the front bumper
is activated, then move to escape. Do you see that here? That's an implied then. Now
let's move on. What if the escape front bump behavior is not activated? We move to else,
which is actually written there. And here's where the programming gets clever. If the escape
front bump behavior is not activated, else kicks the robot into a second behavior called
seek light. That second behavior is programmed such that if the photoresistors that BotBall
uses to detect light are activated, then it will seek light. Else BotBallBot will just
keep moving forward, exploring the world and looking for light in all the right places.
With a behavior based program like this, you can see how one behavior escape takes priority
over another, seek light. Because we program in this priority and it doesn't change, we
call it a fixed priority architecture. Another point worth driving home here is that sensors
sometimes determine which behavior is activated. As soon as bumpers are activated, then their
behavior takes control. And what's really cool about this is here, right here, you're
looking at how a robotic controller makes decisions. If blank, then blank, else, something
else. And sensor guided and selected movements are Sense Act behavioral modules. We've done
it. Welcome to behavior based robotics. What are the positives of a behavior based robotic
architecture? Because it doesn't have to plan, your behavior based robot can react quickly.
Think fast because the software that runs the computer on the robot doesn't have to
be complex. And it means the whole robot is less likely to break down. So you can be robust.
And the modularity of the Sense Act reflexes means that you can use them like building
blocks to easily create complex behaviors. Modularity, gotta love it. But best of all,
the sensor guided movements of behavior based robotics have gotten engineers to focus on
building better sensors and sensors rock. So because behavior based robots work, they are
now a fascinating part of our contemporary life. Thanks to pioneers like Rodney Brooks.
