Beyond the many exciting robots already in existence,
what makes robotics such an exciting endeavor is that there are so many different possible
futures, plural.
This variety of robot futures comes partly from the fact that robotics is a multidisciplinary
collaboration.
It's the best of mechanical and electrical engineering, computer science and neuroscience
and cognitive science and biology.
To understand our robotic futures, we need to look at our multiple pasts.
What are the beginnings of modern robotics?
In one beginning, there was the factory, and there was Unimate, the robotic manipulator
with revolute and prismatic joints creating precise and rapid motion in three dimensions.
Actuators, kinematic degrees of freedom and precise manipulation are the stars.
Over time, manipulators cross over onto mobile robots into surgical robots and onboard spacecraft
and finally attached to anchor tees.
In another beginning, there was warfare and the Whitehead torpedo, the self-propelled
weapon with a pressure sensor controlling a trim actuator and underwater cruising depth.
Sensors, propulsion and feedback control are the stars here.
Over time, simple feedback control of motion becomes the embedded robotics in commercial
aircraft, road vehicles and recreational drones.
A third beginning was with computers and artificial intelligence.
And this is where robotics starts with Shaky, the mobile robot from Stanford Research Institute
that was the first to use a digital electronic computer to reason about sensors and to make
plans for the future.
The computer as intelligent controller is the star.
Over time, programmable computers miniaturize and move into mobile robots, bringing portable
and rapid reasoning outside into the wild of the highway and the deep sea.
And in a fourth beginning, robotics was inspired by animals whose simple solutions to effective
behavior point the way to modular control of sensor-guided movement.
And here, Gray Walters robotic tortoises show us how to engineer reflexes and a simple brain
to create autonomous behavior.
Behavior-based roboticists like Rodney Brooks, creator of Roomba and Baxter are the heirs
of this approach.
And if they had their way, the exploration of our moon and the nearby planets would be
carried out by thousands of cheap, animal-inspired robots.
Over time, fast-acting, modularized sensory motor behaviors become required components
on the mobile robots doing our work in homes, factories and in the airways.
Now robotics is such a diverse field that we could keep going with distinct robot origins
and robot futures to be found in virtually every domain of modern life and every field
of science and technology.
What unites these different beginnings is that they are all on the same quest to build
autonomous physical machines that act and react on their own in order to get work done.
What also unites robotics is a fundamental trade-off that comes up in the design of nearly
any tool or machine, specialist versus generalist.
A specialist does one job, or a few jobs, and usually the specialist does the tasks
very well, very efficiently, or very cheaply.
Roomba is an example of a specialist built to vacuum floors.
In contrast, a generalist does many things, but usually doing no single task as well as
a specialist would.
But the value of a generalist is that you can do many things with a single robot.
And Baxter, built by Rethink Robotics, is a great example of a generalist.
Hello, Baxter.
How's it going today?
So Baxter is able to do any number of different jobs that involve the classic picking and
placing.
You pick, and you place.
And unlike classic factory robots that specialize in one job, Baxter is actually built to move
from job to job in the factory, and Baxter's ability to be a generalist begins with its
ability to be quickly and easily trained, reprogrammed by a human.
A generalist from science fiction would be a walking, talking humanoid that could work
in any of the physical spaces that a human can and do all the physical tasks that a human
could.
Now, in the short term, I think it's more likely that we'll be in a world where the
specialist robots continue to dominate.
We'll have a robotic vacuum cleaner, a robotic window cleaner, a robotic lawn mower, a robotic
dog walker, and a robotic car.
In this future of specialized robots, we probably won't even see most of these robots.
They'll just be our appliances, vehicles, and workplace machines.
And they won't always be called robots.
For example, is an automatic cow milking machine like the DeLaval VMS a robot?
See, just watching a system like this work, I can see that when a cow puts her head through
the stanchion to eat the grain placed in the trough, there's some movement near the ground
below her.
What's up?
I don't see anyone operating this machine remotely, so it looks like the machine moves
itself.
Next up, I see what look like lasers are making spots on the cow's udder and teats.
Then quickly, the inflation milking cup is placed on the teat.
Now, honestly, this is happening very quickly, and I'm not there to see what happens, for
example, if I put my hand in front of the lasers.
But I'm pretty impressed to see that this machine appears to be sensing the presence
of the cow, then using lasers to figure out where her teats are, and then using a manipulator
arm to place the inflation cup on her teat.
Now, if I've got that right, and there's no human behind the curtain, then most of us
in the world of robotics would call this automatic milker a robot.
One of the challenges for roboticists is to recognize that when we build a robot to
interact directly with an animal like a cow, the robot has to be considerate of the animal's
emotions.
If a cow learns that the robotic milker is an unpleasant or painful experience, then
it won't go into the machine.
But in practice, what farmers report is that most of the cows have positive experiences
with the robot, such that after they become comfortable, they will milk themselves.
To understand this positive relationship between cow and robot, keep in mind that a
milk cow produces six to seven gallons of milk each day.
As each quarter of her udder becomes engorged with milk, it becomes uncomfortable for the
cow.
So milking releases that pressure and is welcomed by the cow.
So what's really cool is that cows will happily collaborate with autonomous robots.
Now how humans of the future react emotionally to robots will be important too.
In 1951, a year after proposing what we now call the Turing Test, Alan Turing wrote, quote,
at some stage, we should have to expect the machines to take control in the way that is
mentioned in Samuel Butler's Aeroan.
In 2014, the physicist Stephen Hawking co-authored a comment on the Johnny Depp movie, Transcendence.
He warned that ignoring future advances in artificial intelligence would be potentially
our worst mistake in history.
Rodney Brooks has created many robots and he sees a middle path between utopian and dystopian
visions of the future.
He admits it's very difficult to make long-term predictions, but he thinks that a likely scenario
is one similar to that which we experience with the technology we've already created.
Gradual improvements in evolution in intelligent machines.
He writes, quote, nothing is ever as bad as we expect or as good as we expect, end quote.
And even if something highly improbable does eventually happen, there's no reason that
we still won't exercise control over our machines in the same way that we do with every other
machine in our lives, the good old on-off switch.
I don't mean to say that with robots we have nothing to worry about.
In fact, what kinds of on-off switches to install for each robot will become increasingly
critical matters to consider.
As with any tool or technology, we have legitimate concerns about how we choose to use robots.
My view is that we can focus our concerns on something that we know how to create and
control in robots, and that's autonomy.
For every kind of robot, for every task, in every workplace, we can ask this general question.
What type and level of autonomy will allow the robot to accomplish its task without harming
humans?
Is energy autonomy important because the mission is long-term and refueling is impractical?
Think about the wave gliders swimming across the Pacific Ocean and harvesting energy from
waves to do so.
Is behavioral autonomy important because the mission is remote and humans can't rapidly
communicate to the robot about how to perform a task?
Think about curiosity on Mars.
As we give robots more and different types of autonomy and withhold other types of autonomy,
we can also think in legal terms, if you're so minded.
The European Commission funded a project called Robolaw led by a group of lawyers and roboticists.
Their view about liability is unequivocal.
Quote, robots cannot be held liable for acts or omissions that cause damage to third parties.
End quote.
Now, this recommends that, least in terms of liability, robots will not be legally autonomous.
The manufacturer, owner, or operator of the robot will still be held responsible.
However, a new practical issue not seen with simpler machines is that, as the complexity
of the job and the workplace increase, the behavior of a robot can become unpredictable.
Now why should that be the case?
Behavior is the result of the robot and the workplace interacting with each other.
The computer code in the controller only controls the linkage between the sensors and
actuators and not behavior directly.
What this means is that you can't simply point to the controller and the code inside
and say that the computer code alone is responsible for the behavior of the robot.
Now this removes your ability to hold the human programmer alone as fully responsible
for a robot that does harm.
A further possibility is the second clause of Asimov's idealistic first law of robotics.
Quote, a robot may not injure a human through inaction.
Now this is really tough.
In human jurisprudence, we have good Samaritan laws to encourage bystanders to help an emergency.
Perhaps we will see specialist robots whose only job is to intervene when an emergency
arises.
Now one way to envision how the various dimensions of robot autonomy will be managed is to focus
on something called cloud robotics.
Most of the robots that we've built or dissected in this course have been communication islands
if you will.
Cloud robotics uses the internet to open up communication channels much wider and just
like cloud computing where you share applications or storage capacity over the worldwide web,
the idea behind cloud robotics is to link up those individual robot islands with shared
programs and storage capacity.
RoboEarth is a cloud robotics project started by the European Commission in 2009.
The idea was to create a collective database that robots access directly.
If a robot encounters a new situation, it queries RoboEarth to see if another robot
has already solved the problem.
If a robot learns something new on its own, then it uploads code for its newly created
solution to be shared.
So in a cloud robotics future, intelligence is linked to your ability to communicate
with other robots and learn from their experience without you having to learn the task directly
yourself.
Think of it this way.
Do you remember in the movie The Matrix how humans could be uploaded with instructions?
Boom!
I know kung fu.
Same idea here with cloud robotics.
Now to make a cloud robotics a reality, the RoboEarth project created Reputa, the working
cloud platform.
In addition to sharing solutions with each other, you actually may be able to do your
computationally expensive processing in the cloud too.
So in this model, you are splitting up the work of the controller into two parts.
Part of your controller is operating in the robot and part of your controller is operating
on a clone of your system in the cloud.
Knowing what you know now about robots, how might you divide up the work of the controller
into those two parts?
I know that I'd be tempted to put the rapid, reflex-like decisions of behavior-based architecture
into the robot and put the model-based working of mapping and planning in the cloud.
Your cloud controller could be working with your model-based architecture, updating the
model, creating alternate plans, and even continually searching Reputa for help as it
predicts what is likely to happen next.
Now here's a possible future of cloud robotics that may blow your socks off.
Let's start with the fact that Google was awarded a U.S. patent for cloud-based robotics
in 2014.
The name of the patent, and it's an important one, is the following.
Shared robot knowledge base for use with cloud computing system.
Ryan Hickman was the lead inventor and was filed first as a provisional patent in May
2011.
The idea is similar to that of RoboEarth, to connect up robots to and through the cloud.
In fact, Google cites the RoboEarth idea as described in a publication by Marcus Weibel,
a partner on the project and currently the co-founder of the robotics company Verity
Studios AG.
The fact that Google has been able to patent what may be seen in the courts as the huge
part, or all, of cloud robotics gives them an advantage over any other company that wants
to use the web to coordinate their robots, or have their robots learn from the experience
of others, or share controller space.
Let's see how cloud robotics has synergies with three of Google's other purchases in
robotics.
And maybe we can envision a future in which Google becomes for robotics, what AT&T was
for telecommunications in the U.S.
Google purchased the Japanese company Shaft, who built the humanoid HRP-2, who did so well
in the DARPA Robotics Challenge in 2014.
Google has also purchased Boston Dynamics, famous for Big Dog and Atlas.
Both Shaft and Boston Dynamics are famous for their excellent work with legged locomotion,
and the control logic that makes a hard task, like walking or standing still, look easy.
Add in a company called Industrial Perception, which works on computer vision systems and
robot arms for loading and unloading.
Now think about that entire sequence.
We've got Kiva robots owned by Amazon, building robots that move shelves in the warehouse to
where stationary humans can pick off the item and place it in a box.
Now put a smart humanoid or a smart multi-articulated arm in place of the human.
That sounds like a smart business opportunity.
And if you are a Google, you have all the great maps that you've developed and navigation
software as well for your driverless cars.
So why not drive to deliver?
And once you arrive, have a humanoid robot walk up the steps, knock on the door, and
hand out the package.
So Google also bought DeepMind, whose software can help the delivery service robots better
understand and respond to the humans that they encounter.
For more unusual deliveries, both Google and Amazon have explored using robots for air
delivery of products.
Now flying is energetically expensive, so that can be a niche service when driving is
not fast enough.
And Cloud Robotics coordinate handoffs from one type of delivery robot to the next until
the package is finally delivered.
So Cloud Robotics seems perfectly adapted to large scale network challenges like transportation
of goods and people.
But that's not the only future for increased autonomy in robotics.
Let's imagine professional and personal settings where you cannot or you don't want to expand
the abilities of your robot by using Cloud Computing or Cloud Robotics.
What might be an innovative and alternate way forward?
I'm tempted by one of the most intelligent computer systems on the market, IBM's Watson
System, an example of what's called cognitive computing.
Watson made headlines in 2011 when it defeated in a game show Jeopardy, the two very best
humans to ever have played that game.
Jeopardy Watson was followed by a variety of more practical applications including Dr.
Watson diagnosing patients, Chef Watson coming up with new recipes, Finance Watson, and customer
service Watson helping callers and offering technical support advice.
Now all these versions of Watson were just a computer program, not a robot.
But just as this scarecrow needed a brain, Watson needs a body.
So what can we do with Robot Watson?
Imagine an emergency situation where you need to be able to think on your feet.
For example, what if communication with remotely located humans had degraded because you, the
robot, were underground or electrical noise was swapping your transmissions?
What would you do?
Well, first responders acting on their own are trained to assess the situation at hand.
What's happening?
Why is it happening?
Will anything be done and in what order should I proceed?
Enter Robot Watson, the first responder.
It walks in, compares the readings on its cameras, chemical sensors, hazard detection
systems to database patterns that it's got.
Now, it then has to answer this question.
What's happening?
Watson makes an educated guess.
Natural gas is being vented in the room.
Why is it happening?
A pipe has been broken.
What can be done?
Cut off valve can be closed.
Now because Watson, the robot, has the ability to judge the confidence of its answers and
then make a decision based on the particular situation at hand, it recognizes that if in
fact the leak is natural gas, the situation is then extremely volatile.
Any spark will cause a devastating explosion.
Thus Watson accepts this natural gas leak answer with a relatively low confidence threshold.
It's better to assume that the leak is natural gas and be wrong than to waste time seeking
a higher confidence answer.
So Watson walks over to the shut off valve and turns off the gas.
That's situational intelligence.
Another important trend across robotics will be miniaturization, allowing us to pack more
intelligence into smaller and smaller spaces.
Now, remember Arduino?
This is the controller that we used in Tadro.
We can replace that now with Tinyduino, which is smaller than a quarter.
Imagine the swarms we can build with the incredible shrinking controller to the future.
We've come a long way since Shaky's room size controller.
Now another great benefit of Watson style cognitive computing is that a robot can estimate
when it doesn't know enough about the situation to make a decision.
Sometimes it's better not to guess.
First humans knowing when not to guess is often the difference between being a novice
and being an expert.
Robot Watson could be good at knowing when to call in other robots.
In fact, because the various approaches to robotics all share the same goal of autonomous
behavior, each approach, each discipline has something to offer the other approaches within
robotics.
The result is that we have solutions, principles, and concepts that can serve as modules that
we can mix and match.
Take Swarmanoid.
Now this is a distributed robotic system originally developed in 2006 to 2010, and it literally
combined and recombined autonomous robot modules to accomplish as a group different tasks.
Three types of modules made up Swarmanoid, a sensor robot that can also fly, explore,
and hang overhead, providing environmental information to the other robots.
A manipulator robot with two arms that can grasp and climb, and a locomotion robot that
can drive on the floor, physically link with the other robots, and transport the non-locomoting
manipulator robots.
Given a specific task like find and retrieve an object, the modular robots of Swarmanoid
work together, hands and wheels, wheels and flying eyes.
In the sense of robots explore and locate the objects, although locomotion robots position
themselves to form a communications network.
So once a sensor robot has found the object, it attaches to the ceiling, and then it signals
its location to all the other robots in the Swarmanoid.
Two locomotion robots then find and transport a manipulator robot to the general area of
the object, and if located up high, the manipulator climbs hand over hand up to that object, using
a repelling line that has anchored on the ceiling for support.
Once the object is retrieved, the manipulator repels down and is transported back by the
locomotion robots.
When you have modules, you can reconfigure their physical connections and reassign their
tasks.
One of the challenges of a swarm is that specific jobs have to be assigned, and even swapped
dynamically as the situation develops and not ahead of time.
When this problem is solved as it is in Swarmanoid, that whole system is able to adjust as the
task unfolds, that circumstances change, or that some robots fail, and this is the power
of modules.
Imagine giving Roomba a module to handle extreme conditions, or how about a Roomba module for
swarm robotics, allowing a team of Roombas to coordinate how they clean a very large
space?
Or how about a social robotics module for Roomba, making it more interactive and thereby
easier and more fun to take care of?
Modules and modular robots expand the limits of what we think of as a robot.
And where do we get crazy ideas like modular robots?
How do we think of new kinds of bodies, sensors, controllers, and actuators, or new modes of
behavior and cognition, action and perception?
This shows that roboticists are inspired in many different ways, but two paths to the
future stand out.
The first source of inspiration, as we've seen in everything from behavior-based robotics
to biohybrids, is nature, especially biology.
The bio-inspired future is a wet one.
This future involves an even more fundamental shift away from the classic approaches of
mechanical and electrical engineering to the bio-inspired approaches of tissue engineering
and synthetic biology that are helping us to create biohybrid systems.
Classically, we build robots out of dry materials, such as metal, rubber, and plastic.
But when we look at animals, they build themselves out of wet materials, such as muscle and bone.
A biohybrid system contains materials of both types in order to get the best of both worlds.
One of the first biohybrid robots is medusoid, a robotic jellyfish created by Jenna Naroth
and her colleagues at Caltech and Harvard.
Medusoid's body is built out of silicon rubber, and on top of that body, Naroth deposits proteins.
She uses those proteins as a scaffold upon which to grow cardiac cells which have been
harvested from a rat.
Now after those cells grow and connect to each other, they then take the medusoid and
put it into a tank and pulse electric charges in the water.
The cardiac cells respond by contracting, just like they would in a heart stimulated
by an emergency defibrillator.
Now because of the pattern in which they're arranged and the protein upon which the cells
grow, the contractions actuate the body in a way that allow the whole robotic jellyfish
to swim.
So medusoid is a great, simple proof of concept that we can begin to grow parts of robots.
Since cells are wet and need to be kept wet in order to function, the key to the growth
of wet robotics will be either to work in a wet medium or to build bodies that contain
wet medium inside.
And we can also do things like use real muscles to move robots.
Now why go to all this bother?
The promise of biohybrid systems like medusoid is that with growth of tissue, like muscle,
we expand our toolbox of solutions for robots.
For example, animals are great at building extracellular tissue, like tendon and ligament.
And those extracellular tissues have mechanical properties that are tuned to the motion and
load of the system in which they're operating.
In addition, a system that can grow itself should also be able to heal itself.
Wouldn't that be a terrific ability for any robot on a remote mission?
Break down, heal thyself.
A final path to the future, as the best writers and movies make clear, is art.
Now, as a society, we sometimes get into a kind of team competition, science versus
art.
But some of the greatest scientists were on both teams, like Leonardo da Vinci, who
was also arguably the very first roboticist.
Moreover, the teams of art and science do different and complementary things.
Art can imagine the impossible, the highly improbable, but art can also offer scenarios
for what is possible or even likely.
Consider science fiction, for example, as prototyping the future in our imaginations.
I remember Kit, the autonomous talking car in the TV series, Night Rider.
That was 1982.
And guess what?
Now we have autonomous cars.
What I find particularly exciting, which I hope you have come to find exciting as well,
is that of the many present paths and future visions that we've explored, none are mutually
exclusive.
You can think of these paths as complementary, methodological modules.
With this in mind, I predict we'll see a medley of these modules push the limits of
robotics.
I expect that we'll see a generalist robot built from autonomous specialist modules continually
reconfiguring itself as the job changes, the world changes, or as individual modules
are deleted or added.
What about a biohybrid robot that guides the growth and healing of its organic components
with feedback from its probabilistic cognitive pattern predictor about what near future scenarios
are likely for it to encounter?
What about those that could travel into deep space, manufacture variable offspring, test
those offspring in terms of performance in novel environments, and then select the best
ones for the job at hand?
How about robotic replacement systems for humans and animals that we not only control
directly through our neuromuscular signals, but that also can develop, grow, and change
as we learn better how to use them?
Or how about transportation robots combined with energy harvesting robots to find new
ways to provide low-cost transportation of people and goods?
Or humanoid robots with non-humanoid components?
Surgical robots that work autonomously inside of our bodies with constant feedback from and
to the robotic cloud for learning and guidance about the procedure at hand?
Or companion robots that read our emotions, understand our medical history, continually
consult the latest medical research, and work with us and other robots in our lives to coordinate
activities, diet, and medications that will help us prevent disease and prolong the satisfying
portion of our lives?
Part of what's exciting about this list is it is undoubtedly incomplete.
The moment we encounter new problems and challenges will be the moment that we see the necessity
to create new kinds of robots to help us respond.
As we've learned, autonomous robots are goal-directed machines, and they move themselves and other
objects.
Those movements are created in concert with the sensory patterns that that mobility creates.
This allows robots to work, to build, to farm, communicate, heal, entertain, explore,
and transport.
And using the principles of robotics, we learn to understand and design the robots of our
many futures.
Robots are cool.
This is Exciting Robot Rule.
