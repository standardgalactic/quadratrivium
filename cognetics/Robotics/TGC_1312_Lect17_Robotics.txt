Space is the final frontier and robots are the vanguard of our
human explorations of the universe. So what's it like to be a robot in space?
Well, you have to contend with micro or zero gravity, you have a lack of oxygen
in water, you have the vacuum of space and the very different atmospheres of
other planets, you're exposed to extremes of cold and heat, and you have to bring
with you a completely autonomous supply of energy. You have communication delays
and limits. Now these same challenges face humans in space. By sending robots
before we go, we learn how to overcome these limits before we put our lives on
the line. Robots carry sensors and instruments that allow us to understand
where human missions might land, where we could mine water or other chemical
compounds, and make preparations for deep space forays. But perhaps the biggest
challenge in space robotics is distance. If a robot breaks, we can't just go
fix it. To increase the chances of success, robotic systems need to be
hardened in an engineering sense, to withstand those extremes, and made robust
to avoid catastrophic failure. With space missions, we push the extreme of working
remotely with and through our robots. Robots in space use many of the standard
robotic solutions that we've developed for robots here on Earth. The rockets that
push our payloads out of Earth's gravity well have inertial guidance systems
akin to the automatic pilots we've seen in robotic planes. The probes that chart
the moons of Saturn shoot pictures in a remote control dance with humans at
mission control, like that that guides our unmanned aerial systems here on Earth.
And the rovers that scour Mars for signs of life use navigation systems like
those we use on wave gliding robots in the Pacific Ocean. Nearly all
transportation in space is carried out by robots. Robots in space include rockets,
spacecraft, shuttles, satellites, probes, landers, and rovers. On board shuttles or
space stations we also find manipulator robots and humanoids. Much of the work
in space robotics has focused on low Earth orbit and the surfaces of other
planets, so we'll focus on those things too. Now the most exciting project in
orbital robotics is the International Space Station. Hello microgravity.
Here's where humanoid robots are being developed to help humans work inside the
space station. Robonaut 2 or R2 for short arrived on the International Space
Station in 2011. R2 is shown in this picture with NASA astronaut Dan Burbank.
Now like Baxter, R2 is built to interact safely with humans. It consists of a
head and a torso with two arms that function independently. Robonaut 2 can
be controlled by a human operator in the space station who wears a glove and a
headset or it can be remotely controlled by ground control and R2 can also be
programmed to do some tasks autonomously. Now many of its low-level operations
like grasping control are done autonomously even if for example the
object to be grasped is determined by its human operator. How can Robonaut 2
help humans and the challenging work conditions of microgravity? Why bother
to have a humanoid in space? Well inside the space station R2 could set up an
area for the manipulations that only humans can do. Help set up an operation
and during an operation R2 could assist by providing tools or holding something
in place and then it could clean up afterwards. Now NASA plans for Robonaut
2 to work on the outside of the station as well where conditions are extremely
harsh. At the Earth's distance from the Sun like the International Space Station
temperatures in sunlight can rise to over 120 degrees Celsius or 248 degrees
Fahrenheit well past boiling for water. But on the side of the craft not facing
the Sun heat radiates away so quickly that temperatures on the dark side can
sink to negative 100 degrees Celsius or negative 148 degrees Fahrenheit well
below freezing. In space no one can hear your equipment freeze or boil. Since it
takes about 90 minutes for the space station to orbit the Earth if R2 were
working outside for a few hours it could easily encounter those extreme
temperatures. Without thermal control R2 would likely suffer a number of
problems including camera and battery male functions. Robonaut 2 would need to
be outfitted with an array of passive thermal control such as insulation and
external coatings and active thermal control systems such as electrically
powered heaters and coolers. Once thermally hardened for work on space walks
R2 will be able to replace humans on these dangerous external tasks. One of
the reasons to design R2 as a humanoid is so that it can use the same tools,
handholds, hatchways and equipment as humans do. And this is why each hand of
R2 has 12 degrees of freedom. Amazing hand dexterity. Robonaut 2 can use that
manual dexterity to do things like pick up a space blanket. Robonaut 2 can also
use its dexterity to communicate to sign in American sign language. Part of
Robonaut 2's great ability to reach and grasp and interact safely comes from the
touch sensors that it has on its hands. These sensors provide R2 with feedback
so that it can hold on to very fragile objects or to apply stronger force when
needed. Robonaut 2 is also strong. It can lift a 20 pound weight and can do
things that humans can't like hold that 20 pound weight with an arm outstretched
for long periods of time. Robonaut 2 has been designed to be compliant like
Baxter. It knows when it has encountered an unexpected object and will shut down
immediately. This makes R2 safe to work with. Robonaut 2 also has a vision
system. It can pick up an envelope and examine it. And this function involves
rudimentary object identification. Even though Robonaut 2 only works inside
the space station at the moment, other robots are at work outside. They are all
versions of arm-like robots that remind me a lot of the pick-and-place robots
that we see at work in factories. The first of these outside outer space
robotic arms was the so-called Canada Arm. Its real name is the Shuttle Remote
Manipulator System. Built for the space shuttles by the Canadian Space Agency,
the remote manipulator system was teleoperated by crew members through a
computer interface. The remote manipulator system turned out to be a real
workhorse for the whole shuttle program. In 1997, the Space Shuttle Discovery
serviced a Hubble telescope. And this involved picking the telescope from
its orbit and then placing it into the payload bay of the shuttle. Once the
telescope was safely in that payload bay, astronauts could spacewalk to install
new instruments. What NASA quickly figured out is that they could use the
remote manipulator system to actually move astronauts around on spacewalks. I
think maybe if you're going to do that, you should call them space reaches. Now,
after the Columbia Shuttle disaster in 2003, the remote manipulator system was
mounted with an extra long bone, if you will, a long boom that used an extension
that allowed it to hold the camera underneath the shuttle to view the heat
shielding for damage. The remote manipulator system proved to be incredibly
useful and it spawned the creation of Canada Arm 2 for use on the
International Space Station. What's really cool about Canada Arm 2 is that it
assembled the International Space Station. See why we need robots in space? So,
how does Canada Arm move around? First of all, the Canada Arm sits on the mobile
bay system, which allows Canada Arm 2 to move around on a track, giving the whole
system a translational degree of freedom. A second way that Canada Arm 2 moves
around is end over end. It can grab onto a fixture that provides power and
computer control and then it unhooks the end that used to be the base. So, Canada
Arm 2 does cartwheels. Acrobatics like that are facilitated by the microgravity
of a low Earth orbit. The effect of constant free fall in orbit is that
objects are nearly weightless. So, the Canada Arm 2 can move its mass around
with much smaller motors than it would need for the same genetic gymnastic
routine on Earth. Canada Arm 2 also has more automated behavior modules than
Canada Arm 1 did. Canada Arm 2 has automatic collision avoidance, automatic
vision feature for grabbing free-flying payloads, and force movement sensors that
provide it with touch-like sense. Having built the International Space Station,
one of the main jobs of Canada Arm 2 is to grab, dock, and unload unmanned
spacecraft that deliver vital supplies. Canada Arm 2 has the ability to perform
finer scale manipulations by adding onto its end a two-armed robot called Dexter.
Dexter is loaded with many degrees of freedom, allowing it to swivel, pitch, yaw,
and grasp. Now, what's really neat about this combination in robotics terms is that
Canada Arm 2, which is a robotic manipulator, can use as an end-effector
another robotic manipulator, Dexter. So, hanging onto and moving around another
larger robot is made possible because of the near weightlessness of being in orbit.
Dexter is used, along with Canada Arm 2, to change batteries, replace cameras.
These are jobs formerly done by astronauts in long-tiring and dangerous space walks.
Dexter is built specifically to work on the International Space Station.
Dexter itself has a toolkit from which you can select different end-effectors,
and different graspers are built for different jobs.
Now, one important job is to replace failed circuit breakers that are part of
the station's electrical system.
Dexter can manipulate the circuit breaker box, remove the failed circuit breakers,
and replace them. Then Dexter can put its end-effectors back in its toolbox.
Robonaut 2, Canada Arm 1, Canada Arm 2, and Dexter are all designed to help humans
grasp and manipulate the world. Whether located on a shuttle, crawling around
the outside of the space station, or hanging out inside, they all operate as
nearby extensions of our own arms and hands.
These robots are all examples of work in orbital robotics.
The other major field of space robotics is that of interplanetary robotics,
whose stars are the planetary rovers.
Unlike robots in orbit around the Earth, robots on other planets are separated
from us by both long distances and long lag times communications.
But the robotics challenge is not just about how to communicate with your robots,
but how to control them over long distances, for the purposes of virtual
presence and virtual agency.
Presence and agency are essential for us as scientists and explorers, and for
being able to adjust the original mission as new information reveals faulty
assumptions, or the robots have problems, or we have an increased
understanding of what the planet is providing in terms of challenges and
opportunities. And so we create new tasks and goals.
Twin robot rovers, Spirit and Opportunity, were built to study the complex
geology of Mars. Their primary goal was to understand how the activity of water
on Mars influenced the planet's environment over time.
Thus, the Mars exploration rovers are part of NASA's long-term mission to
follow the water and achieve four scientific goals that extend across
multiple missions and into the future. The first goal is to determine if life
ever arose on Mars. The second goal characterized the climate on Mars.
Goal number three characterized the geology of Mars and finally, prepare
for humans to travel to and explore Mars. Now the Mars exploration rovers are a
watershed moment in space robotics. The incredible success of this mission has
set a new standard in space robotics for planetary science and exploration.
Because of their scientific tasks, Spirit and Opportunity are often
characterized as robot geologists. Four teams of humans on Earth guided the
geological work which falls into four categories, mineralogy and geochemistry,
soils and rocks, geology and atmosphere. Early on in the mission, these four
scientific teams met daily to discuss the latest information from the robots and
then create a new plan for the following day. Here's what's really cool, those
plans were then uploaded via radio to each rover. The rovers would then
autonomously enact the plans and then report back. And what's really nifty
about this process is that scientists at Mission Control were reprogramming the
autonomous robotic rovers every day. Now contrast this with real-time remote
control where instead of a plan enacted in computer code, scientists have the
plan in their heads and they directly control direction, speed and any
instruments by inputting commands manually. Now here's the problem with
real-time remote control for robots on other planets. Delay. Radio waves,
laser pulses, however you want to communicate, can only travel at 186,000
miles per second, around 11 million miles per minute. That's the speed of light.
So any command you send to Mars takes a minimum of 10 minutes to reach the
rover, plus 10 more minutes for the rover to respond. And that's the good news.
10 minutes one way is when Mars and Earth are only 110 million miles apart. The
bad news is that that delay can almost double to 18 minutes when the distance
between the two planets grows to 200 million miles. Because of these
communication delays, NASA decided early on in its planetary exploration programs
in the 1960s to avoid any attempts at real-time remote control.
Now autonomy for planetary robotics comes in two flavors. One type is fixed
autonomy, where the programming that controls the interactions of sensors
and actuators is fixed. It's not reprogrammable. The other type NASA calls
batch processing, but I think it's better described as reprogrammable autonomy.
So reprogrammable autonomy is the type employed in spirit, opportunity, and
curiosity rovers on Mars. The idea here is the one we just talked about.
Every day a new program, a new plan is uploaded to each rover.
Now the trade-off with reprogrammable autonomy is one of adaptability versus
time. If you can reprogram your robot every day, then every day it can do
something new. It can adapt to the information it discovers and any
unforeseen changes in how it's operating. However, it takes time to reprogram
to evaluate what to do next and to communicate those plans. So robots with
reprogrammable autonomy compared to those with fixed autonomy may need more
time to accomplish a given task or mission. Better understand this trade-off,
we need to look at the other challenges with communication. Not only are
there delays that make remote control difficult, but the windows for
communication are limited. Time intercedes here from a different angle.
First of all, the two orbiters, Mars Odyssey and Mars Global Surveyor, relay
the signal from Earth to the rovers, but they only have communication for
8 minutes with each rover per Mars day. In that time, about 8 megabytes of data
can be transmitted to an orbiter. 8 megabytes, by the way, is about the
size of a high-resolution image that you might ordinarily send as an
attachment on an email. That sample of 8 megabytes of data then takes
between one and a half and five hours for the orbiters to send back to Earth,
which receives the signals on the deep space network of antennae.
Another part of the communications breakdown is that the rovers only have
enough battery power to transmit for three hours a day. Moreover, even if
they could broadcast continuously, the lines back to Earth would be busy.
The deep space network has multiple clients, namely all of the spacecraft
that NASA has flying outside of Earth orbit. Whew! So, in summary then,
interplanetary communications, which happen in both directions to and from
Mars to and from the robotic rovers, are delayed, are of short duration and have
to be of small size. When we speak of communication, we learn the importance
of the robot's energy supply, which powers the radio transmitter and the
receiver. Spirit and Opportunity have two rechargeable lithium ion batteries
and a solar array to charge those batteries. When under full sunlight on
Mars, the solar array generates 140 watts at 14 to 18 volts for up to four
hours. Driving the rover takes 100 watts of power, so travel can be
accomplished in sunlight using the solar panels. Six wheels, each with their
own motor, are powered, powered the forward motion. Top speeds of about
five centimeters or two inches per second, about one-tenth of a mile per hour,
can be reached. Space turtles, watch out. Now, the battery system has a capacity
of 15 amp hours at 12 to 16 volts and is used for communications and nighttime
operations. Batteries can also be used to heat the temperature-sensitive
electronics during the cold nights and the Martian winter, but the primary
heating is provided by eight radioisotope heating units, which by radioactive
decay continuously generate one watt of heat. All of the sensors and instruments
on the rovers also require energy, so their use has to be carefully planned
and scheduled. A host of cameras are mounted on the mechanoid body of Spirit
and Opportunity. On top of the masks, it's a dual camera system for 3D panoramic
shots of the landscape. They rotate 360 degrees and swivel up and down. Located
on the arm of the rover is a microscopic camera that can take close-up shots of
surface materials. And six other cameras use power and are used for navigation.
Four wide-angle cameras are located on the body of the rover and are used to
detect navigational hazards like cliffs and large rocks. Two navigational cameras
are mounted on the mast alongside the panoramic cameras and are used in path
planning. So these eight cameras are the primary sensors for giving scientists
on Earth virtual presence through the rover. So vision is a very important
means of perception for geologists. Layers of rock indicate sedimentary action
usually caused by wind or water. Valleys can indicate erosion or glacial movement
and colors of rock or sediment can give clues to mineral composition.
Now during the first four years on Mars, the rovers each average 25,000 pictures
per year. Spirit and Opportunity have created an incredible catalog of images
that really do make you feel like you are there, present on Mars.
Three scientific sensor systems, all of which are spectrometer instruments,
look for water, iron and chemical composition. A spectrometer measures the
properties of light that are emitted or reflected or transmitted by objects or
reactions. How that light changes as it interacts with the material tells us
something about the composition of that material. Looking for signs of water
and located at the bottom of the mast is the miniature thermal emission
spectrometer, which analyzes the infrared light reflected off of objects.
This instrument looks for carbonates and clays and minerals formed by water.
Now looking for iron bearing minerals, the Mossbauer spectrometer is located
at the end of the rovers arm so that the head of the sensor can be placed
directly on the soil or the rock. Analyzing the composition of elements,
there's the alpha particle x-ray spectrometer and that's an active sensor.
It emits alpha particles that reflect off materials. The alpha particles
create x-rays as a byproduct of their interaction with the material.
The patterns of the returning alpha particles then indicate the elements
that make up the material. Like the sensor head of the Mossbauer spectrometer,
the sensor head of the alpha particle x-ray spectrometer is located at the
distal end of the rovers arm. Now the main electronics of both the
sensor systems are located in what's called the warm electronics box,
the heated main body of the rover. A fourth instrument is on the robotic arm
and hand and it's an actuator, an end effector called the rock abrasion tool.
This tool grinds and abrades the surface, creating a shallow hole about
two inches in diameter. Now getting below the surface, any hard rock allows
for analysis by the other instruments on the hand. Action enhances perception.
Now what makes it possible for these instruments to do their jobs is the arm
or what NASA calls the instrument deployment device. The rover's arm has
five degrees of freedom. The joints of the shoulder, elbow and wrist are all
revolute joints that rotate. The shoulder has two joints oriented perpendicular
to each other. The elbow has a single joint and the wrist has two joints.
As the wrist rotates, it brings different instruments on the hand to bear on the
soil. The Mars exploration rovers have met and exceeded NASA's expectations.
Originally designed for a mission of 90 days, Spirit sent its last communication
in 2010, six years after it began operations and traveled nearly five miles.
Opportunity operated for over ten years and logged 25 miles. Scientific achievements
of the mission have been many. Spirit took movies of swirling dust devils, found
circumstantial evidence of water by detecting sulfate minerals and found
silica deposits that indicate a much wetter past. Opportunity found evidence
for water over a wider range of training indicating a past with large bodies
of standing water. Now opportunity landed in a dry lake bed so its first major
discovery was of the mineral hematite which typically forms in water. That was
followed by finding brightly colored gypsum created in veins likely by flowing
water. Now this is just awesome. Robots as our mobile reprogrammable science
laboratories. I can't help but it's like nerd goose bumps, right? This is seriously
cool stuff. Spirit and Opportunity are two of my all-time favorite robots. In
spite of this excellent work and amazing robustness, things did go wrong and those
failures are informative about the challenges of facing robots in space. One
of the biggest problems encountered early on was that the solar panels got dusty,
really dusty. Remember there's no surface water so the dust is extraordinarily dry.
Static electricity builds up worse than on a cold winter day and all that
electrostatic dust is attracted to the electronics of the rover. Now the team
anticipated this problem in the design of the mission and they were right that it
would be a problem. Dirty solar panels resulted in a reduction of by 50% in the
power supply. This is the kind of problem that only gets worse, right? The dust and
dirt continue to collect. In 2013 a Chinese robotic rover U2 sent to the moon
ceased to move two months after arriving. While it's not completely known why U2's
stopped working, the China National Space Administration concluded the problem was
likely electrical and likely caused by cold damage in the long lunar night.
Remember that a single night on the moon lasts two weeks and U2 failed to restore
movement after its second lunar night. In the face of environmental challenges
like dirt and cold, a reprogrammable rover offers the possibility of at least
trying to come up with a solution. What the mission engineers for Spirit and
Ignite is that the winds on Mars could be used for cleaning the solar panels.
For example, over a one month period in 2009, Spirit solar panels increased their
power supply by over 60%. Cameras on the mast of the rovers document the cleaning.
These cleaning events are one of the major factors in the extension of the
mission time well beyond the original 90 days. The fact that the surface of Mars
and the moon are dry and dusty will continue to be a problem for robots.
In fact, Spirit and Opportunity went to Mars with a magnetic array to analyze the dust
they were accumulating. The magnets attract charged particles and those particles
can then be analyzed by microscope and spectrometers on the rovers arm.
For the Curiosity rover, which landed on Mars in 2012, engineers solved the dirt
problem by removing solar panels altogether. For energy, they installed a
radioisotope thermoelectric generator whose power comes from the steady decay
of 11 pounds of plutonium 238. This nuclear power source was designed to last
for at least 14 years and produced four times the amount of power produced by
Spirit and Opportunity's solar panels and you don't have to worry about the dust
and the cleaning. Another problem that Spirit literally ran into was getting
mired in loose sandy sediments on the edge of a small crater.
Spirit's final resting place is where it got stuck at Troy in 2009.
Getting stuck, however, permitted a wonderful scientific accident to occur.
When engineers were attempting to get Spirit to drive out of the sediments,
the wheels spun through material and churned up the surface.
Doing much more than digging the rock that the abrasion tool could ever do,
those wheels ended up being digging actuators and they uncovered sulfates.
Now these minerals are formed in steam vents and steam is usually associated with water.
So getting stuck created a new perception action link.
The act of trying to drive turned into digging. That digging allowed new perceptions
of buried materials. So the sad accident of Spirit getting stuck
has actually resulted in one of its most important discoveries.
Both Spirit and Opportunity had problems with their drive wheels,
with one of the six getting stuck at an odd angle.
So the adaptation was to drive the rovers backwards.
Fortunately the two wheels on the front and the two on the back can steer.
What ultimately led to the demise of Spirit was power.
Stuck in the sand, Spirit couldn't be maneuvered to angle its solar panels in such a way
as to get enough light during the Martian winter to continue to charge its batteries
to allow for communications.
This movement problem was coupled with the gradual degradation of the performance of the batteries.
Now all rechargeable batteries slowly change,
and even with a backup battery like Spirit had, you eventually have to pay the piper.
Opportunity was able to operate for years longer because it was able to maintain its solar power.
Towards the end of its life, Opportunity's flash memory began to wear out.
The flash memory allows data to be stored when the rover is powered down.
As we all know using our computers here on Earth,
sometimes memory or mother boards or screens just stop working.
Well, perhaps in the future a robot like Dexter will be on hand to repair
or replace components when they break.
As we celebrate the remarkable careers of Spirit and Opportunity,
it's worth mentioning Lunacod 1, the first robotic rover which landed on the moon back in 1970.
Lunacod built by the Soviet Union was the first mobile robot to explore an extraterrestrial body,
and it returned hundreds of pictures and many tens of data points of soil samples.
Given its success, Lunacod's design was carefully studied by the United States in 1971,
and we can see many of Lunacod's design features in Spirit and Opportunity decades later.
Multiple wheels, scientific instruments, and a radioactive source for heating electronics during the night.
Lunacod differed from Spirit and Opportunity in two important ways.
First, its autonomous capabilities were fixed, not reprogrammable.
Chief among those fixed capabilities were the operations of the scientific instruments.
Second, Lunacod did not drive itself autonomously, but rather relied on a human pilot working via remote control.
As we place our robots deeper into space and farther away from our direct control,
we will actually need them to be much more autonomous than they are now.
And while we've seen that Spirit and Opportunity can be reprogrammed as their mission evolves,
there's a limit to how much reprogramming can be done, and the delays only increase as our robots go deeper into space.
So a likely alternative is to have the robots reprogram themselves.
We call this process machine learning, and it's a field that's well developed in the highly structured environments where many robots work on Earth.
So the challenge for robotics is to build learning systems for the highly unstructured environment of space.
Soon I expect to see our robots learn as they go, fix problems they encounter themselves,
and further extend our abilities to explore and work throughout our solar system.
