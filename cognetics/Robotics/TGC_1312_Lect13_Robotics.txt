You might suppose that robots working in a single workplace would be of similar design,
right?
Well, most of the personal service robots in our homes, for example, are small, mobile,
autonomous, whether moving about on floors or on our lawns.
But the service robots working in hospitals and healthcare are actually about as different
as robots can be.
Their diverse designs and functions reflect the complexity of the modern medical workplace.
Robots and hospitals work in at least three very different ways.
The first way is the busy social world of bustling humans in the entrance hall, waiting
areas and patient wards and rooms.
The second workplace is the carefully controlled world of the surgical theater.
The third workplace is the intimate world of prosthetics and exoskeletons used and
worn by an individual.
So one thing that's remarkable about service robots in the hospital is how long they've
been deployed there.
So let's start with the social world of a busy hospital.
In January of 1991, a helpmate robotic courier was installed in the Danbury Hospital in Danbury,
Connecticut by Transitions Research Corporation.
This was a company founded by Joseph Engelberger after he sold Unimate to Westinghouse in 1984.
Helpmate was the first hospital courier robot designed to transport items and information.
Now with funding from NASA, Transitions Research Corporation wanted to tackle the problem of
delivering material in the relatively structured world of a hospital.
So in addition to using a model-based navigation system for hallways and elevators, they needed
Helpmate to be able to detect objects and avoid them.
To navigate, Helpmate used a map-based control system and a variety of sensors, including
lidar, to avoid people and other moving objects.
Now Helpmate got around the difficult problem of using a manipulator to open doors by sending
a radio signal to each door to open itself.
Brilliant, right?
Elevators were also modified to respond to that radio control by the robot.
So this is really a great example of redesigning your workplace to work with and not against
your robots.
As a courier, Helpmate could deliver late meals and special dietary foods.
It could replenish supplies to nursing stations or return samples collected by the nurses
to the laboratory for analysis.
Alongside with a locked cabinet, Helpmate could also securely move and deliver medications
from the pharmacy, along with patient records stored in its computers.
Up to 200 pounds of mail and packages can be delivered during a single trip.
Helpmate used lights to signal its intent to humans, move through the door, and then
signal to turn.
In addition, Helpmate pioneered the use of three types of sensors to safely interact
with people and things, sonar, video, and bump sensors.
Working with sonar was accomplished with 28 transducers on the robot.
That's a lot, right?
But these transducers send out an ultrasonic signal and measure the time for that sound
to return and a short time of flight signals that an object is close by.
So it's very important in this situation not to run into anything.
So robot vision has historically been very difficult to implement in part because there's
so much information to process in the visual modality.
The team at Transitions Research Corporation, working on Helpmate in the 1980s, employed
two very different and creative engineering solutions to the robot vision problem.
First, they drastically reduced the amount of information they were trying to get from
the camera.
Instead of trying to detect and classify any and all objects, they only sought to detect
the ones right in front of the robot.
With detection alone as the primary goal, they could secondarily calculate the distance
to the object and its size.
They did this in a really cool way, using a system they called structured light, and
it worked like this.
Mounted on the robot down low were two strobe lights that had light projectors that send
out two thin sheets of light parallel to the floor.
The contrast of the camera was adjusted so that the picture appeared blank to the robot
unless some object was illuminated by those sheets.
Because the light in the sheets, when the light hits an object, illuminates that object
with a horizontal line.
That's what it was detecting.
So looking for the presence of a line solved that detection problem.
Now what about distance and size?
What the robot, roboticists realized was that they could use the vertical distance from
the bottom of the screen to the first line to estimate the distance.
All they had to do was fix the position of the camera.
It's tilt and it's focal length and hold those things constant, or have the onboard
computer calculate adjustments as the robot moves.
In this system, objects that are far away will create a horizontal line that is higher
on the screen.
Now you can get a sense for this by walking forward with your head tilted down at about
45 degrees, right about like this.
If I walk towards an object, like what's in front of me here, a table, that object will
appear low down on my visual system.
Something that's up higher and farther away is at the top of my visual field.
So as I move closer, the closest edge of the bench moves down my visual field.
Distant objects are up and close objects are down.
And once you know the distance to the object, you can use the horizontal length of the line
of light illuminating the object to estimate the size.
Bump sensors provided a third way to detect objects, the now classic way pioneered in
a service setting by Helpmate, of letting contact with an object depress a simple electrical
switch.
Now at first it might seem strange why three different sensor systems delivering information
about objects.
The answer is one big word, disambiguation.
That is a single sensor system can give you an ambiguous answer.
Is there an object?
Yes, no, we're not sure.
For example, if you relied on the vision system alone, you might only detect objects directly
in front of you and nothing overhanging like a cantilevered countertop.
In that case, the 28 sonar sensors all sending out sound signals would come in very handy.
To improve your ability to get the truth about the world, to disambiguate, you combine information
from the redundant sensor systems in a process that's called sensor fusion.
Now sensor fusion requires that the robot be programmed to go beyond simple reflexive
reactions and make decision about when to trust one signal and when to infer the most
likely situation using multiple sources of sensory information.
Helpmate's solution to the problem of navigational drift, another problem out there, was to use
little errors that add up over time, that's the drift, was to get a navigational fix.
And they did this by, I love this technique, putting retroreflective tape on the ceiling
at fixed intervals.
So Helpmate could sense those bands of tape using a pair of long range infrared sensors
that continuously emitted IR light upwards and then marked the peaked intensities of
the IR light reflected off of those bands.
One of the challenges that the system did not initially anticipate, I don't know if
you would have either, was the strange and irregular behavior of people.
For instance, humans around Helpmate quickly learned that they could get Helpmate to dance
by triggering its object avoidance system shifting from side to side.
So the solution was counterintuitive and simple, just slow down Helpmate's avoidance responses.
I don't want to dance, no I'm not going to dance with you.
So in addition, people behave differently in different parts and at different times in
the hospital.
Under the cafeteria, for example, the normally empty service hallway would suddenly fill
up at 11.30 a.m. with crowds of very hungry humans.
So Helpmate would initially get overwhelmed.
Context cues were added to Helpmate's control system, providing rules for how to adjust its
behavior depending on location and time.
Helpmate ceased production in 2006, but its success as an intelligent mobile courier has
spawned a variety of hospitals in the halls, variety of robots, excuse me, in the halls
of hospitals, cleaning robots, disinfection robots, robots that can check the status of
defibrillators and fire extinguishers.
One of the trade-offs for courier robots like Helpmate is maneuverability versus carrying
capacity.
A very maneuverable courier robot is Swiss Logs Robo Courier, which can turn in place
like a Roomba with a turning radius of zero, but it has a maximum payload of just 66 pounds.
By contrast, Atheon produced a hospital delivery robot capable of carrying 500 or even 1,000
pounds of supplies.
These robots are called tug robots and the name fits.
The slim robot up front pulls along a large storage cabinet behind, and instead of needing
tape on the ceilings or other markers, Atheon chose to create pre-planned travel routes
for the robots, either using CAD drawings in the building, upload it onto the robot,
or lasers to manually guide the robot through pre-planned travel routes in the facilities.
Now the tug robots avoid drift, that navigational error adding, by taking their navigational
fix from charging stations.
The company developed software that is said to open any public elevator in the world.
But because the tug is less maneuverable, it may get stuck.
In that case, one thing the tug robot can do is to ask nearby humans to take specific
steps to help it out.
Moreover, thanks to increasing computing power, the company can link the tug robots
with an automated tracking system for materials moving within the hospital.
For example, if you sent a sample to the lab, you can now know exactly where it is in transit.
In fact, the tug robots are also monitored by automatic algorithms from the company's
cloud command center.
And if a problem is detected, a human is contacted who can connect to the robot remotely and
even take remote control if necessary.
As good communication connections continue to improve, there are increasing opportunities
for telepresence robots.
iRobot, the company that made Roomba, teamed up with Cisco to create Ava, a platform for
a mobile service robot capable of wireless telepresence.
So this makes possible a robot that can map and travel a floor but also engage in desktop
quality teleconferencing.
Another company, Intouch Health, has used the Ava platform as the foundation for the
robot called RP Vita.
Intouch envisions a number of jobs for RP Vita.
Vita can be present in a patient's room to help caregivers integrate data from multiple
sources by presenting them at a single console.
In addition, Vita can also collect the data too and serve as a hub for monitoring patients
remotely.
Now, a great social feature is that Vita can allow family to connect with the patient anywhere
that patient might be in the hospital.
Or family, patient, and team of physicians may gather in person and remotely for consults.
So caregivers can call Vita in to assist them in monitoring, aiding, or communication tasks.
And Vita can now navigate to a new place on its own, having a complete world model of
the hospital and the lower level reflexive behaviors to avoid hitting people and things
that aren't on those maps.
One of the really nifty things that Vita can do is find a patient.
Now I know this sounds straightforward, but if you've been in a busy hospital, you know
it's not.
There are times, especially when a hospital is overrun with victims of a large accident,
and patients can be temporarily misplaced.
To identify a patient, Vita uses a combination of face recognition and radio frequency ID
tags.
So in a fluid situation, you could even have a bunch of Vitas keeping track of which patients
were where, what their status was, what was going to happen next, and then help schedule
and guide the hospital staff.
For distraught patients, Vita can also serve the social function of helping them keep track
of times until dosages, procedures, or the arrival of specialists, or even the next meal.
As we move now from mobile robots, working in hallways, wards, and rooms, let's go into
the surgical theater, where we see telepresence in a completely different kind of robot.
For surgeries and patients that qualify for minimally invasive surgery, intuitive surgical
created Da Vinci.
Now Da Vinci is a tele-operated robot that is a variation on the classical robot arm,
also called a robotic manipulator.
The difference is that Da Vinci robotic surgical system moves robotic arms inside the human
body.
The Da Vinci surgical robotic system has four robotic arms that are outside the patient,
and each one of these arms has extensions with sensors and actuators that go inside the
body.
Now, before you get too freaked out about this, keep in mind that there are humans in
the control loop here.
The company's senior director of medical research, Dr. Catherine Moore, makes the case
that Da Vinci is not disruptive technology, saying this is just surgery from the inside
out, an extension of standard minimally invasive surgery.
Let me explain.
During surgery, two human surgeons were exceeded at the consoles.
One surgeon looks into the consoles and puts her hands on the controls.
The robotic arms and the patient are in another part of the operating room, and the patient
is attended directly by another human member of the surgical team who's able to watch and
monitor using a computer console.
Looking into the console, the surgeon sees inside the patient.
Tiny surgical tools, the end effectors on those manipulators, work together to sew stitches,
close an incision, make cuts.
The surgeon controls multiple degrees of freedom of the actuators using her hands on the controls,
and she can manipulate the jaws of the grippers with her thumb and forefinger.
She can slide the actuators back and forth.
And most importantly, for any kind of fine scale surgery, the Da Vinci system scales
her larger motor movements into smaller ones.
So six centimeters of movement by the surgeon is scaled down to just two centimeters inside
the patient.
So that scaling of movement allows the surgeon to move her actuators more accurately compared
to trying to make the same movements directly with her hands fine motor skills, where she
performing conventional laparoscopic surgery, for example.
So the surgeon in minimally invasive surgery is doing so without having to stand up and
operate like this by working through the robot.
She also has to watch a computer monitor in order to know what's happening inside the
patient if she's standing up, right?
So they're like this, standing up doing the conventional surgery, and that turns out to
be physically tiring.
Instead, when the surgeon is actually using the Da Vinci robot, she's sitting down.
She's more telepresent because of the immersive experience of actually having her eyes on
a console and her hands directly here.
So this immersion is primarily visual and three-dimensional, thanks to miniature sensors
that have two parallel cameras that are inside the body.
So here's the point for us.
A surgical robot keeps the surgeon in the control loop.
The surgeon functions as the controller in a remotely controlled robotic system.
And hence, the surgeon senses the world inside the patient through 3D cameras, they're telepresent,
and issues orders to the actuators inside the body.
One way to think of this is that robotic surgery shrinks the hands of the surgeons and now
think of miniature actuators as your fingers.
Not only can you pick things up, but you can also swap out your fingers.
How cool would that be for scissors, clamps, and tissue manipulators?
I can't help but think about the old 1966 movie, The Fantastic Voyage, when we think
about being inside a body and present.
Now you've probably seen this movie, at least on parodies or parodies of it on cartoon shows.
So the premises, a crew of five humans, including two surgeons, are miniaturized and placed
in a mini-sub and injected into a scientist in order to perform life-saving microsurgery.
What we have in science fact with robotic surgery is something functionally similar
to the science fiction of The Fantastic Voyage, a way for the surgeon to do surgery from the
inside out.
That is, for medical robots, the workplace is the human body.
Very cool.
So we've looked at robotic manipulators in the human body, what about mobile robots?
And this leads us to something called a capsule robot.
Capsule robots carry sensors and actuators into the body and are not connected to external
manipulators, and the really cool ones are actually self-propelled.
Capsule robots got their start from a technique called capsule endoscopy.
This capsule can be swallowed and it takes with it into your body a tiny video camera,
a camera working in dark needs light, so these robots bring small LEDs to illuminate
the scene.
They light it up so you can take really cool pictures of your small intestine, for example,
or your stomach lining.
These kind of images can be absolutely crucial for diagnosing problems like ulcers and colon
cancer.
These capsules are passive, though, and not really robotic in the sense that they don't
have actuators that help them move.
But in 1994, a robotic endoscope was patented and built to be able to actively wiggle its
way through your gut, like a worm or a snake.
These actuators would, quote, move the segments together in a part and change their angular
orientation, in other words, bend, making it possible for the robot to move like an
inchworm or a snake inside the patient.
I don't know about you, but I think that's a totally cool design.
In the front of the snake, if you will, carries a camera and other sensors and a little knife-like
device to cut tissue samples.
The ability to snake around literally opens up new areas of exploration.
Now, there is a problem.
The robotic snake endoscope is pretty big, and one danger is that it can perforate or
break through the wall of the intestine, and that would not be good.
So a number of biomedical engineers have been trying to combine the small size of that original
camera with the self-propulsion of the robotic endoscope.
A group at Scuola Superiori Sant'Ana in Italy has come up with a design that really could
be right out of the central casting for an upcoming remake of the fantastic voyage.
You ready?
Self-propelled robotic capsules.
I visited the lab doing this work and spoke to Dr. Gastone Quitti about his project.
Their capsules are just over 20 millimeters in length, in fact nearly identical in size
and shape to the camera capsule that we've used to see in the original endoscopy work
with capsules.
But the big difference is that these newer capsules are self-propelled.
They can have four propellers on the back end, and really this is like a mini-submarine
controlled through a wireless communication system.
The four propelled, four propellared, gut boat is still in early stages of prototyping
and testing by several groups.
Mobile robots are also working with the human body in a more ongoing way collaborating with
us to overcome medical challenges on an everyday basis.
There are robots that help in physical and occupational therapy.
These are so-called therapy robots.
Assistive robots are strapped onto your body to help supplement or retrain your existing
abilities.
The walking assist created by Honda Motor Corporation is a spin-off technology from
their work on the humanoid robot, Osimo.
The walk assist is again a jointed powered robotic limb, in this case a robotic leg,
in principle and in practice, working with the human to help strengthen joints and power
each step.
This is meant for people with certain kinds of muscle or joint problems who can still
walk but can use the actual help of that assist.
And it's being used to help people recover from stroke, to help guide them through physical
therapy and in addition Honda has also developed an exoskeleton that's meant for workers.
The robotic legs for workers have their own sensors, controllers and actuators and a battery.
So this is definitely a robot and it helps support body weights for humans when they
have to do things like stand for long periods or perform bend, crouch or kneel repeatedly
like if you happen to be working on a machine for a long period of time.
And as you crouch then all you have to do is balance, the robotic exoskeleton actually
does the work of supporting your weight.
So the idea here is to help for example auto workers because they have to stoop and crouch
to do their job and since the walk assist with body support is helping workers support
their own weight you really avoid things like back problems and lower limb problems and
so you can think of this as a kind of preventive medicine.
So the trick is to have the Honda leg cooperate with the human and this works because sensors
in the joints of the robot detect the initiation of movement by the human.
The legs then work to make strides even and to make them longer.
You as the patient have the job of cooperating with this robot that you are wearing.
Now an even more impressive area for this kind of cooperation is the field of prosthetics.
Robotic limbs can be very complicated machines that we attach directly to the nerve endings
after amputation.
Let's take the case of a prosthetic hand with five mobile fingers.
Dr. Quiti's colleagues at the School of Superior Santa Ana were demonstrating a robotic hand
when I visited.
Now look at all these degrees of freedom in the fingers.
Each finger has to be powered and controlled separately and they all have to be coordinated.
It's amazingly complex.
Now this is already a small and complicated robotic manipulator but it's even more than
that.
Imagine trying to make an interface that allows that complicated hand to be controlled by an
amputee.
The problem is to build a remote control system that doesn't require the human acting as the
controller to use his hands to move levers, joysticks or push buttons, right?
Enter the field of neuro prosthetics.
The idea here is to tap directly into the human's nervous system for control.
This is done with what amounts to a special antenna called an electrode designed to pick
up and amplify the electrical signals generated by nerves and muscles.
The electrode can be on the surface of the skin or under the skin touching muscle or
even in the body right next to or inside the nervous system.
Once you have that electrode in the right place to detect nerve signals then you have
to train the human how to voluntarily make and control the electrical signals that the
electrode reads.
Now I have to kind of laugh when I hear in the news about devices controlled in this
way because the headline often uses the phrase mind controlled prosthetic arm.
And I laugh because we use our minds all the time to control things like my jaws and my
tongue and my lungs so I can talk to you right now or I use my mind to control my legs so
that I can stand here.
So the idea of mind control is really trying to get it two serious things here.
First, the patient has a prosthetic, a robotic arm or leg and second, the human in the functional
loop here acts as the part of the controller and it's communicating directly to the robotic
arm via his body's own electrical signals instead of his body's muscular forces like
with hands.
That's the basic idea.
So the signals that command, for example, the DECA arm come from electrodes that detect
electrical signals called EMG signals that muscles make near where that robotic arm is
attached.
EMGs, by the way, stands for electromyography.
Now a person who uses the DECA arm is doing something unremarkable for most of us.
For example, drinking from a bottle of water.
But what is remarkable is that the same person has picked up the bottle, raised it to his
mouth and can tip it back and squeeze it even a little bit with a robotic arm that's attached
to his body.
I want to take a minute to appreciate how this is working.
Now a great thing about this robotic arm called the DECA arm is that it has 10 degrees
of freedom.
In this video clip we can watch this person pick up eggs, not break them and then transfer
them to another carton.
Now I don't know if you recognize this but this is a classic pick and place job usually
done by a robotic arm.
The difference here and what makes it really cool is that the control signals are not coming
from a computer but from the electrical signals from this person's muscles.
So those muscles signal, muscle signals come from the nerves and that's why this device
is called neuroprosthetic.
Before we leave the DECA arm, I want to point out that despite having a human in the loop,
the arm really is partially autonomous.
While the human may learn to guide the overall movement of the joints and even the strength
of the grip, the DECA arm has onboard sensors that the human doesn't monitor or feel.
In particular, the arm has movement and force sensors that are needed to help translate the
signals from the human into smooth and slow movements.
So given this level of autonomy for the arm itself, some people like to talk about it
being a smart prosthetic, fair enough, I agree.
I like to think about the human and the robotic arm though as cooperating, right?
Both do some of the work of the controller, both provide actuators and sensors and bodies
for that matter.
By coordinating their activities, they form a single human robot system that can do more
together than either could apart.
Medical robots and other robots working in the hospital show an incredible diversity of
types, from mobile robots carrying materials to robotic manipulators in the hands of surgeons
to prosthetics using neural signals of the human body.
Robots in the hospital do an amazing variety of work with us.
We build medical robots to work with us, and together we genuinely get vital work done.
