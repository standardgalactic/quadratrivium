The complex social behavior that we call social and emotional communication is a field that's
ripe for robotics. Spoken words, expressions, tone of voice and body language can all be
used to create the user interface between a human and a robot. Social signals can communicate
very quickly. And here, let me show you with Baxter. Hello Baxter, what's up? Now I can
see right away with Baxter's eyes that there's a problem. This is his puzzled look and it's
a communication to me that we can't do anything until we figure out what's going on. One raised
eyebrow and the other depressed eyebrow. So I'm just going to hit the navigator button
here. Ah, and that's all I had to do. Baxter just needed my attention and we'll get going
again here. Problem is resolved. These are simple but powerful signals. And thanks to
the sensors and controllers that robots have, they can produce and recognize standard social
and emotional signals to help us interact more effectively. Now of course, movies have
shown us social and companion robots for a long time. In the Star Wars movies, R2D2
famously expresses emotions, understands humans and acts as a trusty companion without ever
saying a word. Social robotics is the field where such work and more comes together in
reality. A social robot, as the name indicates, is an autonomous robot with a very specific
function to interact and communicate with humans and other autonomous physical agents.
But as with Baxter changing his eyebrows, the findings from social robotics can diffuse
much more wildly into other kinds of robots as well. Social robotics is a field that combines
findings from engineering, biology, human psychology, and of course psychology continues
to be a dauntingly complex field. We're complex, right? We don't even know enough about how
humans approach the world and communicate that we can state principles. We know something,
but it's just the beginning of understanding human behavior. We know for sure humans tend
to anthropomorphize. Most owners of a robot vacuum cleaner give that vacuum cleaner a
name. The classic example here is when we describe our own thoughts and emotions to
our pets, that's anthropomorphizing. We do the same thing to each other and we call
that empathy. Here's something else we know about how humans behave. Humans expect a social
agent to be interactive. We expect a puppy to react to us with eye contact, sounds,
or a change in posture. With humans or social robots, if we ask a question, we expect a
response. Third thing that we know here, humans expect a social agent to show initiative.
We expect a puppy to have her own drives and desires, like wanting to play or chew on
our shoes. Same for a social robot. So if we think of these as really basic principles,
they get us started thinking about what robots need to be to be socially competent. There's
a lot. Social robots must be able to perceive and express emotions. They have to understand
and express spoken language, and they have to understand and express the body language
of gaze, expression, gesture, posture, and movement. And they have to establish and maintain
a social relationship, and they must have a personality. While all these skills feed
into social communication, their diversity make it clear that the challenges for social
robotics is more than just about having a spoken or written conversations. That we can
already do with chatterbots, also called chatbots. Computer programs like Joseph Weisenbaum's
famous Eliza of 1966, whose text-based interactions launched the field of artificial conversational
entities. The development of chatbots was provoked by Alan Turing's famous 1950 paper,
introducing what he called the imitation game. Now known as the Turing test, the goal of
the imitation game was to create an operational way to determine if a machine had human level
intelligence. The interrogator's job is to figure out which of two agents it's communicating
with is a computer and which is a human. What makes this a challenge is that the interaction
takes place only via the written word. The interrogator cannot see or hear either agent.
The Turing test has been turned into an annual competition for chatbots called the Loebner
Prize. A single, one-time, $100,000 prize will be awarded if the Turing test itself
is ever passed. Less surprises are awarded each year for the best of the imitating machines.
Well look, social communication in robotics is both easier and harder than these kind
of artificial intelligence tests. Easier because even simple physical cues like eyebrow changes
on Baxter's face turn out to be useful, right? Yet social robotics is also harder because
what works in the controlled space of a chatbot may not work in face-to-face communication.
Social robotics focuses on communication that is embodied, communication between two physical
agents. Therefore the design of the body of a social robot is extremely important. And
in fact, if you take a behavior-based approach, the first thing that a robot's body has to
do is invite the human to interact. Cuteness is an invitation to touch. And Takanori Shibata
realized that when he sought to design a therapeutic robot in the 1990s. He chose the baby Harpseal
as a model with its white fur and big black eyes. In 2001, he revealed Paro as a therapeutic
robot meant to offer the benefits of animal therapy in environments like hospitals and
extended care facilities where having animals is problematic. When you touch Paro, it responds
thanks to tactile sensors by moving its head and body and making sounds. If you talk to
Paro, it has directional microphones that allow it to turn towards you and its controller
has algorithms that allow it to learn words like the name you give it and to recognize
praise. Paro interacts with language even without producing language. Paro also learns
what behavior you prefer in order to initiate interaction. Using stroke as a sign of positive
reinforcement, Paro's controller looks for patterns in its behavior that came before
the stroking. Now this is really cool. Paro will then try to initiate the stroking by
exhibiting those pre-stroking behaviors to you. In addition to tactile and audio sensors,
Paro has light sensors so that it can tell the time of day and alter its behavior accordingly.
It also has posture and temperature sensors to give information about how it's being
held and the nature of its environment. So Paro is a great example of social communication
through body language and non-verbal auditory signals. Research has found that interactions
of Paro robots with humans can reduce patient stress, improve patient motivation and improve
communication between patient and caregiver. Humans also use non-verbal body language in
the form of facial expressions and vocal intonation. These dynamic features of our social communication
are part and parcel to emotional communication. The most important early robot for emotional
communication was Kismet, a robot developed in the late 1990s, about the same time as
Paro. Kismet made use of natural and non-verbal emotional communication that we have with
infants. Kismet's designer was Cynthia Brasile, who at the time was working on our PhD in
Rodney Brooks' lab at MIT, an effort that included funding from DARPA, the Office of
Naval Research, and NTT of Japan. Now what Brasile did was to make use of the natural
and non-verbal emotional communication that we all have with infants. The approach was
revolutionary and it literally set the stage, the infant stage if you will, for humanoid
social robots. She built and programmed Kismet to react to human emotions in a way that signaled
to the human Kismet's own apparent emotional state. Brasile took advantage of that first
principle we mentioned, our human tendency to anthropomorphize. Humans interacting with
Kismet use words and intonation. They treat Kismet like a baby, scolding or praising,
good robot, no don't do that, uh oh. Intonation, stress, rhythm of the speech are all parts
of what linguists call prosody, and Brasile programmed Kismet to respond to five types
of prosody, approval, prohibitions, attention, comfort, and a neutral stance. Now depending
on the type of prosody, Kismet responds by changing its posture, hanging or lifting its
whole head, changing the position of its big ears, the shape of its mouth, and its eyes.
So Kismet is communicating through its postural changes that it has understood the emotional
content of the human. Kismet signals surprise by raising its eyebrows, thinking about Baxter
here, and it raises its ears and opens its eyes and mouth wide. Kismet signals sadness
by tilting its head down, lowering its ears and drooping its eyelids. Now the robotic
manipulators in the Iron Man films, I don't know if you remember, the ones that helped
Tony Stark build the Iron Man suit use gestural communication too, lowering their manipulators
right when they are being yelled at by Tony. Humans interacting with Kismet reported that
they enjoyed the experience and felt some kind of emotional connection. Now what Brazil
quickly realized was that Kismet was allowing her to study humans and what triggers in humans
emotions and empathy. So it's a really great tool she developed. Even though Kismet looks
like just ahead, maybe like an automaton or an animatron if you might have seen one at
an amusement park, Kismet is actually an autonomous robot. It has sensors in the form
of cameras and a microphone. Now there are four cameras, two are wide angle cameras that
move with the head and provide information to allow calculation of distance to objects
and to what objects or faces Kismet should direct attention. The other two cameras are
mounted in the eyes and help monitor the gaze of the human. In addition to visual processing,
Kismet also has an optional auditory system. But in order to minimize noise, Kismet's microphone
is not mounted on its ears, but rather on the person with whom it's interacting, much
like the lavalier microphone I'm wearing right now. Kismet has 15 degrees of freedom
in its face. Each ear has two degrees of freedom. Like an animal, it can be perky to show interest,
folded back to show anger, each eyelid can blink or wink. Eyebrows can move up with surprise,
down for frustration or an outward downward slant to show sadness. The mouth's expression
is driven by four different actuators, one at each corner and the jaw adds one more degree
of freedom. Together, these make up the expression system. Kismet also has three degrees of freedom
in its neck and three more degrees of freedom to control the direction that the eyes gaze.
Now precise control of the neck and gaze are actuated by servo motors that are paired with
high resolution optical encoders. This actuation system is important to acquire visual data,
but gaze and neck position also communicate to the human important social information
like attention. Kismet also has the ability to vocalize, not words, but cues that sound
like those of a young child. The voice synthesizer has parameters that can be altered to add emotional
qualities and adjust the perceived personality of the voice. The sensory input, motor output
and having processing demands are handled by 14 different computers. Each motor must
be controlled and each sensory input must be processed, but in addition, high level perceptions,
motivations and behaviors have to be modeled and coordinated. Kismet's control architecture
consists of six subsystems, the low level feature extraction system, a high level perception
system, the attention system, the motivation system, a behavior system and the motor system.
Now the motivation system contains Kismet's six basic emotions which are derived from
our understanding of the six basic emotions in human psychology. Anger, fear, disgust,
joy, sorrow and surprise. Keeping in mind that Kismet communicates its emotions through
its facial expressions and non-verbal vocalizations. These different emotional states can be varied
continuously in order to create smooth transitions and intermediate neutral states.
Kismet presents these emotional states in response to the person with whom it's interacting.
Kismet can also initiate a particular string of interactions because it possesses what
Brazil calls drives that signal Kismet's own agenda. These longer term drives along
with immediate emotions create the motivation system, all of which is located in the computer's
network to Kismet's head. We can understand Kismet's drives by thinking about it as a
model of a human infant as Brazil did. Infants get tired by too much interaction, but they
get bored without some. So it's up to the human interacting with Kismet to make sense
of how to keep Kismet content. In this way, both Kismet and the human interact to satisfy
Kismet's drives or needs. In the short term, emotional interaction begins
with Kismet assessing whether a stimulus is positive or negative. One example is personal
distance. If you move too close, Kismet backs away. If you're too far away, Kismet makes
cooing type noises to try to draw you in. Now, all of this interaction, the social communication
based on emotional signaling only works because both you and Kismet can exercise attention.
In humans, the direction of our gaze tells us most of the time where attention is focused.
Primates, the group of animals that include humans, apes, and monkeys have whites in their
eyes. Look at your dog or guinea pig sometime. No whites are visible most of the time. Whites
let us see the direction that someone is gazing. Dogs, it turns out, are remarkably good at
following the gaze of human eyes. Dogs can see white dark differences as our eyes move
around and dogs can use those changes to notice what we're seeing and even anticipate what
we're thinking. Experiments have shown that dogs not only follow our gaze, but that they
understand that it conveys information about direction and even the presence of a desired
and hidden object. Robots can follow our gaze and respond with changes in their gaze, too.
Brazil built into Kismet an attention system that was directly linked to the control of
the eye motors. So Brazil uses eye position to signal to the human what Kismet is watching.
By the way, Kismet has eyes with whites. If Kismet is presented with various objects in general,
it will attend to the larger of those objects. Motion can attract Kismet's attention, too,
and many humans found themselves shaking an object at Kismet trying to get its attention.
Now, I think it's important to recognize that successful designs of social robots
capitalize on our human innate automatic abilities to anthropomorphize.
To put it too simply, without even meaning to, we humans project ourselves
into other creatures and robots. This happens even if we are roboticists working on these
robots. Rodney Brooks reported that when Brazil was working on Kismet, other graduate students
working late would put a curtain up between them and the robot. Having Kismet look at them broke
their concentration. No one can turn off these automatic reactions that we have.
One of Brazil's later robotic creations seems to take lessons from Kismet to heart.
She founded a company aimed at offering the first inexpensive family robot,
Jibo. Jibo has a body that is not cuddly like peril, but it is small and simple,
only 11 inches tall and weighing about six pounds. Jibo has a head, which is a round screen that can
project an eye and swivel around. You can see right away the ideas embodied in Jibo's design
don't rely on facial expression to engage humans. What has been carried forward from
Kismet here is voice recognition, speech production and attention seeking and giving
in the form of movements that Jibo makes. When you hear Dr. Brazil talk about Jibo,
first and foremost the idea is that Jibo is a family robot. Jibo's job is to be your companion,
to help your family to assist you in a variety of tasks. It has two high resolution cameras
and has the software to process and recognize faces. Jibo can take photos and videos at parties
without being told to do so and Jibo can involve you in immersive video calling.
It has microphones to hear your voice commands and it can speak to remind you about your meetings
or give you messages. Jibo uses natural social and emotional cues to aid in understanding.
It has learning algorithms actually that allow it to adapt to your household
and learn to recognize your faces, voices and patterns. Now one of the things I love
about Jibo is the way it moves. It can swivel its head or body to signal that it's giving you
attention. It has one eye represented in software on the screen and that one eye blinks. That's one
of those emotional cues. A blink is a subtle way to signal that there is an intelligence in your
midst. Brazil also says that test subjects using social robots have been found to learn better
and follow through on personal projects like weight management better than they do using only
a flat screen device. Another project in social robotics attracting attention right now is Jimmy.
Jimmy is a robot kit with open source software and a manifesto to be intensely social. Intel
and Trosin robotics are the lead backers. You can even 3D print a Jimmy who is known as the
Interbotics HR OS 1 and Jimmy's body has the potential to be more expressive than Jibo.
Obviously humanoid is bipedal and it has a head. A third project also built on the mobile
humanoid design but with a wheeled base instead of legs is Pepper. Launched in 2014 Pepper is
built to learn, understand and adjust its behavior to human emotions. Pepper was the joint project
of the French robotics company Aldebaran and the Japanese telecom company Softbank. Built with
the purpose of being a social companion Pepper can sense and respond to some emotions in humans.
Pepper has what programmers are calling an emotional engine and it can learn about emotions
through its interactions with you and from other Peppers who are interacting with other people.
They talk about us. Its aim is to interact with you in a way that is engaging and positive for you.
Whether you're at home or in a commercial setting. Let me put it to you another way.
A robot like Pepper is designed to cooperate with you by understanding and I'm going to say this
manipulating your emotions. I'm sorry to put it so cynically but it's true. So how is it
that a social robot can read and understand emotion? What's going on when we talk about
reading emotions anyway? Well certain emotions that humans have trigger what are called
micro expressions. Brief involuntary and transient faces that we make. Kind of strange right?
What's key to micro expressions is that we can't hide them. They tend to come out in high pressure
situations and they express six universal emotions that we've just talked about. Disgust,
anger, fear, sadness, happiness and surprise. While we know a lot about reading faces keep
in mind that this is an open area of research and psychology. Work is being done to understand
how humans react when robots express emotions. So this is what we mean by manipulating your
emotions. Understanding how you are likely to react so as to avoid some kinds of emotions and
to evoke others. How is it that Pepper is equipped to interact socially with humans?
Let's start by analyzing Pepper as we have other robots. Sensors. Pepper has four microphones,
a pair of color video cameras in the eyes and mouth, 3D depth sensor behind the eyes,
touch sensors in the head and hands, bump sensors, lasers, sonar and a gyroscope in the torso.
So Pepper can sense also the position of its joints using hall effect sensors
which use magnets to measure motion. Pepper's base has two sonars,
six lasers, three bump sensors and another gyroscope. Pretty impressive. So let's start
thinking about Pepper as having three modules. Starting at the bottom Pepper has a transportation
module. It can roll around at speeds of up to three miles per hour and it uses its bump sensors,
sonar, laser and a gyro to navigate and avoid objects. In the middle we have the manipulation
module. This is where we get to see more robotic arms. The arms touch, reach and gesture. On the top
we have the interaction module where vision, hearing, sounds and signals are created to facilitate
communication. But a diagram of the robot only in terms of modules is really too simple. For example,
instead of having a screen on its top module, perhaps like Baxter industrial robot, Pepper
actually wears a touchscreen on its chest, a bit like a teletubby from the children's television
show. Also you could claim that the gestures that Pepper makes with its hands are for interactions
and hence should be part of the interaction module instead of form manipulation. All true.
But what I'm trying to do here is give us a new way to think about the design of robots.
For a very sophisticated robot like Pepper, all three of these functions or sets of functions,
interaction, manipulation and transportation are likely to coexist and have to cooperate
on that robot. So I think it's pretty cool to think about each of these modules as its own
autonomous robot. So each module is independent from the other in the sense of how it creates
functional loops of sensors and actuators to do its basic set of jobs, right? However,
each module is dependent on the others to get the overall job of the robot done. It's at this
higher functional level that the modules have to cooperate, sharing goals and information,
making and changing priorities about what happens next. And let us not forget sharing a body.
This cooperation among modules has to be coordinated not just by sharing a body,
but also by having a controller that is monitoring the activity of each module
and making decisions about what the overall robot is going to do next.
Given that a controller has inputs, usually from sensors and outputs, usually as actuator
instructions, we can think about this higher level controller in a similar way. As inputs,
it would take information about interaction, manipulation and transportation, not as direct
sensory information, but as what are called state variables. The high level controller then uses
the state value, the values of the combined state variables to update its model of the robot
and the robot in the world. It can then plan for achieving its goal and issue new commands to
the actuators to do so. How robots make decisions depends on the type or style of architecture.
The pepper modules run in a cross-platform software framework called NaOQ that Aldebaran
created for an earlier educational robot called NOW. Voice and visual recognition algorithms are
preloaded that help the emotional engine analyze and respond to emotions from nearby humans.
SoftBank expects pepper robots to learn from their interactions and enhance their learning by having
them exchange information with one another about what they learn. Now this way it may work especially
well for pepper robots stationed in stores where the same kinds of interactions may occur many times.
Pepper can also record and summarize how people respond, making it a likely tool for market research.
The most obvious change in context for a social robot is the person with whom they are interacting.
What a social robot learns about a given person becomes their model of that person,
but it might not apply to other people. Another change in context for social robots is when
they ask for help and what they do if you don't get it to them. Dr. Manuela Veloso and her colleagues
have invented robots who aren't afraid to ask for help. They're called co-bots which is short for
collaborating robots. Co-bots have a specific set of tasks that they do. They can navigate to
specific locations in the building. They can deliver messages, greet and escort visitors,
transport objects, and provide telepresence for conferences in addition to offering companionship.
The only problem with this list of tasks is that sometimes the world is quite cruel to robots
without arms. Co-bots can't pick up objects so they have to rely on humans to place objects in
their basket. They can't push an elevator call button so they have to ask humans to help.
What a co-bot does when it gets stuck is to wait for a human to come by. Here a co-bot is asking
you when you approach the elevator to literally give it a hand. And when you tell it that the
elevator has arrived, it works its way through the crowd and gets on the elevator. Once on,
the co-bot asks you to please push the button for its floor and to let it know when you've
gotten to that floor. The co-bot's assignment might be to get and deliver coffee, for example.
While it can't be sure where it might find coffee, it reasons based on information about
the function of different rooms in the building that it might find coffee in the kitchen.
In the kitchen, it asks for help, receives it from a human, and then can deliver that
coffee to the human who requested it. One of the things that I love about co-bot
is that it does get impatient. You could even imagine, like the robots in 2014 movie Interstellar,
that the degree of impatience displayed by the co-bot could be made adjustable with a human
by a human it interacts with. Call it a patient's parameter. And when co-bot simply can't find a
human to help, and it's getting impatient, it emails Dr. Veloso. Then she or one of her students has
to go find it and help out. What we see in co-bot is not only a robot who can navigate around a
building using low-level behavior-based systems and high-level world model systems, but we also see
that co-bot is a robot who understands what it can and can't do. This is not very far from
self-awareness, at least in a very functional sense, and it shows the importance of understanding
context. When co-bot is aware that it is stuck because of a particular limitation it has,
it asks humans for a very directed kind of help. As Veloso says, robots proactively and
autonomously ask for help for the following things. What they cannot do, think of that as actuation,
what they do not know, think of that as a cognitive issue, and what they do not find,
and you can think of that as a sensing or perception issue. Veloso sums up this robotic
self-awareness and robotic request for cooperation as symbiotic autonomy with humans. Robots are
aware of their functional limitations. Robots ask for help when needed from nearby humans,
and robots send for remote help if no human is nearby. I want you to imagine now the symbiotic
autonomy of co-bot, the expressive arms of pepper, the speech capabilities of Jibo or pepper with
the head of Kismet. We have all three of the robotic modules we mentioned, and we can communicate
socially and emotionally. And because social communication conveys more of what we want in
both directions, we can cooperate more effectively. The drive in robotics is to improve the social
communication of robots by making them more engaging. Engagement is a two-way street of
continual feedback between human and robot, which can be enhanced by recognizing and using
multimodal channels involved in both sensation and actuation. On the sensory side, we have vision,
hearing, and touch. On the actuator side, we have facial, limb, and body movements that may or may
not involve touching the other agent. We have the production of complex sounds that convey
information by changes in tone, volume, rhythm, and expressed words. As robots engage more
effectively with us, some people worry that robots will replace humans as our primary social partners.
This is similar to the thesis of MIT researcher Sherry Turkle, that relentless connectedness
through social media actually isolates us from others. In her words, we are alone together.
Avoiding a deeper divide between people will be a larger challenge of social robotics.
But as social robots help us learn more about human psychology, that may help us address and
perhaps reverse a trend towards social isolation, using robots instead as tools to enhance human
interactions. Welcome to the brave new world of robots and humans sharing emotions.
