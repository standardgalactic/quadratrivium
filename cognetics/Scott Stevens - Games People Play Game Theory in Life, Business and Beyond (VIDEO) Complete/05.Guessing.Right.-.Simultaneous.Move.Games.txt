The classic 2x2 games that we looked at last lecture were simultaneous move games. They're
also called static games. Each player had to make a decision without knowing the decision
that anybody else made. This kind of thing comes up a lot. Here's a worker trying to
decide whether to work or surf the web. Here's his boss trying to decide whether to monitor
it. Here's an expert in computer security trying to protect our data from there's a hacker
who's trying to figure out what that protection will be. Some situations start sequentially
but ends simultaneously. Here's a labor union leader and here's company management and here's
the deadline before the strike. Now, does someone cave or do we have a strike? Simultaneous games
are extremely common in war. Think about D-Day. Both Germany and the Allies knew that the
invasion of France was coming. The question was where? Cherbourg was the most attractive landing
site. It was close and it had a nice harbor. Normandy was an alternative but had no good
harbors to speak of. So what should the Germans do? The Allies want to go to Cherbourg. It's better.
So the Germans are going to expect an attack there. But the Allies are going to know that the
Germans are going to expect an attack there. So they're going to choose Normandy. So the Germans
can realize that and they'll, you get the idea, round and round and round until someone gets tired
of second guessing. Game theoretically, this is a really simple pursuit-evasion game. Cops and
robbers. The cops want to be where the robbers are and the robbers want to be where the cops aren't.
It's hard to believe that there's a better way to play a game like this than just making a wild
stab at what you think the other guy's going to do. But there is. John von Neumann found part of the
answer in 1928 with his Minimax theorem. The rest of it was found about 22 years later by John Forbes
Nash. Von Neumann was a certifiable genius. He received his PhD at 22 and was unquestionably one
of the greatest mathematicians of the 20th century. He made many important contributions to
mathematics, science. He played an important role in the development of the U.S. atomic bomb in the
Manhattan Project. Much of Nash's game theory work was done as a grad student in Princeton. A letter
of recommendation written for him by his undergraduate program advisor consisted of a single sentence.
This man is a genius. Like von Neumann, Nash was an excellent mathematician in spite of his
later bouts with mental illness. So the work that I'm talking to you about comprises only a small
part of the accomplishments of von Neumann and Nash, but an important part. A lot of games have the
D-Day puzzle inside them. I second guess you, you second guess me, and so on forever. Von Neumann
and Nash found a way to solve that puzzle. But at the moment, I can't tell you what it is.
I'm not being coy. You need a couple of tools in your toolbox for us for dealing with uncertainty.
I'll show you how to do these in a couple of lectures, and then you'll be able to handle these
problems just like John von Neumann and John Nash without having to be a genius at anything.
The good news for today is simultaneous games don't have to require an infinite chain of I think,
you think, I think. In some games, the reasoning stops. In some games, it doesn't even have to
start. You can just do what you want and ignore the other players. This lecture is about the
games that don't require infinite reasoning in this sense. I'm going to show you how you can boil
down an unnecessarily complicated game to a simpler one. I'm going to show you how to tell
whether that game has the infinite I think, you think thing required. And if it doesn't,
I'm going to show you how to solve it. To get started, let's look at another example from
World War Two, the Battle of the Bismarck Sea. It's February 1943, the island of New Guinea.
The Allies control the southern coast of the island, and the Japanese control the northern
coast. And the Japanese are bringing in reinforcements, lots of them from China and Japan.
They've already reached the city of Rebal. Rebal is on the eastern tip of an island east
of New Guinea, an island called New Britain. The troops massed in Rebal are intended for the city
of Le in New Guinea. They'll be convoyed by ship and everybody knows it. What's not clear
is what route they'll take. New Britain runs east-west, so the trip will either be along
the north coast of New Britain or the southern coast. Either way, it's a three-day trip.
The significant difference is the expected weather. To the south, the weather is clear.
To the north, the weather is stormy. The Japanese fleet is fine with either kind of weather.
It's the Allies who care about the weather. They're going to send recon planes out to
find the fleet, then bombers to bomb it. In clear weather, both of these things can be done on the
same day. In bad weather, the bombers go out a day after the enemy fleet is sighted. That cuts
down on bombing days. And of course, if you don't even find the fleet, you can't bomb it at all.
The recon planes can't search both routes on the same day.
All of this information is known to both sides. So what should the military commanders do?
Well, let's set up the payoff matrix, like we did last lecture. Each side has two choices,
north or south, so it's a two-by-two game. The payoffs are the number of days of bombing that
occur. Let's work it out. Remember, north is bad weather, south is clear, and the trip takes three
days. In the upper left-hand cell, the Japanese sail north, and the Allies find them right away.
Since the weather up there is bad, the bombing doesn't start until the second day.
The Allies get two days of bombing, so they get a payoff of plus two and minus two for the Japanese.
Okay, look at the lower right-hand corner. This time, the Japanese sail south, and the
Allies find them right away. In clear weather, the Allies can attack on the same day that they
find the fleet, so that's three days of bombing. The other two cells correspond to the Allies
guessing wrong. In the upper right corner, the Japanese are in the clear southern waters.
The Allies waste the day looking north, then find the fleet on the second day and bomb it.
Two days of bombing. In the lower left-hand corner, the Japanese sail north. The Allies
waste one day looking in southern waters, waste the second day finding the fleet to the north,
and only get one day of bombing on the third day. Okay?
Notice something funny about the payoffs in this matrix. In every cell, the payoff to the Allies
was exactly counterbalanced by the loss to the Japanese. The two numbers in any cell,
in other words, always added up to zero. For that reason, this game is called a zero-sum game.
Zero-sum games are always games of pure competition. If someone gains something,
someone else is going to have to lose it. Think of dividing a pie. There's only so much pie gang.
Whatever somebody gains, somebody else is going to lose. By the way, another name for a zero-sum
game is a constant-sum game. The two payoffs always add up to the same number, but it might
not be zero. If you and I divide a pie between us, you can look at it two ways. What you gain,
I lose, so my change is the negative of your change, and they add up to zero. Zero-sum. Or
you could say your piece plus my piece always adds up to one pie. Now the total of our two
payoffs is always a constant number, one. There's really no meaningful difference between these
descriptions, so I'll usually say zero-sum. Okay, the fleet and the planes are waiting their orders.
Let's get to it. Let's start out by taking a really pessimistic perspective. You know Murphy's law.
Whatever can go wrong will. I'm going to start out by seeing where this outlook is going to take
the commanders of each of the two sides. What would you do if you knew that the other side
was going to capitalize on the choice that you made? For the Japanese, this means adopting
an outlook that no matter where they sail, the allies are going to be there and do their
maximum damage. The question for Japan then is, which of my options makes this maximum damage
as small as possible? That is, which of my options minimizes my maximum damage? In the language of
game theory, this is called Japan's mini-max strategy. Minimize the maximum damage resulting
from my choice. Alright, let's find it. Remember, mini-max is pessimistic. If we sail north,
the allies will fly north and we'll get bombed for two days. If we sail south, the allies will
fly south. They'll find us right away and they'll bomb us for three days. Two days is better than
three days, so we'll sail north. We'll be bombed for at most two days and less if we're lucky.
Alright, how would the allies be thinking? Again, adopt the pessimistic strategy. Murphy's law for
the allies says, no matter where we look, they'll have gone the other way. So, if we search north,
they'll sail south. If we search south, they'll sail north. If we search north, they'll go south
and we'll be bombing them for two days. But if we search south, they'll sail north. We'll waste
one day looking in the wrong place, the second one to find them in that lousy weather and only
get one day of bombing. So, two days guaranteed is better than one day guaranteed. We better go north.
You see that the American commander is assuming that whatever option he chooses,
he'll do the minimum damage possible for that option. He's trying to find the option for which
that minimum damage is as big as possible. He's trying to maximize his minimum damage. That's
why this is called his maximin strategy. For the allies being pessimistic, searching north
gives two days of bombing and searching south gives only one. So, the maximin strategy for the
allies is search north. I've used the names minimax and maximin, but the pessimistic reasoning used
by the two sides is really exactly the same. For this reason, it's common to refer to both
of these strategies as minimax strategies. In the game, the two minimax strategies are search north
and sail north. And even though we found them from a pessimistic perspective, these are actually
the very best strategies for both sides. That is, I'm saying everyone goes north is actually
a Nash equilibrium for this game. Let's look at this more carefully.
When you've got a Nash equilibrium, it doesn't help the player to switch to a different strategy.
Let's see if this is true. Start out with both players going north. It results in two days of
bombing. Can the allies change their plans and get more? No. The Japanese chose north because it
limited their bombing to two days regardless of what the allies did. Two days of bombing was
their security value. It didn't matter what the allies did. It won't help. Okay, go back to the
minimax solution again with both going north and two days of bombing. Can the Japanese plan
change their plans to be bombed less? No. The allies chose a strategy which guaranteed them
at least two days of bombing regardless of what the Japanese did. That's their security value.
And the fact that these two security values are the same number means that nobody can benefit
by unilaterally deviating from going north. It's a Nash equilibrium. But I have to point out
that this may be a different definition of the meaning of best strategy than the one that you're
used to. The best outcome for the allies would be that everyone goes south and the allies get to
bomb for three days. How can I say that going north is the best thing for the allies when it
gives them no chance at this best outcome? Well, I can say it because in a game, a player gets to
make his or her decisions. They don't get to make the decision for the other players. Imagine a
potential bank robber who's none too good at his craft. If the alarms are on in the bank,
he always gets caught. For this bank robber, would you say that robbing the bank was a good strategy?
After all, if the bank is so accommodating as to turn off the alarms, then the robber,
we get the money. It's his best payoff. The point is, of course, why would the bank do that?
There's no reason at all. And in light of that, the idea of going to rob the bank
is a lousy idea. In the same way, the only way that the allies can get their best payoff in this
game is if the Japanese leave themselves open to it, if they don't play their Nash equilibrium.
And the only way that the Japanese can be bombed less than two days is that the allies don't play
their equilibrium strategy and leave themselves open to it. If you don't play your equilibrium,
you open a door for someone else to be able to take advantage of your choice. It's in this sense
that we're saying the Nash equilibrium is the best solution.
And in the historical battle of Bismarck, that's exactly what both sides did. Both sides went north.
Unfortunately, if you try to use this minimax approach on an arbitrary game,
it won't necessarily work out so well. When the two sides don't have the same security value,
then that leads to the sort of infinite I guess, you guess, I guess kind of thing we talked about
at the beginning. The solution to this kind of problem is going to have to wait a couple of lectures.
Well, our last example was a pretty simple game. Let's look at a more complicated one
from retail business. Breakfast on the run. We have two merchants who are competing for
breakfast food sales in a train station. One sells Danish pastries. Let's call him Hamlet.
He's Danish. Sorry. The other sells breakfast sandwiches. His name is Ed McGuffin.
To keep it simple, let's assume that the selling prices are a whole number of dollars.
These foods are partial substitutes. If the price of one goes up, some people may switch to the other
food. Some people may simply stop buying. So the more you charge, the more you make per item,
but the less demand you have. This is especially true if the other guy has a low price.
Just take the numbers in the table as given, if you will. But before we go on,
I would like to explain one oddity of the matrix. Look in the first column.
When McGuffin charges $2 per sandwich, his profits are always zero, regardless of what Hamlet charges.
That's because the variable cost for a sandwich is $2. It costs $2 to make a sandwich.
You don't get rich selling your product at cost. Hamlet's variable cost in this problem is only
$1. Okay. What happens in the game? Well, I'm sure you'd agree that McGuffin is not going to charge
$2. $2 never makes him anything, and $3 always makes him at least something. So no matter what
Hamlet does, McGuffin is always better charging $3 than he is charging $2. A formal way of saying
this is that the $3 price dominates the $2 price. $3 is always better. Okay. The $2 price is dominated
by the $3 one. Got it? So we can just cross out the $2 option from the table. In fact,
we can do better than this. Why should McGuffin charge $3? Compare the $3 column in the table to
the $4 column and note that for any given row, that is, for any price that Hamlet is charging,
the red number in the $4 column is always better than the red number in the $3 column.
24 is better than 20, 28 is better than 22, and so on. So the $3 price is dominated by the $4
price from McGuffin. Let's get rid of the $3 column too. Hmm. Actually, $4 is also always better
than $5. Check the corresponding red numbers in the last two columns. So $5 is a dominated
strategy from McGuffin too. Get rid of it. Wow. $4 from McGuffin actually dominates every other
column in the payoff matrix. Charging $4 is a dominant strategy. A strategy is dominant
if it dominates every other strategy. This is going to make things really easy.
If a rational player in any game has a dominant strategy, he or she is going to play it.
Period. End of story. This is the only time that you can safely ignore the payoffs of your opponent.
What they care about and what they do won't affect your decisions.
Okay. McGuffin's charging $4. What does Hamlet do? There's no need to go back to the original
matrix. Hamlet's a cerebral kind of guy. Hamlet is going to know that McGuffin's charging $4.
In the unshaded column, you can see that Hamlet gets his highest payoff. Green is his payoff
by charging $3. He makes $48. Get rid of the rest of his options.
There we are. So Hamlet charges $3, McGuffin charges $4, and Hamlet makes a profit of $48,
while McGuffin makes a profit of $28. Those higher fixed costs really cost him.
By the way, if we'd started our analysis with Hamlet, we would have found out that he did not
have a dominant strategy. Sometimes he wants to charge $3. Sometimes he wants to charge $4.
It depends on what McGuffin does. But since McGuffin does have a dominant strategy,
Hamlet can use this fact to decide what he wants to do. This leads to an approach called the
iterated elimination of dominated strategies, or IEDS. Basically, wipe out a dominated strategy
and see what you've got left. Keep on doing this until there aren't any more dominated strategies.
That's IEDS. If the game has a Nash equilibrium, it'll still be among the cells that are left
after IEDS is done. In fact, if there's only one cell left, that cell's guaranteed to be a Nash
equilibrium. That's what happened here. We can check this. Start with the equilibrium,
the white cell. Hamlet can change that cell by moving up and down when he changes his payoffs.
But look at the green numbers in that column. The best is $48, right where we are in the white
cell, so Hamlet doesn't want to change. McGuffin can move the cell back and forth left and right
by changing his price. But look at the red numbers in that row. His best, $28, is in the equilibrium
cell. So neither player wants to make a unilateral change in this game. It's a Nash equilibrium.
Well, if a player has a dominant strategy, then life is very easy. But what if no one does?
Let me give you another pricing game, this one with perfect compliments.
A long time ago, when the Nash Game Theory Center and Wildlife Park was opened up on an island,
they had a single company owning it, and you would get there by way of a tour boat and come back
on a cable car. In the years since, the cable car company and the tour boat company have been
sold to two different business interests, creating a rather odd situation. Obviously,
any passenger that goes out to the island is going to have to get back so that these two
companies have exactly the same customers. Each individual person, though, is going to decide
whether to take this trip based on the total round trip cost. Again, take the matrix that I've given
you here as granted, and let's figure out what each company should do in setting their prices.
Okay, what happens? It doesn't take long to see that there are no dominant strategies.
There's no column in which the red numbers beat every other column,
and there's no row in which the green numbers beat every other row.
But the fact that there are no dominant strategies doesn't mean that there are no
dominated ones. Let's apply IEDS. Focus on the $8 and $9 columns. Each number in the $9
column is less than the corresponding number in the $8 column. That is, no matter what the price
of the boat, the price for a cable car is better at $8 than it is at $9. $9 is a dominated strategy
for the cable car. Wipe it out. Look at the first two columns now. The red numbers in the $7
column always beat the corresponding numbers in the $6 column, so $6 is dominated. Wipe it out too.
If you ever think about charging $6 for a cable car, charge $7 instead. You'll make more money,
guaranteed. Well, there's no more domination among the columns. Sometimes $7 is better than $8.
Sometimes $8 is better than $7. But now, let's go back to the rows.
Ignore the shaded cells since the cable car operator will never use them.
Do the green payoffs in any one row always outperform the corresponding payoffs in another row?
Just look at the two unshaded columns. Since those are the only ones the cable car might use,
that's all we need to look at. And in these columns, the $7 price for a boat is always the best.
$78 is the biggest green payoff in the first unshaded column,
and $66 is the biggest green payoff in the second column. And this beats all the other rows.
So keep the $7 boat price and get rid of the other possibilities.
Now, go back to the cable car player. If she charges $7, she makes $65. If she charges $8,
she makes $66. So $8 is better. And we've found the one and only Nash Equilibrium for this game.
The boat is $7, the cable car is $8, and both businesses make $66 profit on their customers.
Nice. Does IEDS always work this well? Unfortunately, no. I'm going to make a
tiny change in the game we just played. It actually corresponds to reducing very slightly
the total amount of demand. And when I do this, the new matrix looks like this.
Okay, you know how to do IEDS now, so I'll do the work. It goes just like last time,
except you can't get rid of the first row. And we're stuck. We've got it down to four cells,
and we run out of domination. For either player, neither option left is strictly better than the
other one. So what do we do now? You probably want to say that the second uncrossed out column
dominates the first. 72 is better than 70, and 60 is just as good as 60. Well, that's not quite
domination. To dominate, one option always has to be better than the other, not simply as good.
In the language of game theory, we said that the $8 column weekly dominates the $7 one. In the same
way, the $7 vote price weekly dominates the $6 one. Seven is always as good for a vote as $6,
and sometimes it's better. Can we get away with wiping out weekly dominated strategies?
We can try. Let's see what happens. Ah, well, good. We get down to a single cell. This is what happens
when you get rid of weekly dominated strategies. If there's only one cell left, it's going to be a
Nash equilibrium, so that's good. But there's a cost. When you wipe out weekly dominated strategies,
you can wipe out other Nash equilibria in the Matrix 2. The cell in white, where the boat cost
seven and the cable car costs eight, is a Nash equilibrium. But we don't know if there might be
other Nash equilibria in the table as well, if any of the other three cells shown in blue
might be Nash equilibria. Well, there's a way to find pure Nash equilibria in any Matrix game,
and it's time I showed it to you. It's called the best response method, and it's pretty easy.
Here's how it goes. Let's go back to our original Matrix for the new game.
In each column, I'm going to highlight the best green payoff in that column. I'm saying,
if the cable car costs six bucks, and I'm in this column, what's the best I can do for the boat
player? And so on. You can see the results here. In the first column, 84 is the best. In the second
column, it's 72. In the third column, it's 60. And in the fourth, it's 50. The highlighted cell
show you the best response that the green player could make to any given choice by the red player.
Okay, now do the same thing for the cable car. I'm going to look in each row, and in each one,
I'll highlight the best red payoff. I'm basically saying, okay, the tour boat set this price.
At that price, what's the best the cable car can do? Look at the red highlight. The biggest number
in the first row is 72. In the second row, it's 60. In the third row, it's 50. And in the fourth row,
40. These highlighted red numbers show the best response that the red player has to each possible
choice by the green one. Okay, here's the good part. Look at the cells in which both numbers
are highlighted. Those are Nash Equilibria. We have three of them, which I've tinted a coral color.
So of the four cells that were left after IEDS, three of them were Nash Equilibria. Okay,
multiple equilibria. We saw this happen with sequential games, too, which is the right way to
play it. Well, with sequential games, we refine the idea of equilibrium by using the idea of
sub game perfection. Here, a commonly used refinement is to look for strategies which are
admissible. This just means that the strategy isn't weekly dominated by any other strategy.
We already saw that only the 60-60 equilibrium has this property. It's the only one that survives
the elimination of weekly dominated strategies. So what we expect is that the boat costs $7,
the cable car costs $8, and both of these companies make $60 on their passengers.
But wait a minute. That just seems crazy. Both of the other equilibria have somebody doing better
and nobody doing worse. True. But the 60-60 equilibrium isn't Pareto optimal. It isn't
efficient. The trouble is that nobody can help him or herself by changing from the 60-60 solution.
They can help the other person, but not them. Well, okay then. How about the upper left corner
in the 2x2 game where both companies make $70? That's better than 60. Good idea. But, sorry.
That's the one cell out of our 2x2 table that's not a Nash equilibrium. As soon as you thought
that the 70-70 solution was going to be played, you'd want to change your choice and get $72
instead. And again, don't tell me that you'd be willing to give up $2 so that the other guy gets
10. It's not that it's not allowed. It's just that if it's true, then your altruistic feelings need
to be reflected in your payoffs. Remember, your payoffs factor in everything that you care about.
Okay. You can't help yourself by choosing one of the inadmissible equilibria.
But is there any harm in it? Well, think of it this way.
Suppose I'm setting the boat price. If you choose your cable car price at the higher value,
eight, it doesn't matter what I do. I make $60. But if you choose your lower price of seven bucks,
I'm better off with a $7 price than a $6 one. So, by choosing $7, I set myself up to gain
if you decide to go with your lower price. And I cost myself nothing to do it.
Of course, you have exactly the same kind of reasoning. Your higher price,
Trump's, weekly dominates your lower priced one. So we both end up with our higher priced
alternatives and the 6060 equilibrium. Ironically, you end up in exactly the same place as the
Japanese commander back in the Sea of Bismarck. Going north might not be any better than going
south. But the important thing is, it's never any worse.
