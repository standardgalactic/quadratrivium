Remember our discussion of uncertainty? We said that uncertainty could enter into game
theory in any of three different ways. One of them we explored thoroughly. A player
may use chance in determining which strategy to play. The result was a mixed strategy.
Today I want to look at the other two possibilities. Maybe your game includes random events. Or maybe
you're not exactly sure what game it is that you're playing. This is an unfortunate but real
possibility. Games have players, strategies, and payoffs. And we generally assume that these
things are common knowledge. But what if they aren't? You might not know exactly what strategies
another player is capable of. Can they really steal my best customer? Or you might not know
what another player's payoffs are. What is this promotion really worth to me? A game that includes
either of these two types of uncertainty is called a game of incomplete information. It means
basically that you have questions about the structure of the game. But even if you have
complete information, there's still another source of uncertainty. Secrets. Insider knowledge. You
may know that a new zoning law is being passed. I may not. I may be interviewing for a change in
jobs. But I haven't told you. When one player knows something that another player doesn't,
it's generally called asymmetric information. Asymmetric information arises when one player
knows something about the history of the game, either a random event or a player's decision,
that the other player doesn't. Asymmetric information is a special case of imperfect
information. Imperfect information comes up anytime that a player doesn't know some aspect of the
game's history at the time that that player makes a decision. It turns out that acts of nature,
chance of events, don't cause much trouble in a game, as long as everybody knows the same thing
about the event. If we all know what happened, no problem. We can approach it with the expected
value approach that we've been developing over the last several lectures. This is also the case
if no one knows about the outcome of the event until later. In game theory, you can just delay
the appearance of the chance event until later in the problem. For example, suppose that we're bidding
on a box, which contains either $1 or $100. The amount of money that's in the box could be
determined before the bidding gets started. But game theoretically, we can treat it in the same way
as saying that the amount that's in the box isn't decided until the last moment before the box is
unsealed. It really doesn't matter which way we model this. But how about when you have questions
about the structure of the game itself? That is, when you're dealing with a game of incomplete
information. The answer to that difficult question was found by John Harsany, who got a Nobel Prize
for being so clever. He introduced the Harsany transformation. It allows any game of incomplete
information to be turned into a game of imperfect information. The basic idea behind the Harsany
transformation builds on the idea of types. You don't know exactly what your other player's
abilities or payoffs are. Then create a spectrum of possible other players, types, and assign a
probability to each one of them. Then figure out how you would play against that variety of
actual players. The likely types are going to get higher probabilities than the unlikely ones. A
solvent IBM is going to be more likely than an insolvent one. And when you've got this worked
out, you just start the game tree with a chance node. The chance node randomly determines which
type you're dealing with, and then you play the game. Your optimal strategy is going to be the one
that does best in the sense of expected value against this entire spectrum of opponents. Usually,
each player knows his or her own type, but may not know the type of another. In that case,
the Harsany transformation turns a game of incomplete information into a game of asymmetric
information. It's like you're dealing with a player who has a secret. So, how do we handle a game
of asymmetric information? Good question. I'm going to address it by presenting a classic
example of how a little asymmetric information can have far-reaching consequences. It's generally
called the market for lemons. It was presented in a paper by an assistant economics professor named
George Akerlof back in the 1960s. Akerlof had trouble getting this paper published,
which is funny, because when people eventually caught on to what he was saying, he ended up
being another Nobel Prize winner in economics. Let's start out by assuming that there are
two kinds of used cars, bad ones, lemons, and good ones, peaches. Peaches are obviously the
better cars, but you'd be willing to buy a lemon if the price were right. Let's say you're willing
to pay up to $6,000 for a peach, but only $2,000 for a lemon. I'm selling the cars. To me,
a peach is worth $4,999, and a lemon is worth $999. For either type of car, then, the car is worth
$1,001 more to you than it is to me. So, it looks like we should be able to make a deal on either
kind of car. Okay, you come to my lot, you pick your favorite car, and we're going to assume
that the car that you picked is equally likely to be a peach or a lemon. You make an offer. For
simplicity, let's frame this as an ultimatum game. You set the price, and I take it or leave it.
I'll accept any offer which is anything over what I find the car to be worth. I'm also going to make
one more assumption, one which I'm later going to change. Let's assume that both you and I can
tell the value of the car. We can tell whether it's a peach or a lemon. Then, this game involves
chance, but not imperfect information. When we make decisions, we know the entire history of the
game. The tree for the sequential game would look like this. Notice that it starts with a move by
nature, determining whether the car is a lemon or a peach. Technically, you could offer any price
you wanted for the cars, but I'll never accept an offer below $1,000, and I'll always accept $5,000
for any car on the lot. So, I put these two on the tree, and the midpoint between the two prices,
$3,000. Replace the $3,000 with a different number if you want between the upper and lower limits.
You'll get the same results. I'm just trying to keep things clear. Okay, so what happens? The game
starts with a chance node played by nature. Half the time, we go to the upper branch where the car
is a lemon, and half the time to the lower branch where the car is a peach. You can see the little
one half next to each branch, indicating those probabilities. Since this is a game of perfect
information, we roll it back exactly the same way that we did with our earlier trees. I'll stop
when I get to the chance node. Take a look. What does the tree tell us? It's no surprise,
really. If the car is a lemon, you offer me $1,000, and I take it. You make $1,000, and I make a buck.
If the car is a peach, you offer me $5,000, and again, I take it. You make $1,000, and I make a buck.
That chance node at the beginning really didn't have a big effect on our analysis. There are really
two different games of perfect information being played here. One for a lemon, one for a peach,
each being played half the time. That means that our average payoffs are half of our lemon
payoffs, plus half of our peach payoffs. Probability times payoff added up over all the cases, remember?
By coincidence, you make $1,000 either way, so your average payoff is $1,000, and I make $1,000
either way, so my average payoff is $1,000. You are on the right side of the ultimatum game.
But this analysis assumes that both of us can tell a lemon from a peach. They look identical
on the dealership lot. How about if neither of us can tell the difference? Can we model
this? Yeah, easily. We now have a piece of information that's unknown to everyone until
the deal comes to a close. That means it can have no effect on the play of the game until
we get to payoffs. The tree now looks like this. We don't find out what the car is until
the very end of the deal. We're going to roll this back by computing the expected payoffs
at each of those chance nodes. For example, at the top chance node, we see what happens
when you buy the car for $1,000. Half of the time you make $1,000, half of the time you
make $5,000. Probability times payoff added up over all the cases gives half of $1,000
plus half of $5,000. That's $3,000. We'll carry on this way and roll back the whole
tree. It's actually pretty easy to understand. The car we're talking about has a 50% chance
of being a peach and a 50% chance of being a lemon. That means its expected value to
you is your average of a peach or a lemon, which is $4,000. Its average value to me is
halfway between my values for a peach and a lemon, $29.99. So I'm not going to accept
an offer of less than $3,000, but I will accept $3,000 or more. You knowing that, you offer
$3,000 and we close the deal. Half the time you get a peach, half the time you get a lemon.
And on the average, that means that you're going to make $1,000. Similarly, on average,
I'll again make $1. Well, neither of these cases that we looked at so far were very hard
to handle. That's because both of us had the same information, but things aren't always
like this. And this is where Akerlof's paper comes in. What if I, the car dealer, can spot
a lemon, but you can't? I know the car is better than you. We now have a game of asymmetric
information. I know something you don't when making our decisions. And that does change
things. We can still represent the game on a game tree, but it's got something new. See
those dotted lines between the two red nodes? They mean that those two nodes are in the
same information set. It's like having two identical rooms in your house. If you woke
up in one, there'd be no way for you to know that you weren't in the other. In the same
way, there's no way that a player can distinguish between one node in an information set and
any other. So when you reach your decision node, the tree knows whether the car is a
lemon or not, but you don't. How about me? Take a look at the tree. You can see that
each one of my blue nodes is its own information set. I know where I am. I know whether the
car is a lemon and I know how much you offered. I know everything. So what happens in this
game? Well, in the second game, you didn't know whether it was a peach or a lemon, so
you just went with the average value of a peach and a lemon. That was $4,000 to you
and $2,999 to me. So how about doing what you did last time? Offer me $3,000. Do you
see the problem? You may not know if it's a lemon or a peach, but I do. You offer me
$3,000. If I know the car to be a lemon, I'll be more than happy to sell it to you,
but you'll be sorry that you bought it. You just paid $3,000 for a car that you think
is only worth $2,000. Well, that happened in the second game too. Sometimes you overpay
for a lemon and sometimes you get lucky on a peach. Ah, but you don't. Not anymore.
I can tell whether the car is a lemon or a peach. If you picked a peach, I'll refuse
your $3,000 offer. A peach is worth almost $5,000 to me. Do you see what happened? I'll
only agree to your $3,000 offer if it's on a car that you wouldn't be willing to pay
$3,000 for. We can reason our way through all the possibilities. If you offer me less
than $1,000, I won't sell you any car. If you offer me between $1,000 and $4,999, I'll
sell you a lemon, but not a peach. A lemon is worth $2,000 to you, so we've got some
room between $1,000 and $2,000 to make a deal on a lemon. If you offer me $5,000 or more,
I'll sell you any car on the lot. The trouble is, half of the time that car is going to
be a lemon. We've already worked out that if a car has a 50-50 shot at being a lemon
or being a peach, its expected value to you is $3,000. So you're not going to offer enough
to me to sell you a peach. Ever. There's a market for lemons worth $1,001 more to you
than they are to me. There should be a market for peaches for the same reason, but because
of asymmetric information, this mutually beneficial trade will never take place. That's the reason
that Akerlof got the Nobel Prize because he showed this problem of asymmetric information
applied to a lot more than used cars. It crops up all over the place. Think of health insurance.
There's been a lot of debate recently over how the health crisis in America should be
handled. Should coverage be mandated? Let's look at the question from a game theoretic
perspective in terms of asymmetric information. Each American is going to need some level
of health care expenses for the coming year. I'll record this information, but do it in
an unusual way. For each American, I'll take a grain of sand. I've got a wooden plank here
with a dollars marked on it at different points, and I'll take your grain of sand and put
it with whatever your health care costs are. If your costs are $20,000, I'll put your grain
of sand over the $20,000 mark. I'm going to do this for everybody. There's a 60.
There's a 160. When I'm done with everybody in America, I'll have a big pile of sand
that might look like this. In this fictionalized example, nobody had medical bills higher
than $160,000, the upper end of my plank. This picture is only meant to be suggestive.
If I drew it with the real numbers, it'd be much harder to see all the sand. Anyway, as
you can see, we have a lot of sand on the plank near a health care cost of zero. You
can also see that there's a small pile in the center of the plank near $80,000 and some
grains extending all the way up to the high end. A picture like this is usually called
the probability distribution of health care costs. Well, when you want to find the expected
value or the average of a probability distribution, you generally use calculus, but the result
can be thought of much more easily. Imagine that we're going to make a teeter-totter
out of my plank. I have to find the point where the two sides just balance. I've drawn
a very skinny plank here because it's really the sand that I want to have balancing, so
assume that the weight of the plank itself is negligible. You can show that the average
value, the expected value of this distribution is the point where the seesaw balances. Well,
if you've ever had a brother or sister who's considerably heavier or lighter than you when
you were a kid, then you know how to make the thing balance. The larger kid sits near
the center of the seesaw and the lighter kid sits way out toward the end. I've shown the
balance point of my distribution with a red triangle. This balance point, or fulcrum,
falls at about $10,000 on my plank, which means that the average health care costs for Americans
is about $10,000 in this example. Okay, time to buy health care coverage. How much does
it cost? Well, you're one of the grains of sand on this plank, and the average cost per
grain is $10,000, so that's a reasonable amount for you to pay. If you don't know which grain,
you are. But people often know more about their general health than the insurance company
does. This creates an information asymmetry. You won't be able to exactly predict what
your health care costs are going to be, but you'll probably come close. You might miss
by a lot, but it doesn't happen much. I'm going to keep the analysis simple and pretend
that people are able to exactly predict their health care costs. It won't change the argument
in any substantive way. To cover expenses, the health care coverage is going to have
to cost $10,000. That's where the fulcrum of our teeter totter is. So, who buys insurance?
Well, you're not going to buy it if you're to the left of the fulcrum. People on the
left of the red triangle are paying $10,000 for less than $10,000 worth of health care.
They're not going to pay, so get rid of those people from the picture. Great. Or is it?
The heavy kid just got off with a teeter totter. Obviously, the remaining sand doesn't balance
at the red fulcrum anymore. We're going to have to move the balance point. Quite a lot.
There. That'll do it. The average health care cost of the people still riding is $75,000.
So that's what we have to charge for the system to break even. $75,000? That's a lot of money.
And the people whose costs are expected to be less than $75,000 aren't going to want
to pay it. They'll drop off, getting off of the teeter totter and all that sand leaves,
and the fulcrum has to move again. I won't draw you any more pictures, and I'm sure
that you don't need them. What happens is simple enough. Whoever's on the left side
of the teeter totter gets off, unbalancing it. The fulcrum moves to the right to rebalance,
and this procedure keeps on going until eventually, health care costs $160,000 and no one buys
it. This is why universal health care plans need to be universal. Otherwise, information
asymmetry destroys the market. And it's not just cars and insurance. Think about employment.
The applicant for the job knows whether or not he's competent and intends to work hard.
But the interviewer doesn't. So some of the employees are going to turn out to be lemons,
and some will turn out to be peaches. The employer offers a lower salary. This salary
is a weighted average of what you'd pay a peach and what you'd pay a lemon, depending
on their relative proportions in the market. But that average is going to underpay a peach.
So peaches are going to start to drop out of this market and find something else to do.
That means the actual pool of participants has more lemons than you previously thought.
That means that the blended salary that the employer is paying is now too high. So they
lower it, which discourages more peaches from being in the market, which means that
there's more lemons, which means that the salary gets lowered again. And as you can
see, we've got the same problem. It's not that the employer would not be willing to
pay more for a peach. She would. It's simply that the inability to identify a peach destroys
the market for peaches. In some ways, it can get worse than this. We've assumed so far
that there are peaches and there are lemons. But what if the very act of making an offer
can turn a peach into a lemon? This is a very real problem, and it comes up a lot. The general
term used for this situation is moral hazard. The problem comes up when a party doesn't
bear the full costs for his or her choices. When this is the case, the party is likely
to be less careful in making those choices. I mentioned in my first lecture that the question
of why American League pitchers threw more bean balls than National League pitchers.
And they do about 15% more. And the reason is moral hazard. I'll wait a few seconds before
explaining what I mean by this. For those of you who are baseball fans and would like
to figure it out on your own. In the meantime, here are a couple more examples. I'm generally
not going to park my expensive car in a poorly lit back street. Too much chance of vandalism.
But I'd be more likely to do so if I knew my insurance company would cover the cost
of any damage. Moral hazard. When my doctor says that he thinks that he should have some
more tests run, I generally don't ask how much they cost. I've already paid my copayment,
and the insurance company will do the rest. This means the medical community can charge
more for those tests than I'd personally be willing to pay. Moral hazard. Mortgage companies
are more likely to make high risk loans if they know that if things go badly, the government
will bail them out. In fact, any time that a government considers a bailout, the risk
of moral hazard is really quite real. All of these examples involve asymmetric information.
And moral hazard arises when the party with more information acts in a way that would
be deemed inappropriate by the party with less information, if they knew what was going
on. Okay, the bean ball question. Why do American
League pitchers throw more bean balls? The reason is the designated hitter rule. In the
American League, pitchers don't bat. Instead, another player, a designated hitter, stands
in for them. In the National League, there are no designated hitters. Pitchers have to
bat. This means that if you're a pitcher and you're being a batter on the other side with
your pitch, in the National League, you're going to be in the batter's box yourself in
a little bit. And after you're a little trick, their pitcher might have a 90-mile-an-hour
fastball with your name on it. American League pitchers face no such threat. In games between
the National and American Leagues, the pattern holds. Sometimes these games have a designated
hitter. Sometimes they don't. In games with a designated hitter where the pitcher doesn't
have to bat, more batters get hit. Here's another one. In the first lecture,
I promised to revisit the question of why car insurance costs more in Philadelphia than
it does in Pittsburgh. And it does, almost twice as much. Clearly, both cities are in
the same state, Pennsylvania, so it's not state law. Nor is it rates of vandalism. There
are actually lower in Philadelphia. It's a matter of different equilibria, driven by
an issue of moral hazard. Less people in Philadelphia have auto insurance, even though it's mandated.
This means that more accidents involve uninsured drivers. If you have an accident with an uninsured
driver, that means that your insurance company is going to have to pay for the damages. This
costs the company money, so they raise your insurance rates. With higher rates, less people
can afford car insurance and the cycle perpetuates. The moral hazard aspect of this comes in with
the fact that the uninsured are not responsible for the extra damage or risk of damage that
they introduce into driving. And as a consequence, anybody who buys car insurance in Philadelphia
is likely to be unhappy with the prices they have to pay. The effects can be quite chromatic.
Okay, I want to end today's lecture by returning to the idea of information sets and decision
trees. You'll remember how in the used car problem you had two nodes in the same information
set, since you didn't know whether the car that you were buying was a lemon or a peach.
Since your decision came immediately after that chance node, we were still in a pretty
good shape for being able to analyze the rest of the game. You knew there was a 50% chance
that you were in the lemon branch and a 50% chance that you were in the peach. The situation
can get more complicated, a lot more complicated than this. I want to give you an example
that's only slightly more complicated. Here is a very simplified version of poker, which
I call tiny poker. To begin with, I put $40 in the kitty. The kitty is simply a pool
of money. If one player wins, that player gets all the money in the kitty. If we tie,
the money in the kitty is split evenly. So I put $40 in to begin the game and you have
to put in $70. That's $110 total. You have to put in more because as it turns out in
this game, there's a first mover disadvantage and I'm going to be going first. Okay, we
put in our initial cash and after that we're dealt a single card from a deck of only three
cards. The three cards are labeled one, one and two. There are three equally likely deals
of course, because we're all get, we both get one card. Either you get the two, I get
the two or nobody gets the two. Each happens one third of the time. We each see our own
card but no one sees the card of the other player. If neither of us quits the game, at
the end, whoever has the two wins and gets all the money. And if neither of us has the
two, we split the pot right down the middle. But someone might quit before this happens.
After I look at my card, I can decide whether to quit, to fold or to continue. If I want
to keep on going, I have to add $70 more to the kitty. Look at the tree. You'll see that
my top two nodes are in the same information set. Those two nodes correspond to my looking
at my hand and seeing a one. I don't know where the two is. Maybe you have it. Maybe
no one does. I think it's pretty clear from the problem that if I have a one, then half
the time you have the two and half the time you don't. That's the one card left that wasn't
in the deal. So I can keep going or fold. If I fold, I lose and you get my 40 bucks that
was already on the table. If I keep going, I have to pay another 70 bucks. Then it's
your turn. Like me, you can decide to fold if you want. If you do, I win the kitty.
So that means I get your 70 bucks. I get my money back too, but that's not winning anything.
On the other hand, you may decide to keep on going after you look at your hand. And if
you do, you have to pay an additional $30. Look at your information sets on the tree.
Two nodes tied together are where you have a one and didn't quit. Maybe I have the two.
Maybe no one does. You don't know. If you pay $30 to stay in the game, then we both reveal
our hands. Whoever has the two wins. Since I've put in 110 total and you only put in
100, a win is a little better for you than it is for me. If we both have a one, we both
tie. Again, because I put in $10 more than you did, this actually means that you make
$5 off of me in this case. Well, the tree makes pretty good sense once you look at it
carefully. The thing that's tricky is to figure out what to do and things here are actually
pretty tangled. Suppose I look at my hand and see I have a one. I know there's a 50-50
chance that you have the two. That's not too hard. It's easy to see that I should continue
with a two since I can't lose, but how about if I have a one? With a one, I'm guaranteed
to lose unless you quit, which you might if you have a one. Maybe I should bluff with
a one. How often should I do it? Well, that depends on how often you're going to quit
with a one, but whether I fold with a one depends on how likely, whether you fold with
a one depends on how likely you think it is that when I keep on playing, I have a two.
That depends on how often I bluff. So how often I bluff depends on how often you keep
playing with a one, which in turn depends on how often I bluff. Does this sound familiar?
It should. It's the same kind of reasoning we went through with simultaneous games. And
believe it or not, it's simultaneous games that are going to get us out of this mess.
Every sequential game can be expressed as a simultaneous game, saying it another way.
If you can express it in a game tree, you can express it in a matrix. We're going to
see how our little poker game here can be rewritten to be written as a game tree. I'm
going to start as a matrix. We're going to be also seeing how we can use the ideas of
information signaling and imperfect information to solve this problem. Signaling combines
the notion of credibility from the last lecture with the idea of imperfect information from
this one. Basically, if I know something that you don't, how can I tell it to you and get
you to believe me?
