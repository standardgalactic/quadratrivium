It's an important fact of human existence. We're social creatures, intensely social.
When we think of encounters with others, we're usually thinking about repeated encounters.
Almost by definition, a relationship with someone involves a history of past interactions and an anticipation of more interactions to come in the future.
And the history with the person that you've built with someone has an important role in shaping your expectations, and theirs.
We've largely ignored this in our study of game theory so far. Some games really are one-shots.
When you let a car into traffic in front of you, it's probably someone that you'll never see again.
But many, if not most games, are really just a link in a much larger chain.
When we link up all those little games into a bigger one, what happens?
That's what today's lecture is about.
We're looking at a super game made up of a series of stage games.
How does the fact that the game has multiple stages influence the choices that we make in each?
It obviously does make a difference. When your car is repaired in a strange town, you're likely to pay more for that repair than a local would.
There are a number of factors, but one reason, surely, is simply the fact that you are a tourist.
Whether the service is great or terrible, the guy in the garage knows that you aren't coming back.
I can't do a general analysis of super games. It's very complicated.
What I can do is to give you a feeling for how we approach such games and some important results in the field.
As a particularly interesting example, I'm going to return, once again, to the prisoner's dilemma.
I want to see if, in an iterated version of this game, there's a way out of that sorry defect-defect trap.
Can cooperation make sense in an iterated prisoner's dilemma?
But first, let's make sure we know what we're talking about.
As I've said, a super game is made up of a series of one-shot games called stage games.
All strung together.
In a lot of cases, the stage games are all going to be the same game.
And that's going to be my assumption for most of our work today.
When that's the case, the super game can be called a repeated game or an iterated game.
That is, a repeated game consists of a number of repetitions of the same stage game.
Each repetition is called a round of the game.
So, what does a strategy for a repeated game look like?
Well, a strategy is going to have to do what a strategy always does in game theory.
It's going to have to tell you what decision you should make in every situation in which you could find yourself.
What does that mean for an iterated game?
Well, let's make it concrete.
Let's imagine that the stage game is the prisoner's dilemma,
and this game has two strategies for you, cooperate or defect.
Suppose that you and I are going to play, say, five rounds of the prisoner's dilemma.
What would a strategy for you look like?
Well, here's one, a rather obvious way that you could specify a strategy
before the game ever begins, decide what you're going to do in each round.
For example, I'll cooperate in the first two rounds, then I'll defect in round three,
and then I'll cooperate for the last two rounds.
This is a kind of strategy called an open-loop strategy.
That just means that your decisions in any given round don't depend on what happened in earlier rounds.
Stated more briefly, it means that your strategy ignores the history of the game up to that point.
But I'm sure you can see that many sensible and interesting strategies
for this kind of game are going to be closed-loop,
meaning that what you do in a given round can depend on what happened in earlier rounds.
As an example, think of our five-round prisoner's dilemma game.
One closed-loop strategy would be, I'll start by cooperating,
and after that, I'll cooperate every single round until I'm betrayed.
Then I'll never cooperate again.
You can see how the idea of punishment is going to arise really quite naturally
when we talk about closed-loop strategies.
The strategy I just specified is a very harsh example of this.
Trust until burned, then never trust again, period.
We'll be looking at this strategy again in a short while.
It's generally called the grim trigger strategy.
A trigger strategy is one that changes behavior
when the play in a stage game deviates from some particular baseline.
The grim trigger is about as extreme an example of this as you can get.
Betray me once, and I'll hate you forever.
That's grim.
We're going to try to figure out the best way to play iterated games.
We're going to need to look for Nash Equilibria as we did before.
You already know that games can have more than one Nash Equilibrium.
If the stage game has more than one Nash Equilibrium like chicken,
then the repeated game also has more than one, usually a lot more.
It's easy to show that if you and I play an equilibrium game in each individual stage game,
then that entire collection of strategies is in fact an open strategy equilibrium for the repeated game.
For example, consider a 10-round game of chicken.
I swerve on the first seven rounds while you go straight,
and then you swerve on the last three rounds while I go straight.
Since we did play an equilibrium in each individual round,
one person going straight, one person swerving,
then that collection is an equilibrium for the 10-stage game.
The resulting play is going to be an equilibrium.
But you can see this can give rise to a lot of Nash Equilibria.
And actually, there are usually a lot more than that.
And they're hard to characterize.
You can show that in any equilibrium of a repeated game,
what goes on in the last round is going to have to be an equilibrium of that round.
But that's not saying much.
Sometimes, though, we can say a lot more.
We can do this, for example, if the stage game has only one Nash Equilibrium,
like the prisoner's dilemma does.
When that's the case, it's pretty easy to identify out of the myriad of strategies,
and there can be billions, those that lead to sub-game perfect equilibrium.
Essentially, we can identify the right way to play an iterated prisoner's dilemma
when the game ends after five or a hundred or any number of rounds that you please.
In fact, we're going to see that there's only one equilibrium,
which is, in fact, sub-game perfect.
We can reason our way to it.
Let's imagine that you and I are going to play a hundred rounds of the prisoner's dilemma.
To make it clear, let's use the payoff matrix below.
For a single round of play, this game has only one equilibrium.
Defecting is the dominant strategy for each of us.
That is, no matter what you do, best choice for me is to defect.
And the same thing is true for you, of course.
So if we play this game for a hundred rounds, we'll defect for a hundred rounds,
and each get a payoff of 200 points.
Well, we can do that.
It will be a Nash equilibrium since we played a Nash equilibrium in each of the repeated games.
If you know that I'm going to defect every round, regardless of what you do,
then you can't do better than defect right along with me.
But this is such a stupid equilibrium.
Can't we work together, at least for a while, without being a sucker for the other guy?
Well, when people play this game in real life,
they often make an agreement before they start, something like this.
Okay, I'm going to cooperate as long as you do, but if you betray me, I'll betray you back.
It's not a binding agreement.
It's not quite a complete strategy, either, so let's make it one.
I'll cooperate, but if you ever betray me, I'll defect the rest of the game.
That is, I'm playing the Grim Trigger strategy.
Does this make a difference?
Well, experimentally, yes, it does.
When real people are playing this game, they generally cooperate, at least for a while.
But my question for the moment is, according to game theory, should it make a difference?
Let's make sure we're clear on what the question is.
There are three things that you have to keep in mind.
You and I will play 100 Grounds of the Prisoner's Dilemma.
We've both said that we're going to cooperate until the other guy defects,
but that's cheap talk, not a binding agreement. You can do what you want.
And lastly, remember that your payoffs are what you care about.
Any play that has you getting more points total is going to be better for you
than any play that gives you less points.
This means that for now, you're going to have to keep your feelings of looking out for the other guy
or of keeping your word out of it.
So, what do you do?
Well, the payoff for cooperating is better than the payoff for defecting, if both players cooperate.
And we said we were going to. So, start off cooperating.
Each round, we both get three, which is better than two.
And on and on we go.
In real life, people sometimes break from this cooperation just for a little excitement
or to get ahead of the other guy.
They can do this, it's real life, but it is irrational behavior
if the player really does want to maximize his or her payoff at the end of the 100 rounds.
So, cooperate, cooperate, cooperate, cooperate.
Until the last round.
What do you do in the last round?
This, as it turns out, is an unimaginably important question.
And it'll determine a lot more than the handful of points that are at risk in the final stage.
What do you do in the last round?
Well, you want to maximize your points, right?
And this is the end of the game.
Final round, defect or cooperate.
You don't have to worry about retaliation if you defect.
There's no chance to retaliate. It's game over after this one.
And whatever the history of the first 99 rounds has been up to this point, that's in the past.
From where you're standing now, you're playing a single shot prisoner's dilemma.
And we all know there's only one way to play a single shot prisoner's dilemma, and that is to defect.
No matter what I do in round 100, you do better by defecting.
And by the same reasoning, of course, so do I.
All right, so what? Big deal.
We said we'd cooperate until the other guy defected, but it wasn't quite true.
We cooperated until the last round, then we both shot a defect at each other, and we both lost one point.
Oh, no. It's worse than that.
We've just realized that the last round is going to be mutual defection, regardless of the history of the game.
You know it. I know it.
So the last round is a done deal, and nothing that either of us can do can change it.
There is no sense trying to impress the other guy as we go into the 100th round.
Which means that the 100th round isn't really the last round after all.
The real last round is round 99.
That's the last time that earlier play might have any impact on what happens next.
Do you see where this is going?
If the 99th round is the real last round, then the logic that I just used on the 100th round now applies to it.
We both defect because it's the dominant strategy.
How's the other guy going to punish you for this?
Defect on the 100th round? They're going to do that anyway, remember?
So we both defect in round 99, regardless of what happened earlier in the game.
So the 98th round is the real last round.
So we both defect in round 98, so now the real last round is 97, and like a sleeve with a loose piece of thread,
the pool of logic runs backward through all 100 rounds, unraveling cooperation at every stage of the game,
until the first round is the real last round.
And so we betray them too.
I'm saying what you think I'm saying.
The only sub-game perfect equilibrium for the prisoner's dilemma of a fixed length is to defect every single round.
Do you find this dissatisfying?
Because I do.
In this equilibrium, we both get 200 points, but normal people playing this game without any game theory experience almost always get more.
This deserves a much closer look, since it really feels like there's something wrong here.
I'm going to revisit this fact and the implications of it a lot more closely in my next lecture.
There we'll be taking a practical perspective, comparing what game theory prescribes to what actual people really do.
But given the game as I've just described it, game theory gives us a conclusion that is inescapable.
The only sub-game perfect equilibrium is to defect in every round.
It's starting to seem that the prisoner's dilemma is a hopeless game.
We've looked at a single round version and played the dominant strategy, defect.
We could have done better by mutual cooperation, but we couldn't do it.
And this problem of not being able to trust the other guy is still there, even if we play in multiple rounds.
And even if I knew that you were going to cooperate, it's still better for me to defect.
Our hope in this lecture was that multiple plays would make a difference,
that knowing that we had a future together would allow us to cooperate.
Isn't there a way that we can make cooperation make sense?
What we've seen so far doesn't give us much hope.
If all the players know that the play is going to go a fixed number of rounds, one or a thousand, we're doomed.
The desire to cooperate logically unravels from the end of the game up and we defect every single round.
Depressing.
Well, it's not quite as bad as it seems.
There are still two possible ways out.
One of them has to do with society.
I'll discuss what I mean by that and how it's relevant to our problem in the next lecture.
But the other possibility I'll discuss now, let's deal with the problem head on.
What kills cooperation in a multi-round prisoner's dilemma?
Where does the problem start?
In the last round.
The last round was effectively a one-shot and defecting there was the only sensible move.
So what if there wasn't a last round?
Or what if there was one?
But you don't know when it is.
The idea of there actually being no last round is a fiction for real-life games.
We're all mortal.
So an infinite number of rounds is not literally going to happen.
So I'll set that aside for today.
On the other hand, it's very common to have a series of encounters with other people
and never to know if today's encounter is going to be the last one.
We often treat people with the presumption of possibly meeting them again
even when the chances of it seem quite small.
Think of how many people that you've been introduced to that you never saw again.
So suppose we look at iterated games where the players know that they might play the game again.
Does that affect our results?
After all, if you don't know when the last round is, you can't decide that you ought to defect on the last round.
Maybe we can keep things from unraveling.
This is an important question, but the mathematics that describe it give them some pretty heavy stuff.
I'm going to make some simplifying assumptions for our discussion.
I'm going to stick with the prisoner's dilemma, and I'm going to assume that you and I are playing the game.
After the game, there's a certain chance that we'll play again.
I'm going to call the chance of continuing Delta.
Most people are more comfortable with numbers than they are with symbols,
so for most of my discussion, I'm going to pretend that Delta is one-half.
After a game, there's always a 50% chance that we'll play again.
You can imagine that after each round of play, you flip a coin.
Heads, we play again.
Tails, we're done.
How likely is it that the game is still going on after a certain number of rounds?
Certainly, we play the first round.
After that, you flip a coin, and if it comes up heads, we also play a second round.
So the second round occurs half the time.
So the chance that we get to the third round is the chance that you flip two heads in a row.
The chance that we get to a fourth round is the chance that you flip three heads in a row, and so on.
You'll remember from our probability lecture that when events are independent, like coin flips are,
you find the probability of all of them occurring by simply multiplying together the probabilities of the individual events.
The chance of flipping a head, one-half.
The chance of flipping two heads in a row, one-half times one-half.
Three heads, a half times a half times a half, and so on.
If you flip a coin, n times in a row, the chance that it comes up heads, all n times, is just one-half multiplied by itself n times,
or one-half to the n.
Now remember that it takes two flips to get to the third round, three flips to get to the fourth round, and so on.
That is, to make it to a given round, you need one less coin flip than the round number.
So the chance that you'll get to the nth round is actually one-half to the n minus one.
Okay, let's imagine that we're both playing the always-defect strategy, just for purposes of discussion.
Then we each get a payoff of two for each round, as long as the game goes on.
I'll write it like this.
What's your total payoff if you play this way?
Well, the game's going to end eventually, but you don't know when.
You'll certainly get your first round payoff, but if the coin flip is at tails, that's going to be it.
But half the time, you'll also get your second round payoff.
So on average, your payoff from the second round is one-half of two.
If you like, in the second round, half of the time, you get a two, and half of the time, you get nothing.
On average, you get one-half of two.
Great.
How about the third round?
Well, this requires two coin flips, so the chance of you making it to the third round is only one-half times a half, a half squared.
If you make it, the payoff is two.
So your expected payoff from the third round is one-half squared times two.
From the fourth round, one-half cubed times two, and so on.
I'm not simplifying the arithmetic because I want you to see the pattern I'm creating.
If we add up the expected payoffs for the first four rounds, we've got a half, or two, plus a half times two, plus a half squared times two, plus a half cubed times two.
And if we include all the later rounds that we might reach, this pattern goes on forever.
Each term in this sum tells you how likely it is that you make it to that round times the payoff you get if you get that far.
Add them all up, and you get the average payoff you can expect in this game, which I've called T.
But this sum goes on forever.
How do you add up an infinite number of terms?
This is really cool.
And a summation like this one?
It's a really easy as well.
Let me start with the expression for T that goes on forever, like this.
Now, if I multiply both sides of an equation by a non-zero number, the resulting equation is still true.
I'm going to do that.
I'm going to multiply this equation by one-half.
You'll see why in a minute.
Now, I can get rid of those outer parentheses on the right-hand side by multiplying the one-half by each term inside the parentheses.
This is just that distributive law that you loved so much as a child.
Okay, now look at this.
Remember, both of these sums go on forever.
My next point's going to be easier to make if I add a couple of spaces to the second equation to move the terms over a bit.
Now, take a second and compare the right-hand sides of these two sums.
See how the top and bottom terms match up?
The top has a two that the bottom equation doesn't, but that's really the only difference.
You might think the bottom equation has an extra term at the end, that one-half to the fourth times two, but it doesn't.
These sums go on forever, remember?
So that term is actually on the top, two.
These two summations are perfectly identical except for the number two.
It's as if you started out counting at the number two, and I started out counting at the number three, and we never stopped.
Really, the only number that you said that I didn't is two.
Same thing here.
So if I subtract the lower equation from the upper one, like this, look at what I get.
Everything on the right-hand side cancels except for the two.
So t minus one-half t equals two, which means that t equals four.
So if you get two points per round, as long as the game goes on, then on average, you'll get four points total.
Half the time you'll get two, a quarter of the time you'll get four, an eighth of the time you'll get six, and so on.
But when you add all these terms together, you get an average of four.
If you didn't like all the math, just remember this.
You can figure out what your average is going to be if you get the same payoff in every round by simply doubling the payoff that you get in a single round.
So if you get three every round, then on average, you're going to make three times two or six.
But watch out.
This shortcut only works if you're letting the coin flip decide whether the game's over or not.
If there's not a 50% chance that it's continuing, then the work that we've just done generalizes in a nice straightforward way,
and I'll give the general result here for those of you who like to follow the math more closely.
Suppose that you get a payoff of x each round until the game is over,
and the chance of continuing from one round to another is always delta.
Then your expected total payoff for the game is just x divided by one minus delta.
Our example before was just delta equals one half.
All right, we now have the machinery to see if there's a way out of the prisoner's dilemma for this kind of repeated game.
We know that both of us always defecting is a Nash equilibrium for this game.
This will give each of us a payoff of two every single round until the game ends.
Using our little shortcut trick, that means the average payoff to each of us will be two times two or four.
If we use delta instead of one half and doing the calculations, it's two over one minus delta.
Okay, how about if we're both playing the Grim Trigger strategy instead?
Remember that Grim Trigger means that a player cooperates until the other one betrays.
If we're both doing that, we never betray.
Well, if both of us do it and no one betrays, we're going to get three every single round.
Using our shortcut formula, that means our average payoff from always cooperating is going to be three times two or six.
No big shock there.
Cooperating every round gives us a better player payoff than defecting every round.
Six is better than four.
But that's not the question.
The question is, is this a Nash equilibrium?
Suppose you know that I'm playing the Grim Trigger strategy.
Question is, can you do better than playing Grim Trigger yourself?
Well, let's think about it.
Grim Trigger has us cooperating every single round.
If you're going to do better than that, it's going to have to be because at some point in the game, you're going to betray me.
Cooperate, cooperate, cooperate, just like a Grim Trigger and then bam, betrayal.
Up to that point, you'll get the same payoffs as a Grim Trigger would.
So we can really look at what happens from that point on.
Once you betray me, you might as well keep doing so.
If you've betrayed me, I am never going to forgive you.
I'll betray from that point onward.
So you're going to get four points for the first round when I get zero because I trusted you when you betrayed me.
This is in comparison to the three points you would have gotten if we cooperated.
And four is bigger than three, no doubt.
But after you betray, you're going to get two points every round thereafter until the game ends.
As I said, you're going to betray every round because I'm going to do so and you can't do better than that.
So let's compare the two payoff streams.
If you play Grim Trigger, you get all threes.
If you betray me, you get a four followed by a bunch of twos.
It may look to you like you're better off with all the threes, but that depends on how long the game goes on until it ends.
There might not be that many of those threes to worry about.
The game might be over in a round, in two, in three.
We need to figure out what's going to happen on average.
Okay, let's suppose that you're playing all threes.
As we said, the average payoff is going to be a series of threes, three times two, which gives six.
If on the other hand you betray me once, it's best for you to keep doing it.
You'll get a four followed by a series of twos.
I can think of that as a series of twos, which is worth two times two or four points, plus a two point bonus for that first round when you got four.
So if you do betray me and keep it betraying me, your average payoff from that is six as well.
So you gain nothing by defecting, but you lose nothing either.
That is, if you know that I'm playing grim trigger, you could always cooperate or always defect.
On the average, you'll make the same payoff.
Well, we've turned the key in the lock of the prisoner cell, but we haven't opened the door.
Cooperation is fine for you, but so is defecting.
But wait, the coin flip calculations assumed a 50% chance of continuing the game to a new round.
Let's use the last column with a probability delta of continuing.
Then it's best for you to cooperate if three over one minus delta is bigger than two plus two over one minus delta, or one over one minus delta is bigger than two, or delta is bigger than one half.
That is, if the game has more than a 50% chance of continuing, then the only optimal play against the grim trigger is always to cooperate.
So two players playing the grim trigger strategy and thus always cooperating do form a Nash equilibrium.
And there was much rejoicing.
So there is a way out of the prisoner's dilemma.
You'll find that purely cooperative play between two grim triggers is a sub game perfect Nash equilibrium of any prisoner's dilemma game as long as delta is big enough.
That is, the game can't end in a known number of rounds, and it has to have a high enough chance of continuing beyond the current round.
Under these circumstances, cooperative play every round can make perfectly good game theoretic sense, which to me is a relief.
By the way, it's possible to look at the work we've done here in a totally different light dealing with what's often called net present value.
Imagine that you could put your payoffs from one round in the bank and let them earn interest at a rate of r per round.
Then getting the payoff next round isn't as good as getting it now. You lose the interest for one round.
It turns out that this model is described by exactly the same mathematics that we've just done here today.
Delta gets replaced by one over one plus r. That's all.
So in this model, cooperative equilibrium is possible provided that the interest rates aren't too high.
Unfortunately, while the cooperative equilibrium exists, it's a long way from the only equilibrium.
Both defecting is still an equilibrium.
In fact, there's a sub game perfect information for equilibrium that will give players any average payoff between two and three in this game,
provided that Delta is high enough. It doesn't even have to give the same average payoff to the two players.
Well, we can't be greedy. We have what we came for.
There's finally a way to cooperate in a prisoner's dilemma without being a sucker.
And in my next lecture, we're going to see that there's another way that this can happen.
I can't tell you now, but put simply, it's this.
Yeah, you can defect every round. But what are other people going to think of you?
