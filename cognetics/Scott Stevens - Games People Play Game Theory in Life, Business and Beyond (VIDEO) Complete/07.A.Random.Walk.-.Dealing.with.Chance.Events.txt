Welcome back to Game Theory.
The topic for today's lecture is, uh, five, uh, randomness and chance.
That was lucky.
On a twelve, I'd be talking about the Peloponnesian Wars.
Of course, you wouldn't really expect me to pick my lecture topics that way.
In putting a series like this together, it's obviously a better way to do it, and the good
people at the teaching company would be understandably concerned if I let chance to saw what my next
lecture was going to be about.
In fact, when we convey to people that one is exceptionally well-prepared, we often say
that they have left nothing to chance.
And that's the kind of games we've been talking about so far, deterministic, nothing left
to chance.
But sometimes that's not good enough.
Being predictable isn't always an advantage.
It may sometimes be in your best interest to inject a little unpredictability into your
decisions.
And even when it isn't, you may not be able to avoid it.
Like it or not, real life is loaded with unpredictable events.
We're just going to have to learn to be able to deal with them.
There are really three main ways that unpredictability or chance works its way into game theory.
As uncertainty about the outcome of an event within the game, as an uncertainty about the
structure of the game itself, or as an uncertainty about the pure strategy that a player will
choose.
Let's take a moment and consider each of these three possibilities.
Chance events may occur within the structure of the game.
This is a common occurrence, and that's why we often make plans with contingencies in
them, recognizing that events beyond anyone's control may play a part.
The plans for a family outing will depend on the weather and on people's health and
on whether an emergency comes up at work.
We can think of these events as decisions of nature.
Later we'll see how we can model them by adding a new player to the game, named nature, and
letting nature's decisions be based on probability rather than on strategy.
This causes very little trouble unless there are some players who know the outcomes of
nature's acts and some who don't.
Which leads naturally to the second kind of game, where the structure of the game itself
is uncertain.
Such games are called games of incomplete information.
As an example, the U.S. interacts with North Korea, but we may not be certain of their nuclear
capabilities or their views on certain actions, such as the nuclear attack of another country.
Not knowing their capabilities were uncertain of their possible strategies.
Not knowing their views were uncertain of what payoffs they would assign to certain outcomes.
Such uncertainty can make a game very difficult to analyze.
We'll handle it by using a clever approach that was invented by John Harasani, who won
a Nobel Prize for his work.
It's called the Harasani Transformation, and we'll look at it in more detail in a later
lecture.
But the third way that Chance can enter a game is the one that I want to focus on here
today.
Chance can play a role in a player's selection of which pure strategy to play.
Up to this point, we've been analyzing games from the perspective of pure strategies.
Players specified how they would respond to every situation they could find themselves
in.
Remember the idea of a complete instruction book?
Well, once each player has selected a strategy, the game essentially plays itself out.
You could play the same game a hundred times, and you would get exactly the same sequence
of events, and exactly the same outcome all a hundred times.
For a game with only one Nash equilibrium, this generally seems quite sensible.
In fact, even for games with multiple Nash equilibria, it's still sensible, provided
the players can find a shelling point.
But how about things like pursuit-evasion games?
Suppose a terrorist is attempting to smuggle a bomb into the U.S. across one of the two
borders, Mexico or Canada, and the U.S. security forces are trying to interdict the shipment.
They don't have enough forces to effectively police both borders.
In this game, like the D-Day invasion game, has no Nash equilibrium.
Every solution to the game either has the U.S. matching the terrorist move, or the missing
it.
If they match, the terrorists would like to unilaterally change their strategies.
If they miss, the U.S. would like to unilaterally change its strategies.
In either case, the original solution was not an equilibrium.
Many games where competition plays a significant role have the same property of no Nash equilibrium.
But it turns out that such games do have a Nash equilibrium.
At least they do if you're willing to broaden somewhat the definition of what you're willing
to consider a strategy, and to rethink how you compute payoffs.
This is what von Neumann did in that 1928 paper.
He showed that you can always find an optimal strategy for each player in a two-player zero-sum
game.
Later Nash extended this result to non-zero-sum games.
He also extended it to non-cooperative games with more than two players.
The key idea for this new result is the notion of a mixed strategy.
We played around with this idea in previous lectures.
We said that a mixed strategy was one that depended on randomness.
Let me be a little more specific about that today.
To make ideas concrete, let's suppose that you're playing a negotiation game, and your
strategy corresponds to the mode in which you'll conduct the negotiation, aggressive,
friendly, or restrained.
Your strategy library then would only have three books, aggressive, friendly, restrained.
But you could also play a mixed strategy.
You do this by rolling a die, for example, and letting it decide which of the strategies
you'd play.
You let chance decide.
For example, on a one to three, I play aggressive.
On a four, I choose friendly.
On a five or a six, restrained.
You'll note that choosing randomly doesn't mean that each option's equally likely.
In my particular case, one through three all meant the same thing, adding to playing aggressive
half the time.
In fact, I can pick any probabilities that I want as long as they all add up to 100%.
The introduction of mixed strategies into a game gives you a lot more flexibility in
playing it, but this flexibility comes with a cost.
The mixed strategy approach lets you keep your opponent guessing.
That's good.
But a mixed strategy also makes the game no longer finite.
How do mixed strategies keep the opponent guessing?
Well, she may know the probabilities that you assign to each strategy, but she can't
know what particular strategy you're going to play.
You don't know that yourself.
Why is a game with mixed strategies no longer finite?
Because the player now has at his or her disposal an infinite number of different mixed strategies.
She simply varies the proportions with which she plays the various pure strategies.
It's like a chef who can make limitless variations on a recipe by changing the relative
proportions of the ingredients.
Since the game is in finite, ordinal payoffs are no longer good enough.
We need cardinal payoffs.
That is, it isn't enough to simply know that you like outcome A better than B. I need to
know how much you prefer A to B.
Maybe A is winning and B is losing.
In a finite game, all I need to know is that you prefer A to B, winning to losing.
In an infinite game, I need to know that, for example, you win 10 bucks, but that you
lose 50 bucks.
Then there's still the question of how we measure the payoff for such a game.
When pure strategies are played against one another, the outcome is certain and always
the same.
But if players are playing a mixed strategy, then the game won't always give the same result.
If the U.S. border security plays against the terrorist, sometimes the terrorist gets caught,
sometimes he doesn't.
Well, we won't be able to make much progress in dealing with mixed strategies until we
lock down some fundamentals about probability.
That's what we're going to be doing in the rest of this lecture.
This discussion is going to be informal, but it should give you the tools that you need
to be able to work with mixed strategies and understand the logic that leads to them.
The study of probability dates back to the mid-1600s.
British mathematicians Blaise Pascal and Pierre de Fermat were the world's first probability
pen pals.
They wrote a series of letters back and forth, asking each other questions on how likely
certain events were in games of chance, like rolling dice.
Essentially, the probability of an event is the fraction of the time that it would occur
in repeated trials.
Something that never happens has a probability of zero.
Something that always happens has a probability of one.
A fair coin has a probability of 50% or 0.5 of coming up heads and so on.
Pascal and Fermat came up with some rules for computing probabilities, and they soon discovered
that the likelihood of an event occurring is sometimes very different than people's
commonly held beliefs about them.
I'm going to just show you two nice examples today to show you how counterintuitive probability
can be.
You may be able to use them to astound, or at least confound, your friends.
I'd like you to imagine that you're with a group of 40 friends or colleagues at a party.
The question comes up of whether any two people have the same birthday.
I'm talking birth date, like May 14th.
How likely do you think it is that in a group of 41 people, two have the same birthday?
There are 365 days in a year, or 366 if it's a leap year.
A lot of people will guess that the chance of two people having the same birthday out
of a group of 41 is around 41 over 365, about one chance in nine, 11%.
Sounds reasonable, but it's a little off.
The chances of a match aren't 11%.
If you have 41 people, the chance that there are two of them who have a birthday on the
same day is more than 90%.
More than 90%.
Try it the next time that you're in a group of around that size, you're likely to amaze
your friends with the results.
In fact, in any group of 23 people or more, the chances are bigger than 50%, but two of
them will have the same birthday.
How can this be?
Well, to answer that, I'm going to teach you one rule of probability, and it's one
that we're going to need for game theory anyway.
It lets you figure out how likely it is that a collection of events all occur.
The basic idea is to find the probability that each individual event occurs, and then
essentially multiply them all together.
If the events don't affect one another, that's really all there is to it.
For example, a recent article in Switched Magazine said that 82% of Americans own cell
phones.
Suppose that you pick an American at random, and so do I.
How likely is it that the person that you pick has a cell phone, and that mine does
not?
Well, there's an 82% chance that your person has a cell phone, according to Switched.
Also, there's an 82% chance that my person has a cell phone, which means there's an
18% chance that they don't.
100 minus 82 is 18.
So the chance that your person has a cell phone and that mine does not is 82% times 18%,
or about 15%.
I could just multiply the probabilities together because whether or not your person had a cell
phone didn't affect whether or not mine did.
In the language of probability, they were independent events.
Our birthday example is a little more complicated, since the events are dependent on one another.
For example, if everybody else in the room shared the same birthday, the chance of me
matching a birthday would now be much smaller than if everybody in the room had their own
private birthdays.
When events are dependent, we need to change our procedure just a little bit.
First, find the probability that the first event occurs.
Then find the probability that the second event occurs, given that the first one did
occur.
Then find the probability that the third event occurred, given that the first two did occur,
and so on and so on.
When you get all these numbers, multiply them all together.
Let's see how this works for our birthday problem.
I'm going to assume that it's not a leap year.
It just is a small complication, and it doesn't change the answer very much.
So I take the 41 people and I line them up in front of the calendar.
I give the first person a pen and I have her mark her birthday on the calendar.
She goes up and puts an X on the date.
Then she passes the pen to the second person who does the same thing, and so on.
This keeps going on until everybody has had a chance to make an X on the calendar, or
until someone goes up to make an X and find out that their date's already been used.
If two people, if that happens, two people have the same birthday and we're done.
The question is, how likely is it that all 41 people get to make their X on 41 different
days?
That is, how likely is it that no two of those people have the same birthday?
Well, the first woman's guaranteed to be able to make her X.
Their aren't any scores crossed out yet.
The chance of her being able to put her X on is 100%.
The second person is fine, as long as his birthday doesn't fall on the one day that's
been crossed out.
There are 365 days on the calendar, 364 of them are still clear.
So his chances of hitting a clear square are 364 out of 365, or 364 at 365ths.
Given that the first two people could put their X's on the calendar, the third person
must now miss the two eliminated squares, which she'll do 363 out of 365 times.
And we keep on going like this until we get to the last guy.
If the game isn't over yet, then 40 people have put 40 X's on the calendar and the last
guy is going to have to miss all 40 of those squares.
There are 325 ways to do this for a probability of 325 over 365.
Okay, now remember the rule.
To find the probability that all of these things happen, take all those probabilities
and multiply them together.
It looks like this.
And when you do it, you get an answer of about .097.
So there's only about a 9.7% chance of everyone getting his or her own square.
That is, there's only about a 9.7% chance that all the people have different birthdays.
Turning that around, it means there's about a 90% chance that at least two people match.
Surprising, huh?
My second example is sure to spark a lively debate among your friends.
It caused quite a storm when it was first printed in Maryland Vos Savants column in
the Sunday Parade magazine in 1990.
The problem's older than that, and it's usually called the Monty Hall Paradox after the TV
host of the old show Let's Make a Deal.
It goes like this, Monty Hall has three doors on stage.
Before the show, behind one of the doors, he's hidden a car, but the door was chosen
at random.
Behind the other two doors, he's put trash.
You choose which door you would like.
Door number one, door number two, or door number three.
After you do, Monty opens one of the two remaining doors and shows you trash.
He can always do this, you see, because there are two trash doors.
If you pick the trash door, he'll open up the other one.
If you pick the car door, he'll randomly choose one of the other two trash doors to open.
At this point, Monty asks you whether you'd like to keep the door that you originally
chose or switch to the other unrevealed door.
As an example, if you chose door number one and Monty showed you trash behind door number
three, you'd then be able to decide to keep one or switch to the still concealed door
number two.
The question is, of course, should you switch?
Or does it make any difference?
What do you think?
I'll tell you, by the way, that most people, well, at least in one study, only 28% of the
people asked this question got the right answer.
I'll also tell you that 10,000 readers, including a couple of hundred math teachers, wrote to
Parade Magazine after this article came out insisting that the right answer was wrong.
Okay.
The answer is you should switch.
If you do, your chances of winning the car are two thirds, up from one third.
The common but wrong answer is that it doesn't make any difference.
People generally feel that after the first door is revealed by Monty, there are two choices
left that each is equally likely to have the car, but this isn't so.
Let's look at how the game plays out, assuming that you switch.
If you start with the right door to begin with, Monty opens up a trash door and you switch
to the other trash door, you lose bad luck.
So if you guessed right to begin with, you end up losing.
But how about if you guessed wrong to begin with, then you start with a trash door.
Monty then has no choice.
He's got to reveal the other trash door and when he does, you switch the car.
So if you're initially wrong, switching will always give you the car.
In short, if you switch, then if you're originally right, you lose.
But if you're originally wrong, you win.
So what are the chances that your original guess was right?
Well, there were three doors and the car was randomly put behind one.
The chance that your original pick was right is one in three or one third.
So if you originally picked the car, which you'll do one third of the time, switching
makes you lose, but if you didn't, which will happen two thirds of the time, switching
will let you win.
If you don't believe it, try it at home using two black cards and a red card.
You might want a friend to play Monty at Monty Hall, but if you play it for a bit, you should
find out that by switching on average, you'll win two thirds of the time.
Counterintuitive, but it's true.
Okay, let's assume that you suss this out and that you always switch.
Let's talk about payoffs.
What's your payoff in this game, assuming that you always switch?
Payoff.
Well, that depends, doesn't it?
If you win the car, your payoff is 30,000, assuming the car is worth $30,000.
If you don't switch, if you don't win the car, your payoff is zero.
So two thirds of the time, you get 30,000 and one third of the time, you get nothing.
To express this as a single payoff, we compute the expected value of your winnings.
Expected payoff is really not a very good name.
Expected payoff sounds like what you'd expect from our game, and we just said that's $30,000.
What happens two thirds of the time?
Unfortunately, expected value doesn't mean this.
Expected value means exactly the same thing as average value.
That is, if you played this game over and over and over and wrote down your winnings after
each game, and then averaged them all, how much would you win per game?
On average.
Well, you win two thirds of the time and lose one third of the time.
So in every three games on average, we'd expect two winners and a loser.
The winners pay $30,000, the loser pays zero.
So if you want to find your average winnings, add them all up and divide by three, 30,000
plus 30,000 plus zero divided by three is $20,000.
So in the Monty Hall game, if you always switch from your originally chosen door, your expected
payoff is $20,000.
You'll never get that.
It's an average.
So it turns out there's a very easy way to compute expected payoff.
You take each possible payoff that you could get, multiply it by the fraction of the time
that it occurs, and then add up all the results.
So that in our Monty Hall example, we put the possible payoffs, 30,000 and zero.
Now I multiply each of these by the fraction of the time that they occur.
And finally, I add them up, $20,000.
That's all there is to it.
We're going to be computing the average or expected payoff a lot.
So remember the simple rule, to find expected value, find each possible payoff, multiply
it by the chance that it occurs, and add up the results.
Well, the Monty Hall paradox is surprising to most people, but it isn't really a game
in the game theoretic sense.
It only has one player.
The other player, if you like, is nature, who decided randomly where the car was going
to be and which door would be opened if you happened to pick the right door to begin with.
Such acts of nature come up a lot, and when they do in a game of perfect information,
well, I handled them basically the way we did here.
And because the outcome of the game depends in part on chance, players are now going to
be trying to maximize their expected or average payoff.
Okay, let's look at a new game and put these ideas to work.
This is very much a game for fun kind of game, but it's a good one to introduce a lot of
our ideas.
It's kind of like rock, paper, scissors, except that each player now shoots either
one, two, or three fingers.
That's it.
Add the two numbers together, and that's the payoff in dollars, like five.
If the total is even, Stephen wins.
If the total is odd, Mod wins.
The loser has to pay the winner.
So if Stephen shoots a one and Mod shoots a two, the payoff is three, an odd number,
and therefore Mod wins.
Got it?
It only takes a second to see that this game doesn't have an equilibrium in pure strategies.
If the total number of fingers is even, Mod's going to want to change.
If the total number of fingers is odd, Stephen's going to want to change.
You can see that the game is zero sum, of course.
The payoffs in any cell add up to zero.
So it's a game of perfect competition.
If the players aren't going to be suckers, they're going to have to keep their opponents
guessing to randomize their choices in some way.
Now in the next lecture, I'm going to show you how to find the best mixed strategies for
each player.
For today, we're just trying to get comfortable with the tools.
So we'll let the players take their own best crack at finding a good strategy.
So Mod looks at the matrix and says, when I play a two, I only lose if Stephen plays
a two.
But if I do that all the time, then he's going to play two every time and win.
So I'll play two, 60% of the time.
I'll play one 30% of the time and three 10% of the time.
I have bad numbers in my three column.
Maybe her reasoning's good.
Maybe it isn't.
But if she's going to do this and Stephen knows it, what should he do?
Well, there's no strategy he can follow that will always give him a win.
He's not going to be able to accurately predict what Mod's choice is going to be in each game.
Mod herself doesn't know what it's going to be.
That's the idea behind a mixed strategy.
What we have to do is to find out what the average payoff for Stephen will be against
this strategy.
The average payoffs are computed in the same way that we just did for the let's make a
deal problem.
Any possible payoff times the fraction of the time that it occurs added up over all
the cases.
So suppose Stephen plays a one.
He's playing his first row.
You can see that from the blue payoffs that 30% of the time he'll make two, 60% of the
time he'll lose three and 10% of the time he'll win four.
His expected payoffs are therefore 30% times two plus 60% times minus three plus 10% times
four or negative 0.8 or negative 80 cents.
Not so good for Stephen.
Against Mod's strategy, playing a one on average loses an 80 cents a game.
Okay, how about if he plays two?
Again, look at the blue payoffs but this time in the second row.
Compute probability times payoff added up.
This time he loses three, gains four or loses five and the calculation comes out 1.0.
We'll make Stephen smile.
When he plays two against Mod's mixed strategy, he'll generally win $1.
Okay, finish it up.
Look at the last row.
What happens when Stephen plays a three?
Again, take each of his payoffs times the probability that he gets them and add them up.
This time the calculation gives negative 1.2.
This is the worst of the lot.
Stephen loses on average $1.20 when he plays three against Mod's strategy.
Okay, let me summarize the results of our calculations.
Here they are.
Is this good for Stephen or Mod?
Well, remember in our analysis, we have Mod following her 30-60-10 strategy for
her three columns.
If she's doing this and Stephen knows it, Stephen's best choice is to always say two.
In doing so, on average, he'll win a dollar a game.
It doesn't matter that the other rows favor Mod or that there are two of those as opposed
to this one or anything else, Stephen gets to choose the row.
Okay, but this was hardly fair to Mod.
If Stephen were playing two all the time, Mod wouldn't want to stick with her current
strategy.
She'd want to play three every time and paste in for five bucks.
So let's turn it around.
Let Stephen pick a mixed strategy and see what Mod wants to do.
Well, Stephen feels that Mod's going to play two quite a lot.
So he decides to play two half of the time to kind of protect himself.
He wants to avoid big losses, so he's going to play one 40% of the time and he'll play
three the last 10%.
Again, I'm not saying that this is Stephen's best strategy.
Let's just see what happens if he does it.
Take a second look at the calculation and make sure that you know what's going on in
this matrix.
For each of Mod's choices, that is for each column, I took her payoffs in that column,
multiplied them by the fraction of the time she got them based on Stephen's mixed strategy
of 40% one, 50% two, and 10% three, and then added them up.
You can see that when she plays a one against Stephen's strategy, she'll win 30 cents a
game on average.
If she plays a two, she loses 30 cents on average.
If she plays a three, again, she wins on average the same 30 cents per game.
So what should she do?
Well, we can say what she shouldn't do.
She shouldn't play two.
Stephen's strategy has built a strong defense to two, and if she plays it, she'll lose
money on average.
But building that defense made him vulnerable to her saying either one or three, and both
are equally good.
If she plays one, on average, she makes 30 cents a game.
If she plays three, on average, she makes 30 cents a game.
She can play either one or any mixed strategy that combines the two.
That will give her 30 cents a game, too.
It's like mixing two kinds of bulk candy that had the same cost per pound.
The total cost of a pound of candy is going to be the same regardless of what particular
proportions of jelly beans and licorice there are in the mix.
All right, let's take a second and see what we've seen today.
If you know the mixed strategy being played by the other player, you should look at each
of your pure strategies in turn.
For each pure strategy, you compute the expected payoff from following that strategy.
Perhaps only one pure strategy gives the best payoff.
That was the case for Stephen.
Two was his best.
In that case, you play that one as a pure strategy all the time, end of story.
But sometimes there may be a tie between two or more pure strategies.
This was the case for Maude.
Every one of those gives the same best expected payoff.
And that's the case.
You can play one of the strategies or another or any mix of them that you want.
All of them will give you the same expected payoff.
That's what happened for Maude.
And believe it or not, we've actually made incredible progress toward being able to find
the optimal mixed strategies.
We're not there yet.
In my example today, as soon as one player learned what the mixed strategy of the one
player was, they'd want to immediately change their own strategy.
That's not an equilibrium.
To find an equilibrium, we have to find a mixed strategy for you so that your mix is
the best against my mix and my mix is the best against yours.
It's not at all clear that you can do this.
But von Neumann showed that you can.
And furthermore, he showed how to do so.
And that's going to be the focus of our next lecture.
And with Stephen and Maude, it turns out that their best strategies are identical and that
this game is completely fair.
