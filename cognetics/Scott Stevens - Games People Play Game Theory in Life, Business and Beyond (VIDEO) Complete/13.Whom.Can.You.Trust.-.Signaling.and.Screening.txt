I
ended my last lecture with a question. If I know something you don't, how can I tell
it to you and get you to believe me? I'm sure you'll agree from personal experience
this is a pretty common problem. It can only come up when we have asymmetric information,
when one of us knows something, but the other one doesn't. The question of how to convey
such information, or to ferret out the information known by somebody else, is the topic of today's
lecture. The activities go under the rubrics of signaling and screening.
Screening occurs when I know something and I'm trying to convey what I know to you.
The flip side of that is screening. Screening doesn't mean putting something behind a screen.
It's screening in the sense of screening applicants. You're trying to get me to tip my hand to
reveal what I know, but you don't. You're trying to pick my brains. The methodology
of both signaling and screening are the same. You look at the actions that a player takes
and hope that they convey something about the information that he or she has.
Signals can occur in a lot of forms. You might have heard the famous Sherlock Holmes quotation
from the story The Silver Blaze. The police had arrested a man suspected of committing
a murder in a horse stable. Holmes was confident that they had the wrong man. He reminded the
police detective of the curious incident of the dog in the nighttime. The detective puzzled
and said, the dog did nothing in the nighttime. That was the curious incident, replied Holmes.
Holmes then pointed out that the dog would have barked when the murderer approached the
stable in the night, unless the murderer was someone that the dog knew well. And the suspected
man was a complete stranger. Scotland Yard had the wrong man.
Here's a great real world example. A company wanted to establish a hazardous waste disposal
site. The nearby community was economically depressed and the additional jobs and tax
revenues to the community would have been a blessing. But there were concerns.
Homeowners were worried that property values would plummet if the site were established.
The waste company assured the homeowners that no, they took every precaution in their operations.
The chance of a negative environmental impact was insignificant. In fact, they said, they
were quite confident that due to the economic stimulus, the property values in the area
would actually rise. If you lived in the community, would you believe them?
Well, they're telling you just what you want to hear, but that doesn't necessarily mean
that they're lying. And if the claims were true, this town would, this would be a great
economic opportunity for the town. But the assurances of the company weren't credible.
The company would want to tell the town spoke exactly the same thing, even if the claims
weren't true. Now someone in the town came up with a great proposal. An independent real
estate firm would assess the current value of each home in the community. Five years
after the hazardous waste site was built, the company would agree to buy out any of
those homeowners who wanted to sell for the price of today's assessment. Oh, no, we couldn't
do that. Said the spokesperson. We go broke. Well, now there's a lovely example of screening.
Suppose the firm had told the truth when it said that everything would be great, then
it would cost them virtually nothing to follow through on the promise. A few residents in
a prosperous community would want to leave in five years? Fine. The company could buy
up the houses and easily sell them at higher price or even absorb the cost. No, the plan
was only going to break the company if there were going to be a lot of houses available
in five years and the houses weren't going to be worth what they were now. These examples
that we've looked at today have been easy in a sense because with a certain response,
you can be pretty much sure of the real state of affairs. But let's go back to the example
from the end of the last lecture to see how we can handle cases that are less clear cut.
You'll remember that we ended with a variant of poker, tiny poker. It went like this. To
begin, I put $40 on the table and you put 70. We then each get one card dealt from a
deck of only three cards with a one, a one and a two. Each of us looks at our own card.
Then I decide whether to quit the game and give you all the money on the table or to
add 70 additional dollars to the pot. If I stay in the game, you look at your card and
decide whether to quit the game and give me all that money or add an additional $30 to
the kitty. At that point, we both reveal our cards. Whoever has the two gets all the money
on the table. If nobody has the two, then we split the pot evenly. Half to you, half
to me. You don't need to keep all of those numbers in your head. They just give you this
decision tree. I ended my last lecture by scratching my head about how to handle a tree
like this. The big problem was the issue of bluffing. If I have a two, I'll always continue
to play since I'll always win some money. And if I quit with a two, I'll always lose
some money. So if you have a two, playing is a dominant strategy. That's true for you
too of course. But how about when I have a one? I might win if I keep playing, but only
if you have a one, two, and then decide to quit. If I never bluff, you should always
fold with a one. Why? Because if I never bluff, the fact that you get a turn is a credible
signal that I have the two. And if I have a two, you should fold. But if you're going
to fold with a one, then I should always bluff. This sounds like just the kind of second guessing
that we saw in our mixed strategy games. And that's actually the way that we solve games
like this. The key is every sequential game can be recast as a simultaneous game, saying
it another way. If you can represent it in a game tree, you can represent it in a matrix.
How? The easiest way to explain this is to go back to the image that I introduced earlier
in the course, the image of a strategy library. Each book in the library was a complete strategy.
Once that book was selected, you really don't have any other choices to make. You could
imagine each game being played out by each player selecting a book from their respective
strategy libraries. The book's instructions could then play themselves out until the game
was over. This means that you could represent any two-player game as a matrix. The rows
of the matrix would correspond to the books in the first player strategy library, and
the columns of the matrix would correspond to the books in the second players. So in
our tiny poker, what books are available? What situations could I find myself in? Well,
when I look at my hand, I could see a one or a two. Those are the only things that could
happen, my two information sets. And based on what I see, I have to decide whether to
play or fold. So I have four strategies. Always play, always fold, play with the other
one, but not a two, or play with a two, but not a one. How about you? Well, you too can
find yourself in either of two situations. If you ever get a turn, it's because I did
in fact decide to continue. So your two sets are, I look at my hand and see a one, or I
look at my hand and see a two. And just like me, you're going to have to decide whether
to bet or fold in each of those cases. That means that the matrix is going to look like
this. There are 16 squares in this matrix, but we can do better than this. As we've already
said, it's never sensible for either of us to fold with a two. Folding with a two is
dominated by playing. If the game goes to the end and you have a two, you win everything.
If the other person quits, you still win. But if you quit, you lose. Since winning is
always better than losing, it's better to play with a two than fold with a two.
Okay. Let's look at the matrix. Ah, there now. That's more manageable. In filling in
the results, there's still one complication. We don't know for sure what will happen when
my strategy meets yours, because it depends on where the two is. Because of this, the
payoffs that we put in this matrix are actually going to be expected, or average payoffs.
Let's look at the lower right cell, where both of us always bet. Then the game is going
to go all the way to the end, and $110 will be in the kitty. I've contributed $110, and
you've contributed $100. What happens then? Well, one third of the time, I'll win, because
I have the two. One third of the time, you'll win, because you have the two. And one third
of the time, nobody has the two, and so we split the kitty evenly. Okay. Let's compute
my average payoff. A third of the time, I win and gain your $100. A third of the time,
I lose, and lose my $110. And the remaining one third of the time, I lose $5. That's because
I put in $110 while you put in $110 more than you, so when we split the money, I only get
$105 back. Huh. Not so good for me. If we both go all the way, then on average, I lose $5.
On average, I lose $5. I'll put this in our matrix. Again, note how the presence of chance
forced me to change the payoffs to expected payoffs. We can work out the other cells using
the same approach. I'll do it for you, and you can verify it if you like. These values
really drive home that we really are talking about average payoffs. Obviously, I'm never
going to lose $3.30 in any given game, but if neither of us is going to keep playing
with a one, which is what the upper left hand cell says, then I lose my $40 whenever I have
a one since I fold and you never get a turn. You lose your $70 kitty whenever I have the
two. So the expected payoff to me is two thirds times negative 40 plus one third times 70,
which works out to negative three and a third. And that's what we put in the matrix.
Okay, the matrix is full and we're now on familiar turf. This is a two by two simultaneous
game. It's also obviously a zero sum game since the winnings of one player come from
the losses of the other. Let's use our little trick for two by two games to find the optimal
strategies. Remember, take the difference of the payoffs in one column and write it over
a top of the other column. The arithmetic is a little ugly. Do the same for the rows.
For example, the two red payoffs in the bottom row differ by 15. So we write 15 beside the
first row. The two red payoffs in the top row differ by 10. So we write 10 beside the
bottom row. 10 plus 15 is 25. So we divide both the 10 and the 15 by 25 to turn them
into probabilities. We'll do the same for the column differences. And we've solved
the game of tiny poker. As we know, I should always bet with a two. But if I have a one,
the bottom row says I should bluff 40% of the time. The rest of the time I should fold.
How about you? Well, like me, you should always bet with a two, of course. But if you have
a one, and I just bet, you should fold 7 15ths of the time, just under half. The rest of
the time you should bet. It only costs you $30. And it means you keep me honest, at least
to the extent that it's best for you to do so. You keep me from bluffing too much. So
who does this game favor? Well, we know from our study of mixed strategies that every row
and every column that actually get played are going to give us the same expected payoff.
That's because we saw that a player only mixes among those options with which they're indifferent.
So let's figure out the average payoff by looking at the bottom row, since the numbers
are the easiest there. When I play that row, I get $10, 7 15ths of the time, and lose $5,
8 15ths of the time. That makes my average payoff 7 15ths times 10 plus 8 15ths times
negative five or two. That is on average. If I play my optimal strategy, always bet with
a two and bluff with a one 40% of the time, then I'll take you for on average $2 a game.
More if you sometimes are foolish enough to fold with a two. This is really the approach
that we take with any sequential game with imperfect information. Fortunately, you don't
have to do all of this stuff by hand. All of the decision trees that I've shown you
so far in this course were created by using a piece of freely distributed software called
Gambit. I included the website information for getting the software in the course booklet.
Gambit lets you easily represent a game tree or a matrix. It can then do the number crunching
you did to find the equilibrium of the problem. Gambit will sometimes choke on more complicated
problems. What do you want for free? But it does a very nice job with smaller problems.
It took my tiny poker problem and solved it in under a second without breaking a sweat.
Before we leave tiny poker, I'd like to remind you of a question that we brought up in the
first lecture. It was Von Neumann's question. Can something so quintessentially human as
bluffing be put on a solid mathematical footing? The answer as we've seen is yes. In tiny poker,
we've seen that I should bluff 40% of the time when I'm holding a one. And how often
I should bluff in a game like this in general would depend on the payoffs. As an example,
let's change one aspect of the game only the size of your last bet. It's currently $30.
If you work it out, you'll find that if your last bet is zero free, then I should never
bluff since it costs you nothing to keep me honest. Between $1 and $60, I should sometimes
bluff and do so with increasing frequency as the size of your last bet goes up. If your
bet is $60 or more, it turns out I should always bluff and you should always fold with
a one. So how does all this tie in with what I began with today? The question of signaling.
Well, in the tiny poker game, it's very easy for me to credibly signal to you that I have
a one. I can fold. If you see me folding, you can be sure that I have a one since it's
terrifically stupid to fold with a two. On the other hand, it's much more difficult for
me to credibly signal to you that I have a two. If I don't fold, there's always a chance
that I have a one and I'm simply bluffing. Watching to see if I fold gives you some information
about what card I'm holding, but it doesn't tell you everything. In fact, if you work
out the math, you can determine that if I bet and you have a one in your hand, there's
a 29% chance that the card that I'm holding is also a one, but that's really the best
you can do. In the language of game theory, tiny poker has a semi-separating equilibrium.
When you saw one action on my part folding, you'd know what type I was. The guy was holding
a one. If you saw the other action though, my betting, then I could be of either type.
A game can also have a separating equilibrium in which each type takes different actions.
Our waste disposal company example was one of those. A company that was telling the truth
would agree to the agreement. A company that wasn't would not. Other problems, each type
can respond in exactly the same way so that what they do gives you no information at all.
This type of an equilibrium is called a pooling equilibrium.
Okay, these have all been examples of screening. In screening, you try to figure out what the
other player knows, but you don't. Greek mythology is full of such stories. James Miller and
Debbie Felton wrote an article collecting some of them. One of the nice ones deals with
the Trojan War. Helen, the most beautiful mortal woman, had many suitors, so to avoid
strife, they all pledged to help Helen and whatever man she chose for her husband. The
man that was picked ended up being the red haired Menelaus, and then Helen was kidnapped
and carried off to Troy. So, Menelaus started calling in debts, recruiting those that swore
to help on their voyage and war to recover Helen. Well, not surprisingly, this idea didn't
sound like a real winner to Odysseus, one of the men that Helen had passed over. He
really didn't want to go to Troy. He had married since his Helen days and now had a
bouncing baby boy named Telemachus. Odysseus is almost always referred to in the Iliad
as skilled tactician. He was a clever fellow. He was the guy that came up with the Trojan
horse idea. So, when Odysseus heard that Menelaus and his buddies were coming, he thought about
what he could do. It appears that he was less worried about breaking his vow than he was
about his fellow Greeks thinking that he was a creep. Well, to be fair, he was given a
prophecy that if he went to Troy, he wouldn't get home for 20 years and all of his friends
would be dead. So, maybe you can't blame him for wanting to be counted out of this particular
expedition. Anyway, Odysseus' solution was to feign insanity. He started plowing his
lands in random corkscrews, sowing stolt instead of grain, and just generally acting
like a lunatic. And when Menelaus and the others arrived and watched his performance,
their reactions varied. Some were pretty sure that Odysseus was faking it, but what could
you do? If you said to Odysseus to his face that he was shaming, he could just babble
and drool on you and then go back to plowing figure eights. So, what happens? Well, what
happens is that there's one particularly clever guy named Menelaus, a guy named Palamides,
who was almost sure that Odysseus was faking. So, Palamides goes and gets the baby Talinicus,
Odysseus' son, and throws him in front of the plow. He cries out something appropriately
poetic, put aside your deception and come with us. Now, if Odysseus doesn't turn aside,
the blade will cut the kid in half. This is a screening game. Nature chooses which type
of Odysseus we're dealing with, sane or insane. Odysseus knows which one he is, in the story
he's sane. Palamides creates a situation where the sane Odysseus will respond differently
than an insane one, a separating equilibrium. Either type of Odysseus can continue to plow,
but their costs are different. An insane Odysseus will have a minimal cost. He won't even know
what he's done. But for the sane Odysseus, killing his son is more costly than going
off to war. So, not surprisingly, he stops and the jig is up. Odysseus goes off to Troy
and it is 20 years before he gets home. Bad luck.
You can find examples of signaling in Greek mythology too. When the wine god Dionysus
is captured by pirates, they believe him to be a king's son, so they decide to hold him
for ransom. They tie him to their mast with strong ropes. Dionysus makes the ropes untie
themselves and fall to his feet. Since this is not something that your average mortal man
could do, Dionysus has credibly signaled that he's a god. The ship's helmsman realizes
this and he warns his shipmates to honor the god. They didn't, of course.
The story ends with Dionysus turning all of them into dolphins, except the helmsman.
Important safety tip. Never ignore a credible signal from a god.
You can think of lots of signaling games in legend and mythology. The Sword of the Stone
can only be drawn by one who is right wise king born of all England. Assuming that the
inscription is credible, Arthur's drawing of the sword signals that he is that king.
Signaling occurs in nature too, in interesting ways. Here's one of my favorites.
When a cheetah is stalking a Thompson's gazelle in East Africa, there's a chance that the
gazelle will do something called stodding. That is, when it sees the cheetah, the gazelle
jumps straight up, stiff-legged, in the air. It'll run away after this, but it's clearly
made an action that makes it visible to other predators and does nothing to get it further
away from the cheetah. What's it doing?
A likely explanation is that it's signaling. Cheetahs are very fast, but not distance runners.
Cheetahs are speedy too, and agile. A gazelle that starts is sending a credible signal.
A sick gazelle couldn't jump like this. I'm perfectly healthy. If you chase me, we're
both going to waste a lot of calories, and you're not going to catch me. So let's not
bother.
In point of fact, cheetahs often do give up after a gazelle starts. And no gazelle that
has started and then been chased has ever been seen to be caught by the cheetah. Isn't
that wild?
Charles Darwin puzzled over why some species, especially the males of the species, would
adopt such elaborate ornamentation. The peacock's tail is probably the most obvious example.
Others are bright plumage that makes birds easier for predators to see, like cardinals,
or complicated nest-building displays, like bower birds.
The best theory to date about what's really going on was proposed by Amot Sahavi.
Sahavi proposed that the ornamentation is a deliberate handicap used as a signaling device.
It essentially says, I am so fit, such a strong male, such good genes, that I can accept this
sizable handicap and still survive. The support for this model has actually been growing over
the last 20 years.
You think this sounds silly? Let me ask you then. What is the purpose of an ad that simply
tells you Coke is it to inform you, to persuade you? Is there anyone who didn't already know
about Coke, or that didn't know that it would be the appropriate pronoun to use?
What the Coca-Cola Company does with such commercials is to signal, we're here to stay.
We are so strong that we can afford to blow wads of cash on these commercials. A company
that was weak, or shoddy, or about to disappear, or not with it, wouldn't do it.
Here's another business example of signaling. Simplify just a bit to make it fit in the lecture.
For the next two months, you're going to be running a summer groundskeeping business.
We'll call you a gardener. There are two kinds of gardeners, good ones and bad ones.
Three quarters of a gardener is going to be low quality, one quarter high quality.
Low quality service costs the gardener $300 a month to provide. High quality service costs
the gardener $500 a month. But low quality service isn't very good. It's only worth $200
to the customer. High quality service is worth $1,200 a month to the customer.
We'll assume that the quality of service provided by a gardener is the same in both months,
and that the customer makes a decision each month on whether to employ the gardener.
We'll also assume that a gardener can make an introductory offer during the first month
to effectively set different prices during the two months.
What happens? Well, if the potential customer could choose a gardener at random, he could
afford to pay three quarters of $200, plus one quarter of $1,200, or $450 per month.
The trouble is, a high quality gardener won't work for that. Her expenses are $500 a month,
however, and he doesn't want to hire a low quality gardener because they're only worth
$200. The market for lemon strikes again, this time with a vengeance. Neither kind of
gardener can get hired. So is there any way that you can work and make a profit? Yes.
You can use your prices to create a separating equilibrium, signaling to the customer that
you're high quality. Here's how.
Being a low quality gardener costs $300 a month to do, right? So you set your introductory
price at $290. No low quality gardener is going to do that. It loses them $10 in the
first month and they'll never be rehired in the second one. So you're willing to work
for $290, but why would you? Because after the first month, the customer is going to
know that you're a high quality gardener and that your services will be worth $1,200
a month to them. In the second month, you can charge, say, $1,190. So for the two months,
you make $1,480, which is $480 over your costs. Here's the funny part. The very day that
the customer pays you that initial $290, he knows your high quality and that he's going
to hire you for the second month too. But if you offered him a two month contract on
that day for the $1,480, he wouldn't take it. Why? Because he wouldn't know that you
were high quality.
Okay. I want to close today's lecture with two examples from a movie that has more game
theory in it than any other one that I know, The Princess Bride. For those of you who haven't
seen it, the movie is a comedy adventure movie with a wickedly ironic twist. Our first scene
is the battle of wits to the death to save the life of the princess who's kidnapped by
the bad guy. Wesley, our hero, has a tube of poison powder. The kidnapper, a self-styled
mastermind by the name of Vizini, watches with interest as Wesley takes the two wine
goblets, turns his back and pours the poison into the wine. Vizini has to decide which
goblet he wants and then both will drink from their respective goblets. This is obviously
a game of asymmetric information since Wesley knows where the poison is and Vizini doesn't.
But it's so simple, Vizini cries and then sets into a bewildering display about common
knowledge. A clever man would put the poison into his own goblet because he would know
that only a great fool would reach for what he was given. I am not a great fool, so clearly
I cannot choose the wine in front of you. But you must have known I was not a great
fool, you would have counted on it. And so clearly I cannot choose the wine in front
of me. Wesley realizes that Vizini is trying to screen him. You're trying to trick me into
giving away something. It won't work. Well, eventually Vizini does choose and dies from
drinking the poison. It was obviously that he was playing a game of asymmetric information.
What he didn't realize was that he was also playing a game of incomplete information.
Wesley comes in two types, you see, and one of them had built up an immunity to the poison.
So this type had a different option available to it, putting the poison in both govets.
Wesley didn't want to be screened by Vizini, so he used a poker face, signal jamming.
Toward the end of the movie we see another example of this. Wesley has been paralyzed
and is unsure of, we're unsure about whether he can even move. He's lying on the bed of
the evil Prince Humperdink in his bedroom when the prince enters. Now, Humperdink would
have no scruples about killing a helpless Wesley. He's that kind of guy. But a healthy
Wesley is more than a match for him. So he wants Wesley to reveal his type. Wesley lies
on the bed saying that it's possible he's only lying here because he's too weak to stand
up or maybe, maybe he's perfectly fine and the prince should surrender before Wesley
does unspeakable things to him. Wesley is signal jamming. He's creating a pooling equilibrium
where either type of Wesley does the same thing. Humperdink's choice has to be based
on the weighted average of the outcomes of a healthy Wesley and a helpless one. The prince
seems to be wavering. Maybe he's just a bit slow at computing expected payoffs. Wesley
breaks the deadlock by standing, coming on guard and resolutely ordering the prince to
drop his sword. Which Humperdink does. Wesley then collapses, but it's too late. The prince
has been captured, trying, I knew it. I knew you were bluffing or the like, which I'm sure
you will agree is not credible. What happened? Well, when Wesley stood up, the prince assumed
that the game had a separating equilibrium. Wesley's standing had signaled that he was
actually healthy and so the prince surrendered. But in fact, the equilibrium was a pooling
one and in the pooling equilibrium, both the strong and weak Wesley's would stand. Ah.
If only the bad guys had studied game theory.
