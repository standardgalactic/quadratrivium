Our work in game theory so far is focused almost exclusively on non-cooperative games.
These are the games in which players can't make binding agreements among themselves.
Today I want to take a look at cooperative games.
Cooperative games naturally involve coalitions of players, and a cooperative game, each subset
of players, can work together to create or capture a certain amount of value, a payoff.
Most games are generally super additive.
This means that if you take a larger group and break it down into two smaller groups,
the payoff that these groups can capture isn't any bigger than what the big group could do
to begin with.
In general, the sum is at least as big as its parts.
This raises the possibility that players working as a grand coalition, with everybody
included, can capture the maximum value.
The question then becomes, how much of that value does each player receive?
Here's an example.
Three companies each have a company plane.
They've realized that it would be worthwhile to build a small private airport rather than
pay the costs at the public one.
Short lines plane only needs a short runway, medways plane needs a medium length runway,
and long fellows needs a long runway for its plane.
The cost of building and operating an airport is $90,000 for a short runway, $150,000 for
a medium runway, and $240,000 for a long runway.
There are obvious advantages to the companies working together.
A coalition of companies can build an airport for the most demanding partner, and then all
of the companies in the coalition can use it.
A table here shows the cost of the various possible coalition airports.
The obvious solution is for the three companies to share one airport.
But a moment's reflection will make it clear that that solution isn't a solution at all.
How much does each company pay toward the construction of the airport?
Well, they could share the costs evenly, paying $80,000 each.
After all, that's less than what anyone would pay to build their own airport.
True.
But then short line and medway together are paying $160,000.
They'd be better off to build their own airport with a medium runway for $150,000 and leave
the grand coalition.
You can do reasoning this way to determine the most that any company could be required
to pay for the airport before they'd break off and make a better deal someplace else.
When you've done so, you've identified the core of the game.
The core is the set of all ways to distribute the costs for the airport among the three
companies so that no company, or group of companies, has a reason to break out of the
grand coalition.
It's the fact that the allocation gives no subgroup or reason to leave that makes it
part of the core.
Unfortunately, this concept of the core isn't as helpful as we'd like.
In our current game, there are a lot of different allocations that are in the core.
For example, longfellow could pay for the airport by themselves and everyone could use
it.
It's in the core.
There's no way that anybody can do better by breaking away.
At the other extreme, short line could pay $90,000, medway could pay $60,000, and longfellow
could be paid the remaining $90,000.
The core you see in cases like this doesn't tell us much.
In fact, sometimes it's worse than that.
A game may not even have a core.
Just imagine that you and two of your friends are discussing how to divide a pile of money,
$3,000.
A proposal is made for dividing the money, and a majority vote, two out of the three,
is required to approve the proposal.
In this game, singleton coalitions, only one player, don't have any power at all.
They can't guarantee themselves anything.
On the other hand, any coalition of two players is a majority, and they can guarantee themselves
everything, majority rules.
So you and Kate agree to take $1,500 each, leaving Chris with nothing.
You can get this with a majority vote.
But wait, Chris says to you, let's you and I split the $3,000.
I'll let you keep $2,000 and only take $1,000 for myself.
Well, that's better for you and for Chris than the previous deal was, and so you agree.
And then Kate says to Chris, wait, I'll tell you what, make a deal with me and I'll split
it evenly with you.
You see, every division of money leaves open the door for two players to make a deal that's
better for either of those players than the one that currently exists.
Two players is a majority, and it's all that you need.
Well, let's set aside for the moment the question of how these deals can be enforced.
Let's try to get a handle on what kind of a division might be reasonable or fair.
One of the best-known proposed solutions to this question is the shapely value, named
after Lloyd shapely, who introduced it in 1953.
Here's how shapely would decide how much value you have in the game, how much of the
total pie that you deserve.
I'd like you to imagine that we build up the grand coalition of all members by adding
members to the coalition one at a time.
I want to focus on the step when you get added.
How much more is the growing coalition worth with you than without you?
That's your value added.
Your value added is going to depend on who's already in the coalition with you at the time
that you were added to it.
So imagine repeating this process, building up the grand coalition by adding players in
every possible order.
Your shapely value is just the average of your value added in all of these possible processes.
We'll demonstrate this with the airport example.
I'll abbreviate the forms with S, M, and L for short line, medway, and longfellow.
I've expressed costs in thousands of dollars.
The grand coalition of all three companies could be constructed by adding the companies
in six different orders, and I've given each a row.
Look at the first.
The companies are being added in the order of short line, then medway, then longfellow.
The singleton short line coalition costs $90,000 for the short runway airport.
Adding medway to the coalition increases this cost to $150,000.
That's a $60,000 increase.
Finally, adding longfellow to form the grand coalition raises the price from $150,000 to
$240,000, a $90,000 increase.
In the second row of this table, the grand coalition is built by short line, then longfellow,
then medway.
Short line still costs $90,000 for the short runway, but the second step is now the addition
of longfellow.
They need a long runway.
The cost of the long is $240,000, which is a $150,000 increase from the cost for short
line alone.
Longfellow incurs this.
Adding medway to the pair incurs no extra cost at all.
We'll keep proceeding like this for every possible ordering.
In the last row, we find the average increase caused by short lines joining is $30,000.
For medway, it's $60,000, and for longfellow, it's $150,000.
These are the shapely values, shapely's proposal of how much each company should pay toward
the construction of the airport.
Fullchild and Owen, who first introduced and analyzed problems like this, pointed out
that shapely values here could be interpreted in a different way.
All three companies need at least a short runway, so the price of the short runway,
$90,000, is divided evenly among all three of them, $30,000 a company.
Two of the companies require the expansion from the short to the medium runway, and
the expansion costs $60,000.
So those two companies divide that $60,000 evenly, each paying $30,000.
Finally, the last expansion from medium to long costs an additional $90,000.
Only longfellow needs it, so only not longfellow pays.
Take those contributions, add them up, and you get the shapely values.
How about the three-way division of the $3,000 and the majority rule problem?
Here, you increase the value of the coalition only if you're the second person to join it.
The first person has no power, and the first two people create a majority and control all
$3,000.
One-third of the time, you're going to be the second person added.
So your shapely value works out to be one-third of $3,000, or $1,000.
The solution is that you each get $1,000 cash.
That certainly seems to be a sensible solution for this symmetric game.
The shapely value is tedious to calculate for a large number of players, but it does
have a host of desirable properties.
First, it's efficient.
The total of the shapely values of all the players always equals to the total size of
the pie.
And if two players are always making the same difference when being added to a coalition,
they always have the same shapely value.
If you never add anything to the value of a coalition, then you join, then your shapely
value is always zero.
And every player gets at least as much from their shapely value as they would have gotten
if they had acted independently from a coalition.
The shapely value is important in legislatures, with more than two parties, given that it
takes a majority to get a piece of legislation passed.
In this context, the shapely value is usually called the power index.
Take the simplified model of the parties will vote as blocks, that all the members
of a given party will vote the same way.
In the 109th Congress, the Senate consisted of 55 Republicans, 44 Democrats, and one independent.
If you crank out the shapely value, you'll find that it says that all of the power is
with the Republicans.
And this is exactly correct, assuming that each party votes as a block.
Bills will pass the Senate if and only if the Republicans decide that they should.
In contrast, look at the 110th Congress, 49 Republicans, 49 Democrats, one independent,
and one independent Democrat.
Calculate the shapely values and you'll find that there are about 33% for each of the two
major parties, and about 17% for each of the two remaining congressmen.
This ignores the tie-breaking rules of the Senate, and it does assume that parties are
voting as blocks.
But it's revealing nonetheless.
By shapely value, a single independent senator has half as much voting power as an entire
Republican or Democratic party.
And this does make sense.
If those two independent senators decided to vote as a block, they would be as important
as the Republicans and the Democrats in deciding legislation.
Legislation would pass if and only if two of those three groups consented to it.
Shapely wasn't the only one to try to resolve the question of equitable distribution of
spoils in a cooperative game.
Earlier, John Nash turned his hand to it and came up with the Nash Cooperative Bargaining
solution for games with two players.
As an example, suppose that I can make $6 on my bottle of Chateau Stevens wine and you
can make $3 on each block of your exotic cheese.
But together we can market them as a wine and cheese pack and make a profit of $18.
Well, clearly it's in our best interest to do so.
We get an additional $9 profit per set.
Question is, how do we split it up?
Nash looked at the game in which we specify each how much of the money we want to get.
If the claims that we make are $18 or less, we each get what we asked for.
If they add up to more than $18, the deal falls through.
In deciding what to do here, the Nash equilibrium isn't going to get us very far.
Suppose that you asked for $17 and I asked for 1.
The total doesn't exceed $18, and so we each get what we asked for.
Neither of us can do better by unilaterally changing our decisions, so it is a Nash equilibrium,
but it hardly seems fair.
On the other hand, what is fair isn't quite clear.
There's an $18 profit surplus, so a first thought at fairness might be that we split
it down the middle and each get $9 of profit.
But remember, on my own I was getting $6 profit and you were getting $3.
These are referred to as our batoness, our best alternatives to negotiated agreements.
Batoness are also called backstops.
Your batoness is what you can walk away from the deal with and still keep.
So if we're going to divide the $18 profit down the middle, you gain $6 over your batoness
and I only gain $3 over mine.
Your gain from our deal was twice as big as mine.
I could argue that since my original profit margin was twice as great as yours, I should
get two thirds of the current profit, or $12.
You should get $6.
Nash was interested in finding good, defensible answers to how this kind of dilemma should
be resolved.
In some ways, his approach was similar to the work done by Aero that he did on voting
theory that we talked about last time.
He started with a list of properties that a good division should have and then looked
to see what possible division system would meet this requirement.
First, he wanted a solution that would be efficient.
It should divvy up all the profit between the two people.
Secondly, he required that changing the units of measurement wouldn't change the division.
If we looked at profits in dollars or cents or pounds, we should still get the same answer.
And third, just like Aero, he required the independence from irrelevant alternatives.
If a particular division wasn't going to be used anyway, then taking it off the table
as a possibility shouldn't change the division that he's agreed to.
Just like in voting, that last requirement isn't as trivial as it sounds.
Imagine that you and I ran gas stations and that we were going to divide a thousand gallons
of gas available from the distributor.
Either we agree on how much each person gets or no one gets anything.
I don't think it would take long for us to come up with the idea of a 50-50 split, 500
gallons each.
But suppose we walk out to the parking lot and you see that my truck won't hold more
than 500 gallons, while yours could hold the entire 1,000 gallons.
That is, any deal where I get more than 500 gallons is actually impossible.
What Nash is saying is that this new information can't change the right deal that we should
strike.
These three conditions pretty much specify how the division has to go.
If you assume that both players have the same bargaining power, and Nash did, then Nash
said that you have to take the surplus over the batons and divide it evenly between the
two players.
In the wine and cheese examples, you could make $3 on your own and I could make $6 on
my own, and those are our batons.
Another, our profits are $18.
That's a surplus of $9 over what we could make on our own.
Nash said that $9 gets divided evenly, 450 each.
So you get 750 profit on your cheese, your $3 baton plus the 450, and I get 1050 profit
on my wine, my $6 plus the 450.
It's worth noting that if you could do better in a deal with somebody else at all, you can
raise your baton and thereby do better in our deal.
Let me be clearer.
Suppose that we imagine that you can set up a potential deal with another winemaker that
would give you $4 profit per cheese block, then your baton has increased to $4.
It's increased by $1, and that'll give you an extra 50 cents in our deal.
This Nash division is also what you get when you compute the shapely values for this problem.
But the Nash cooperative bargaining solution has an additional nice feature.
It can be extended to cases where the two parties don't have equal bargaining power.
Nash's approach says that the surplus above the total of the baton of the two players
should be divided in the same proportion as their bargaining power.
So in our wine and cheese example, if I have twice the bargaining power that you have,
I should get twice as much of the surplus.
By working together, our joint profits jump from $9 to $18, a surplus of $9.
I get twice your bargaining power, I should get six of that $9, and you should only get
three.
I get twice as much as you.
But this rather begs the question, how can we determine the bargaining power of the players?
And even if we can determine that, how do we get these players to accept the cooperative
solution?
You see, we've been talking about what would be an equitable division, but we have no idea
of how to get there.
We can get a better grip on these issues by returning to the non-cooperative games
that we've been studying, and in particular, our third lecture on sequential games of perfect
information.
A bargaining situation can be modeled as such a game as haggling.
One player proposes a division of the surplus to the other who accepts it or makes a counter
offer.
This continues back and forth until a deal is struck.
But why would a deal ever be struck?
When I make you an offer, why not just ignore it and make a counter offer that's better
for you?
If I refuse it, then we're right back where we started, nothing lost.
Well, in reality, something is lost.
Waiting has a cost.
Maybe negotiations will simply fall through after an unknown number of offers and counter
offers, from frustration, fatigue, alternative authors, or even death.
Maybe haggling could go on forever, but both parties could be losing money because of the
delay in reaching an agreement.
The money from the settlement could be invested with interest, for example.
Or maybe the bargaining won't go on indefinitely because the value of the surplus explicitly
decays over time.
I'm going to look at that one first.
Suppose your business makes plastic jugs, and you've received a call from your supplier
of plastic beads from which you make them.
Her equipment just broke down, and you won't be able to get beads for the next three days.
Your inventory of beads right now is pretty much exhausted.
You need a new shipment.
Not having beads is going to lose you $11,000 per day in revenue.
Well, I'm another supplier of plastic beads, and I have all the beads you need for the
next three days.
A day's worth of beads, including delivery, costs me $1,000.
We'll model the situation a bit simplistically this way.
You call me up today and make me an offer on a day's worth, three days worth of beads.
I either accept it, or I call you back tomorrow, and make a counter offer on beads for the
remaining two days.
If I make that counter offer, you either accept it, or call me back the next day with a counter
offer on one day's worth of beads.
If I refuse that, we have no deal, of course.
Let's look at the game tree.
You'll notice a difference between this tree and our earlier ones.
The offers are being represented as fans.
This is because offers and counter offers can actually be any value at all.
Fortunately, this fact isn't going to affect our rollback procedure.
Note that the first fan is an offer on three days' worth of beads, the second one's on
two days, and the third one's on a single day's worth of beads.
Okay, let's roll it back, starting at the end.
The final decision that I have to make is considering your offer of C dollars.
Since a day's worth of beads is worth $1,000 to me, I have a choice between nothing, if
I refuse your offer, and a profit of C minus $1,000, if I accept it.
So I'm only going to accept your offer if you give me at least $1,000 and change.
Giving me $1,000 plus leaves you with a surplus of almost $10,000, so you'd rather have me
accept your deal than refuse it.
So your best value for C is $1,000.
Actually, just over $1,000.
To keep the discussion simple, I won't keep on saying just over.
I'll say that you offer $1,000, and we'll both know that I mean a tiny bit more than
this.
Okay, roll back another step.
How big should be be?
Well, with your final offer, if you've made it worth my while, you're going to be pocketing
$10,000.
So if I want you to accept my offer, I have to leave you with more than $10,000.
I can do this by setting B at $12,000 and change, but I'm not going to bother saying
that every time.
If you accept my $12,000 asking price, I make $10,000 in the deal, which is certainly better
than the tiny payoff I get if you don't accept my offer.
So B is $12,000.
Okay, back up another step.
How big should A be?
Well, in order to get me to accept your original offer, you have to leave me with more than
the $10,000 I'm going to get by making a counter offer that you accept.
You can do this by offering me $13,000 and change at the first stage.
Making this offer gives you a surplus of $20,000, which is better than you get by having your
offer refused.
So the rollback shows that the optimal play is that you offer me just over $13,000 originally
and for me to immediately accept it.
This outcome is interesting for a number of reasons.
First, as is always going to happen in bargaining solutions with diminishing surplus, the deal
will actually be struck on the very first offer.
Waiting simply results in a smaller pie to divide and that means less for everyone.
The fraction of the pie that's going to each player is determined by what would have happened
if the haggling continued.
Notice how our surplus is divided.
Originally, the value of the beads to you is $30,000 higher than its value to me.
We're bargaining over the division of that $30,000 surplus.
And look how it worked out.
With your accepted offer of $13,000, you get to keep $20,000 of the surplus while I only
get $10,000.
In terms of Nash bargaining solutions, you have twice the bargaining power that I do.
Why?
What gives you the advantage?
Well, here's a cute trick to figure out the rollback equilibrium of any such shrinking
pie bargaining game.
The actual deal, as we said, will be struck on the first round with the offer being accepted.
But to figure how large that offer should be, imagine that the haggling goes on to the
bitter end with refusals at every single step until nothing is left.
With each refusal, more and more of the surplus is destroyed.
Add up all of the surplus destroyed by one player's refusals in this hypothetical game.
That amount is the surplus awarded to the other player in the actual first round of the deal.
In our problem, each day's worth of beads has a $10,000 surplus.
It's worth $11,000 to you and $1,000 to me.
So when I refuse your day one offer, I destroy one day's worth of surplus, $10,000.
Your refusal of my second day counter offer would destroy $10,000 as well.
And if I refused your final offer on day three, going to the bitter end, I would destroy
the last $10,000 worth of surplus.
Since I would destroy $20,000 worth of surplus in this haggling, and you destroyed only $10,000,
you get to keep $20,000 of surplus while I only get $10,000.
Isn't that simple?
Okay, let's look at another kind of diminishing returns.
Even when the surplus to be divided is really unchanging, the bargainers would still prefer
a speedy settlement.
Maybe for financial reasons, each person could invest their share of the surplus once they
obtain it, so time lost is money lost.
There could be other reasons for impatience, dislike, wasting time, pressing financial obligations,
a personal deadline, whatever.
Whatever the cause, I'd like to look at the effect of such impatience in bargaining, especially
in the case where the two parties have different levels of impatience.
Impatience means that there's a cost to waiting.
It means that a dollar next round isn't as good as a dollar now.
Let's say that for me, one dollar in a given round is as good as one plus R dollars in
the following round, while to you, one dollar now is as good as one plus S dollars in the
following round.
For example, if S were .05, then for you, a dollar five in the next round is just as
good, no better, no worse, than a dollar right now.
Saying this another way, one dollar next round is only as worth one over a dollar five for
you right now, or about 95 cents.
Okay, I'm also going to suppose that the amount to be divided between us is a hundred dollars
just to fix ideas.
We know that the actual deal is going to go through on the very first offer.
The size of that offer depends on who goes first.
Let's say that if I make the first offer, I receive A dollars, and you get a hundred
minus A.
While if you go first, you get to keep B dollars, and I get a hundred minus B.
The question of course is, how big should A and B be?
Well, to get you to accept my original offer, it has to be at least as good as what you'd
get by refusing it, and then having me accept your offer.
We'll measure these values in terms of present value.
That is, getting one dollar next round is only worth one over one plus R to me right
now, and getting one dollar next round for you is only worth one over one plus S to you
right now.
So if I propose keeping A dollars, I'm offering you a hundred minus A right now, and if you
reject this offer, and I accept your offer, then you get to keep B dollars, but that'll
be next round.
B dollars next round is only worth B over one plus S to you right now.
To get you to accept my current offer, I have to offer you at least what you'll be getting
by waiting.
So the value of my offer to you now has to be greater than or equal to the current value
of what you'll get from your next round offer, or one hundred minus A has to be greater than
or equal to B over one plus S.
To be assured that you take my offer, I'd need to make the offer a tiny bit bigger than
what you'd get from your Tanner offer.
As before though, I'm going to work out the math by making the payoffs exactly equal for
the two options, and then I'll remember to add and change at the end when we're done
all the math.
So if I make you the smallest offer that you would reasonably accept, then the numbers
A and B will satisfy a hundred minus A equals B over one plus S.
The same kind of analysis goes through if you go first.
We just reverse the roles of A and B and replace your impatience, S, with my impatience, R.
When we do that, we have two equations in two unknowns, A and B, and we can solve them
simultaneously.
Here they are.
And if you do the math, it's not hard, you get these.
Remember, these numbers tell us how much we get to keep if we make the first offer.
So let's look at two cases.
Okay, you can look at this table this way.
Regardless of who goes first, I get to keep R, I'm sorry, S, one hundred S over R plus
S plus RS.
You get to keep a hundred R over R plus S plus RS.
Then there's a bit of a bonus that goes to whoever goes first, a hundred RS over R plus
S plus RS.
Let's look at this with some actual numbers.
Suppose that R and S are equal so that you and I have the same degree of impatience.
Remember, the higher my impatience, the higher R is.
The higher your impatience, the higher S is.
Okay, let's say that our impatience is 0.05.
Getting $1 right now is as good as getting $1.05 next round.
And the equations say that we each get $48.78 and the do-ever goes first gets to pocket
the additional $2.44.
You can see that when R and S are small, the difference in what you get really doesn't
matter very much on who goes first.
And in the negotiations where a long time doesn't need to pass between one offer and
the next, R and S are usually quite small.
Small enough that the extra money gained by going first can be pretty much ignored.
Using that bonus for making the first offer, the payoffs to the two players are then just
what I said before.
And if you look at them, you'll see that my numerator and your numerator are exactly
the same.
Our denominators are the same.
Our numerators are the same except that I have 100 S in mind and you have 100 R in yours.
To say that just a little bit better, the result of our bargaining is this.
We divide the money in a way which is inversely proportional to our impatience rates.
If you are twice as impatient as me, I get twice as big a payoff as you do.
That's a big deal.
If I thought that $10 now was as good as $10 in one cent next round, while you thought
that $10 now was as good as $10 and four cents next round, the difference may seem tiny.
Three cents on $10.
But your impatience is four times as large as mine, so my payoff is four times larger
than yours.
I get $80 out of the $100 and you only get 20.
The actual figures differ from this by about seven cents, depending on who actually goes
first.
This analysis of this non-cooperative game gives us an understanding of where the bargaining
strengths of the two players might come from in the Nash cooperative bargaining solution
and what mechanism might bring it about.
And the practical upshot of this analysis is impatience in bargaining has serious costs.
This isn't just a theoretical result.
Prices who give advice on, for example, the purchase of homes advise you to adopt an appearance
of patience, even if it's a ruse.
If a buyer knows that a seller needs to unload a house quickly, she can use that information
to drive a much more advantageous deal.
Similarly, if the seller knows that the buyer has to be moved into a new home before school
starts, well, the positions are reversed.
This same issue arises in negotiations among countries and leads to the U.S. getting the
short end of many such agreements.
You see, our form of government puts pressure on the officials to get results and to get
them now.
We're not a patient, people.
In negotiations with countries that don't have that kind of popular pressure, American
Impatience puts us in a significantly weaker bargaining position.
Patience is a virtue it would be to our benefit as a people to cultivate.
