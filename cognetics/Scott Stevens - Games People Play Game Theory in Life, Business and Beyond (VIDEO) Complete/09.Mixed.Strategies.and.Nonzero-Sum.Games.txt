To those of you who view math as a second language, perhaps a distant second language,
my thanks for your patience in the last two lectures.
Today I'm going to back away from the math for the most part.
I want to focus on what actually went on last time, some things I kept hidden behind
the curtain too.
My goal here is a clearer look at what we've done, what we've done, what it means, and
how far we can run with it.
Last time we learned to find Nash Equilibria for any two player zero sum game.
You might still have some questions on whether it makes sense to call these strategies best
possible.
To look more closely at this, let me return to a problem we discussed two lectures ago
between Steven and Maude.
You don't remember how this game went.
Steven and Maude were playing a game of shooting fingers.
Each player counts to three, then shoots one, two, or three fingers.
The payoff is the total number of fingers shown.
If it's even, Steven wins.
If it's odd, Maude wins.
And the loser pays the winner.
It can be fun to show this matrix to people who have never studied game theory.
Most people think that Steven has the advantage because he has five ways to win.
Maude only has four.
Other people, a little more mathematically sophisticated, will often think that Steven
has the advantage because if you add up all of his payoffs, they add up to four, while
the sum of all Maude's payoffs add up to minus four.
It turns out that both lines of reasoning are wrong, and they fail for essentially the
same reason.
They're rooted in the idea that each cell in the table is equally likely.
But this is a game of strategy, and so each person is playing the best possible strategies
that they can.
They don't necessarily play each number one-third of the time.
In fact, optimal strategies, found by solving the kind of equations that we did last time,
look like this.
You can see the optimal strategies in parentheses.
Right through the algebra from last time, and you'll see that each player should shoot
two half the time, and shoot one and three twenty-five percent of the time each.
Now what does it mean to say that these are the best strategies?
Well, let me start out with what it doesn't mean.
It doesn't mean that regardless of what Maude does, Steven should shoot a two, fifty percent
of the time, and so on.
If Steven knew or found out that Maude always shoots a two, then Steven should shoot a two
every single time.
In doing so, he'll win four, while by shooting anything else, he'll lose.
In a similar way, if Steven knew that Maude always shot a one, then he should always shoot
a three.
Again, he'll make four dollars, or if he does something else, he'll lose, or he'll win less.
But I hope that you see that definitely, predictably, shooting a two or a one would be a very foolish
thing for Maude to do, and to be foolish for precisely the reasons that we said Steven
can capitalize on it.
And we don't need to limit ourselves to Maude being predictable in so simple-minded a way.
She could follow any pattern of ones and twos, one, two, one, one, two, one, one, one, two,
one, one, one, one, two, and so on.
And as soon as Steven caught onto the pattern, he could use it against her.
Any pattern, no matter how complicated.
But doesn't this create an opportunity for Maude?
How about if Maude follows a pattern just long enough for Steven to catch on, and then
suckers him into it by changing her pattern?
Good idea.
But when exactly is that?
And wouldn't Steven, knowing that Maude was trying such a suckering trick, pretend to
believe the pattern, and then really expect a switch?
Do you see what's happening?
We're sliding down the same slippery slope of the reasoning that motivated us to consider
mixed strategies to begin with.
There isn't a reliable way to outclever your opponent, not without more information than
we have available for this current analysis.
Either player could continue their levels of reasoning one more step and change their
decision.
Now, I don't need to say that in human interaction, it's impossible to read an opponent.
Humans are very good at watching other humans, and some people are very good at anticipating
the trains of thoughts of other people's minds.
If you're convinced that you can read your opponent with that fair degree of accuracy,
then you're essentially playing an asymmetric game, a game of asymmetric information, where
you know more about your opponent than they know about you.
We'll be able to model that kind of game later, but it's a different sort of model.
But even if that's the case, let's take a moment to consider.
Either two players are equally good at reading each other, or one's better than the other.
If they're equally good, that's the situation I intend to discuss today.
You can't rely on being cleverer than your opponent.
If you have a pattern, your opponent's going to be able to figure it out.
On the other hand, if one player is better at reading than the other, then the weaker
player is going to find it very, very interesting to find a method of play that will allow
them to do the best possible performance that they can, even given that the other player
is going to know how he's making his decisions.
It's in this sense that the strategies that we've found are optimal.
They provide you with maximum protection.
They make you bulletproof against your opponent's tricks and traps.
No matter how clever she is, no matter how well she understands your thinking.
Let me show you what I mean.
Suppose that Steven follows our advice from the table above.
He plays the 25-50-25 strategy.
And suppose that Maude, in this particular game, shoots a 1.
That puts us in the first column of the table.
What happens?
Well, look at the first column payoffs.
25% of the time Steven wins 2, 50% of the time he loses 3, and 25% of the time he wins
4.
Computing the expected payoff then gives us all of this, which comes out to 0.
If Maude plays a 1, then Steven, playing this strategy, on average, breaks even.
But what if Maude chooses 2 instead?
Now we're looking at the second column of the table.
We do the same kind of calculation, here it is, and when we work it out, we see that
we get 0 again.
In this column, Steven has two ways to lose and only one way to win, but he plays that
winning one half the time.
In the final column, I'll let you verify things work out exactly the same way.
If Maude plays a 3, on average, Steven gets a 0.
Did you get that?
I'm saying that Maude can know exactly how Steven is making his decisions.
It doesn't matter.
If Steven plays what we told him to play, then Maude, on average, breaks even regardless
of what she does.
She could always shoot a 1, always shoot a 2, or a 3, or mix them in any way that she
wanted.
It doesn't matter.
On average, she'll always break even.
In the language of the last lecture, she is indifferent among her three options.
Well, we just showed that with his current strategy, Steven's security value is 0.
That is, if he plays this strategy, he at least breaks even, on average, regardless
of what Maude does.
But is there a strategy with which he can do even better?
Well, he can't if Maude plays the bulletproof strategy that we've given her.
Let's see what happens if she does.
Maude is playing 1, 25% of the time, 2, 50% of the time, and 3, 25% of the time.
Let's see what happens if Steven shoots a 1, plays the first row, take Maude's payoffs,
times the probabilities, and add them up.
There you go.
If Maude does what we told her, and Steven plays a 1, on average, she breaks even.
You see where this is going.
I'll let you verify that for Steven's other two choices, the same thing happens.
Steven's average payoff, in each case, will be a 0.
So as long as she plays her bulletproof strategy, her security value is 0, too.
Nothing that Steven can do will give her an average payoff of less than 0.
Why am I calling these strategies bulletproof?
Because if Steven does his, nothing that Maude can do can make his payoff go below 0, even
if she knows how he's making his choices.
And similarly, if Maude does what we told her to do, there is nothing that Steven can
do that can give him a payoff above 0.
If you do worse than that 0 payoff, on average, if you lose money on average in this game,
it's because you deviated from your minimax strategy.
In this example, both players have the same security value, 0.
Now, for games in general, that security value might not be 0.
The game might favor one side over the other, like it did in the Sea of Bismarck.
But for zero-sum games, the two values will always be equal.
And that means that there's no way to force a better payoff for yourself than what you
get from your bulletproof minimax strategy.
So you play your bulletproof strategy, and we've said that keeps your opponent indifferent
among all of his choices.
They all do equally well against you.
Well, actually, that's a little overstated.
It may be that your opponent has some options that are actually inferior to other ones.
In that case, that player's minimax strategy will include only the options that give the
best expected value.
The inferior options will never be played.
Our next situation is going to give us an example of this.
It's going to be more real world than the Steven and Maude game, and more importantly,
for the purposes of the lecture, it's going to be a non-zero-sum game.
This last point is a big deal.
Von Neumann showed that a two-player zero-sum game will always have a Nash equilibrium,
which is what our bulletproof strategies were.
But his proof and our analysis of it depended on one key idea.
What was good for one player was bad for the other.
In analyzing soccer kicks, I said something like, the goalie is going to do whatever is
best for himself, and that's the same as whatever is worst for me.
In a non-zero-sum game, you don't have that.
In the coordination game, for example, what's best for you is also best for me.
Okay, so here's the new problem.
I'm sure you've heard about the overuse of our national parks and the environmental damage
that it can cause.
This real-life game obviously involves a lot more than two players, but I'm going to
pare it down by focusing on a particular campsite with a fragile environment.
Okay, two families, the Smiths and the Joneses, are the only two that like to camp at this
site, and each will do so zero, one, or two times per month.
Camping's fun, but the more times that the site is visited, the more damage it sustains.
That makes the stays there less pleasant.
In any event, here's the matrix.
The payoffs are measured in utils, pleasure points, of how much a family enjoys their
stays at the site.
Before we dive into analyzing this game, it's worth taking a look at the matrix for a minute
or two.
First, the game obviously is non-zero-sum.
The sum of the two payoffs in the upper left-hand corner are zero, the cell below it, the two
total up to 800.
Quite different.
Second, this site really is environmentally sensitive.
If one family visits, you'll see that they actually have a better time by visiting only
once per month than by visiting twice.
Third, we can see that the Joneses are evidently bigger nature lovers than the Smiths.
The payoffs to the Joneses in a given visitation pattern is always twice as great as the equivalent
visitation pattern for the Smiths.
This brings up an interesting question for those interested in equity in fairness.
If it were your decision, who would you give more time at the campsite?
The Joneses get more pleasure per visit to the site.
Does this mean they should get more time?
Or that the Smiths should, since it takes them more time to get the same degree of pleasure?
Or did you remember that it often doesn't make sense to compare the payoffs of one person
to the payoffs of another?
Fortunately, this isn't our concern at the moment.
Each family is going to decide on its own how often it's going to visit.
Since they don't know one another and make their decisions independently, this is a
simultaneous game.
Okay, let's analyze this game.
The old ways are the best ways.
We start in the same way that we did when we first worked with simultaneous games.
Look for dominated strategies.
Well, pretty clearly, you get no payoff if you don't visit the site and a positive payoff
if you do, so going zero times is dominated for each family.
We can cross them out.
We're now looking at a two by two game.
We can look for Nash Equilibrium and Pure Strategies by using our best response method,
remember?
We'll highlight the best blue payoff in each column and the best red payoff in each row.
Here's the result.
How about that?
It has two equilibria and pure strategies.
This game's a lot like the coordination game, since both players prefer the equilibrium
where both families go one time to the one where both families go twice.
But it differs from the coordination game, too, in that a player prefers one of the non-equilibrium
payoffs to the one in the lower right corner.
Remember the coordination game?
The worst thing that could happen was that you didn't match the other player.
Regardless of how often you go to the site, you'd prefer that the other family go once,
if at all.
But in the reduced game, where not going at all is ruled out, the best solution for both
families is for each family to visit just one time.
The two by two game here is another of the atomic classic two by two games, but it's
one we haven't yet talked about.
It's usually called the stag hunt.
Since we've seen how these little games hide inside other games, I'm going to take a minute
to talk about the stag hunt with you.
The name comes from the French philosopher Jean-Jacques Rousseau, who described the following
situation in the 1700s.
Two hunters are out hunting for food.
Either hunter by himself can nab a rabbit, but if they work together, they can bring
down a stag, which provides considerably more food to both of them.
The problem is, if one of the hunters goes rabbit hunting, the other hunter can't bag
the stag on his own, and so goes hungry, and both of them looking for rabbits reduces
just a little bit the chance that they'll find one.
Using ordinal payoffs, the game looks like this.
Best is to hunt the stag together, then rabbit on your own, then rabbit together, and the
worst is to be stalking the stag while the other guy is having rabbits do.
If you take a second to compare this to the camping matrix, you'll see the similarities.
In both games, the preferences for the blue player, from best to worst, trace a big blue
U through the game matrix, while the preferences for the red player trace a reversed C.
Well maybe you don't care about stags.
Rousseau's example is pretty dated, for that matter, maybe you don't care about camping
either.
Okay?
How about the arms race?
When you have two superpowers, like the U.S. and the USSR, each nation can decide low or
high expenditures on WMDs.
Weapons cost money, lots of it, and using them means that the world is at war, which
isn't really desirable.
If the two nations think being the only one with nukes is the best situation, then the
games are prisoner's dilemma, and both sides build nukes.
In this regard, by the way, it's interesting to note that the discovery of the prisoner's
dilemma occurred in 1950.
The Soviets tested their atomic bomb in 1949.
But here is a different model.
What if the edging weaponry over the other country is viewed as being second best?
After all, having the weapons advantage means that your country is spending a great deal
of money on weapons that may be used offensively, but aren't needed defensively.
If that's your view, then we end up with this situation.
Best is a world in which both countries use their money for other purposes.
Second best is the one where we have the edge over the other side.
Third is both sides developing weapons.
And fourth is that they have the edge over us.
Take a look at the matrix.
It's a stag hunt.
International politics, not your thing?
How about collaboration at work?
You're working with a colleague on a project that's going to be reviewed by your common
superior.
It'll be judged, and you'll be rewarded based on the quality of the work.
Let's say that you each get a $6,000 bonus for an excellent project, a $3,000 bonus for
a good project, and a $2,000 bonus for a fair project.
But to make the project excellent, both of you are going to have to do $2,000 worth of
extra work.
If only one of you does the extra work, you end up with a good project.
If nobody does it, you end up with a fair project.
Okay, here's the payoff matrix.
Look for the blue U.
Look for the red reversed C.
Look familiar?
It should.
It's a stag hunt.
It's the same matrix we had for the arms race and the stag hunt, with three extra zeros
on each payoff.
The point that I'm trying to make here is, when mathematicians analyze a problem, the
particular example they're looking at is usually secondary to them.
It's the structure of the problem that's interesting.
What excites a mathematician is that once you've analyzed all of the, one of these problems,
you've analyzed all of them.
If you solve the camping game, you solve the stag hunt and the arms race and the project
at work, too.
So what do you think would happen in these games?
Your first thought is almost certainly that both players would opt for the better equilibrium.
I agree.
That seems natural.
The upper left corner is the best for all of these payoff matrix.
It gives the best solution possible to each of the two players, compared to the other
equilibrium in the game.
Such an equilibrium is called payoff dominant.
That makes it very attractive.
But you know this isn't always what happens.
It certainly isn't what happened during the Cold War.
Why not?
Well, the other equilibrium also has an appeal, although it's less obvious.
It's a hedge against uncertainty.
In cruder terms, it lets you cover your butt.
Let's look back at the collaboration at work version.
It's certainly true that by choosing to do the extra work, you set yourself up for your
best payoff.
But if you coast, you're guaranteed to get at least $2,000, while if you do the extra
work, you're only guaranteed $1,000 extra.
Whether you're willing to put in the extra work depends on how likely you think it is
that the other guy will coast.
But why should he?
Why?
Because he thinks that you might coast.
And why would you do that?
Because you think that he might coast.
Unfortunately, there's nothing really wrong with this line of reasoning.
It's internally consistent.
If you're both convinced that the other guy will coast, your best choice is to coast.
If you think that the other family is going to camp twice, you should camp twice.
If you think that the Soviets are going to build nukes, you should build nukes too.
But obviously the other equilibrium is better for everyone in each of these games.
And this is a great example of a game where communication is of great, great value.
One side tells the other the obvious truth that they both do better by doing the right
thing.
The other side makes it clear that they understand and agree.
The first side says, I'll do the right thing if you do.
And the other side says, agreed.
They both say that they're going to do the right thing.
No binding agreements were made, none were even possible.
But this communication goes a long way toward establishing a shelling point.
The agreement is self-reinforcing, and both sides would lose by unilaterally changing
their plans.
Sometimes talking with your perceived enemies can buy you quite a lot.
Unfortunately, such communication isn't always possible, or isn't always pursued.
What do you do then?
Well, you can guess at a pure strategy.
That would mean sometimes you pick one, and sometimes you pick the other.
Sound familiar?
That's just what a mixed strategy is.
So are there any mixed strategy equilibria in a non-zero sum game?
Yes.
This is where John Nash did John von Neumann won better.
Nash showed that every simultaneous, non-cooperative game, even non-zero sum, has at least one
Nash equilibrium, maybe in pure strategies, maybe in mixed.
Finding Nash equilibria for arbitrary, non-zero sum games is more mathematically complicated
than for zero sum ones, but the idea ends up being the same.
Find a strategy so that all of the options that the opponents use do equally well against it,
and the pure strategies not used by the opponent would do no better than the ones that are being used.
For a two by two game, the happy news is that the trick that I taught you last lecture still works.
This is really lucky and utterly remarkable.
Let's use it here on our camping game.
We'll find the difference in the blue payoffs in the first column, 200 minus 120 is 80.
And remember we write this difference over the second column.
Then find the difference in the blue numbers in the second column, 80 is 20 more than 60,
so write 20 over the first column.
Then add these differences, 20 plus 80 is 100, and divide these two numbers by that total.
We get 20% for the first column and 80% for the second column.
Then we do the same thing for the rows.
The difference of the two red payoffs in the first row is 40, and we write that difference in front of the second row.
The difference of the two red payoffs in the second row is 10, and we write that difference at the beginning of the first row.
Adding these values gives 50, and when we divide these two numbers by 50, we get 20% for the first row and 80% for the second.
This is an equilibrium.
So we've found a third equilibrium for this game.
Each family gives 20% chance to a single visit and 80% chance to two visits.
Let's go back to the original game and verify that this is an equilibrium.
Here we go.
I found the expected payoffs for each row and column in the same way that I always do.
Probability times payoff added up over all of the cases.
On the right side of the table, we see what happens when the Smiths played the 2080 strategy.
The Joneses can expect 88 points of happiness by going once or by going twice.
They're indifferent between those two choices.
The Joneses, we get less by going zero times, but that's an option that they're not using in the strategy.
As I mentioned earlier, equilibrium doesn't require that every option performs equally well, only that the ones that are being used do.
We see the same thing when we look at the bottom of the table.
The numbers in the column show that if the Joneses play their 2080 strategy, the Smiths can expect 44 points of pleasure from making one visit or two visits.
They're indifferent between the two options.
They do less well by making no visits, but again, this option isn't used in their mixed strategy.
So, given that the other family is following their mixed strategy, your family gains nothing by changing from its mixed strategy.
And that's the definition of an ash equilibrium.
True.
All true.
And yet, it seems that there's something wrong here.
The logic of the zero sum game breaks down somehow.
In the zero sum game, I found an indifference point like this and I argued for it.
Let me remind you of the logic behind that idea.
Then I want to see how it works or doesn't work for non-zero sum games.
First, zero sum games.
What you win, I lose.
Let's start with your optimal mixed strategy.
When you do so, I'm indifferent among all of the options that I'm using in my strategy.
So I really don't care about using my optimal strategy.
I can play the optimal probabilities or not and get exactly the same payoff.
Except, except that if I change my probabilities, then your options are no longer all equally good.
I've upset that balance of your indifference.
Now, at least one of your options is going to be better than it was before,
meaning that you can get a better payoff by playing it.
And since whatever you win, I lose.
What's better for you is worse for me.
So I have a very practical reason for not deviating from my optimal mixed strategy.
If I do, you gain.
So I lose.
He's closed.
But how about non-zero sum games?
The argument starts the same as before.
If I move from my optimal mix, I let you find a better strategy for yourself.
And better for you is worse for me.
Except that it's not.
Not anymore.
We're talking about a non-zero sum game.
I move from my equilibrium strategy, creating a better option for you.
You're doing better now.
Good for you.
I don't care whether you do better or worse.
I care about how I do.
In a zero sum game, your welfare and mine were inversely linked.
One of us doing better was, by definition, the other one doing worse.
But now it's a non-zero sum game.
I don't mind if you do better, as long as I don't do worse.
Let's go back and look at the Smiths and Joneses again.
If Smith plays the 2080 strategy shown, Jones can play any mix of visiting once and twice,
and still have an average payoff of 88.
What happens when she plays an option of 50-50?
As long as the Smiths play the 80-20 mix, it would have no impact on the Joneses.
That's the point.
They're indifferent.
But if Jones played 50-50, then you can easily check this.
Smith gets more by always going to the campsite one time.
He'll get 65 pleasure points on average, while by going twice, he'll only get 50.
So if Jones plays 50-50, Smith is going to change from 2080 to always going just once.
But if Smith is always going just once, then Jones doesn't want to play 50-50.
If Smith is only making one trip, Jones only wants to make one too, then she'll get her
best payoff, which is 200 pleasure points.
And if Jones only makes one trip, then Smith wants to make one trip, and we've reached
that best equilibrium.
A mixed equilibrium in non-zero-sum games keeps the other player indifferent among the
options he's using, but unlike the zero-sum game, it isn't an intentional choice.
It's just an artifact of the mathematics of equilibrium.
In this particular equilibrium, it's like an egg balanced on the peak of a roof.
It's inherently unstable.
A player might increase or decrease the probability of a single visit by just a small amount.
Then the line of reasoning I just presented would quickly lead to the players choosing
one of the two pure equilibrium strategies.
Like an egg on the roof, a slight breeze will send it down one side or the other.
Mixed equilibrium for non-zero-sum games can have other problems than stability.
In the coordination game, the mixed equilibrium solution can give you worse answers for both
players than either pure strategy equilibrium.
These problems don't always arise with mixed strategy equilibrium in non-zero-sum games,
but they're fairly common.
In particular, if a non-zero-sum game has an equilibrium in pure strategies, take some
extra time to consider any mixed equilibria carefully, especially if shelling points are
available.
They do have their real-life applications, but in some cases you may find that these
strategies are about as likely to occur as that egg perched on the rooftop.
