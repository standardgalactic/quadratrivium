Say what you will about John Von Neumann. The man wasn't afraid to think big.
Von Neumann was a prodigy and a genius. His contributions to science and mathematics are legendary.
Von Neumann went toe to toe with the likes of Albert Einstein.
Heck, Von Neumann once put Albert Einstein on a train from Princeton going the wrong way as a practical joke.
Let's just say he wasn't a man who was easily intimidated.
So when he teamed up with the economist Oscar Morgenstern to write The Theory of Games and Economic Behavior,
his goals were, well, pretty ambitious. He wanted to make economics a science.
He wanted it to have as firm a mathematical foundation as, say, physics.
And of course, he wanted it all to work.
He and his successors have had a lot of success. Game theory is used throughout neoclassical economics
and has provided a lot of really good insights. But there were problems. And there still are.
When I was a kid, there was a Murphy's Law called the IBM Polyana Principle.
Machines should work. People should think.
That's as good an introduction to the idea of a normative model as I could offer.
A normative model tells you what people should do, not what they will do.
Or in the case of game theory, it tells you what people will do if the game is modeled correctly
and if the players have the payoffs that we think that they do and if the structure is common knowledge
and if everyone is rational. That's a lot of ifs.
But sometimes it's not a problem. Our real-life data on soccer penalty kicks match the theoretical results to within 1%.
That's truly remarkable. And many of the world champions in poker explicitly use game theory in developing their optimal strategies.
Trying to puzzle out the outcome of an international crisis, studies show that analysts trained in game theory
have a higher success rate than their untrained counterparts.
And a man named Bruce Buenos-Demosquita has used game theory to make predictions for the CIA,
including startlingly specific and accurate predictions about leadership changes in Iran five years before the fact.
By the CIA's own account, his accuracy is somewhere above 90%.
Still, that's not the whole story. Today, I want to look at where game theory has failed.
The heck with what people should do if they're rational decision makers.
If they don't actually do what game theory says, then the theory has no predictive power.
Worse than that, if other players aren't doing what they should do,
then my best choice from my real situation is probably different than what theory says too.
That is, the model doesn't have prescriptive power either.
There are lots of places where the theory matches well with observations,
but there are also serious discrepancies.
Well, what do you want? Game theory is a young field less than a century old.
It's a work in progress, and game theorists are busy trying to fix the holes.
In this lecture, I want to give you an idea of where some of those holes are
and a bit about the avenues of current research meant to fix them.
Remember the ultimatum game? A great example of where things go wrong.
A researcher gives one player $10. Call him the proposer.
The proposer proposes a division of that $10 between himself and the other player, the responder.
If the proposal is accepted, each player gets the money that the proposer proposed.
If refused, no one gets anything.
You can figure out what should happen by rollback.
As we've seen, since some money is better than no money,
the responder should accept any positive amount.
And because of that, the proposer should offer as little money as possible,
a penny or a dollar, and the responder should take it.
Except this almost never happens.
A lot of research has been done playing this game in different conditions,
and the results are consistent.
Here's what happens in study after study.
The proposer generally offers about 40% of the money,
not just a penny or a dollar, 40%.
In fact, in a lot of studies, the most common offer is a 50-50 split right down the middle.
And as it turns out, you need to be fairly generous.
If you offer someone less than 20 or 30% of the pot,
they're likely to tell you to go jump in a lake.
If this happens frequently in study after study,
40% offers and rejections of offers of less than 20 or 30%.
This isn't even close to the rollback equilibrium.
What's going on?
Whatever it is, it's real.
I'm betting that you would probably reject the $1 offer as well,
and almost certainly reject the one penny.
But why is this your reaction?
Having a dollar is better than having nothing, right?
You want to teach the other guy a lesson?
Why?
This is a one-shot game.
There won't be a next time for the two of you,
so you can't benefit from teaching them a lesson.
You're just giving up the small amount of money that you could have had.
Maybe that's the problem, the small amount of the pot.
You'd be getting so little money anyway,
it's worth throwing it away just to spite the other guy.
Well, three researchers, Hoffman, McCabe and Smith,
got tired of hearing this particular claim.
They scrambled around and raised $5,000 to do a new set of experiments.
Their pot wasn't $10, it was $100.
50 pairs of players retested.
And the effect of the change in the size of the pot was remarkable,
in that it had almost no effect at all.
The distribution of offers in terms of percentage of the pot
changed insignificantly.
And the responders, too, kept the same pattern.
In the $10 game, the fair offers of $5 were almost always accepted.
In the $100 game, 75% of the $10 offers,
twice as much money, were refused.
In fact, 40% of the $30 offers were refused.
That's a lot of people willing to give up $10 or $30
to punish the proposer for too lopsided a division,
especially since they knew that they would never play again.
Why?
A sense of fairness?
Maybe, but let's look at some other possibilities.
How about fear?
Maybe the proposer makes high offers, not for fairness sake,
but for fear of having the offer rejected.
It's easy to investigate this.
There's a game that's even simpler than the ultimatum game.
It's called the dictator game.
Here's how it goes.
The experimenter gives you $10.
You decide of it how much of it you're going to keep
and give me the rest.
That's it.
I have nothing to say about it, and we never play again.
This is an odd game.
I never even get a move.
Certainly, it removes any fear of rejection that you may feel.
Game theory obviously says that if your payoff is the money
that you get to keep, then you should keep it all and give me nothing.
Think it happens?
Actually, sometimes it does.
About 20% of the players in the dictator game do do this.
The rest, to varying degrees, share the wealth.
Why?
Well, let's use process of elimination.
Fear of rejection?
No, your offer can't be rejected.
Confusion?
Please.
The only way to be confused about this game is not to believe
that you can do whatever you want.
The experimenters were really careful to make sure
that the dictator did understand that.
So, nix that.
And you can also nix the idea that someone who understands the game
just can't figure out how to get the most money out of it.
It doesn't take a Rothschild to hit upon the keep it all strategy.
So, if the dictator isn't irrational, we must have the payoffs wrong.
The payoffs for the dictator have to depend in part
on how much money the other person gets.
We want them to get some, but not too much.
All this fits, of course, with the idea of fairness.
At least what we could call fairness in a broad sense.
That said, it's not just about fairness.
The average offer in the dictator game is less than in the ultimatum game.
This word fairness is a shorthand for remarkably complicated set of payoff modifiers.
Modify the context of the dictator game in just a bit,
and you can get a very different result.
Elizabeth Hoffman and her fellow researchers explored this
by varying six aspects of the game.
In one variant, instead of being told to divide the $10,
the dictator was told to divide a surplus from an earlier sale,
kind of like you did with the used car.
In another variant, anonymity of the dictator was assured.
Neither the researcher nor the other player would ever know who was the given dictator.
They could do this because they had a bunch of people playing at the same time.
The dictator could also be given a reason to feel that they were entitled to the money in some way.
They could be named dictator as the result of a contest,
or they could be given the $10 to be hired as a monitor for the session.
Each of these factors did have an effect on the rate of donation from the dictator,
and when all six factors were in place, about 65% of the dictators kept all the money.
In general, the less the connection is perceived between the two players,
the greater the social distance, the smaller the donations.
The Hoffman figure of about two-thirds of people keeping all the money
is consistent with other experiments that have been done in a similar context with big social distance.
The ultimatum game shows that there's less...
It shows less variation than the dictator game does, but context still plays its part.
In one experiment, the median offer for division was a 50-50 split of the pot,
but when the same game was recast into dividing the surplus from a legal dispute,
the median offer was only 7% of the pot.
That's much closer to what game theory would predict.
Some very clever variations of the ultimatum game
have been played to try to get a clearer idea of what's going on.
In one, the proposer divides 10 tokens rather than $10.
Each token stole worth a fixed dollar amount,
but a token may be worth more to one player than the other.
If the tokens were the same to both players and both players know it,
we're back to the original ultimatum game and we get the same results.
But suppose that the tokens actually were three times as much to the proposer
as to the responder and only the proposer knows it.
Then a fair offer would be giving the respondent three-quarters of the tokens.
While offering them half the tokens, it's an offer that would appear to be fair.
Experimental evidence shows that the appearance of fairness is the more important thing.
The median offer in this game was to offer about 50% of the tokens.
So there's a lot going on in people's real-life decision-making
that isn't easily captured in the traditional game theoretic analysis.
This has led to a new branch in the field, behavioral game theory.
Behavioral game theory has its roots in the kind of problems we've been talking about today.
Its goal isn't to disprove traditional game theory.
It's to improve it by allowing it to make predictions
that are more in keeping with observed human behavior.
And we'd better, because humans frequently behave in ways that are simply not consistent
with what traditional game theory says.
They often don't just up-and-play the game's equilibrium.
In fact, the idea of an equilibrium spontaneously materializing out of thin air is rather startling.
Nash himself talked about a collective moving toward an equilibrium.
And this idea will be important when we study collective games and evolutionary game theory
in my next two lectures.
Given the chance to repeat an interaction, though,
people's play often does tend to slowly converge toward the game theoretic solution.
Robert Ammon, the Nobel Prize-winning mathematician and game theorist, gives an example.
There's an optimal strategy for matching called the Gale-Shapely Matching Algorithm.
Turns out, it's been used in the real world for matching interns to hospitals since 1951.
It evolved by trial and error over a period of about 50 years.
So, the players eventually played optimally.
But it took them 50 years to do so.
There's a rather cryptic way of dividing wealth among the children of a family
described in the Babylonian Talmud.
It turns out it corresponds to the nucleolus of a cooperative game,
a particular kind of optimal solution.
So, behavioral game theory is attempting to better understand what's going on in real life
by grounding its models in observed behavior.
It makes advances in fields like psychology and neurobiology.
As an example, let's talk a little more about fairness.
Neuroscientists have observed brain activity in players playing the ultimatum game.
When they receive an offer viewed as fair,
the cognitive reasoning parts of the brain in the frontal lobe are very active.
But when a lobe offer comes in, those rational centers are overridden
by a part of the brain called the insula.
The insula generates emotionally relevant context to sensory experience, like disgust.
The more cells in the insula of the fire, the more quickly the person rejects the offer.
The reaction isn't logical. It's visceral.
But what about the offer triggers this sense of unfairness?
It's not just that the prize is being divided so unevenly.
Experiments have been conducted where the division of the $10 is determined randomly
out of the proposer's control.
And the responder knows this.
In these games, the responder is much more likely to accept
the same uneven deal that normally would have been rejected.
It makes a difference to us that the other person did it to us on purpose.
Similarly, an $8, $2 split is rejected
if the responder knows that the proposer had a $5, $5 split as their other choice,
but accepted if the responder knows that the other only other choice was a $10, $0 split.
So, sincerely trying to be fair seems to be good enough.
Interestingly, people were also willing to accept inferior deals
if they knew that the proposer was in a two-stage competition
and had to be in the upper half of the money kept in order to proceed.
An interesting question is whether these behaviors are instinctual or learned.
Experimenters have found only one group of people who will frequently accept
the short end of the 99-1 split.
Small children.
Does this mean that our strong reactions to being cheated or learned,
or that, like some other traits in humans, they simply take a while to develop?
Well, some research done with monkeys provides some interesting data.
In a report published in Nature, Sarah Brosnan and her colleagues worked with
female brown Capuchin monkeys.
Capuchins are a highly social primate and well-known for their cooperative behavior.
Pairs of monkeys from different social groups were trained to relinquish a stone
with a human experimenter to get a food reward, usually a slice of cucumber.
Once the monkeys were trained, they played the Give the Rock game
with a human partner in each other's presence.
Things went fine until experimenters changed the payoffs,
giving one of the monkeys a more tasty reward, for example, a grape for their rock.
Sometimes the working monkey would see the other monkey getting a grape
for doing no work at all.
Brosnan said that the reactions of the cheated monkeys were remarkable.
They refused to play with the human researchers anymore.
Sometimes they refused to eat the cucumber that they just earned.
Sometimes they threw their food at the human researcher.
In short, these were not happy monkeys.
So this reaction to unfairness may go further than just humans.
It still doesn't tell us, though, whether it's innate or learned in more than one species.
From an evolutionary perspective, what would be the advantage of this kind of reaction?
It's worth noting that the reaction is seen in social animals that value cooperation.
We've talked a lot about cooperation in this course.
The matter came to a head in the last lecture.
The Nash equilibrium for the prisoner's dilemma is, in a word, annoying.
Both players could do better if they could just cooperate.
But how can they do this?
If the payoffs and the prisoner's dilemma don't change,
then players are trapped playing the dominant betray strategy.
Well, one way to change the payoffs is if the players are kin.
Biologically, creatures that pass on their genes are winners.
You can do this by reproducing, of course.
But if your sister carries many of the same genes that you do,
then helping her live long enough to reproduce indirectly helps you.
In some species, such as the social insects,
the fraction of genes shared among the individuals is so high
that all the animals in the hive or nest just let the queen breed and help her to do so.
Genetically, this is perfectly workable.
It's been suggested that humans treat their social groups,
communities, military squads, circles of friends, and so on as a kind of family.
It's possible that we cooperate within these groups
in a way similar to how cooperation occurs within genetic families.
An individual can also cooperate with others for completely self-serving purposes.
Remember reputation? Here it is again.
My acts of kindness, if they become well-known,
may lead others to what will help me in the future.
A more restricted context for this is reciprocal altruism.
I help you now based on the idea that you, at a later time, will do the same for me.
Many species of animals do this.
For it to be workable, two conditions have to be met.
First, what I do for you has to be worth more to you than it is to me.
Second, there has to be a way to punish cheaters.
Just like in the prisoner's dilemma, not reciprocating leaves a player in a superior position,
having gotten something for nothing.
Unless a player is punished for not reciprocating, such a player has an advantage.
The trouble is, punishment itself usually has a cost to the punisher.
Jail cells cost money.
But the evidence in the ultimatum game suggests that people are willing to pay it,
even under rather remarkable circumstances.
To get to that point, let us use our traditional game theory tools one more time.
Go back to the prisoner's dilemma, but add a third player.
As usual, the first two players play the prisoner's dilemma.
But after the game is over, this third player, let's call her the arbiter,
has the power to inflict punishment on whomever she pleases.
You can analyze this game with different kinds of arbiters,
and it's interesting to see what the effects are.
Suppose we have a benevolent arbiter, whose payoff is the sum of the payoffs of the other two players.
Then her presence has no effect on the game.
Once the game's been played, any punishment that she inflicts hurts the payoff of one of the other players,
and so hurts hers too.
So she never punishes, and both players defect.
How about a virtuous arbiter, who only cares about the payoffs of the cooperating players?
This arbiter wouldn't care about how a criminal felt when punished for a defection.
But such an arbiter has no real motivation to punish after the fact, either.
And if the punishment has any cost to her, any cost at all, then she's better off not punishing.
So again, both players defect, and the arbiter has no effect.
But suppose we have a righteous arbiter,
one who gets even a slight positive payoff by punishing a defector enough to make it hurt.
Then if a player defects, the arbiter is going to punish that player enough to more than eat up the game from defection.
This makes defection a losing possibility, and so both players cooperate, and the arbiter never has to punish.
You could round this set out with a sadistic arbiter, who gets pleasure out of punishing just for its own sake.
But again, this arbiter will have no effect. Both players will defect, then both get punished.
Worst of all possible worlds.
But look at what we've got.
If we can introduce a righteous arbiter, we have a prisoner's dilemma that can be played cooperatively, with no punishment.
This kind of arbiter, and this kind only, can bring about cooperation with a threat that never has to be carried out.
You can start to see how the desire to punish unfairness, the righteous streak, can confer an advantage on a species that has a lot to gain from cooperation.
And we have the streak in spades.
Many people are willing to engage in what's turned altruistic punishment.
They're willing to punish another for unfairness, even though that punishment costs them.
Even though they know they'll never play a game with the guilty party again.
It can even be anonymous. The guilty party knows he's been punished, but not even by whom.
And no one else knows anything.
This has been documented in a lot of studies.
It's remarkable.
Altruistic punishment won't help you with the target, because you'll never play again.
It won't help you with other players, because nobody knows it was you that punished.
It makes absolutely no sense for an individual to do this in the context of the game that we've described.
But, get enough altruistic punishers in the society.
And it does make sense.
Like the righteous arbiter, a player punished by an altruistic punisher, has to think about the next game with a different partner.
If there are enough altruistic punishers out there, defectors are going to get whacked a lot.
Often enough that it just isn't worthwhile to defect.
And this is especially true in an open book society where past actions are observable by others.
Think neighborhood.
Think internet.
Remarkably, the pleasure of punishing the wicked may be the grease that keeps the gears of society turning.
Not all work in behavioral game theory deals with changing payoffs.
Another direction puts limits on the idea of rationality, what's generally called bounded rationality.
It's easiest to explain with a game.
Get a group of 100 people together.
Each person picks a number between 0 and 100.
The goal is to pick a number which is 70% of the average of all the numbers chosen.
Take all the numbers chosen, average them, take 70% of this.
Whoever's number is closest to that wins.
You might want to take a second, stop the video, and think about what number you'd pick.
Well, let's see what traditional game theory says the answer should be.
We can actually do it by a dominated strategy argument.
Clearly, the biggest possible average is 100 if everybody picks 100.
So 70% of the biggest possible average is 70.
So 70% of the average can't be bigger than 70.
So only an idiot is going to say a number bigger than 70.
So assuming rationality, everybody's going to pick a number between 0 and 70.
If you average those, the average can't be bigger than 70.
And so 70% of the average is at most 49.
Which means nobody should pick a number bigger than 49.
So the average can't be more than 49.
So 70% of the average can't be more than 70% of 49 or 34 and on and on we go.
Each bound is 70% of the last one.
So you whittle away at the upper bound until finally you reach 0.
The Nash equilibrium for this game is for everyone to say 0.
But you already know this isn't what happens, don't you?
The median guess in this game is usually around 25 or 35.
How does this come about?
Well, if people were choosing numbers at random, the average would be 50.
And 70% of 50 would be 35.
But this means that people who thought about it should be giving answers of around 35.
And 70% of 35 is about 25.
This is decidedly weird.
People's answers, clusters around the answer you get, if you follow the correct logic,
one step or two steps, and then stop.
To win the game with real people, you don't want to go to the last Nash equilibrium of 0.
You just want to go one step further in reasoning than the average person went.
So your guess should be around 17 or 25.
This kind of behavior is called bounded rationality.
There's a limit to how far people will carry out a chain of reasoning before cutting it off.
Whatever the cause, it seems very uncommon for people to carry out chains of reasoning
for more than about three levels.
We may simply run out of working memory.
Bounded rationality doesn't mean that people don't learn.
Colin Kammerer, one of the proponents of behavioral game theory, has played this 70% game
with groups of subjects from multiple rounds, and the results of each round were disclosed
before the next round was played.
The results in the first round are nowhere near zero, as we've said.
But when a $7 prize is offered for being the winner in each round,
the guesses of most guesses were less than one by the seventh round.
When played for $28 a round, most of the guesses were less than one by the fifth round.
Players were converging on the Nash equilibrium of zero,
but it wasn't an environment where the results of the previous round were common knowledge.
And that's quite important here.
I may have just come from a group where the Nash equilibrium in 10 rounds is very close to zero,
but if I'm going to play this game with a bunch of raw recruits in the next room,
I'm going to guess around 25, not around zero.
We have to know that the learning experience is shared by others.
Since cooperation is so important in a lot of our recent discussions,
I'd like to finish off today with another experimental result,
and one that has an interesting moral.
It was an experiment conducted by Robert Axelrod in 1979.
Axelrod set up an iterated prisoner's dilemma of tournament,
and he invited game theorists to submit strategies in the form of computer programs.
Each program would play against each of the other competitors,
and a clone of itself, for 200 rounds.
Whatever program got the highest total, won the tournament.
He got 14 entries from people, the longest was 77 lines of code,
and then he added a program that basically randomly decided whether to cooperate or to defect each round.
Then he ran the entire tournament five times to account for statistical fluctuation.
The winning program, submitted by Anatole Rappaport, was tipped for tat.
Its strategy was this, cooperate in the first round.
Thereafter, do to the other person whatever they did to you last round.
This do unto others as they just did unto you philosophy is called by some, the silver rule.
So, a simple strategy for a win, that so simple as you should would, was surprising.
It wasn't that tipped for tat was unbeatable.
Axelrod ran some hypothetical variations of his tournament,
and found two other programs that would have beaten it if they'd been there.
Tipped for two tats, which punishes once after being penalized twice,
and a second program called revised downing.
Axelrod made his results known, then held a second tournament,
allowing people to incorporate what they'd learned from the first one.
This tournament was open to a much wider audience, and Axelrod ended up with 62 entries,
generally more sophisticated than the ones in the first tournament.
The longest was 152 lines of code.
A world expert on game theory submitted tipped for two tats,
a 10-year-old boy entered,
Rathapur entered tipped for tat again, and someone entered revised downing too.
The results of the new contest?
Tipped for tat one again.
The programs that would have beaten it in the first contest, didn't do as well.
Tipped for two tats ended up 24th, revised downing in the lower half of the mix.
This is amazing.
Especially because when you think about it,
tipped for tat never wins a match against anyone.
The best it ever gets is a tie.
It wins the tournament because it elicits cooperation.
And why is it good at that?
Axelrod studied the question carefully and offers four characteristics
that such strategies possess.
Tipped for tat is nice, provocable, forgiving, and straightforward.
It's nice in that it's never the first to defect.
It's provocable in that it gets mad quickly
and retaliates for the defection of others.
It's forgiving in that once it's repaid,
betrayal with retaliation,
it's ready to resume a cooperative relationship.
And it's straightforward in its decision-making process is clear to the other player.
That player can thus accurately predict the consequences of their actions
and easily see that cooperation's in their best interest.
To close this lecture,
think about the relationships in your life where you want to foster cooperation.
And as you do, think about these four traits.
Nice, provocable, forgiving, and straightforward.
It might give you some ideas on how you can make your life and somebody else's
just a little bit better.
