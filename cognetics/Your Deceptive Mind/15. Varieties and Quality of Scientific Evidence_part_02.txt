The big one is that observational studies do not control many variables.
There are some variables that cannot be controlled for, although you can try to account for as
many variables as you can think of.
Or observational studies are always subject to unknown variables, variables that you haven't
thought of or cannot be accounted for.
Observational studies generally can only demonstrate correlation.
Because of their design, they cannot establish definitively cause and effect, although they
can be used to imply cause and effect.
So experimental and observational evidence are complementary.
They work together providing different kinds of information with different strengths and
weaknesses.
When an observation and experiment are pointing in the same direction, then that gives scientists
a lot more confidence in their conclusions.
For example, an observational study can be in a population of 100,000 people, how many
are taking a daily aspirin or other blood thinner, and what is the rate at which they
had strokes or heart attacks or bleeding events.
An experimental study looking at the same question might take 1,000 people and then
randomize them to several different groups, placebo versus low-dose aspirin versus high-dose
aspirin, for example, and then follow them over the course of a year and count how many
of them have different kinds of strokes and heart attacks.
These are two types of studies both addressing the same question, the risks and benefits
of taking aspirin or blood thinners, but they look at the data in very different ways.
Another feature of a study, whether it's observation or experimental, that is very important to
think about in terms of assessing its rigor and quality and the confidence in its conclusions,
is how large is the study?
Studies with only a few subjects, whether it's observational or experimental, or data
points of any kind are suspect.
If a study looks at just 10 patients, the results are likely to be erroneous or quirky.
The probability for statistical noise becomes very, very great.
Another way to look at this is that in the smaller the study, the greater the noise to
signal ratio is.
Large studies are needed for random effects to average out.
Here's an example, take a fair coin and then flip it 10 times and count how many times
you get heads versus tails.
You'll find that often your results differ significantly from what we know to be the
chance results of 50% heads and 50% tails.
Now flip the coin 20 times, 30 times, 100 times.
The more times you flip that coin, the closer your results will be to the 50-50 outcome
that represents the theoretical outcome of statistical chance.
The fewer number of times you flip that coin, the greater the probability that your results
will depart significantly from chance.
That's a simple demonstration of the need for large numbers when doing statistics in
