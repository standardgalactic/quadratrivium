What do you think is more likely? Getting killed by a shark attack or getting struck
by lightning? What about getting hit on the head by a falling coconut? What are you using
in order to determine what the relative risks are of these various things?
Well for example, in the United States, dying from a shark attack happens about once a year.
Your lifetime risk of dying in a shark attack is 1 in 3.7 million. The risk of dying from
lightning is a bit higher. This happens about 47 times a year in the U.S. which means that
your individual lifetime risk of dying by lightning strike is 1 in 79,000. Following coconuts,
that depends on where you live. On islands with lots of coconut trees, there are about three
to nine annual injuries and one to three deaths per year. But we tend to overestimate the probability
of something happening if we see it. This relates to the availability heuristic I described in the
lecture on cognitive biases. We tend to latch on to examples that are available to us, that we
are aware of. This has a huge effect in the media. If the media presents shark attacks in dramatic
documentaries, then we will tend to think that such occurrences must therefore be common. We
tend to worry in fact about insignificant risks, those risks that are available to us or that are
dramatically portrayed or perhaps risks that relate to some deep-seated fear, which may be
cultural, may be evolved, it doesn't really matter. However, we tend to ignore risks that are much
more likely and much more real, but that just don't capture our attention or are not dramatically
portrayed in the media. For example, the chance of dying from accidental poisoning in the U.S.
is, this occurs 19.4 thousand times per year. Many orders of magnitude greater than shark attacks
or lightning strikes. In the previous lecture on enumeracy, I talked about some gambling fallacies
among other errors in the way we think about statistics and streakiness, the tendency to see
streakiness in random events when in fact it's just a clustering illusion. We might think for
example that a number that hasn't come up yet on a roulette wheel is due, or if it has come up a
couple of times that it is coming up more often than chance predicts, therefore there's some
streakiness involved. There are other gambling fallacies as well. One is called playing with
house money. There is a documented tendency after winning in a casino, for example, that we will
tend to increase our bets over time as we win. This is because we have the sense that now we're
not playing with our own money, we're playing with the house's money that we just won. Our risk
aversion therefore goes down and we increase the size of our bets. However, the odds have not
changed. The gambling results are still what we call a drunker's walk of randomness. You're still
just as likely to lose these higher bets as you were the previous bets that you were making. This
comes from the tendency to link previous events with future events as if there is some connection,
even when they are completely statistically independent events. Related to the house money
effect is the break-even effect. After losing, we also tend to increase our bets in order to win
back our losses. We would rather break even than end a night of gambling in the red having lost. We
are more averse to losing than we are to taking a further risk, strangely enough. In both cases,
however, the house money effect and the break-even effect, interestingly, we tend to increase our
bets over time, chasing what has happened previously, even though the odds are completely still
independent. I used the term the drunkard's walk and that is a statistical term to explain a random
event, a random binary event, winning or losing or a drunkard lurching to the left or lurching to
the right. When gambling over a course of time, you will win sometimes and lose sometimes. Again,
if the house isn't cheating, this will be a random event. There is usually a small statistical
advantage to the house, but even if we put that aside and ignore that, essentially, winning and
losing will occur in a random pattern. There will be streaks, therefore, clustering of wins and
clustering of losses and for a time you will be up and then eventually it's statistically certain
that your quote-unquote luck will turn and you will start to regress towards the statistical mean.
However, when the player is winning, they are free to keep playing until they start losing. However,
when they are losing, there is no point at which the house is forced to stop. In other words,
when a player is losing, they can't keep playing indefinitely. At some point, they're going to
run out of money. In fact, even if there were no statistical advantage to the house, if the odds
of winning or losing against the house were always exactly 50-50, casinos would still rake in lots
of money. That's because there is what statisticians call an absorption wall at one end, but not the
other. When a player loses all of their available money, that's an absorption wall. They have to
stop gambling. However, when they're winning, there is no point at which the casino goes broke
and they have to stop gambling. They're always free to continue to gamble until inevitably they
regress back to the mean and they lose their winnings. Another statistically-based cognitive
bias is what statisticians call base rate neglect. This was first described in classic
experiments by Kahneman and Tversky in 1973. You might remember them from a previous lecture.
They did a lot of work describing what we call heuristics or modes of thought that we tend to
fall into. Here's an example from this specific study that they did. The subjects of the study
are given the following description. Tom W. is of high intelligence, although lacking in true
creativity. He has a need for order and clarity and for neat and tidy systems in which every detail
finds its appropriate place. His writing is rather dull and mechanical, occasionally enlivened by
somewhat corny puns and by flashes of imagination of the sci-fi type. He has a strong drive for
competence. He seems to feel little sympathy for other people and does not enjoy interacting with
others. He is self-centered. He nonetheless has a deep moral sense. Now, is it more likely that
Tom's major is a business administration, computer science, engineering, humanities education, law,
library science, medicine, physical life sciences, or social science, social work? This is the
question put to the subjects. Most people said that it is most likely that Tom's major is engineering.
This is because of the representative heuristic. Tom's personality profile is representative of
what we typically think of as an engineer. However, saying that it's statistically more likely that
Tom is an engineer ignores the base rate. How many students have each major? A far better approach
than the naive representativeness approach that I just described would be to take what's called a
Bayesian approach. The Bayesian approach starts with the prior probability, the base rate. In this
example, 1% of all of the students are engineering majors. It then adjusts that probability based
upon new information, like the personality profile that was provided. The probability then may get
it may get adjusted upwards. For example, if the base rate of all students is being engineers is 1%,
then we say, but Tom has a personality profile that's typical of engineers. Maybe that increases the
probability to 3%, 4%, or 5%, but that still makes it very unlikely statistically that Tom is an
engineer, just because statistically the base rate of engineering students is much lower than the
other majors. Another classic experiment by Tversky and Kahneman went as follows. Again, subjects
were given the following description. Linda is 31 years old, single, outspoken, and very bright. She
majored in philosophy as a student. She was deeply concerned with issues of discrimination and social
justice and also participated in anti-nuclear demonstrations. Now, what do you think is more
probable that Linda is a bank teller or that Linda is a bank teller and is active in the feminist
movement? Well, our naive sense is that Linda's personality profile is representative or typical
for somebody who might be involved in such a movement. Therefore, we commit what's called the
conjunction fallacy. The probability of A must be greater than the probability of A and B. When
stated in that way as a mathematical equation or expression, it seems obvious. A of A alone has to
be greater than A plus B. However, your gut instinct leads us to the wrong conclusion. We
conclude that Linda must be B because she's representative of us. That sense overwhelms the
mathematics of this inclusion in one or both groups. Related to this is the disjunction
fallacy. Let's say that Mary is into crystals, new age beliefs, and spiritualism. What is greater,
the probability of being either a tarot card reader or an accountant or just the probability of
being a tarot card reader? The probability of A or B has to be greater than the probability of A
alone or B alone. Again, when expressed as a mathematical equation, it seems obvious. However,
we tend to make naively the wrong conclusion. The Mary's personality description is representative
of somebody who might be a tarot card reader. Therefore, our sense is that that probability is
greater. Probability also comes up frequently in diagnosis. In fact, in medical school, most
physicians will learn about formal statistics about how to think about probability as it relates to
disease. That's extremely important because our naive sense of probability would lead us astray.
Let's take, for example, test X for disease A. Let's say that it has a false positive rate of
1% and a false negative rate of 1%. That sounds very good. Therefore, in the affected population,
99% of people will test positive, and in the unaffected population, 99% of people will test
negative. Let's now say that John tests positive for disease A. What is the probability that he
actually has disease A? Our naive sense might be that it's 99% because that's the accuracy of the
test, but that is not the answer. The answer depends on the base rate, the base rate of disease A.
Again, there is a tendency to fail to consider the base rate when thinking about probability or
statistics. Let's say, for example, that one person in 1,000 has disease A. Then 10 people per
thousand will test false positive with test X. A 1% false positive rate over 1,000 people means
that there will be 10 false positives in 1,000 people, and there will be 0.99 people who will test
true positive. One person with the disease in the 1,000 plus a 1% false negative rate, 1% of
one person is 0.01. There will therefore be about 10 false positives for every true positive. Therefore,
if John tests positive, there is a 1 in 11 or only about a 9.09% chance that he is true positive.
But again, that's not our naive sense. There's a greater than 90% chance that John does not have
a disease he tests positive for, even with a highly accurate test that only has a 1% false
negative and a 1% false positive. This has huge implications for screening programs.
In the general population, the false positive count may vastly exceed the true positive count,
depending on the base rate of the disease. This can lead to follow-up tests, which may be more
invasive and more risky than the screening test that is being recommended. It may cause more harm
from unnecessary tests and treatments than it actually is beneficial from early detection.
This seems counter-intuitive. How could a screening program that gives us more information
that detects disease early? How could this actually hurt people? But this comes up all the time.
When the base rate is low enough, the false positive rate can vastly outnumber the true
positive rate, leading to unintended consequences of complications and side effects from further
testing and interventions that result from those false positive tests. So when designing a screening
program, you always have to consider the base rate. There are other biases in estimating probability.
For example, we tend to be favorable to ourselves. This gets back to the need for self-esteem that
I described in an earlier lecture. We like to think positively about ourselves and to present
ourselves positively to others. We therefore tend to overestimate our own ability. This is called
simply enough overestimation. We also overestimate our relative ability or worth compared to others.
This psychologists call over placement. This is the classic sense that in this town,
everyone is above average. Also, there is what psychologists call overprecision. So we
overestimate our own ability, absolutely. We overestimate our own ability in relation to
other people. And we overestimate the accuracy of our own knowledge. In one study, researchers
asked subjects to give a range of answers to a question so that they were 90% likely to be correct.
If their calibration, if you will, was correct, meaning that they gave a range that actually was
90% likely to contain the correct answer, then 90% of their answers should have been correct.
However, on average, people score only about 50%. Interestingly, I took a short version of this
test myself just to see how I would do, and I scored 20%. So I'm not sure what that says about
my confidence level. The overconfidence effect relates to our need for esteem that we discussed
in lecture two. It seems to be universal. This is a universal human trait, the tendency to be very
kind to ourselves and to overestimate our own abilities. However, researchers have found very
persistent cultural and also gender differences as well. For example, Chinese subjects tend to
significantly overestimate their own abilities greater than subjects from Japan or the United
States. The research hasn't clearly delineated exactly why this is the case. This is one of those
situations where there are so many variables, it's hard to control for them all. So we don't know what
the reason for that is, in other words. But that signal is persistently there. There's no question
that confidence varies by culture. So it's not just something that is genetically inborn. There
is a certain amount of cultural programming going on as well. Let's turn our attention now to
probability puzzles. This is a good way to demonstrate our inherent lack of intuition about
probability. Here's an example. How many people would you need to have in a room together in
order to have a greater than 50% chance that two of them share a birthday? Think of an answer in your
own head. Well, the real answer is only 23. But this is much lower than what we naively might
guess. Most people guess that the number is much higher. Similarly, what are the odds or how many
people would you need to have in a room together for there to be a 97% chance that two of them share
a birthday? The answer is only 50 people. Here is my favorite probability problem that some people
just cannot wrap their heads around. It's very interesting. It's called the Monty Hall problem.
Let's say you're playing the old game Let's Make a Deal with Monty Hall, where the name of the
problem comes from. You have three doors in front of you. Behind one door is a new car. Behind two
doors are goats. You are asked to select a door. Of course, you are trying to select a door that
has the new car behind it because then you win it and get to take it home. Now, Monty Hall knows
where the car is. That's a very important piece to this puzzle. You make your guess and then Monty
Hall, regardless of what door you guess, Monty Hall will open up one of the other doors to reveal
a goat. There's always a goat behind the door that Monty Hall reveals. He then asks you if you
would like to change your choice to the remaining door or stick to your original choice. What should
you do and does it matter? The intuitive answer that many people give is that, well, it doesn't
matter either way. Your chance of winning is 50-50. There are two doors left. One has a goat.
One has a car. Whichever door you choose, there's a 50-50 chance of you winning. However, the truth
is if you keep your door, your chance of winning is one-third. If you switch to the new door,
your chance of winning increases to two-thirds. This is honestly the correct answer to this
puzzle. But how is it possible? Well, there are many ways to explain this answer. Different people
respond to different explanations. It's key to remember that Monty Hall is giving you new information
by opening up one door and revealing a goat, since he knows where the goats and the car are.
Your chance with your initial pick, when you pick one of the three doors,
it's one-third. And it remains one-third if you stick with that choice. If you don't change your
initial choice, your odds do not alter. However, if you wish to switch to the other door, in essence,
you are switching to the other two doors, because no matter what you chose, Monty Hall will reveal
a goat. You're in essence choosing both remaining doors at the same time, and therefore your odds
increase to two-thirds. If you run this through a quick computer simulation, for example,
that's exactly how the odds shake out. Another way to envision this, and this is how I find it
most useful to explain it to other people, imagine that instead of three doors, there are a thousand
doors. And Monty Hall opened up every door to reveal a goat, except for one remaining door.
So let's say out of the thousand doors, you choose door 173. Monty Hall then opens up every single
other door except door 520. And then he asks you, do you want to switch to door 520 or stick with
your original door? Now what would you do? I think it's a little bit more intuitive at this time,
since Monty Hall knew where the car was, that he was able to open up every other door that had a goat.
The chance of your original choice having the car is one in a thousand. Your chance of the
new door having the car is 999 out of a thousand. Let's take another example. You have a newspaper
that is one two-hundredth of a centimeter thick. That's pretty thin. Suppose, hypothetically,
you can fold it in half as many times as you want without limit. If you folded it 30 times,
how thick would it be? Again, one two hundredth of a centimeter folded 30 times. The answer
is that it would be 54 kilometers or 34 miles thick. That is a lot greater than we intuitively
might think. The reason for this is that we underestimate intuitively the effect
of geometric progressions. It's simply not something that we have an evolved intuitive sense of.
Let's say we folded the newspaper 43 times. How thick would it be then? Just 43 times,
and the folded newspaper would make its way all the way to the moon, about a quarter of a million
miles. Multi-level marketing schemes also prey upon our enumeracy. This is again a failure to
appreciate the power, the statistical power of geometric progressions. In order to maintain the
pyramid structure of a multi-level marketing company or scheme, you would have to recruit
geometrically more people. For example, if each level of a pyramid has 10 people,
then with only 10 levels, you would exceed the population of the earth. What typically happens
in a multi-level marketing program is that a salesman is encouraged to build their downstream,
to increase the number of people they recruit who then sell for them. Sometimes they're required
to recruit a certain minimum number of people in order to make back their investment. It may be
five people or six or whatever. Those people then, in order to make back their investment,
have to recruit the same number of people, five people, six people, whatever. But these
progressions are inherently unsustainable. We underestimate how quickly you can saturate a
community, a city, a region, even the world with any multi-level marketing program.
It's also very easy to lie with statistics, to use the public's naive sense of statistics in
order to create an impression that is misleading. A very common one that crops up with medical issues
is the difference between relative risk and absolute risk. Let's say one person in 10,000
gets disease A and an environmental risk factor increases that to two in 10,000. The relative
risk is 100% or double. You may therefore read newspaper headlines that reveal that this environmental
exposure doubles the risk of developing this disease or 100% increase in the risk of developing
this disease. It sounds dramatic. However, the absolute risk only increases to two out of 10,000
or 0.02%. Reading a headline that says 0.02% increase in the risk of developing this disease
wouldn't have nearly as much of an impact as 100% increase of the risk of getting the disease.
The relative risk can be used to exaggerate effects, therefore, the effects of both risk factors
and of treatments. And if it's not made clear exactly what type of risk you're dealing with,
it could be extremely misleading. Another common statistical type of fallacy is what's
called the sharpshooter fallacy. It is using the name derived from an analogy to shooting a gun
at the broad side of a barn and then going up and drawing a target around the hole and claiming
that you got a perfect bullseye. Of course, it's easy to do that when you draw the target after
you know where the bullet hits. Again, that's a good analogy because it seems obvious, almost absurd,
but it's amazing how often that exact kind of behavior occurs but just hidden in more subtle
ways. This relates to choosing criteria or interpreting the significance of those criteria
after the fact, after you know what the outcome is. This is also called post hoc analysis.
This relates further to what I described previously as our really developed ability to
mine data for patterns. We look for patterns, we're really good at making connections.
For example, one fun example is the connections between the assassination of Lincoln and John F.
Kennedy. There are researchers who found all sorts of apparent connections between these two.
For example, the assassin who killed Lincoln shot him in a theater and then ran to a book
depository, whereas JFK's assassin shot him from a book depository and then ran to a theater.
After the fact, these coincidences seem amazing, like there must be some explanation there,
but this is all post hoc analysis. This is looking for connections and then assigning some meaning
to those connections after the fact. There is also a fallacy in exaggerated precision.
An answer may be technically correct, but if it has more precision than is warranted in the inputs,
then that results in what we call exaggerated precision. Here is an example from a website
called Bad Physics. This website will comb through newspaper articles, journalistic reports,
looking for instances of enumeracy. Here is one example. A recipe was given with ingredients
such as one teaspoon of sugar or one cup of minced onion, etc. Those kind of measurements.
It then says at the end that this recipe will make from four to six servings and then it gives a
per serving calorie count of 271 calories with 28.73 grams of protein, 5.38 grams of fat,
25.90 grams of carbohydrates and 591 milligrams of sodium.
Those are incredibly precise figures, two figures beyond the decimal point, 28.73 grams of protein
when it makes between four and six servings. It doesn't even give you precision in terms of the
number of servings. The inputs were things like a teaspoon, which is a remarkably imprecise measurement.
In conclusion, throughout this course, we discuss human cognitive strengths and weaknesses.
Statistics and probability is a general area of pretty extreme weaknesses for most people.
Like many things, there's a lot of variation. Some people do have a better mathematical sense
than others, but for an average person, the fact is we just have a terrible sense of numbers and
statistics. However, it's easy to compensate for this with the learned skills, specifically mathematics.
