Have you ever heard or used the phrase studies show?
It's often used as a vague reference to data or research that supports a position.
You've probably heard me use a similar phrase throughout this course.
I refer to studies that are backing up my points.
Although I'll point out that in this course, there is an accompanying book that has the
actual specific references to the studies I'm talking about.
But in casual conversation, people often will say, well, studies show this or that without
detailing which studies, what kind of studies or evidence, how powerful and rigorous they
are, et cetera.
If you want your beliefs and conclusions to be based on solid evidence, then you have
to know how to interpret that evidence.
It's not always easy or obvious how to do so.
There are different kinds of evidence, each with different strengths and weaknesses that
you need to know about.
Also on any complex topic, there's going to be contradictory evidence.
So you can't look at any single piece of evidence and get a full picture as to what's going
on.
Some methods of balancing and comparing the different kinds of evidence.
In order to make sense of the scientific evidence, you need to know how to assess an individual
study.
So we'll start there.
Then you need to be able to pull it all together to balance all of the available evidence,
to compare different studies to each other.
When confronted with an experiment or study that purports to demonstrate some theory or
conclusion, how do you analyze it?
Where do you begin?
What features do you have to look for?
There are some basic questions you should ask yourself.
The first is, what kind of study is it?
The different kinds of scientific studies are roughly divided into two broad categories,
experimental versus observational.
First to deal with experimental studies, they are designed to perform a specific intervention.
They do something to affect nature or people, and then they measure some specific outcome.
The goal of a well-designed experimental study is to control for as many specific variables
as possible.
Ideally, one variable will be completely isolated, so that the effects of that variable can
be determined.
Examples of experimental evidence are many, there are many different kinds of experiments
that scientists can do to answer various questions.
For example, injecting a drug versus a placebo, a fake drug, into lab rats and then measuring
some specific outcome, like their liver function or perhaps their strength or whatever biological
outcome the researchers are interested in.
Or smashing particles into each other at relativistic speeds and then seeing what they
break into.
Or placing an unknown mineral into a jar of sulfuric acid to see if it dissolves.
These are all different kinds of experiments that scientists can do.
The strengths of experimental studies include controlling and isolating variables.
They also can be highly quantitative.
You can measure some specific feature or outcome.
You can come up with a number, and then you can perform specific statistics on those numbers
because there are comparison groups.
You could actually compare what happens under one condition versus another condition.
But there are also weaknesses to experimental studies.
They may not represent, for example, the real-world experience.
Controlling for variables is good, but it also introduces artifacts.
Artifacts are things that are inadvertently introduced in an experiment that make a system
behave differently than it does in nature or in the world at large.
Often they're not practical.
For example, we cannot withhold an effective, a known effective treatment or randomize people
to be exposed to some toxin or other risk.
So there are certain kinds of experimental studies that we simply cannot do.
What about observational studies or observational evidence?
Well, this is very different from an experiment.
Observational studies observe the world without doing any specific intervention or with only
minimal intervention, just whatever is necessary in order to gather data.
There are many types of examples of this.
For example, you can correlate a risk factor to a disease.
How many people who smoke get lung cancer compared to those who don't smoke does smoking
for a longer period of time correlate with a higher risk of lung cancer than smoking
for a shorter period of time.
Or you can dig up and examine fossils.
Here you're not comparing risk factors, you're just seeing what's out there in the world
and what kinds of fossils, at what depths, at what geological strata are there.
Or you could observe the light output from different types of stars of different ages
and compare their spectroscopic analysis.
These are all just observations that scientists are doing without specifically intervening.
The strengths of observational studies is that they can gather large amounts of data.
Because you're not having to do some kind of intervention or experiment, you're just
looking at the data that's already out there.
You can often gather orders of magnitude more data points than you can with experimental
studies looking at the same question.
You're also able to compare different groups and the minimal intervention reduces the risk
of affecting the natural behavior of the system that you're looking at.
But there are also weaknesses to observational studies.
The big one is that observational studies do not control many variables.
There are some variables that cannot be controlled for, although you can try to account for as
many variables as you can think of.
Or observational studies are always subject to unknown variables, variables that you haven't
thought of or cannot be accounted for.
Observational studies generally can only demonstrate correlation.
Because of their design, they cannot establish definitively cause and effect, although they
can be used to imply cause and effect.
So experimental and observational evidence are complementary.
They work together providing different kinds of information with different strengths and
weaknesses.
When an observation and experiment are pointing in the same direction, then that gives scientists
a lot more confidence in their conclusions.
For example, an observational study can be in a population of 100,000 people, how many
are taking a daily aspirin or other blood thinner, and what is the rate at which they
had strokes or heart attacks or bleeding events.
An experimental study looking at the same question might take 1,000 people and then
randomize them to several different groups, placebo versus low-dose aspirin versus high-dose
aspirin, for example, and then follow them over the course of a year and count how many
of them have different kinds of strokes and heart attacks.
These are two types of studies both addressing the same question, the risks and benefits
of taking aspirin or blood thinners, but they look at the data in very different ways.
Another feature of a study, whether it's observation or experimental, that is very important to
think about in terms of assessing its rigor and quality and the confidence in its conclusions,
is how large is the study?
Studies with only a few subjects, whether it's observational or experimental, or data
points of any kind are suspect.
If a study looks at just 10 patients, the results are likely to be erroneous or quirky.
The probability for statistical noise becomes very, very great.
Another way to look at this is that in the smaller the study, the greater the noise to
signal ratio is.
Large studies are needed for random effects to average out.
Here's an example, take a fair coin and then flip it 10 times and count how many times
you get heads versus tails.
You'll find that often your results differ significantly from what we know to be the
chance results of 50% heads and 50% tails.
Now flip the coin 20 times, 30 times, 100 times.
The more times you flip that coin, the closer your results will be to the 50-50 outcome
that represents the theoretical outcome of statistical chance.
The fewer number of times you flip that coin, the greater the probability that your results
will depart significantly from chance.
That's a simple demonstration of the need for large numbers when doing statistics in
experimental or observational studies.
Another question to ask is, were the results statistically significant?
This is often expressed as a p-value.
The p-value literally is the probability that you would have gotten the results that you
did or greater given the null hypothesis.
The null hypothesis is the hypothesis that whatever you're looking for is not true, that
the phenomenon you're studying does not exist, as opposed to the evidence establishing that
there is a correlation or the phenomenon you're looking at is real and does exist.
Typically, a p-value of 0.05 is used as a cutoff for statistical significance, although
this will depend heavily on the type of study that you're doing.
If you're doing a very precise experiment, for example, where you could do thousands
and thousands of trials, it may be more appropriate to use a p-value of 0.01 or even 0.001 as
a cutoff.
That means with a p-value of 0.05, though, for example, that means that about one in
20 studies with such a p-value for statistical significance, where the null hypothesis is
in fact true, will still give a positive result.
This would be a so-called false positive result.
Therefore, barely significant results are not as compelling as highly significant results,
and we expect from chance alone that the research literature will be full of studies that were
positive by chance alone.
It needs to be pointed out that statistical significance is not everything.
Unfortunately, studies are often presented in the media and elsewhere as if, once they
cross that magic threshold of statistical significance, that their conclusions must
be true or are very likely to be true, but this is not the case.
For example, if any systematic flaw or bias in how a study is conducted, whether experimental
or observational, can systematically bias the results in one direction.
If you do enough trials or data points, this can produce very statistically significant
results.
However, they will be erroneous because they're representing simply some bias in the study.
Therefore, you also need to consider the effect size, how big an effect is there.
The smaller the effect size, the greater the probability that some subtle bias was the
result of that outcome.
It becomes increasingly difficult to detect and weed out more and more subtle biases.
Therefore, very tiny effect sizes are always tricky to deal with.
You also have to be suspicious of effect sizes that are right at the limit of our ability
to detect them.
What system are we dealing with?
If you're dealing with a very stable physical phenomenon like electrons, then you expect
electrons to behave in a very consistent and predictable way.
However, if you're doing research on humans, for example, that are highly variable, highly
complicated, then tiny, tiny effect sizes are hard to be confident in because the possibilities
for subtle biases and influences are endless.
You also need to consider, was the data collection systematic and continuous?
This is often a subtle problem with studies that can be difficult to detect unless you
know the very specific kinds of questions you have to ask.
For example, if people are being surveyed, did everyone answer the survey?
Surveys are actually usually considered to not even be legitimate scientific studies.
They're considered to be unscientific because depending on the methods, they're often not
rigorous in their design.
For example, if you send out 1,000 surveys and 100 people send back their surveys, you
really don't have any idea about the likes and dislikes or the features that you were
surveying in that 1,000 people.
You only know about the 100 people who chose to respond to the survey.
That introduces a massive systematic bias in the data.
You can also, in other words, the people can be self-selective and you can't know what
bias has led to that self-selective data.
In a study of patients, you might also ask, was every patient or sequential patients included?
For example, a researcher might say that patients with a certain symptom who presented to their
clinic were tested with a drug versus a placebo or maybe even without a placebo, just a new
treatment to see what their experience were.
But was every patient counted?
Did the patients who had a good response come back while patients who did not have a good
response not come back for follow-up?
Were patients who refused the new treatment different than the patients who accepted the
treatment and what was the dropout rate or the dropout percentage of the study?
This potentially introduces a further bias into the result.
You may, when reading a study, find that, for example, 1,000 people were enrolled into
the study, but then there's data only for 600 people out of that 1,000.
Well, what happened to the other 400?
Those are called dropouts.
It's possible that the subjects simply did not follow up with the study, they were lost
to follow-up, or maybe they decided not to complete the study because they were having
side effects from the treatment or perhaps they were getting worse and figured that they
may be they're only getting the placebo and wanted to pursue a different treatment.
There are many reasons that introduce, again, significant bias into the results.
Generally speaking, a dropout rate of greater than 10 to 20% seriously reduces the reliability
of a study and calls the results into question.
In other words, is all the data being counted?
It's easy to create false results if only a subset of the data is counted for whatever
reason.
Skipping, dropping, or selecting data can be system, can systematically bias results,
making the outcomes misleading, all but worthless.
You can also ask about a study, was it prospective or retrospective?
A prospective study chooses subjects or objects to be observed and then observes their behavior
and outcome going forward.
A retrospective study looks back at events and outcomes that have already occurred.
Generally speaking, prospective studies are considered to be more rigorous because they
are subject to fewer confounding factors.
They also tend to be more systematic and the populations that are being looked at are
more representative.
With retrospective studies, there's already the opportunity for multiple sources of bias
to be introduced into the subjects or the objects that are being examined.
Another critical aspect of a study is was the study blinded or double-blinded?
In any rigorous study, the scientists that are recording the outcome of the results should
be blinded to whether or not what they're looking for is in the intervention or in the
control group.
In other words, no matter what feature they're measuring, they shouldn't know ahead of time
what result they're going for or what result they should be expecting.
This can introduce significant subconscious researcher bias into the results.
And historically, there are countless examples of research and studies that showed a phenomenon
that seemed robust and real when studied in an unblinded way.
And then as soon as the researchers were blinded to whether or not they were looking at intervention
or control, the effects vanish, they go away, meaning that all of their prior results were
due to nothing less than researcher bias, completely subconscious in many cases.
This does not only hold for studies with human subjects, even when measuring inanimate objects,
the person doing the measuring needs to be blinded to the status of what they're measuring.
All studies generally should be blinded in order to be reliable.
Observational studies, on the other hand, can only be partially blinded.
Again, if someone is counting up an outcome, or for example, they're looking through hospital
charts and trying to decide who had a complication or which patients had a stroke, they should
be blinded to whether or not the particular patient's chart they're looking through is
in the treatment or the control group, so that that can't bias any judgment calls they
make, how thoroughly they look, for example, for an outcome, or what they count as a complication,
how they score an outcome.
There's all kinds of possibilities to introduce error or bias when just looking at data.
Often we see that erroneous results disappear when proper blinding is put into place.
So that is a critical question to ask about any study, and essentially I don't trust
the results of a study that don't have at least the minimal proper blinding put into
place.
You can also ask about a controlled study.
Are the controls adequate?
What is the subject of the study being compared to?
Is the control treatment, for example, to use another medical example, truly inactive?
If you're using an active control, then that may obscure the comparison to the treatment,
or perhaps the placebo may actually cause a negative outcome, making the experimental
treatment seem artificially better, or the standard treatment to which a new treatment
is being compared may be ineffective, making the new treatment seem more effective than
it really is.
You can also ask, is the survey group being studied representative of the population that
you're interested in?
The poster child for this fallacy is the famous 1936 literary digest poll.
This poll predicted a landslide victory for Republican Alph Landon over Franklin Roosevelt.
However, this survey, this poll, was systematically biased in two key ways.
First it was a straw ballot sent out to a list drawn from telephone books and driver
registrations.
This biased the study population to the more affluent.
Also, there was a very low return rate, which resulted in a self-selection bias.
Historians now agree that these two factors conspired together to create an outcome that
was very different than the actual election which took place.
So individual studies can be preliminary, flawed, or they can be rigorous and methodologically
sound.
But either way, it's still a single study.
Often in the media, these single studies are presented regardless of their quality and
size as if they're definitive.
Well, scientists now believe X because this one study shows it.
But very few studies are so large, rigorous, and unambiguous an outcome that they can stand
alone, that they are considered definitive studies.
You always have to put individual studies into the context of the overall research, the published
literature as we call it.
The first thing to therefore consider when evaluating an individual study, even if you
think that the design is adequate and the results are therefore solid, is has this study been
replicated by independent labs and researchers?
If so, is there a consistency to the results or are the results mixed?
Are they all over the place?
Were the replications appropriate?
Did they actually look at the same thing and control for the same variables?
For example, in the late 1990s, Andrew Wakefield published a preliminary study looking at the
association between autism and the MMR vaccine.
The results of this study caused a firestorm, especially in the UK, where the public heard
the bottom line is that there is concern about an association between the MMR, the Mumps,
measles, and rubella vaccine, and a serious neurological disorder of autism.
And this led to a decrease in compliance with the vaccine and the return of measles and
mumps to the UK.
However, this was a single study.
As soon as the study was replicated by other researchers and other labs, it became increasingly
clear that this correlation did not exist.
It was not real.
Different kinds of evidence were brought to bear, and even the very specific kinds of
evidence that Andrew Wakefield looked for were replicated in detail, and it was found
that his results were erroneous.
They did not pan out in replication.
And other types of data being brought to bear found also that there was no evidence for
any association or correlation between risk of autism and the MMR vaccine.
But the damage was done.
The single preliminary study was reported, and that created fears which had spread to
other nations and still reverberate today.
So when looking at any research question, does a toxin present a risk factor for a disease,
or does a treatment work, and is it safe, for example, you have to look at all of the
literature and put it into context, not just individual studies.
There are other features to the literature that you need to take into consideration.
One is called publication bias.
In an ideal world, every single study that is performed would be published and would
be part of any systematic review of the literature in order to answer a question.
This is another way of counting all the data.
You can count all the data in an individual study, but you have to also count all of the
studies in order to capture all of the data.
Statisticians do what is known as a funnel plot, which shows the scatter of results from
different studies on the same question.
Statistics predicts that they should vary around the true effect size.
Every study won't have the exact same results.
There will be kind of a bell curve, a scatter of results, with some being more positive
and some being more negative, and then as studies get better and better quality, larger
size, more rigorous, better controls, then the variability in the outcome of those studies
should decrease, that should narrow, that the pyramid should come to a point at the most
definitive studies, which show the true effect that is being looked for.
The question then becomes is, when the best studies get done, does the funnel plot show
that the results zero in on no effect at all or a real phenomenon or a real effect?
However, sometimes what the funnel plots show is that the negative end of that pyramid is
missing.
The negative studies or the less positive studies are simply missing from the literature.
This reflects publication bias.
The tendency for researchers to make more of an effort to publish their study results
when the results are interesting and positive and good for their career and reputation,
and for journal editors to also have a bias towards publishing positive studies, the kind
that will get good press releases and draw attention to their journal as opposed to negative
results which are less interesting.
When researchers look at the literature, the arc of the research on any specific question,
we also find that effect sizes tend to shrink over time as better and better studies are
done.
So, not only does the evidence tend to zero in on the true effect size, but the effect
sizes in general of all the studies tend to shrink.
This is called the decline effect, and it exists even for real phenomena for things
that exist.
Again, the question is, does the decline effect cause the results to shrink down to zero or
do they shrink down to a persistent effect, although generally tending to be smaller than
the preliminary studies?
What this reflects is a researcher bias.
The less rigorous studies not only tend to be more variable in their results, they tend
to be more shifted or biased towards the positive, and then that positive bias gets worked out
as studies become more rigorous.
So truly, it takes time.
It may take years, more than a decade, for any specific research question to mature and
advance to the point where we're seeing reliable results.
Preliminary studies are very misleading and need to be considered preliminary.
One type of analysis that looks at many different studies that are addressing a similar question
or the same question is called a meta-analysis, and you may read that often, a meta-analysis
shows.
Now, this is not new data.
This is not a new observation or a new experimental study.
Rather, a meta-analysis is looking at previous studies, and then it's a mathematical method
of combining the results of multiple studies into a new statistical analysis.
This is a way of getting greater power.
If you combine 10 studies, each with 100 subjects on average, now you have one meta-study with
a thousand subjects and greater power.
However, the meta-analysis introduces new possibilities for bias.
It generally follows the rule of garbage in, garbage out.
If the preliminary studies were poorly designed and biased, the meta-analysis will just be
combining that biased and unreliable data.
So it still reflects the bias of these preliminary studies.
In fact, the predictive value of a meta-analysis has been looked at by researchers.
If you take any question that we consider we now know the answer to because large definitive
rigorous trials have been done, and then you look back at previous meta-analyses, you find
that a meta-analysis is a very poor predictor of the outcome of those definitive studies.
Maybe about 60 to 70 percent, which is not much better than chance or flipping a coin.
There are other types of systematic review that can be done as well.
Systematic reviews themselves look at all the evidence and they consider the quality
of each study.
That can also be done in a meta-analysis, but specifically a systematic review focuses
on this consideration of the quality of each study.
Looking for patterns in the literature, consistency, replication, relation to effect size and study
quality.
So for example, if we see when we look at all the literature that the results are very
mixed, different outcomes seem to be positive in different studies rather than the same
outcomes occurring.
Or we see that there is an inverse relationship between the quality of a study and the size
of the outcome with the best studies tending to be negative.
That kind of pattern in the literature reflects the null hypothesis, a phenomenon that is
not real.
Systematic reviews are a great way to look at the evidence in the literature.
They themselves are also subject to bias.
For example, which studies do you include in your systematic review?
What methods did you use to find studies to include in the review?
What were your inclusion criteria?
All of these are choices made by researchers that can affect the outcome of the systematic
review.
It should also be noted that there are organizations dedicated to doing high quality systematic
reviews.
In my own area of medicine, for example, there is the Cochrane Collaboration, an evidence-based
medicine organization that specializes in producing very high quality and thorough systematic
reviews.
Though they are not always considered definitive, they are highly relied upon by the medical
community.
In the end, there is always a judgment call to be made and there is no replacement for
that judgment.
For example, as Andrew Lang wrote, an unsophisticated forecaster uses statistics as a drunken man
uses lamp posts for support rather than illumination.
You don't want to use scientific evidence in the same way to support ideas and beliefs
that you already have.
Rather you want to use the scientific evidence in order to determine what you should believe.
You should back away from any of your biases and your a priori conclusions and let the
evidence take you where it will by doing as unbiased an evaluation as possible.
All this complexity in designing studies, scientific studies and evaluating the literature
is a way of compensating for the flaws and weaknesses in our brains.
It's important to use the evidence to figure out what is true, not to defend what you already
wish to be true.
