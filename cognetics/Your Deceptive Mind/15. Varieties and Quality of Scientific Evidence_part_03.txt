experimental or observational studies.
Another question to ask is, were the results statistically significant?
This is often expressed as a p-value.
The p-value literally is the probability that you would have gotten the results that you
did or greater given the null hypothesis.
The null hypothesis is the hypothesis that whatever you're looking for is not true, that
the phenomenon you're studying does not exist, as opposed to the evidence establishing that
there is a correlation or the phenomenon you're looking at is real and does exist.
Typically, a p-value of 0.05 is used as a cutoff for statistical significance, although
this will depend heavily on the type of study that you're doing.
If you're doing a very precise experiment, for example, where you could do thousands
and thousands of trials, it may be more appropriate to use a p-value of 0.01 or even 0.001 as
a cutoff.
That means with a p-value of 0.05, though, for example, that means that about one in
20 studies with such a p-value for statistical significance, where the null hypothesis is
in fact true, will still give a positive result.
This would be a so-called false positive result.
Therefore, barely significant results are not as compelling as highly significant results,
and we expect from chance alone that the research literature will be full of studies that were
positive by chance alone.
It needs to be pointed out that statistical significance is not everything.
Unfortunately, studies are often presented in the media and elsewhere as if, once they
cross that magic threshold of statistical significance, that their conclusions must
be true or are very likely to be true, but this is not the case.
For example, if any systematic flaw or bias in how a study is conducted, whether experimental
or observational, can systematically bias the results in one direction.
If you do enough trials or data points, this can produce very statistically significant
results.
However, they will be erroneous because they're representing simply some bias in the study.
Therefore, you also need to consider the effect size, how big an effect is there.
The smaller the effect size, the greater the probability that some subtle bias was the
result of that outcome.
It becomes increasingly difficult to detect and weed out more and more subtle biases.
Therefore, very tiny effect sizes are always tricky to deal with.
You also have to be suspicious of effect sizes that are right at the limit of our ability
to detect them.
What system are we dealing with?
If you're dealing with a very stable physical phenomenon like electrons, then you expect
electrons to behave in a very consistent and predictable way.
However, if you're doing research on humans, for example, that are highly variable, highly
complicated, then tiny, tiny effect sizes are hard to be confident in because the possibilities
for subtle biases and influences are endless.
