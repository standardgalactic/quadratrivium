One of the ironic points of this course, and specifically this lecture, is that you should
not trust even me, well, at least not absolutely.
The goal of this course is really to make you think, to think about thinking itself.
But no one person can be the definitive authority on any complex topic.
Besides, as we discussed in the previous lecture, the argument from authority is a logical
fallacy, an invalid form of argument.
On the other hand, some individuals do possess genuine expertise, and their opinions should
at least be taken more seriously than the average person.
This however, is itself a complex topic, and there are many levels of expertise.
I would also argue that true authority, if such exists, rests with the consensus of opinion
among relevant experts, not with any single individual.
Let's explore a bit more deeply what it means to be an expert, and the nature of scientific
consensus.
We rely on experts all the time.
Every time you visit a professional, a doctor, a lawyer, accountant, even a hairdresser,
to some extent you are deferring to the perceived expertise of the professional.
We could not function in our complex civilization otherwise.
Imagine if you were entirely responsible for every aspect of your life, maintaining
your car, diagnosing and treating complex illness, and reviewing contracts.
How about programming your own software, designing the electrical circuits in your
house, and countless other things we often take for granted?
Even if you can do many of those things, no one person can be an expert in everything.
Therefore, it's reasonable to defer to experts and to expertise, to people who spend their
lives mastering one small craft or one area of knowledge.
Although it's also a good idea to have enough of a basic understanding of important areas
of life that you can judge if someone is a true expert or perhaps is just faking it.
How can we know?
From one perspective, it's not easy.
It takes an expert to truly know an expert, but there are some things you can do to judge
genuine expertise for yourself.
For example, is the individual licensed in their trade or profession?
Can they document that they have had adequate training and have maintained their expertise?
Are their views representative of their profession?
Or are they out on the fringe?
And finally, does what they are saying make sense?
If it doesn't sound right, then you're probably better off getting a second opinion.
It is more difficult, however, when an entire profession is suspect.
Many professions become established, create the trappings of legitimacy, and even gain
licensure without ever developing true scientific legitimacy.
Psychics and astrologers, for example, often have their own organizations and institutions,
sometimes their own journals.
In some states, they are even licensed.
Licensure is often granted as a means of controlling an industry, establishing professional standards
and collecting licensure fees.
But too often, licensure is interpreted by the public as indicating that the body of
knowledge on which the profession is based is scientifically legitimate.
Unfortunately, in some cases, that is simply not the case.
The difference is between external and internal validity or legitimacy.
Licensure is usually all about internal legitimacy, following the rules of the profession, filling
out the proper paperwork, etc.
External validity, however, means that the body of knowledge has been tested against
reality, that there is a mechanism of self-correction and that there is transparency.
In other words, the processes of science.
Without the methods of science to truly evaluate a system of knowledge, there really is no
way to establish external validity.
Some professions, like chiropractic, for example, exist in a gray zone.
Some of their practices are supported by evidence, like lower back manipulation for
uncomplicated acute back strain.
That's actually much of what chiropractors do, and it is based on some clinical scientific
evidence.
Getting a back manipulation for an acute strain without any neurological involvement is effective.
But much of what they do is not validated by science and is, in fact, at odds with modern
medical science.
There is no system of external validity within the chiropractic community, however.
There is no universal science-based standard.
And so dubious practices proliferate and continue.
In fact, there are a great number of practices and even philosophies that exist under the
umbrella of chiropractic, because chiropractic is a licensed profession.
The bottom line is that there is no guarantee of legitimacy.
There is no one simple litmus test.
There are indicators, but nothing definitive.
And there is no substitute for a critical analysis.
And this brings us back to the question of consensus, because whose analysis are you
going to listen to?
But first, let's talk a bit more about expertise.
One common mistake is to consider someone who is an expert in one thing to be an expert
in all things, as if they can have general expertise.
Sometimes experts make this mistake themselves and stray from their true areas of knowledge.
We've spoken in a previous lecture about Lord Kelvin, a legitimate expert in physics, specifically
thermodynamics.
By the end of his career, he was the preeminent scientist in the world.
But that did not give him the expertise to make pronouncements about other areas of science,
such as geology, nor to declare that attempts at an airplane would never be successful.
Lord Kelvin goofed yet again late in his career when he declared the initial discovery of
x-rays to be a hoax.
But he did soon change his mind when he saw the evidence and even experience an x-ray
of his own hand.
The lesson here is that by the end of his career, Lord Kelvin had accumulated all of
the accolades a scientist can hope to get.
He had the ultimate authority as a scientific expert.
And he thought that this gave him leave to pontificate, to make pronouncements about
fields of expertise that were outside of his own.
And this resulted in various instances of folly on his part.
Sir Isaac Newton, the 17th century mathematician and scientist, is another example of a famous
scientist from history who maintained ideas that are now considered pseudoscientific.
Isaac Newton was the Lord Kelvin of his time, perhaps even more so.
He was sitting on the pinnacle of scientific achievement in his society.
While there is no question that Newton was a genius and his advances in physics and specifically
mechanics transformed our understanding of science at the time, many people do not realize
that Newton was also very interested in alchemy.
In fact, he wrote more about alchemy than any other topic.
Alchemy was no passing fad for Newton.
He spent much of his time engaged in alchemical research without any tangible results.
To Newton, alchemy was central to his beliefs about how the world works as much as mathematics
and physics.
As one biographer, F.E.
Manuel wrote, the more Newton's theological and alchemical, chronological and mythological
work is examined as a whole corpus set by the side of his science, the more apparent
it becomes that in his moments of grandeur he saw himself as the last of the interpreters
of God's will in actions.
Essentially, what he is saying is that Newton believed he was uncovering the mind of God,
figuring out God's master plan for building the universe, for how everything worked.
One piece of that grand design was alchemy and it was just as much a piece of the bigger
picture as was physics.
There's another phenomenon that we sometimes whimsically call the Nobel Prize effect,
but it is essentially the same thing.
A researcher who gets the ultimate iconic award, the Nobel Prize, is often looked upon
as an automatic scientific authority.
Linus Pauling, for example, is the only scientist to win the solo Nobel Prize twice.
Marie Curie also won twice but shared the first with her husband.
The Nobel Prize is, again, so iconic that it is the ultimate imprimatur of expertise,
but that did not stop Linus Pauling from adopting fringe ideas towards the end of his career.
Linus Pauling promoted the idea that high doses, so-called mega doses of vitamin C,
a thousand milligrams per day or more, could treat or prevent the common cold.
He later expanded this claim to include the flu rather and more serious illnesses and further
went on to claim that mega doses of vitamin C could help cancer and that high doses of
vitamins in general could promote health.
His claims were never based on adequate scientific research and subsequent research has shown
that his claims are essentially false, but because of his prestige he had a tremendous
influence on the public.
Many credit Pauling for the popularity of vitamin C as a supplement today.
Pauling made several of the errors that I'm talking about.
He was not a clinical scientist.
He was a very successful basic science researcher.
The expertise that he developed was in figuring out how the basic building blocks of life
worked.
However, he never did research into clinical claims, figuring out the net clinical effects
of taking an intervention like a vitamin or a supplement.
He therefore didn't really have the expertise to make the claims that he was making about
vitamin C, but armed with his two Nobel prizes he spread his beliefs, had an influence on
regulation in fact, and a profound effect on the public.
And now we know after 30 years of subsequent research that his claims were implausible and
simply not true.
Experts are also notoriously fallible, especially when asked to make predictions.
In his recent book, Future Babel, Dan Gardner presents the research showing how notoriously
inaccurate experts are when asked to make predictions about what will happen, even in
their narrow area of expertise.
More interestingly, he described the features of those experts who do a little bit better.
They tend to be less certain of themselves.
They consider a wide range of factors and information.
And they consider the consensus of their fellow experts.
Those experts who do worse tend to act like gurus.
They apply their one magic system or golden rule to explain a complex system.
And they do so with utter confidence.
Or there tends to be an inverse relationship between individual confidence and accuracy.
The bottom line is when it comes to predicting the future behavior of complex systems, even
experts are all but useless.
They do only a little bit better than chance.
But that does not prevent the popularity of asking experts to make such predictions.
And the evidence of how they do reveals a lot about the nature of expertise.
Those who are more humble, who defer to the consensus of opinion, and who are likely to
be very cautious in their predictions tend to do a little bit better.
Those who make the arrogant assumption that their one method can be over-applied to a
wide range of questions tend to do much worse and often no better than chance at all.
Predicting the future in general is highly problematic.
So-called futurism is very popular and may serve a purpose in terms of preparing for
possible future technologies.
But the track record of futurists is appallingly bad.
When asked to make a prediction for 50 years from now, Harvard professor Stephen Pinker
replied, this is an invitation to look foolish.
As with the predictions of domed cities and nuclear-powered vacuum cleaners that were made
50 years ago.
And he is essentially correct.
Trying to extrapolate something as complex as the development and application of technology
beyond just 5 to 10 years is utter folly.
We may be able to make very broad brushstroke statements about what is likely to happen
in the future.
But specific predictions have a track record of being sometimes humorously inaccurate.
Going back to the overconfident experts, this relates partly to what is now known as
the Dunning-Kruger effect.
Psychologists Dunning and Kruger in 1999 described how those who are incompetent are generally
unaware of their own incompetence.
The same failings that make them incompetent also make them unable to see it.
This leads to what is more casually referred to as the arrogance of ignorance.
New expertise is comprised of an appreciation for the limits of one's own knowledge and
the limits of the system of knowledge itself.
So understanding for example that science only knows so much at any one time.
Therefore this is not just about incompetence that may seem like a bit of a loaded term
but everywhere along the spectrum an expert needs to have a very humble and realistic
assessment of their own level of expertise.
Expertise is not a black and white thing.
I do not want to promote the notion of a false dichotomy.
You're either an expert or not an expert.
There is the full spectrum of expertise from some knowledge to significant knowledge to
being the world expert on a specific topic.
When it comes to scientific questions I also believe that there are two related but distinct
types of expertise.
The relevant science and critical thinking as described in this course and elsewhere.
Those with critical thinking skills but who lack specific scientific knowledge will still
have difficulty thinking about scientific questions.
Specific in-depth knowledge is still important to developing opinions about scientific questions.
For example an excellent science journalist with good critical thinking skills and excellent
investigative skills as a journalist may still come to wrong or inappropriate conclusions
about a complex scientific question simply because they're not scientists, they're not
experts in that field and they don't have the relevant background knowledge to put what
they are being told into the proper context.
Also scientific knowledge itself may not be enough without critical thinking skills.
In the lecture on scientific blunders we covered the folly of many scientists who apparently
were insufficiently skeptical of their own claims.
This led them to make critical thinking errors that resulted in serious errors.
In order to avoid these kinds of errors we need science and critical thinking both.
Another example might be another Nobel laureate, Luc Montagnier, a French researcher who received
the Nobel Prize for his research into HIV, the human immunodeficiency virus.
However, later in his career he went on to promote notions of homeopathy, the idea that
water itself can contain the complex information in molecules and transfer a healing or medical
effect in this way.
Now this runs contrary to a mountain of science and these claims are not accepted by the scientific
community and clinical evidence shows that homeopathic remedies do not work.
But Montagnier has continued to promote these claims despite the fact that they are on the
fringe because he has gone beyond his expertise.
This requires knowledge of physics, not just of virology.
And his Nobel Prize has gone a long way in order to give him the patina of expertise
in claims that he really isn't an expert to make.
But sometimes scientists believe that their scientific knowledge is enough.
This can make them easier to fool in fact because they will be overconfident in their
own ability not to be fooled.
In fact many magician friends of mine have told me that they would prefer an audience
of scientists over anyone else because they're the easiest to fool.
A famous example of this was Project Alpha.
Researchers at Washington University were given a grant to test alleged psychic abilities.
Magician James Randy offered his consultation as an expert in deception.
He specifically offered to show the experts, the researchers, how not to get fooled while
designing their experiments into the possibility of psychic abilities.
However, his offers of consultation were refused.
The scientists didn't think that they needed the advice of a magician.
They assumed that as scientists they had the relevant expertise to do proper scientific
studies and to not be fooled.
To demonstrate the need for specific expertise in deception in ESP research, James Randy
sent in two teenagers, Steve Shaw, then 18, and Mike Edwards, then 17.
They were not trained magicians at the time.
Steve Shaw did later on go on to be a famous magician with the stage name of Banachek.
But at that time they did have some skills and Randy did make sure they had the relevant
skills to do the ESP tricks.
In the end, they were able to completely fool the researchers with simple sleight of
hand tricks, most of which were just ad hoc opportunistic tricks they made up on the spot.
The researchers were overconfident in their research skills but were not prepared for
deliberate deception.
In the end, they were completely fooled because they did not have expertise as skeptics.
Individuals are biased, quirky, flawed, have incomplete knowledge and intended to overestimate
their own knowledge and expertise.
But some people have spent years mastering arcane knowledge.
It's no guarantee of legitimacy or correctness but it's at least worthy of being taken seriously.
The quirkiness of individuals is why a consensus of opinion is much more reliable.
No biases will tend to average out.
Gaps in knowledge and perspective will also tend to compensate for each other.
You end up with what is sometimes called the wisdom of the crowd, in this case the wisdom
of the crowd of experts.
Within science, differences of interpretation will tend over time to get resolved by seeking
better evidence.
Eventually, consensus is built on solid ground.
One example that is often given inappropriately, in my opinion, to represent how the consensus
of opinion can be wrong is one that shows how consensus is built.
In the early 1980s, researchers proposed the idea that a particular bacterium, H. pylori,
was a cause of gastric ulcers.
Prior to that, the consensus of opinion was that gastric ulcers were caused by stress
leading to the secretion of too much stomach acid.
Often the story is told as one of resistance to this new idea and that it took 20 years
for the scientific community to finally accept the notion that H. pylori causes ulcers.
But a review of the literature tells a very different story.
In fact, research interest into the notion of H. pylori started immediately upon the
proposal of this new idea.
The early research was encouraging and therefore more researchers got involved.
The number of papers published each year increased dramatically until a threshold of evidence
was achieved, finally resulting into a change in the consensus opinion in line with this
new evidence.
Now the consensus of scientific opinion is that H. pylori is an important cause of gastric
ulcers.
This does not mean that the consensus of scientific opinion is always correct.
It is in fact often incorrect and will then have to change over time as new evidence comes
in.
But the probability of being correct and scientists all about the probability of being
correct not absolute certainty is higher than the opinion of an individual.
If you are going to disagree with the consensus of scientific opinion therefore you should
at least have a very good and compelling reason for doing so.
It shouldn't be rejected out of hand.
Sometimes it may be difficult to determine exactly what the consensus of scientific opinion
is on a topic.
Different specialties may have different opinions.
For example, geologists are fairly convinced that a single asteroid wiped out the dinosaurs
65 and a half million years ago and this view has dominated in the public consciousness
and in the media.
However, many paleontologists are not convinced.
There was certainly an asteroid impact at that time but some paleontologists feel the
fossil evidence suggests a longer die off of the dinosaurs.
Perhaps the asteroid was just the coup de grace.
There are other factors that were going on at the time that could have also contributed
to maybe even significantly to the mass extinction that occurred at that time.
So there is in a way two different consensus of opinion on this question.
There is a consensus of paleontologists which is different than the consensus of the geologists.
So it depends on which specialist you ask.
What about global warming?
The nature of the consensus has been intensely debated within the public sphere.
This is perhaps the most controversial issue today when it comes to what is the scientific
consensus.
The international panel on climate change, the IPCC, represents a consensus of opinion
among climate scientists.
But some have criticized that process as biased and highly political.
It seems that there is a robust consensus of scientific opinion that the earth is warming
and anthropogenic causes are important to this warming.
But there remains minority dissent.
Political entities with strong opinions on this issue have challenged the notion that
there is a consensus and have promoted the views of the minority of dissenters.
A 2011 study by Richard Muller, a physicist at the University of California, Berkeley,
the Berkeley Earth Surface Temperature Study, sought to replicate prior analysis by NASA
and other bodies of earth temperature data.
Muller was skeptical.
He was in the minority dissenting opinion.
He believed that the previous analyses were poorly conducted and he did not accept their
conclusions that the planet was warming.
So he set out, if anything, to disprove that conclusion.
He and his team conducted their own analysis of all the available data.
They came up with their own statistical methods to analyze the data, to make corrections for
the different sources of data, for example.
And they looked at all the data sets that were available, whereas the prior analyses
looked at only a subset of the data.
They also made their data and analysis completely bit transparent by publishing all of it online.
So anyone could access it and decide for themselves if they are trained to do so, if their analysis
is correct.
In the end, they concluded that the earth is, in fact, warming by 0.9 degrees Celsius
over the last 50 years.
Further, their temperature map closely matches that of NASA and the other previous analyses.
Not only confirming that the earth is warming, but confirming the validity and the legitimacy
of the prior analyses, which were the very analyses that were criticized by the dissenters
from the global warming consensus.
In the end, Mueller changed his mind and accepted the fact that the planet is warming.
This is an excellent example of how science should work.
This agreement was resolved by further analysis, by the data.
In the end, the data is king.
That is what held sway over Mueller's opinion.
And he should be congratulated on doing exactly what he did, a detailed analysis replicating
prior studies and ultimately building on the consensus.
And in the end, the consensus on global warming was strengthened by his work.
Although it doesn't seem likely that this will end the political controversy that surrounds
the many types of claims that are being made about global climate change.
The consensus of scientific opinion is an important factor to consider when evaluating
any claim.
A robust consensus should be built on evidence hammered out in the research and at meetings.
It is more reliable than the opinions of any individual.
But it is no guarantee by itself of being absolutely correct.
As the case with Mueller showed that the data always must hold sway.
And as new information comes in, new ways of analyzing data and new ways of interpreting
the data, new ideas come to the forefront, the consensus will slowly change and shift.
But there are some scientific questions that have been around for so many years for which
there is such a mountain of evidence that the current consensus of opinion is not likely
to change ever.
I don't think it's ever going to be discovered that DNA, for example, is not the molecule
of inheritance.
So while it is reasonable to respect a hard one consensus of scientific opinion, there
is also a role for dissenting opinions.
As with science in general, there is never any guarantee of being absolutely correct.
The goal is simply to maximize the probability of being correct while remaining open to new
evidence, new analyses, and new ideas.
