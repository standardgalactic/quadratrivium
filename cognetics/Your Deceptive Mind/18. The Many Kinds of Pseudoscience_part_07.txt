They are wrong rather than correct more often than chance.
Their psychic ability is keeping them away from the correct answer rather than turning
them towards the correct answer.
Another example is optional starting and stopping.
This is the notion that, well, some individuals with ESP may require a warm-up period before
their abilities kick in.
And then their abilities may fatigue, they may wear out and stop working at some point.
Both of these are excellent examples of inadvertent mining of the data or cherry-picking the data,
looking for and counting only those data points that are positive and eliminating the negative
data points.
That missing, for example, essentially doubles your chances that there will be a run of guesses
that are statistically significant.
Not only a positive run counts, but now a negative run also counts.
Optional starting and stopping is simply a way of cherry-picking a random streak or run
in the data that hits more often than chance by just the clumpiness of random data.
That is nothing less than a way of picking out the hits and eliminating the misses from
a random string of data.
With both of these, you have to, however, look at all of the data and look at it all
in a consistent, statistical way in order to say if there is a real phenomenon happening.
And when you do this, the research shows that there is no phenomenon.
These are not extra streaks of missing or extra streaks of hitting in the middle of
the data.
When you look at all the data, it all averages out to no effect or to the null hypothesis.
The sign missing and the optional starting and stopping comes from a flawed sense of
probability.
It ignores the fact, as we discussed in previous lectures, that in any random sequence, there
will be runs of clumpiness, of chance hits and misses, that will occur by chance alone.
This is probably why this type of research has never been reproduced, meaning that SI
research occasionally comes up with a type of experiment that the researchers claim shows
a SI phenomenon.
But when other scientists try to reproduce the results, they find that they can't.
Any single study could be a misinterpretation or could be a statistical fluke.
If you use a cutoff of a p-value of .05, for example, to use statistical jargon, what essentially
that means is that one in 20 studies with that cutoff will show a statistically significant
effect even when there is no phenomenon at all.
These are so-called statistical false positives.
Essentially, if we mined the studies themselves for the positive ones, and there is a name
for that called publication bias, where you are only motivated to publish those studies
which show something interesting, a positive result, then we will create the false impression
that there are studies which show a SI phenomenon.
However, if it was just a random fluctuation, a clumpiness of the data, then we would expect
