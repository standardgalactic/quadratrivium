It is the, in a certain sense, the philosophical opposite of top-down, of top-down artificial
intelligence.
We'll get to that.
And there were people in the 1960s and early 70s, certainly in the 1960s, between 60 when
the perceptron was demonstrated and the mid-1960s.
And Minsky and Papert, for whatever reason, but it seems to me that they needed to clear
the field and not have DARPA bombarded with rival approaches to artificial intelligence.
They published a paper which showed that the perceptron was hopelessly flawed and could
never solve any more than the most primitive kind of staged logic problem.
And as a matter of fact, neural net development stopped cold until the mid-1980s.
Now between 1970 and early 1980s, I mentioned before, there was a flood tide of programs
that had considerable practical and eventually commercial value.
I mean, 1967 was the first program that played chess and 30 years later, Deep Blue beat the
World Chess Champion in a tournament.
And today, the very latest chess playing programs really are international grandmaster
status programs.
Nevertheless, by the early 1980s, there was a feeling that top-down AI was stagnant.
In spite of a tremendous portfolio of accomplishment, because even though computers were getting
faster and more powerful, the amount of information you had to plug in to make them be able to
do anything intelligent seemed to grow even faster.
It looked like a hopeless thing to try to plug into a computer, all of the kinds of
things that a child seems to learn in the course of the first few years of its life
so that it knows how to function in the world.
And concurrently, it was shown that Minsky and Papers hatched a job on Perceptron was
not correct.
That by a very modest adjustment, the Perceptron could in fact solve significant problems.
And in the mid-1980s, neural nets came back.
Now, neural nets are computers whose circuitry is designed in such a way that the input circuits
and the output circuits are linked by almost random interconnections in between.
And there can be an inner layer of processors that the input is connected to, the output
that could be multiple inner layers, but it gets complicated pretty quickly.
So most people keep it to one or at most two intermediate layers.
You put in the input, whatever it is, let's suppose that you want the computer to read
English language sentences.
So you show it, so to speak, a line of an English language sentence, and you want it
to sound it out.
And it's designed so that it makes noise in response to photons that are reflected off
the printed page, but it doesn't know an A from a B.
So it starts making random noises.
You train it by putting in, this is what the sentence sounds like.
You train it so that it keeps going until it finds, until it does it right.
Then you say, right.
The computer then essentially adjusts its own internal circuitry so that it reinforces
doing it that way again.
So neural nets are trained, they are not programmed.
We do not control what's going on in between the input and the output.
This is obviously an extraordinarily simplistic and hopefully not too misleading account.
But what you have going on inside a neural net computer is activation patterns.
Different nodes are being activated depending on whether the output is the correct output
or not the correct output.
And using, you may remember what I said in the first lecture on psychology about Edward
Thorndike's 1911 book on animal intelligence and the two laws that he came up with, that
behavior is shaped by its consequences and reinforced by repetition.
That's the basic logic of neural nets.
Are activation patterns thoughts, feelings, consciousness, are they concepts?
Neural nets work.
And in fact, they are terrific at recognizing patterns, at finding relationships.
And there are many commercial neural net programs now that are out there in the world and the
programs continue to be increasingly sophisticated and developed.
Interestingly, very often neural net computers are simulated on fundamental architecture
computers because it's easier to do it that way.
You can actually run a program that simulates a neural net on a non-neural net computer
and get away with it.
But there are also neural net computers from the ground up, the dedicated neural nets.
So this is neural nets represent a bottom up approach to artificial intelligence.
And one can say I think today that both approaches are being aggressively pursued.
