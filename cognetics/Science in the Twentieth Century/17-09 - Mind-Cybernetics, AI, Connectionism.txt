And McCulloch and Pitts piped up with,
hey, our neural net model of the nervous system
can do that too.
If you plug these problems into our neural nets,
then the answers come out at the other end.
We can do that.
Now, that suggested an interesting way
of attempting to model what goes on in the mind,
neural net modeling.
Just set that aside for the moment.
Shannon's electrical circuits of how you could input A and B
and get input to signals and get as an output A or B, A and B,
if A then B, A, then not A, et cetera.
Those are the classical logical rules
for deductive inference that he designed those circuits.
Those circuits became the basic circuits of computer chip
design to this very day.
That was published in 1948, the very year
that John von Neumann was at Princeton designing
the first stored program digital electronic computer.
ENIAC was not a stored program computer.
It had to be programmed every time you used it.
It was called EDVAC and was built at Princeton.
It actually became operational in 1949.
The architecture of that computer, which emerged out of,
in part, out of von Neumann's participation
in the Macy conferences and, of course, his own work
on Turing's publications in the 1930s,
became the basic architecture for all digital electronic
computers, virtually all digital electronic computers,
and again, right into the 21st century.
The von Neumann architecture is that a computer,
the way you use a computer is that it processes and signals
serially one at a time, but it does it very fast.
So it's a serial processor in which a program is written
that controls the state of the computer at every moment,
at every step of the way, what the computer does,
it does because it was instructed to do it.
So it's an absolute control system.
That's the von Neumann architecture,
and it is overwhelmingly the design of all computers.
The difference is that there are some starting,
especially in the 1980s, some computers
that have multiple processors.
So they can do a certain amount of parallel processing,
but it always comes down to all that processing converges.
The alternative to the von Neumann architecture
was the McCulloch-Pitts neural net architecture.
And that's hanging in the wings.
And was, so to speak, overwhelmed by the power
of Shannon circuit designs, von Neumann's architecture
for EDVAC, which then sort of propelled
the first few generations of the computer industry.
In 1956, by 1956, enough work had been done in this area
of trying to model reasoning by using computers
that a conference was held at Dartmouth in 1956.
The chairman was John McCarthy, a pioneer computing
scientist, and the title of the conference
included the phrase artificial intelligence.
By 1956, people believed it made sense
to talk about artificial intelligence.
Notice, by the way, the assumption here
is that mind is just reasoning.
All the other things we do, love, hate, feel,
the all of our attitudes, hopes, all of that,
mind is reasoning.
If we can model reasoning, we've got an artificial mind.
Now, 1956, and how did it come to be?
What was the background to this?
Well, as early as 1952, an IBM research scientist
named Arthur Samuels started playing around
with a computer program that played checkers.
And over the next 10 years, he developed a program that
became the world checkers champion.
In 1955, Herbert Simon and Alan Newell, Simon's name
has come up in multiple fields as well,
published the first program that they claimed could reason.
It was called Logic Theorist.
And it was a computer program that
would solve propositional logic problems, classical
propositional logic problems.
In 1957, they published a more sophisticated program
that they called the General Problem Solver that
solved a broader range of logical and mathematical problems.
And these were just a few of the things
going on in the background that led McCarthy
to announce a conference to get people together who
were writing these kinds of programs and to share ideas.
This was in 1956.
In 1963, the Defense Advanced Research Project Agency,
D-A-R-P-A, DARPA, which over the last 40 years or so,
has had an enormous influence on advanced technological
development in the United States.
Their charter is to leapfrog current technology,
to leapfrog what's on the drawing boards,
and go beyond that to what will be on the drawing boards
in five years in order to make those technologies available
to primarily to the Defense Department.
But in 1963, the DARPA director decided
that there's a very interesting thing going on here
in machine-aided cognition.
They were not prepared to use the phrase
artificial intelligence.
But they gave MIT a modest grant, $2.2 million,
in order to set up a research center for machine-aided
cognition.
This was an ethical event in the field.
Marvin Minsky became the director
of the Artificial Intelligence Lab, the AI Lab at MIT,
which is today still one of the great world centers
of artificial intelligence research.
