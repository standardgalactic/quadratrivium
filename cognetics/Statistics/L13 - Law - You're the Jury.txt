Welcome back to Meaning From Data, Statistics Made Clear.
With this lecture, we begin the second part of the course, which, as advertised, looks
at applications of statistics to different areas.
In this lecture, we're going to look at applications to the law.
And before I begin, let me just say that it's not the intent of these lectures to give any
sense of overview of the potential applications of statistics to the different areas.
Instead, what we're doing is just picking out some interesting individual applications
and looking at them.
No attempt to give a global view.
But certainly in the law, there are many places where statistical data and inferences are
used to help in the making of legal decisions.
And in this case, we're going to look at two cases where you will be the jury.
And in fact, there's a good reason to think of the law as a good application area as the
first application area to look at in this second part of the course in statistics.
Because really, ultimately, statistics is a reasoning tool.
It's a tool by which we can make decisions about the world.
It helps us to evaluate evidence and construct persuasive arguments about things.
So the idea of imagining ourselves as on the jury, I think it really puts us in the right
frame of mind to think about the real purpose of statistical analysis.
In this case, we're going to present some evidence and interpret the evidence to see
how significant that evidence may be to make a decision.
And in particular, in the examples that we'll give today, you'll see that the evidence may
not actually have the implications that at first appears.
So we'll begin with case number one.
Now remember, you are the juror.
Here is the scenario.
This is a hit-and-run accident case.
The scenario is that at twilight of one night, Mr. Jones was taking his garbage out to the
side of the street.
And he looked down the street, and a couple blocks away, he saw an accident.
A taxicab drove into an intersection, hit another car, and then drove off.
Well, a cabbie has been accused of this crime and is the defendant.
And the evidence is quite convincing up to this point.
But the jury is in weighing the balance of guilt has decided that the guilty verdict
will hang on the testimony of Mr. Jones, as I witness.
And in particular, it's going to hang on the issue of whether Mr. Jones saw a green cab
or a blue cab.
So here's the way the evidence was presented, and then you're going to evaluate the import
of the evidence.
So the prosecutor asked Mr. Jones, what did you see?
And Mr. Jones said, well, I thought that the cab was blue.
I definitely saw a cab.
Everybody agrees it was a cab.
And I think it was blue.
I'm pretty sure it was blue.
And you know how lawyers are, then they'll jump on that.
What do you mean pretty sure?
How sure are you?
Well, Mr. Jones, fortunately, having taken some statistics courses, was ready with a
response to this, because he said, well, I'll tell you how sure.
I'm 80% sure, because here's what I did.
We took thousands of trials where we randomly chose blue and green cabs at Twilight.
And I stood out by my garbage can, and I looked down at exactly the same conditions, the exact
same time at night.
And we did this experiment many times over.
And it turned out that I was able to, in 80% of the times, say correctly that it was blue
when it was blue, and 80% of the time when it was green, I was able to identify that
it was green.
So the prosecutor summarized the situation by saying that Mr. Jones is 80% sure that
the cab was blue, and therefore the jury should convict.
Well, it sounds pretty persuasive, sounds pretty persuasive, but the defense lawyer hasn't
started yet.
The defense lawyer then comes up and has some additional information and poses a hypothetical
question to the jury.
The additional information is that it turns out that there are exactly 100 cabs in this
particular city.
90 of the cabs are green, and 10 of the cabs are blue.
Now, everybody agrees that Mr. Jones is 80% accurate in correctly identifying green cabs
as green or blue cabs as blue in the conditions of the moment when he was the witness.
But let's do the following thought experiment.
Knowing that there are 90 green cabs in the city and 10 blue cabs in the city, before
we begin thinking about the guilt of this cabbie, we have to be open-minded and imagine
that it's equally likely that any one of those 100 cabs might have been the guilty cab.
Well, the eyewitness is correct 80% of the time.
Let's just see the implications of that.
If we did the thought experiment of all 100 cabs in the city having this accident and asking
ourselves what would the witness, Mr. Jones, testify over those entire 100 different trials
Here's what he would do.
For the 90 times that the cab was green, since he's 80% correct, on 18 occasions he would
actually testify that he thought it was blue, because you see he's wrong 20% of the time.
20% of 90 is 18.
On the other hand, if in the 10 instances in which the cab actually was blue, he would
say that the cab was blue 8 times, and 2 times he'd say it was green.
So in his testimony about the entire city of cabs, the entire 100 cabs, he would actually
testify that it was blue in 26 times of those testimonies.
In other words, 18 times when the cab was green, he would say it was blue, and 8 times
when the cab actually was blue, he would say it was blue.
So the total number of times that he would testify blue is 26 times, but it actually
was blue only 8 times.
So the probability that the cab actually is blue, given the testimony that he says it's
blue, is actually only 8 out of 26, or 31%.
So given this interpretation and this analysis, you on the jury now have a very different
view of the likelihood of the cabs actually being blue, and in fact you should acquit.
Rather interesting.
So this was an example where your initial impression about the cabs was changed. 69%
of the cars, when the witness would say they were blue, actually turned out to be green.
Let's look at another example that involves a similar statistical issue.
This time, let's imagine that we're in a very large company. It has 280,000 employees, and
we're concerned with drug use, particularly of employees, particularly for a very serious
kind of a drug.
And so in the wisdom of the company, the company decides, well, why don't we think about instituting
universal drug testing?
Just test every single person in the company, and then we'll find out whether or not we'll
detect who is using this very dangerous drug.
Well, you need to look into the accuracy of the test.
What we do is we find a test, and we can actually measure the quality of the test by doing experiments
with the test, and seeing how often the test actually correctly identifies a drug user.
And let's suppose that in this particular test that it is 95% accurate if the person
uses drugs, that is, it says the person uses drugs 95% of the time that the person does
use that drug, and 1% of the time it will say that the person uses a drug even though
that person does not use the drug.
So let's just do some analysis and see what would happen if we instituted universal drug
testing.
So here we go. Let's just start looking at some data. Let's suppose that the reality
is that about 500 people use this serious drug out of the 280,000 employees. So the reality
is that there are 279,500 non-users and 500 users of this drug. Well, the drug test, as
we said, is 95% correct for the drug users, it identifies them as users, and 99% correct
for the non-users. Let's see what happens when we apply the test. Applying the test
to these non-users, most of the time, 276,705 times, it correctly says they're non-users,
but 1% of the time it gives a false positive, 2,795 times, it says that the person is a
drug user even though that person is not. Among the users, 475 times, the test says that
the user is a user when indeed that person is a user. 25 times does not identify that
person as a user.
Let's see what happens overall if we gave this test to everybody in this company, these
entire 280,000 employees. Well, how many times would it come back positive, saying the person
is a user? Well, it would be these 2,795 times plus the 475 times here for a total of 3,270
times. It came back as a positive test. However, there were only, actually among that group,
only 475 of those people were actually users of this drug. That means that if an employee
got a positive drug test, the actual correct interpretation of that positive result is
that there's only a 14.5% chance that that person actually is a user.
So there's a great danger of inappropriate accusations or firings if you instituted this
policy. And in fact, I think there'd be a class action suit against the use of this policy
because you were condemning so many people for drug use that when in fact only 15% of
the people with a positive test actually used the drug. Specifically, 85% of the people
who test positive for that drug are not actually users of that drug. Next, let's turn to a
different kind of a legal issue. And this is an issue having to do with discrimination
on the basis of gender. This is one of the types of cases that come up all the time.
You want to know whether or not in a hiring practice or acceptance to a university if
there are discriminatory practices that make it unfair in dealing with different categories
of people. So let's just look at an example and see whether or not we are going to be
convinced by the data that this university has committed a serious crime of discrimination.
So let's look at this particular example. And here is a hypothetical university admissions
case where there are a total of 2,000 applicants to this particular university. 1,000 men are
going to apply and 1,000 women apply. Now let's assume for the sake of argument that
all of these people are completely equally qualified for admission to the university.
Of course this is not realistic, but these are some of the kinds of things that you have
to presume if you are going to make a determination about discrimination. You have to eliminate
some of the variables. So we will eliminate all of the variables except for the gender.
So we have 1,000 men and 1,000 women who apply to the school and look what happened. 70 percent
of the men were accepted and only 40 percent of the women were accepted. Well, we should
be outraged. Or should we be outraged? How do we decide whether we are outraged at this
blatant discrimination against women? Well, we have to think. We have to do some analysis
and say how rare an occurrence would it be for only 40 percent of women to be accepted
and 70 percent of the men to be accepted if the overall acceptance rate as it is here
is 55 percent. In other words, just by random luck alone, if you randomly chose from 2,000
people or don't even think of them as people, suppose you had poker chips and you had blue
poker chips and red poker chips. You had 2,000 of them and you randomly selected 55
percent of them. What's the chance that the difference in the proportion of blue ones
that you had differed as much as from the proportion of reds as we have here, 70 percent
versus 40 percent? Well, you see that this is putting us in the realm of the kind of
statistical and probabilistic analysis that we have become accustomed to in the last several
lectures. Namely, we can look at all possible gatherings of 1,100 out of this 2,000 and
see what fraction of those have a disproportionate balance that is recorded in this example.
In fact, there's a specific test that accomplishes this. Namely, what we do is we look at every
box in our table and we know that there's an expected value, namely 55 percent of the
men. If 55 percent is the overall acceptance rate, then the expected value in this square
would be 55 percent of the 1,000 men. 550 would be the expected value here and the rejection
square would be 450. Similarly here, 550, 450. So those are the expected values. We can
take the observed values minus the expected values, square it and divide by the expected
count and add them up over the four squares and this gives a statistic called the chi
square statistic. And all it's doing is what I previously said. It's looking at what you
would expect by random chance alone and then you look up on a table to see how far the
number that you get, if it's a larger number because of the fact that the expected value
versus the actual values are differed, differed by a lot, then when you square them and add
them up, you get bigger numbers. If you get a bigger number, then that's an indication
that there is some sort of unlikeliness to the outcome. In this case, the value is of
that, of adding up those numbers is 182 and we see that the chance of that actually happening
is extremely small. 2 times 10 to the minus 16th, that means you put a decimal down, you
put 15 zeros and a 2. That's the probability that there'd be that much of a difference
in our count. So it sounds like if we're on the jury, we're going to say there's great
discrimination against those women, right? Okay. Oh, wait a minute, though. I forgot to
tell you something. I forgot to tell you something. And what I forgot to tell you was that actually
the university had two sub-programs, and the university had two sub-programs, and the university
had two sub-programs, and although it's true that 2,000 people applied to the university,
actually there were two sub-programs. And one of the sub-programs is an excellent sub-program.
It's an excellent program, and it has higher standards than the other program, which is
the mediocre program. Now, in this excellent program, 200 men applied and 800 women applied.
The overall acceptance rate to this excellent program was only 24%. It's a more discriminating
program than the overall university. And look what happened. The men had an acceptance rate
of 20%, and the women had an acceptance rate of 25%. So for the excellent program, the rate
of acceptance for the women was actually higher than the rate of acceptance for the men. Oh,
wait a minute. Wait a minute. Now let's look at the mediocre program. In the mediocre program,
800 men applied and 200 women applied. And look what happened there. For the men, 82.5%
were accepted, but every woman who applied to this mediocre program was admitted. Now,
let's make sure you understand here what's happening. What this says is that overall,
let's go back to the chart. Overall, the acceptance rate of the men was 70%, and the
acceptance rate of the women was 40%. But in each of the sub-programs, in the excellent
program, the acceptance rate of the women was higher than the acceptance rate of the men.
And also in the mediocre program, the acceptance rate of the women was higher than the acceptance
rate of the men. This possibility, first of all, it occurs because of the fact that there
are two sub-programs. And in the two sub-programs, there's a different rate of acceptance. And
more women applied to the tougher sub-program, and consequently, the effect of this was this
paradoxical feature that in both of the sub-programs, it looks, in fact, as though the
discrimination is going the other way, right? Because in the sub-programs, here the women
have a higher acceptance rate in both the mediocre program and in the excellent program.
In both cases, the women have a higher acceptance rate. This is an example of a phenomenon called
Simpson's Paradox, which is where the overall percentages seem to go one way, and yet both
of the sub-categories go the other way. I think it's a fascinating issue. Now, if you're on
the jury, now you have to decide, should you be outraged that the men are being discriminated
against in each of the sub-programs, you see? And so how do we do that? Well, once again, we
would use our statistical analysis to see how unlikely is it if the overall acceptance
rate is 24% to a program, and this many men apply, 200, and this many women apply, what
is the chance that the acceptance rates would be in this category? And we can run the numbers
and see that there would be a 0.14 chance. It would happen 14 times out of 100 just by
luck alone that you would have that much of a disparity in the acceptance rates of the
men and the women. And same thing for the mediocre program, we can do the same kind of analysis,
and in that case, we see that it's extremely unlikely that we would have as much disparity
in the acceptance of the men and the women, so that in this sub-program where all of the
women were accepted, it would be very unlikely by luck alone that you would, when you chose
86% of the total applicants, that the imbalance of the proportion of the women being accepted
to the men being accepted would be as great as it is. And so this would be evidence, if
you wanted to convict, you would want to say that in fact there was discrimination against
the men. So this is a wonderful example where your first look at the data appeared to say
one thing, but the second look at the data appeared to say something entirely different.
Now, I'd like to discuss two other jury issues that came up during the OJ Simpson trial.
As you recall, when OJ Simpson was on trial, Johnny Cochran was the lead defense lawyer.
And during the course of the trial, there was some interesting statistical evidence
usually by the prosecution, but sometimes by the defense as well. And this is an example
of a statistic that was presented by the defense. At the trial, evidence was presented that OJ
Simpson on previous occasions had beat his wife. You may remember this, he'd beat his
wife, and that was part of the evidence, there were 911 calls and so on, that showed that
he had beat his wife. And the prosecution had put forward this evidence as an indication
of his habits of violence. Well, the defense said, made the following argument. They said,
well, we've looked at statistics and we've found that only one out of a thousand people who beat
their wives go on to murder them. And so they presented this argument as exculpatory evidence
that in fact, you know, that OJ Simpson would probably not be guilty because, look, only
one out of a thousand people who beat their wives actually go on to murder them.
Now, I bring this up for a couple of reasons. The first one is, if you hear something that
sounds ridiculous and is claimed to be the result of a statistical argument, then you
should be very skeptical of it. It probably is ridiculous. Of course, it's ridiculous.
The argument that the fact that he beat his wife is exculpatory to the idea that he murdered
his wife, that is on the face of it ridiculous. Let's look at why the statistics don't actually
mean what it was presented to the jury as meaning. First of all, if one out of a thousand people
who beat their wives go on to murder their wives, the question is, how many people, how does
that figure compare to the people who don't beat their wives? You see, it's not true that
among the whole population of people who don't beat their wives, that one out of a thousand
of us, you know, murders our wives. That doesn't happen. It's much lower than that. So in fact,
the fact that one out of a thousand people who beat their wives go on to murder their wives
actually is an indication that they're in the much smaller subcategory of people who may go on
to murder their wives. And the second part of this that makes it a poor argument is simply
that in this particular case, the wife was in fact murdered. So it wasn't the case that we were
talking about a hypothetical population of people, you know, who had not yet murdered their wives
and then we say one out of a thousand of them will, but instead we already had a murdered victim.
So that was a silly argument. However, it has the effect of complicating and confusing jurors.
And I think that this is one of the purposes of statistics used in some jury trials is how can you
confuse the jurors with statistics? There's one more example I wanted to bring up that came up
during the O.J. Simpson trial, but also came up. It comes up frequently and that has to do with DNA evidence.
You've all read that when at a murder scene or a crime scene, there is evidence that is gotten from the
blood samples at the scene. And if that, if for example, the perpetrator of the crime had been cut
and there was blood found there and they find they run it through tests and they get DNA description
of the blood type in great detail. And then at the trial, they will present this evidence and they'll say
only one in a million people have these characteristics in their DNA that we found at the crime scene.
And therefore, of course, that's strong evidence that the person who has the corresponding DNA must be guilty.
But we have to be a little careful. Suppose that this is what the police actually did. Suppose at the crime scene
they found the blood evidence and they typed the DNA. And then there was an archive of DNA evidence for all the
people in the city. And we don't have this yet, but suppose there were. And then they went down the list
and they found somebody in, for example, a city of 10 million people. Suppose they picked out somebody
whose blood type matched the DNA at the blood scene. And then at the trial, they said they put this person on trial.
Now, you're the jury. How do you evaluate the blood evidence against the defendant? Well, if the DNA type was the reason
the defendant was selected as a defendant, then you can't say, well, there's only a one in a million chance that the evidence matches.
In fact, you would say the following. You'd say, if the person were just chosen at random to match the blood type
and if there were 10 people, that's 10 out of 10 million, if it's one in a million chance, so if there were 10 people in the city
who had that same blood type and you just chose one at random, in fact, the chance that that person was the guilty party on that
evidence is only one out of 10. In any case, I think that these are fascinating issues of where the use of statistical
and probabilistic evidence in the criminal justice system can cut both ways and it requires some really sophisticated thinking.
I hope that you enjoyed this discussion of applications of statistics to the law.
