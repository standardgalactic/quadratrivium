Welcome back to Meaning From Data, Statistics Made Clear.
In fact, this lecture makes that title particularly pertinent because in this lecture we're going
to be discussing experimental design.
In previous lectures we've talked about statistical inference and discussed things such as taking
random samples from a population.
We discussed the idea of hypothesis testing, which involved making a hypothesis about the
null hypothesis with some hypothesized view of the world and then gathering experimental
data to decide how rare the experimental data would be under the hypothesis of the world,
and then rejecting that null hypothesis if we found that it would be a rare thing to
find the data we found given the null hypothesis.
And then in the last lecture we discussed the notion of confidence intervals, which were
a measure of how sure we were that the sample that we got from the population really was
representative of the whole population.
But often the way that we get these data are to do experiments.
We undertake experiments for the express purpose of obtaining data that is amenable to analysis.
So that's the challenge of experimental design, to make sure that we gather the data in such
a way that we are able to draw the meaning from the data, that we are able to use the
techniques of hypothesis testing and confidence intervals to make the mathematical kind of
deductions that make logical sense and that allow us to actually infer from the data the
ideas of interest.
There is a very famous statistician who lived at the beginning of the 20th century by the
name of Ronald Fisher, Ronald Eilmer Fisher.
And he was a pioneer in many parts of statistics, but in particular in experimental design.
One of the famous cases involving Fisher occurred at a tea party where he was at this tea party
and there was a lady at the tea party and they had tea which included drinking milk in
the tea.
So it was common to have milk and tea and this was common in those days.
And this lady at this tea party asserted that she could tell just by tasting whether the
milk was poured into the hot tea or whether the tea was poured into the milk.
And then both in both cases stirred.
Well, of course, what are you going to do under these circumstances if somebody makes
such an outrageous claim?
You have to design an experiment in order to gather evidence that will help us decide
whether or not the lady's assertion was correct.
And Ronald Fisher was at this party and this is, I believe, an actually accurate thing,
something that actually happened.
He was at this party and on the spot he devised an experiment which was intended to detect
whether or not the lady could in fact detect the difference of whether milk was poured
into the tea or tea was poured into the milk.
And here is the experiment that he devised.
What he did is he took 20 cups of tea, 10 pairs of cups of tea, and for each pair in
one of the cups he put the tea than the milk and in the other cup he put the milk than
the tea.
And then presented the lady with a pair of cups.
So he gave her a pair of cups and he said, one of these cups is milk than tea, the other
is tea than milk.
And the lady tasted the cups and then said, this one is the milk first cup, this one is
the tea first cup.
And then the next pair of cups were brought out and the lady did the same testing and
then the next pair of cups were brought out and of course they were randomly put which
side was the milk first one and which side was the tea milk one.
That was just randomly done so it wasn't as though every single time the left hand
one was the milk one first.
And then 10 pairs of these cups were given to the lady and then she had to, she made
her decisions each time.
Well this is in a way an excellent example of the design of an experiment where the results
of the experiment can be usefully used to make the determination that we're intending
to try to make.
So the reason it is a useful experiment is that we have a concept of the likely outcomes
of this experiment if in fact the lady were not able to detect the difference between
milk first or tea first.
You see if she were not able to detect the difference then it would be a 50-50 proposition
for each pair of cups whether she was right or wrong.
So we can look at this chart which gives us an analysis of our expectations of how many
times she should have been right or wrong if she were in fact just guessing.
Now you'll recognize this as a classic example of hypothesis testing.
The hypothesis would be that she was not able to detect the difference, that she could not
detect the difference at all.
By doing the experiment or before doing the experiment then we'd have a concept of the
distribution of likely results if a person who couldn't tell the difference made guesses.
If you made 10 guesses and you really only had a 50% chance of guessing correctly it would
be rather rare to get 9 or 10 of them correct or even 8, 9 or 10 of them correct.
More often you would get between 3 and 7 correct by just luck.
Well so the design of experiments idea here is that you've set up the experiment so that
when you get a result you can interpret it.
Now Fisher himself talks about this experiment in one of his books but in his book he doesn't
actually say whether the lady could tell the difference or not but it was reported by somebody
else who was at the party that in fact the lady actually was able to tell on all 10 occasions.
That the in fact which one came first, the milk or the tea.
So if that in fact is a result of the experiment you can see that from this graph we can see
that it is a very unlikely proposition for her to have gotten all 10 correct by random
misalone.
That would have happened but it would be a very rare occurrence for that to happen
and so the experiment by being designed in this way we could say it's so rare an occurrence
for that to have happened by accident that we will say that we reject the null hypothesis
that she can't tell and we accept the alternative hypothesis which is that in fact she can tell
the difference between milk first or tea first.
Well this is an example of experimental design and notice by the way we would have to decide
how to interpret a result of 8 correct or 9 correct or 7 correct that we see what's
going on in reality and then it's up to us to decide how much evidence we're going to
believe to be persuasive about her ability to detect the difference.
Well the context of an experiment and our beliefs about the underlying possible causes
or lack of causes can influence how we interpret the results of an experiment.
For example suppose we couldn't think of any possible physical or chemical reason that
there'd be a difference between the milk first and the tea first.
Suppose we thought and thought we couldn't think of any reason then we might be very
skeptical of this and we might think well maybe the lady somehow was able to see into
the kitchen where the tea cups were prepared or maybe somebody came in and told her what
was going on but if you think about it maybe there are some physical reasons I mean maybe
if you have a cold milk there and a hot thing hits it maybe that it scolds it in a certain
way that doesn't happen if the milk goes into the tea.
It's possible to think of some potentially detectable physical difference and so if we
can think of that then we accept the results of this experiment more readily.
So that is a question.
One of the purposes of the design of experiments is to try to avoid serious mistakes that you
don't want to make and we have to realize that statistics is all about coming to conclusions
that are not certain.
They're just conclusions that are likely to be true and for example in our last lecture
we talked about confidence intervals and we saw that we did an analysis that gave us
a 95% chance of capturing in that case the actual proportion of voters who are going
to vote for a given candidate in our interval but we were sometimes wrong.
So one of the issues that we have to deal with is the fact of potential error in statistics
and then design our experiments so that the errors that are bad are avoided.
Well there are actually two kinds of errors that are very common and famous for in hypothesis
testing which is the core idea such as the example we just had.
The two kinds of errors are imaginatively referred to as type 1 errors and type 2 errors.
Let's remember what kind of things can happen during a hypothesis test.
Remember the idea of the hypothesis test like the lady tasting tea is we make the null hypothesis
that in this case she couldn't tell the difference and then we declare that if the result would
happen only and then we choose a number such as 5% of the time that if the result would
happen only 5% of the time under the null hypothesis assumption then we will reject the null hypothesis
reject the null hypothesis and that's the conclusion of our statistical inference.
Well occasionally what can happen is that just by accident alone the experimental data
will be in that 5% it will lie outside this region.
In other words if our null hypothesis would view it as a distribution of what would happen
with a sample such as the lady just randomly choosing yes or no for each cup of tea it
would randomly happen that she would occasionally get it right many times like 9 or 10 times
but it would only happen say less than 5% of the time.
So if she got it correct just by accident alone and we rejected the null hypothesis that
she were just guessing then we would have committed a type 1 error.
Type 1 error is that we reject the null hypothesis even though the null hypothesis is true.
A type 2 error on the other hand is the opposite it happens if the reality of the world is
actually different from the null hypothesis but we fail to reject the null hypothesis.
You see because and this happens in this region suppose that we get data so here the null hypothesis
is this dotted line so these are the results that would come about if in the case of the
lady tasting tea the null hypothesis that she was just guessing.
Well suppose that she actually could get it right say 80% of the time so the null hypothesis
was false in fact she was good at it 80% of the time but sometimes she would miss and
suppose doing the experiments she happened to miss more times than usual so she the experimental
result came in this region well this is the region where we do not reject the null hypothesis
so we would be committing a type 2 error a type 2 error is an error where we fail to
reject the null hypothesis even though the null hypothesis is wrong.
Let's think about the jury system remember when a defendant is on trial the the judge
in such a situation is is saying to the jury this is an innocent defendant you must undertake
your deliberations with that in mind this is an innocent person.
Now you look at the evidence and if the evidence is unlikely to have occurred under the assumption
that the juror is innocent then you find the defendant guilty.
Framed in terms of type 1 and type 2 errors the the American jury system is strongly saying
try to avoid committing a type 1 error you see a type 1 error would be where the null
hypothesis is in fact true that is the person is innocent and yet the the jury condemns
the person finds them guilty then that would be committing a type 1 error so so the the
admonition to the jury that that judges should make and in fact I think we should advocate
this and and have this become part of American jurisprudence is that the judge should say
your null hypothesis is that the defendant is innocent and you want to avoid a type 1
error by making the p-value of your statistical significance less than 0.01 you see and then
that would be useful and then all jurors I'm sure would instantly know exactly what to
do so so that is a so type 1 error would be to convict when the juror is innocent the
other side of the coin type 2 error occurs if the the defendant is actually guilty you
see so the reality here this is the graph here the reality is that the defendant is guilty
but the evidence was not found to be sufficient to reject the null hypothesis that the defendant
was innocent so those are our type 1 and type 2 errors well um fisher him said now returning
to to ronald fisher fisher really began his career at an agricultural research station
in england and when he arrived he discovered that there were lots there was lots of information
lots of data that had been gathered over many years about crop yields rainfall fertilizer
usage but that the way the data had been collected made it very difficult for him or anyone else
to draw useful conclusions from the data because many of the variables involved were confounded
with each other that means that several different aspects of the growing conditions were changing
at the same time so you couldn't tell what feature of the difference was causing the
resulting change so for example you have things like the kind of fertilizer you were
using the kind of soil the amount of water these are all variables of in associated with
how tall the crops are likely to grow so if you want to confidently assign an effect to
a particular kind of fertilizer for example what you need to be able to do is to disentangle
the possible effects from the other variables well if you focus on one variable one basic
experimental design that that will will will help you to to get an answer about one variable
is to fix all the other variables in other words make everything else identical and just
have the one varying feature change for example here are two contrasting experiments involving
two kinds of fertilizer and you you will will see which one will give evidence that would
likely to be interpretable in one experiment suppose you take a particular kind of of of
plant of corn type a and you put it on a hill and you were a field that's on a hill and
you use fertilizer type one and then you take corn type B and you put it in the valley and
you use fertilizer type two well suppose one of the crops grows better than the other one
what what can we conclude well not much because we don't know whether the difference in the
fertilizer was the reason for the difference in growth it might have been the valley versus
the hill it might have been the type a versus type B corn so we don't know what what the result
really is telling us there might have been six significant reasons for the difference
but we don't know what caused them so a better experiment would be to have all of the variables
the same except for the fertilizer so that is to say if you could plant the things the
same kind of of of corn you put it in exactly the same kind of soil you have the same amount
of irrigation the same amount of sunlight but you used a different fertilizer in different
parts of it and it would be better if you put the fertilizer really very close to itself
like an alternate rose and better yet instead of alternate rows it would be better if you
randomly put the fertilizer in different rows in case there was something like the the soil
happened to lie in streaks where some of it was richer in alternate rows you see by doing
it random by making randomness a central part of the way you design the experiment you could
get better results more likely to get better results so if you fix all the factors except
for the one of interest that's an excellent way to do an experiment so let's look at at
an example though where you have more than one variable that you're interested in so here's
an example you have fertilizer one fertilizer two corn type one corn type two and you you want to
ascertain the effect of the fertilizer and the corn type a good way to accomplish a design of
experiments so that you can get meaningful data would be to try to have all combinations present
in your in your experiment in other words you don't just use fertilizer one with corn type one
but instead you use fertilizer one with both the corn types and you use fertilizer two with both
the corn types the effect of this you you can see this table here and these these are are are
potential data suppose that that the average height the mean height of the plants that had
fertilizer one and corn type one or 80 inches tall now by the way I don't know what is the
height of an elephant's eye but I think this should be as high as an elephant's eye and it may be
80 inches and then here with fertilizer one corn type two 60 inches tall you see and maybe maybe
the mean is 70 inches tall well if we have have this data from all of our experiments it's helpful
in making a conclusion for example we can just look at this one row and see that fertilizer one
for the different corn types this is telling a difference in the way the corn types are affecting
the growth and if we look down the column we see that 80 here and a 40 here would mean that fertilizer
two applied to the same same corn type gave a much smaller growth so this is suggestive
of the idea that fertilizer one was efficacious in its application to corn type one now by the way
it's not it's not 100 clear how we can interpret these data even as they are because there's no
indication in this chart about the standard deviation of the data if there was a lot of spread in how
how tall the plants were we may not be able to make robust conclusions about about this even
even from these data so looking at all different combinations and designing your experiments so
that all the variables are are combined allows us to disentangle the influence of the different
variables and analyze the experiments in many different ways there are other examples of
experimental design that bring up other issues in medical experiments real effects often come
when there's no physical cause at all and you've got to take these effects into account when
when you're trying to test new new medications that is we have to consider seriously the placebo
effect it turns out that that people are very susceptible to the suggestion but particularly
by a trusted doctor if a patient comes into a doctor and the doctor says here's a pill that's a
wonder drug for your ailment you'll feel better almost instantly well it often happens that
indeed the patient does feel better and and even if they're that the actual
pill itself has no active ingredient that could be the cause of that well so the placebo effect
is a fact but it makes testing of drugs more difficult because if we give some people a new
drug say for curing the cold and they get over their colds more quickly than people who didn't
take the drug we don't know whether it really was the drug that did it or whether there was some
sort of a placebo effect that that was accomplishing it so we've got to figure out an experimental
design that rules out this placebo effect so a basic design to to study the efficacy of drugs
then is to take a population of people who have the disease that you're interested in in testing
its its efficacy on and give half of the patients the drug and give the other half of the patients
a placebo and not tell the patients whether they're getting the placebo or getting the drug
so so the intent is that if there's an average difference in the outcome between the patients
who got the placebo and the patients who got the the drug then we would be able to attribute that
difference to an actual effect of the medication but it turns out in real life that there's an
actually another aspect that you have to take into account in medical testing and that is
that the experimental experimenters themselves people giving the drugs are often not unbiased
observers you know they are hopeful that a particular medication will work and if there is
something about the experiments that involve judgment for example if you ask somebody how
are you feeling today or you you're saying you know how much pain are you in do you've you've
hurt if you have an experimenter who has a vested interest that person may hear the result quite
differently from from somebody else so the interpretation of whether the patient has improved
is sometimes a judgment call on the basis of the experimenter this is another characteristic that
needs to be taken into account when when you're doing the experimenters when you're doing experiments
about medical tests so it has been found that that it makes a difference whether the experimenter knows
that this particular patient is getting the placebo or this particular patient is getting the drug
so there are the the kind of experiments that are done are called double blind experiments
these are experiments where neither the patient nor the people who are directly in contact with
the patients or administrating the drugs or assessing the progress of the medication
that none of those people knows whether the the individual patient had received the real drug
or just a placebo so that that is the strategy and that is the gold standard for medical tests
double blind experiments of course it's also important that the other characteristics of
good experimental design such as choosing patients randomly are also also used and by the way that
this placebo effect works not only in medical arenas but the Hawthorne effect is a famous effect
that was named after a western electric plant in Cicero Illinois where experiments were conducted
between 1927 and 1932 and the goal of the experiments originally was to determine the effects of things
like lighting and humidity and these kinds of variables on the worker productivity but what
happened is that they discovered that no matter what change in the lighting and humidity were
done it had a positive significant positive effect on the workers productivity the the idea was that
just being looked at turned out to be have a very positive effect on on people's work
one of the issues that are very important to avoid in experimental design is the concept of
the lurking variable the lurking variable refers to the idea that there may be something
that's causing a change in the thing that you're trying to experiment with but that you haven't
actually taken into account in your experimental design and there's a great example of this that
comes from Ronald Fisher's work at in the agricultural station one of the fields was called
broad balk field in which the wheat output in the measured wheat output had an interesting
occurrence it turned out that in 1876 the output of the wheat in that field got quite a bit worse
and then got even worse in 1880 and then approved improved again in 1894 and got worse in 1901
for no reasons that were associated with the experiments that were being done on the field
it turned out there was a lurking variable in 1876 the education act was passed in England
which made school attendance compulsory it turned out that this that in before that time
little children were hired to weed the field and so they would pull the weeds from the field and
that would help the wheat grow so when this education act was passed they were not out there
available to weed the gardens and the the field and so the the wheat production went down and in
1880 they actually made penalties for people who kept their kids out of school so the production
went even further down because fewer kids were there but then in 1894 a headmaster of a local
girls school thought that good physical activity was important for girls and so we had the girls
going out pulling weeds as part of their physical activity and this increased the wheat production
in that field until 1901 when he died and the next headmaster didn't have that same philosophy
the girls quit pulling weeds and so the wheat production went down in the field this is an
example of a lurking variable other examples of lurking variables suppose that you want to know
does are married people paid more than unmarried people in the workforce well answer yes in fact
people who are married for 10 years are paid more than than people who are not married why well
because they're older people who are married tend to be older than people who are unmarried
on average and consequently you would expect them to be paid more because of their age the age
is the lurking variable designing experiments in order for us to get good results is a very
tricky business and it allows us to think ahead to the kinds of statistical strategies of inference
that we're going to use this concludes part one of the course and in part two of the course we're
going to take different application areas and see examples of statistics in action I look forward
to seeing you then
you
