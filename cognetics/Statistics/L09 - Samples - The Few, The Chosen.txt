Welcome back to Meaning From Data, Statistics Made Clear.
You recall that the structure of our course is to view statistical analysis as having
basically two parts, and the two parts are how can we describe, summarize, and organize
a collection of data if we know all of the data in the population of interest.
That was the first part.
And then the second part is how can we infer information about the whole population when
all we know is information about part of the population of the data.
So in this lecture, we're going to come to grips with the question of what only part
of the data really means.
So this lecture is about the concept of a sample.
So let me first of all define some terms that I've actually used already, because I think
they're quite familiar, but let me actually define them a bit.
The term population doesn't, first of all, refer necessarily just to people, but in the
world of statistics, the population refers to the total collection of people or things
that we're considering.
For example, if we're talking about students at a university, then the whole population
would consist of all of the students at the university.
But maybe we're thinking about manufacturing auto parts, and then the population might
be the set of all auto parts that were produced during a year.
So the word population when used in statistics refers to just the entirety of the collection
of people or things which is of interest.
A sample is a subset of the total population, and whether that population is people or auto
parts or anything else that we're talking about.
So a sample is just any sub part of the whole population.
Now in general, we want to infer information about the whole population from information
about the sample.
In other words, it's not our interest to actually know things about just the people whom we
ask in the sample.
It's not the sample that we're interested in.
What we're really interested in is whether it's opinions or the quality of parts, we're
really interested in those aspects of the entire population.
But the information that we have comes from just finding the information about this sample.
So there are different kinds of techniques that we might consider about how to draw inferences
about the whole population from a sample.
Let me just start with an example.
Those that were the head of a company and were considering producing one of two different
alternative products, well what we'd like to know is how the entire population of potential
buyers would respond to the two products.
Would they prefer this one or this one?
We couldn't ask those millions and millions of people to look at the two products and
make a decision because first of all we couldn't afford it and we couldn't get to all those
people.
So instead we have to be content with asking just a few people, a sample.
Well there are several ways that we might think of doing this.
One way is to, we might be very careful about whom to ask.
We might carefully pick people who have a certain kind of characteristics that we think
might be pertinent to their decision.
For example we might pick a certain number of very conservative people and a certain
number of politically liberal people or we might consider taking a certain number of
people from a certain ethnic group.
And in fact those kinds of techniques do play a role in some kinds of sampling and I'll
talk about that later.
But the central idea of good sampling instead of involving intent is randomness.
This is the key to good sampling and so if you're looking, for example if you read about
a survey or an experiment in a newspaper, if they are describing the experiment and
they don't talk about randomness in the way the sample was chosen then that's sort of
a red flag that maybe this data that you're reading may not be appropriate.
If we choose the sample randomly the advantage is that we can measure the likelihood using
probability as we discussed in the last lecture.
We can use our understanding of probability to make inferences about how well the opinions
of the sample do in fact represent the opinions of the whole population.
On the other hand if we intentionally choose certain groups to reflect what we believe
to be reflective of reality it may be that we bring our own biases to the selection process
and that those biases are then going to be reflected in the people whom we ask.
So in that case the sample may not be representative of the whole population.
And by the way, representative of the whole population, I hear myself using that phrase,
all that means is that the sample should have the same characteristics that the whole population
does.
In the case of these two potential products we would want the people in the sample to
prefer the one product over the other product in the same proportion as the whole population
would think.
So this is one of those sort of happy moments where paradoxically we're better off not thinking,
avoiding thinking.
It's not a great idea.
You avoid thinking instead when you're trying to think of how to select a sample.
Instead you intentionally go out of your way to choose your sample randomly.
So the whole concept of choosing the sample randomly is that we have a better chance that
the proportion of people in the sample with a certain opinion will be in fact the same
as the entire population.
The most common occasion where this comes up that's most familiar, and we've talked
about it before, is before an election.
We are interested in what proportion of the voters will vote for the Democratic candidate
and what proportion will vote for the Republican candidate.
That's a typical situation before say a presidential election.
Now of course the whole point of sampling is that we actually don't know before we take
the poll, before we take the sample, we don't know what the whole population is like.
So taking appropriate samples is the way that we get information that we don't know about
the whole population.
Well there are several major pitfalls in the way sampling can be done and so I'd like to
discuss a couple of them now and they are, I think they're really quite interesting.
So here's the first example I'm going to give.
In the 1936 election you may recall the two primary contenders for the presidency were
the incumbent Franklin Delano Roosevelt and the opponent, the Republican opponent, was
Alfred Landon.
At the time there was a magazine called the Literary Digest and the Literary Digest for
several elections had conducted polls, major polls, to make an estimate of who would win
the upcoming election and they had successfully predicted the elections in several of the
previous elections.
This was a major poll so in the 1936 election the Literary Digest sent out 10 million surveys
and they received 2.4 million replies to those surveys asking the question of how the voters
were going to vote in the upcoming election.
And based on those surveys the Literary Digest predicted that Landon would win in a landslide.
In fact their prediction was that Landon would win 370 electoral votes to Roosevelt's 161.
Well you may not recall reading about President Landon in your American history books and
the reason you don't recall reading about Landon is that in fact he obviously did not
win the presidency.
And in fact the only correct aspect of the Literary Digest's prediction was that in fact
the election was a landslide but unfortunately for them the landslide was the other way.
Roosevelt won the election by an incredible 523 electoral votes to 8 for Landon.
Roosevelt won 62% of the popular vote in this election.
Obviously the Literary Digest sampling method was not representative, was not getting a
sample that was representative of the whole population.
What went wrong?
Well let's think about it.
What could have gone wrong with this huge sample of voters?
Why was it wrong?
One thing is that the Literary Digest got their samples, the names whom they sent the
surveys to from several different kinds of lists.
And one list was the subscribers to their own magazine, the Literary Digest.
That's a good list of people that you had.
So they sent out their surveys to obviously everybody who subscribed to the Literary Digest.
They also looked at car registration records and that was an available list of a lot of
names and they sent their surveys to those people and also used telephones.
1936 was in the middle of the Great Depression and many people were having financial problems
and were cutting back on their budgets.
And probably one of the first things to go in tight times would be one subscription to
the Literary Digest.
And in addition, not many people owned cars or telephones.
These were luxury items for a lot of people in 1936.
So because of this, the list of people to whom the Literary Digest had sent their survey
was strongly biased toward wealthy people.
And so those wealthy people had opinions that were not representative of the population
at large, obviously.
So but this is actually a story that has two additional sides to it.
One is, of course, that the Literary Digest, which otherwise would simply be lost in the
dustbin of history, will now live on forever in statistics textbooks because everybody
is aware of this example, it's a great example of bias in sampling.
But another success story that came from this Literary Digest fiasco was in the form of George
Gallup.
At the time, George Gallup was a young statistician just starting out.
And what he did is to do a poll of his own for the 1936 election.
He had a survey of only 50,000 people, actually 50,000 people, by the way, is an enormous
number of people, as we will see in the next couple of lectures.
It's a huge number of people.
But he took a survey of 50,000 people and made his own predictions for the election.
Well he predicted two things.
One is he correctly predicted that Roosevelt would win the election.
But in addition to that, he also predicted that the Literary Digest poll would be wrong
and estimated how wrong they would be before their poll came out.
So he was one of the people who introduced the concept of randomness in political polling
as a key feature of sampling techniques.
And in fact, that is absolutely one of the fundamental criteria to look for when you're
evaluating whether a sample survey is, in fact, a good one.
Mathematics is a basic ingredient of essentially all of the standard statistical techniques.
And the reason it's an ingredient is because it's the analysis of randomness and probability
that allow us to apply mathematics to the understanding of the results that we get.
Well, the Literary Digest poll had another pitfall.
The second pitfall to their survey techniques was that it was a voluntary response survey.
They sent out all of these surveys and only some people sent back their replies.
Voluntary response.
The problem with that is that sometimes the people who send back replies have a particular
bias, that is to say, instead of people across the board sending back replies in the same
proportion, maybe just some people with a certain opinion are much more apt to send
back a reply.
And there's a wonderful illustration of this pitfall which occurred in the columns of Ann
Landers.
And so I wanted to tell you and read you a letter that Ann Landers received in her column
and the response that came from it.
So the letter to Ann Landers was a simple letter and it came from a young couple who
was about to be married and was writing for guidance.
So they were undecided about whether or not to have a family.
That was their question.
So they wrote the following letter.
They said, so many of our friends seem to resent their children.
They envious our freedom to go and come as we please, then there's the matter of money.
They say their kids keep them broke.
One couple we know had their second child in January.
Last week she had her tubes tied and he had a vasectomy just to make sure.
All this makes me wonder, Ann Landers, is parenthood worth the trouble?
Him and I are very much in love.
Our relationship is beautiful.
We don't want anything to spoil it.
All around us, we see couples who were so much happier before they were tied down with
a family.
Will you please ask your readers the question?
If you had it to do over again, would you have children?
So this letter appeared in Ann Landers' column.
And as she phrases it herself, she says, I printed that letter and the sky fell in.
The word didn't come from chicken little.
It came straight from the gut of young parents and old parents, from Anchorage to San Antonio.
I heard from junior liggers and welfare mothers.
I had struck an unprecedented number of raw nerves.
The question unleashed an incredible torrent of confessions.
After five days of reading, counting and sorting mail, we had received over 10,000 responses.
And are you ready for this?
70% of those who wrote said no.
If I had it to do over again, I would not have children.
70% of people think they would not have children again.
It turns out that very soon after that, so there was no change in the world, people did
an actually accurate survey, taking random response, using the concept of random surveys,
making certain that they weren't asking the question in a biased way.
And the effect of that survey was that they found that 91% of people who had children
said they would have children again, which is a little bit heartening, I would say.
So the point of this example is that the bias that can come from voluntary responses may
not just give an answer that's a little off, but it can give a completely, completely erroneous
view of reality.
So that's something to keep in mind, that these technique issues are not just a question
of gradient, of being off by a couple of percent.
In this case it was a question of being 70% saying they wouldn't have children to the
more realistic estimate that 91% said that they would have children again.
So one of the basic, the most basic way to get an accurate sample is to take a sample
that's called a simple random sample, SRS, simple random sample, which is as the name
implies, simply to take the entire population with which you're interested and say how
many people you want to survey and randomly select them from that group and then get the
answer from each of those member of that selected sample.
Of course there are lots of problems in getting the answer from that selected sample, but the
simple random sample is the gold standard for finding a representative sample.
Well, there are other kinds of pitfalls in surveys.
Another kind of pitfall that occurs in surveys is the idea that people may not tell the truth,
particularly if you're asking a survey about something that is an embarrassing question
or maybe something that people don't want to confess to.
But it turns out that there are methods for getting a good sense of the total view of
the population even on embarrassing questions and I want to illustrate that technique now.
Suppose that you want to ask a bunch of students how many of them cheat.
You're interested in this problem of cheating at a university, say, and so you want to find
out how many people actually do cheat.
And of course if you walk into a room and you have a big auditorium and you walk in
and you say, well, would all of those of you who cheat please raise your hands?
Well you're not going to get an accurate answer, obviously.
So instead, but it turns out that you can get an accurate assessment of what percentage
of people cheat by just having people raise their hands with the following technique.
So this is a very clever way to get an answer of the whole population without any individual
having to confess that he or she cheated.
So here's the way it works.
You say, suppose that we're now in front of a big auditorium and there are a thousand
people, thousand students in this auditorium.
So here's the way the procedure works.
You say, okay, everybody take out a coin and take the coin and flip the coin and just look
at it and don't let anybody else see whether it's heads or tails, okay?
Follow me?
So you flip the coin, everybody looks but doesn't tell anybody else and they put the
coin away.
This person has in their head, I did a heads or a tails.
And then you ask people to raise their hands in the following way, that they raise their
hands if they either cheated or tossed the heads.
So everybody with the heads who flipped their heads in that secret coin toss will raise
their hands.
But they will also raise their hands if they cheated, even if they threw a tails.
So the point is that if you look at any individual whose hand is up, that person may have simply
tossed a head and never cheated in their lives.
Those people may not have cheated in their lives, or it could have been somebody who
threw a tails and cheated and therefore has his or her hand up.
So the idea is that this way you're getting an ability for people to raise their hands
without confessing to cheating.
Follow me?
Let's actually do an example to see what you could deduce from this experiment.
So here let's suppose that we have a thousand people in the room and we ask everybody to
flip a coin and what we see in the raised hands were that 800 people raised their hands
and 200 people did not raise their hands.
Suppose that were the data that we collected from this thousand people.
What can we deduce from this data, from these data?
Well let's make an estimate.
Just for now let's assume that 500 of the people in the audience flipped a heads and
500 flipped tails.
So all of the 500 people who raised their hand, see they threw a heads, would raise
their hand of course.
And among people who, these 200 people who did not raise their hands, well first of all
they had to throw a tail and in order to not raise their hand, if you threw a head you
definitely raised your hand.
So 300 people raised their hand and threw a tail, you see, because if 800 people raised
their hand and 500 people threw a head then that means 300 people who raised their hand
had thrown a tail and also raised their hand.
Is this clear?
So that means that 300 out of the 500 people who threw a tail raised their hand.
You see, 300 people who threw a tail, which we're assuming was 500 people threw tails,
300 of those people raised their hand anyway.
So that means that 60% of the people raised their hands who had thrown a tails.
But throwing a tails is just a random event.
So we deduce from that that an estimate for the number of, the proportion of people who
cheat would be 60%.
So you can deduce from, even though you know that you don't know for sure for any individual
whether or not that person cheated, you deduce that approximately 60% of the people actually
cheated.
Is that clear?
Okay.
Now, by the way, if we want to remember in making deductions, one of the things that
we want to try to do is to see how accurate we are.
And we can actually look and see what the range of likely throwing of heads and tails
would be in order to decide how accurate the actual outcome is.
In particular, we can look at when 1,000 people throw a coin, what's the likely distribution
of heads to tails.
And we get this kind of familiar looking histogram that tells us that the number of people who
threw a heads is very likely in this range.
If we expand the picture, it's very likely between 0, 460 or 70 to 540, that's the likely
range.
Then using that fact, we can say, looking at the extremes, we can say, well, suppose
only 460 people threw a heads and that we can go through the analysis and see that under
those circumstances, going through the same analysis, we see that 63% of the people would
have cheated.
That would be our deduction.
And then we can do the same analysis on the other side.
Suppose at 540 people threw a heads, we can go through the same analysis and deduce what
percentage cheated, 57%.
And so that we have a range of between 570 and 630 cheaters in the room would be a good
estimate for the likely number of cheaters there are.
Now I wanted to say just one anecdote about this, which was that I actually did this experiment
as an illustration in a class I was teaching and the class had 50 people in it.
And I explained it all in detail and I said, raise your hands if you cheated.
And I was expecting about 25 to flip heads and then maybe a few more than that would
have cheated and so on.
Well, it turned out that everybody in their hand raised their hand except for two people
in the whole room, 48 people raised their hand.
From this I would deduce that 92% of the people cheated.
And these were wonderful students and I was thinking, well, now do I need to lock up the
silver when I invite them over to my house the next time?
But these were great students and I said, how can this be?
I told them the reasoning and the deduction.
How can this possibly be that you're all a bunch of cheaters?
And they said, well, the way you phrase the question, you said, have you ever cheated?
And they said, well, I was thinking back to the third grade when I, you know, somebody
hit my mother helped me with my homework or something and I thought that was cheating.
So this actually brings up a very important point about sampling and that is the way the
question is phrased can make a huge difference on the answers that you get.
So as a matter of fact, it really turned out to illustrate one of the basic principles
of good sampling and that is that you have to ask questions in such a way that the question
doesn't cause a person to answer in one way or another.
Bias questions are one of the veins of existence of surveys.
So in this lecture, we have been introduced to the concept of sampling and the basic principle
of sampling, which is that we want our sample to accurately represent the whole population.
And we've seen a couple of different pitfalls that are possible.
One, a biased sample where the people who were asked do not in fact represent the opinions
of the whole population or the idea that you have a question that may be asked in the
wrong way or that you have a voluntary response survey which may give a very distorted view
of reality.
The basic principle of sampling is that it must involve randomness as the fundamental
ingredient.
In the next lectures, we're going to see examples of how random samples allow us to
make good deductions about the reality of a whole population.
I'll see you then.
Thank you.
