Welcome back to Meaning From Data, Statistics Made Clear.
In this lecture, we'll discuss two statistical issues that arise from the social sciences.
One of them is a statistical technique that's called factor analysis, and it's a technique
by which a statistical model of a complex situation leads to underlying conceptual organization
of the phenomenon that we're discussing.
The second item is that some social scientists feel that there's an over-reliance on a rather
formulaic application of hypothesis testing to their field.
So in the second part of the lecture, we'll discuss the Bayesian view of statistical reasoning,
which in some sense presents an alternative take on hypothesis testing, and a different
view of how we can look at the world and how statistical evidence changes our mind about
what the world seems like.
So we'll begin with factor analysis.
So factor analysis is a statistical technique that tries to find whether data comprising
a number of variables can be summarized or explained by a small number of factors.
And additionally, we can get insight about the field by interpreting these factors which
come about in a mathematical way that we can interpret them as explanatory concepts.
And we'll see examples of that to see what we mean.
So factor analysis is a way of creating concepts, and it creates concepts that sometimes can
be helpful in the description or analysis of phenomena in the social sciences.
It was Charles Spearman, who's credited with inventing the technique of factor analysis
about 100 years ago.
He was studying intelligence, and he hypothesized that there was one underlying factor of general
intelligence, he called it the G factor, that would underline the results of all sorts of
different measures of mathematical and verbal skills.
The term factor analysis was actually introduced by Thurstone in 1931.
The assumption that underlies factor analysis is that there's a small group of latent factors
that account for the correlation among a much larger group of observed variables.
And I know this is very abstract now, and we'll bring it down to some examples in a
minute.
But let's think of an example.
Suppose you have a questionnaire, and you're given a questionnaire, maybe it asks 50 questions
about emotion, and each answer has some numerical value.
So a question might be, how much fear are you feeling, or how much control are you feeling?
That kind of question.
Well each person who's surveyed is then, after the results are getting gotten, each person
it has 50 numbers that represent the results of that individual.
So there are 50 observed variables, and if you give this instrument two thousands or
more people, then you have these 50 variables from each person.
So the results for any two of the questions may be correlated to each other.
That is in the population.
You may find that most of the people who answer question number three, with a strong positive
answer, will also answer question number 17, with a strong positive answer.
So for example, it may be that having more fear may somewhat correlate with having less
control.
They're correlated to each other.
So factor analysis is a way of looking for these underlying factors, and measuring how
much of the variation of the observations, that is the survey results from all the people
in the population, how much of the variation is explained by focusing on just these factors.
Now a factor is not just say one question or another question, although it could be that,
but a factor generally is a linear combination of the answers to a collection of the observed
variables.
So what I mean by that is that you might have, for example, a factor B, the answer to the
fear question minus two times the answer to the control question.
And that combination would be a factor.
Generally speaking, if we're talking about 50 things, it would be a factor would include
many, many combination involving many of the answers to many of the questions, maybe 20,
30 of the questions, or maybe all 50 of the questions.
So a person's value for each factor is computable from the values that that person answered
from the original variables.
One of the features that you want from a factor analysis is that the different factors should
be uncorrelated with each other.
In other words, if you isolate a factor that is capturing some aspect of that personality,
you want the other factors to vary independently over the population.
So in particular, we don't want any correlation between the answers to the one factor, the
computed factor of one of the factors and the computation of another factor.
So they're like the X and Y axis in a graph.
If you change the X value, you don't necessarily change the Y value.
The goal is to find independent characteristics of a person that are being used to describe
the phenomenon.
So a successful factor analysis would yield a small number of factors that explain a lot
of the variation we see in the population about whatever it is that we're discussing,
and yet there are just a few factors that explain all of it.
And so if you know the values for the factors, you can get a good estimate of the values
of the original variables.
So that's another characteristic you want.
So the goal of factor analysis is to summarize complex data sort of in a sense analogous to
how linear regression was a way of summarizing data.
So okay, let's ground our discussion with an example that will make it maybe more reasonable.
So we'll begin by looking at the famous Myers-Briggs personality type indicator.
And most of you've probably heard of the Myers-Briggs test.
There are over 2 million people a year take this test, and it was created originally to
capture Carl Jung's personality type concept.
He had a concept of personality types, and the designers of the Myers-Briggs test were
trying to capture Jung's concept into this practical form.
Now it's a little bit ironic, by the way, to use something that came from Jung in this
lecture on statistics.
He has a quote about statistics where he said, you can prove anything with statistics.
So he was not necessarily a great fan of statistics as being the right way to come to insight into
human behavior.
But nevertheless, the Myers-Briggs test is an instrument that tries to categorize personalities.
And here's the way the test works.
After you've answered about a hundred multiple choice questions, the Myers-Briggs test takes
those answers and then presents you with a summary of your personality or preferences
along four scales.
And these scales are a scale of extroversion-introversion, the scale of sensing and intuition, a scale
called thinking-feeling, and a scale called judging-perceiving.
And the answer to the output of taking the test is that you get a score for each of these
scales, how extroverted or introverted you are, where you are on this line, thinking
versus feeling, how you tend to, whether think carefully about things or rely on your feeling
about them.
These are four independent axes.
So the concept of this instrument is that it summarizes a very complex thing.
I mean, the ultimate goal is to summarize your personality type by using just these
four scales.
But in particular, the actual information that it's taking are the answers to these
hundred or more questions that are then used to compute your answers on each of these scales.
So the point is that these are factors that summarize the much more complicated situation
of the answers to all these questions.
For example, we have a hundred questions, and maybe the questions that are associated
with the extroversion-introversion factor would be a combination of the answer to question
one plus three times the answer to question two minus four times the question five and
so on.
Those are a specific combination of those questions, of the whole hundred questions that
you ask, that are combined to put you on that one scale.
A good factor analysis has the property that the scale, that the factors are independent
of each other, meaning that if we look at all of the people who take this test, and
as I say, two million a year have taken this test, so we have lots of data about people
who take the test.
In order to have a good factor analysis result, what you would want to have is that there's
independence about the extroversion-introversion result compared to the sensing intuition result.
In other words, if we plotted a scatter plot of people in the population, and for each
one we put down their score on the introversion-extroversion scale, and then we put down their score on the thinking
feeling scale, so you see every person would have a dot on this scatter plot of these,
I've just picked two of the four factors, and we would find that these results are uncorrelated.
That's one of the features of factor analysis that you would want the results to be uncorrelated
because you want each of the factors to be giving you an independent concept about what
factor it is that you're measuring.
The Myers-Briggs test was created, as I said, to capture Carl Jung's concept of personality
types.
So the Myers-Briggs test, the factors, were not a result of a mathematical analysis, but
they have been confirmed by a mathematical analysis.
That is to say they are basically independent and there isn't correlation among the factors,
so they do capture what it is they're trying to capture.
But the factor analysis in the case of the Myers-Briggs scale was an after-the-fact concept.
On the other hand, sometimes if you can use factor analysis as the way to gain insight
about the field under study by first looking at the original variables and combining them
into factors that are obtained from a mathematical process, and seeing whether the suite of questions
seems to suggest that that quality that came from this mathematical analysis really is
something of significance in the world.
For example, or to give you a little more insight about what the mathematics is involved.
If you take all of the factors, all of the variables that you originally measure, in
other words, you're given these 100 questions in the Myers-Briggs test, so you actually
have a hundred different collection of values, and you have a whole population of people
who take this test.
So you can ask the question for every two questions on the Myers-Briggs.
You could ask, to what extent are the answers correlated among the population or not correlated?
And so you can make a matrix of a correlation matrix where you have every variable on one
side and every variable on the other side, and for every point in that rectangular array,
you have a correlation measured, as we've seen before, using the correlation coefficient.
You get a number.
That is a matrix that operates on a many-dimensional space, and it's a mathematical side of doing
this factor analysis is a question of doing some linear algebra that's associated with
trying to diagonalize a matrix and doing some mathematical themes whose effect is to produce
this linear combination of the various collections of variables so that you have one collection
that is not correlated to the collection that's representing the next scale that is created
by the factor analysis.
Well, okay, so let's give an example where the factor analysis itself created an idea.
So finding the combinations that give uncorrelated combinations is a mathematical procedure.
The mathematical result of factor analysis then sometimes gives insight about the field
under study by looking at the original variables that are being combined into the factors and
seeing whether that suite of questions that came out from the math seems to suggest a
quality that should be identified and explored.
So a researcher then would call each factor by some evocative summary name depending on
the ingredient variables, trying to say, well, what is that combination capturing?
For example, in a study on jealousy, many measures of qualities about jealousy were measured,
and people were given survey questions from several surveys, such as an interpersonal jealousy
scale and a romantic relationship scale, and from these, 20 variables were extracted
from all these surveys.
Factor analysis was done on the result, and three factors were identified.
Well, these factors then are combinations of all of those things that were measured,
but the researcher looks at one of those combinations and says that collection of qualities that
are measured by these different questions seems to be capturing something, I'll give
it a name.
So in this case, the researcher called one of the factors reactive jealousy, another
one anxious suspicion, and another one interpersonal insecurity.
And the correlations between each of the three factors and the 20 original variables divided
the 20 into these three groups.
That was the point of the factor analysis, but the variables in the first group are highly
correlated to the first factor, but have low correlation with the other factors.
So in other words, you could have a person who was high on one of these three scales
and low on the other ones or high on both, they move independently from each other.
So the names of the three factors then reflect the researcher's interpretation of the meaning
of the underlying factors.
So part of the intent that something has been learned about jealousy is that the three
principle ingredients that underlie the 20 variables somehow explain the concept of jealousy.
And so it's identified a psychological feature of people.
And of course, care has to be taken in this interpretation, but there is insight that
is to be gained about the psychological or social issue that's being studied by finding,
by seeing what's reflected by the factors that came up from this mathematical analysis.
So one of the interesting features of factor analysis is that the statistical and mathematical
technique is actually valuable in identifying and isolating new ideas.
So in a sense, the mathematics points the researcher in a certain direction, and then
the researcher is challenged to see whether that combination of qualities somehow is a
core idea in the subject.
Well, let's move on to the second topic for this lecture.
Einstein once said that mathematics was merely a refinement of common sense.
And in fact, that's right, mathematics and statistics, the strategies of mathematical
and statistical strategies, all of these ultimately arise from just looking at the world and taking
our common way of interpreting and organizing it and then refining those methods into some
sort of a system.
And hypothesis testing that we've talked about is an example of a strategy for testing
features of our world.
But actually, in our everyday experience, it suggests that hypothesis testing may not
capture all of the common methods by which we evaluate and judge our world.
And some social scientists and many others also feel that there's traditionally been
too heavy a reliance on hypothesis testing as the way of interpreting evidence.
So there's another model that is progressing toward a clearer understanding of the world
that may actually be closer to the way that we actually live our lives.
And let me give an example.
Suppose we meet somebody and we don't know anything about them at all.
Then we're open-minded about what it is that they're like.
We don't know whether, where on the scale of honesty they are, how kind they are.
We have a sense that there's a range of what they may be like, but we don't know where
they fit on that range.
But what happens?
Well, as we get to know them, we see them in action.
We see them do something.
We talk to them.
We hear them act.
And then what happens?
We update our view of what they're probably like.
So this idea that we start off with some opinion or maybe pretty bland opinion, but then we
get some data.
We see this person being particularly kind.
And so we update our probability and we say that person probably is kind.
And we see the person doing something else kind.
Then we say it's more likely that person's kind.
And then we see the person do something awful and then we change our minds again.
It's a continuous process of updating our view of what might be.
Well, perhaps the best way to understand the distinction between this sort of updating
model and the standard hypothesis testing is to look at the following example.
So I'm going to give you an example that involves three instances in which we do hypothesis
testing.
And we're going to do three experiments, talk about three experiments.
And statistically speaking, these three experiments are identical to each other.
In other words, from the point of view of a hypothesis test, they're identical.
But my question to you will be, what is our reaction to these three experiments?
Okay.
So here's experiment number one.
There's a person who you are told is an expert, a musicologist.
And you do the following experiment.
You say this person's an expert musicologist, so you take two pieces of music, one written
by Haydn and one written by Mozart.
And you hand these pieces of music to the supposed expert without saying who composed
which one.
And then the person says which one was composed by Mozart.
And then this process is done again.
You take another pair of pages of music, one composed by Mozart, one composed by Haydn.
You switch them around.
You give them to the musicologist.
Once again, that person makes the deduction.
And you do this 10 times.
All 10 times, the person gets it right.
Okay.
Now, you feel that, you know, your sense about that is that the, first of all, you're not
surprised.
And you're pretty confident that this person does, in fact, is, in fact, able to distinguish
Mozart from Haydn.
Okay.
That's a pretty persuasive experiment.
Okay.
Let's look at another experiment that we actually talked about before, which was the Lady Tasting
Tea Experiment.
In the Lady Tasting Tea Experiment, the lady was given 10 pairs of cups of tea, one where
the milk had been poured into the tea and one where the tea had been poured into the milk.
Milk than tea.
And in each time, she was given a pair of these cups and asked to tell which one was
which.
And she, in fact, did it all 10 times.
She was successful all 10 times.
Okay.
Now, now we're going to say how we're going to react to this experiment.
So I'll give you one more and then we'll talk about all three.
The third example is that has exactly the same structure to it.
But suppose that a drunk person comes into a bar and says, I can predict the way a coin
is going to land.
You're not a drunk person.
And he say, okay, we'll test it out.
We'll do this.
You tell me, I'm going to flip the coin in the air and you tell me how it lands.
And so the drunk person, he flips the coin and he said, heads, he said, you know, before
it lands, he says, heads.
And it was heads.
And then you do this 10 times.
Stales, he gets it right.
He got it right 10 times in a row.
Well, how do we respond to these three scenarios?
In the case of the Mozart Haydn, we're very strongly convinced that that person does indeed
is indeed able to distinguish Haydn from Mozart.
In the case of the Lady Tasting T, we may retain a little skepticism, but we're pretty persuaded.
But still we may have some doubts because we can't really think of what she could possibly
be detecting.
But in the case of the drunk, it's pretty clear that we still have a lot of skepticism.
We know it's going to happen one in a thousand times, roughly, that a person will just by
random luck get it right.
But we're not really convinced by this experiment that, in fact, this person can predict the
future.
We're still skeptical.
The point of these experiments is that all three of them had the same hypothesis testing
structure.
The null hypothesis, in the first case, was that the person could not distinguish Mozart
from Haydn.
And then the result was significant, had a low p-value.
It would happen only one out of a thousand times by luck alone that that person would
be able to detect the difference 10 times in a row.
Likewise, with the Lady Tasting T, the null hypothesis was that she could not tell the
difference.
And yet she could, and therefore we were persuaded.
The null hypothesis for the drunk guy, same thing.
He couldn't tell the difference, but he got 10 of them right.
And yet our response to these is different, and legitimately so.
It seems different.
Well the Bayesian point of view is to acknowledge this reality, that this kind of evidence really
updates a prior assessment of what we view the world to be.
So let me give you an example to illustrate the concept of Bayesian, the Bayesian point
of view of the world.
Suppose you go into a magic shop, and from the magic shop you select a penny, well a
coin, it's not a real penny, from the magic shop.
And you know it's a magic shop, and you know that the magic shop sells coins that have
every different probability of landing heads up.
In other words, some of the coins always land heads up, some of the coins land heads
up only, never land heads up, always tails up.
And for every place in between, it's a magic shop.
So because we know where this coin came from, we're absolutely agnostic about our predisposition
of thinking what the probability of landing heads up that this coin actually has.
In other words, our original concept of the probability that the coin will land heads
up, by the way, we're assuming that the coin does have some probability associated with
it.
If you flipped it thousands and thousands of times, some fraction of the time it would
land heads up.
And that's what we call the probability associated with this particular coin.
But when we take this coin out of the magic shop, we don't know.
So the picture that we have then is that our a priori concept of the world is that
we don't know what the probability of its landing heads will be.
It could be anything from zero to one.
How are we going to change our view of what the probability is?
Well we do some experiments.
We simply flip the coin sometimes and take data.
So suppose that we flip the coin four times, and three times the coin lands heads, and
one time it lands tails.
Then from the Bayesian point of view, we get a new graph about our belief in the probability
that the coin is of what the probability of landing heads is for this particular coin.
One thing for sure, since we got three heads and one tails, we know that it is not a zero
probability coin of getting heads, because we actually got some heads.
But let me show you how to read this graph.
What this says is that there is some chance that the coin only has a point four chance
of landing heads, and yet we got this data.
But there's not too much of a chance.
On the other hand, there's more of a chance that the probability is higher, and the way
to assess this is the following.
You say, for every possible probability of the coin, for example it might be a fair coin,
have a 50% chance of landing heads.
Under that assumption, we could compute what the probability is of getting three heads
and one tails in four flips, and mark that probability here.
We notice that if we have a coin whose probability of landing heads is 70%, then it's more apt
for us to get three heads and one tails, and so we compute what that probability is and
we mark it here.
So for every possible probability between zero and one, we compute what the likelihood
would be of the outcome of the experiment if that were the probability.
And so our view of the world is that this entire distribution of possible world states
is our actual belief system associated with this penny.
When we get more data then, what we do is we update our view of the way the world is.
When we got an extra tails, it shifted that way.
So the Bayesian concept of the world is that instead of saying the world is exactly one
way or it isn't, instead we say the world has all ranges of possibilities, like the
coin has all range of possible ways that it could be, and what we're updating is our
sense of the probability that the world is one way or another.
And that experiments then help us to update our probability.
This is a little different view of interpretation, but we'll see an example in a later lecture
where this comes up.
I look forward to seeing you next time.
