You may remember Mark Twain's quote that he attributed to Disraeli, that there are three
kinds of lies. Lies, damned lies, and statistics. So in this lecture we're going to embrace
that quote and talk about how to distort, mislead, and lie with statistics. So we're
going to give some examples. One of the common ways to mislead with statistics is to use
the mean inappropriately. One example we already talked about before was the fact that the
average American has one testicle and one ovary. Completely legitimate mean there's
no question about actually computing, doing the computation correctly, but it's somehow
misleading because it fails to mention the lurking variable of half of the population
being male and half of the population being female. Another example. Suppose that you
are trying to find the school that's just perfect for your child and you look around
the country and you think about various qualities such as academic quality, but eventually you
decide that it would be better simply to go for the bottom line and ask what graduates
from the school, what school has graduates whose average wealth is higher than other
schools? And after looking around you find a particular school where the average, the
mean of the wealth of each student is in the millions of dollars over all graduates over
the whole many decades history of the school. So you say that's the school for my child
and you enroll that your child in the lakeside school in Seattle, Washington. And then you
go to the alumni meetings and you see people around and you notice that although they are
in fact probably far wealthier than average, you discover that well they're not all millionaires
and then you realize the error in your ways, namely that two of the graduates of the lakeside
school were Bill Gates and Paul Allen, founders of the Microsoft Corporation. And their wealth
is so vast that taking the mean over the remaining many, many hundreds of students who graduated
from lakeside school, everyone else could make zero money and still there'd be millions
of dollars for every graduate. Here's another example. It's very simple and yet it can be
misleading. Suppose that you're at a university and you want to advertise how much attention
individual students get. So you advertise that you want to tell what the average class
size is among all your classes. Well, what does that mean, average class size or mean
class size? Well, one way to think about it is that you simply take the number of classes
and divide by the size and then get the average. So suppose that we have a school that has
a thousand students in it. It has one class with 901 students and 99 classes that are
individual tutorials. In that case, there are 100 classes and one of them has 901 students.
The other 99 of them have one. So the average students per class are 10. So but on the other
hand, if you ask the students what their opinion is of the average class size, of course, 901
of them feel that they're in a school that has a class size of 901 and only 99 of them
feel that they're in a school that has class sizes of individual attention. So from the
point of view of the students, the mean class size is the each of the 901 students says,
I'm in a class of 901. So we multiply those together at the 99 students who are in class
size of one divide by the thousand students. And we see that the the mean from the point
of the view of the students is 811.9 students per class. In this case, the median class
size being one is a is not a very good representation either. And the median class experience of
901 may be better. Sometimes the median is a better summary than than the mean. For example,
if we're looking at the lakeside school example again, the median would be a more representative
number for telling what the what the sort of speak medium central wealth is of the graduates
of that school. So taking the mean is a good way of distorting data. Another kind of example
is bias. Bias occurs in many places in statistics. We saw in the literary digest taking a sample
of only wealthy Americans in the 1936 presidential election and getting a far unbalanced view
of reality. But I want to talk about the kind of bias that happens to each of us every single
day. And that is that all of us are our existence and our impression of the world is obtained
by looking around at the people who are immediately around us, namely our friends and our families.
Those are the people from whom we get an impression of the world. Well, our friends and family
are wonderful people. But what they are not is representative of the whole population.
The people whom we know in general tend to be like us. That's why we like them. We can
easily believe that everybody thinks exactly like us because every time we ask people around
us, sure enough, they more or less agree with us. So our friends are wonderful people.
They are delightful to have around us. But what they are not is a representative sample
of reality. Another way to get bias samples is by having wording of surveys that are completely
biased. If somebody takes a survey and asks the following question, would you rather have
the very risk taking Smith as your leader or Jones, who is likely to save us from desolation
and also loves his dog? Well, which one are you going to choose? So asking surveys in ways
that are either intentionally or unintentionally misleading can cause the answers to surveys
to be very biased and not representative of what people really think. But one of the most
serious biases of all are the bias of newspapers. Now, I know you've all heard of the bias of
newspapers. And usually when I say what's the bias of newspapers, you're thinking the
bias of being too liberal or being too conservative. But I'm talking about a bias that is far
more fundamental than that. The real bias of newspapers is that they are interesting.
Every newspaper puts stories that are interesting. That is the fundamental criterion for any
story. But what does it mean to be interesting? To be interesting means that it's a rare event.
That's almost the definition of interesting. It's got to be a rare event. And usually,
by the way, it has to be a rare event and a bad event. If you go through the first section
of a typical paper and just go through it line by line, you will be amazed at the, or
you probably won't be amazed. But the fact is that almost all of the articles are bad.
When my daughter was 16, I was thinking, well, wouldn't it be good to get her in the habit
of reading the newspaper? So for a couple of days, we took the New York Times and I read
some stories from the newspaper. And after a couple of days, she said, Daddy, I don't
want to do this anymore. And I said, why? These are interesting articles. And she said,
because everything is bad. It talks about war. It talks about criminal behavior. It talks
about death. And I'll actually illustrate this with numbers to illustrate to you how
biased the reporting is in that sense of choosing interesting things.
Because you're contemplating taking a trip to Israel. Now, in a recent year, if you're
thinking about going to Israel, of course, they're constantly having terrorist attacks
in Israel. People die in these attacks. And you might ask yourself, is it safe to go to
Israel? Well, one way to determine that is to actually look at the data. If you look
at the data, in a recent year, the data say that there were 248 people killed in this
one year from terrorist attacks. The population of Israel is 6.5 million. If we compute the
rate of death per person in Israel for a year, and then we do the similar computation for
the rate of death from automobile accidents in the United States, we will find that the
rate of death from automobile accidents in the United States is three times the rate
of dying from terrorist attacks in Israel. Yet why do we say to ourselves, well, I get
in the car every day and it doesn't bother me. That's not something I worry about. And
yet going to Israel is something where you'd consider this. It's because every one of those
248 deaths in Israel was on the first page of the paper. This is bias. If we had a newspaper
which was not biased toward the interesting, it would be a very dull newspaper indeed.
It would say, today Mr. Jones did not commit murder. Today Mr. Smith did not commit murder.
Today, you see, it would be a very dull newspaper. There are other issues about selective reporting,
by the way, that are other ways of giving misleading reports. Suppose that you have
invented a miracle drug. It's a miracle drug in the sense that it really doesn't do anything.
It's a complete wash. It has no actual active ingredients. But you want to sell it as a
miracle drug. And you know that to be persuasive and to sell things as a miracle drug, you
need to have good statistical evidence that it's an effective drug. Well, we know how
to get such evidence. We know how to take tests because we followed this course and
we know that the way to do it is to do double blind procedure tests where you take a placebo
versus the purported miracle drug. You give it to a collection of people. You don't tell
them which they're getting. You don't tell the experimenters which ones they have. We
know how to design an experiment. So here's what you do. If you want to sell your miracle
drug, all you do is realize that there's the possibility of a type one error. Remember
what that means. The null hypothesis when you're testing a drug is that the drug has
no effect. A type one error occurs if it actually, if in fact it has no effect, but the evidence
indicates that it does have an effect. So if you want to sell your miracle drug, all
you need is a type one error. How could you get a type one error? It's very simple. All
you do is do several hundred experiments, perfectly designed experiments, wonderful
experiments, double blind. Everything is perfect. You know that a certain percentage
of the time, if your significance level is 0.05 as is typical, then about one out of
20 times. In fact, the results of the experiment will indicate that your drug is in fact efficacious.
So you say, that's great. I'll take that one out of actually 2.5% because half the time
will be lower than average than the other. So 2.5% of the time, the test will indicate
that the drug is effective even though it has no effect whatsoever. So this is a great
way. Then all you do is you report that test. The person reads the test in the paper. They
say this is a very well conducted test and they're persuaded to buy your miracle drug.
There's another example of this and that is how to become a perfect predictor of the future
of stock prices. Here's what you do. It's very simple actually to be a perfect predictor
of the future of stock prices. Here's what you do. You send out 1,024 predictions at
the beginning of a week to 1,024 people and you say, I'm going to tell you, I have special
insight and I will tell you whether Dell stock will go up or down next week. But what
you do is you send 512 people the prediction that it will go up and you send to 512 people
the prediction that it will go down. After the week is over, you see which it did. So
you've been right. For 512 people, your credibility is a little bit up. So to 256 of them, you
send a further note saying next week it will go up to 256 of them. You send a note saying
it will go down. Suppose it goes down this time. Then you send 128 up, 128 down and so
on all the way through. Pretty soon at the end, you have been right 10 times in a row.
And when you write 10 times in a row, that's a lot of very credible evidence to the person
who received those reports that you can actually predict the stock price. And then maybe you
can ask them to pay you for the next week's prediction and that person would probably
pay you. That would be great. Another way that we can be misled with selective reporting
is by simple lying. People on surveys lie. For example, you have probably heard that
child molesters often were molested when they were children. Well, it turns out that some
studies and I'm not an expert in this, so I don't claim that this is absolutely definitely
true. But some studies seem to indicate that in fact that's simply not true. That it's
just that child molesters realize that there's some advantage to claiming to have been molested
as children in the penal justice system. They're treated more leniently in court. They think
they simply lie about that. So it's not 100% clear what the truth is. Another way to lie
with statistics, it's a very common way of lying with statistics, is by the use of appropriately
drawn graphs. Now graphs are methods for displaying data and they can be very useful, of course.
But here's an example of a graph that shows an amazing trend in this particular example
where you can see that this is an example of the increase in the national debt over
a month period and you can see how vast it is. You see? And the big numbers on the side
that show how strongly it increases. The problem is that this graph has a big distortion
to it. Namely, you've chosen to make the labels on the vertical axis be spread out so much
relative to starting from zero that it looks as though this is a huge increase when in
fact the reality is that the increase is only a small amount. So by choosing the axis appropriately,
you can make even a little tiny percentage rise increase look vast just by your choice
of the values that you put on the y-axis. Another wonderful example of graphical distortion
comes about by drawing graphs that try to display data in one way and actually mislead
by having a three-dimensional aspect to them. Suppose that you're trying to show that Americans
are becoming more overweight and you've detected an increase. Well, one way to display that
would be to just draw a bar graph that showed a modest increase from one level to another.
But if you wanted to really drive the point home, what you could do is draw a figure and
you've seen these figures in graphs where they try to draw them. So you could draw a
graph where you have a picture of a very thin person of a certain height, very thin person
here, and then next to that person you have a person whose height really does accurately
indicate the data that you're trying to convey. But you draw this person as a rather large
person that's sort of overweight looking. Well, the effect is that visually when we
see something that has volume to it, we naturally have an intuition that it's vaster because
it deals with the volume rather than just the height. So that's an excellent way to
distort data by drawing a three-dimensional histogram when you're really just referring
to the height. Another example of ways to mislead with data is the following. Suppose
that you want to scare people about the effect of a particular danger in society, whether
it's a food that may be dangerous or some other danger. One thing that you might find
is that you might report a 30% increase in the risk of, say, heart attack if you eat
this particular candy. Well, it may be completely right. Maybe there is a 30% increase. But
what is it 30% of? Maybe your original danger from whatever it is you're talking about was
1 in 10 million. If there's a 30% increase, it increases to 1.3 in 10 million, and it
may be completely insignificant. But by representing it as a 30% increase, it sounds really bad.
On the good side, suppose you're in a company, and it's a multinational company, and your
company really hasn't been doing so well. So maybe in a quarter, it just barely made
a little tiny bit of money. And to exaggerate, let's say it made $1,000, and this multi-million
dollar company just barely on the positive side. And then the next quarter, it reported
50% increase in profits. You see, so it goes up from $1,000 to $1,500. Well, it's accurate.
It's not lying. But on the other hand, it is a misrepresentation to get the wrong sense
of the significance of that 50% increase. A wonderful example of looking at data in
different ways to get a distorted view, or not actually so much a distorted view, but
just a different view, actually. I'm going to argue that these are both completely legitimate
views of, in this case, is the question of tax cut savings. So people often are proposing
tax cuts or enacting tax cuts. And there are always two sides to the question of whether
or not that tax cut is a good thing for the population in general. And people draw charts
to represent whether or not it's a good thing. So here's an example of a chart that represents
the effect of a particular tax cut that is proposed. And the tax cut, here it has on
this axis, it's a histogram that has ranges of incomes. And then on this axis, it has
the percentage savings from the tax cut, the expected percentage savings for a person
in these different ranges. And you can see that it's a rather even-handed tax cut that
all of the people in the whole range of salaries roughly will benefit the same, get the same
benefit from the tax cut. And even the people here who make more than $200,000, they actually
get maybe a slightly smaller percentage increase because benefit from their tax cut than some
of the other people in the population. Now, the opposing political party will produce
a different graph that really illustrates exactly the same material. Namely, its graph
will say, what is the dollar amount savings for the total group of people in that tax
bracket? And in this way, you see a vast, much higher graph at the end because of the
fact that the total number of people who make more money, the total wealth in the higher
income brackets is so high that the total amount of actual dollar savings would be much
more for those people. Now, which of these views of the tax savings is correct? Well,
the answer is they're both correct. They're both presenting actually accurate information,
but one of them is trying to make a point that the wealthier people total are saving
more money, whereas the other person is making the point that everybody is being treated
even in an equal way from the point of view of percentage savings. I am waiting for the
day when a politician will show us both graphs and explain why both graphs are telling the
same information and that it's up to us to try to judge which one is the more persuasive
based on philosophical grounds. Another category of how to distort and mislead from statistics
has to do with extrapolation. Inappropriately extrapolating data is a case that can lead
to some really ridiculous conclusions and can also be very scary. One example has to
do with population growth trends. In the 20th century, that is between the year 1900 and
2000. In the 20th century, the world experienced a very fast increase in population growth.
It was 1.3% per year was the rate of population growth during that century. Well, one thing
that we can do when we have a rate like that is to extrapolate that population growth to
a much longer period of time. So, for example, let's just take that population growth starting
at the year 2000, assume that we have a 1.3% growth rate as we in fact experienced for
the entire last century. That's 100 years. That's a lot of information that gives us
a rate. Suppose that we say, oh, how many people will be alive in the year 3000? Well,
one of the properties of growing at a certain rate like 1.3% is that there is a exponential
growth effect which leads to a curve such as this one. If we simply extrapolate to the
year 2000, we will discover that this is the astronomical number of people that there would
be on earth in the year 3000. Now, this is so astronomical. It's two quadrillion, 441
trillion people. To show you how vast this is, if we divide that number of people by
the number of square feet on earth, we'll discover that roughly speaking, if I did the
computation right, that basically there'll be two of us in each square foot in the year
3000. This, by the way, will not happen. So, just for your information, this will not happen.
But there are other trends that are followed. For example, here's a trend. These were data
about the debt held by the public for a few years, and it actually declined in about 1998.
And here's a quote from the Office of Management and Budget. It said, in 1998, the federal
budget reported its first surplus, $69 billion, since 1969. In 1999, the surplus nearly doubled
to $125 billion, and then again in 2000 to $236 billion. Under the President's budget
proposals, $2.0 trillion in federal debt held by the public will be retired over the next
10 years, all of the debt that can responsibly be retired. That was written. And, oh, by
the way, let's see what actually happened. Ah, here are the new figures, the real figures
for the public debt. Well, okay, that trend simply didn't quite happen as hoped.
Other kinds of extrapolations that we can make. The world records. If we look at the
record for running a mile, how long, the world records for running a mile over this last
century. You see these dots here. This is a scatter plot that shows the world record
for the mile as it has occurred during this last century. And you can see that it's well
approximated by this straight line. Okay, so this looks like a good trend to extrapolate.
Let's go ahead and extrapolate it. Here it is. We extrapolate the trend at exactly that
same slope. And we discover that in the year about, well, 2600, the person will have run
a mile before they even start. It'll take less than zero time to run a mile. So that's
an example of extrapolation that may be a little bit exaggerated. Another category of
misleading with statistics comes from confusing correlation with causation. And we talked a
little bit before about some examples of these. One example is that suppose that you wanted
to promote family values by pointing out that married people make higher salaries than unmarried
people. That may be a true fact, but why? Well, married people may on average be older. In fact,
it would be more, it'd be more persuasive to say people who have been married for 20 years make
more salary than people who are unmarried because there's a lurking variable. There's a hidden
variable that really explains the increases in salaries. And it's not really a cause and effect
relationship. Another example of correlation that you may detect is not actually cause and effect,
is IQ versus shoe size. So little babies are often cute, but their IQs and their shoe sizes don't
tend to be that big. So as they grow, their shoe size grows and their IQs grow, but in fact,
that's not cause and effect. So it probably wouldn't be useful for my students to buy big
clown shoes before the next test on statistics in order to help them out to do better on the test.
Another example of correlation versus causation is that the people who are in first class on
airplanes tend to be wealthier than people in coach. So you might think that the road to wealth
is simply to buy first class tickets whenever you fly. But of course, unfortunately, that may
actually have the opposite effect. Well, I hope you've enjoyed an excursion through how to distort,
mislead, and lie with statistics.
