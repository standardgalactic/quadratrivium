Welcome back to Meaning From Data, Statistics Made Clear.
In this lecture, we're going to be talking about science and applications of statistics
to science.
Certainly statistics and the statistical analysis of data are obviously central players in all
aspects of science, and during the last 400 years we humans have fundamentally altered
our conception of the universe and our position in it, in many cases as a result of a scientific
advance that ultimately is based on the analysis of data.
So let me first talk about the concept of the solar system.
In the year 1600, Johannes Kepler was in the observatory of Tico Brahe and was looking
at the data associated with the positions of the planets as they moved in the night
sky.
Tico Brahe had accumulated the best observational data, this is before the invention of the
telescope, he had the best data that had ever been collected and one of the anomalous challenges
was to explain the funny behavior of the motion of the planet Mars, which had this retrograde
motion and other anomalies that made it difficult to explain.
Kepler took the data from these observations and basically fitted a curve to the data to
make his laws of planetary motion, namely that the planets revolve around the sun in elliptical
form.
So Kepler's contribution to our understanding of the solar system and our place in it was
a statistical one, it was data fitting, finding a model that approximated the data that was
actually observed.
Later our understanding of the reason for having elliptical orbits was put on a different
foundation when Newton proposed his universal law of gravitation from which it follows as
a mathematical consequence that planets that were undergoing this kind of inverse square
law of gravitational attraction would in fact automatically follow elliptical orbits.
So that changed the presentation and our concept of the reality from a statistical curve fitting
concept of Kepler to a one based on a more fundamental physical law.
This is frequently the way science proceeds that at one point we have an observation that's
a rather empirical kind of observation that later is put on a firmer foundation when a
theory that implies that foundation is found.
In our concept of the entire universe we have the example of Hubble's red shift which
was a statistical observation that there were differences in the spectra of stars that were
further away that implied that further away stars were moving faster.
There was the concept of the discovery of the three degree background radiation that
was discovered when it was found that in trying to create a very precise radio telescope
the people were unable to tune it properly to the level that they thought no matter what
direction they turned it in the night sky they discovered this background radiation.
That was a statistical or a measurement issue which was then later explained as the leftover
from the Big Bang and so it was confirmation of the Big Bang theory of the universe.
So measurements are used in both finding out interesting things about the universe and
also to confirm a model or reject a model.
For example, Einstein's theory of general relativity was confirmed or we should say
Newton's theory of the universe was rejected when observations were made about the light
bending around the sun in order to say that this would not have happened the measurements
that were obtained would not have been obtained had Newton's theory been correct.
In physics the concept of the most fundamental understanding of the basic particles that
we live in are probabilistic in nature, live with or made of, are probabilistic in nature.
Time field theory implies that we are not to think of a subatomic particle as being
in a particular place at a particular time but instead it's a probability distribution
and that that is the actual reality and this very strange idea puts the concepts of probability
and chance as fundamental players in our understanding of the physical world.
Measurements and the interpretation of measurements are very basic to the scientific process and
I want to give an example where measurements had an interesting wrinkle to them.
Back in the 1970s people began measuring the thickness of the ozone layer in the stratosphere
or the upper atmosphere and I wanted to tell the story as an illustration of some part
of a statistical practice.
When measurements were made about the thickness of the ozone layer there were many measurements
that were very similar to each other as you would expect if you were measuring something
where the concept was that the thickness was uniform.
Many of the measurements were the same but there were a few measurements that were much
much smaller that were close to zero and there was a computer program that was taking these
measurements and it was set to identify measurements that were too far out of the bounds of the
preponderance of the evidence and label them as potential outliers.
Outliers are data that don't seem to fit the pattern of the remaining part of the collected
data and so by noticing that there were these measurements which were close to zero the
computer program threw them out as potential outliers.
Now it actually kept the data and the data were recalled later so that was important
but the issue is that we can, in collecting data very often there are errors that just
occur accidentally.
For example our measuring instrument may have been on the blink that day and got a value
that was way off.
Before we had an understanding that there was an actual hole near the south pole and
near the poles in the ozone layer it was just viewed as the hypothetical image of our concept
of the ozone was that you could imagine that there was a uniform distribution of the ozone
layer across the whole earth.
So one question that we have about data is what is the application of the generality
of the application of the data that we collect?
In other words if we collect data from various spots on the earth and it was pretty uniform
suppose no data had been collected around the north pole at all, around the poles at
all, the south pole at all then it might have well been that we would claim that the data
implied that there was a uniformly thick layer of ozone across the whole earth.
But the fact that there was this anomalous outlying data which we then threw out turned
out to be actually a real phenomenon and we shouldn't have thrown it out and it would
actually point it to a phenomenon that is actually real.
I'm trying to make the point that sometimes the question of the applicability of the
data we get is not clear how generally applicable it is.
Suppose that we assume that all fish are the same with respect to some property then we
might do an experiment that just used say a salmon and then draw conclusions about
all fish and that's certainly fine if all fish really are the same with respect to
that property but it's not always clear that that is the case.
There could be some lurking variables that make a different species of fish behave differently.
So in any case science proceeds by developing models of what we think the world is like
and then looking for data that test the models by comparing the experimental results with
the predictions of the model.
And in many case the scientific theory is tested by comparing experimental results to
the prediction of such a theory and one famous example of that is in the case of Mendel's
famous experiments with peas.
In the middle of the 19th century Gregor Mendel was a monk in a monastery and he conducted
many experiments that are now viewed as seminal experiments with respect to our understanding
of heredity and how characteristics of the parents are passed on to the children.
In particular he had a model in mind where he envisioned that each of the parents had
two genes of which they contributed one to the daughter plant and then those two genes
were made up the genetic basis for the actual appearance of the plant.
So what I'm going to be doing now is to explain some of Mendel's experiments and try to describe
how it is that those experiments proceeded.
So we'll begin with this chart.
So here's an example of a typical kind of experiment that Mendel performed.
The peas that he was working with had actually many different characteristics that he worked
with but I will just talk about one characteristic namely color.
He also talked about things like whether it was a wrinkled pod or a smooth pod and other
kinds of characteristics but I will just talk about the two different colors yellow and
green.
And what Mendel did, the basic concept of his experiment was this.
He imagined that he had some plants that had two yellow genes and he took these yellow
genes and then he crossed them with plants that he assumed to have two green genes.
It turns out that yellow is the dominant gene so that if you have a plant that has a yellow
and a green gene or has two yellow genes then the plant will appear yellow.
So when he crossed plants that had both yellow genes and this is called by the way homozygous
meaning that both of the genes are the same.
When he crossed a homozygous yellow gene plant with a homozygous green gene plant all of
the offspring had one of the genes from this parent and one of the genes from this parent
in its genetic makeup.
So all of the offspring of these plants had one yellow and one green gene and all of them
appeared yellow because yellow is the dominant gene.
Now the interesting part of the experiment occurs at the next generation.
Sometimes that we take plants that were the result of that previous breeding and we had
plants each of which was heterozygous that had a yellow gene and a green gene and we
combined them together to form the potential offspring that they could have.
The concept that Mendel and the theory that Mendel proposed and that these experiments
were intended to support would predict that for each parent it would randomly contribute
one or the other genes to the daughter plant.
So there are four possible things that could happen in this kind of an experiment.
First of all you could have both parents contributing the yellow gene or the first parent could
contribute green and the second yellow or the first yellow and the second green or both
of them could contribute green genes.
Only in this corner here the plants that have two green genes would the plant actually appear
green.
This is the pod of the pea plant.
It would appear green only if both of its genetic contributions from the parents were
both green genes.
Now so the way that these experiments proceeded was that Mendel crossed a bunch of heterozygous
plants and looked at the proportions of the offspring plants that were yellow and the
percent that were green with the expectation that in fact a quarter of them were green
and three quarters of them were yellow.
That was the expectation that he was trying to prove this theory by getting evidence by
doing these crossings.
And among these three he expected one third of these to be homozygous with yellow yellow
genes and the other two thirds to be heterozygous, one green and one yellow gene.
Now let's look at some data.
Suppose that we had done an experiment in which there were 200 plants expected in each
of these quadrants.
Then we would expect that in doing this experiment many times which he did he would have expectations
of this kind of an outcome if this were the size of the experiment.
However you might ask the question how does he know whether a plant is heterozygous or
a plant is homozygous?
Well, what Mendel did and he specified his method for determining whether the plant was
homozygous or heterozygous is that he took these plants that are within in this quad,
these three, that is they appeared yellow, and he bred them with themselves 10 times.
By breeding them with themselves of course if it's homozygous on each of those 10 times
he would always get a yellow plant.
However if he has a heterozygous plant he reasoned that the chances were very good that
if in 10 breedings one of the self-breeding would contribute both green genes and the
plant would come out green.
So that would be an indication that the plant that he started with was a heterozygous plant
that it had the green gene as well as the yellow gene.
So his method was to take his plants and do 10 self-crossed breedings and on that basis
classify the plant as being heterozygous or homozygous.
Now this was the data that he collected.
He collected a great deal of data which all supported his theory.
In many instances he received in data of this size he would find that there were 201 plants
that he had classified as being homozygous with yellow and that the ratios were appropriate
very close.
In 1936 Ronald Fisher, you may remember Ronald Fisher, Ronald Fisher was the statistician
of the Lady Tasting Tea fame.
In 1936 Ronald Fisher wrote a paper in which he investigated Mendel's data and he made
some observations about the quality of Mendel's data.
In particular he noted that Mendel's data was too good to be true.
Remember that when we are dealing with a random process we don't expect the answers to always
be exactly according to expectation.
We expect a distribution of the answers.
That most of the time the answers will be within a certain distance of the expectation
but a certain fraction of the time we would expect to have outliers.
We would expect to have unusual occurrences, rare occurrences.
So one of the things that Ronald Fisher pointed out were that the number of experiments in
which Mendel's data was very close to expectation was too great to be believed.
So let me give you an example of the reasoning that Fisher used.
Suppose that you take a coin and you flip it a thousand times.
You know that on average that the mean of the distribution of the flips is going to
be 500 but you also know that if you actually flip a coin a thousand times that often the
number of heads will be less than 500 and more than 500 and several times during these
lectures we have seen the distribution of the number of heads for example that would
be likely to appear if you did the trial of flipping a coin a thousand times.
This is the distribution, in other words this tells you that occasionally you would get
540 heads but the proportion of times you'd get it would be very rare and most of the
time you would have some number of heads in your thousand flips that lies somewhere
in the big part of this curve.
However this very thin region in the middle here is a region which contains only a small
fraction of the data about 7.5% and most of the time that you flip a coin it would not
lie between 499 and 501 most of the time you would get values that differed from 500 by
some bigger amount.
What Fisher did was to notice that Mendel's data tended to give more outcomes that were
within one standard deviation of the mean than would be expected.
You would expect outcomes of an experiment that involved random chance to lie within
one standard deviation of the mean in a normal distribution about 68% of the time.
But that means that about 32% of the time you would expect the results of that random
experiment to have values outside of one standard deviation from the mean.
And yet it turned out that the data that were reported by Mendel had too high a frequency
of good data, of the data being too close to expectation.
So Fisher was arguing that the data was not properly constructed but Fisher went on to
make another interesting claim about the results from Mendel's data and that is the following.
If you recall, the strategy by which Mendel chose to classify a plant as heterozygous
was to do the experiment of self-breeding ten times and if the plant came out yellow
all ten times it was classified as homozygous.
There is a chance, a small chance, that if one took a heterozygous plant and by randomness
alone bred it with itself ten times that every one of those ten times it would have contributed
a yellow gene and would be yellow every one of those ten times.
And in fact it's not a difficult computation to see exactly what proportion of the times
that would happen.
Namely, the chance of when you crossbreed a heterozygous plant that is a green and a
yellow plant and you randomly pick a green and a yellow, you randomly pick the two.
The chance is three out of four, one, two, three, out of one, two, three, four that one
of the two genes contributed at least one will be yellow.
So there's a three-quarters chance when you cross a heterozygous plant with itself there's
a three-quarters chance that it will be yellow.
If you do it ten times there's a three-quarters to the tenth power chance that it will be
yellow every single time.
Three-quarters to the tenth power is five point six percent.
So the probability of misclassifying a heterozygous plant is five point six percent.
In other words, if you use the method that Mendel did use of saying, I take a plant is
it homozygous that is having two yellow chromosomes or two genes or does it have a green and a
yellow, he bred it with itself ten times.
If it was always yellow he classified it as homozygous.
But five percent of the time that you started with a heterozygous plant it should have been
misclassified as homozygous.
What that means is that Mendel should have, in doing his experiments in the way he specified
that he did them, he should have misclassified twenty-two plants that were in fact heterozygous.
He should have classified them as homozygous.
So what Fisher pointed out was that the actual expectation for that square of the classical
classification of plants should have wrongly classified twenty-two extra plants as homozygous
that actually were heterozygous.
So that the actual expectation from the experiment should have been two hundred twenty-two point
five, not the two hundred, which was the actual expected outcome that are really homozygous.
The effect of this is that when Mendel reported an experiment, and this is a specific example
of one experiment of many, in which the reported number two hundred and one is very close to
the quasi expectation of two hundred.
But you see that it's rather distant from the what should have been expected including
the falsely classified heterozygous plants that were should have been falsely classified
as homozygous using that method.
Well one can do a statistical analysis where the null hypothesis is that the fraction of
the plants should have been two hundred twenty-two point five out of six hundred, and the alternative
hypothesis is that the fraction is less than that.
You find that the p-value for that hypothesis test is point oh three seven.
So if you use the traditional cutoff margin of five percent, you would actually reject
the null hypothesis that the two twenty-two point five was the was the actual expected
value, but it should have been the expected value.
Let me quote from so this is this is the kind of reasoning that Fisher used in his article
to show that the data that Mendel got were too good and that were in fact in this case
Mendel wasn't subtle enough to realize he should have been expecting two hundred twenty-two
point five instead of expecting two hundred in that in that box.
And so his data came out more like the two hundred than what he really should have found.
So I wanted to just quote a few lines from this paper by Ronald Fisher.
This titled has Mendel's work been rediscovered and it was published in nineteen thirty six
in the annals of science.
I did want to read that the description of Fisher.
He is the Galton professor professor of eugenics at the university college London.
By the way that's another interesting wrinkle to Fisher which we will not talk about now.
But what he said in this paper is the following the discrepancy is strongly significant and
so low a value could scarcely occur by chance once in two thousand trials.
There can be no doubt that the data from the later years of the experiment have been biased
strongly in the direction of agreement with expectation.
So he's saying that the data were cooked.
One natural cause of bias of this kind is the tendency to give the theory the benefit
of doubt when objects such as seeds which may be deformed or discolored by a variety
of causes are being classified.
Such an explanation however gives no assistance in the case of the tests of gametic ratios
which is what we were just talking about and of other tests based on the classification
of whole plants.
The bias seems to pervade the whole of the data.
This is a quotation from his work.
One more quotation.
Although no explanation can be expected to be satisfactory it remains a possibility among
others that Mendel was deceived by some assistant who knew too well what was expected.
This possibility is supported by independent evidence that the data of most if not all
of the experiments have been falsified so as to agree closely with Mendel's expectations.
So this is a little wrinkle in the history of science associated with data being too
good to be true.
I want to close this lecture with one other completely different kind of experiment that's
a little on a lighter tone and that is that the here is a basic scientific question that
you maybe have wondered about.
How can people count tigers that are in the jungle when they can't find all the tigers?
How would you go about counting tigers?
There's a very clever way of counting tigers in a jungle and this is the way you do it.
You capture 50 tigers and then you put ear tags on those 50 tigers and then let them
back into the jungle and then you let them mix around for a while and then you come back
and you capture some other group of tigers, perhaps 100 tigers and you see what proportion
of the newly captured tigers have ear tags.
Then the computation is very simple to get an estimate for the number of tigers in the
jungle so here it is.
So we've tagged 50 tigers so you imagine 50 tigers going around the jungle randomly,
you randomly capture 100 of them, suppose for example that 8 of the 100 that you recaptured
were tagged, then you would have this little formula here that 50, the number of tagged
tigers is to the whole population as the number recaptured who were tagged are to the number
that you actually captured.
So in other words, you captured among the 100 that you captured the second time, 8 of
them had ear tags.
So if 8 out of 100 had ear tags, you'd expect the whole population to reflect the same proportion.
So just doing the math here we see that the population would be estimated to be 50 times
100 over 8 or 625 tigers in the jungle.
So I always found this a very clever method, it's a question of capturing and then you
let them out and then you capture some more and you see what proportion of the newly captured
ones have the tags and using the proportions you just figure the whole proportion is reflective.
You can do the same kind of an experiment with finding the volume of water in a lake.
You can take one pound of salt in this clear water lake and dump it into the lake and then
wait till it sort of stirs around maybe several weeks until it's completely evenly dispersed
throughout the lake.
Then take one cubic foot of water from the lake and boil off all the water until you just
measure the salt and you very, very fine measurement of the salt.
Suppose you have one millionth of a pound of salt in that one cubic foot of water then
you would infer that there were a million cubic feet of water in the whole lake.
So you can imagine doing this sort of capture, recapture method in many instances for example
if you had a natural gas deposit under the ground and you want to know how big it was,
well you could put in a certain volume of some number of molecules of something that
mixed around and if it mixed around then you could take out a sample from the natural gas
and see what proportion of the molecules were in that volume of the second sample and deduce
what the volume of the whole find was.
So we've seen then in this lecture several examples of statistical applications in science
of which of course there are many.
This is just obviously the tip of the iceberg in this area.
I look forward next time to talking to you about the use of statistics in discovering
the authorship of papers.
