Welcome to Meaning From Data, Statistics Made Clear.
This is the third lecture.
In the previous lecture, we described how to organize, describe, and summarize a set
of data if we knew all of the data.
The conclusion of that lecture could be summed up by saying that if we know all the data
in a population, then we will understand its distribution by describing its shape, its
center, and how spread out it is.
It's spread, shape, center, and spread.
So in this lecture, we're going to introduce basic concepts and principles of statistical
inference.
Namely, how can we use information about just some members of a population to infer that
information about the whole population?
Well, for example, the center of the population.
Suppose we know, we get, we just measure the heights of a certain number of people and
we want to estimate what the height of, the middle height of everybody is.
And we see the kinds of results of this type of analysis frequently, particularly before
an upcoming election.
A pollster will say, a poll shows that candidate A is preferred by 56% of the voters with a
margin of error of plus or minus 3%.
But I can assure you that most people don't know what that means.
And in fact, you won't know what it means at the end of this lecture, but you'll begin
to understand that.
But you will understand that after lecture 11.
Okay, now I'm going to do something I hate to do, which is to tell you the conclusion
of this lecture right now.
And the reason I hate to do it is because it spoils the drama.
See, and I like drama, but I think it's important to say this.
Namely, the conclusion of this lecture will be that randomness is a central idea of statistical
inference, randomness.
And that the goal of statistical inference is to answer two questions.
How close and how confident?
Meaning how close are the shape, the center, and the spread of this sample of a small number
of elements from the population?
How close are they to the actual population's shape, center, and spread?
So how close and how confident are we that we are that close?
So those are the two questions.
How close and how confident?
Well, in the last lecture, we discussed several examples, including heights of men and women,
batting averages of baseball players, salaries of people, mule kicks, remember the mule kicks,
and SAT scores, those were some of the ones that we talked about.
So the challenge of this lecture then is if we don't know all of the information about
every single member of those populations, how can we infer the whole population?
So let's begin with the heights of adult American men.
Suppose that we know just some of their heights, and what we really want to know is an accurate
description of the distribution of the heights of all adult men in the United States.
We don't want to know the shape, not the shape of the individual men, but the shape of the
distribution of the heights of the men.
We want to know what the center value is, like the mean of the heights, and we want
to know how spread out they are, how many people are quite a bit taller than that average
center value, the mean or the median, and how many are lower.
So we'd like to get a picture of the whole population by choosing a few good men.
We would like to choose a few men whose heights somehow mirror the heights of all the men
in the population.
So for example, for the center, really that is to say the mean value of heights, all we
would really need to do is choose one man, namely choose one man whose height is average,
and then that would be great.
Wouldn't that be a wonderful strategy?
Because then we'd get the center value with just one person.
There's only one slight defect to that wonderful strategy for figuring out the whole population's
distribution and center.
The problem is, of course, that we don't know the answer before we start.
We don't know the whole population's center.
Otherwise, we wouldn't be doing it.
That's the whole goal of our exercise of trying to infer what that center height is.
If we knew the answer in advance, we could just select the people that reflect the properties
of the whole population.
So we could pick them to have the same average height, we could pick just one person or a
collection of them that have the right average height, and we could pick a few people whose
heights were distributed in the same kind of a way that the whole population's heights
are distributed.
The same number that are close to each other, close to the center, and the same fraction
who are much taller and so on.
So our first thought of choosing a few good men, a few that actually describe the whole
population, was sort of a flop, it was a sort of a flop.
But we should never give up.
We don't give up just because we make a mistake, because there is a bright side to this.
The bright side is that we know that if we had a representative sample, if we had a few
good men, a few people whose heights, centers, and spread really did represent the whole
population, then that would be great.
So the question is, how can we find those few good men without knowing what the population
looks like in advance?
This sounds like mission impossible, right?
Mission impossible.
How can we possibly find something that when we know we don't know it?
Well, we have to take a very crucial step, and that is to resort to thought.
How are we going to get this representative sample when we don't know what it's supposed
to be representative of?
Well, by the way, it's true that in things like heights, since you see people all the
time, maybe you have some sort of a guess about what the heights are, but in other kinds
of things, like the thickness of the ozone layer, as an example, or something obscure,
you have no idea, you see, so that you can't judge the fact from what you know.
But so since we don't have any knowledge at all, let's use that fact that we don't
know, that we don't know.
I mean, life gives us lemons, let's make lemonade.
Let's choose our sample using our total ignorance.
In other words, randomly.
We'll go out of our way to not pretend that we know anything.
We choose people randomly.
That actually is the key idea to statistical inference, to choose people randomly.
And let's just try it.
Suppose we choose one adult male to measure, one adult male.
Now you might say, well, you don't know, the person may be extremely short, that one person
may be very tall.
But as a matter of fact, if we are dealing with a case like heights of people, there
is something that we do know about heights, namely that people are different heights,
but most people are somewhere near sort of the middle.
So one thing that we could do is knowing sort of knowledge of humanity, having some knowledge
about the distribution of heights in the population just by looking.
And by the way, this is the kind of general knowledge where we don't know where it's
centered, we don't know what the average is exactly, but we know that most people are
about the same.
This kind of knowledge can come from previous experience with other collections.
For example, you might think about rat experiments, because that's the kind of thing that we'll
actually be using in future discussions about things like tests of medication.
From different kinds of contexts, we might know, in the case of heights of men, that
the heights of men are basically distributed in some sort of a fashion where there are
people who have this sort of average height, and then there are some that are sort of tapers
off to the sides.
That we might sort of have a predisposition to suppose that that is the general picture
of the heights.
What we don't know, you notice that there's nothing on this axis, there's no number given.
It doesn't say where the center value is, because we don't know that.
It also doesn't say how spread out they are.
We don't know that either.
But we can guess that the distribution of heights is something like this.
Well, knowing that the distributions of heights can be drawn in something like this figure,
when we choose a random person, notice that we're much more apt to choose a random person
in the center area than we are to choose this random person out here, because most people
in the population are closer to the center value than they are to an extreme value.
You follow me?
So even choosing one random person and measuring that person's height, we're more apt to be
close to the middle than we are to be an extreme.
Of course, it could happen.
It could happen that we randomly choose some NBA basketball player who's seven feet tall,
and that's our random person, and then we're misled.
It could happen, but chances are better that we'll choose a random person in the middle.
Okay.
Now, how can we improve our chances of getting yet closer to the actual mean height of the
whole population?
How can we improve our chances?
Well, how about choosing two men instead of just one?
You see, if we choose two men and then we take their average, their mean height, we
would tend to get closer to the overall mean, because a lot of times we would choose one
randomly who was a little bit too short, and we chose another one who may be too tall.
Or even if we chose two people who are both too tall, notice that the mean of those two
would give us a number that wasn't as extreme being off as the taller one was.
You see, it's sort of bringing things in, taking the average of two, tends to get closer
to the true mean.
So in any case, as we choose more people and take their mean value, mean heights, we're
getting closer to the mean.
Those which choose three men or four men or five men or a hundred men, it's increasingly
unlikely that their combined values are going to stay an extreme value out from the mean.
In other words, they would tend to cancel each other out by randomness, because it would
be very unlikely, just think about it, if you choose a hundred people, it would be very
unlikely that you chose all hundred to be above average.
A property of average is half the people are below average.
So they would tend to naturally cancel each other out and have the population mean come
about without even trying.
So random collections of men will on average have close to the average height, and their
average height will tend to be more average as we get more people in the sample.
It gets increasingly likely.
Now let's actually do some numerical experiments here and do some examples.
So here is a table that lists what we did.
Here's what we did.
We took a list of actual heights, and there were some thousands of actual heights.
So we'll think of that as the total population.
And then from that, we took a hundred men at random from that list, and just took the
average, took the mean of those hundred, and got a number.
So the first time we did that, this number right here, 68.58, was the first time we took
a hundred people and took their mean, and we got 68.58.
We did it again, 68.32, again 68.66, again 68.18, and so on.
The actual mean of the whole collection was 68.68 inches.
And notice how close these values are.
Notice how close they are.
I mean this one is the most extreme at the top, 69.3, and let's see, is this the smallest
one?
68.18, which is half an inch too short.
So by taking a hundred people, the mean of those hundred just automatically got very
close to the mean of the whole population.
So what's interesting about this is that random chance leads to the actual correct value.
You see that?
Luck is on our side.
This is great.
Now, by the way, I did the same kind of experiments.
Of course, I have to say, as you look at these tables, you say, well, maybe I cheated, you
think, and maybe I cheated because maybe I tried it and I really got a whole bunch of
things and I just put down the ones that were close to the average.
Could have happened.
We do not know, but we just have to take my word for it.
Okay, now, so here are some other examples from the populations that we looked at before.
So we had a collection where the population mean was actually 68.5 and the mean of the
sample was 68.557.
So you see what I did is I took a hundred people and took their app just at random and
they're average.
Look how close it is.
In fact, just taking 25, the sample was pretty close, 68.315.
SATs, the actual population mean from this big collection was 169.85, taking a sample
of size 100, 1086.
In fact, even a sample of size 25, 1076, in fact, in this case, the sample of size 25
is closer than the sample of size 100.
Those things can happen just by random chance alone.
You see, there's no guarantee in this business.
There's no guarantee that we're going to get the exact value because just by randomness
you could get more or less.
The male heights, you see how close we got when we took 100.
Female heights, look at that.
We got very, very close from the actual value was 63 and a third.
The value from the just taking a sample of 100 was 63.45.
Batting averages, you see are very close.
The point of this is that randomness leads us to get close to the actual population values
by random chance alone, and that's rather interesting.
This comes up in political polling.
When we take a political poll and you ask people, who are you for, candidate A or candidate
B?
Close that we had a political poll being taken where the reality was that 60% of the population
are for candidate A. We did some random computer simulations of this and asked 100 people, just
chose 100 people at random when we assumed that 60% were for candidate A, and the fraction
of those 100, and there's the number from 100, how many of those 100, were for candidate
A in our random sample, were 55, 67, 59.
Each of these represents taking a sample of 100.
Follow me?
Take 100 at random and you see how many of those 100 are for candidate A. Went from a
population where 60% are for candidate A. The question is how close are these values
to the actual population 60%.
You can see that doing it, in this case we did it 10 times, that is taking 100 people
10 times, and notice that the furthest away that we got was 50, 50 was 10 away from the
actual being completely representative of the true population, and on the top side it
was 67.
Among those 10 examples, our sample of 100 was pretty good.
In other words, it was within 10 percentage points of the actual value, and in fact we
will show by using probability, we will see what fraction of the samples of 100 tend to
be close to the actual population mean, the proportion in the population, and how many
tend to be far off.
In the case of taking a sample of size 100, 95% of the samples of size 100, if the true
voting population is 60% for candidate A, 95% of the time that you take a sample of 100,
the proportion will lie between 50 and 70, and that's what we can prove using an investigation
into probability.
That's the kind of issue that we're getting to, so you can see that this is an example
of where we're trying to answer the question of how close and how confident.
The answer in the case of this population poll is that if we take 100 people from a
population of a very large population, like 100 million, if you take 100 people at random,
if the truth is that 60% are for candidate A, 95% of the time that you choose those 100
people, you're going to get an answer between 50 and 70%.
So that's how confident we are that we're that close.
How close?
10%?
How confident?
95%.
Okay.
This is a tricky notion by the way, and I want to make sure that you understand that
we'll be discussing this in detail in lectures 10 and 11, that it really is a rather subtle
thing that we're doing both how close and how confident, and figuring out why did I
pick 10 rather than 5, and all of that is a result of a study of probability of samples.
But one thing I do want to say, just as sort of a surprising thing to realize, is that
if you take a sample of 1,200 random people from a population, even a population of 100
million, if you take 1,200 random people, you get a very good estimate of the proportion
of people who are, say, for a given candidate, specifically, you can be 95% sure that you're
within 3 percentage points either way.
So that's rather interesting.
For now, the important principle is that, one, randomness is involved in statistical
inference, and that the conclusion of a statistical inference is, the conclusion is not a certainty
about the whole population, but it's actually a pair of numbers, and the pair of numbers
tell us how close and how confident, how close to the true population value, the value of
our sample is likely to be, and how confident we are that we are that close.
And by the way, the same kind of reasoning applies when we're talking instead about proportions
of the population, or the center value like the mean of the heights.
The same kind of thing pertains when we're trying to talk about, say, how spread out
the population is, that we get an estimate from the sample, and we use that estimate
to tell us how good the sample is, how representative of the whole population the sample is.
Well, let's think of another kind of challenge of statistical inference.
Suppose somebody hands me a coin, like this coin here, and says, this coin is balanced
and it's a fair coin.
If you flip it, you're as likely to get heads as a tail.
How would you test such an assertion?
How would you test it?
Well, what you would do is you would conduct some experiments.
You'd flip the coin, you'd see whether it's heads or tails.
Then what would you do?
You'd flip it again, see whether it's heads or tails.
And you would continue to try the coin out and see whether or not you got a proportion
of heads and tails that seem to be about even.
This strategy is a strategy that I want to describe, and I'll tell you right now that
I'm very proud of this, so I want you to be impressed.
The way that you can tell whether a coin is unfair is like the American system of criminal
justice.
The way we do it for the coin is we're going to say, assume that the coin is fair, we do
some experiments and we see whether or not the experimental outcome would be a rare event
given that it's a fair coin.
Now, you may not have understood this, but think of the American justice system.
When the judge admonishes the jury about a defendant, what does the judge say?
The judge says, that defendant is innocent.
That's your presumption.
You presume the defendant is innocent, and then evidence is presented.
And the evidence is saying, well, that person may actually, you know, it looked like a witness
says, it looked like the person who was in the neighborhood of the crime.
This person was driving a car that looked like the car that was seen to be fleeing and
so on.
Evidence is adduced about the individual who's on trial.
If the jurors feel that the evidence is very unlikely given the innocence of the person,
then the jury finds that person guilty.
You follow me?
You assume innocent, you get evidence, and if the evidence is very rare given the presumption
of innocence, then you have to change your mind and say that that person is guilty.
So in the case of the coin flipping, if you had a coin that was not fair, suppose that
in fact this coin was not fair, it wasn't a balanced coin, it wasn't a fair coin, that
when you flipped it, 70% of the time it came up heads, then what you would do is you'd
presume it innocent.
That is, you'd presume that it had an even chance of coming out heads or tails.
And then you'd flip the coin a lot of times, and if you discovered that it came out maybe
65% of the time heads, then you would say to yourself, well, that is a very unusual
thing to have happened if it were in fact a fair coin.
And so the presumption of innocence of this coin is the evidence against it is overwhelming
and you would change your mind and say, oh no, it's not a fair coin.
So that is the strategy of statistical inference, and let me just show you a slide to illustrate
it of how many times you might flip a coin to see how close the values of 100 flips tend
to be for a fair coin.
And so this is just an example of 10 flips of a coin, and you can see that these flips
are, do in fact, tend to cluster around 50.
They do go down in this case from 43 up to what's the highest number, 64.
So 64 is actually pretty far out.
I'm surprised that there would be such a one.
But that could happen, by the way, even rare times you might flip a completely fair coin
and have it happen that 64 out of 100 times, it came up heads.
Because each time you flip it, it has an even chance, and occasionally things will happen
where it doesn't come out even.
Let me just give you an example where the same strategy of statistical inference comes
in commonly, and that has to do with the testing of medications.
If you want to test whether or not a medicine is efficacious, does it actually cure the
disease that it's intended to cure?
What do you do?
Well, you take a bunch of people who have the disease.
You give half of them the medication, and you give half of them a placebo.
And then you see who gets better.
And of course, first of all, they all get better, because we know that placebos work.
You know, placebo, often people, you just give them a placebo, and they feel much better
soon afterwards.
That's why in medical tests, it's not good enough to just give the medication, and not
give anything else, because it could be that if you give somebody absolutely nothing that
has any actual medical value, it still has a positive effect.
So instead, what we need to do is you give medicine to some people, placebo to others,
and you see whether there's a difference in the number of people who get cured taking
the medicine compared to those who take the placebo.
So in this case, it's one more wrinkle of subtlety to tell the difference.
You have to compare two different changing values and see if there's a significant difference.
How do you tell if it's significant?
You have to consult probability and say, if 40% of the people are getting better just
with the placebo, what's the chance that 60% of the people got better taking the medication?
Is that within just random luck that that could happen, or does it illustrate that the
medicine is actually good?
Well, I want to do one more example before the end of this lecture, because I think it's
fun.
And that is this.
Those of you who have children or dogs, or both, probably have in your household a drawer
that contains decks of cards.
Now I call it a deck of cards, but if you have a collection of cards like this one that
has a rubber band around it, do you have any of those like this with a rubber band?
That means that these cards were collected probably from the floor when a bunch of kids
were playing with them, and you picked them up and you thought it was the whole deck,
but who knows whether the dog went off with some of them and a few cards from other decks
came in there, you don't know.
So what we have here is a deck of cards, well, a deck of something.
So how can you tell whether this is actually a real deck of cards?
Well the natural way, of course, is that you turn them over, you look at them, you sort
them, and you see whether any are missing or whether you have too many of something.
But here's a method that you probably haven't thought of.
What you could do is you could randomly choose a card, seven of diamonds, and then put it
back, shuffle the deck, and randomly choose another card, seven of hearts.
Put it in, randomly choose another one, six of clubs, put it in.
You could do that, you could do that, say, 3,000 times.
Now suppose you did that 3,000 times, you never looked at the cards, you never looked
at the back of the cards except one at a time.
Notice something, suppose you made a histogram of your answers.
So I happen to have one here, here's a histogram of the cards, and instead of listing all the
cards I just listed the spades.
Sometimes you get spades, sometimes you get the other things.
Now if you did this and you found this histogram, you would notice that roughly a lot of the
cards have about 50 occurrences of being chosen at random.
Look at this three, the three of spades was never chosen at random.
What that means is this is very good evidence that the three of spades is not in this deck.
Because you're expecting it to occur 50 times by randomness alone, and if it occurs never,
that's good evidence that it's not there.
Same thing with the queen of spades, and that's natural because some card games the queen
of spades is bad, so probably the dog ate that one when somebody was playing and handed
it to the dog instead of having to confess you had the queen of spades.
Now look at this 10, you see how it's about twice as tall?
That's evidence that there are two tens of spades in that deck.
And look at this one over here, you see this?
This is four times as tall as what you would expect on average if there were only one ace
of spades.
This indicates that in fact there are since there are over 200, and there should be about
50 if it were evenly distributed.
It means that there are four aces of spades in the deck, and that's probably right because
having an ace of spades is a good thing, so probably people have added other aces of spades.
So this is an example of when you take large statistical samples, you get information.
So the logic of statistical inference is always to compare data that we collect to our expectations
about what the data would be like if the world were random with respect to whatever the property
is, like choosing people at random.
And so the analysis of randomness and probability is what allow us to quantify our confidence
in our extrapolations from some of the data what the whole population looks like.
Randomness and probability are the cornerstones of all methods of statistical inference.
So here we've introduced some of the really rather subtle logic by which statistical inference
flows and we'll be looking forward to amplifying these and making them quantifying them in
the future lectures.
Thank you.
