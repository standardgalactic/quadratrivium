Welcome back. The basic goal of probability is to describe what it is that we should expect
from randomness. And so in this lecture, we're going to try to undertake an understanding
in some detail of the nature of random processes. The first thing about randomness is that clear
order actually arises from random processes, particularly if we do them enough times. Randomness
refers to situations where we don't know what any individual result will be, but we do have a sense
of what will happen in the aggregate, or that is to say if the experiment or the trial is repeated
over and over again. This is captured in a theorem called the law of large numbers,
which says the following thing, that if we conduct an experiment and think of it as rolling a die,
or dealing cards, and there are many different possible outcomes that there could be, and we're
asking, we're inquiring about the probability of a particular outcome or event, then it turns out
that if we do this over and over again, the fraction of the trials that exhibit that event
outcome that we're looking for, divided by the total number of trials that we try, will get
closer and closer to the probability of that event. Now this is really a concept that is
what you would guess intuitively. There's no surprise in the law of large numbers,
except that maybe that it actually works. So let's go ahead and look at some examples here.
These were done by simulations of rolling a die. Now suppose that we roll a die and we ask ourselves,
what's the probability of getting a three? Oh look, I got a three that first time there.
Now what we did is we simulated on the computer rolling a die many, many, many times to see
how many threes we got, and to try to demonstrate this law of large numbers, which tells us that
the fraction of threes that we get over many, many repeated trials, that fraction divided by the
total number of trials we have, the number of threes divided by the total number of trials,
will become increasingly close to the probability. Now we know what the probability is of getting a
three. It's one out of six, or 0.1667, a 666666 forever actually, which we round to 1667.
So we did this experiment several times of rolling the die six times, where we got no threes in
this experiment, 60 times. You see we expect on average to get about 10 out of 60. In fact,
we got 9, and its ratio then was 0.15, that is 9 divided by 60, is 0.15. As we repeated the experiment
more times, 600 rolls, we got 92 for a fraction of 0.1533, 6,000 times, we expected 1,000, in fact
got 997, very close, 0.1662, 60,000, we expected to get 10,000 heads, in fact we got 10,037 threes,
not heads, threes, and so we got 0.1673. You can see that this last number here, after we've
repeated the experiment 60,000 times, that the fraction of times we get a three is getting very
close to the actual probability. We repeated the same experiment of rolling the die 60,000 times,
and you can see in each case down here, the fraction that we get of the actual outcome
approached very close to the expected probability of 0.1667. That is the law of large numbers.
In effect, but let me tell you that the law of large numbers works even when we're talking about
relatively rare events, and so let me explain that by giving you an example of a relatively rare event
and seeing how the law of large numbers will apply. So suppose that we take three decks of cards,
and now we just pick a card randomly from each of these three decks.
Well, we can ask ourselves the question, what is the probability if we pick one card from each deck
that we will pick precisely the same card at random from each of those three decks?
Well, it's really quite rare, quite rare, and we can be specific about how rare it is. Namely,
let's do an analysis to see what the probability is of picking three cards that are the same.
Well, when we pick a card from the first deck, it doesn't matter what it is,
because we have to match it by the card in the second deck. What's the probability that the
card from the second deck will match? Well, it's one out of 52, and what's the probability that
the card in the third deck will match that same thing, one out of 52 again? So the probability
of all of them being the same, which they weren't of course in this case, but the probability of
they're being the same is just one out of 52, that's the probability that the second deck will
give a card that matches the first times, of the times that the second deck matches, only one out
of 52 times will the card from the third deck also match. So the probability is the product of one
out of 52 times one out of 52, which is one out of 2,704, or 0.00037. That is 37 out of 100,000
times we would expect to get a match. Quite rare, quite rare. Well, we actually undertook some
simulations to see if we could actually see that the law of large numbers works even for very
unlikely things, like picking three cards that are exactly the same. So here we go. We did a
simulation of, first we drew just 2,704 times. We simulated drawing these cards, and none of them
produced a match. By the way, we picked 2,704 because the probability tells us that one out of
2,704 times we should get the same. So then we tried 27,000, we got 12, our expected number was 10,
and you can see that the ratio of the number of successes that is all three the same,
divided by the number of trials, in this case 27,040, 12 divided by 27,040 is 0.00044.
And the actual probability, as we see in this column repeated, is 0.00037. Well, when we did it 270,400
times, we got a ratio of 0.00030. It's getting close. When we did it more than 2,704,000 times,
we expected to get about 1,000. In fact, we got 1,037 times that they matched exactly,
which had a ratio of 0.00038, which you see just differs in that last decimal point from
the actual probability that is expected. So the law of large numbers is in operation,
even when the event is rather rare, such as getting three cards that are exactly the same.
This lecture is about the unexpected reality of what to expect from randomness, and there are
counterintuitive aspects of what is produced by randomness. And I love these. I love the idea
that randomness produces things that are sort of surprising to us. The next one I'm going to give
is a visual example. Suppose that we undertake the following experiment. We just take a square,
and in the square we pick some dots at random. That is, by picking them at random, I mean that
we pick a place on the horizontal axis at random and the vertical axis at random. We just pick
a number and then put a dot there. And we do that. In this case, we did it 12 times and drew 12 dots.
And what we did is we undertook that experiment of doing it several times. And so let me just show
you what we got from just randomly picking 12 dots in a square. Now look at all these things.
Look at that. Look at that. The thing that a lot of people find surprising about these 12 random
dots in a square is that they expect something that's a little bit different. These were all
produced just by random process, randomly choosing the location of the dots. Look at that. Isn't
that surprising? See, look at that. That's like a question mark pattern. In contrast,
here are some non-randomly positioned collections of dots on the page. Now, the reason I draw these
is that if I ask you to take out a piece of paper and randomly draw 12 dots on the page,
I think you'd more apt to be drawing something like this. You'd be more apt to draw the dots
so that they sort of evenly are spread out over the whole square than was the case in the actual
randomly collected dots. And that's what is surprising about randomness, that it produces
the unexpected, these big gaps in the clusters and things that we saw in the randomly generated
positions. Well, of course, and by the way, in these randomly generated positions, we can find
all sorts of patterns that seem to just come by a miracle. And one place to look at this
is in the night sky. When we look at the night sky, the night sky is sort of a random collection of
dots. And so that's sort of a fun thing to look at. And if we look at the night sky, of course,
by the way, it's not random when we look at the Milky Way because the Milky Way is not evenly
likely to see a star that's out of the plane of the Milky Way. But if we look in some other
direction, it's rather a random collection of dots. And sure enough, from ancient times, people
have found patterns among these stars that are randomly put there, namely, all the constellations.
So this is an example of a way in which we see patterns in randomness.
Okay, so let's move on to another example. And this is, so we've talked about a visual
example of randomness, the dots on a page. Let's talk about flipping a coin. If we take a coin
and we flip it, we take a coin and we flip it. We flip it many times. And we record whether it's
the heads or tails. And then we flip it again. We see whether it's heads or tails. And we record
the flips that we make. Let's suppose we do this 200 times. And we just write down what we get.
And here is what it is. You don't have to look too carefully at this. But this is the
actual result of flipping a coin 200 times at random and actually was simulated. And then
putting down H's for the heads and T's for the tails. Now, suppose that I ask a human being
to write down a random list of 200 H's and T's and just write it down. And I often do this when
I'm giving talks to people to say this. I say, okay, I want one group to actually flip a coin
and another group to just write down what you think is a random sequence of H's and T's.
Write anything down. And then I have them put it on the blackboard or display it. And I come in
and I'll tell them which one was done by a human and which one was done by the coin. And this is
really fun because they say, how can you possibly know that it's all random? Well, let me just
demonstrate it in this example. So here are 200 random H's and T's and here are human-generated
H's and T's. Now, of course, I know it just looks like an ocean of H's and T's to you.
But what I look for is the following. I look for strings of long sequences where they're all H's
in a row or all T's in a row. Here we go. In the 200 randomly-generated H's and T's, you can see
that I've colored sequences of H's here and T's that were at least four or five long. And look
at this long sequence of H's. See that long thing? That was all by randomness alone. Look,
six T's in a row here, five T's here, here are six T's in a row. A lot of streaks of many things
in a row. Now, a human being, how often will a human being write more than four strings of the
same letter in a row when they're trying to be random? Well, they're sort of resistant because
they don't think that's very random. They think you've got to sort of alternate H, T, H, T. And so
here, in this human-generated one, you can see there are very few strings of H's and T's in a row.
Well, as a matter of fact, the probability when you flip a coin 200 times, the probability of
having at least one string of six or longer of H's or T's is like 96%, very likely. And the
probability of having at least one string of five is 99.9%. It's essentially certain. You'd be
very unlikely to flip a coin that many times without getting these long strings. And in fact,
if you actually simulate this on the computer, you'll see that that plays out, that you just
almost always get these strings. Well, we're looking then for counter-intuitive features
that are presented by randomness. And this next one is one of the common misconceptions that a lot
of people have about randomness. And that is that suppose that you're flipping a coin again. This
time, we're going to flip a coin. And we'll flip a coin many times. And suppose that just randomly,
it happened to come out, that 10 times in a row, you got heads. 10 times in a row, you got heads.
Don't you sort of feel in your heart that that coin is wanting to make a tails next time? Doesn't
it feel like the next time is more apt to be a tails? And the answer is, of course, that the coin
doesn't know what it's just done. To the coin, every flip is a new flip. And it's just as likely to
be a heads as a tails after it's done 10 heads in a row, as it was to get a heads than a tails if
it had done none of them. Now, so let me show you what we did to demonstrate this. What we did
is we simulated the following experiment. And that is we took a coin and over a million times,
a million times, what we did is we flipped the coin 11 times. You follow me? In other words,
we took a coin and with the computer, computers are great by the way. They don't care. A million
times, I'll just go ahead and do it. Okay, so you do it a million times. And we said,
what are you getting that wrote down this string, HT, TT, HT and so on, 11 in a row.
And then we did it 11 more times 11 more. And we did that 11s, 1,024,000 times. And the reason
we picked 1,024,000 is because every 1024 times, that's the probability of getting 10 heads in a
row. That'll happen on average one out of 1,024 times. So in other words, if you do the experiment
of flipping the coin 11, 1,024,000 times, and each time you flip it 11 times, you expect that the
first 10 will all be heads about 1,000 times. About 1,000 times. Well, we did that experiment,
that the experiment of flipping 1,024,000 times 11 times each one. And what did we get? Well,
the number of times we got 10 heads in the first simulation was 1,008. We expected 1,008,
extremely close. What happened to the 11th coin? Well, 521 times, it turned out to be a head also.
And 487 times, it turned out to be a tail. You see, there's no memory about half the time heads,
half the time tails. We did it again, 983 times, it came out heads, the first 10 times in a row were
heads. And then the next time, 473 heads, 510 tails. And third experiment, 1,031 times, it came
out heads 10 times in a row. And of those, 502 had the next coin be a heads and 529 tails. So you
can see that the coin has no memory. After it's gotten 10 heads in a row, it's just as likely to
be heads in the next time as it was the first time you flipped that coin. Okay. Now, I wanted to talk
about some, another counterintuitive aspect of probability. And that has to do with, and this
is, it's really interesting to think about what is rare and how we view rarity in probability.
Suppose that somebody were, you were delta the following hand, the two of spades, the nine of
spades, the jack of clubs, the eight of spades and the five of hearts. Well, you could look at that
hand and say, wow, the two of spades, the nine of spades, the jack of clubs, eight of spades, five of
hearts. I'm going to write home to my mother and to my, my friends and relatives and say, I got the
most amazing hand today. It was the two of spades, the nine of spades, the jack of clubs, the eight
of spades and the five of hearts. That's amazing. The chance of getting that particular hand, you
know what the probability of getting that hand is? One out of 2,598,960. That's the probability
of getting that hand. Now, here's another hand. Here's another hand. This is the ace, king, queen,
jack, ten of spades. This is a royal flush in spades. What's the probability of getting this
royal flush in spades? Exactly the same. One out of 2,598,960 and yet you would write home to your
mother about this hand for sure and this one, you wouldn't write to anybody. This is just an average
hand and yet in your whole life of playing cards, you know what? You will probably never get that
hand again because its probability is almost one out of 2,598,960. So this is one of the
counter-intuitive concepts of probability that rare events happen all the time but you may not
recognize them as of significance. Okay, now let's look at some other rare events. Rare events
absolutely happen by chance alone. Here, the most common rare event that you see mentioned in the
newspapers every day is the lottery. The probability of winning the Powerball lottery is about one
out of 146 million. This is the big multi-state lottery in some states. One out of 146 million.
That chance is so remote you'd think it would never happen but it happens regularly. Why?
Because a lot of people try. A lot of people buy random numbers and some of them then occasionally
win. If you try something that's rare often enough then it will actually come to pass.
This concept of that rare things will actually happen if you repeat them enough and you look for
them enough was encapsulated in an observation that was first made by the astronomer Sir Arthur
Eddington in 1929 and he was describing some features of the second law of thermodynamics
and he wrote the following. He said, if I let my fingers wander idly over the keys of a typewriter
it might happen that my screed made an intelligible sentence. If an army of monkeys were strumming on
typewriters they might write all the books in the British Museum. The chance of their doing so is
decidedly more favorable than the chance of the molecules returning to one half of the vessel.
Talking about the second law of thermodynamics. Well, so this brought up the question of if you
let monkeys randomly type on a typewriter they will eventually write the entire script of Hamlet.
You know they'll get it exactly right. So Hamlet happens. This is this is the thing. Now actually
people have have made some so I wanted to talk about these monkeys typing typewriter on the on
the typewriter. So it is true. You see the reason that it's that it is certain to happen if you let
those monkeys type long enough is the following. There's some chance they'll get the first letter
right and then having gotten the first letter right there's some chance they'll get the second
letter right. And then there's some chance getting the third letter right so the probability of getting
all those right is well if one out of a hundred times they get the first one right then of that
one out of a 100, one out of a hundred times they'll get the second one right and one out of a
hundred they'll get the third one right and so on, and so there is some non-zero chance that
they'll get it right every single time and if they're going forever well forever is a long
long time, and they will eventually get it right.
But I thought it was sort of amusing to see how much you could really expect to get.
Suppose that you tried to type to be or not to be.
That has 18 characters.
And suppose you have a keyboard that has 100 keys.
So there are 100 to the 18th, 100 times 100 times 100, 18 times different 18 character
patterns that you could write.
That is 10 with 36, 1 with 36 zeros after it.
That's the number of different patterns.
So here's the question.
Suppose that you tried the following thing.
That at the moment of the Big Bang, you started trying to type to be or not to be by randomness.
And you type these 18 characters, you tried 18 characters, and then you'd say, is it right
or not?
And then you try another 18 characters and so on.
And you did a billion of those per second, a billion trials per second from the beginning
of the Big Bang till now.
What would be the probability that you would have typed just the phrase to be or not to
be?
Answer?
Your probability is about one in a billion that you would have even got to be or not
to be by this time, much less the complete works of Shakespeare.
So I thought it was sort of amusing to see what the actual answer is.
However, you can find patterns in random writing.
And in fact, this happened and a person made a lot of money by this.
An enterprising author made a lot of money when he wrote the Bible code a few years ago.
You may remember the Bible code.
And what he did is they took the Bible written in Hebrew and they found patterns of words
by taking certain, skipping a certain number of letters and then finding that by skipping
a certain number of letters in that pattern of skips, they would find words written out.
One of them was like Atomic Holocaust Japan 1945.
They found this somewhere in the, by taking letters that were spaced out evenly in the
Bible.
And they said, so this is an example of showing how the Bible was showing the future.
Well, of course, of course, the truth is that this is just a matter of probability.
If you take all possible sequences of different lengths, you can find, you can by randomness
alone find surprising things.
And here, just to demonstrate it, people found, in debunking this analysis, they found patterns
in war and peace and so on.
Since this is a mathematics class, they also found in a very famous mathematician, namely
the unabomber Ted Kaczynski.
You may remember him.
He mailed bombs to people and he wrote a manifesto.
And so they used the strategy of the Bible code to find things in the manifesto of Ted
Kaczynski.
And here's what they found.
They found, but they took the letters and of the manifesto, putting them together and
they used the same methods that were used in the Bible code analysis, which involved,
by the way, they would string the letters together, but then they would just cut out
little squares from it.
You see, so it gave them a lot more flexibility of finding patterns.
And in this unabomber writing, this part of it, look what you find.
Bomb mail in this nice organized pattern, just exactly like they found in the Bible
code kind of analysis.
So this is another challenging part of probability.
And that is that if you look for rare things, but you have a lot of places to look, you'll
tend to find them.
There are many challenges, I think, to randomness.
One of them is that looking at things in retrospect, you look back at things that appear to be
random and yet you can explain them in retrospect.
For example, stock movements.
This is something we see all the time, that there are lots of people who can tell us exactly
why the stock market moved in exactly the way it did from last week to this week or from
yesterday to today.
There's no end to that.
And then you wonder, well, geez, why don't they just tell me what's going to happen tomorrow
and then we can all be rich, you see?
And they don't do that because you can explain randomness by, you can find out the actual
things that happen to cause things that are better described randomly.
Even rare events happen randomly, as we've seen before.
And in fact, in the case of the stock market, of psychics and others of that ilk can often
be correct in their prediction of stock market predictions because if there are enough psychics,
they'll guess all sorts of things and randomly some of them will be correct.
Well, it's not always easy to detect what the difference is between something that was
created by a random process versus something that is more determined by some process.
And so one of the challenges is how can we tell the difference?
For example, if we had a sequence of H's and T's as we did before, how could we tell
whether or not that was created by a random process or by a non-random process?
Well, we already talked about the fact that if you flipped a lot of coins, you would expect
to have a certain number of sequences that were a certain length.
And in fact, that kind of analysis of saying, well, first of all, you expect about half
heads and about half tails.
And the law of large numbers tells us that as we take more and more flips of the coin,
we would expect that fraction of heads to tails to become closer and closer to even.
The fraction becomes closer to 50%.
And then you can be more refined and say, well, what number of H's is in a row would
you expect?
What fraction of those would you find in the whole pattern?
What fractions of T's, what fractions of 4H's in a row and so on?
Each one, you can compute what the probability is, and then you can look at your list of
H's and T's, see whether or not the fraction is somewhat what would be predicted by randomness,
and then that would give you evidence that the pattern is or is not generated by a random
process.
But there are some philosophical challenges here.
And I'm going to leave you with one of the ones that I think is very difficult to think
about, and namely, the digits of the number pi.
Pi, as you know, is the ratio of the diameter of a circle to its circumference.
That is, the circumference over the diameter is exactly the number pi, and the number pi
is 3.1415926 and so on.
And it goes on forever.
It has infinitely many digits, if you got it exactly right, and it's been computed to
billions and billions of digits.
There's nothing random about the digits of pi.
It's absolutely determined, every single digit.
But yet, if you look at the digits of pi and you try to analyze them from the point of
view of randomness, you find that it has a lot of the characteristics of randomness.
Here, for example, is a chart that shows us the distribution in the first 10,000 digits
of pi of how many times there is a 0, a 1, a 2, a 3, or 4 among the digits of pi.
And you can see that it's a very even kind of distribution, as you would expect from
a random list of numbers, just randomly choosing digits from 0 to 9 and putting them in order.
And likewise, if you look at other patterns, how many groupings of numbers that appear
together and so on, pi satisfies a lot of those conditions.
Well, the challenge of looking at and asking, what is random in the world?
Is it random that a basketball player gets a certain number of free throws in a row,
or is there some underlying cause?
These kinds of questions are a real challenge to us.
And when we think about analyzing the world, which things we want to describe as random,
and which things we want to describe as deterministic, that has caused an effect of something that
doesn't have randomness in it, those kinds of issues present us with a real philosophical
challenge.
In the next lecture, we're going to be talking about the concept of expected value, which
is a way of taking probabilistic situations and giving a measurement to the value of different
outcomes in order to make good decisions.
I look forward to seeing you then.
