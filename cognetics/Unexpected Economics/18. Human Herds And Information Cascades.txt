For just about all of the 20th century, doctors were certain they knew the causes of stomach
ulcers.
It was stress, maybe combined with a nervous temperament along with consuming things like
spicy foods or alcohol.
But in 1982, two researchers named Barry Marshall and Robin Warren found evidence that most
peptic ulcers are actually caused by a bacterial infection.
The bacteria they found had not previously been known.
Their evidence was ignored and even mocked for a time, but it changed how we think about
ulcers.
Instead of being a chronic condition where only diet can fix it, ulcers are an infection
that can be treated with antibiotics and acid suppressors.
And in 2005, the two doctors received the Nobel Prize in Medicine for this work.
Here's an obvious question, not just here but on many topics.
How is it possible that so many experts were so wrong for so long?
Marshall and Warren didn't use any incredible new scientific techniques in studying ulcers.
They looked at biopsies, they noticed the inflammation, they noticed the presence of
this bacteria, and they managed to cultivate it and study it.
Described that way, it sort of sounds like something that should have been discovered
about 1950, not in the 1980s.
The answer, of course, is that conventional wisdom, whether on ulcers or anything else,
is often adopted without reconsidering the evidence as it arises.
It can turn out that the conventional wisdom, this thing that everyone knows, is actually
something that only a small number of people have actually studied and considered, and
everyone else has just accepted their authority and their conclusions.
This reason to be uncertain about the conventional wisdom has been known for quite a while.
For example, the great philosopher Arthur Schopenhauer wrote back in the 19th century
on the subject of universal opinion.
So-called universal opinion is the opinion of two or three persons, and we should be
persuaded of this if we could see the way in which it really arises.
We should find it is two or three persons who, in the first instance, accepted it or
advanced it and maintained it, and of whom people were so good as to believe they had
thoroughly tested it.
Then a few other persons, persuaded beforehand, the first were men of the requisite capacity,
also accepted the opinion.
These again were trusted by many others, whose laziness suggested to them it was better to
believe at once than to go through the troublesome task of testing the matter for themselves.
Thus the number of these lazy and credulous adherents grew from day to day.
It's no more established than a historical fact reported by a hundred chroniclers who
can be proved to have plagiarized it from one another, the opinion in the end being
traceable to a single individual.
It seems to me that while Schopenhauer is maybe exaggerating just a bit for effect, there's
some truth in this.
There are things in economics I think I know with a fairly high degree of certainty because
I've spent the time to study the data and the theory and the competing views and I've
reached some sense of it all.
But if you ask me about, oh I don't know, appropriate diplomatic policy toward North
Korea or Venezuela, or the chance that an innovation will dramatically increase the power
of batteries, or how future weather patterns might be affected by climate change or sunspots,
what you get from me is the opinions of other experts that I happen to read, and for all
I know, they may just be listening to a few other experts too.
There's a conceptual trap for economics here.
Economics is based on the notion of people making decisions in a situation of scarcity
and after considering all the possible trade-offs.
However, the task of gathering information and forming beliefs also takes time and energy,
and it's impossible for any human being to investigate every topic thoroughly.
So in a world where time and knowledge are scarce, many of us, at least some of the time,
will make our decisions by following conventional wisdom.
For economists and really for all social scientists, it's important to try to sort out when people
follow the crowd and when they might not.
The idea that we follow conventional wisdom because others before us have followed conventional
wisdom goes under various names.
Sometimes it's called herding behavior, running with the herd, and information cascades.
We'll talk about these terms as we go along, but the key point is that a lot of the time
conventional wisdom is correct.
I mean, after all, that's why it makes some sense to follow it.
But in some cases, conventional wisdom can be incorrect, and the results can be dismal
or even tragic.
As a tragic example, I think of the thalidomide disaster of the early 1960s.
Thalidomide went on the market in 1957, and it was widely prescribed to pregnant women
for morning sickness.
In 1961, however, it was identified as the cause of an epidemic of severe birth defects,
such as missing arms and legs.
Now, I was born in 1960.
I'm glad my mom wasn't subject to morning sickness, but I really couldn't blame the parents
who took it.
I can't even blame the doctors who prescribed thalidomide.
After all, a doctor can't possibly retest every drug that's approved all over again.
More recently, the idea of many people acting like a herd and following conventional wisdom
about, say, buying houses can help to explain part of what happened to the U.S. economy during
the lead-up to the Great Recession of 2009.
But before getting to some applications of herding, let's first investigate just how
it works.
At some level, we all know that herding is possible.
If you're sitting in a crowded theater and many people around you start yelling fire
and running for the exits, it might make sense for you to head for the exits, too, even if
you don't actually see the fire.
But economists like to break down issues into a model of how the behavior happens.
These models are very useful for figuring out the specific conditions under which this
kind of herding behavior happens or doesn't happen, and what makes those outcomes more
or less likely.
Three economists should be mentioned here in particular.
One is Sushil Bikshandani, who is now at the University of California Los Angeles, David
Hirschleifer at the University of California Irvine, and Eva Welsh, who is now at Brown
University.
They wrote a series of papers in the early 1990s investigating the situation of information
cascades and herding.
Now their modeling involves a lot of mathematical equations, but there's one illustration of
the underlying principles I've used in a classroom a few times.
So for a moment, imagine you're sitting in one of my classes.
You're not right up front and not as far away as possible, but kind of a few seats back.
I take out two cups.
I call them cup A and cup B. In cup A, I put in two dark marbles and one light marble.
In cup B, I put in two light marbles and one dark marble.
Now I turn my back to the class so you can't see, and I pick either cup A or cup B. But
all you students out there, you don't know which one I've chosen.
The question for this exercise is students are going to try and figure out which cup
did I pick up.
Did I pick up cup A with more dark marbles or cup B with more light marbles?
You can think of this belief about cup A or cup B as a belief about what is true.
For example, cup A might be one way of thinking about the causes of ulcers or whether housing
prices will continue rising, and cup B might be another way of thinking about these issues.
So here's how the experiment works.
I walk to one student and I let that student choose one marble.
That student looks at the marble, but here's the key.
The student doesn't show that marble to anyone else.
Then the student announces a prediction of whether the marble was drawn from cup A or
cup B. Now this first student really has a pretty simple task.
If they see a dark marble, they predict it was more likely to come from cup A, which
had the two dark marbles to start with.
If they see a light marble, they predict it came from cup B, which had two light marbles
to start with.
So I write the student's prediction up on the board.
But again, this is not what they actually saw.
This is just the prediction that the student made.
Now I take that same cup, I go to the next student, and I repeat.
That is, the student draws a marble, doesn't show it to anyone, and announces their prediction,
whether they think I'm passing around cup A or cup B.
Now it's a little more complicated for this second student.
If that student draws the same thing as the first student, then they'll announce the same
choice.
That's pretty clear cut.
If they draw differently, then they can't be quite sure they could decide either way.
And now you get to the third person, and the potential for fun really starts here.
Let's say the first two people announced the same prediction.
Let's say they both said it was cup A, which has, remember, the two dark marbles.
If the third person draws a dark marble, then they just say cup A also.
But say the third person draws a light marble.
What does that person think?
What process do they go through?
It's quite possible the person will reason this way.
I think the previous two people have drawn dark marbles, so even if I drew a light one,
I should disregard what I drew, and I should predict that actually it's cup A.
In other words, the information from the conventional wisdom, what has already been announced, should
overrule the bit of evidence I'm seeing with my own two eyes.
And if the third student makes that choice, then the fourth student is in a situation
where everyone before them has predicted it was one thing, say cup A, and almost regardless
of what they draw, they will let the multiple previous statements override what they actually
see.
And information cascade has started.
Now, I want to back up a moment here.
I could go through all kinds of possibilities and permutations here and probably confuse
you and myself.
But I just want to be clear on a couple of other outcomes.
Let's say that I did choose cup A. That's the one I'm showing people with the two dark
marbles and one light marble.
The first person, however, draws a light marble just by chance.
After all, this happens one time out of three, and so they erroneously predict cup B.
Now, let's say the second person also gets a light marble and erroneously predicts cup
B. It's not their fault either.
After all, one-ninth of the time, that is one-third times one-third, both of the first
two people will draw a light marble even when I'm holding out cup A. So now, we can get
an information cascade that goes in the wrong direction.
There are other ways you can get such a cascade, too.
Maybe the first two choices are divided, the third person gets a certain draw, and that
can start an incorrect information cascade as well.
The key characteristics here are, observe a private signal, in this case a marble.
Make a public announcement.
Others can't see the actual information.
They only see what's announced, the conventional wisdom.
At that point, people have an incentive to disregard their own actual information because
all the public announcements of others, the conventional wisdom that's built up, makes
it appear that their own information, their own evidence, is the exceptional and unlikely
case.
At some point, the accumulation of social information is large enough that it outweighs
the information actually available to an individual.
And the key to an information cascade happens when at some point, people start disregarding
their own personal information and following the conventional wisdom instead.
That might limit this kind of an information cascade from forming.
There's some built-in fragility in an information cascade because, after all, at some point,
people in the cascade are aware that they and others are ignoring their own evidence
and information.
They're just going with the earlier predictions of others.
There can always be someone in the class who, after observing the cascade for a time, decides
just to make the other choice, just to see what happens, especially in the classroom,
the cost of announcing another choice isn't too high.
The cascade doesn't form if everyone can observe what is actually drawn, if they all can see
the actual evidence.
A cascade won't form if people can express their opinion and say, well, I'm going with
a previous group here, but I just want to announce that I did get the unexpected color
and maybe others should take that into account as well.
You need to have a situation where all people see is what's announced, not the evidence
itself.
It's also harder to form a cascade, turns out, if there's a wide array of choices, not
just, say, two possibilities, because, over time, a wide array of possibilities leaves
the possibility you can shift a little bit between one and another as the evidence evolves
over time.
You don't have to make just a right, white, or black choice.
In the real world, what can shatter a cascade can be the arrival of better informed individuals.
Imagine that housing prices are increasing because of a herd effect, or stock market
prices are increasing, and then somebody, famous in the markets, makes an announcement.
It could be Warren Buffett or Ben Bernanke or somebody like that.
That can break the cascade and cause it to go another direction.
In the real world, there can be new public information that comes out.
You don't just need to rely on other people's announcements of conventional wisdom.
And the risks may change.
Maybe for a time, following the cascade looks risky.
Another time, maybe not following the cascade looks risky.
With a small change in any of these factors, the entire group can rethink its position.
It's essentially like starting over again with a clean slate, maybe with a suspicion
that things aren't quite right.
That cascade will form more quickly if it costs a lot to get actual new evidence and
information on your own.
It'll form also if social information is relatively cheap, if it's easy to get social
information and much easier than getting your own private information.
It's possible to end up in a situation where instead of observing one person at a time or
a sequence of people when you know a cascade has been established, you can see the beliefs
of other groups of people not connected with your group.
If you can see the beliefs of others outside your group, then maybe you can draw a better
inference because you know those people weren't affected by the same cascade that you were.
And a lot of advertising strategy is actually based on this thought.
If you see an ad about a whole bunch of unrelated people all trying out a new product like a
new car, the implicit theme of that kind of an ad is these people aren't connected.
They're a random group.
They aren't choosing based on the decisions of others.
It's all independent people that really like this car.
Now, of course, a lot of time it's an ad.
They only chose people who liked the product or they chose actors.
So this claim isn't actually true, but it's appealing to that part of your brain that
knows you should be worried about information cascades.
As you listen to this discussion of how information cascades and herding work in theory, you probably
have some examples going through your mind.
Some common examples would be things like management fads or diet and exercise fads.
Everybody seems to believe these things for a time, not because of careful personal consideration
versus all the alternatives, but because everybody else believes them for a time.
For example, if you asked, what's the cutting edge way to manage a company or an organization?
Over the last few decades at various times, you would have gotten all sorts of different
answers.
Consistent by objective, the pursuit of excellence, employee empowerment, business process engineering,
core competencies, Six Sigma, the Japanese model, improved corporate governance.
Or you can imagine if you made a similar list, what's the diet you should follow?
It seems like one day it's no carbs, the next day it's all pasta.
It's oat bran or fish oil, low fat, no trans fat.
They're vegetarians and vegans and organic food people and people on all protein diets
and so on and so on and so on.
Here what I want to do is focus on some examples that maybe have a little more effect in the
day to day economy.
First example is financial markets.
In making predictions about the price of a stock or how much housing prices will rise,
there's a tendency to run with the herd.
My wife was for a time a stock market analyst at an investment bank, so I saw some of these
issues close up, but there's an academic literature here as well.
Think about the incentives.
If you're making predictions and you go with the crowd in your predictions, at the end
of the year you'll be about the average of the entire group.
Of course you might deviate now and then for sure, but you're not going to be that far
from the consensus.
And if you're not that far from the consensus, the risk of getting fired is not that hot.
You're not going to be way below everyone else.
Think about the pressures here.
Say that back in 2003 or so, you saw clearly that the housing market prices were going
up in an unsustainably fast way.
Moreover, say that even though the overall national average of housing prices hadn't
declined since the Great Depression, you knew there wasn't just going to be a drop,
there was going to be a crash.
You know that stock prices would fall, you knew that housing prices would fall, you knew
there would be a deep recession, and you saw all of this clearly in 2003.
And there were some people in this situation.
Now you invest on that basis, but in 2003 housing prices keep rising, stock prices stay
high and you lose a lot of money.
In 2004 you keep investing on this strategy and you keep losing money.
And the same in 2005, and the same in 2006, and the same in 2007.
Well by 2008 and 2009 you're proven right, hallelujah, except you've just lost tons
of money for five straight years.
In fact, you were probably fired after the first year or two of losing money.
And all around you over those five years, everyone was making tons of money by investing
in the other direction.
Among a line that became famous among financial economists was a comment from Charles Prince,
who was then the CEO of Citigroup in July 2007.
He said, when the music stops in terms of liquidity, things will be complicated.
But as long as the music is playing, you've got to get up and dance and we're still dancing.
Optimists about the beneficent forces of free markets point out that sooner or later, new
information will come out, the herd won't continue forever.
People will stop buying stocks at the high prices, people will stop buying houses at
the high prices.
But if you're in the world of finance, you can't bet this change will happen right away.
John Maynard Keynes once said, the market can stay irrational longer than you can stay
solvent.
And as long as that holds true, there are incentives for all of those making financial
predictions to run with the herd.
Mix metaphors as long as the music keeps playing.
A second example of herding behavior in quite a different area is geographic disparities
in the practice of health care.
I mentioned at the start of this lecture, cascades and medical procedures.
No doctor can study every possible procedure all the time.
And most doctors tend to get in a comfort zone of what they will do, which is confirmed
by other doctors around them who are in the same comfort zone.
As a result, even a fairly common condition like a stomach ulcer can be misunderstood
for decades.
Patterns of medical treatment and spending vary a great deal across geographic areas.
This has been observed for decades.
You look at one metropolitan area and you find that in that area doctors do two or
three times as many operations for coronary bypass or prostate cancer or caesarean sections
as they might in other areas.
This same patterns are followed with drugs, exercise, all kinds of different medical approaches.
A group of health care economists at Dartmouth University have been pointing out for years
that the amount spent on a Medicare patient in a high-cost state like Florida after adjusting
for age and illness is nearly double what's spent on a Medicare patient in a low-cost
state like my own Minnesota.
Now trust me, the good folks of Minnesota are not tossing old people out into the snow
to die.
It just is in Minnesota we don't believe that every health episode for an elderly person
requires a long hospital stay and the most interventionist high-tech treatment either.
In Florida, there's a different perspective.
But studies of these geographic differences typically show that although the practice
of medicine is very different in different cities and states, there's really little
or no effect on health outcomes.
Instead, these differences are best understood as local information cascades in which groups
of doctors in a certain area reinforce each other's decision-making.
This pattern suggests an important lesson for the ongoing debates about holding down
health care costs.
If you could break the information cascade, stop the herding behavior in high-cost areas
and apply the level of health care in low-cost areas would actually go a long, long way toward
fixing Medicare and toward fixing America's overall issues with high and rising health
care costs.
Of course, how you break a local cascade among health care providers is a difficult and controversial
subject.
A lot of consumer marketing is an effort to form an information cascade.
Why should you see that movie?
Everyone is talking about it.
While I watch that TV series, everyone is talking about it.
And of course, for lots of people keep talking about it, they'll keep watching at least for
a while.
Identifying fashion leaders or opinion leaders is also a matter of having a cascade in following
these folks.
A classic example is that in old-style communist countries, if a few people were standing together,
a line might form right behind them.
No one knew quite what was happening, at least not right away, but in a society with a shortage
of goods, if you see people standing in what might be a line, it's a good idea to get in
line first and ask questions later.
Follow the social information first, later on figure out if it applies to you personally.
There's a similar logic in a lot of political campaigns.
When you focus on the question of who can win or who is electable, what will the conventional
wisdom be?
You think about the early primaries in Iowa, New Hampshire and those early states and even
earlier in straw polls that happen.
Every candidate wants to create a sense that the herd is moving one way, so people will
disregard their own private preferences and get on board the train, go with the herd which
has a chance of winning.
And here's an interesting counter example of trying to block an information cascade.
In military trials in the U.S. Navy, the judges announce their votes in reverse order of seniority.
That is the least senior person announces first.
In a way, this reduces the fear of an information cascade where everyone just follows the most
senior highest ranking judge and follows their rankings the whole way down.
When firms make choices about products and investment, they often deliberately look
at what their competitor is doing and they deliberately form a herd.
One TV network has a popular reality show, others have reality shows.
One has a popular show about forensic investigation of crimes, the others start doing that.
There's similar logic in consumer goods as well.
Often you follow the popularity of others, especially smaller firms following larger
firms because they figure the larger firms know what people want.
If I was starting a fast food restaurant, one of the first things I might do is find
out if I can put it not too far from a McDonald's because I figure McDonald's thought about
a good location.
There's also evidence that investment plans of firms are strongly shaped by the investment
plans of other firms in the same industry.
For example, if there are a bunch of takeovers, that leads to more takeovers.
If some companies take on a lot of debt to buy some of their stock, you see other companies
doing the same thing.
There seems to be herding behavior in those sorts of decisions.
As a final example of herding behavior, let's talk about what happens under repressive governments
where many citizens will tend to hide their private reactions to that government and instead
go with the conventional wisdom or the crowd in professing that they too really support
this repressive government.
But circumstances may disrupt this information cascade.
There can be actions of a few individuals who decide to make their private feelings known.
There may be other information that comes out as well, but when a certain number of
citizens begin revealing their private information all at once, the result can be the overthrow
of a repressive government.
This kind of pattern helps to explain the overthrow of communist governments in Russia
and Eastern Europe in the 1990s.
It seems as if those governments were sort of here one day and gone the next.
A similar if not identical dynamic may apply as well to the demonstrations against autocratic
governments in the Middle East in 2011, in Tunisia, Egypt, Libya and other countries where
one day everyone was in favor of the government and the next day many, many people were not.
In a world of specialized knowledge and experts, none of us can know everything.
You'll often have a situation where you feel one way, but you know the conventional wisdom
leans the other way.
Always following your own information and ignoring the conventional wisdom is not smart.
Conventional wisdom is often correct, and if you ignore it, you're likely to end up
sicker, poorer and less happy as a result.
Just because it's conventional wisdom doesn't mean it's wrong.
The flashiest examples of herding behavior are when the herd heads off in the wrong direction,
but it's also quite possible and likely indeed a lot of the time to be herding in a good
direction.
However, recognizing the existence of herd behavior does have a couple of implications
for us all.
You want to know that reliance on the expertise of others creates a danger that conventional
wisdom might persist just because it's conventional, not because it's wisdom.
One implication is to value those who run against conventional wisdom.
Value the entrepreneur who starts a company that no one thought made sense.
Value the dissenters who've clearly spent a lot of time studying and thinking about
a subject, and maybe have some history of being right on at least some things.
Even value the cranks.
The sociologist Mark Granavetter has offered an interesting thought about what he calls
the strength of weak connections.
The first point is that we all operate in a social setting with lots of strong connections,
people we know well and talk to a lot, but for most of us, our strong connections are
also all talking to each other, as well as to us, so there's a risk of a herd mentality
forming within all of our strong connections.
When you recognize this risk, what becomes important is to think about our weaker connections,
people we perhaps don't know as well or communicate with as often, and are largely or totally
outside our usual strong network.
We should treasure those weak connections and keep up with them from time to time, because
those are the connections that are most likely to give us genuinely different perspectives
of information than our strong connections are.
We all run with the herd much of the time.
In a world where collecting information is costly, we really have no choice.
But remember that if choices are made provisionally and with an awareness of doubt and uncertainty,
that's exactly when herding is less of a danger, and ultimately, convergence to truth is more
likely.
So when you run with the herd, as we all do, at least do so skeptically.
