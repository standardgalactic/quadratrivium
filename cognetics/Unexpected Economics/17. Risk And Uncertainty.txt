Some choices are fairly straightforward.
If you're offered a choice between, say, two candy bars, or if you're offered a choice
between buying a blue shirt or a red and white striped shirt, or ordering out pizza or Chinese
to eat on a Friday night, in all of these kinds of choices, some people will prefer
one and some will prefer the other.
But the choice is clear, in the sense that you know what you would be getting with a
fairly high degree of certainty.
But what about if you have to make a choice in a situation where the outcome isn't certain?
It could be a trivial choice, like, should you take an umbrella with you on a cloudy
day?
It might rain, it might not.
How much should you worry about a nuclear power plant that produces electricity and
is located 50 miles from your house?
Should you invest your retirement money in stocks or bonds?
Stocks might go up more over time, but they also seem subject to severe drops every now
and then.
These are all choices with an element of risk and uncertainty.
These choices are more difficult because you don't know what you're going to get.
And no matter what your choice, there's a chance the outcome won't turn out the way
you would have preferred if you had known in advance what was going to happen.
I'm going to start this lecture by introducing some terminology that economists and statisticians
and actuaries use to think about the issue of risk.
I'll then discuss the very real possibility that many people are not good about thinking
about risk in a rational way.
And as a result, they may be making choices in situations of risk that they themselves
would view as misguided if they understood the situation more clearly.
Let's start with a simple example of risk and uncertainty, flipping a coin.
Let's say you want to bet on flipping a coin, and this is a fair coin, there's no tricks
or anything like that.
But there is risk, and a few terms are useful here.
These terms will be useful throughout the discussion.
If I bet on whether a coin comes up heads or tails, I'll win by random chance 50 percent
of the time.
The first concept here is the idea of actuarial fairness.
That is, this is not fairness in a moral sense or a sense of justice, but it's fairness
in the way that actuaries and economists use the word.
It means that if I bet over and over and over again on this same bet, on average, I will
break even.
So when it comes to flipping a coin, here's an actuarially fair bet.
Let's say I bet $1 on whether a coin comes up heads or tails.
If I guess right, I get $2.
If I guess wrong, I lose my money.
With a lot of tosses, this bet will, on average, break even over time.
This idea of actuarial fairness is basically an average value or an expected value of the
bet if it happened many, many times, and it's useful to know.
It's useful to know, for example, that, on average, life insurance is actuarially fair.
That is, on average, what a person puts into life insurance is what they get out of life
insurance after death.
This idea of actuarial fairness and the expected or average value of a risk can be used to
help classify attitudes about risk.
One term here is risk-neutral.
If you're risk-neutral, you would be indifferent between one choice of getting a dollar for
sure and having a 50-50 chance of getting either $2 or zero.
Notice the expected value of these two things is the same.
All that's different is that one is certain and one involves some degree of risk.
If you're risk-seeking, you would rather have a 50-50 chance of getting $2 or zero rather
than having one for sure.
Most gambling is an example of risk-seeking behavior.
You're giving up money for sure, and you're going to have either zero or a big gain, so
you're adding to your risk intentionally.
The third category, if you have risk-neutral and risk-seeking, the last category would
be risk-averse.
If you're risk-averse, you would rather get $1 for sure than having a 50-50 chance of
getting $2 or getting nothing.
In other words, you're giving up on the chance of something good happening, but you're also
avoiding the chance of a big negative outcome.
Anytime you can reduce the range of future possibilities and keep your expected value
pretty much the same, you're reducing the amount of risk that you face.
One of the famous economics papers about the economics of risk was done by the great economist
Milton Friedman and a famous colleague of his named Leonard Savage back in 1948.
They explored the question of why it is that some people seek risk by playing the lottery
or gambling.
After all, that's paying a sure amount or a certain amount with some risk of either
a big gain or a lot of loss.
And on the other hand, some people buy insurance to avoid risk.
When you avoid risk, you're paying some amount to reduce the chances of a large loss in the
future.
Friedman and Savage suggested that this pattern might be related to income.
They argued that for low-income people, playing the lottery might seem worth it.
For a chance at really high wealth, they might not feel they could get in any other way.
For higher-income people, buying insurance seemed worth it because it was a way of protecting
the wealth that they had.
And this explanation has some feeling that it's partly true, but it can't be completely
right.
After all, lots of high-income people do gamble, sometimes at quite high stakes.
And lots of low and middle-income people buy insurance.
So the answer certainly isn't a complete one.
But that argument, that paper, opened up an interesting idea, which has been pursued
ever since, that people might have inconsistent beliefs about risk.
My suspicion is that a better answer to their question of why do some gamble and why do some
buy insurance is related to the size of the losses in the gains.
It may be that we are risk-seeking if the cost is relatively small, but the gain is
potentially big.
Buying a lottery ticket would be that kind of situation.
But many of us are risk-averse when the cost is relatively small and the loss is potentially
big.
Life insurance would be a situation of that example.
But of course, there are exceptions to these rules too.
Different people have different attitudes about risk, just like they have different preferences
about everything, and they'll balance out these factors in different ways.
So to sum up the economist's view of risk here.
People look out at the probabilities of what might happen.
They can figure out, at least in some general way, over time with experience what the actuarially
fair outcome is.
They act risk-averse much of the time, especially when they can make small payments to avoid
large costs.
But sometimes people are willing to be risk-loving or risk-seeking if they can make relatively
small payments and perhaps get a very large gain, as in the case of gambling.
This work opened up a literature in thinking how people actually see risk.
Especially people who have what in the economics and psychology business we call bounded rationality.
That means that people don't actually have perfect information about probabilities.
They don't have a perfect ability to calculate.
And sometimes they take mental shortcuts that sometimes work and sometimes may not.
Let's think about ways in which people actually think about risk.
Some risks to many people seem especially salient, as the term that's used, meaning
especially clear or vivid in their mind, even though a glance at statistical evidence suggests
that they're not that huge compared to other comparable risks.
A common example is airline crashes.
There was a study some years ago that looked at the number of front page stories in the
New York Times about various causes of death, and it compared the number of news stories
to how many people actually died from these causes.
They found, for example, that on average, there were 138 front page stories about deaths
in airline crashes for every actual death that happened.
On the other side, there were two front page stories about homicides for every homicide
that happened, and there was about one twentieth of a front page story on average for every
death from cancer or suicide or automobile crashes.
After all, about 40,000 people die every year in the United States in car crashes, and maybe
200 people die in an average year from airplane crashes.
But air deaths seem a lot more salient somehow to many people.
People worry about airplane crashes in a way that they don't quite worry about driving
in a car.
This pattern of airline crashes is related to other illusions about risk.
For example, there's an illusion of control that often comes up in gambling situations.
If I personally roll the dice, then I'll do better because I'm in control.
If I drive, I'm in control and I'm safer.
If an airline pilot flies, I'm less safe because I have less control.
Now that control is of course not clear in a car or anywhere else.
A drunk driver can get you whether you're in control or not, and with the dice it makes
no sense at all.
In general, we also overreact to the risks of striking big happenings, and corresponding
we underreact to what seem like everyday events.
A common example here is shark attacks versus dog attacks.
There's a lot more media coverage and worry whenever there's a shark attack.
There are a lot more actual injuries and deaths from dog attacks.
When we obsess over airline safety, we create delays in checking in for flights, we do lots
of extra security, and that of course feeds the worries that some people have about air
travel.
It pushes some of those people to drive cars instead.
When air travel is reduced as a result, say after security tightens a lot or fears about
flying go up, you have people who decide to drive a few hundred miles rather than do
a flight.
But on average, when people decide not to fly and to drive instead, they're just more
likely to end up dead or injured, because driving per mile is a lot riskier than flying
in an airplane.
Let's consider a second way in which people don't see risk clearly.
This is sometimes called knighty and uncertainty after an economist named Frank Knight, who
was a prominent economist back at the University of Chicago in the 1920s and 1930s.
Frank Knight wrote a famous essay in 1921 that distinguished between what he called
risk, where the outcome is uncertain, but the probabilities of what might happen are
fairly well known.
And what Knight called uncertainty, which is what happens when the probabilities of what
might happen really aren't known either.
Uncertainty where the probabilities aren't known, where you really have no idea what
the different chances are of what might happen is known as knighty and uncertainty.
An example of how this works out in practice is called the Ellsberg paradox, and it's
named after Daniel Ellsberg, and yes, that's the same guy who in 1971 released the Pentagon
papers to the New York Times and other newspapers.
Right in 1969, Ellsberg managed to photocopy several thousand pages of classified documents
that he had access to.
It showed that what the military was saying to the public about the Vietnam War, which
at that time was kind of happy, smiley-faced stuff, was not remotely close to the pessimism
and bad news of the internal documents.
Here, however, what we're focused on is work Ellsberg did as a professional economist
and psychologist, which was presented in a 1961 article in the Quarterly Journal of
Economics.
And I'm going to simplify his example here just a little bit.
Let's say that you have an earn with 100 balls in it.
You know that 50 of them are white and 50 of them are black, and you're offered a chance
to make a favorable gamble.
Pay a dollar, draw something out.
If you get a white ball, you get paid 250.
If you get a black ball, you don't get paid anything.
Now over time, this is a 50-50 gamble, but it has a positive expected value.
It has positive, it's more than actuarially fair, and over time this bet would make you
money if you did it over and over and over.
Now, let's say that you have an earn, there's 100 balls in it, and some are black and some
are white, but you don't know how many there are of each one.
You don't know the probabilities.
Now you're offered a chance to make the same gamble.
That is, pay a dollar, draw out, if you get a white ball, you get paid $2.50, you get
a black ball, you don't get paid.
Do you view that as a good gamble?
Now the key point here is, if you really don't have any information about the probabilities
and could be anything, your best guess of the probabilities is that it's 50-50.
After all, it could be one, it could be the other, but if you don't know, 50-50 expresses
your ignorance.
So your best guess about the probability is the same as it was the first time.
In fact, we have a mild joke, a very mild joke about this in our home, when the weatherman
says there's a 50% chance of rain and a 50% chance of sun, we say this is basically confessing
total ignorance of what's going to happen that day.
But when people don't know the probabilities, it's very hard for them to feel as if the
probability really is 50-50.
People don't like making that gamble in a situation of nighty and uncertainty.
They don't like making decisions in uncertain situations when the probabilities aren't known
to them.
And when people don't know the probabilities, they really can react in a couple of ways.
One way is they can get all gung-ho and charge ahead even though they don't know what's
going to happen.
The famous economist John Maynard Keynes in the 1930s referred to this as animal spirits.
He said, if people starting a company, if entrepreneurs could really see what the probability
of success looked like, they would never go ahead.
It's only because they were ignorant of the probabilities they could be so optimistic.
Another possible option is that people might hunker down and avoid making a choice, just
because they don't want the probabilities to become clear.
In the worst of the recession, in late 2008 into 2009, even into 2010, my sense was a
lot of firms took this attitude about hiring and investment.
In a very uncertain situation where the probabilities weren't clear, they chose not to make choices,
not to hire or invest.
And of course, not making choices is a choice, perhaps not the best choice, but it's how
folks often react in a situation of nighty and uncertainty.
Here's another difficulty that most of us have about probabilities.
It's very hard to get a good intuitive feeling on how much to care about a low probability
event and to act on it.
How much should we prepare for, say, an earthquake the size of the one that caused the Japanese
tsunami of 2011, when you haven't had one that large for decades?
How much do you prepare for a financial crisis in recession, like 2007 to 2009, when you
really haven't had a financial crash that way since the 1930s?
How much do you prepare for the risk of, say, climate change, doing something really bad
to the world climate a few decades or a century down the road?
How much do you prepare for the risk of, say, an asteroid hitting the earth?
Sure, when a rare event is actually happening, it's easy to say, oh, they should have done
more.
But what about all the decades in between?
Missing Nicholas Taleb, a professor and practitioner of finance, has been arguing in recent years
for the importance of what he calls black swans.
A black swan is an event that is unlikely at any given time, but will occur from time
to time.
And he argues that financial markets, in particular, often don't take these risks of rare events
into account.
Even when an event is rare, it will occasionally hit.
And we always seem to be unprepared when that happens.
Another concern about risk is what happens when there are a series of uncertain events
and people are faced with a random pattern.
In that situation, people often have a tendency to try and impose a mental order on that pattern.
One common example is that many basketball players and people to watch basketball believe
that players get a hot hand.
That means they become more likely to make shots for a time.
But there's no statistical evidence to support this belief.
Say it's true a player makes about half of his or her shots, that player will sometimes
have a streak of 2 in a row or 3 in a row or 8 out of 10 or 15 out of 20.
But based on the probabilities, you can actually say how likely those streaks are to happen
just at random.
If there's a hot hand, it should be that good streaks and bad streaks happen more frequently
than the laws of chance would suggest.
There's actually no evidence of this.
People picking mutual funds also seem to believe in the hot hand.
That is, they look at who made money last year or the last five years, and they assume
it will continue into the future.
Again, if you assume returns on mutual funds have a certain distribution of probabilities,
you can say whether there are more hot streaks than random chance would suggest, and no,
they're not.
When housing prices were rising in the mid-2000s, this probably played in as well.
There was an assumption that there was a hot hand happening and one should go with it to
make money.
That didn't work out so well for a lot of people either.
Yet another factor is there's a difference between facing a risk once and facing a long
series of similar risks.
Consider the following bet.
You bet $1,000, and if you win, you get $200,000, but you only win one time out of every 50.
Is this a good bet?
First of all, it's important to be clear that this is an expected value, or actuarial
fairness, a really good bet.
Imagine you do this bet 100 times.
You would bet $100,000 total, and over that 100 bets, on average, you expect to win twice,
one out of every 50, so you would win the $200,000 prize twice for a total gain of $400,000.
If you can make this bet many times, not just 100 times, but 1,000 times or 10,000 times,
you're going to come out way, way, way ahead.
But that said, if I can only get to do this bet once or twice or three times, I'm not
going to be very eager to do it.
The chances are really good.
I'll lose a few thousand dollars before I win.
If I only get to make the bet a few times, it doesn't seem like a good deal.
But here's the key point.
Life is long.
Over my life, I may have many opportunities to spend $1,000 in a way that offers a potentially
large payoff, a small proportion of the time.
If I look at each of these bets individually, I will probably turn them down one after another
after another after another.
But if I look at it over my whole life, I should be willing to take all of these bets.
I know in advance that most of them won't work out, but the few that do work out will
have very high payoffs.
The difficulty is that many of us view our reactions to risk one at a time, not over
a lifetime of making such choices.
Most of us need to think a little bit more like venture capitalists.
They know that when they invest in startup companies, the chances are pretty good that
any given investment won't pan out.
The plan, though, is that if one of the investments pays off big, that will make up for all the
others that didn't quite make it through.
An analogy for a lot of retirement planning is that people often choose to put their retirement
money in extremely safe investments, like government bonds, because they see the stock
market jumping up and down in the short term and they don't want the risk.
Well, if you only have a few years to retirement, then it's like you only get to bet on the
stock market a few times over those years, and it makes sense to avoid the risk.
But if you're young and you think of making 40 or 50 annual bets in a row, one each year
until retirement on the stock market, then it looks like a better deal.
One fundamental question about how economists view risk is that even seemingly simple questions
about probabilities can have counterintuitive answers.
It might be that a lot of people just aren't good at figuring out questions that involve
probabilities.
As an example, let me tell you the birthday paradox.
Imagine that you have five people in a room.
The question is, what are the chances at least two of them will have a birthday in the same
month?
Many people would answer that question by saying the answer is 5 out of 12, or less
than half.
But this answer is actually wrong.
The easiest way to think about the right answer is to turn the question upside down.
Instead of asking, what are the chances that at least two people have a birthday in one
month, ask, what are the chances that all five people have birthdays in different months?
Which is the same question.
If you think about person one, they have a birthday in some month out of the year.
Person two has 11 chances out of 12, that they will have a birthday not the same as
person one.
Person three has 10 chances out of 12, that they won't have a birthday the same as the
first two.
Person four has nine chances out of 12, they won't have a birthday the same as the first
three.
Person five has eight chances out of 12, they will have a birthday that's not the same as
the first four.
So in order to calculate the probability, you need to multiply those fractions.
11 twelfths times 10 twelfths times 9 twelfths times 8 twelfths and if you do the multiplication
it turns out to be a 38 percent chance that none of the people have a birthday in the
same month.
Or to put it another way, if you have five people in a room there's about a 62 percent
chance that two of them will have a birthday in the same month.
Similar logic can be applied to the question of how many people do you need to have in
a room before it's likely that two of them will share the actual same calendar birthday.
I don't mean the same year but just one day out of 365.
It turns out that if you have about 23 people in a room there's better than a 50 percent
chance two of them share a birthday and if you have 60 people in a room there's 99 percent
chance that two of them have the same birthday.
It seems wildly counter-intuitive to me at first to think this way.
But I trust the actual math and in mulling it over there's a sort of intuitive explanation
here.
Imagine you had a wall that had all the dates of the year on it and you start throwing a
ball randomly at the wall and the ball randomly hits different dates.
The question is, how many times do you need to throw the ball until it randomly hits a
date you already hit?
As you mark off more and more dates on the wall it becomes more and more likely that
you're going to hit one you already hit.
When you get up to 30 for example if you get that far, one-twelfth of the wall is covered.
If you throw 12 more times the odds are fairly good, you're going to hit a covered spot one
of those times.
And 12 more times after that, 12 more times after that, it gets pretty likely you're going
to hit some part of the wall that's already been covered.
But there's no question that answers to probability questions like these feel really counter-intuitive.
So how can we have much confidence people can manage more complex probabilities like
the future of the stock market or nuclear power or the possibility of what happens with
certain medical treatments.
All probabilities need to be understood in a context making sure you have all the needed
information.
I've already mentioned a number of problems that can arise when probabilities aren't
well understood.
These issues with animal spirits, retirement planning, the fear of flying and others.
I want to finish this lecture with just a couple of examples from public policy when
risk isn't understood well.
When risk isn't understood, there are difficult questions for public policy.
Let's talk first of all about energy choices.
There are a variety of possible energy choices for the future.
There are fossil fuels like oil, natural gas and coal, there are hydroelectric dams, there's
nuclear power, there are renewable sources like solar and wind power and many others.
Imagine we ask the question, what's the safest?
Well, the International Energy Agency did a study in 2002 that compared different energy
sources per watt of energy produced and they looked at all the health and environmental
risks involved.
For example, with coal mining, there are costs of mining deaths and pollution.
With oil drilling and natural gas, these are dangerous things to do and they lead to pollution
when burned as well.
With hydroelectric power, there's environmental damage and sometimes dams collapse.
In the mid-1970s, some hundreds of thousands of people were killed when some hydroelectric
dams collapsed in China.
Nuclear panels have some materials that can be toxic and workers can get hurt or injured
falling off roofs when they're installing them.
Nuclear power is safe most of the time but it does produce radioactive waste and there's
a small chance of a very large problem occurring as we saw in the aftermath of the Japanese
tsunami that hit a nuclear reactor in 2011.
For what it's worth, the International Energy Agency, after looking at all these kinds of
factors, argued that nuclear energy was the safest per amount of energy produced.
And just to be clear, this isn't an argument that it's 100% safe, it's just that compared
to all the others, it looks relatively good.
Now my point here isn't to argue for or against a specific energy source nor to say that energy
conservation isn't needed or really any kind of claim like that.
My point is that when we think through the costs and risks of energy alternatives, a
lot of the time we need to think about what biases might be affecting our judgment.
Some risks are very salient, like the risks of nuclear energy.
Some risks are a small risk of a big disaster, like nuclear power, versus a 100% chance of
ongoing costs, like the air pollution from coal or oil and the damage from mining.
Imagine hypothetically you had an energy source that worked perfectly and at low cost
for 50 years at a stretch, but once every 50 years there's a huge disaster and it kills
50,000 people.
You compare that with an energy source that has no huge disasters, but every year it kills
2,000 people with accidents, pollution and things like that.
So one of these things will kill 1,000 people a year on average, but in two huge catastrophes.
The other will kill 2,000 people a year for sure every year.
I'd say that a tough minded approach will favor fewer deaths over an extended period,
but it's not sure a political system is set up to allow an occasional catastrophe.
Political systems tend to be better with costs that just happen steadily over time.
The biggest example in recent years of risk gone wrong is the financial crisis that hit
the global economy from 2007 to 2009, telling the whole story of the financial crisis needs
to be a set of lectures for another day, but I can sketch a sense of what happened.
U.S. housing markets had a period from the Great Depression up through about 2006, when
at no point was there an overall national decline in average housing prices.
There were times when one region had a decline, like certain parts of Texas in the 1980s when
oil prices came down, New York City for a bit in the 1990s, there are other examples,
but for the country as a whole there's no overall decline in housing prices on average.
What happened during this time was financial people began to get mortgages from banks,
bundle them together, and sell them.
It's kind of like when you buy a mutual fund in the stock market.
With a mutual fund you get the overall average of what all the stocks do.
If you buy one of these mortgage backed securities, as they're called, you get the overall average
of what all the mortgages pay.
And again, on average there hadn't been an overall decline in housing prices in 70 years
or so, so they seemed very safe.
Lots of houses viewed this as very safe, so they bought housing as well on the idea
that the price would keep rising, and as people did that, a bubble in housing prices developed,
a quick increase in housing prices that couldn't be sustained over time.
These mortgages were then cobbled together into big mortgage backed securities and sold
banks.
Some of these securities were rated AAA safe by credit ratings agencies like Moody's and
Standard & Poor's.
They basically ignored the risk of what would happen if the housing market came down because
after all, it hadn't happened since the Great Depression.
Some saw these risks, but many didn't.
Why not?
Well, one difficulty as I keep saying is that housing prices hadn't fallen in 70 years
for the country as a whole, and people didn't try to recognize there was a new context.
If they'd actually looked back over a hundred years and included the Great Depression,
they would have included the risk that housing prices might fall, and they would have acted
differently.
I talked before about Nassim Taleb, the black swan guy, that finance guy who says sometimes
rare events happen, and if you ignore the effect of the highly improbable, then once
every 20 or 50 or 100 years, it's going to happen.
A metaphorical example that economists sometimes use is a company that sells earthquake insurance.
Every year, there's no earthquake.
The company seems to make a big profit, and everybody gets a big bonus, everybody is happy.
The earthquake hits, the company goes broke.
Next year, you start a new earthquake insurance company, and too many financial players were
sort of acting like that kind of earthquake insurance company, planning to go broke and
then start over again.
And just to be clear, this isn't a few people at a few companies.
It's those making loans, those taking out the loans, those packaging and reselling the
loans, those buying the packages of loans including big sophisticated financial companies, commercial
banks, hedge funds, investment banks, pension funds.
The bank regulators didn't see this coming.
The financial regulators, the auditors, most of the financial press didn't see it either.
When the financial crisis went bad, then in some ways, the aversion to ambiguity and
fear of doing the wrong thing kicked in, and everyone more or less froze in place.
The recession ended in 2009, but at least in the United States, the period that follows
has been spent trying to work through the aftermath of the financial crisis.
Some economists have been calling it the long slump.
Making choices in a situation of risk is just enormously difficult.
It's not like choosing two different flavors of ice cream.
Your choices really come down to thinking about the probabilities they're involved, as best
you can figure them out, thinking about your tolerance for risk, and then thinking about
all the ways in which your judgment about risky situations might be biased by salience
or adding patterns where there aren't patterns, and all the issues we've talked about in
this lecture.
Worst of all, in thinking about situations of risk, is the difficulty that even when
you make the right choice, the choice that is the proper one given the situation you
face, it can sometimes turn out badly.
