Today you're interested in some question about the future.
Maybe it's a question with public policy implications, like trying to predict the chance the euro
will remain the currency of Europe or not, or whether Iran will successfully test a nuclear
weapon or not.
Or maybe it's an economic issue, like what will inflation or gross domestic product be
in the next year?
You can also imagine some more small scale questions, like will this movie that's just
been released be a commercial success?
Will a product that my company is releasing be a success?
Most people, when they're faced with these kinds of questions, they basically try this
approach.
They study up on it.
That is, they ask a bunch of people who know, they do some background research, and they
try and reach some sort of an opinion.
They know they're not an expert, so they try and gather more information from other sources.
However, this approach always has a problem.
Even if you study up and ask people, you tend to bring your own set of biases and limitations.
If you ask other people, they bring their biases and limitations.
If you ask me about, say, a question about some country in Africa or Latin America or
Asia, or a question about breakthroughs and alternative energy technology, or whether
the next movie starring Russell Crowe will gross $200 million, I honestly don't trust
my own answer, almost regardless of how much research I do.
Even if I ask a bunch of people, there's a question of whether they really know or whether
they're really motivated to give their best answer.
When you survey a bunch of people in New York City about who is likely to win the Superbowl,
they have a tendency to answer with a team from New York City.
That answer isn't necessarily their best guess.
It's about a kind of loyalty, which is fine, but doesn't necessarily tell me what I want
to know.
This lecture is about a different way to predict the future, which is to use a market.
The idea is to have people place what's essentially a bet on what's going to happen in the future.
You look at the outcome of these bets and view that as a prediction about what's likely
to happen.
An obvious example of this that should make sense to a lot of people is betting on sports,
for example, betting on professional football.
That's typically set up as betting on one team or another that they'll win by a certain
amount of points.
If you ask me what I think about a game, my answer will tend to be polluted by the array
of teams that I root for or against.
And honestly, a lot of experts are just guessing too.
But if you look at how people bet, now we're getting somewhere toward what's likely to
be a pretty accurate prediction.
I'm not saying that's always the right answer, it's just a more realistic guess than a lot
of other approaches.
The question is, can we take this approach to predicting other kinds of events that might
occur?
As a starting point for understanding the pluses and minuses of this approach, I want
to start by talking about a kerfuffle that happened in 2003, in which the United States
Department of Defense first proposed and then withdrew that proposal to run a prediction
market, which was often derided as a market for letting people bet on terrorism.
There's an organization in the U.S. Department of Defense called the Defense Advanced Research
Projects Agency, known as DARPA, a lot of the time.
It started back in the late 1950s as a response to the Soviet launch of Sputnik.
It was responsible for funding lots of technological developments over time, including helping
set up early versions of what later became the World Wide Web and the Internet.
So DARPA is always proposing and considering stuff, and some of it's maybe a little bit
crazy, but after all, that's often the point of research.
You try a bunch of stuff and you don't know what will work, but because you don't know
what will work, you just give it a try and find out.
In June 2003, DARPA proposed setting up what it called a policy analysis market, where
investors would place bets and potentially earn profits by betting on the likelihood
of events in the Middle East.
It created a huge tempest in a teapot.
There were news stories and op-eds and press conferences and talk shows.
The basic criticism was it was a bad idea to accept wagers on the fate of foreign leaders
and the likelihood of terrorist attacks.
The project was canceled a day after it was announced, and the head of the project, an
admiral named John Poindexter, resigned later that year, partly as a result of the fallout
from that proposal.
Exactly why was this such a bad idea?
Most of the common arguments were that the proposal was silly and somehow distasteful
or unethical, but to economists like me, these sorts of arguments are like catnip for cats.
They make us roll over on our backs, stick our feet in the air, and wiggle all over.
Let's see, on the one side, there's potentially useful information about the likelihood of
terrorist activity that could kill hundreds of people and cause worldwide turmoil.
On the other hand, getting that information might be in bad taste.
If that's really the choice, it's not a very difficult call for me.
Speaking as an economist, it's a little difficult even to know where to start from here, but
let me talk through the details of the plan, and that should give you a sense of the kinds
of actual, more serious issues that can arise in thinking about these kinds of markets.
First let's talk about the actual proposal.
The idea behind the actual proposal was to focus on political, economic, civil, and military
futures of key Middle Eastern countries, especially Egypt, Jordan, Iran, Iraq, Israel, Saudi Arabia,
Syria, and Turkey, and U.S. policy involvement with each one.
The idea was they were going to start off with a group of a thousand experts in the field,
which was going to eventually expand up to 10,000 people, and these experts would make
very limited bets, no more than $100.
These would be on topics like, would the United States pull its troops out of Saudi Arabia
by a certain time in the future?
Would the currency of Egypt fall by a certain amount by the end of the year?
The most publicized and controversial feature of this market was that there might be bets
on assassinations or terrorist events, but these were not actually listed as part of
the market.
It's sort of weird if you think about it, a lot of the accusations about this market
were actually about things that actually were not included in the market at all.
Remember, this isn't all that long after the terrorist attacks of 9-11, so there was definitely
some need for the U.S. to get better information about part of the world.
And yes, one of the reasons was we just experienced a major terrorist attack from that part of
the world, but critics sort of filled in the rest about betting on terrorist events.
This idea behind a market like this is that if you have hundreds or thousands of experts
who in turn are getting information from all over the place, it's useful to have a mechanism
to collect their input and find out what they have to say that tells you something.
One common objection to the prediction market was the nightmare scenario that goes something
like this.
Terrorists could infiltrate the market, make bets that terrorism was going to occur, and
then actually make money when they committed a terrorist act, making their prediction come
true.
In that way, you could actually have a financial incentive to commit terrorism.
Now, given the actual proposal, this was never very likely.
Remember, it was a small number of users, experts in the Middle East, small bets all
under government supervision.
No one was going to make a lot of money here.
But perhaps more to the point, those who want to make financial bets that will pay off if
they commit a terrorist act already have a lot of ways to do so.
Lots of markets that allow indirect sorts of predictions like this already exist.
Simple example is the futures market in petroleum.
The futures market is the market that lets you make an investment based on the expected
future price of oil.
If you're thinking about political disruption in the Middle East, that often tends to drive
up the price of oil.
Indeed, there were rumors, although I should say they were never proven, they were really
rumors that before Saddam Hussein invaded Kuwait, he attempted to make purchases in
the oil futures market so he would make money as a result of the invasion.
But more broadly, you could make any number of investments if you knew a terrorist event
was going to happen.
You could bet that the price of insurance companies was going to fall.
You could bet on an overall fall in the stock market price for the next day.
You could bet that prices for travel-related industries are going to drop.
And again, there were some rumors, and I should emphasize again that these are rumors, that
before the terrorist attacks of September 11th, there was unusual action in the stock
market as if someone was trying to make those kinds of investments in advance of the action.
The point here is that the DARPA proposal for prediction markets would have let the
government organize a group of experts and pose questions of interest.
It wouldn't have been possible really to infiltrate this or to make a lot of money,
and those who would like to make money by investing in advance knowing that violent disruption
is going to occur already have a bunch of ways to do so.
The other common criticism of this proposal was largely aesthetic, that it was somehow
distasteful or inappropriate to try and gather serious information through what seemed to
some folks like a whimsical approach.
But you think of all the ways that the U.S. government tries to gather information about
forthcoming political and economic events in countries that are not free and open.
Of course, you can look at whatever is available in open sources, but there's also things like
spying on people's communications and searching people in airports, all the stuff of war,
killing on battlefields, bribery, pressure tactics, blackmail, probably behind the scenes
betrayals, compared to all these sorts of issues, which in a way I kind of hope are going on
all the time as the U.S. government tries to gather information.
It's hard to see what's really distasteful about having a thousand experts put small
amounts of their own money on predictions about the future.
Maybe it's silly or it's useless, but ethically speaking, it's just not that bad compared
to a lot that happens in the world of bribery and espionage.
Of course, this is a story of a prediction market that didn't happen.
What about prediction markets that did happen?
Is there any evidence they might be useful or successful?
Let's consider some real world examples.
Perhaps the best known is the Iowa electronic market, which is run by folks at the University
of Iowa.
They got a special waiver from the government rules that govern stock market exchange and
other sort of financial deals to allow trading on this exchange, as long as it's small scale
trading and is focused on elections.
They started this market for the 1988 elections, and in that market, people could buy and sell
a contract, which would pay two and a half cents for each percentage point of the popular
vote in the presidential election, which was won by Bush or Dukakis or others who were
running.
Now, the Iowa electronic market covers a wide variety of elections at state level as well
as the national level around the country.
You can look it up on the web if you're interested.
Now, if you look at the predicted winner in the Iowa market, say the August before the
November election, and you compare it to predicted winners in the polling data, you
find that overall, the Iowa market tends to predict more accurately.
Putting money on the event seems to matter and work better than survey technologies.
There are also a lot of private companies and organizations that let you take bets on
future events.
Some of them are called companies like Intrade or Foresight Exchange.
You can go to their websites again, check it out.
And here's some questions where you could put money.
For example, will the U.S. have a ban on handguns by the year 2020?
Will Hillary Clinton still be Secretary of State in 2015?
Will someone construct an operational fusion reactor?
Will the Catholic Church allow priests to be married by the year 2015?
And again, you can put down money on what you think is likely to happen on one side
or another of those questions.
A similar operation is called the Hollywood Stock Exchange.
And there you put money down on predicting events in the movie industry.
This exchange has been predicting the Oscar award winners, for example, with about 90
percent accuracy.
It also predicts things like the gross revenues of movies that are just coming out and how
well they're likely to perform.
This idea of predicting revenues of movies might seem natural in a way.
After all, if you can predict votes in an election, why not predict sales of movie tickets?
But that leads to another idea.
If you can predict revenues from one line of business, namely movies, can't you also
predict other possible business outcomes?
A number of different companies have tried internal prediction markets.
That is, a market where the participants are employees of the company and they ask them
to predict things about the company's future.
Here's some examples from the 2000s.
Some of the first companies who did this were Hewlett Packard and Siemens.
In both cases, they had small groups of 20 to 60 employees.
They used real money with real winnings, but the firm provided the money to them or matched
what the people put in in one way or another.
These weren't very large groups, but they still were remarkably interesting and accurate.
For example, an internal market at Hewlett Packard produced more accurate forecasts of
printer sales and did so in a more accurate way than the firm's internal process for forecasting
those sales.
An example at Siemens was that an internal market predicted the firm would definitely
fail to deliver a certain software project on time, even though everything else inside
the company suggested that deadline was going to be met.
Again, putting money on it had people tell the truth.
In recent years, Google is probably the best known firm for using prediction markets inside
the company.
They actually have a vice president who has the job of setting up and keeping track of
these internal prediction markets.
They might ask questions like, how many Gmail users are there going to be in five years?
Will the company make its quarterly predicted profit number?
Will a certain project get done on time?
Will a certain new product succeed?
Will a related high-profile firm like, say, Apple, will they release a certain kind of
competitive product?
The accuracy of these predictions Google has found is quite good and honest.
I mean, anyone who's worked in the company knows the internal processes, the committee
meetings and stuff like that aren't always honest about things like, will a project be
on time, or will it be a success, or will we beat the other firm to market?
Do you really believe everything?
If you work for a company that the committees find out, I suspect not really.
The prediction market gets employees to reveal a more honest version of the truth.
At Google, they also found an interesting side benefit.
By looking at the bets that people make and the beliefs that people have, they can track
the flow of information within a company.
For example, do people who sit near each other tend to place similar predictions in the market?
And how do those predictions spread across the company?
If you rotate people around to different offices within the company, as Google often does,
that sets up networks of information flows across the company.
And by watching the predictions people make, you can actually watch those networks of information
inside the company form.
Now, do you always get the right answer?
Is it always a perfect mechanism?
Of course not.
And no one's claiming that.
It's just that this is an additional useful tool.
In fact, at Google, there was apparently a case where one employee wrote a computer program,
sort of a trading robot, to try and guarantee that they could make money in this market
and succeeded to some extent until they were found out.
You can imagine people trying to manipulate the market so they could buy at one price,
sell at another price and make money.
We need to come back to some of these issues of possible market manipulation a little bit
later in the lecture.
One of the new frontiers for prediction markets is market research.
About $25 billion a year gets spent on market research explicitly, and it'd probably be
more than that if you could count what happens inside companies as well.
What I'm talking about with market research are things like focus groups, giving away
samples, different surveys, stuff like that.
But as an alternative to all that conventional market research, you could also set up a prediction
market.
Give the ideas that you're thinking about are the possible products to a group of maybe
200 people.
Give them a limited amount of play money and let them predict not what they personally
like best, but what they think will be liked best by the group.
They can give reasons, they can argue back and forth online, and after a week or so,
whoever made the best predictions for what the group would ultimately decide will win
in this prediction market and be paid in real money.
People often play these games hard.
They enjoy them, and they give real and honest and detailed feedback.
This probably works at least as well as having somebody standing in the shopping mall courtyard
with a clipboard checking off things from people who went by or grabbing a bunch of
random people in a room to run a focus group.
Betting on which products will be preferred or will do well makes it worth mentioning
a couple of other well-known prediction markets.
One is betting on horse races.
The interesting thing about horse race betting, if you think about it, is some of the people
who are betting are real experts who studied all the form and everything else, but a lot
of people don't know much at all about the particular horses.
They can look up and say, well, who's the favorite and who's not, but that's really
about it.
Most of them are just out, out of the day at the track having some fun.
Why would their bets be an accurate reflection of what's going to happen?
But in general, they are quite accurate.
The main systematic departure from accuracy in horse race betting seems to be at the
extremes.
Some favorites to win races sometimes aren't favored quite enough, and extreme long shots,
100 to 1, 200 to 1, sometimes should be even longer shots that are very, very unlikely
to win.
But otherwise, it's really pretty accurate.
The horses that run at, say, 3 to 1 or 5 to 1 or 7 to 1 win just about as often as those
odds would suggest.
And of course, from horse racing, it's only a small mental step to thinking about stock
exchanges.
It's certainly true that stock markets can make mistakes.
They can miss companies that are about to do very well or that are about to do badly.
They can go through periods of boom and bust like the late 1990s.com boom or the drop during
the recession of 2007 to 2009.
That said, if you're trying to figure out if some event is good news or bad news for
a company, watching how it affects the stock price net company is a really good way to
figure that out.
And if you're thinking about how a company might do 6 to 18 months off in the future,
an obvious starting point is to look at the stock price of the company compared to other
similar firms.
There's nothing else that's systematically a better way of understanding what's going
to happen with companies.
After all, if there was something systematically better, you could use that information in
the stock market and be rich beyond your wildest dreams.
Now, I want to take half a step back.
Talk about how these different sort of prediction markets function, what they tell us about
underlying beliefs, and then what causes them to succeed when they succeed, what might
cause them to fail.
There are three main types of prediction market contracts.
Each of them has a different kind of information about the underlying beliefs that are being
modeled.
One is called a winner-take-all market.
In a prediction market, it's a winner-take-all market.
The contract costs some amount that varies and pays off a fixed price if and only if
an event occurs, like a particular candidate winning an election.
Say for example, the contract might pay $1 if Barack Obama is elected president.
The question then becomes, what would you pay for this contract?
If you think it's a near certainty, then you might be willing to pay $0.90 or more for
this bet because you're pretty sure it's going to happen and you'll get a dollar.
You think it's very unlikely, but it might happen and you might pay $0.10 or less just
on chance.
You think it's a 50-50 bet, you'd pay $0.50 for the chance to win a dollar.
So by looking at the price people are willing to pay on this kind of contract, you can see
what probability they're placing on the outcome.
You can watch the probability rise or fall as the price changes and as events evolve.
A second kind of contract for a prediction market is called an index contract.
In that kind of contract, what the contract pays is based on some number that rises or
falls.
I mentioned when the Iowa market started, that operated with an index contract that
paid two and a half cents for every percentage point of the popular vote.
So if I think some candidate would get 40 percentage points to the vote, I should be
willing to pay a dollar for that contract.
In that way, the price you pay for a contract represents the average value that people in
the market are assigning to the outcome.
What number do they think is going to happen?
A third kind of contract is called spread betting, in which traders differentiate themselves
by bidding on a certain cutoff number that determines whether an event occurs, like whether
a candidate gets more than a certain percentage of the popular vote or not.
This should be in some ways familiar from betting on pro sports, especially football.
The typical bet there would be that one team wins by at least X number of points.
points gets adjusted up or down, so that 50% of the money is on one side of the bet
and 50% of the money is on the other side of the bet.
Now, if you think about what that means, that prediction, half or on one side, half or on
the other side, tells you what the middle level, the median expectation is going to
be in that kind of a market.
You can imagine offering a variety of bets on different outcomes.
Will they win by 10 points?
Will they win by five points?
Will they win by two points?
All these different sorts of events, and so you could understand the whole distribution
of what people are likely to be believing about these events one way or another.
Why is it that prediction markets work?
Well, one reason is people have some skin in the game.
For a lot of these settings, people are playing for virtual money with some real world prizes,
but there's also bragging rights for the winners.
In other studies, they do play with real money.
There's not yet a proven consensus on this, but studies of this subject suggest that play
money actually works about as well as real money in getting people to participate and
give accurate answers.
What matters is that people feel they're competing for something.
They can win or lose.
They can be proven right or wrong.
It's a powerful incentive for people.
You take that ingredient and you mix it together with what's sometimes called the wisdom of
crowds, which is actually the title of a fine book by James Surioechi, an excellent economics
writer whose work often appears in the New Yorker magazine.
Here's an example from his book.
The great British scientist and statistician Francis Galton went to the West of England
fat stock and poultry exposition many years ago in 1906.
They were having a guess the weight of the cow contest.
So Galton watched all the guesses for a while, and then he persuaded the organizers to give
him data for all the more than 800 guesses that had happened.
The actual weight of the cow was 1,198 pounds.
The average of the guesses from everyone in the crowd was 1,197 pounds, and it's pretty
common that these results are very close.
Here's a classic experiment many teachers have run in the classroom, the jelly bean experiment.
Put a bunch of jelly beans in a jar and guess the number.
It turns out that the average of everyone's guess in the room is not only pretty accurate,
but on average it's closer to the truth than all but one or two percent of the people in
the room would have guessed.
Another example cited by Surioechi is that in 1968, a U.S. submarine named the Scorpion
disappeared off the coast and they just couldn't find it.
Salvage experts defined a search and rescue zone, but it was 20 miles wide, it was way
too big for undersea search at that time, thousands of feet down, so they didn't know
what to do.
There's a naval officer named John Craven.
He got together a bunch of experts and they all sat and hypothesized together about the
currents and how fast would it sink and what might the captain have done and so on and so
on.
And at the end of it all, he had them all offer their best guess on all these different kinds
of things.
The winner who was closest to the group average on each area would win a bottle of scotch.
They put it all together, they gave an estimate, and five months later the submarine was found.
It was just 220 feet away from the estimate of that group.
In crowds, it's often true that everyone knows a piece of the truth.
Even non-experts know a piece of the truth.
If you can get their perceptions all together, you can get a remarkably accurate answer.
It really helps prediction markets if the players aren't all from a single group.
You don't want to just have them reflect back some pre-existing consensus.
Instead, you want them drawing on different perceptions in meaningful ways.
And I think this is the reason why prediction markets often outperform, say, a committee
inside a company or certain organizations.
Committee members get too close to each other.
They form a sort of group opinion.
They don't really value the diversity of views.
They don't challenge themselves enough.
They start down a certain path and they keep going down that path.
The prediction market is group decision making without the committee.
There's room for dissenters to have their say.
They can put in their money according to how confident they are.
This may be one of the real secrets as to why prediction markets are often so accurate.
Moreover, prediction markets actually have a specific time reality anchor.
They're aimed at a specific fact or question at a certain time.
This might make them better at predicting than, say, stock markets.
In a financial market, like a stock market, there's not an ultimate cutoff time where
you know the true value of the company and what it's really doing.
You're always looking ahead as to what might happen at some point in the future.
You might really suspect, for example, that a rising stock price is too high.
But even if it is too high, you know it might stay higher or go higher for a time.
And so you might debt that it'll go higher and might fall later.
And this, of course, is how bubbles happen in asset markets.
People chase the trend, buying something as it keeps rising, it soars too high, and then
it plummets back.
But prediction markets can't work like this because they're anchored in specific events,
specific times, and specific places.
There's some possibility of manipulation, but they're less susceptible to those sorts
of problems of bubbles than you would expect might happen in other kinds of financial markets.
In other words, prediction markets are less focused on the path of what will happen, will
it keep going up in the future, and more focused on an eventual outcome.
The theory of prediction markets and the wisdom of crowds is actually sort of a humbling
set of beliefs for academic types like me.
After all, we tend to believe that if I just study harder, if I just learn more, I can
be more accurate than other people.
We sort of believe that even if the average of the group is better than 99 percent of
the predictions, if I really study, if I really work at it, I'll be in that 1 percent that's
more accurate.
And in many cases, if you just compare an expert to a regular individual, the expert will be
more accurate than the individual.
But that's not what a prediction market is.
In a number of questions, it's quite unlikely that the expert view or my personal view is
more accurate than the consensus view of many, many people coming together and needing to
put their own money on something.
This all leads, if you think about it, to a bit of a paradox.
True experts need to know enough to recognize the limitations of their own knowledge.
