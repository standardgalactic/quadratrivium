This lecture we're going to look at the life and work of Bernard Riemann, one of the great
German mathematicians of the middle of the 19th century.
In fact, between Riemann and Weierstrass, you certainly have the two mathematicians
who really dominate German mathematics and most of mathematics in the late 1850s and
well into the 1860s.
Riemann was born in 1826 in Hanover in Germany.
He went to the University of Göttingen initially with the intention of studying theology and
he went there simply because that was the appropriate university at which to teach,
to learn theology for his particular denomination.
However, he got there and discovered the mathematics teacher Gauss.
Gauss quickly realized that in Riemann he had an exceptional student and Gauss also
realized that Göttingen was not the right place for Riemann.
Gauss was really the only mathematics teacher there and Riemann was someone who needed exposure
to the entire breadth of mathematics.
And so at Gauss's suggestion, Riemann switched to the University of Berlin, which really
was the exciting place where most of German mathematics was happening at this time.
And Riemann did his undergraduate work at Berlin, but he then returned to Göttingen
to do his doctoral work with Gauss.
Riemann would earn his doctorate in 1851 and then he stayed on at Göttingen in order
to do his habilitation.
This is something that happens in the German system beyond getting a doctorate in order
to prepare to be a research mathematician and to get a position as a chair of mathematics
in a German university.
You have to build up a body of work that shows that you really are capable of doing original
research in mathematics.
And this then leads to a second thesis, a more advanced thesis that is called the habilitation
and involves an oral defense.
And both the habilitation and the oral defense are part of Riemann's story and I will come
back to them later.
At the same time that Riemann was a graduate student in Göttingen, there was another graduate
student there also studying mathematics with Gauss.
That was Richard Dedekind.
Dedekind would be one of the people who really explored the nature of the real number line.
He's the person responsible for what is called the Dedekind Cut, which again goes back to
the ideas of Eudoxus and Euclid and Archimedes in trying to explain what we mean by an irrational
number by saying it's not one of the numbers that's larger and it's not one of the numbers
that is smaller.
We're also going to see Richard Dedekind coming back into the story as we talk about
Fermat's last theorem.
In 1855 Gauss died.
It was Gustav Lejeune Dirichlet who took over his position as chair of mathematics at the
University of Göttingen.
But Dirichlet did not live much beyond that in 1859.
Dirichlet died and Riemann was appointed to the chair of mathematics at this university.
This also was the year in which Riemann published his very important paper on the prime number
theorem and I'm going to say much more about that later in this lecture.
It seemed that Riemann was set for a very auspicious career but just a few years after
taking over the chair of mathematics at Göttingen in 1862 Riemann came down with tuberculosis.
He traveled through Italy for several years after that hoping that the warmer climate
would improve his health and Dedekind often accompanied him on these trips through Italy
and Riemann did work with many of the Italian mathematicians at the time and in fact he
had a lot of influence on Vito Volterra who was then a young graduate student in Italy
and Volterra would go on to build on many of Riemann's ideas.
But in fact the tuberculosis did not get better and in 1866 Riemann died.
At the time that he died he had left much of his most important work unpublished.
It was Richard Dedekind who discovered his manuscripts, went through them, prepared them
for publication and saw that they got published and they had an enormous impact on the mathematical
community.
I want to talk first about his doctoral dissertation in which he was looking at the problem of
calculus applied to complex numbers, so complex analysis and we're looking at functions that
go from one complex variable to another complex variable and I've talked about some of these
functions taking the reciprocal for example, so z goes to 1 over z and I've talked about
how that moves points around in the complex plane.
Another example of a complex function is the simple exponential function.
So it takes z to e to the z and what that does is it takes the real number line and
the real number line if I take e to a real number I get a positive real number.
So it takes the real number line and it maps it to a ray that starts at the origin but
does not include the origin and then travels infinitely far off to the right.
If instead of the real number line I move up by pi over 4 times i and consider the horizontal
line whose imaginary part is pi over 4 times i and I now take e raised to a point on this
line, this line is now going to become a ray that goes out from the origin at an angle
of 45 degrees.
If I move up again to the horizontal line whose imaginary part is pi over 2 times i,
that now gets mapped to a ray that goes vertically upward and so as I take these horizontal lines
that go upward, so horizontal lines become these rays going out from the origin.
So if I take horizontal lines at pi over 4, pi over 2, 3 pi over 4 and so on, I'm going
to get rays going out from the origin at 45 degree angles from each other.
If I now consider the vertical lines, if I take the imaginary axis and I look at what
happens to that under this mapping that takes z to e to the z, e to a purely imaginary number
is going to be a point on the unit circle.
So this transformation takes the vertical line and turns it into a point on the unit
circle and as I take other vertical lines, if say I take the vertical line that has real
part equal to 1, that's going to map into a circle with radius equal to e to the first
power and the vertical line with real part equal to 2 is going to map to the circle with
radius equal to e squared and if I then take say the vertical line with real part equal
to minus 1, that's going to turn into a circle with radius equal to e to the minus 1 power.
So it's a radius less than 1.
So as I take the horizontal lines, I've got get rays going out from the origin.
As I take the vertical lines, I get concentric circles with their center at the origin.
And one of the interesting things that happens if you think about the grid that is formed
by my horizontal and vertical lines, that turns into a kind of polar grid where I've
got rays going out from the origin and I've got circles.
If I look at my original domain with my horizontal and vertical lines, these lines meet at right
angles.
If I look at the image which has these rays going out from the origin together with the
circles, each of these rays cuts the circle at a right angle.
The ray is perpendicular to the tangent of the circle at that point.
Now it turns out this is not just true of the horizontal and vertical lines.
They meet at right angles and so their images meet at right angles.
This actually is true of any pair of curves.
If I take two curves in the domain and I consider the angle at which they meet, that is to say
I take the tangent lines at the point they meet and I look at the angle between those
tangent lines and I do the exponential function and I look at the images of those curves.
They're going to be two different curves that still meet at a single point.
I can find the angle between those two curves and the angle between the curves in the range
is going to be exactly the same as the angle between the curves in the domain.
And so this is a mapping that preserves angles and there is a name for it.
It is called a conformal mapping.
Conformal just means angle preserving.
And in fact it's not just the exponential function, most of the nice functions that you
might think of, polynomial functions in a complex variable, trigonometric functions
in a complex variable, rational functions, a function that's a ratio of two polynomials
in a complex variable, these are all conformal mappings, all conformal functions.
They preserve angles.
Now what Riemann did in his thesis was not just to study this problem of preservation
of angles, he actually turned it around and he asked if I've got some region over here
in my domain and I've got another region over here in my range and I want to come up with
a complex valued function that goes from this region to this region over here.
Can I always find a conformal mapping that does it?
And in fact what he proved in his thesis is that the answer is yes.
You always can do it, there's some qualifications, the boundary can't be too tricky, but if you've
got a reasonably nice boundary over in the domain and a reasonably nice boundary over
in the range, then you can always find a conformal mapping that goes from one to the
other, an angle preserving mapping from one to the other and this then would be the subject
of Riemann's thesis.
I want to move on now to the prime number theorem, the result that Riemann published
in 1859 and it deals with the problem of counting the number of primes that are below a given
number.
The primes start with 2, 3, 5, 7, 11 and given a number like 20, one can ask how many primes
are below 20, how many primes are below 100, how many primes are below 100,000 and we can
actually graph this prime counting function, it's going to be a step function.
Every time we hit a new prime we increase the value by one and a number of mathematicians
had explored this question of how fast does that function grow, is there a good way of
estimating the number of primes that are less than X and it actually was the young
Gauss in 1792, he was only 15 years old at the time, who realized that the right function
for modeling the growth of the number of primes is something called the logarithmic integral.
Now one of the functions that can be used is simply to take X, that number below which
we want to count the primes, X and divide it by the natural logarithm of X. So the number
of primes less than X is about X divided by the natural logarithm of X. But a more accurate
function is this logarithmic integral which is obtained by taking the reciprocal of the
natural logarithm, so one divided by the natural logarithm. So this is a function that's going
to approach infinity at one and then it's going to drop off and gradually get closer
to zero, the logarithmic function approaches infinity as the variable gets larger, so the
reciprocal of the logarithmic function is going to approach zero. But the logarithmic function
grows very slowly and so its reciprocal approaches zero very slowly. And what you can do is look
at the area underneath this reciprocal of the logarithmic function from two up to some
variable X. And the area underneath the reciprocal from two to X is called the logarithmic integral
of X. And that turns out to give you a very accurate approximation to this prime counting
function. If you look at the prime counting function X over log X and the logarithmic
integral of X for values of X up to 100, you'll see that the prime counting function, this
step function, lies between these two curves. And in fact, up to 100, you can see that the
logarithmic integral is above the step function and X divided by log X is below the step function
and it looks like the actual prime counting function occurs roughly halfway between the
two. Well in fact, that's what it looks like up to 100, but if you go a good deal larger
than that, say if you go up to 100,000, so you're looking at the prime counting function
all the way up to 100,000. Now our horizontal increments are small enough and our vertical
increments are small enough that it's hard to see the steps. This prime counting function
now begins to look like a smooth function. And it's quite clear as we go up to 100,000
that it's much, much closer to the logarithmic integral than it is to X divided by the natural
logarithm of X. But one of the things that you observe in comparing the prime counting
function with the logarithmic integral is that the prime counting function always seems
to be below the logarithmic integral. And a natural question to ask is, is it always
less than the value of the logarithmic integral? And there's an important result that was done
in the very early 20th century by Littlewood, and we're going to run into Littlewood again
when we talk about Srinivasa Ramanujan, but Littlewood who was at Cambridge at that time
was able to prove that in fact the prime counting function does not stay below the logarithmic
integral. Eventually it will become larger than the logarithmic integral and then it
will go back below the logarithmic integral and then back above and then again below.
And as you go out toward infinity, the prime counting function and the logarithmic integral
are going to interchange places infinitely often. But a question for mathematicians in
the 20th century was where is the first place where the prime counting function is larger
than the logarithmic integral? They did calculations out to millions and billions and it was clear
that the prime counting function was staying below the logarithmic integral. Where is the
first place where they switch and the prime counting function is larger? This first place
became known as the skews number, and for a long time the best that we knew was that
it was absolutely humongous. Well in fact it is very large, just in the past few years
people have been able to pin it down a little bit more closely. The first place where the
prime counting function is larger than the logarithmic integral is at a number that is
about 10 to the 316th power. In other words it's an integer of 317 digits. It's not known
exactly but it's known that this 317 digit number starts with 139 and then continues on
with another 314 digits, not all of which are yet known. This is work actually that
was done by the mathematician de Michel in 2005. Now as I said we know that the prime
counting function can be well approximated by this logarithmic integral and in order to
do that what Riemann did was to look at the zeta function. This is the sum of the reciprocals
of the integers raised to a power that is an arbitrary complex number. So we're looking
at the sum 1 over 1 to the z plus 1 over 2 to the z plus 1 over 3 to the z and continue
this infinitely far out. If the real part of z is larger than 1 then this infinite summation
converges. But one of the things that Riemann realized was that you could actually take
this function and the fact that it's defined when the real part of z is larger than 1 implies
that it must be defined for all complex values except exactly at z equals 1 where as you
approach z equals 1 this number is going to approach, this function is going to approach
infinity but it can be defined over the entire plane. And something else that Riemann realized
was that understanding the nature of this function would enable you to prove that the
number of primes less than or equal to x is well approximated by the logarithmic integral
of x. And in fact what he showed was that all you need to do in order to prove that
logarithmic integral is a good approximation is to show that this zeta function does not
take on the value 0 anyplace else on the vertical line with real part equal to 1. It would take
a long time before this actually would be proven. In the 1890s it would be two French
mathematicians, Adamar and de la Vallée Poussin who would finally put together this proof
of the prime number theorem but they exactly followed the outline that Riemann laid out
for them in what it would take in order to prove it. Riemann went beyond that, he also
looked at the size of the error term, how close is this approximation, how close does
the prime counting function stay to the logarithmic integral. And he realized that the key to that
was understanding the location of the other places where the zeta function could equal
0. Now he showed that the zeta function has a kind of symmetry about the vertical line
with real part equal to 1 half so that if I know the values on one side of that line
I can take the mirror image of those values and I know the value of the zeta function
on the other side. Well since we know the values when the real part is larger than 1
simply using the series expansion, we can use that to find very good approximations of the
values, in fact we can really pin down the values when the real part of z is less than
0. The trick comes for complex numbers where the real part lies between 0 and 1, this is
known as the critical strip. And what Riemann showed is that if all of the zeros of the zeta
function that lie within this critical strip actually lie along the single vertical line
with real part equal to 1 half, then the error term in approximating prime counting
by log integral, that error term is about the square root of x, much much smaller than
the size of the logarithmic integral. This would become known as the Riemann hypothesis
that all of the zeros of the zeta function that lie inside the critical strip, all of
the places where the zeta function is 0, actually lie on this single vertical line.
Now it's known that there are infinitely many places where the zeta function is 0 inside
this critical strip. And a lot of people have worked on this problem, the Riemann hypothesis,
it is probably the most important open problem in all of mathematics today, just because
so much of what we study depends on having a good idea of the gaps between the primes
and exactly how large a given prime is going to be. And so pinning down this error term
turns out to be very important. A lot of work has been done on actually showing that many
of these zeros, hopefully most of these zeros, lie on this critical line. And in 2004, Gordon,
using a method developed by Andrew Adlisco and Schoenhaage, actually was able to check
the first 2.4 trillion zeros of the Riemann zeta function that lie within the critical
strip and show that all of them lie precisely on this single vertical line. Now that may
seem like pretty convincing evidence, but remember that there are infinitely many of
these zeros. So checking the first 2.4 trillion really tells you nothing about the next 2.4
trillion or the next 2.4 trillion after those. I want to look now at some of the work that
Riemann did with geometry. This was part of his habilitation. So I said that the habilitation
that he did in 1854 involved both a written part and also an oral part. His written part
was on Fourier series. It was very important work on trigonometric series. And normally
your oral report, your presentation would be based on what you wrote. But usually the
candidate for the habilitation had to submit several different topics that he was willing
to talk on. And Riemann had said that he was also willing to talk about geometry. Well
Gauss, of course, was interested in geometry. He had done all of this work on differential
geometry, was interested in non-Euclidean geometry. He wanted to know what Riemann
had to say about geometry. And so that became the topic of Riemann's habilitation talk.
And what Riemann did was to take what Gauss had done and invert it, turn it around in a
very clever way. I've talked about Gauss and differential geometry where the idea was that
you're given a surface and you want to define distances and areas. And you use the differential
calculus in order to find the distances on a given surface. What Riemann did was to say
rather than start with the surface and ask how to define these differentials, how to define
distances, let's take an arbitrary definition for the distance and look at the kind of geometry
that that implies. So start with the definition of distance which can be defined very generally
in terms of notions of differentials and calculus and see what geometry that implies. This would
be the root of what today we refer to as Riemannian geometry, building geometries up
from our understanding of how distances are connected. And this would be the key insight
that would enable Einstein to do his work in general relativity. Because in general
relativity what Einstein actually did was to come up with a particular definition of distance
that was consistent with his understanding of the physical world. And what Einstein then did
was to go back to Riemannian geometry and the way other geometers had developed it through
the remainder of the 19th and into the early 20th centuries and see how to start with this
definition of distance and use that to see what it implies about the nature of the geometric
space in which we are actually working. Another one of the people to make important contributions
to geometry at this point is Arthur Cayley born in 1821. He would take a look at this problem
of projective geometry. We've seen how you can project from a sphere onto the plane. Projective
geometry generally considers taking some kind of a space with the geometry on that space
and then projecting it into another space and seeing what kind of geometry you get. And Cayley
was able to show that all of these projective geometries can actually be explained in terms
of differential geometries. It's the differential that is key. Felix Klein born in 1849 would
really advance our understanding of geometry in very fundamental ways. One of the interesting
things that Klein came up with is something called a Klein bottle. This is really an extension
of the idea of a Mobius Strip. If you recall a Mobius Strip is something two dimensional. You
can give it a half twist through the third dimension and get something that only has one
side. What a Klein bottle is is a three dimensional bottle that is given a twist through the fourth
dimension so that there is no distinction between the inside and the outside of the Klein
bottle. So what we have is the neck of the Klein bottle comes up and comes back inside
and what had been the inside of the Klein bottle now becomes the outside. Now of course I can't
actually hold the Klein bottle for you in four dimensional space. This is a three dimensional
model and so you see that handle passing back through the side. A true Klein bottle doesn't
use that. That's really a cheat so that we can create a Klein bottle in three dimensional
space. A real Klein bottle really only lives in four dimensional space. Klein took on a
program of trying to understand geometries not just in terms of differentials but also
in terms of the transformations that leave the geometries unchanged what are called the
invariance. And Klein set out a problem while he was at the University of Erlangen to try to
characterize all geometries and to show how all possible geometries could be explained in
terms of these invariance. That became known as Klein's Erlangen program. He would work on
it, other mathematicians would work on it and eventually come to a way of thinking of all
geometry in terms of transformations. Now I've said that algebra is really the study of
transformations and so what begins to happen in the late 19th and early 20th centuries is that the
geometry and algebra begin to merge. Algebra is the study of transformations. Geometries are
understood by understanding what transformations act on them. What transformations leave a given
geometry unchanged. And so we begin to get an area of mathematics that is known as algebraic
geometry which may sound like it should be analytic geometry, high school algebra and high
school geometry but it really is extremely sophisticated mathematics in which you look at the
algebra that sits behind the kind of geometric spaces that you want to study. Klein would go on to
become chair of mathematics at Göttingen and while he was there he decided to turn Göttingen into the
premier center of mathematics for the entire world and in fact he was successful in that in the early
decades of the 20th century, first under Klein and then under David Hilbert, Göttingen would be the place where
you had to go if you wanted to study with the greatest mathematicians if you wanted to find out what was
really happening in mathematics. Unfortunately in the late 20s and into the 1930s with the rise of
Hitler's Germany, the Jewish mathematicians at Göttingen were forced to leave the university. Many of the other
mathematicians at that time realized that it would be wiser to leave and go to many of them to the United States,
some of them to England or to other countries and in fact that mathematical community at Göttingen was
destroyed by the rise of Hitler's Germany but during the time that it was at its peak during the 1890s and right up until
about 1930 it was the very center of mathematical activity in the world. David Hilbert would make a name for himself in
1900 when he addressed the International Congress of Mathematicians and laid out his 23 problems. These are problems that would
motivate much of the work of mathematics in the 20th century.
