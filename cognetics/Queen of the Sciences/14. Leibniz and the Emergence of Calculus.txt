In the last lecture, we looked at Isaac Newton and his use of calculus. Certainly, Newton
understood calculus in its full complexity. He was able to use it very efficiently. But
as I said in the last lecture, Newton really did not put calculus out there in a way that
others could build on it and use it. That job would fall to Leibniz and two Swiss brothers,
the Bernouillis. I want to say a little bit about the development of the calculus. There have been
references to it throughout these lectures. And calculus really would be one of the foundational
aspects of mathematics that would then carry mathematics through the following centuries.
Calculus lies at the very heart of most of the mathematics that is done today.
The origins of calculus really lie with the method of eudoxus, the method of exhaustion.
We saw how Euclid described this method for finding areas and volumes, how Archimedes used it
very efficiently. And this is the critical idea behind the integral calculus, this idea of trying
to find the area underneath a curve by taking that area and subdividing it into small rectangles,
whose area it's easy to find, adding up the areas of those rectangles, and get using that as an
approximation to the area, then taking smaller and smaller rectangles, narrower and narrower
rectangles, and seeing what number you approach, what number you get close to as you take more
rectangles. This is the method of exhaustion. This is the basic idea behind integral calculus.
It was used and developed not just by the ancient Greeks. We have also seen that the Islamic
mathematicians use this idea very effectively. Ibn al-Haytham used it in order to find the volume
of the dome that's obtained by rotating a parabola. The Chinese mathematicians of the first millennium
AD used this idea in order to find areas and volumes. The other part of calculus is the
differential calculus. This involves looking at rates of change. So in the case of motion, if you
know where something is at any given point, the velocity is the rate of change of position.
And if you know the velocity at any given point in time, then the acceleration is the rate at which
the velocity is changing. And so we have this other problem of looking at rates of change.
And geometrically, rates of change can be thought of as the slope of the tangent line to a curve.
And we've also seen that this idea of looking at rates of change and building mathematical
models around rates of change is something that goes right back to the Indian astronomers of 400,
500 AD, that they were beginning to look at the rate at which the sine function changes,
and using that in order to find their interpolating formulas. And this really came to a high level
of perfection under Bhaskara in the 12th century. Bhaskara, who really in some sense was able to
invent the derivative, at least for a very small class of functions such as the sine and the cosine,
and begin to work with it and see the power of that. We have seen how in the 17th century,
Fermat and Descartes invented analytic geometry, this ability to go back and forth between a
geometric object in its algebraic representation or an algebraic expression and its geometric
representation. And this really accelerated this whole process of looking at slopes of tangent
lines, looking at areas. And as I said in an earlier lecture, Pierre de Fermat actually was able to
find many of the formulas that we commonly associate with calculus. Formulas for finding the area
underneath a polynomial curve, or finding the slope of the tangent to an arbitrary polynomial,
not just polynomials, but also negative powers of x, rational powers of x.
Wallace and Gregory would take these ideas and further develop them. They would come up with
the idea of power series that I talked about in the last lecture, these kinds of infinite polynomials
that Newton would use very effectively when he wrote the Principia, and that would be a very
important part of calculus as it moves forward, especially in the 18th century. We will see
how Leonard Euler uses this idea of power series as a way of really exploiting the full power of
the calculus. We've also seen how Isaac Barrow, in his geometrical lectures, was able to tie together
these two ideas of the differential calculus, knowing where something is at any given point
in time, figuring out the rate at which it is changing, and the integral calculus which finds
the area under curves. In fact, these two problems are related. If you want to reverse the process
of differentiation, if you know the slope of the tangent line at every point, and you want to find
the original curve, that can actually be accomplished by using the idea of integration. If you take this
slope of the tangent line, express that as a curve, a curve which at each point gives you
the slope of the tangent, and you then find the area underneath that, that area is going to give you
the original function of which you had been finding the slope of the tangent line. The same kind of
reverse process also works in the other way if you consider a curve, and you consider the function
that describes the area as you go from say zero up to any point, so the area underneath this curve
from zero up to x is a function of x. If we now take that function of x and look at the slope of
its tangent line, look at its derivative, that takes us back to the function that we started with.
So integration and differentiation are reverse processes of each other. This would become known
as the fundamental theorem of calculus, this realization, that you can do this, and it's where
calculus really gets its name. Calculus comes about as a tool for calculating, and very often an
easy way of finding the area under a curve is to recognize that the curve in question is the
derivative of some other standard curve, and so by recognizing that integration is the reverse
process of differentiation, it can make the calculations easier, hence we call this subject
calculus. Newton was interested in calculus in order to explain motion. We really see him using
it most effectively when he's looking at celestial mechanics, when he's trying to explain the motion
of the planets, and Newton is using calculus primarily in order to understand the relationship
between the position of an object, say the position of a planet, its velocity, and in particular
interested in the velocity vector, not just how fast is it traveling, but what direction is it
traveling in, and the acceleration, how fast is this velocity changing. And the fundamental law
of gravitational attraction, that the force is inversely proportional to the square of the distance,
that is, if I know the distance from the sun to the earth, then the force of gravity exerted by
the sun is going to fall off as the square of the distance, then what that gives me actually
is what we call a differential equation. It's an equation that involves the derivative,
because the force depends on the distance, but the force is telling us the rate at which
the velocity is changing, the force tells us the acceleration, and the velocity itself is
the rate at which position is changing. And so what that tells us is that the rate at which
position changes, the velocity, if we then take the rate at which that changes, the acceleration,
that acceleration depends on the distance, and so that's equal to 1 over the square of the distance.
Leibniz enters the picture now as someone who goes beyond what Newton or any of the others did
in looking at calculus in a somewhat different way. I want to back up and talk a little bit
more about the story of Leibniz first, and then I will come and look at how he contributed to
our understanding of calculus. Gottfried Wilhelm Leibniz was born in Leipzig in 1646. He intended
to become a lawyer. He studied philosophy and law, and he became attached to Baron von Boeinberg
in Frankfurt as his private secretary, as a librarian, as a personal advisor,
and at the same time he opened a law office in Mainz, and he also had an independent law
practice at that time. But he was interested in science, and partly because he had a patron,
he had a personal sponsor, he was able to travel in order to study science, and in 1672 Leibniz
traveled to Paris, and while he was in Paris he met Huygens, Christian Huygens, who I talked about
in the last lecture, certainly one of the great continental scientists of the 17th century,
and Leibniz began to talk physics with Huygens, and Huygens quickly realized that there were
serious gaps in Leibniz's understanding of mechanics and physics, and so he took the young
Leibniz under his wing and began to teach him, and part of teaching him was to introduce him
to the work of Wallace and the other people who had been working on the basic ideas on which calculus
would be built. Leibniz became fascinated with this idea of calculus, he began to work on it.
Two years later he traveled to London and would actually be elected as a fellow of the
Royal Society in London, and corresponding with Oldenburg, who was an English scientist at that
time, he learned that Newton was also working on these same ideas of what today we would call the
calculus, and so Leibniz wrote to Newton and asked him about what he had done, and Newton wrote back
to Leibniz, and Newton was fairly guarded in his response, he was, as I said in the last lecture,
Newton was jealous of his results, but he did share some of the things that he was able to do
with his tools of calculus, not explaining how he got those results, but stating some of the
things that could be done. Leibniz took these ideas and he had enough understanding of calculus
that he could figure out how to do them, and so he wrote back to Newton and said, oh yes,
those are interesting insights, and this is how I'm able to do it, and he was hoping at that
point to develop a further correspondence with Newton. Newton became suspicious at this point
and cut off the correspondence, he was afraid that Leibniz would steal his ideas, and this was
this initial correspondence that would be the foundation for Newton's later claim that Leibniz,
in fact, had stolen his ideas, that those results that Newton had sent to Leibniz
were key things that pointed to the kind of calculus that Leibniz would need to be able
to develop, and Newton believed that those were the keys that Leibniz had used to develop his
understanding of the calculus. In 1676, Leibniz is back in Germany, he becomes librarian to the Duke
of Hanover, he gets a much more prestigious patron in the Duke of Hanover, he gets much more free
time, and he's really able to devote himself to the development of the calculus. Unlike Newton,
Leibniz begins to publish his results, and fortunately for Leibniz one of the first scientific
journals had just appeared. In 1682, the first scientific journal in Germany, the Akta Aruditorum
had appeared, and in 1684 Leibniz begins publishing in this journal the first known actual publications
of Leibniz on the calculus appear in 1684, the same year that Newton is beginning his work
on the Principia. And in 1686 Leibniz publishes his full statement of the fundamental theorem of
calculus, explaining in very clear language the connection between the differential calculus,
the finding of slopes, and the integral calculus, the finding of areas. The actual controversy
between Newton and Leibniz would not break out for quite a while, not until 1711, and it would be in
1713 that the Royal Society would actually declare that Leibniz had plagiarized these ideas, but we
know enough about what Leibniz had been doing that it really is legitimate to credit Leibniz with
coming up with an independent understanding of the calculus in its full complexity.
Now I said that when Newton looked at calculus, he was interested in calculus as the mathematics of
motion, and he wrote about it in terms of something that he called fluctuations, what today we call
derivatives Newton referred to as fluctuations. He's interested in these changes and in these rates of
change, but Leibniz had a very different way of thinking about the calculus. He was looking at the
integral calculus not necessarily as areas, but as limits of sums of products. And he was interested
in these rates of change, not as rates of change necessarily, not as velocities and accelerations,
but rather as ratios of differences. He was really focused in on this idea of why depends on x. You've
got some kind of equation in which y depends on x, and you look at the amount that x changes,
what I'll refer to as delta x, the change in x. You're interested in the amount y changes as x
makes this change, that's your delta y. And then Leibniz is looking at the rate of change as the
ratio, the delta y change in y divided by the delta x, the change in x. And if we're trying
to find the slope of the tangent line, if we're trying to find this rate of change, we need to
take the delta x smaller and smaller. And if we're trying to find the limit of this sum of
products, the kind of sum of products that you would get in the method of exhaustion
in finding the area underneath the curve, you're looking at this idea of the limit of a sum of
products. Newton sort of finessed this problem of what do we mean by this limit of sum of products,
what do we mean by this limit of ratios, as the delta x is becoming smaller. Leibniz tackled
this problem head on. He said that what eventually happens is this delta x shrinks down to the point
where it becomes an infinitesimal. It becomes a number that is so small that it is smaller than
any positive number, but it's not zero. It's greater than zero and yet smaller than every
positive number. That's a strange concept to try to wrap your mind around, and yet it's very useful
and intuitively, it actually gives us a way of making some kind of sense of the calculus.
That what we're looking at when we take a derivative is a ratio of two infinitesimals,
two quantities that are so close to zero that we can't assign a positive value to them,
but they're not quite zero. And one of the things that Leibniz does is to completely glide over
Zeno's paradox and much of the mathematics that the ancient Greeks had done, because Greek mathematics
was based on this assumption that numbers are commensurable. If I've got two numbers, they
represent lengths, and it's always possible to take one of those lengths and measure out the other
length in terms of the shorter one. And what Leibniz is claiming is, no, we need to introduce some
numbers, but you can't do that, because if you take an infinitesimal and you take a finite length,
no matter how many infinitesimals you take, you'll never be able to equal that finite length.
Two other mathematicians who play a critical role here at the end of the 17th century and
into the early 18th century are the two Swiss brothers, Jakob and Johann Bernoulli from the
city of Basel. Jakob was born in 1654, Johann was born in 1667, and they have an incredible family
tree. There would be a total of eight Bernoullis who would become great mathematicians, not all of
them of the stature of Jakob and Johann. They were two of the very greatest. Johann's son,
Daniel Bernoulli, would certainly be the equal of his father and his uncle, and the others were
lesser mathematicians, but still very important mathematicians in the story of the development
of mathematics. Jakob did not set out to study mathematics. He began by studying theology at
the University of Basel, but he would eventually travel to France, and while in France he would
begin to study mechanics and mathematics, he went on and spent some time in the Netherlands.
He actually traveled to England, continuing to explore the exciting work that was happening
at that time in mechanics and mathematics, and eventually he would go back to Basel in 1683.
This is the year before Newton writes the Principia. Jakob Bernoulli finds himself back in Basel,
and by 1687 Jakob Bernoulli has become the chair of mathematics at the University of Basel.
Johann was a good deal younger than Jakob. He enters the University of Basel in 1683,
the same year that Jakob had returned to the University to begin teaching mechanics.
Johann enters the University with the intention of studying medicine,
but under the influence of his older brother, he becomes interested in these questions of
mathematics, and especially the questions of the development of calculus, and in 1687 the two
brothers discover the papers that Leibniz has begun to publish on the development of the calculus,
and they are fascinated by what's going on there. In 1690 Jakob publishes his first important work
on calculus, and what he does is to publish the solution of the isochrone. So I need to set up
the situation here with an isochrone. If you have a ramp and you take a ball and you let it roll
down the ramp, and you then take the ball and you put it at a lower position on the ramp,
it's going to take less time to get to the bottom. So how long it takes depends on where we start on
the ramp, and the ball that starts further up is going to take longer to get to the bottom than
one that starts further down. But what if instead of a ramp that's simply an inclined plane, we take
a ramp which descends extremely steeply, and then when it gets very near the bottom suddenly levels
off. If I take a ball and I start it at the top, it's going to pick up a great deal of speed as it
falls quickly down the steep portion, and that speed will continue and get it to the output very
quickly. If I take a ball that starts on the relatively flat part of this ramp, it's never
going to pick up much speed, and it will actually take longer to get to the end of the ramp than
the ball that started high up. So in this case, the one that starts high up gets to the endpoint
before the one that started lower down. So which ball gets there first is going to depend on the
shape of the ramp. And the question that came up that several people had looked at and Jacob Bernoulli
actually was able to solve is, is there a ramp? Is there a shape of a ramp that you can design
so that wherever you put the ball on that ramp, it will take exactly the same amount of time
to get to the end? And Jacob showed that you could do this, and he used the tools of calculus in
order to find out what the shape of that curve should be. It's a curve that today we call a cycloid.
Johann would go on to Geneva and then Paris. In 1691, Johann began a correspondence with Leibniz,
and the two of them would then do an extended collaboration over the succeeding years,
sharing their ideas of the calculus and really working together to build up the foundations
of the calculus. In 1694, Leibniz wrote a very important letter to Johann Bernoulli,
explaining how to get these power series, these polynomials of infinite degree for any kind of
function, an absolutely foundational tool that Bernoulli would go on to use in important ways.
Bernoulli did not have a patron at this point in time. He did not have a university position.
He was in Paris and a lot of the royalty, a lot of the nobility in Paris,
were interested in what was happening in mathematics and science at that time.
And one of the noblemen who actually had a fair amount of mathematical talent was Guillaume de
L'Opital. For two years that Bernoulli was in Paris, Guillaume de L'Opital paid for his costs,
supported him, and the contract that was drawn up agreed that any mathematical results that
Bernoulli came up with during this time, L'Opital was free to use as his own.
L'Opital would later go on to publish the very first textbook on calculus. This was published in
1696, the Annalise des Infiniments Petit, the analysis of the infinitely little things, or as
one of my colleagues likes to say, analysis of the itty-bitty things, which is this idea of
calculus's tape built up of these infinitesimals. And L'Opital explains this idea and contained
within his textbook is a result that today we call L'Opital's rule. And it's a general way of
taking the problem of a ratio of two quantities and looking at what happens as both of these
quantities approach zero. So you have some algebraic expression, or it might be a trigonometric
function, or an exponential function, something involving exponentials in the numerator. And
you've got another expression in the denominator. And both of these expressions depend on the same
variable, they depend on some x. And you observe that as x gets close to zero, both of these
quantities are approaching zero. So it's a ratio where the numerator is getting close to zero,
and the denominator is getting close to zero. Well, if the denominator stays fixed and the
numerator approaches zero, the entire fraction is going to approach zero. And if the numerator
stays fixed and the denominator approaches zero, the whole fraction is going to blow up toward
infinity. But what if both of these are approaching zero? Well, this is exactly the kind of idea
that's used in the differential calculus, because that's a ratio where both numerator and denominator
are approaching zero. And in fact, the answer that you get, what this approaches as the x goes
toward its limit, is going to depend on the relationship between the expression in the
numerator and the expression in the denominator. And one of the things that L'Hopital publishes,
what today we refer to as L'Hopital's rule, is a general way of using derivatives in order to
find this limit of this ratio of two things that are both approaching zero.
After L'Hopital's death, Bernoulli would state that L'Hopital had learned this idea from him,
and that it really was his, and later on, not until actually many, many years later. Finally,
the correspondence was discovered in which it was shown that this was an idea that Bernoulli
had come up with. He had shared it with L'Hopital. But at about the same time, the mathematical
community discovered this contract between the two, this agreement, that the results that Bernoulli
came up with were ones that L'Hopital was free to use and attach his own name to. And so today,
we call it L'Hopital's rule still. It was Bernoulli who first discovered it, but L'Hopital paid for it
quite fairly. L'Hopital was a good mathematician, and so it's not at all unreasonable to apply
his name to this result. In 1705, Jacop Bernoulli dies. Johann takes over his chair at the University
of Basel. And over the next several years, Johann completes the great work that Jacop Bernoulli had
begun during his lifetime on the mathematics of probability. In 1713, Johann finally publishes this
work, the Ars Conjectandi, the Art of Conjecture. It's a work on probability, but it has so much more.
One of the important things that I will come back to later is something called a Bernoulli
polynomial. And so this is a polynomial that makes it very efficient for finding the sums of powers.
I've talked about this before in connection with Al-Haytham, who found a formula for the sum of
the fourth powers. You can also ask for formulas for the sum of the fifth powers or the sixth powers.
And there are ways of using binomial coefficients, Pascal's Triangle, in order to find formulas for
the sum of consecutive powers. So we're looking at the sum of the fifth powers or sixth powers up to
a certain number. Bernoulli polynomials would turn out to be the most efficient way of doing this,
but it turns out there's much, much more going on with these Bernoulli polynomials. They would be
absolutely foundational. And they give rise to an interesting sequence of numbers known as the
Bernoulli numbers. And they're rational numbers. It's a curious sequence, 1 6th, 1 30th, 1 42nd,
1 30th again, then 5 66th, then 691 over 27 30, and so on. And we will see how these come up in a
fundamental way connected with Fermat's last theorem. We're going to leave Leibniz and the
Bernoullis at this point. In the next lecture, we're going to turn to Leonard Euler. Euler was
also from Basel, and maybe there was something in the water there. Euler is someone who would be
a student of Johann Bernoulli. And Leonard Euler would not only be his greatest student,
Leonard Euler would go on to become the greatest mathematician who ever lived.
