Almost everyone would like to be a strategic genius, especially if it required no special
investment of time or of effort.
But the mind is complex, and understanding how it reaches important decisions is the
subject of entire fields of science and social science.
In this lecture, we step back to explore some of the theories and practical applications
of those theories.
Our goals are to cultivate skills that can further improve our strategic decision making,
as well as avoid some of the pitfalls of irrational decision making.
We also learn what psychology teaches us about how the mind actually functions when making
decisions and how to identify and resist outside pressures that can distort our own
cognitive processes.
We can become better strategic analysts simply by avoiding psychological pitfalls that make
us bad analysts.
A strategic personality is more adept than the norm at developing powerful and effective
strategy, and we can identify those characteristics and we can cultivate them.
The good news is that keen analysis, which is so central to strategic thinking, can be
cultivated.
There are cognitive decision making processes that all people share to a degree.
These processes, once understood, can be adjusted to yield advantageous decisions more often
than before.
We can also learn to avoid the thinking traps that tend to lead us astray.
In turn, we can learn to craft strategy that has a greater chance of success by adapting
our strategy to the difficult waters that make up the perplexing world of human behavior.
The analytical process can help us discover what it is we do and why.
This is where the mind clashes with a confusing outside world and attempts to make sense of
it.
We learn the barriers to perception and the weaknesses and biases in thinking processes
and why accurate analysis is so, so very tough.
We become familiar with the group dynamics that yield suboptimal outcomes.
And finally, we learn analytical tools and techniques designed to reveal and to overcome
the distortions created by the various pathologies that affect our analysis.
Intelligence analysts think and analyze for a living.
They do this perhaps even more intensely and rigorously than many scientists in a laboratory.
The stakes can be much higher, a time crunch even greater, and the sources of information
and its sheer volume overwhelming.
Most important, they are attempting to divine the capabilities and intentions of a moving
target, other human beings acting in complex situations.
Analysts examine large amounts of data, combing them for clues, patterns, linkages, according
to a set of priority intelligence requirements.
Cognitive psychology has much to say about how we process data and analyze it, especially
when we're searching for particular intelligence that either corroborates or disconfirms our
hypotheses.
The analyst usually works with incomplete and often contradictory data.
There are gaps in the information and what information we do have, what may be ambiguous.
To fill these gaps, the analyst uses what we call judgment.
We use this word judgment often, but the ultimate nature of judgment remains a mystery.
One dictionary definition of judgment is the process of forming an opinion, evaluation,
estimate, notion, or conclusion by discerning and comparing as from circumstances presented
to the mind.
Analysts fill gaps in their knowledge with their judgment, and this is a quality built
up over time from experience, knowledge, consultation.
Judgment necessarily means going beyond the available information.
Newshewer notes that, quote, it always involves an analytical leap from the known into the
uncertain.
Most of us fancy ourselves cool and rational people when it comes to making decisions.
Sure, we'll make snap judgments on trivial issues, but where the big issues of the day
are concerned, we believe we are spot on.
And so, we investigate a bit, we accept the inputs as they come in, we strive to be objective
about the issue at hand, and we affect a cool and rational attitude.
We come to a conclusion based on the facts, and we make a decision or recommend a course
of action.
But the fact is, we don't think about thinking.
We don't think anywhere near enough about thinking.
And this is a basic finding of cognitive psychology.
We remain unaware of the brain's functions, even as it yields results to us.
We get our mind product without knowing its provenance.
We don't understand how the human mind does what it does as it copes with an incredibly
complex world.
Much of what we know about decision-making, it comes from researchers who study human
behavior in decision-making scenarios, both as individuals and in groups.
Research has demonstrated, above all, our ignorance of how we think.
How we perceive, what we remember, how we call upon that memory, and how we combine
it with new information.
These functions occur independently of any conscious direction on our part.
All we know, for certain, are the results of our thought processes.
One prominent cognitive psychologist, Reinhard Sulton, put it this way.
We may decide what to think about, but not what to think.
The results of thinking become conscious, but most of the procedure of thinking remains
unconscious and not even accessible to introspection.
We've tried to crack this nut of thinking about thinking, and one model of how we think
is that we are rational beings who seek to maximize our utility.
We consider a range of likely options, and we make the best choice.
For a long time, this was a fundamental assumption of much of economic theory.
But it's not really practicable, or even true, and most of us already know this.
But the influential social scientist Herbert Simon modified this assumption and introduced
us in 1957 to something he called bounded rationality.
Now, thanks to Herbert Simon, we have a way to articulate the fact that we don't make
purely rational decisions.
Simon's fundamental insight was that we don't consider all information at our disposal.
The world is too complex for that.
Instead, we decide in an environment of what he called bounded rationality.
This means that our rationality has limits.
These limits can be information, the cognitive limits of our minds and handling large amounts
of data, and the limited time available to us to make decisions.
Simon contended and subsequent research has confirmed that before our minds ever begin
to grapple with rational decision making, the brain automatically filters the incoming
information to make it more manageable.
It reduces the choices for us.
It does so on the basis of past experience, education, cultural values, role requirements,
and organizational norms, as well as by the specifics of the information received.
Our mental models don't always match up well to what's actually out there, particularly
in high-stress, high-stakes situations.
The mind simply can't cope with the vast sea of information cascading onto it, and so
the mind constructs a simplified mental model of that reality.
And that's what we work with.
Within the boundaries of this simplified model of reality, well, we act rationally.
This view of the mind as filter, an autonomous model maker, is widely accepted.
One of the most influential experts on intelligence analysis is Richard Heuer.
He put it this way, people construct their own version of reality on the basis of information
provided by the senses.
But this sensory input is mediated by complex mental processes that determine which information
is attended to, how it's organized, and the meaning that's attributed to it.
Ironically, the folks who study a subject most intently and become experts on the subject
are usually the most susceptible to falling into a reified mindset.
This mindset can color and control our perceptions.
Quote, an experienced specialist may be among the last to see what is really happening when
events take a new and unexpected turn.
When faced with a major paradigm shift, analysts who know the most about a subject have the
most to unlearn.
An example of this occurred within the CIA during the period of upheaval in Europe in
1989 and 1990, known as the Velvet Revolutions.
As the Soviet Union voluntarily withdrew from Eastern Europe, the reunification of Germany
was suddenly on the international agenda.
Specialists on the former East and West Germany were slow to see and understand the implications
of what was actually happening, both in Eastern Europe and in Russia, and how fast it would
happen.
In this case, generalists in the agency could see and accept what the specialists were missing.
The encrusted mindset of the specialists had not encompassed the possibility of rapid change
of this sort.
Such a mindset is just one of the problems blocking our clear understanding of what's
happening in the world around us, particularly the issues we focus on as most important.
A mindset, that's a set of assumptions or methods held and used by one or more people,
or groups of people.
A mindset can become so ingrained that it creates and perpetuates a powerful conformity
of approaches, of tools, and of unquestioned assumptions for analysis.
We can speak of groupthink whenever the mindset of a group of people becomes especially locked
in on itself, and in one sense, any mindset eliminates certain possibilities and hypotheses.
It squelches them before they can even emerge to be considered.
One of the most well-known is the Cold War mindset that led to myopia on the part of
the U.S. foreign policy establishment.
This myopia led to a framing focused almost exclusively on the U.S.-Soviet superpower
rivalry.
This two-actor rivalry colored conflict around the world, and because of this, anti-colonial
and nationalist movements were automatically assumed to be part of the Soviet architecture
for world revolution.
It fostered reliance upon two-player game theory to explain what was going on, which
actually obscured what was going on in what was then commonly called the Third World.
The Cold War mindset led the U.S. to ally itself with certain unsavory regimes in the
1950s, 1960s, and 1970s to counter what were perceived as aggressive Soviet moves in the
Middle East, Africa, and Central and South America.
This doesn't mean that mindset is inherently bad.
In fact, for any aspiring strategic thinker, the point is to cultivate a strategic mindset,
that is, a set of methods and assumptions most conducive to effective strategy.
But there exist a number of strong barriers to perception, and this is why accurate analysis
is so very tough.
Generally speaking, decision makers face major challenges with respect to uncertainty.
The human brain has trouble dealing with the natural fog of conflict.
On top of that, the brain has to parse out the artificial uncertainty that opponents
generate to deceive us.
And finally, even as we grapple with this welter of uncertainties, our own natural biases
and cognitive limitations impinge on our perception of these issues.
As you can see, we have several issues that can interfere with accurate perception.
This business of perception is really more complicated than most of us realize.
Barriers to clear perception can warp our analyses, can cause us to stumble, and lead
us unknowingly astray.
Group decision making dynamics make our task even more demanding.
Here are just three of the barriers to our understanding.
First, mindsets tend to be quick to form, but then resistant to change.
Second, we tend to perceive what we expect to perceive.
Third, new information is assimilated to existing images.
Our mindset.
We form mindsets quickly, but they are resistant to change.
Once we achieve a certain mindset, or we accept a certain paradigm with regard to a group
of issues, it locks us in analytically.
It can become a mind rut.
It's difficult to crowbar ourselves out of that mindset, even when we become aware of
inaccuracies in that mindset.
A common experiment that illustrates a mind rut is a graphic with several figures in a
row.
They are more distorted from the original than the last.
The sequence is carefully prepared so that the concluding image is actually an entirely
different subject than the original.
For example, when the subjects begin with the first figure, they might see an image
of a man's face, but looking at each in turn, at some point, the figure of a woman may begin
to be seen.
The gradual transitions can make it quite difficult to notice that the final image is
actually a quite different image, not just a distorted version of the original.
By contrast, subjects who are shown a single image recognize the changed image as a man
or a woman sooner than those who view the image in a context of prior expectations.
The mind rut suggests what makes sense and ought to be there, but it also suggests what
we should ignore.
In this way, our earliest experiences have already and mistakenly determined that these
new developments are mere warts and distortions on a pattern that we already know, and preventing
us from noticing new patterns that we ought to be noticing.
We tend to perceive what we expect to perceive.
As our analytical mindsets become rooted, we begin to take shortcuts in our decision-making.
We begin to think in repertoires, in routines, and standard operating procedures.
We think based on what we already know about a situation, and as a result, we have trouble
processing truly new information.
We interpret it and assimilate it with what we think we already know, our expectations
influence our perceptions.
In short, you see what you expect to see, and you don't see what you expect not to see.
Politicians, scientists, analysts, and businessmen retain these types of pre-existing image
images and beliefs even in the face of discrepant information.
We become vested in our mindsets.
Famous examples come to us from conflict.
In World War II, the Germans shot down an Allied fighter over the city of Aachen.
This indicated that the Allies had developed a long-range fighter plane to escort their
bombers over Germany.
But when informed of this information, the head of the German Luftwaffe said, I'm an
experienced fighter pilot myself.
I know it's possible, and what isn't?
I officially assert that American fighter planes did not reach Aachen.
I herewith give you an official order that they weren't there.
When Germany attacked the Soviet Union in June of 1941, Joseph Stalin refused to believe
the reports from his own units under attack.
He had been warned by U.S., British, and his own intelligence sources that an attack was
imminent.
He ignored them.
Belief in the paradigm of Soviet-German non-aggression was so strong as to overrule actual reports
of the attack.
Soviet units that reported the German attacks were not believed, and they were ordered not
to respond to provocation by firing back.
It took two full weeks for Stalin to process the disinit information that had challenged
his mindset with regard to Germany.
In that period, it's reported that he suffered a short-lived nervous breakdown.
As you see, expectations can be very strong.
We develop our expectations from many different sources, such as past experience, professional
training, cultural, and organizational norms, and sometimes just habit.
Look at these diagrams, each with a well-known phrase.
Now look again.
Did you notice the additional word?
It's more than likely you didn't, because we have been conditioned to see what we expect
to see and overlook that, which we do not expect.
We tend to assimilate new information to existing images.
The order in which we receive information can affect how it's used.
We receive information and we begin to process and to build our story.
Subsequent information is then added to the story and substantiates it.
In this way, subsequent information is treated not independently, but in relation to thought
processes and theories already underway.
When we receive information this way, in small increments, we tend to assimilate it into our
existing worldview.
This incrementalism can lead us to false conclusions.
This happened to the U.S. intelligence community prior to the 1973 Arab-Israeli War.
A possible conflict that U.S. intelligence had not anticipated.
Analysts were proceeding on the basis of the days intelligence received, rapidly assimilating
it with a narrative that was already constructed from the previous day.
This gave rise to an ongoing assembly line intel narrative, rather than a coherent understanding
of the big picture derived from systematic under consideration of an accumulated body
of integrated evidence.
Finally, the group dynamic that yields suboptimal outcomes is revealed.
What about group decision making and problem solving?
Several pathologies afflict decision making in groups.
The variables go up, with every additional person complicating what is already a very
complex process.
Political considerations enter into the deliberations, along with tendencies of groups to take on
dynamics of their own.
In other words, a group is much more than the sum of the individuals who comprise them.
People behave differently in groups and in group decision making.
They reinforce each other's inclinations.
Those who disagree tend to be forced out and replaced with team players.
Irving Janus investigated the effects of group dynamics on decision making and found that
concern for group solidarity and consensus seeking could degrade the quality of strategic
decision making.
Janus offered his examples of the Bay of Pigs invasion decision by John F. Kennedy's team
in 1961 and later the cover up of the Watergate burglary in 1973 and 1974 by President Richard
Nixon and his circle of advisors.
Both political crises exhibited symptoms of group think that led to a series of poor
decisions resulting in disasters in both cases.
These eight symptoms cluster around the overestimations of the group's power and morality, a tendency
toward close-mindedness and pressures to conform and achieve consensus, illusions of invulnerability
that create excessive optimism and encourage risk taking, unquestioned belief in the morality
of the group that causes members to ignore the consequences of their actions, rationalizing
warnings that might challenge the group's assumptions, stereotyping those who are opposed
to the group as weak or evil, biased, spiteful, impotent or stupid, self-censorship of ideas
that deviate from the apparent group consensus, illusions of unanimity among group members,
silence is viewed as agreement, direct pressure to conform placed on any member who questions
the group, couched in terms of disloyalty, mindguards, self-appointed members who shield
the group from dissenting information, and when the above factors are in play in group
dynamics, suboptimal decisions result, and these suboptimal decisions include incomplete
survey of alternatives, incomplete survey of objectives, failure to examine risks of the
preferred choice, failure to reevaluate previously rejected alternatives, poor information search,
selection bias in when we collect information, failure to work out contingency plans.
One typical pathological result of GroupThink is to settle on the first available alternative
instead of identifying the best available alternative.
We see how decision-making groups are structured and perform have a powerful effect on the
policies that come out of them.
Leaders can move groups such as committees toward either greater successes or greater
losses, depending on how the group is composed and how it takes in information, evaluates
options and eventually arrives at a decision.
When members of a group know how to avoid GroupThink, the group can come to a much better decision
for the group itself, as well as those whom the group represents.
We've seen many of the internal cognitive obstacles that obscure reality, hinder our
understanding and block clear analysis.
Analytical skill can be acquired with practice and the use of tools and techniques to overcome
these challenges.
Here at first is an analytical process model that aids our decision-making and ensures
that various courses of action are considered.
I followed this model with several specific tactics that can generate hypotheses that
may be outside your usual comfort area.
We use these analytical techniques to enhance our generation of plausible hypotheses and
to correct cognitive distortions.
These include situational logic, the application of theory, comparison with historical situations
and data immersion.
Moreover, the various techniques can be linked together to form an analytical method that
helps us think strategically and delve deeper into any issue we grapple with.
Defining the problem.
This is where we ask, what's going on here?
What is actually going on?
Are we defining the problem so that the answers yielded will make a difference?
Of course, this could often be the toughest part of the challenge.
That's why we have other techniques.
Generating hypotheses.
Suspend judgment and identify all the plausible hypotheses that should be considered.
Brainstorm and then trim the list to a workable number of hypotheses for more detailed analysis.
Don't screen out reasonable hypotheses only because there's no ready evidence to support
them.
Collecting information.
Collect information to evaluate all the reasonable hypotheses, not just the one you already have
in mind, the one that seems likely.
When you explore information supporting alternatives that have not been seriously considered before,
it can generate creative thought and new possibilities.
Argument against the hypothesis.
You'll probably find that much of your evidence supports your expectation of which is the
most likely hypothesis.
But that same evidence may be consistent with different hypotheses as well.
Here is where you develop arguments against each hypothesis rather than looking for reasons
to confirm them.
Hypothesis selection is not a popularity contest.
Proceed by trying to reject hypotheses rather than confirm them.
The most likely hypothesis is usually the one with the least evidence against it, not
the one with the most evidence for it.
Monitor for surprises.
When we commit something to paper or our computer screen, it tends to take on a fixed character.
Remember that we don't deal in a static situation, rather our world is rapidly changing, and
our analytical conclusions are always tentative.
The situation can change.
Or it may remain unchanged, but you receive new information that alters your understanding
of it.
Specify critical nodes to monitor for changes that might significantly alter the probabilities.
Pay particular attention to any feeling of surprise when new information does not fit
your prior understanding.
Remain poised to be proven wrong.
When we generate hypotheses, we find ourselves susceptible to coming up with explanations
that comport with our store of information dictated in a fashion by our own mindset.
Here are some ways to break out of the mind rut.
Situational logic.
This is an inductive approach that begins with consideration of concrete elements of
the current situation rather than with generalizations involving similar cases.
We regard the situation as one of a kind rather than as a class of events.
This has the virtue of enhancing details that make the case unique.
Application of theory.
This is a textbook-style deductive approach that reasons from the general to the specific
case.
It suggests to us that when a given set of conditions arises, certain other results
ought to follow with some degree of probability.
Comparison with historical situations.
We can compare our current situation with earlier historical examples.
An entire method of instruction in higher education is built on this technique.
It's called the case study method.
Historical cases are dissected to understand the event and how actors arrived at their
decisions, whether good or bad.
We seek understanding of current events by comparing them with historical precedents
in the same country or with similar events in other countries.
We can then fill in the gaps of our current knowledge by looking at the historical analog.
This has a danger when we apply an historical analogy indiscriminately.
For instance, after Neville Chamberlain's debacle at Munich in 1938, the appeasement
analogy was used for decades as a negative example that eliminated certain diplomatic
options in foreign affairs.
No one, of course, wanted to be accused of appeasement in dealing with other nations.
Finally, data immersion.
One method of analysis, if used judiciously, can aid the clarification process.
The analyst immerses himself in the data while consciously suppressing any preconceptions.
The idea is that patterns emerge from the data that can then be tested.
This method has problems in that data, numbers, facts, never speak for themselves.
But as a means of jump-starting original thinking, it can be useful if other techniques are overused.
Now, the world of cognitive psychology is always changing and pursuing new research
agendas into how the mind works.
There's always more to learn about how the mind functions when working on a problem.
For strategy, it's especially important to recognize and to resist both outside distortions
that can interfere with analytical clarity and ways that our internal cognitive processes
can also distort our perception of the outside world.
To see how we can be more conscious of how we perceive the world, reduce the degree to
which we misperceive it, and in the process, develop a more fully strategic personality.
Refining our practical understanding of the perplexing world of human behavior takes time.
But on that basis, we can learn to craft strategy and behave strategically in ways that have
a greater chance of success.
