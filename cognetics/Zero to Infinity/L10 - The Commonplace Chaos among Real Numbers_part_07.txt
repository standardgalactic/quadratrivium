Well, we'd guess that every digit from 0 to 9 would appear on average an equal amount
of the time.
In other words, each digit would appear about one tenth of the time, so we'd guess one tenth.
In fact, if we select a real number at random, we expect that one tenth of the digits in
its decimal expansion will be 0, one tenth of the digits in the decimal expansion will
be a 1 and so forth down the line and one tenth of the digits will be a 9.
Well to extend this idea further, let's now consider strings of two digit numbers within
the decimal expansion.
So let's look at two digits at a time.
Well how many, I mean how likely would it be to see any particular run of two digits?
Now what does that mean to generate two digits at random?
Well we have to imagine rolling a 100-sided die.
There's something you don't see in Vegas, it would start with 0, 0, that's the smallest
two digit number and it would go all the way up to 99, the largest two digit number.
And the question is if I were to generate a number at random, what proportion in that
run of digits would I see a particular two digit number as I look across?
Well first of all let me show you that these dice actually exist, this is one, in fact
this one actually makes a little bit of noise, it's almost like a maraca and it's actually
a 100-sided die, it almost looks like a golf ball doesn't it because it has little dimples
on it.
And if you roll it, this would generate two digits at random.
For example if I roll it once, let's see, hard to read but that looks like a 10, so
that's a 10.
Roll it again and I see a 43 and so forth.
Okay, so when I look at a random number, what's the likelihood that I'll see 43 and
how often will I see 43?
Well we might guess that because there are 102 digit numbers from 0, 0 to 99, we would
see in an average real number in its decimal expansion that it would contain the 43 roughly
one in 100 times because there are 100 equally likely possibilities for two digit numbers
and this is just one of them at random, so one in 100.
So in general for any two digit number we'd expect to see that that two digit number appears
about one in 100 times throughout this run of the decimal expansion.
Now of course we could consider the exact same issue with runs of three digits.
I don't have a thousand sided die but it's the same principle at work.
We'd have a thousand possibilities, the smallest three digit number would be 0, 0, 0, the largest
would be 999.
So the question would be how often would we expect to see the run of 0, 0, 0 appearing
in the decimal expansion for an endless real number, well we would expect it to be on average
one in a thousand because they're all equally likely.
Well real numbers for which every length of digits appears the appropriate or the expected
amount of the time in their decimal expansions are called normal numbers in base 10.
What does this mean?
