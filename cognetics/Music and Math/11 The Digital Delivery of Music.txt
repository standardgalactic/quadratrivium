Music
Welcome back.
We're nearing the end of our long journey that started with vibrating objects and their overtones,
and then we figured out how to construct scales and chords.
We used those to find some compositional techniques that all used mathematics.
But now, how does the music get to our ears? How is the music delivered?
Today we're going to talk about a little known subject.
It's very important, but underappreciated, the digital delivery of music and the mathematics of that process.
First, let's take a quick tour of how music was delivered through the years.
The original delivery of music was only in person.
I think it's really hard to imagine a world without any recorded music.
Prior to about the US Civil War, no music had ever been recorded to be played back later.
All of those performances of Bach, Beethoven, Mozart, all gone.
You wanted to hear Beethoven's ninth?
You got to hear it maybe two or three chances in your entire life because it had to be live.
People traveled around to hear concerts.
Hector Berlioz was famous for traveling Europe extensively, both to conduct, but also to hear concerts.
He was a particular fan of concerts conducted by Franz Liszt.
Early recordings were really scratchy.
The first known musical recording is actually not Edison, but it comes from 1860.
Edward Leon Scott de Martinville was singing Claire de Lune, and it was written to paper by something called a phonograph.
This recording was not re-read.
It was not heard in sound until 2008 from lines drawn on this paper.
It was reconstructed and played for us to hear.
Now, the first music recorded onto a replayable cylinder was 1888.
It was Handel's Israel in Egypt, and it was done for Thomas Edison.
It took 4,000 voices, and the sound from 4,000 voices is sounds incredibly faint.
It's really scratchy.
You can find this online, and you can listen to it.
It's oddly at the National Park Service website.
They moved on to better sounds, and there were important milestones that were achieved.
They moved from cylinders to flat discs around 1890 or so.
And that made multiple copies of a recording possible, much easier to produce,
and you saw the rise of the gramophone company.
Radio broadcasts of live music started around 1906, 1910,
and the recording and playback quality quickly improved.
Through all of this, no mathematics was needed.
The waveform was transferred directly to the medium that held the recording.
They were really limited only by the quality of the recording instruments and the playback instruments.
Let's listen.
Here we have a Victrola machine.
This is from right around 1905, 1906.
The amazing thing about this is it requires no amplification.
There's no electricity here.
It's just mechanical.
So let me get it going.
And let's listen to what it sounds like.
It's really amazing to me that this recording required no electricity at all to play.
There's nothing plugged in.
We've done some video trickery in this course, but we're not using any trickery at all here.
In fact, I can show you what the volume control on this is.
Here, I'll start it over.
That's how you control the volume on these old Victrolas.
Now, it's a wonderful thing to be able to record music and play it back,
but it actually degrades over time.
Every time you play this, it gets a little bit worse.
In fact, it's particularly bad.
This album is vinyl.
It's from after the 1950s when albums were made out of vinyl,
but we're using the original technology, the original needles come here,
and they're fairly heavy, and it actually degrades the vinyl.
It rips the vinyl off.
If you look closely on the end of that needle, you can see a little bit of vinyl on it.
What happens is the recording degrades over time.
Now, let's compare this with some more modern technology.
Let's play a CD.
So, here we have a CD.
It's the Bach Brandenburg Concertos.
It's brand new.
I'm going to unwrap it for us.
And this is how we play CDs.
We pop it into a machine, and we assume that it starts.
And there we hear Bach Brandenburg Concerto.
Now, this is brand new, right?
This came right out of the package.
You saw me open it.
It's digitally encoded.
It's all zeros and ones encoded on here.
While it's true that these zeros and ones never degrade like the album over there would,
it's also true that in the manufacturing process, there were over 50,000 errors.
More than 10 errors per second on this disc.
If you take a used disc, this one's nice and shiny and new,
but if you take a used disc, there are probably half a million errors.
Half a million errors on this disc.
Did you hear any errors?
I didn't think I heard any errors.
Let's see if we can hear any of the errors,
the probably 50,000 errors that are on the CD.
Now, I didn't hear any errors on that at all.
But let me do something, and I want you to think about it.
I mean, so I brought a knife with me today.
We all know if I took a knife to this album and I scratched it,
we all know what that would sound like.
You would get this annoying click every time it went around.
Let's take the CD, and here, I'll slice it.
So there, you can see I put a nice slice across this with a knife.
Maybe now that we've added more errors, we're going to hear it just like we would on the album.
Now this album has a lot more than 50,000 errors.
I scratched across every single line of data on this.
I don't know about you, but I think that sounds the same.
I didn't hear any errors on there.
Maybe that wasn't enough errors.
Maybe I didn't put enough errors on there.
Let me add some more errors.
Here's a permanent marker, and you can see the scratch there.
I'm just going to sign my name across here.
There's my name scratched across this.
Now I'm going to play it again.
Now this has many more than 50,000 errors.
I put tons of errors.
I'm all in a row.
Let's see what happens when I play this on this machine here.
That sounded perfect still.
I didn't hear any errors there at all.
In the days of the Victrola, you didn't need any math or any electricity to produce music.
In today's modern times, music is delivered digitally,
and digitally delivered music is full of mathematics.
That's what we're going to talk about today.
The three main topics we're going to talk about today are these.
We're going to talk about three different ways that mathematics has changed
the delivery of music in our digital world.
We're going to talk about how it's changed the notes that we hear,
how mathematics helps make bad singers sound good.
That's something called autotune.
We're going to look at how much music we can carry
and how we can fit more songs into smaller space,
and how we can stream more things wirelessly over the internet.
That's the mathematics of audio compression.
Finally, we're going to look at how much cleaner music sounds because of math.
The example that we just saw on a CD,
compared with records, cassettes, 8 tracks,
music sounds much better when it's delivered digitally.
Part of that is error correcting codes.
Those are the three topics.
We're going to talk about the notes we hear,
how much we can carry,
and how much cleaner the music sounds because of math.
Let's look at the first of those.
Now, this is a bit of a departure from our usual focus on classical music.
Autotune was first heard widely on Cher's album Believe.
It was an effect that was greatly exaggerated so we could hear it.
The technology comes from 1997.
It's from Anteris Audio Technologies,
and it was recently called one of the 50 worst inventions by Time Magazine.
So the idea is that even when a singer sings out of tune,
you can fix it digitally.
Now, originally, this was done just in the studio.
It took a long time to process this computationally.
At this point, they can do it live in performance.
When a singer sings out of tune,
the correct notes come out of the speakers
because they're fixing it so quickly.
There are other versions other than autotune,
but autotune has become what's called a proprietary eponym.
Think about things like Frisbee or Xerox.
And now, it's not just used to fix pitch.
It's actually also used for effects.
The rapper T-Pain uses autotune a lot
to give his music a particular quality.
There are two steps we're going to talk about in autotune.
The first is pitch detection,
and the second is pitch correction.
So let's talk through an example of this.
Now, I'm not a particularly good singer.
I can't control my voice. I haven't had any training.
But I can use this to demonstrate.
So here's the finale of Beethoven's Ninth
in German.
And now, I'm going to ask Gordy, our sound engineer,
to play that back to us with autotune,
and the computer is going to correct the pitches,
and you'll hear it sounds much better.
Exactly how does this work?
To do it live, it requires fast computers,
but it also involves some interesting mathematics.
And the two parts we're going to talk about
are the pitch detection and then the pitch correction.
So think back to lecture two on timbre.
We figured out that the waveform of a sound might look like this,
and these are the pressure changes of the sound wave.
We could also take the Fourier transform to get the spectrum,
and then we're looking at the frequencies.
Now, if you want to do pitch detection,
if we want to know exactly what pitch I was singing at,
we can do that. We have several options.
If we want to look at the Fourier transform,
we could look where that first peak is.
That should be the fundamental frequency.
But also the distance between those peaks,
because all of these sounds have this very arithmetic sequence of notes,
it's just the fundamental frequency added to itself each time,
the distance between the peaks
should also give us the fundamental frequency.
Finally, if we didn't want to go to the Fourier transform,
if we wanted to say on the waveform side,
we could look at how long,
how the distance between the wave repeating,
what that period is,
that should give us one over the frequency.
Any one of these methods will give you a singer's frequency.
So let's say somebody sings at 430 Hz.
Once we've done the pitch detection
and we know that she's singing at 430 Hz,
now we could ask,
should it be at 430 Hz or should it be something else?
Is she singing out of tune?
If we know the frequencies of the notes,
if we know what the frequencies of an A flat are
for an equal-tempered scale at about 415.3 Hz,
and an A natural should be about 440.
Well, the 430 that we're hearing is probably close to the 440.
So I think that should be a 440.
That should be upwards a little bit.
It should be a little bit sharp.
We should correct that note.
So how do we do the pitch correction?
Well, to do the pitch correction,
we think that we should go from 430 Hz to 440 Hz,
and so we should just add 10 Hz.
In other words, we should just shift the graph over 10 Hz.
That does not work.
Remember, the overtone series is multiples.
And so if the singer is singing at 430,
we're also hearing the overtone at 860, 1290, 1720,
all the multiples of 430.
And what it should be is multiples of 440,
which are 440, 880, 1320, 1760.
If we were just to add 10 Hz to her voice,
we wouldn't get the right sequence.
We would get 440, that would be correct,
but then 870, 1300, all of the rest wouldn't be correct.
The key thing here is to remember
that intervals are multiplicative, not additive,
and that tells us how to get our solution.
So to solve this, we should really be multiplying, not adding,
and to get from 430 to 440,
we're going to multiply by 44 over 43.
To think about this, we would take the spectrum
and we need to stretch it by a factor of 44 over 43.
Remember, when you do algebra,
g of x over 2 is the same graph as g,
except it's stretched by a factor of 2.
And so we can find the new spectrum in this way.
We can just stretch it by a factor of 44 over 43.
And now we sort of, in theory, have a process that will work.
At least this is one, a process that could work.
We start with the waveform, we get the waveform from the recording,
we compute the spectrum, we detect the frequency,
we compare that with a table of correct frequencies,
probably equal-tempered frequencies,
we multiply by a constant to correct it,
and then we have to invert,
we have to take the inverse Fourier transform
to get back to the waveform,
and now we can simply play that corrected sound.
So in theory, this is how we can fix the tune of anything like that.
In practice, it's a little bit more complicated.
There's some analog to digital switching that has to go on,
that's called sampling.
We'll talk more about that later.
We also have to split the voice up into individual notes.
When a singer moves from one note to another,
we have to redo all the computations,
and we have to figure out when exactly
she might be moving from one note to another.
We also have to make sure we don't change the initial attack.
Remember earlier, we took the attack off of a note,
and you couldn't hear it as a banjo,
but when we put the attack back on, it was a banjo.
That's telling us that the attack is very important,
and so we have to make sure we don't change that initial attack.
Finally, one of these computational issues is that
the Fourier transform and the inverse Fourier transform
are computationally very difficult.
There's actually something called a fast Fourier transform,
which is a little bit better, it works a little bit faster,
but all of these are much slower than you'd need
in order to do things live,
and so AutoTune does everything on the wave side.
It does what's called time domain pitch correction,
and they manage to do that fast
because there are no Fourier transforms involved.
But I hope this does give you an idea of the issues that are involved
and some of the mathematics involved in fixing pitches.
So that takes care of fixing the notes that we hear with AutoTune.
Let's move on to the second topic,
how math changes how much music we can carry or we can stream.
Now, the problem is you simply never have enough space.
My iPod seems to always be full even when I get a new one that's larger.
This is even more of an issue for streamed audio.
We don't want streaming audio, internet radio,
or the audio that comes with the streaming video.
We don't want to take up all of the bandwidth
of this pipe coming into our house of digital information.
The problem here is that recorded audio and video
just take up too much information.
If we recorded sound at its highest fidelity
and put that on a CD,
a CD would actually hold less than two minutes of music.
Imagine having to do that.
If you want to hear Beethoven's Ninth Symphony at 70 minutes long,
you would need 35 CDs.
Every two minutes you'd be switching CDs to hear the next part.
That would just be a complete mess.
The goal of audio compression is to reduce this,
to reduce the amount of stored information
while still minimizing the effect on the listening experience.
Now, this is a really interesting area.
It's sort of on the border among mathematics and psychology,
electrical engineering, computer science,
and there are a lot of incredibly complicated details
when you study this topic.
We're just going for the main ideas today.
First, we have to talk about analog versus digital signals.
The Victrola here uses a groove that's modeled on the waveform
so that when the needle goes through that groove,
it vibrates exactly like the waveform does.
Digital audio uses zeros and ones,
and we have to find a way to convert from this continuous waveform
to discrete individual points that can be represented by zeros and ones,
and that's called sampling.
Now, this is a really key decision
because we're going to sample this waveform at a bunch of points this way
and also at a bunch of heights,
and we have to decide how tightly we should space the points on the wave.
If we have more points, we're going to get a better sound.
If we have fewer points, on the other hand,
the sound might be worse, but the size of the file will be smaller.
The mathematics needed for this is actually something called the Nyquist theorem.
Harry Nyquist proved this in 1928,
and as far as I know,
Nyquist was actually the first person to prove this,
and so the name Nyquist theorem is appropriate in this case.
What he proved is that if you sample at frequency f
so that you're putting points every 1 over f seconds,
or you're putting exactly f points per second,
what you're going to do is you're going to save information on all waves
that have frequency less than f over 2.
If we think about this in terms of the Fourier transform,
if we think about decomposing this complicated wave
into its individual component frequencies,
you're going to retain all of the information on frequencies
that are less than half of the frequency you're sampling.
Now, the limit of human hearing is about 20,000 Hz.
The Nyquist theorem is telling us that if we sample at more than 40,000 Hz,
40,000 times per second,
that will accurately reproduce all sounds that are less than 20,000 Hz.
Just to be safe, we sample at a little bit higher rate.
The common sampling rate for audio CDs is actually 44,100 times per second.
It's a little higher for video equipment,
and that has to do with getting it exactly right with the frames of video,
about 48,000 for video.
Now, if we sample at higher rates, the audio is just not that different.
We can't audibly tell the difference between those,
and the file sizes become much bigger.
And so we have our answer.
We're going to sample at about 40,000 points per second,
and now we have to decide if we want to discretize this,
if we want to make this into zeros and ones.
We have to decide what are the vertical levels
that we're going to sample our nice continuous wave.
Because, again, the analog on the Victrola was a continuous amount.
You could put any amount on that wave as you want.
Digitally, we can only have certain levels because it's zeros and ones.
So how many output levels do we want?
We turned to psychology to figure this out.
The question we could ask is,
how many levels can we actually distinguish?
And when we do that, we come up with the rate for CDs is 16 bits.
Now, 16 bits doesn't sound like a lot,
but if you have 16 zeros or ones,
that's giving you two to the 16th different levels.
That's 65,000 different levels that you can sample at.
Once you have how many sampling per second you have,
and what the possible output levels are you have,
that gives rise to what's called the bit rate,
how much data per second of music.
If you want CD quality sound,
you have to have 1,400 kilobits per second.
1.4 million zeros and ones in each second of music.
If you want to fix the scratches and the bad data,
you have to stay tuned until later
and we get back to correction of these errors.
But if you want further compression,
if you want to make the file size even smaller,
it gets a little bit more complicated.
And that's called MP3 compression.
MP3 compression is one of the many different ways
we can compress music and make the file sizes even smaller.
MP3 uses perceptual coding.
You keep the parts of the data
that you think people will notice
and you drop the parts that you don't think they'll notice.
This is an area called psychoacoustics.
What can and can't you hear?
And it's really partly the ear and it's partly the brain.
Some of the issues that come up are the threshold of hearing.
What is the softest sound you can hear?
It turns out that the answer is different at different frequencies
and this curve gives you the threshold of hearing.
Now if a sound doesn't rise above this line,
then there's no use keeping it in a recording.
People won't hear the difference
because it's below our threshold of hearing.
If you want to drop it, you need to do a little mathematics.
You need to create what are called bandpass filters
in order to get rid of those.
There's another very cool thing they do with this,
which is called masking.
Say you have a loud sound at about 300 Hz.
That actually keeps you from hearing a soft sound at 320 Hz.
The louder sound masks the sound that's close in frequency.
This is much more pronounced for nearby frequencies
than when frequencies are far apart
and this is actually a biological effect.
It's the resolution of the auditory systems within the cochlea
that are giving rise to this phenomenon.
Let's listen to this.
The first recording we're going to hear is going to have pitches
and they're going to have one pitch and then two pitches
and then one pitch and then two pitches
and the frequencies of these pitches are going to be quite far apart.
As we go through this, the second pitch
is going to get softer and softer and softer
and what you'll notice is you can continue to hear these
and the reason you can is because they're far apart.
The second pitch gets progressively softer
but you can always hear it.
Let's listen to that.
Now what if those two notes were much closer in pitch?
That's when we get masking.
What we're going to hear now is that the higher note starts to disappear.
It's masked by the lower note
and after a while you're not going to be able to hear the higher note
even though they're at the same levels, the same loudness
as the example we just played.
Let's listen to that.
The MP3s that you might download use this phenomenon.
They calculate when will a sound be masked.
If it will be masked by a louder sound
which is close to it in frequency,
we can drop it from the waveform.
Every time we drop a sound, the file gets a little bit smaller.
Now perceptual audio compression is a very complicated subject.
It uses many different levels for these different uses.
You can have low values of compression,
you can have high values of compression
and it's a great example of the intersection
how different fields can work together,
math, music, psychology, computer science.
But let's hear the different levels of compression.
So when you import a CD,
sometimes it asks you what level of compression you want,
how big is the file that you want to save.
If you have less compression,
you're going to have better sound.
But bigger files, if you use more compression,
you're going to get worse sound and smaller files.
Here's a demo.
So here's a passage of Bach.
This is from the Gavotte in Partita No. 3.
Let's hear it first with no compression.
So this is essentially CD quality sound coming through.
And now let's hear that with a lot of compression.
So we're going to compress this a lot
and you should hear that the sound is much worse.
And now I want to let you hear the progression
as we go from no compression down to this last level,
a lot of compression.
So I'm going to play this piece five times in a row
and every time we're going to get a little bit more compression
and you should hear the sound getting a little bit worse.
The quality of the sound is getting a little bit worse.
The trade-off is that the size of the file
is getting smaller each time.
And now I want to let you hear the progression
as we go from no compression down to this last level,
where the file is getting smaller each time.
So that's a quick tour of some of the issues
involved in the compression of music.
Again, the whole point of this
is that you want to be able to fit more tunes on an iPod
or stream music without eating too much of your bandwidth up.
CDs have no compression other than the fact
that they've been converted from analog to digital
at 44,000 Hz.
And the amplitudes are set at 65,000 different letables.
Interestingly, on a CD,
since there's no perceptual compression,
cages four minutes and 33 seconds
would use the exact same number of zeros and ones
as any other four-minute and 33-second excerpt of music.
But there's another issue with CDs especially,
and that is errors.
And CDs use a very smart, really ingenious
bit of mathematics in order to fix the problem of errors.
And here I'm talking about scratches,
manufacturing defects, people signing these, things like that.
Now remember from the beginning
that even a very well-produced CD
can have about 50,000 errors.
If it's scratched, it would have more.
If it's signed, it has even more.
But somehow the disc plays perfectly.
We heard that at the beginning.
And that's the last topic of today,
the mathematics of CD encoding.
Now this has a technical name.
What's used on CDs is called
a cross-interleaved Reed Solomon code, C-I-R-C.
The main ideas in here are error detecting codes,
error correction codes, and finally interleaving.
And we're going to walk through all three of these.
Interestingly, some of this mathematics
you use every day and not just on CDs.
Error detecting codes, error correcting codes,
things like this come up when you're using the Internet,
when you're using credit cards,
when you're at the grocery store, all sorts of places.
So let's first look at error detecting codes.
Here's a conversation that happens all the time.
A representative answers the phone
and says something like,
thank you for placing your order at the teaching company.
May I have your credit card number, please?
Then the customer reads off a credit card,
maybe this one here, 3-1-3-2-4-2, on and on.
And the representative, as soon as the customer
is done reading that number, might say,
I'm sorry, could you read that again?
How does the representative know that they got something wrong?
Well, certainly if they just couldn't hear a number,
then they would ask that.
But sometimes if they've heard all the numbers perfectly,
if the customer made an error,
the representative knows that the customer made an error.
And the reason is the representative's computer
is using what's called an error detecting code.
The credit card number is wrong,
and the representative knows it.
Let's look at how exactly she knows it.
So here's how this works.
You take a credit card number,
and you double every other digit,
starting with the first digit.
So if this were our credit card number,
we would double the 3, and then the 3 again.
We would double the 4, the 3, all of these things.
We would double all these numbers.
And now if some of the doubled numbers might have two digits,
for instance, over here we doubled an 8, and we got 16.
And if we have two digits, we have to subtract 9 from that.
So instead of 16, we're going to replace that 16 with a 7.
And we need to do that any time we get a two-digit number
when we double it.
Now we take our long string of digits,
and we simply add them.
And in this case, we get 75.
Now if the answer to that had been a multiple of 10,
then we knew that the credit card number would check.
It would be possibly correct.
It might be exactly what the customer wanted.
If it doesn't add to a multiple of 10,
we know the credit card number is wrong.
At least one of the digits is incorrect.
Ours, in this case, we got 75.
That's not a multiple of 10.
And so we know that ours is wrong.
The salesperson's computer does all this computation immediately,
and simply tells the salesperson,
you need to get that number again.
This is called a credit card check digit.
And it catches all single digit errors.
And that's about 60% of all errors that are made on a credit card.
In fact, it catches almost all neighboring transpositions.
If you take two numbers and switch the order,
that accounts for another 10 to 20% of all errors.
Together, this check digit is catching 70, 80% of all errors made
when you read a credit card number to somebody else.
The information rate, on the other hand,
the first 15 digits are actually encoding important information.
It's just the last digit, which is called a check digit.
And so our information rate is 15 out of the 16 digits we use
contain actual information.
The last digit is only the one that's extra.
Check digits like this save companies and customers
valuable time and money.
And there are lots of other uses of error checking codes.
Airplane tickets.
The UPCs, when you're reading items at a shopping center,
sometimes you'll hear a beep, beep, beep.
It gives you a different beep because it read it incorrectly.
And how does it know it read it incorrectly?
Because it's doing an error checking algorithm
and finding out that it didn't check.
And so it asks you to re-scan.
Bank routing numbers include check digits,
ISBN numbers on books.
The VIN numbers on cars include them now,
but they've only included check digits since 1981.
Internet communication includes check digits
in the packets that you're sending back and forth.
Now, that's not good enough for music.
Error checking isn't good enough to fix things on a CD.
If you check and find that there's an error,
what are you supposed to do about it?
Detecting errors isn't good enough.
Error correcting would be much better.
To understand error correcting, we have to go back to 1947.
Richard Hamming was working at Bell Labs in 1947,
and he had access to a very early computer on weekends.
Now, there was a long line of scientists
who were really eager to try out their programs
on this new machine, this new computer.
And what they would do is they would put their programs
on cue over the weekends,
and they would have these programs just run.
And if there were an error in your program,
the computer would detect the error and it would just stop.
You'd be kicked out of the line,
and it would start the next scientists' program.
You would have to go back, fix it,
and go back to the end of the line
and run your computer program again.
Hamming thought if the computer could detect the error
and fix it, then the program could just continue running.
There would be no stopping.
The advantages of this are that it would find and fix the errors.
The downside was the information rate.
In his original system,
only four out of every seven pieces of data were true data.
The other three were checked digits.
So let's look at how his system works,
and let's look at it in the example of sending a short message.
So digital messages are all zeros and ones.
Letters, sounds, video, audio,
they're all encoded as just zeros and ones.
So imagine sending a very simple message,
just a four-digit message, one, zero, one, one.
And imagine that you transmit this message
to somebody else in some way, and they read it.
It's sort of like the kid's game, telephone,
where you keep telling a message to one kid after another,
and it goes down the line,
and you know that sometimes the message gets badly garbled by the end.
Error correcting codes help get around these problems
that you have in telephone games like this.
So let's try to send our message.
Our message has four digits, one, zero, one, one,
and we're going to append three digits to the end.
Those are the checked digits,
and we have to tell you exactly how we're going to append these digits.
So we take this diagram and we put the original message,
those four digits, into regions one, two, three, and four in order.
And now we're going to add digits into regions five through seven
in such a way that each one of these three circles has an even number of ones.
Let's look at region five.
In that circle there are two ones already,
so it must be a zero so that there's an even number.
Region six, there are three ones in that circle,
and so it must be a one in region six.
In region seven, there are two ones, and so it must be a zero.
So now we've figured out our checked digits.
Our checked digits are zero, one, and zero,
and now we can put all seven digits back in order,
and we get what's called a block.
This message is encoded in a block,
and the block reads one, zero, one, one, zero, one, zero.
Now let's think, what happens if there's an error in transmission?
So you send this block to a friend, one, zero, one, one, zero, one, zero,
and through this telephone game your friend reads one, zero, zero, one, zero, one, zero.
So we know there's an error, but your friend doesn't.
But what can your friend do?
So your friend puts those numbers into the same chart and then checks the circles.
Do the circles all have an even number of ones in them?
No.
In this particular case, the top circle and the left circle both have an odd number of ones.
And if there's only one error, then that error has to be in region three.
That's the region that is in exactly the circles with an odd sum,
and it's not in any of the circles that has an even sum.
Because of this, we know that region three should be, the zero that's there should really be a one.
We fix that, we write out the block, one, zero, one, one, zero, one, zero,
or rather your friend does this writing,
and your friend has detected the error and corrected the error.
That's an amazing bit of mathematics.
There's another way to think about this, and that has to do with distance in some space.
Remember the received block was one, zero, zero, one, zero, one, zero,
and that was not a valid block.
It was not correctly encoded.
The correct block was one, zero, one, one, zero, one, zero.
Now not every sequence of seven zeros and ones is a valid block.
How different are two valid blocks?
Well, if we think about the distance as measured by the number of digits in which they're different,
then the distance between two valid blocks is always three or more.
The distance between what we sent and what our friend received was just one.
Those messages were different only in one of the digits.
And because of that, we knew that it had to be wrong.
The message that was received was one, a distance one away from a correct message.
What correct message was it?
Well, the one that we sent, and that's how the error correcting went.
If you wander too far away from a valid block, more than one digit,
then things might be a problem.
But if you only wander one digit away, then you can not only detect the error,
you can correct it.
Let's look at what happens if it's more than just one error.
Say, suppose we send the original message we did before,
but now our friend incorrectly reads it with two errors.
One, zero, zero, one, zero, one, one.
Now, when your friend puts the numbers into this figure,
the bottom left circle has three ones in it.
Now, the easiest fix, if we assume it's just one error,
then it must be an error in region six.
We must change that one to a zero.
And when your friend fixes that, they get one, zero, zero, one, zero, zero, one.
And our method just screwed up the message a little bit more.
What we're seeing here is that this method cannot detect or fix two or more errors,
but it does detect and fix one error.
Hamming codes, more precisely the seven, four Hamming code that we've seen,
have a message length of seven set inside of a block length,
I'm sorry, a message length of four set inside of a block length of seven.
That gives an information rate of four sevens.
About 57% of the digits that are sent are actual data,
and the others are checked digits.
Now, this particular system can check one error,
and it can also correct one error.
And the distance between valid blocks is three.
You can extend this and do more complicated mathematics,
and you can get what's called an extended Hamming code,
which checks for two errors and correct for one.
Now, the next step in encoding a CD is called interleaving,
so let's look at that.
You see, CDs have a particular way that they have errors.
Sometimes errors are randomly placed,
and that might be the manufacturing process,
but sometimes when you scratch a CD,
it corrupts a lot of the zeros and ones all in a row,
and the solution to get around this,
to not lose that bit of music,
is to intersperse the data,
to spread out the data from any one particular moment in music
in a lot of different places.
Let's look at an example of this with letters.
So suppose that my message is,
math is my favorite, that's a great message,
and suppose during the sending of this message
we lose four letters in a row.
Boy, you know, our message isn't clear.
We can't really figure out what that message is going to say.
The solution is to interleave the message,
and let's look at how we do that.
We put these letters in a 4x4 grid horizontally,
but then we send the message vertically.
So let's interleave math as my favorite
and see what it looks like.
Here we put the message in horizontally,
and then we read down instead of across.
M-I-F-R-A-S-A-I.
It doesn't look like much of a message,
but when we send the message,
if we were to lose, say, four letters in a row,
this is called a burst error.
Think about a scratch on a CD.
We put the remaining letters in a grid down,
and now we read across.
And what we've done is we've spread out those errors
into different parts,
and so we can easily read this as math is my favorite.
There's no problem because we spread out the errors.
So let's put error detection, error correction,
and this interleaving together to see how a CD works.
So let's think about encoding these 16 digits on a CD.
Now, the first thing we're going to do
is we're going to split it up into groups of four,
and so now we have four groups of four,
and now our first try might be to add check digits
to each group of four.
So each group of four, we can run through the Hamming code,
get three additional digits, and it would look like this.
Now, we can write that in a 4x7 grid and interleave,
and that would be a good way to do things,
but we can do better, and that's called cross-interleaving.
Again, the goal is to encode these 16 digits,
and so we do what we did before,
we split them into groups of four, we add three digits,
and now we write them in a 4x7 grid,
but before interleaving,
we're going to add three additional rows at the bottom,
and then we're going to read the columns,
and we're going to add three check digits using our Hamming code
in order to figure out what those digits should be.
Now we have a 7x7 grid, and now we interleave that,
and so now we read those vertically instead of horizontally.
This would be a cross-interleaved Hamming code.
Let's see how it deals with errors.
So let's introduce two kinds of errors.
Let's suppose we send this 49-digit message,
and we have some errors that are single-digit errors.
These are sort of random, think about manufacturing errors,
where a one should be a zero or the other way around,
but we also have this big scratch here in the middle,
and it's completely unreadable.
We don't know if those are zeros or ones.
What I've demonstrated here is a huge error rate.
It's about a 20% error rate.
Is it fixable?
Let's try to decode it.
So we write the data in the grid,
and now we check the columns,
and we check them with our Hamming code.
Column 5 checks out perfectly,
but columns 1, 2, 6, and 7,
there's one error in each one of those,
which we can detect and correct using our Hamming code.
Columns 3 and 4, there's too many errors in there.
Our Hamming code doesn't help us,
and that's where we use the interleaving.
So now instead of reading the columns, we read the rows,
and now in each row there's at most one error,
which is detectable and correctable.
We correct those errors,
we reverse the interleaving,
we keep only that top block, the 4x4,
which were the 16 digits,
removing all of the rest, which were check digits,
and now compare it with the original.
That was amazing.
We get the exact original message back,
precisely without a single error.
Let's review what exactly we just did.
We took our 16-digit message,
we added check digits in one direction,
we interleaved that and added more check digits,
and then we managed to correct a sent message
with a transmission error rate of 20%.
That's really phenomenal work that we did.
I want you to notice how both the Hamming code
and the interleaving played a really important role
in doing this.
Now it turns out that CDs are a little bit more complicated
than that.
Instead of using a Hamming code,
it's called a Read Solomon code.
I want to just give you a quick idea of the ideas in that.
A Read Solomon code replaces the single digits
that we were just talking about with groups of 8 digits.
And because that, a Read Solomon code
is really working in a particular group.
It's actually a field called Z256.
The resulting Read Solomon code
can detect up to 3 errors,
and it can correct up to 2 errors.
And that's what we need in CDs.
So now we have the basic ideas in place.
We use a Read Solomon code, we use cross-interleaving,
and then there are a couple of extra things
we have to deal with.
There are two channels.
We listen to things in stereo now.
So there's a left and right channel,
and those have to be interleaved.
There's extra data like track information timing.
There's some technical issues.
But all of these are taken into account in coding a CD.
How good is this system?
Well, remember a reasonable error rate
was 50,000 to a half million errors on the disk.
That's out of about 20 billion bits.
All of these are corrected.
On a CD-ROM, if you think about it,
all of them have to be corrected,
because otherwise programs would crash.
Now, if we have a scratch,
if we have a burst error,
not like an individual manufacturing error,
if we have a burst error,
a burst error 2.4 millimeters wide,
that's 3,500 bits in a row,
can be completely corrected
with this amazing algorithm that we have,
this cross-interleaved read Solomon code.
You know, I was lucky.
If I had signed with a thick Sharpie instead of on a thin one,
then it would have probably skipped
instead of playing perfectly.
So what I hope is that you get the ideas
that mathematics is packaged
into all of the music that we have.
Mathematics is encoded somewhere
in all of the ways we deliver music to e-digitally now.
Every CD is encoded the same way.
The brilliant mathematics that people have invented,
Hamming, Read, Solomon, is used
so that this disk still plays perfectly when I put it in.
And it's not just CDs.
These ideas are used for communicating
in deep space missions, satellite TV,
even storing data on your hard drive.
All of these involve error correction, error detection codes.
In all of today's mathematics,
in all of today's topics,
there was mathematics helping improve the musical experience.
We learned how mathematics is used to fix the notes we hear,
how we use mathematics to increase
how much music we can put on a device,
and how we use mathematics to not just detect
but correct errors in CDs and other communication.
Next time is the culmination of our journey,
the culmination sort of in performance.
When music reaches our minds,
and we're going to talk about math music in the mind next time.
Thanks for joining us.
Let's hear a little bit of error corrected Bach to take us out.
