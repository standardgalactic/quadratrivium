This is lecture 32.
We're going to take a different tack today from the one we did last time.
In the last lecture, we discussed how several key individuals
popularized the increasingly visible results of natural science
to promote a philosophical world view of scientific materialism.
People like Ludwig BÃ¼chner, Jakob Moleskow, and Karl Vogt
all were generally aware of the great strides that were being made in natural science.
They were aware of the advances in electromagnetism,
which by the 1840s were beginning to become visible even to laypeople,
and they certainly were aware of achievements in astronomy
that were the talk of the time.
I'll catch you up on those in our next lecture.
As for the popular accounts of natural science from outside Germany,
they knew of Chamber's book on the vestiges of the history of natural creation
because Vogt translated it into German in 1851.
But their message had a German flavor to it.
I mean that it was more classically philosophical.
It focused on the nature of reality and how we should go about trying to discover its laws.
They were essentially preaching a sermon of metaphysics.
There's were the big questions.
What is the nature of reality?
What is life?
And how do we get a truth?
Well, their assumption was that reality is simply molecules in motion.
Nothing more.
So once we know the laws governing the motion of molecules,
we'll know all that can be known.
And as we saw last time, the negative implication of all this
was that anything that supposedly eluded capture by the motion of molecules,
like an immaterial soul, for example, that simply didn't exist.
The attitude of the scientific materialists was,
let's get real here.
All there is is matter.
That's what the body's made of.
And when the body's no longer able to keep the exchange of matter that defines life going,
it dies.
A different exchange of matter called disintegration takes over.
And the physical matter of the body rots away.
And that's it.
There's no soul left over.
It's up to this.
It's what natural science has made clear.
And we also saw that this attitude of the scientific materialists
about getting real was just one example of a broader mood
in European society after mid-century.
This whole period has been called the age of realism.
What we want to do today is to see how this attitude of realism,
both benefited and challenged scientists as they pursued deeper knowledge of matter,
as they sought to become clear about the laws of molecules in motion
that the scientific materialists felt held the keys to the future.
We're going to see that it wasn't quite as straightforward as Buchner and the others imagined it to be.
There would be some unexpected obstacles lying in the path to knowledge of molecular motion.
For example, the scientific materialists assumed that the laws they envisioned,
the ones that would someday be discovered and give us ultimate knowledge of nature,
these laws would be like the laws of nature uncovered so far.
The ideal for those laws were the ones Newton had discovered for the heavens.
But what if that kind of law simply couldn't be achieved where matter was concerned?
So that's our agenda for today, to see how this pursuit about the laws of matter played out.
To accomplish that, however, I need to do what we've occasionally done before in this series.
That is, I'm going to back up a bit and review for you what had been achieved
since we talked about the nature of matter last.
That was back in Lecture 6 when we dealt with the chemistry of Lavoisier.
In the era after Lavoisier, that is, in the early 19th century,
there were already hints that the laws of matter might be different from those Newton had found for the heavens.
That's not to say that the investigators of the time weren't trying to find Newton's kind of law for, say, chemical affinity.
Affinity described the way one kind of matter reacted with certain other kinds, but not with just any matter.
In the 18th century, much of chemistry was dominated by what historians have called the Newtonian dream.
The idea was to discover the mathematical force laws for other disciplines like Newton had for celestial mechanics.
What he'd done was to find what forces were involved and then he'd discovered mathematical expressions for them.
So where Newton had described the laws governing the gravitational force between heavenly bodies,
chemists should likewise find the laws governing chemical forces.
The forces matter exhibited when chemical reactions took place.
Now the assumption was that these forces would be very short-range forces.
They wouldn't be acting over much of a distance since they would be acting between the parts of matter,
the parts of the chemical reagents that were brought together when an interaction occurred.
But I have to tell you that this effort to realize the Newtonian dream in chemistry, it came up short in the 18th century.
Yes, there were new tables of affinity drawn up and a few general attempts to define something called stoichiometry,
measuring the quantitative proportions or mass ratios in which chemical elements stood to one another.
But in spite of various attempts to quantify short-range chemical forces,
no one had been successful in uncovering a general result as the century came to a close.
There were just too many different kinds of reactions.
Were you supposed to come up with a different force law for every possible reaction?
Then the game changed.
Chemists in France and England began to pursue a different agenda in chemistry at the turn of the 19th century.
And it was because they began asking a basic question about how chemical substances combined.
The question was, when chemical substances combine to form a composite,
do they always combine in the same proportions?
Or can the proportions vary?
In France, the question produced a debate.
On the one side stood a chemist named Joseph Proust,
who asserted that when two chemical substances combined, the proportions were fixed.
On the other side was Claude Bertollet.
He argued that chemical substances could combine in an infinite variety of proportions
to produce composites with different properties.
Now this may sound to you like a simple question to decide.
Just try various substances and see.
But the debate was in fact not that easy to resolve.
You see, Bertollet appealed mainly to substances in solution,
while Proust illustrated his claims with solid substances.
So Bertollet seemed to be able to demonstrate his case, but so did Proust,
which of them was right in general.
Over in England, a few years later, a Quaker named John Dalton
became fascinated with a related question.
It had to do not with solids or liquids, but gases.
Dalton got to thinking about the different gases that made up the atmosphere.
Why did they remain mixed together instead of separating out in layers?
It's an intriguing question.
Back in the 18th century, Lavoisier had wondered about that too.
He'd asked himself whether, in fact, there were various levels
into which common air, as he called it, might separate out into various components over height.
Well, in order to explain why he thought the components of air remain mixed,
Dalton came up with an idea.
Suppose the gases that made up common atmospheric air were each composed of particles.
And suppose these particles repelled themselves selectively.
That is, the particles of each kind of gas repelled themselves,
but they didn't repel the particles of other gases.
So the particles of oxygen repelled other oxygen particles,
and the particles of nitrogen repelled other nitrogen particles.
That would indeed provide a vehicle for mixing up all the particles of the various gases present.
It would result in a mixture that would stay mixed.
With this general approach to gases, that is, that they're made up of particles,
Dalton decided he'd extend this approach to all elementary chemical substances.
It might give him a way to determine the number and weight of elementary chemical substances
when they entered into combination.
So he started with a well-chosen group of reactions.
He knew that sometimes you could get different compounds from the same elementary substances.
Depending on how much of the substances you used.
So he studied such cases, and he came to the conclusion that the proportions
in which elementary substances combined were in fact fixed.
Not only so, he found that in many cases the proportions were simple multiples of one another.
Let me give you an example.
Dalton found that two different compounds, ethylene and methane,
were both formed when carbon and hydrogen combined.
But when he went the other way, when he took the ethylene and methane apart,
he got twice as much hydrogen from the methane for a given quantity of carbon than he did from the ethylene.
So this is where Dalton became bold.
He now took the step of suggesting that the elementary units of carbon
were all alike in weight and figure, that there were atoms of carbon.
And if that was true for carbon, it was true for all other elementary substances.
With this foundation, he was now in a position to formulate a general claim about the way elementary substances combine.
He stated what has become known as the law of definite proportions.
That law says that the number of combining atoms of the different elements
form simple definite ratios when they form compounds.
So when atoms combine to form compounds, they don't do so in a haphazard way.
No, only certain numbers of one atom combine with certain numbers of other atoms.
With this result in hand, Dalton was now able to determine the relative weights of the atoms that combined.
For example, he believed that water was made of one atom of hydrogen and one of oxygen.
And when you dissociated water into these two gases, you found that the oxygen you got weighed seven times the hydrogen.
So an oxygen atom must weigh seven times a hydrogen atom.
Now these were early results. They don't conform to the present day relative weights we have for oxygen and hydrogen.
But that's not the important point here.
What is important is that Dalton had taken chemistry in a new direction.
He'd abandoned pursuing the Newtonian dream.
He was not trying to quantify the force mechanism involved in chemical reactions.
It's not that he'd given up trying to build to bring a quantitative dimension to chemistry.
On the contrary, he did do that, but it was a different kind of unit he was quantifying.
It wasn't the forces involved.
It was the relation among the amounts of elementary substances that reacted to form compounds.
This new direction began to bear additional fruit over the course of the century.
One way a new idea can show its merit is when it generates healthy debate as the idea is pushed by others.
In this case, there appeared new ideas in atomic theory in France and in Italy that created a difference of opinion.
In France, Joseph Gay-Lussac argued, based on experiments, that when gases act on one another,
as they do, for example, when water forms from the combination of hydrogen and oxygen,
then the volumes of the combining gases will exhibit simple ratios.
In the case of the formation of water, he found that two volumes of hydrogen combined with one volume of oxygen.
Now Dalton objected to this claim of simple ratios by volume.
He'd reasoned that you could determine the relative weights of elementary substances,
but he'd assumed that the sizes of their atoms were not equal.
It seemed to him that Gay-Lussac's result required that these sizes be at least proportional to each other,
an idea he refused to entertain.
Why would they have to be proportional?
There were several points, in fact, at which Dalton's assumptions and the work of Gay-Lussac didn't mesh.
But things were helped along a bit by the work of Amadeo Avogadro in Italy.
He suggested that gases are not necessarily made up of single atoms, as everyone seemed to be assuming.
They might be composed of two or more similar atoms, united into what he called a molecule.
Dalton was not won over.
He opposed this idea since Avogadro hadn't explained what held the so-called diatomic molecules together
without causing the gas to condense.
But Avogadro nevertheless concluded that equal volumes of gases under the same conditions
possess equal numbers of molecules, a law that still bears his name.
Avogadro's law may be something we recognize today, but it wasn't something widely accepted at the time.
Recall that many still regarded heat during these years as one of nature's imponderable elementary substances,
albeit not a gross material substance like oxygen.
It would take some time before all these conclusions were sorted out.
In fact, it wasn't until much later in the century after the concept of caloric had been abandoned.
Then, as we'll see in a moment, it was the physics of gases that claimed widespread interest.
There was another aspect of chemistry that I need to mention.
It has to do with a question that paralleled the one we examined from this era about forces.
Recall that many had asked whether there might be one basic force of which all the others were derivative forms.
Ersted had even suggested that the fundamental force was likely chemical.
The comparable question for matter was whether there was a basic element.
Perhaps nature had one fundamental particle, and the rest were all built from it.
It was in the second decade of the century that the English physician, William Prout,
suggested that the atom of hydrogen was the true fundamental particle.
The atoms of all other elements were combinations of different numbers of hydrogen atoms.
This implied that atomic weights should be whole multiples of hydrogen's weight,
a result that early on seemed confirmed by Dalton's law of definite proportions.
If elementary substances combined according to definite ratios,
then didn't it make sense that the reason was because there was a fundamental unit that combined in the different ratios?
But then, as the years passed, more and more experimental results indicated
that very few atomic weights were exact multiples, so they couldn't really be multiples of hydrogen.
Prout's hypothesis fell out of favor if it had ever been in favor.
But there were other intriguing results to ponder.
One area that began to attract attention was the relations among the properties of elementary substances,
and the list of elements was growing from the 30-something elements known to Lavoisier and Dalton to the high 40s by 1815.
Chemists began looking for similarities in chemical properties among different elements.
For example, fluorine, chlorine, bromine, and iodine have very different atomic weights,
but all combined with metals to form white crystalline salts.
Was this just a coincidence, or was there some underlying reason for the common property?
Of course, as this kind of investigation continued, some patterns seemed to be purely numerological results.
By that I mean that some similarities were identified that were mere coincidences.
For example, in 1829, Johann Dorberiner identified chemical families in which the atomic weight of one element
had a value equal or close to that of the average of its two immediate neighbors.
Fascinating, but no one could find a physical meaning for this curious property.
And that meant that this business of finding patterns and periodicities among the elements was a risky business.
How could you tell when what you had found was worth anything?
In 1869, a Russian chemist, Dmitry Mendeleev, published a book on the principles of chemistry
in which he arranged 60-some elements according to increasing atomic weights.
He formed a new row when he came to elements that displayed similar properties to an earlier element.
But once again, it wasn't an exact science. Sometimes he had to leave gaps
because no element had been discovered with the atomic weight that would exhibit the expected property.
Further, Mendeleev's values for atomic weights were not always in agreement with those determined by others.
So as you can imagine, there were critics.
Even though you had to admit that what he had put together was curious,
still no one understood why the table worked as well as it did.
One critic even asked if anyone had tried arranging elements in the order of the initial letters of their names.
But relationships Mendeleev's periodic table displayed couldn't really be denied.
Chemists were not about to stop arranging elements in periodic tables.
Well, how did the physicists react to what the chemists had been doing?
Physicists had always been interested in the behavior of matter.
By the middle of the 19th century, their attention was focused on something called the kinetic theory of gases.
The kinetic theory of gases is the explanation of properties of gases
based on the assumption that atoms and molecules move freely through space
and are not confined to motions of vibration around fixed positions.
It brought chemical atomic theory together with physics
and it added yet another instance of extending physics beyond the classical Newtonian outlook.
Chemists had let go of the Newtonian dream by turning their attention away from the pursuit of short-range forces.
Physicists working on the kinetic theory of gases would find that their techniques would also move well beyond what Newton had envisioned.
In this connection, we once again encounter the work of Rudolf Klausius when we met in lecture 30.
He was a key figure in the development of kinetic theory.
I didn't tell you much about Klausius when we discussed his treatment of Carnot and Joule.
Let me rectify that now at least a little.
He was born the son of a minister in a region of Prussia that is now part of Poland.
As a student, he thought about studying history but decided eventually to pursue a degree in physics and mathematics.
He got his doctoral degree in 1848 from Hala University
and worked on the mechanical motion of the particles of gases in connection with his studies in thermodynamics in the early 1850s.
Like James Joule, he was convinced of the mechanical theory of heat
that the heat of an object was due to the motion of elementary particles that made it up.
What Klausius wanted to explore, however, was the motion of the particles.
In particular, what more can we know about it? How can we describe it?
Most everyone had assumed that the motion of particles that caused heat was not a free motion of the particles through space.
It was conceived of as vibrational motion.
And that was true even for the motion of molecules in a gas.
There were a few before 1850 who pursued the idea that molecules in a gas moved freely,
that they had the kinetic motion of translation.
But these ideas didn't capture much attention.
Now, as I mentioned, Klausius adopted a kinetic theory of gases in the early 1850s
in conjunction with his work on thermodynamics.
But it wasn't until 1856 when a German chemist named Koernig
published a rough depiction of the motion of gas molecules that Klausius was spurred into action.
It was just the kind of question Klausius had been working on for some time.
So a year later, in 1857, he published what became a fundamental paper on the subject.
It was entitled, On the Nature of the Motion We Call Heat.
Klausius agreed with Koernig that gas molecules moved freely through space at constant velocity
until they struck other molecules or the sides of their container.
But Klausius took this mechanical model of the motion of gas particles more seriously than had Koernig.
I mean that Klausius noted that just because you gave molecules free movement of translation through space,
that didn't mean that they also didn't have other motions as well.
The molecules were rotating and even oscillating with respect to each other.
So if you have a gas in equilibrium, he asserted, then you can account for its total energy with all these motions together.
Although he had no way of actually calculating the individual energies,
he believed that the total energy of the system was made up of these various kinds of motions of the molecules.
Using the model, Klausius correlated various properties the gas exhibited with the motion of its molecules.
For example, the pressure the gas exerted was due to the molecules striking against the sides of the container.
And he explained how a liquid could change its state and become a gas as well.
Here's how he did that.
He said that even though the average speed of the molecules was too slow for them to overcome the attractive forces
of other molecules and escape from the liquid surface,
some individual molecules moved exceptionally fast and did escape from the liquid surface.
In 1860, a Scottish physicist named James Maxwell contributed to the further development of kinetic theory.
In an important paper, Maxwell developed further this idea of an average speed of the molecules.
He applied the emerging mathematical study of statistical variation to kinetic theory.
And with this move, Maxwell helped to lay the groundwork for changing how scientists might have to think about nature.
Oh, it would take some time before the implications would settle in.
But using statistics to describe nature, carried with it the implicit claim that maybe, at least in some of its aspects,
nature was more accurately depicted statistically than with the kinds of equations we're used to seeing.
I'll come back to this at the end of the lecture.
Right now, let me say a little more about what Maxwell did.
Maxwell was aware of the law of normal distribution that had been developed by Carl Friedrich Gauss in Germany.
You know the law I'm talking about.
We all do.
It's the famous bell-shaped curve that shows the frequencies of the deviations from an average value.
Students know this curve from the frequencies of grades on an exam.
The middle of the curve, the top of the bell, represents the average.
That's where most of the individual values fall.
But around that mean there are distributed other values that deviate from the average.
How far they deviate, the width of the bell shape measures what's called the standard deviation.
Gauss had worked this out a little earlier in the century.
So what Maxwell proposed was this.
He suggested that the speeds of molecules in gases were similar, not identical, but similar to Gauss' law.
So in a gas, you have molecules that are moving at various speeds that hover around an average.
And according to Maxwell's distribution, when they collided elastically with each other, they continued to move at different speeds.
The collisions, in other words, didn't tend to make all the speeds equal, as you might expect.
Of course, this idea of Maxwell's was not something that lent itself to easy verification.
How can you measure the speeds of individual molecules to check it?
So even though Maxwell's proposal became part of kinetic theory, it was not until the 20th century when experimental proof was obtained.
What's really interesting to me here is that the success of kinetic theory contributed to the introduction of a new style of thinking about nature.
The work of Clausius, Maxwell, and others showed that the dream of the scientific materialists would have to be modified.
That dream assumed that if we knew the laws about nature's fineness, as Buchner once put it, then we'd be in a position to predict everything that would happen.
Why? Because everything in nature was determined by nature's laws.
It was a dream that went back to Laplace's vision of a world machine that ground on forever.
Laplace felt that once the laws had been discovered for all aspects of this machinery, then we'd know everything about it, even what was to come next.
In a famous passage, Laplace envisioned a mind who knew all these laws.
Presumably, as a deist, he was talking about God, who knew the cosmos, but anyway, here's how he imagined how God knew things.
This mind stood, incidentally, or it stood, this law as the ultimate goal humans should aspire to.
First, he established cause and effect.
We ought, he wrote, to regard the present state of the universe as the effect of its previous states and as the cause of the one to follow.
Then he went on to describe the all-knowing mind.
Given, for one instant, a mind, he wrote, which could comprehend all the forces by which nature is animated and the respective situation of the beings which compose it,
a mind sufficiently vast to submit all these data to analysis, it would embrace in the same formula the movements of the greatest bodies of the universe and those of the lightest atom.
For Laplace, and for many after him, the motions of the molecules of gases were thought to be governed by exact laws.
Well, Laplace then finally drew the conclusion that this kind of knowledge of nature implied.
For such a mind, he said, nothing would be uncertain and the future as the past would be present to its eyes.
In this view of things, nature was a machinery whose operations, because they followed nature's laws at every level, were rigorously determined.
What Maxwell was saying was that knowledge of that sort for the motion of molecules and gases was not something humans could acquire.
But all was not lost.
We could aspire to knowledge of the average speed of the molecules.
We could acquire knowledge of the whole, even if we could not know about each individual part.
We can use the kinetic theory to calculate the average pressure of a gas, even if we don't know the motions of the individual molecules making up the gas.
But how seriously are we to take this statistical description of nature?
Is it just something that we have to settle for because we can't get at the real knowledge Laplace described?
Or could we imagine the statistical description in some sense is the real description of nature?
Maybe nature is more accurately depicted as the average of individual processes, none of which are individually predictable.
If so, then nature is no longer best envisioned as a well-oiled machinery that always gives the same output when presented with the same input.
The outputs vary around a mean.
To see nature this way does not, of course, mean that nature is chaotic.
Nature remains regular, but in an overall sense.
While nature's regularity is guaranteed overall by the statistical law, individual outputs can't be predicted with complete accuracy.
And that has a fascinating implication.
If you're going to say that nature is more accurately described by statistics than by an older Laplacian determinism,
then in one sense nature becomes more like a person than a perfect machine.
Why? Because people are in general reliable, but they also deviate from regularity.
The bottom line here is that if natural scientists are beginning to take statistics seriously as the means to depict physical processes,
then the old, completely deterministic order announced by Laplac back in lecture two,
in which a perfect Newtonian mind could know all of nature with certainty, that has been replaced.
Of course, that's what will happen, but it will take further developments in quantum theory to make it crystal clear later in the century and in the 20th century.
We'll touch on that in our last lecture.
In our next lecture we'll turn to the realm of astronomy, where we'll learn more lessons of both pride and humility.
