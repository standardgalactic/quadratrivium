If we fit a line, we get the type of chart that you see.
Notice though that the data isn't matching at every point.
The line isn't passing through every point.
So we aren't predicting the points we measured perfectly.
We are off on those values.
This is because in a sense we are assuming the presence of that measurement error or
noise.
What if we didn't?
What if we wanted 100% accuracy for the points that we measured?
One possibility, bad, is just to change the data.
Gregor Mendel, the famous 19th century researcher into heredity is an example.
He published data about heredity and peas that was much later found to be too perfect.
Decades later, a statistical analysis of Mendel's data by R.A. Fischer found there was too little
noise in Mendel's results.
Fischer's comment was, quote, I have no doubt that Mendel was deceived by a gardening assistant
who knew only too well what his principal Mendel expected from each trial made.
Regardless of who done it in this case, we always want to avoid overfitting data, even
if it's for a cherished model.
But there's also the reverse problem.
Another way to strive for 100% accuracy is to overfit the model itself.
We could leave the data alone and make too many adjustments to the model.
We could measure with a very complicated curve rather than a line.
Here we see this.
Notice how this data is dipping at the end.
This data will have a much lower value if we extend the plot to the value of 8 on the
x-axis than we would have had if we used the line.
This type of difference could be significant as we are about to see.
But first, remember, data is generally not exact and can contain spurious components.
It may help to think of static in a phone line.
When someone calls and has static in the connection, you try to hear the person talking, so you
work to listen to the voice within the static of the call.
In the book, The Signal and the Noise, Nate Silver lays out a case for the Fukushima Nuclear
Disaster as a dangerous outcome of overfitting.
Earthquakes continue to be unpredictable.
Even with today's supercomputers, modern geophysicists are only marginally better than simple historical
means.
In hurricanes, we can collect data, lots of it.
With earthquakes, relevant data is much harder to collect.
We cannot directly measure stresses 20 kilometers or more underground, so the data is scarce
and underfitting is an issue.
