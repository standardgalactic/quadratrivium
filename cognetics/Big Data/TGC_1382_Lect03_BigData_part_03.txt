An important consideration in my research is how big does data get?
I deal with this all the time.
For instance, on one research project, I wanted to analyze a data set.
And suddenly, when I did the computations, I figured out that I would need more memory than the biggest supercomputer just to look at the data.
It's easy to assume that you can still do the problem if you add more detail.
But that's not always the case.
It's very easy to be orders of magnitude off from what you expect.
That could mean the difference between easily analyzing a problem and having to think of a brand new approach.
So we could end up with a data set that's huge, really huge.
But that doesn't mean we need all of it, even if it is relevant to questions we might have.
Still, with so much data comes the opportunity again for new insight.
Because of this, many people are working with big data in trying to analyze it.
Take NASA, which has big data on a scale that can challenge current and future data management practice.
NASA has over 100 missions concurrently happening.
Data is continually streaming from spacecraft on Earth and in space, faster than they can store, manage, and interpret it.
Two different kinds of spacecraft send data in different forms.
Craft from deep space send data back on the order of megabytes per second.
Earth orbiters can send data back in gigabytes per second.
In time, there may be optical or laser communication, which will lead to about 1,000 times increase into the range of terabytes per second.
One thing about some of the largest data sets.
They are often being analyzed to find one specific thing.
For example, Kepler was a NASA base observatory launched in 2009 to discover the Earth-like planets orbiting other stars.
Kepler's main instrument was a photometer.
It continually monitored the brightness of over 145,000 main sequence stars in a fixed field of view.
Why?
Periodic dimming is caused by extrasolar planets when they cross in front of their host star.
As of July 2013, Kepler had found 134 confirmed exoplanets in 76 stellar systems.
It also had produced a lot of future work and analysis with over 3,000 unconfirmed planet candidates.
So it was a big data set, but the mission was only looking for one thing, a rare change in signal corresponding to a dimming.
Many data sets are much more complex.
In fact, when things get too big, you sometimes peel off part of your data to make it more manageable.
