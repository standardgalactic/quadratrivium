We stand within a data explosion of sorts.
Organizations talk about drinking from a fire hose of information.
Commentators refer to a data deluge.
But there is no need to drown in data.
In this lecture, we will see how data analysts of many kinds think about their data, the
amount of data, the types of data, what constraints there may be on an analysis, and what data
is not needed.
In this way, we'll see how the deluge can be put to work answering questions we may
have by developing the mindset of a data analyst.
First, as we discussed in the opening lecture, there is a lot of data, and it can be hard
to wrap one's mind around the huge numbers, but it is possible.
As data analysts, this is what we do.
Let's turn to the amount of data being generated on Facebook.
In only 15 minutes, the number of photos uploaded to Facebook is greater than the number of
photos stored in the New York Public Library photo archives.
Again, that's every 15 minutes.
Remember, though, that Facebook archives all this data.
Yet even with so much data being added, you see a picture on your Facebook news feed within
seconds after it's posted.
Yes, it's a lot of data, but with data analysis, it can be managed in a way that's both timely
and useful.
Keep in mind that advances in storage play into this.
Consider 50 gigabytes, which is about the amount of storage on a Blu-ray disk.
This would hold the textual content of just about a quarter of a million books.
Just simply a disk you might have lying around your television.
Storage capacity of a high-end drive from companies like Seagate or Western Digital
can hold five terabytes or more.
A terabyte is 1,000 gigabytes.
With all this data, we begin to see why there began to be a lot of talk about big data.
But without analysis, the data is essentially a lot of 1s and 0s.
If you can't analyze it, it may not be helpful.
Why?
Imagine a stadium full of people talking.
What topic are most of the people talking about in that instant?
Proper analysis can enable one to gain insight even within big data.
But how big is big?
When we ask how big is big, it's really a relative term.
A kid who was tall in my elementary school class would be really short relative to Michael
Jordan, and Michael isn't that tall when it comes to all players in the NBA.
The same is true with data.
We may call something big and only mean it in the context of data that in some other
arena might seem small.
Our sense of size changes with time, too.
The Apollo 11 computers were fast and stored a lot of data for the time.
Later during the time of floppy disks holding 1.44 megabytes, a gigabyte seemed about as
remote as the petabyte or exabyte are for many of us today.
When I worked at the national labs, they'd have a new fastest supercomputer every few
years.
I remember how they periodically work at a speed I hadn't heard before, and I'd have
to ask, what does that mean?
So size is relative, and let's just keep in mind as we talk about what's big.
To talk about size, which we began doing in lecture one, we need to talk about how we
measure and have a sense of what each measurement means.
So hold on to your numerical seat belt as we go from small to super large.
A bit is a single binary digit.
It equals zero or one on or off in the hardware.
A bit is a character.
Eight bits makes up a byte.
Ten bytes is a written word.
One kilobyte is equal to 1,000 bytes and equals a short paragraph.
Two kilobytes is equal to a written page.
A megabyte is equal to 1,000 kilobytes and equals a short novel.
We already mentioned that 10 megabytes is enough for the complete works of Shakespeare.
Seven minutes of HDTV video is one gigabyte or 1,000 megabytes.
A DVD can hold from 1 to 15 gigabytes.
Blu-ray discs can hold 50 to 100 gigabytes.
1,000 gigabytes equals 1 terabyte.
10 terabytes equals all the text information in books held by the U.S. Library of Congress.
400 terabytes might be sufficient to hold all the books ever written.
1,000 terabytes is equal to one petabyte or 10 million four drawer filing cabinets filled with text.
In April 2013, the Titan supercomputer was upgraded to hold 40,000 terabytes or 40 petabytes.
Google processes about 20 petabytes per day, as I mentioned in lecture one.
All the phone calls in the U.S. if stored for a year might equal 300 petabytes.
1,000 petabytes is an exabyte.
Numbers this big came into use more recently.
Using PETA for quadrillion and EXA for quintillion was proposed in 1975.
Now we get to streaming of video data.
Streaming one movie for one hour is about one gigabyte.
So when a billion hours of video are streamed, that's a billion gigabytes or one exabyte.
If you wanted to store that, one very large data center might hold one exabyte.
All the words ever spoken by humankind one decade into the 21st century may have equaled about five exabytes.
1,000 exabytes equals one zettabyte.
That is roughly the scale of the entire worldwide web, which may be doubling in size every 12 or 18 months.
With one zettabyte reached perhaps in the year 2011.
One zettabyte is also about 250 billion DVDs, roughly 35 DVDs for every person on Earth.
Then what?
1,000 zettabytes equals one yotabyte.
One yotabyte is one quadrillion gigabytes.
Using a standard broadband connection, it would take you 11 trillion years to download a yotabyte.
For storage, one million large data centers would be roughly one yotabyte, pretty close to where we are.
Since an estimate from Emerson Power said there were already half a million data centers worldwide by 2011.
But even after we look at how data comes in various sizes, it's easy to think, well, it's all just big and assume big is all equal.
And that's just not true.
An important consideration in my research is how big does data get?
I deal with this all the time.
For instance, on one research project, I wanted to analyze a data set.
And suddenly, when I did the computations, I figured out that I would need more memory than the biggest supercomputer just to look at the data.
It's easy to assume that you can still do the problem if you add more detail.
But that's not always the case.
It's very easy to be orders of magnitude off from what you expect.
That could mean the difference between easily analyzing a problem and having to think of a brand new approach.
So we could end up with a data set that's huge, really huge.
But that doesn't mean we need all of it, even if it is relevant to questions we might have.
Still, with so much data comes the opportunity again for new insight.
Because of this, many people are working with big data in trying to analyze it.
Take NASA, which has big data on a scale that can challenge current and future data management practice.
NASA has over 100 missions concurrently happening.
Data is continually streaming from spacecraft on Earth and in space, faster than they can store, manage, and interpret it.
Two different kinds of spacecraft send data in different forms.
Craft from deep space send data back on the order of megabytes per second.
Earth orbiters can send data back in gigabytes per second.
In time, there may be optical or laser communication, which will lead to about 1,000 times increase into the range of terabytes per second.
One thing about some of the largest data sets.
They are often being analyzed to find one specific thing.
For example, Kepler was a NASA base observatory launched in 2009 to discover the Earth-like planets orbiting other stars.
Kepler's main instrument was a photometer.
It continually monitored the brightness of over 145,000 main sequence stars in a fixed field of view.
Why?
Periodic dimming is caused by extrasolar planets when they cross in front of their host star.
As of July 2013, Kepler had found 134 confirmed exoplanets in 76 stellar systems.
It also had produced a lot of future work and analysis with over 3,000 unconfirmed planet candidates.
So it was a big data set, but the mission was only looking for one thing, a rare change in signal corresponding to a dimming.
Many data sets are much more complex.
In fact, when things get too big, you sometimes peel off part of your data to make it more manageable.
The Kepler Space Telescope, for example, gathered more data than could be stored and sent back to Earth.
Did that make it useless?
No, it simply meant you couldn't use all the data.
So the scientists carefully pre-selected about 5% of the data, which are pixels, which ones.
Those they deemed most relevant for each star of interest.
But excluding some of the data is important on much smaller scales, too.
Let's consider U.S. presidents.
That's a small group, and we can focus on just one piece of information, their time in office.
Their terms as president have ranged from 4,422 days, which was Franklin Delano Roosevelt to 31 days, which was William Henry Harrison.
Suppose we graph the number of days of everyone who served as president of the United States.
The graph would show a lot of one-term presidents, another group of two-term presidents, and those with non-standard amounts of time in office.
But if our data is accurate at the level of single days, we will see that there are 45 data points for a data set that ends with Obama, the 44th president.
Now, how is that true?
Who is this unknown president?
It's true that Harrison was the 9th president with an extremely short term.
But he doesn't hold the minimum value.
There is somebody, 12th in line, who had an even shorter term.
This is David Rice Atchison.
March 4, 1849 was a big day for then President Atchison.
That was his only day to be president.
It was supposed to be the day Zachary Taylor was sworn in as the 12th president of the United States.
The stormy weather led to muddy roads, so many who were coming hadn't arrived on the 4th.
Zachary Taylor said he refused to be sworn in on a Sunday, and the inauguration was postponed a day.
But James Polk, number 11, was no longer president.
And Taylor hadn't been sworn in, so Senator David Rice Atchison as chair of the Senate became president according to the United States Constitution.
Amazingly, when he learned of his presidency, it was basically over.
The Missouri senator had worked overtime for weeks, exhausted he lay down in the early morning of Sunday, March 4.
He woke up on the 5th.
Yes, sleeping through his presidency.
So, was he president?
Do you count him?
History has essentially ignored him and overlooked him.
In terms of data, he can be considered an outlier.
So once you know he's there, you can decide for yourself.
He has one data point in only a few dozen.
If he is one out of 45, that's about 2% of your data.
How much data can you omit from a large data set and still be okay for the questions you are investigating?
Moreover, do you lose insight if you omit?
How many people know that Atchison was also the youngest president ever?
Could excluding such data be relevant to issues that interest you, but you simply don't know it yet?
Such challenges are inherent in data analysis, making it both more difficult and more interesting.
In fact, a concern for me with the term big data is that although you can do amazing things with big data sets,
the field is not merely about a few big businesses.
The same lessons can apply to all of us.
Sometimes the data is already available.
The trick is recognizing how much to use and how to use it.
Consider roof contractors, some of which are using modern data to drastically cut costs.
The traditional scenario was for a roofer to get a call, visit the home or building to inspect the roof,
and then decide if it was appropriate for them to do the job.
Every job that wasn't a good fit equated to a loss of time and money.
So some roofing companies are now using readily available data, specifically Google Earth, to help their business.
Now, when they get a call, they input the address into Google Earth and inspect the roof there.
If the job looks appropriate, they can also look to see if other roofs in the area can also be inspected,
possibly allowing a deal to be struck for multiple jobs.
Analyzing data related to local neighborhoods, a problem not remotely comparable to the scope of NASA's questions,
allowed contractors to identify business opportunities.
Sometimes relevant information comes from returning to the same place many times.
Forbes.com gives the example of the Spillers Group, a company owning three restaurants in Dallas.
The three restaurants use data analysis software to share business information among the restaurants more efficiently.
They have such information as point of sale data, labor metrics, and accounting numbers.
Managers pay could be more closely tied to each of these restaurant metrics.
By understanding what was happening during overtime hours, they found ways to cut their labor costs by 10%.
The bottom line is saving thousands of dollars every two weeks.
By collecting data across three restaurants and integrating it together, they could make better decisions that improved their company.
In other situations, we might not even know what we don't know.
Gus Hunt of the Central Intelligence Agency stated this really well in a 2013 talk.
He noted, quote, the value of any information is only known when you can connect it with something else which arrives at a future point in time.
So we want to connect the dots, but we may not yet have the data that contains the dot to connect.
So this can lead to efforts to collect and hang on to everything.
Further, the data from the past may not have been stored in a usable way.
So part of the data explosion is having the data today and for tomorrow.
The cost of a gigabyte in the 1980s was about a million dollars.
So again, a smartphone with 16 gigabytes of memory would be a 16 million dollar device.
As always, that scope keeps changing.
So as we said before, a 16 gigabyte phone may not seem like much memory.
Exactly. That is why yesterday's data may not have been stored and it may not be suitable for how we're storing it today.
This leads to a common way to categorize data into two types.
There is structured data and unstructured data.
Thus this level of categorization can help you learn more about your data and even help you think about how you might approach the data.
First, structured data. This is the type of data many of us are most accustomed to dealing with or thinking of as data.
Your list of contacts with address, phone numbers and email addresses or recipes are examples of structured data.
It can be a bit surprising though that most experts agree that structured data accounts for only about 20% of the data out there.
Where does this data come from?
There are two sources, computer generated data and human generated.
The sensor we talked about for athletes would offer computer generated data.
While it is tracking a human, the actual device inputting the data is a kind of computer.
Many other sensors also gather data from airplanes to smart buildings to scientific experiments to medical imagery devices to closed circuit cameras.
Another example of computer generated data is web log data.
Every time you log onto a web page, the server that contains that web page generates pieces of information.
For example, you can know what web page you came from and what time you came.
With just those two pieces of information, you can begin to analyze the traffic on a site.
When is it heaviest? Are there particular websites from which people come to your web page?
There is also computer generated financial data.
Think of stock indexes such as the Dow Jones Industrial Average.
This type of data enables us to predict the future such as the price of a stock based on its previous price movements in the previous hour, day or month.
The boundary between computer generated and human generated data is not fixed.
In sports, you might have someone collecting data by hand about where players shoot rather than having a sensor.
A professor might input grades manually or the grades might be read and entered by a machine.
A doctor may personally input medical information into a case file, but that may be in combination with data read automatically from routine scans or computer based lab work.
Now, we turn to unstructured data, which doesn't follow a pre-scribed format.
While perhaps 80% of identified data comes in this form, until recently, we didn't have mechanisms for analyzing it.
In fact, there were even problems just storing it or storing it in a way that could be readily accessed.
Scientific data often is unstructured and can be anything from seismic imagery to atmospheric data.
Everyday life these days also produces a lot of unstructured data.
There are emails, text document, text messages and updates to sites like Facebook, Twitter or LinkedIn.
There is also website content that's added to video and photography sites like YouTube or Instagram.
Now, if data is structured, it is more likely that a method, possibly around for some time, has been developed to analyze it.
Unstructured? Much less likely.
A million records in a structured database are much easier to analyze than a million videos on YouTube.
Now, so-called unstructured data can still have some structure.
Let's take one example of unstructured data, satellite images.
For a given satellite, all the images come in some aspect ratio.
All the images have the same number of rows to columns.
But this type of data is quite different than a record containing student data.
With student data, I know the first entry is the student number, second is the name.
While I know that the picture will contain pixels, I don't necessarily know what is in the picture and what is not in the picture.
I don't necessarily know where to look.
So, the framing provides some structure and adjacent frames may be related, but overall, the data is much more unstructured.
The information contained in satellite imagery depends in part in what interests you.
Keep in mind, a satellite image database can contain terabytes of data.
Roofing contractors are using only a very small portion of Google Earth.
Suppose you are trying to identify native rainforests or places where deforestation or reforestation is occurring.
Well, it's whether it's the condition of roofs or forests we want to identify.
Can the identification be done automatically?
The answer increasingly is yes, and moreover, it can be done in minutes.
Note, a human could do this, but it would require days or weeks of hard work from human specialists.
Now, high precision results can come in minutes.
This is part of what it means to think like a data analyst.
For instance, I was doing research with our NASCAR racing team, where the speed of the analysis was even greater.
During a pit stop, a professional tire changer can install and tighten a nut on all five bolts of the wheel in less than 1.5 seconds.
This action requires at least 10 distinct movements, and is naturally prone to error.
Detecting whether each nut was correctly tightened is useful for both training and for error detection during competition.
Unfortunately for data analysts, the racing regulations strictly prohibit any instrumentation on the car that can detect a correctly installed wheel or any telemetry systems that could be used to access the data.
All instruments must remain behind the pit wall and in the garage area.
The tire changer uses a pneumatic torque gun. In particular, the team could easily begin recording the time-varying pressure of that torque gun.
When the nut tightens, the pressure in the gun increases.
My colleague, a computer scientist on the team, noticed during races that a member of the team would look over the data and see variations and identify that, for instance, bolt four might be loose.
But this meant he was being pulled away from other tasks during the race.
Given a human could see variations in the data, my colleague wondered if a computer could recognize this variation and collect the data.
In the end, my collaborator and I developed a system with an undergraduate that could correctly identify most loose wheels with a low incidence of false positives.
It was refined and used by the racing team, both for training and competition.
A lot of measurements were being taken from that torque gun, but the key wasn't the amount of data, rather that someone was seeing something with it.
We then automated the data gathering, which allowed members of the team to work on many other important aspects of any given race.
Deciding what to analyze and how is always important.
In addition to getting the data and having a sense of what form it comes in, you need to consider how quickly it comes in.
Is the data going to come in real time?
If so, how quickly will you need to analyze it?
Google looks at billions, possibly trillions of web pages in order to be ready for your search query.
When you type something and hit enter, the search engine has already analyzed the web pages on a recurring basis.
At one time, Google updated its search engine results monthly.
The shifting of web pages in the ranking led to the term Google dance.
Even if you changed your website during the time of Google dance, Google might not see the change as they were not attempting to index everything in real time.
Now, Google releases largely minor updates, often throughout the day.
Some businesses are leveraging the real time input of Twitter.
For example, during a big sporting event, Twitter streams can indicate
when a big play occurs.
To do this, you must be reading Twitter live and analyzing it.
Note that the moment you know that, you must be analyzing data read from live tweets.
This also changes what type of techniques are even possible.
Really robust methods in natural language processing may give a lot of information,
but if it takes minutes to run and you need that information in seconds, the robust method may not be possible,
or you simply may not be able to do them in real time.
Today, knowing where data is coming from, how much of it is coming,
and how quickly you are going to need to analyze it are all very real and very important questions.
Clearly, there are a number of things to consider. How do you keep them all in mind?
You remind yourself of them as you learn methods and look at problems.
That's exactly what we'll do in coming lectures.
You don't need to master everything before trying specific methods.
Try, reflect, try again, and learn.
As you learn a variety of tools, you'll also develop the mindset of a data analyst.
When you begin, having a lot of data can be overwhelming,
but before long, having a lot of rich data, not just sporadic pieces of data you happen to find,
becomes more like a huge toy store that lights up a child's eyes.
So yes, we are in a data deluge, and for a data analyst, this is truly exciting.
I see data analytics like this. Each data set offers its own puzzle,
but unlike a jigsaw puzzle, there are many answers.
It's like a maze with all kinds of paths and rewards emerging all the time.
Thank you.
