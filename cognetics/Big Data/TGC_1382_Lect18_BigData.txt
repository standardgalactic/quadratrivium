We are approaching a time when every book ever written is available in a digital form
that is both readable and searchable.
Not just one by one, but all at once.
That is, all the books ever written, perhaps 130 million and all, are becoming part of
a vast new data set humanity has never had before.
As if that weren't enough, we are now digitally creating, storing, and analyzing many, many
more words than appear in traditional publications.
People update Facebook statuses, write emails, text, tweet, and comment on articles posted
on the internet.
As we will see, there can be more words posted on Twitter about just one topic than are contained
in the average book.
All these words offer rich data sets to understand and predict phenomena in our world.
The challenge is that all this data is unstructured.
No one has filled out a questionnaire, responded to a carefully crafted poll question.
Have their behavior sorted into box scores or any of the other helpful things that have
made data analysis easier for us to this point?
So in this lecture, we discuss how to watch the words and gain insight from unstructured
data.
These are the realm of sentiment analysis and text mining, what we can call text analytics.
Now these techniques are often different from the sort of textual analysis you might
expect in a literature class.
To see this, let's look at how social media were used to predict the winner of American
Idol.
This was May 2010, in the ninth season, when the final show drew an American audience of
over 23 million.
Who would be the American Idol?
There were some data analysts at a group then called IdolStats.com that sat comfortably
awaiting their predicted winner's name to be called.
And yes, they did get it right, lead to wise one American Idol that year.
In the group at IdolStats.com saw the victory coming far in advance.
Many other people were surprised.
MTV.com reported the outcome with the headline American Idol finale, lead to wise, upsets,
crystal, bower socks.
How did the analysts know?
They didn't have polling data, so waiting polls like Nate Silver wasn't an option.
Instead, the group monitored millions of interactions on social websites.
Those interactions gave them the data they needed for their analysis.
But they didn't simply look for which topics or contestants attracted buzz.
They also looked at what was being said.
Were the comments positive or negative?
How much was being said about each contestant?
Using a variety of tools, the group summarized the buzz and evaluated the chatter from publicly
available blogs, forums, online news, and social networks.
In the end, their data analysis unveiled the same result that millions upon millions of
votes would produce on that final American Idol show.
IdolStats created a tool that heard what all those fans were saying without ever really
talking to anyone.
Just by collecting and evaluating the chatter, they heard what was being said, as clearly
as if they were talking to all the people calling in for their favorite singer.
We all have had some experience with text analysis.
At the simple end, think of spelling and grammar checks.
These already contain a level of text analysis.
They tell you how many words you have, and many programs will also auto correct.
When I was growing up, my parents' computer would even tell me the reading level of what
I wrote.
Authorship is a traditional area of text analytics.
Who wrote the works of Shakespeare?
That sort of thing.
For example, in 1996, the anonymous author of the best-selling novel, Primary Colors,
was uncovered by Donald Foster, an English professor at Vassar College.
The professor had analyzed the frequency of words used in the novel, and he compared this
frequency distribution with the writings of 35 likely suspects.
The frequency distributions fingered Joe Klein of Newsweek Magazine, who later admitted to
being the author.
You can also look at the frequency distribution of individual letters, A, B, C, D, and so
on.
I do this with my students at Davidson College, where I have them use a program that identifies
language.
The program looks only at the frequency of letters, and yet it does a great job.
In fact, my students often spend time when they complete the assignment trying to fool
their own program.
Frequency distributions of letters also have an important place in cryptography, where
the distribution of letters can make some ciphers very easy to break.
As for word frequencies, at the end of 2010, Google released an extraordinary tool that
looks at frequency distributions across millions of published books all at once.
You put in a word or phrase and it looks for the frequency across all Google scanned books.
This tool is called Google Books Ngram Viewer.
Here's what it looks like for the word computer.
Not surprisingly, things slowly climbed between 1940 and 1950, with sharper inclines since
the 1970s and an even sharper incline in the mid-1980s.
Here we are looking at the frequency of the word computer, which is called a one gram
or unigram, as it is a single word.
The vertical axis shows how frequently the term you are searching for appears compared
to all other words, or in this case, single words.
So computer appears about 0.01% of the time among all one grams.
How many one grams are there?
Over 472 million.
How do I know?
You can download the entire data set.
It's huge.
This data set has about 1,000 times more items than the number of word forms defined or illustrated
in the Oxford English Dictionary.
But you don't have to download anything.
Simply type in the n-grams that you're interested in and create the graphs yourself.
What about typing in data analytics and data analysis?
Let's search both terms together.
Here is what we get from 1950 to 2008, which was the last year of the original data set.
Goodness, look at data analytics.
Remember, a graph usually tells us only one story, and that can be misleading.
So let's look only at data analytics.
Suddenly, what looked like nothing is indeed something.
It's a small rise after all, but we would have missed the increase entirely if we hadn't
looked more closely.
And if we rescale the less common term, we can see the trend for both together on the
same graph.
Remember, this is simply showing us a trend.
We'd need to do more analysis to understand why.
For example, consider the terms which and wizard from 1800 to 2008.
Look at the spike in the 1990s.
That's when the first Harry Potter book was released.
But it isn't about just one book.
The frequency of wizard increased sixfold across all books at that time.
Google and Grams are already an astonishingly powerful tool.
You can drill down to search the underlying publications year by year.
You can refine your search in all sorts of ways to include what you want and exclude
what you don't.
What about social media?
Let's look at tweets responding to what happened in the Denver Broncos football stadium on
the wintery evening of January 8, 2012.
The game was in overtime and the Broncos were on a three game losing streak playing against
the heavily favored Pittsburgh Steelers.
But only 11 seconds into overtime, Denver Broncos quarterback Tim Tebow threw the longest
overtime touchdown pass in NFL history, ending the game and the Steelers season in incredibly
dramatic fashion.
The fervor in the Denver stadium created an internet tsunami.
USA Today reported that thumbs were typing at a rate of 9,400 tweets per second on that
Sunday night as football fans tweeted about the victory.
And how much textual content is contained in 9,400 tweets?
This brings up an important point in data analysis.
You can derive a number, but then you must interpret it.
Well, a tweet cannot exceed 140 characters in length.
More important, the average tweet is about 82 characters.
So in one second, around 770,000 characters were zooming through Twitter after the Broncos
win.
Then take this another step.
A word in English averages 5.1 characters in length.
So around 150,000 words were tweeted in one second after Tebow's Haas.
It's about double the first Harry Potter book, which contained just under 77,000 words.
In just two seconds, more words were tweeted than all the words in the first three Harry
Potter books combined.
The second book was about 85,000 words and the third about 107,000 words.
So we can look at the volume of posts, which can be huge.
But what about the words themselves?
Researchers at IBM and elsewhere have programmed computers to automatically generate journalistic
summaries of soccer matches using only Twitter updates.
Spikes in the volume of tweets on a topic help identify key moments in the match.
Now, keep in mind, no reporter was sent to the soccer match.
So the people tweeting were, in a certain sense, the only reporters.
The data analysis synthesized the information.
One can write a program to grab the data straight from Twitter using what is called an API.
API stands for Application Programming Interface.
It defines ways you can retrieve information, in this case, from Twitter.
You define a function that calls for username, number of we tweets, or whatever information
you want, and have appropriate access to.
Some information may require a user password, but often you can do a lot, even without that.
I worked with a Davidson College undergraduate on tracking tweets related to NASCAR races.
We searched Twitter for specific terms and wrote the tweets to a text file for processing.
We weren't producing live results, so it was relatively easy to store everything and
process it later.
There are also websites that offer to download data for you.
For example, Datasift.com offers the ability to enter search terms and download the data
live.
When this was first demonstrated to me, I wrote a tweet with one of the terms the demonstration
was searching, and about a minute later, my tweet appeared on the screen along with
others containing that term, so it's pretty fast.
Social media do more than predict who will win a contest or summarize an event.
In 2009, negative sentiment in tweets appeared to reduce opening week sales of the movie
Bruno, which led Time Magazine to comment, quote, Bruno could be the first movie defeated
by the Twitter effect.
In 2011, researchers from Indiana University in the University of Manchester published
an article in the Journal of Computational Science entitled, Twitter Mood predicts the
stock market.
According to their paper, watching 10 million tweets during 2008 led to 86.7% accuracy in
predicting the daily up and down changes in the closing values of the Dow Jones Industrial
Average.
Wow!
But wait a minute, what does your intuition say about that one?
Probably too good to be true, right?
Turns out they had a lot of tweets but only 15 days of stock market data.
Their high ratio of predictions involved only 13 out of 15 days.
Okay, it was a small data set, but the paper also did not divide their data into training
and test sets.
We know what that means over predicting the past and under predicting the present.
Good methods aside, there are some issues in sentiment analysis.
Number one, accuracy is never perfect.
How well a computer can assess sentiment is generally judged by how well it agrees with
human judgments.
But humans don't always agree with each other.
That is sometimes called inner-rater reliability.
And you can never expect 100%.
For example, workers on Amazon's Mechanical Turk, where the tasks are designed to be as
routine and straightforward as possible, were found to agree only 79% of the time.
So a 70% accurate program might be doing nearly as well as humans.
Think about it, if a program were measured 100% accurate, humans might still disagree
with it about 20% of the time.
Number two, data sets and documents vary in their amount of sentiment.
A lot of content lacks sentiment.
For example, a person may post, I just did my laundry, or trying a new operating system
for my phone.
One web page may have a lot of comments and yet have a less charged discussion than another
website with fewer comments.
So the quantity and character of sentiment may be independent of one another.
You may also get a different picture using blogs compared to Facebook, and both may be
different from Twitter.
Number three, words don't always mean the same thing.
The word bad famously can mean good as in, man, that was a bad song.
This is important for companies as they watch social networks for sentiment on their products.
There may be different lingo or different inflections for those who market video games
than those who market men's suits.
Further, some words are simply difficult to track.
One restaurant in London is called Eat.
Yes, that's the name.
You can eat at Eat.
But most postings contain the word eat are not about that restaurant.
Imagine someone tweeting out a Dr. Seuss rhyme about this, eat tweets are neat, but nothing
beats eat meets on tweets for defeats.
Number four, another issue for sentiment analysis is getting a baseline.
What counts as negative?
Some things simply get a higher level of negative response all the time.
For example, a president with an 80% approval rating would be doing very well.
In companies, what if you are seeing 20% negative sentiment about your product?
Is that bad?
Not so bad if your competitors are creating 30% negative sentiment.
On the other hand, if 20% negative sentiment translates into 20% returns, that could be
very costly.
Let's turn to how a search engine determines what web pages are more or less related.
This can involve a vector space model in linear algebra, an important tool we saw in the lecture
on bracketology.
Now a vector space model relies on two things.
A list of documents and a dictionary.
The list of documents is pretty much what it sounds like.
It's a list of the documents on which you can search.
The dictionary here is a database of keywords, which could be some or all of the words in
the documents.
Sometimes it's not computationally possible to include all the words.
Those not in the dictionary return empty searches.
We can translate all the words into a table of numbers.
Each row in the table is a keyword in the dictionary.
Each column is a document.
The entry in a particular row and column is a one if that keyword is in the corresponding
document.
Otherwise, the element in the table is zero.
This is often called the document matrix.
But it's really a keyword document matrix.
Note, even with this matrix, there are computational issues.
Should every document be searched in its entirety for each keyword?
Because of the sheer size of the internet, some search engines read and perform analysis
on only a portion of the document's text.
They read only so many words in index from those.
But this isn't the only issue.
Do you require exact matching?
Will you allow synonyms?
If you don't, then you are requiring searches to have the exact words that appear in the
document.
Then there are the issue of word order.
For example, do you treat queries of boat show and show boat as the same or is different?
All these choices impact which documents are deemed most relevant.
What does your favorite search engine do?
Input show boat and boat show and see if you get the same results.
Now, let's return to the document matrix.
How do we use it?
Let's look at a small example.
You'll see that I created this example with a friend who was fond of fencing.
So our dictionary has three words, electric, fencing, and foil.
Our documents are really short.
They are only phrases.
Document one says the art of war.
Document two is the fencing master.
Document three is fencing techniques of foil, epi, and saber.
And document four is hot tips on building electric fencing.
Now let's form the document matrix.
Note, we already have the documents numbered, so that will be the same ordering for the
columns.
Column one is document one, which is the art of war.
So we need to assign each row to a word in the dictionary.
We will let rows one, two, and three correspond to the words electric, fencing, and foil.
Now we're ready to look at a query.
Use you input the word fencing.
We create the query vector.
This will have the same ordering as the rows.
Fencing is second, so our query is the word corresponding to the second row of the matrix.
So we are interested in the vector zero, one, zero.
Now we look at the columns of our matrix.
Each column of the matrix is this vector most like.
Now it's exactly the same as column two, so we would say that that document is most
similar.
Remember, that was the document, the fencing master.
Some of the other documents are similar, but they also contain other words.
Since this document is related only to this term in the dictionary, it is deemed most
relevant.
What do we do when they aren't the same?
You find the distance between them.
We'll explore two methods of computing distance in the lecture on clustering.
So what can we do with an ability to find similarity between texts?
Let's see these types of ideas with a large data set.
Remember Enron filed for chapter 11 bankruptcy in December 2001, an unprecedented amount
of information was released to the public domain through the Federal Energy Regulatory
Commission.
From this body of information, a research group at Carnegie Mellon University created
the Enron email data sets containing over half a million email messages.
These came from over 150 Enron email accounts covering a period from December 1979 through
February 2004.
The majority of messages spanned the three key years, 1999, 2000, and 2001.
In a highly compressed format known as GZIP, the database was about 423 megabytes.
The emails reflected the day-to-day activities of what was the seventh largest company in
the United States at that time.
And certain topics of discussion uniquely linked to Enron activities.
In particular, the infamous practices of price fixing, over-speculation, and deceptive accounting
which led to Enron's collapse toward the end of 2001 are all well-documented in the emails.
And we see this with data analysis.
Indeed, you are essentially forced to use it on something of this size.
Like before, we create a document matrix.
In this case, the rows are key words in the emails, and the documents in the columns are
emails.
Then what?
Well, we look for emails that have a lot of similar words in them.
And we find clusters.
One cluster contained, for example, the word touchdown.
Other words in that cluster were football, longhorn, Texas, quarterback, score, punch,
and tackle.
This group was discussing University of Texas Longhorn football.
The dataset also contained a small cluster of only 12 documents that use 447 terms.
The following terms commonly appear, fortune, CEO, COO, top, women, and powerful.
These terms and abbreviations, in fact, refer to Louise Kitchen, the top-ranking Enron
employee responsible for energy trading in Enron Online.
She was named one of the 50 most powerful women in business by Fortune Magazine in 2001.
In fact, all 12 emails were saved in Louise Kitchen's own private folder.
So what appeared to be a possibly interesting cluster after further inspection turns out
to be what is sometimes called an ego cluster, and thus, perhaps only of marginal interest.
Now, imagine being a legal firm tasked with looking at such a large dataset.
How would you begin?
Clustering analysis can't resolve everything, but it can find groups and speed up the process
of grouping information that could unlock important aspects of the case, all just by
clustering the words in email messages.
Published movie reviews have become another opportunity for text analysis.
The Stanford Natural Language Processing Group researches textual analysis in a variety
of forms.
One of these has been getting a sense of a movie review.
Earlier, we saw a matching of documents to search terms by looking at the words in isolation.
Positive words get positive points, negative words get negative points, and then the points
are summed up.
But if the context or the order of words is important, that gets lost.
So a group at Stanford wanted to take a more sophisticated approach.
Their deep learning model builds up representations of whole sentences based on the sentence structure.
Now, rather than looking for positive words, they determine sentiment by how words compose
the meaning of longer phrases.
Their model confirmed that words like funny or witty were positive.
But it was also able to determine that the following was negative.
This movie was actually neither that funny nor super witty.
How is this done?
It uses a neural network, actually what is called a recursive neural network, which was
initially trained on a data set of about 10,000 sentences.
Because it's a neural network, it needs training, so they have it online to get participants
to help.
Remember, that's how the 20Q ball learns so much about 20 questions.
In this case, the group allows you to enter movie review sentences of your choice and
offer corrections if you disagree with the result.
It also asks for help labeling target sentences they think would help the model.
Sentiment analysis is a big deal in data analytics.
It allows companies to hear their customers in unprecedented ways.
They can hear you when you aren't even talking to them.
You aren't calling customer service, but customer service is still listening.
If you post on Twitter, they hear it and might even respond.
Training this type of information can be worth big money.
In December 2013, Apple paid more than $200 million for Topsy Labs, which develops tools
that can help companies capture sentiment from Twitter.
How could Apple use this?
They could, for instance, automatically offer iTunes radio users real-time information about
songs or artists trending on Twitter.
Today, we can find sentiment available on many topics.
You think about getting a new product.
So you look around the web for online reviews posted by people.
What are they saying?
You are, in your own way, sifting through the data and looking for sentiment.
A computer that is programmed can zip through many, many web pages, and if trained in the
right way, the idea and outcome is the same.
This is a rich resource for businesses.
Text analysis can also be used in science, in applied fields of all kinds.
Consider that more than 8,000 scientific papers were published every week on Google Scholar,
and top-down classification systems increasingly do not capture every way of looking at published
results that could bring insight.
Also, ironically, research published in academic journals is among the most difficult for ordinary
users or even researchers to access in bulk.
To address this problem, in 2004, the University of Manchester began hosting Britain's National
Center for Text Mining, the world's first publicly supported center for text analysis
in the world, with an initial focus on biomedical information.
Technology is a huge area for textual data analytics.
For example, a project to link locations on the human genome back to specific research
articles about those locations is underway at a project called Text to Genome.
Every scientific field could potentially have this sort of mapping between research conclusions
and the research where those conclusions were proposed and confirmed.
On the lighter side, there is also fun and creative analysis going on with text such
as cookbooks, what you might call recipe analysis.
We did a version of this at Davidson, where we focused on salads, working toward a program
that would use existing recipes to create artificially generated salads.
We may have it in the Student Union in order to have a taste test to see what people think
about the computer science.
IBM's Watson team has gotten into recipes too.
This is the same Watson project that previously did so well in answering questions from Jeopardy.
Their work uses not only the text of recipes, but also other data contributed by expert
chefs.
In fact, ways of combining text analysis with other tools will probably become more
prominent in the future.
We often say, watch your words.
But as data analysts, what we want to do is watch everyone's words.
You can do this anywhere without downloading any data.
Think about what someone says.
When could you know, without hearing the inflection, what someone meant?
You can look at text messages, emails, or articles on the web.
Think about how you could get a feeling for sentiment just by looking at the words.
Start by thinking a bit of it as thumbs up, thumbs down, or thumbs neutral.
Then progress into a score.
And if sentiment is not relevant, just start with a simple possible way to sort the words.
When can you get meaningful results just with individual words?
When would you need to look at combinations of words?
These same type of questions are being asked by those doing cutting edge research in this
field.
It all starts by just looking and watching all those words.
