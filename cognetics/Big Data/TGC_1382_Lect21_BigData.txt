Clustering is a powerful family of analytics for sorting data into groups.
What we call clusters.
Which sports players might have similar careers?
What friendships might be made on a college campus?
Or what singers or movies are similar?
There is more than one way to sort data into clusters.
In which clustering method you use depends in part on your data.
In fact, the choice of method can also affect the results you get.
All clustering is not the same.
We saw one type of clustering in the last lecture, when we used decision trees to carve
the data into groups.
Decision trees are great when you have a directed flow of all your data toward a single
target variable.
For example, survive the Titanic or not.
Customers will buy your product or not.
Speaking metaphorically, decision trees have a bunch of leaves in one single root.
But often we want to look for groups where there is no carving of the data based on a
single master variable.
In those cases, clustering techniques are more appropriate.
Let's begin by seeing how clustering can help fight crime.
Fighting crime through predictions can sound like science fiction.
But in fact, the analytics leading to this sort of police work started in the mid-1990s.
The idea began with researchers Lawrence Sherman and David Weisberg, who developed a concept
of clustering known as hot spots.
They defined hot spots as small places in which the occurrence of crime is so frequent
that it is highly predictable, at least over a one-year period.
According to their research, crime is approximately six times more concentrated among places than
it is among individuals.
With this work, it became important to ask not just who done it, but where done it.
For instance, one can find hot spots for robberies in the Bronx, which can help direct police
to that area for that type of crime.
Even better, doubling or tripling the frequency of police patrols in these crime hot spots
was found to reduce street crime rates by two-thirds.
The idea of grouping items has many applications.
In education, clustering can help identify schools or students with similar properties.
In geology, clustering can help evaluate reservoir properties for petroleum.
Market researchers group data from surveys and test panels.
They group customers into market segments.
This can help us find previously unidentified customers, develop new products, and select
test markets.
Let's return to the point that cluster analysis is a family of methods.
To get a sense of how there may be different ways to group items, consider a graph of pictures
for the New York Mechs.
The horizontal axis represents the number of walks the picture allowed.
The y-axis is the number of strikeouts.
This gives us a pair of points for each picture, which becomes a point on our graph.
So again, each point represents a picture.
So now let's group them.
We actually, even here, have a few choices.
Let's discuss a few of them.
First how many groups?
Two, three, four?
That's a common and very important question in clustering.
Let's go with three.
If we form three groups according to points that are close to each other, using a method
we'll learn later, we'd get this grouping.
So we've grouped.
The algorithm will form groups by math.
So we must first see if we can make sense of how the math grouped.
And second, see if there is something unexpected that the math found.
There's an inherent balance here.
Clustering at its best can discover something surprising.
However, if everything is surprising, it's entirely likely that the method isn't working
well with that set of data.
So you look for another clustering method.
According to our method, is this really how we want to group pictures?
Who's being grouped?
We tend to be grouping pictures who have a large number of strikes and walks together.
This is one way to group them.
If that is our targeted insight, then we have a valuable grouping.
But can we group in another way?
Consider drawing a line from the origin to a point on the graph, rather than asking how
far the points are from each other.
Let's measure the distance as angles between those lines.
If two lines have a small angle between them, then the corresponding points will be considered
close to each other.
We can again cluster.
Now what do we get?
What are we measuring?
We are now measuring which pictures have similar ratios of strikeouts to walks.
This is another grouping, possibly in even likely a very different one.
So one way methods differ is over how to calculate distance.
The first distance we just saw is Euclidean distance, since it is measuring distance in
space, in that case the xy plane.
The second measure was the angle and is often called cosine similarity, because you can
measure by the cosine of the angles.
Let's see the difference between angular distance and linear distance on another example,
a questionnaire.
Can we find someone most similar to you?
It isn't yet a grouping, only a pairing, but we can extend this idea to entire groups.
Suppose you and two friends rate the 2011 Oscar-nominated films, Inception, King's
Speech and Toy Story 3, based on your interest in seeing them, either for the first time
or again.
We will rank between minus 5 and 5, with 5 correlating to really wanting to see the film,
and minus 5 being definitely not seeing it.
So you rate Inception with a 5, King's Speech with a 3, and maybe Toy Story 3 with a minus
1.
Jane rates the films minus 1, 2 and 3, and then John rates the films 4, 1 and minus 3.
Now treat these values as a point.
So your point is located at 5, 3 minus 1 in three-dimensional space.
Jane's is at minus 1, 2, 3, so the distance between your point and Jane's is the square
root of 5 minus negative 1, that quantity squared, plus 3 minus 2, that quantity squared,
plus negative 1 minus 3, that quantity squared.
When we take the square root of all that, we get 7.28.
The distance between your point and John's is 5.74, so you and John have more similar
taste, at least according to Euclidean distance.
Now note, this same idea can be extended to a hundred points.
You simply let each person be represented by a point in one-hundredth dimensional space
and use the same formula in a hundred dimensions rather than three.
What about cosine similarity?
An important attribute in this example is a rating of 3, 3, 3 and 5, 5, 5 would be considered
exactly the same.
Is that what you want?
It depends again on your goals and your data.
Let's turn to popular songs and singers.
We'll use data you can try that's available from the Million Song Database, the existence
of which was partially supported by the National Science Foundation.
The one million songs are by 44,558 distinct artists.
The artist in the song database came with a measure of similarity already given.
This is really nice as you don't have to do the heavy lifting.
Also, as with clustering, there are many possible definitions for similarity.
Once we have a measure of similarity, we can cluster.
David Glyke, Assistant Professor of Computer Science at Purdue University, displayed singer
data on an undirected graph.
Make sense, if you are similar to me, then I'm similar to you.
This enables us to use a very powerful clustering technique called spectral partitioning.
So let's get a bit more familiar with the data.
First, the maximum number of neighbors, artists similar to one another, is 875.
That one includes the Beatles.
The mean is about 81.
So we know who's similar to who.
Is that enough?
What can clustering do for us?
Let's create a matrix with this information.
We assign each row to an artist and give the columns that same ordering.
The element in a particular row and column of the matrix is a 1 if the corresponding
artists are similar.
Else, the element is 0.
Now we visualize the matrix.
Pink corresponds to a 1 in the matrix and black is where the zeros are.
Here is what we have.
We have a lot of connections and a lot of similarity.
But the matrix for this picture orders the artists in no particular order other than
possibly alphabetically.
Once we know the clusters, we can order the rows so everyone in one cluster is ordered
together.
Now, look at the resulting matrix when it's visualized this way.
What do you notice?
Suddenly there is structure in the matrix.
Those dark pink regions are our clusters.
Those are the regions with a lot of artists connected to each other but not as connected
to artists outside the group.
So let's find an artist who's in a particular cluster.
How about Elton John?
We know from earlier work that he is similar to Billy Joel.
But now we also learn that they are in the same cluster.
And so are Neil Diamond, Ringo Starr and Paul McCartney.
The pictures you are seeing visualize the matrix with a tool called Viz Matrix created
by David Glyke and one of his colleagues.
David created all the visuals we are seeing here.
The software allows you to move over the matrix and zoom into clusters and move our
crosshairs over elements and see what row and column each element relates to.
Again, when the element is pink, that means the elements are related and you can see who
they are.
So let's move around the matrix.
We find that Nicole Kidman and Amy Adams are in the same cluster with each other.
They're not that far from Elton John but seeing that depends on how zoomed in you are.
In the full matrix which has over 40,000 rows, they really aren't all that far apart.
They could be natural to think about why they aren't all that far away.
While Amy Adams has performed a lot on Broadway, Elton John has composed stage music for such
musicals as Aida, The Lion King and Billy Elliot.
The important thing is to look to be sure things make sense.
In fact, that's what David and I did when we got these results.
We picked Elton John as we both knew his music and thought we'd be probably able to evaluate
the cluster in which he resided.
That let us check for the meaning in the clusters.
If we zoom all the way out, we can see the regions as mathematical genres of artists
like those you see on Amazon or iTunes which group artists together.
This is doing the same thing from the data.
So what about other genres in the matrix?
Let's go to the extremes of the matrix.
If you go to the upper left, you'd find the punk band Boycott, and at the lower right,
you'd find rappers like Dr. Dre in LL Cool J.
So we have a handle on what clustering will do for us.
Now it's time to learn to cluster data ourselves.
The first method we will learn is hierarchical clustering.
For this method, we don't have to decide on the number of clusters until you're done.
This is a big feature of this clustering algorithm, and it's not true of others we see in this
lecture.
To begin, each object is assigned to its own cluster.
Then we find the distance between every pair of clusters.
We merge the two clusters with the shortest distance.
Then with this new group of clusters, we find the distance between every pair of clusters.
No, we already found a lot of them.
We again merge the two closest clusters.
We repeat this process until we end up with one single cluster.
Then we will look at the process visually and make decisions on the final clustering
we may use.
One part of this process is how to measure the distance between clusters.
One way is to measure it as the shortest distance between a pair of points in each cluster.
So let's cluster Italian cities of Bari, Milan, Florence, Naples, Rome, and Turin.
To begin, each city, again, is its own cluster.
Finding the distances between the cities we find Milan and Turin are the closest.
So we group them.
From there, we find the distances between this new cluster and the previous clusters.
After this, we find Naples and Rome are the closest.
So that's a new cluster.
Next, Bari groups with the Naples and Rome cluster.
Where are we now?
We have a cluster with Turin and Milan.
We have a cluster with Bari, Naples, and Rome.
And Florence is still its own cluster.
So our clusters at this point aren't well balanced.
Taking this another step, Florence joins the Bari, Naples, and Rome cluster.
Finally, everything becomes one cluster, and then we're done.
A helpful part of this process is seeing it in a dendrogram.
This method is based on the core idea of objects being more related to nearby objects than to objects farther away.
As seen in the dendrogram, these algorithms do not provide a single partitioning of the dataset,
but instead provide an extensive hierarchy of clusters that merge with each other at certain distances.
Keep in mind that clustering is often used on much larger datasets,
as we saw with the musical artists from the Million Songs database.
But sometimes the amount of data is because each object has more data.
But now we had six cities and we're looking only at the distance between each pair of cities.
Let's move up to 38 objects, but now we'll turn to medicine.
Let's identify a relationship between cancer patients and DNA gene expression.
Todd Gullib of Harvard Medical School and his collaborators looked at 38 leukemia samples
containing gene expression of 6,817 genes.
They formed two clusters.
From the patterns found in the genes, the clusters correctly place 24 of 25 samples into ALL leukemia
and 10 of 13 samples into AML leukemia.
The ideal algorithm would place all AML samples in one cluster and all ALL samples in another.
Clustering discerns structure in hidden relationships within the data.
Such methods can potentially help with diagnosis.
When a new patient with an unknown classification arrives,
their data can be compared with the data from the existing classified clusters
and a classification for this new patient can be determined.
Now let's learn another common clustering method.
This is called K-means.
This creates what I think of as globs of data,
where you choose the number of globs in advance.
That's what K stands for.
This would have been an option with the data about singers we saw earlier.
K-means is a way to do this.
In fact, I have my students do this in class on a data set containing thousands of movies.
We then look for genres.
We look for both what makes sense and then what might surprise us
and also think about what it might be indicating.
The K-means algorithm finds K cluster centers and assigns the object to the nearest cluster center
such that the squared distances from the cluster are minimized.
First, you must choose the number of clusters at the outset.
This is generally associated with the variable K,
which is why the method is called K-means.
Choose K center points or centroids for the clusters you are about to form.
You can pick K points from the set of points you are about to cluster
or even K random points to begin.
Assign each element of the data set to the nearest centroid.
These are the clusters.
Now calculate new centroids.
These are the average or mean of the data points in each cluster.
Again, sign each element of the data set to the nearest centroid.
Once every point is assigned, you have K clusters.
Continue calculating new centroids and then new corresponding clusters
until the clusters don't change.
Let's try this in two dimensions for a very small group of points.
We'll take the points 1, 1, 2, 1, 4, 3, and 5, 4.
Let's take K equal to 2.
Let's take our initial centroids to be for the clusters to be the points 1, 1, and 2, 1.
So 4, 3, and 5, 4 join the cluster with 2, 1 as the centroid
since they are closer to that point than 1, 1.
Now we calculate the center of each cluster.
This is 1, 1 for the cluster 1 since it's the only point in the cluster.
The second cluster has the points 2, 1, 4, 3, and 5, 4.
So the x-coordinate of the centroid is 2 plus 4 plus 5, that quantity divided by 3,
and that equals 11 thirds.
The y-coordinate is 1 plus 3 plus 4, that quantity divided by 3, which equals 8 thirds.
So that gives us our new centroid, which is 11 thirds, 8 thirds.
This time 1, 1, and 2, 1 are in a cluster together since 2, 1 is closer to 1, 1 than 11 thirds and 8 thirds.
And 4, 3, and 5, 4 are in a cluster together because they are closer to the centroid 11 thirds, 8 thirds.
We can again compute the centroids and find which points are closest.
We'll find that the clusters don't change, so that means we have our two clusters.
We did have one big choice, our initial centroids.
It turns out that different initial choices can lead to different clusters.
This can be a downside to K-means, but it is easy to run and quick to check.
That's a huge benefit.
One use of K-means is with down-sampling images, which is usually done to reduce the memory of an image.
Suppose we have a large picture which could take a lot of storage.
If we take K equals 16, we move to saving the image with only 16 colors.
This image becomes this.
We can find the value, thus improving the resolution in certain regions as desired.
Let's step back to get an overview of the family of clustering methods.
In particular, what are you doing and when might you want each?
It's a toolbox, and it can help you know when to pull each one out.
Globs.
That's what I think when I think of K-means.
This method is useful for point-wise data with distances.
Again, it minimizes distance to the center of the clusters, which is why it is good at data that looks for globs in space.
Graphs.
If I have a data that's a graph, vertices, and edges, then I think of spectral clustering, which is also looking for globs, but now in graphs.
Those globs were the dense pink blocks we were seeing in the songs matrix earlier.
Spectral clustering uses eigenvectors from linear algebra to get them more to find those dense connections in the sub matrix and not as much outside of them.
A nice attribute of spectral clustering is that you get a unique solution, and it even finds an optimal solution.
Now, the globs defined using K-means, you have to pick the number of means in advance.
With spectral graphs, you automatically get powers of two, which may or may not be what you want.
You can't tell it to find seven, for instance.
Finally, hierarchical is more flexible, but the results are not always as clear.
The method is really good at multi-level interpretation.
You can easily zoom in and out to find something like sub-genres, and you don't have to know how many clusters you want in advance.
But you do have to specify how to measure the distance between clusters.
It could be the distance between the centers of your clusters, but it also could be the two closest points between clusters, or even the two points farthest from each other in the two groups.
These are only some of the many common methods used in clustering.
Again, many more exist.
Regardless of what method you use, clustering is used as an exploratory method.
Peter Cheeseman and colleagues at NASA developed a Bayesian clustering algorithm called AutoClass that discovered two subgroups of infrared stars where no previous difference was suspected.
In marketing companies group customers into what are called segments, as in segments of a market, the company Claritus, now part of Nielsen, offers a segmentation that groups the United States population into 66 clusters.
Let's look at just one of their 66 categories, up and comers, which are upper income, middle age, or younger without kids.
This is further described as a stopover for younger upper mid-scale singles before they marry, have families, and establish more desk-bound lifestyles.
Such groups are often found in second-tier cities and consist of mobile adults, mostly aged 25 to 44.
This group contains a disproportionate number of recent college graduates who are into athletic activities, the latest technology, and nightlife entertainment.
They order from priceline.com, travel to South America, watch South Park, and drive a Nissan Altima Hybrid.
Is that supposed to be exactly true of everyone in the group?
No, but here lies a fundamental lesson about clustering.
Clustering is about grouping the items so they lose their individuality.
That description isn't entirely anyone I would know in the group, but parts of it might describe people that I would know.
We look for a method that groups each person more accurately than alternatives.
Let's try another example with clusters of the United States based on their presidential election voting records.
Red state and blue states entered popular conversation after the 2000 election, but we can look backward in time for more enduring trends.
New Mexico and Arizona entered the union in 1912, becoming the 47th and 48th states.
Thus, we have presidential election data for 48 states from 1912 through 2008, so we are clustering 48 states using data from 25 elections.
We first form A, where each row represents a candidate in a particular election.
Each election has the two major party candidates and often a fairly well-known third or fourth party candidate.
Some examples are Perot, Buchanan, Thurmond, Theodore Roosevelt in 1912.
Finally, we add a single leftover row for each election for all the remaining candidates.
This results in an 88 by 48 matrix, and we can run a clustering algorithm on that.
Along with Chuck Wessel, a professor from Gettysburg College, I did just that.
The technique we used is called the non-negative matrix factorization, and like spectral clustering, it uses linear algebra.
We'll just look at the results.
The United States has historically been dominated largely by two major parties, so choosing K equal to is a natural start.
We see a large cluster of states stretching across the south and west, joined by Minnesota and New Hampshire.
Now, when examining any of these maps, keep in mind that the clusters do not assign a label corresponding to a particular political party or philosophy.
So let's see what happens if we divide the same data into three clusters.
The new group consists of border states, the Pacific states, plus Michigan and Maine.
How about four clusters?
This is a small group of states, border in Canada plus Massachusetts.
With five clusters, what's new here is splitting the south at the Mississippi River in a new group centered on Texas.
Montana is clustered with Idaho and Wyoming, only when we use five clusters.
When I teach clustering, I always underscore one big issue over and over.
Data analysis or math does the grouping.
With the exception of decision trees, we generally can't immediately see why the groups turn out the way they do.
So once we get the results back, we have to look at them.
When I do clustering work, I often need to collaborate with someone who knows the data or application.
I need their expertise.
So we've learned to group items.
Many times, data analysts take tables of data and use software to cluster.
Many packages come with K-means and hierarchical clustering methods.
There are many more, but inherent to this work is deciding what to cluster.
Two things can go wrong.
One, the distance measure makes no sense.
Two, the clustering itself doesn't fit the application.
The easiest way to see why distance measures change is thinking of driving.
Do you want to know the distance between locations as the crow flies or how far away they are given the roads you have to drive?
Those aren't necessarily the same, and which one you want to know depends in part what you're wondering about.
Clustering algorithms generally have a style of problem they work well on.
For example, the Million Songs database allowed us to create a graph of the artist and use spectral clustering.
We might want to do this even if we had a bunch of ratings by users for each artist.
Why?
It could be that the data is very noisy and, quite frankly, makes it hard for a method to cluster.
In the end, you have the data.
You think about how to measure distance or similarity, and then you choose a clustering algorithm.
Clustering often starts by telling you pretty much what you'd expect.
You may not have needed math to tell you much of what you're seeing, but then comes something unexpected.
Why is it there?
And right there, with that unexpected result, can come the insight.
That can be the fruit of your labors.
You look at it carefully.
If possible, you verify the results and look at supporting data to ground your insight.
And those surprising results can be where you learn something about that item, but often about the overall data too.
In the end, clustering can be most useful when it produces something that you in no way expected.
But even incremental insight can be valuable.
Netflix can use this kind of clustering a great deal to learn more about which movies its customers will most want.
Clustering of users in this way allows Netflix to predict its potential audience for new content.
In fact, it was clustered data about users that allowed Netflix to bid so efficiently and successfully for its hit series,
House of Cards, which appeared exclusively on Netflix.
After all, Netflix was the only media provider that did not ask to see a pilot episode before signing a contract.
Netflix was willing to sign the series for two seasons, not just a few episodes,
and then produce and release the entire first season all at once.
Data from clustering can change the entertainment industry and many others.
Clustering is a powerful widespread technique in data analysis.
It can help identify disease.
It can help businesses, segment items or customers, each of which can then be treated differently.
From political science to medicine to sports to economics, clustering can be a tool to find connections and similarities
in large data sets that otherwise can go unnoticed.
