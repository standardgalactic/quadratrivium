Web robots, often called a web crawler, automatically move through the web retrieving information
about web pages.
For instance, search engines use crawlers to see which web pages link to each other and
even some of the textual content on the page.
Suppose a business would like to know if people repeatedly visit a web site and if so what
products they view.
If someone does visit the same product more than once, do rebates or free shipping aid
in the customer making a purchase.
To analyze such things, we first must remove web robot movement through the web pages.
Rather than analyze such data immediately, we can combine the data in order to better
analyze it.
That is, we use the original data to construct a number of attributes not directly in the
original data.
They include the total number of web pages visited in a web session, the total number
of image pages retrieved in the session, the total time taken by the visitor, the depth
of the web search.
For example, visiting a web page has depth one.
If you click a link on that web page, then you reach the depth of two.
So clicking a link on a web page increases your depth by one.
In short, we are taking the data and essentially making a new data set that we can then analyze
to gain insight on that original data.
This approach was taken in the book Introduction to Data Mining by Tans, Steinbach and Kumar.
The resulting decision tree gives the following insights.
Web robots tend to go over a lot of web pages but with very little depth.
Web pages tend to have much more focused sessions.
They are narrow and much deeper.
Web robots tend not to retrieve image pages associated with web content.
Sessions with web robots tend to be longer and contain a large number of page requests.
Web robots also make repeated requests for the same documents.
Given the results of this decision tree analysis, one can look at a particular visitor and have
a good chance of knowing if it's a web robot and then remove such data.
By doing this, we can gather better information on how humans are moving through a website
and decide how rebates and free shipping, for instance, affect sales.
The decision tree allows us to clear a lot of the noise out of our data and do more meaningful analysis.
Decision trees have some other important advantages.
First, they are simple to understand and interpret.
You don't need to know much about data mining or even much about the data itself to understand
how to use the results.
