A big, big part of data analysis is predicting future outcomes by studying the past.
And we predict where possible by describing a perceived trend with a formula.
The formula might be linear, exponential, parabolic, and so on, but whatever the formula,
it's rare to exactly match the data.
Even so, a formula can be close to the data and capture enough of its essence to give
insight on what might happen in the future.
In this lecture, we look at the power of predicting data just by using equations of lines.
In this sense, we are modeling life as if it were linear.
Let's start by jumping, or in this case running, straight into an application.
In 2012, Usain Bolt electrified the Olympic track and field stadium in London as he won
a second consecutive gold medal in the 100-meter dash.
That race is a premier track and field event of the Summer Olympics.
The winner is crowned the fastest human alive.
This was Bolt's second time to win this title.
In 2008, Bolt ran the fastest 100-meter dash in the Olympics ever.
No other Olympic gold medalist could have beaten him until he ran even faster in 2012.
Turning to data, there have been 28 gold medalists in the 100-meter dash between 1896
and 2012.
Some quick arithmetic shows that there haven't been summer games every four years in that
period.
World wars led to the cancellation of the 1916, 1940, and 1944 games.
So we have 28 times that led to gold medals in Olympic games.
One important part of data analysis is to begin by getting familiar with the data, even
before you try to answer specific questions.
So let's look at these times.
The slowest time was Tom Burke's 12-second sprint to gold in 1896.
The fastest was Usain Bolt in 2012.
In fact, in most of the Olympic games, the gold medal time is faster than the previous
Olympic winning time.
Keep in mind, all these times are fast.
Bolt ran 100 meters in 9.69 in 2008 and 9.63 in 2012.
It takes 400 milliseconds or 0.4 seconds to blink an eye.
So Bolt beats his own time by less than the blink of an eye in 2012 versus 2008.
The difference between Bolt's times in 2008 and 2012 was actually about a tenth of the
time needed to blink.
So what happens in the blink of an eye in such a race?
To help answer this, we can look to Carl Lewis's 1984 run, which only took 9.99 seconds.
This was about a blink of an eye slower than Bolt's 2012 race.
Jesse Owens ran in 10.3 in 1936, which is almost two blanks of an eye slower than Bolt.
This gives a bit more insight on the numbers, but still, what does this really mean?
How far ahead would one runner be than another?
To help with this, we'll compute average speed.
Bolt ran the 100 meter race in 9.63 seconds, which averages to a speed of 23.23 mph.
Carl Lewis averaged 22.39 mph.
So at the end of the race, Carl would have been 3.6 meters or just 12 feet behind Bolt.
Jesse Owens would have been 6.5 meters or about 21 feet behind.
Yes, it's only one or two blanks of an eye, but at those speeds, a lot is happening in
the blink of an eye.
So we have a sense of the speed that these numbers represent.
In particular, this really helps us see how a slight variation can change the time, but
very small changes are important in races at these speeds.
Now let's get a sense of the numbers as a whole.
Can we see any trend in the data?
We already mentioned the numbers are generally decreasing.
One quick way that we as data analysts can get a sense of data is to graph or visualize
it.
In fact, that's one of the first questions I'll ask my students working in data analysis.
Have you graphed it yet?
If the answer is no, we generally sit down and try that first before anything else.
So let's try that together here.
There is some variability.
We see a big drop from 1896 to 1900.
We also see very fast times in 1932 and 1936 when Eddie Toland and Jesse Owens both ran
10.3 races.
There is another drop in 1968 when Jim Hines ran the race in 9.95.
The first gold medal time under 10 seconds.
It wouldn't be until 1984 in Carl Lewis that a sub-10 second time would happen again.
But I look at this graph and I see one overriding thing.
It's why it's in this lecture.
The data can be approximated by a line.
While the line won't pass through every point, it can get close.
If I were wanting to predict future times in this race, that insight right there, simply
from graphing, would be huge.
I graph the data and now I can see how I might model it with a line.
And if the trend in the data continues in the future, we can make predictions as to
how fast we might be running in 2020 or 2040.
We know what we might want to use for our model, but let's learn how to do it.
We want a line so we could simply connect two points.
Let's go with Tom Burke and Carl Lewis times in 1896, the 1988.
We have two points, so we connect them with a line.
Before answering questions, it's important to see how we've done.
That's why I didn't connect a point to Usain Bolt.
If we did, we don't have anything to compare to.
If we take that line that connects Burke and Lewis and extend it to 2012, it projects that
Usain Bolt would have run in 9.38, which would have been amazing, but it is off by 0.25 seconds
or about 8.5 feet.
Part of this is due to the big drop between 1896 and 1900.
That creates a steeper slope that leads to such decreased times.
So we can pick another point.
For example, suppose we work with Archie Hahn's 1900 race of 11 seconds.
Now we do much better and estimate Bolt's time at 9.625 to his 9.63 performance.
It also predicts Bolt's 2008 time at 9.67 to his 9.69 performance.
We've done well, but notice how we're omitting a lot of data.
I also had to choose another point.
I chose the 1900 time.
But what if I also created a steeper slope?
Maybe I could try yet another time.
That's a lot of tinkering, which can bring in my own biases into the computations.
I'd rather, one, use all the data, and two, let the data, without a lot of fiddling on
my part, create the line.
But these points don't lie on a line, so how do I know what line to choose?
Well here, we use the well-used tool of regression, a word that expresses an old idea that we
regress the data back toward whatever model we create.
Before producing the line, let's think about what line to choose.
We are going to approximate the data with a line, so we want to find a line that approximates
or fits the data.
Let's think of only three points that don't lie on a line.
We want the line to be as close to all the points as possible.
When we were connecting the endpoints earlier, we were making the line exact for those points
and not necessarily close for the other points.
Suppose I'm looking for a line that fits four points.
We'll find a line that may not pass through any of the points.
We'll find how far off they are by their vertical distance to the line.
We see this in green in the image.
We could just measure the distances from the line, add those up, and be done.
But that ignores the fact that some values are going to be close to any line we choose
while others will be farther away.
We'd like to choose a line that minimizes those maximum distance points, keeps those
far distances as small as possible.
So instead of taking the least distance for each point, we take the least squares distance.
This is clever because by squaring, our biggest values are now huge, and minimizing those
huge values gets much more attention.
So least squares says take the square of all those distances and add those up.
Whatever line makes that small, that some smallest, is our best fit.
So let's use a least squares regression line for our Olympic times.
Now if we do, we find the equation of the line, y equals negative 0.0133x plus 36.31.
Before we go any farther, how did I just do that?
How did I find that equation?
I used Excel actually.
There are several tools that will do this for you.
From Excel to a program called JumpJMP to SAS to R and many others.
Here I simply had a table of data.
One column contained the years and the other the times.
Then I essentially hit a button and that formula came out for the line.
That's often how regression is done, a powerful technique quickly computed on a computer.
In a bit, we will do more examples as the most important part is learning to think how
to build those columns since the computer will do the rest.
Let's return to our formula for the Olympic times.
It helps if we plot the line in points.
We see that the time from 1896 is still an outlier.
Remember though, it is still being calculated into the data and because we use least squares
distances, our line is closer to that outlier than we would have using least distance.
Notice how close the line is to various points.
For 2012, it predicts Usain Bolt's time at 9.599, which is only 300s off.
Keep in mind it is also doing the best in a least squares sense for every other time
from 1896 until 2012.
Now let's look at our least squares line a bit more.
Remember the slope of the line is negative 0.0133.
Remember x is the year and y is the time.
So this slope predicts that every year we expect to drop the Olympic gold medal time
by just over a hundredth of a second.
So over four years we expect to drop by just over five hundredths of a second.
Could we see a time under nine seconds?
Beware of assuming that every regression line continues indefinitely.
Clearly there is some limit at which a human cannot run any faster.
For instance, the 100 meter dash will never be completed in two seconds.
However, where could the limit be between two and 9.63 seconds?
A book called The Perfection Point considers such a question.
It's by John Brankus, the host of ESPN's sports science show.
Brankus analyzes four distinct phases of the race, reacting to the gun, getting out of
the blocks, accelerating the top speed, and hanging on for dear life at the end.
He gives an analysis of why 8.99 seconds is the fastest hundred meter dash that can ever
be run.
Let's take the first phase.
The rules of international competition assume that it takes 0.1 of a second to react to
the gun.
A runner who begins sooner is considered a false start.
Bolt took 0.165 seconds to react, so there's room of about five hundredths of a second
for improvement there.
The point is, we assume our least square line continues, but not all the way down.
Instead, we can use some other information and set a theoretical lower limit.
Then we can use that limit.
So when would we reach the speed limit for this race?
Doing the algebra on our line, we find this would happen in 2058, which would mean we'd
reach the limit of speed in the 2060 Olympics.
It's just a model.
It may do pretty well for a while.
If the times begin to flatten out as we approach that absolute limit, the model would also
break down.
But it may have some predictive power.
We don't know exactly how much or how long, but it can still be quite useful.
Such plots can also illustrate when someone has significantly, significantly broken from
the trend.
This time, let's graph world record times in the hundred meters whenever they happened
in or out of the Olympics.
If we again perform least squares on this, we get a fit.
But notice the outliers at the beginning and end.
That value at the end has a huge drop.
It's computed into the least squares fit, but it's still a big drop below the overall
line.
What is that?
Indeed it is again Usain Bolt and his world record time of 9.58 on October 16, 2009.
This was in between his two Olympic races, and it was the fastest time of all.
Looking at the least squares line and the data emphasizes the significance of that world
record.
In all of this, we have used only two variables and used one to predict the other.
But regression can also be a much more powerful tool.
Regression is often used to determine how strong a correlation is.
For the Olympic winners, the correlation coefficient, often called R, was negative 0.91.
For the world records, the correlation coefficient R was even better at negative 0.97.
These are extremely high correlation values.
Depending on your field, much lower values may still give insight.
The square of the correlation coefficient is another commonly used statistic.
What counts as a good R square value varies enormously from field to field.
But the correlation coefficient is where the R square you might see actually comes
from.
Your regression may also have far more than two variables.
Even a hundred variables or more could be involved.
Let's look at one with over a dozen.
In May of 2007, a letter was sent to college and university presidents in the United States
concerning the U.S. News and World Report college rankings.
The letter was the president's letter and was initially signed by 12 college presidents
with others joining later.
Among several points, the letter stated that the presidents believe that the rankings should
be limited to, quote, data which is required to be reported to state or federal officials
or which the institutions believe in accordance with good accountability should routinely
be made available to any member of the public who seeks it.
A main request was for college and university presidents not to participate in the, quote,
reputational survey by U.S. News and World Report.
In the survey, college presidents give their subjective opinion of other colleges.
The letter also asks presidents not to use the rankings as a form of publicity.
How much does the reputation score play into the rankings?
Now, U.S. News and World Report doesn't detail the exact formula they use for the rankings,
and no one has reversed engineered it either.
With regression, we can try something different.
Can we estimate it enough to gain insight?
We just need data, which in this case is available.
For me, it only took a quick email to the Associate Vice President of Planning and Institutional
Research at Davidson College.
Within an hour, I had a spreadsheet with the published data on 26 National Liberal Arts
schools.
Each row represents a college, and the columns are categories like academic reputation, selectivity
rank, and acceptance rate.
I had the data, so now I could explore the president's claim.
With the data, I wanted to estimate how much to weight each category to compute the final
score.
Suppose I had three categories, then I wanted to find weights, W1, W2, and W3, such that
the final score would equal W1 times Category 1, plus W2 times Category 2, plus W3 times
Category 3.
I want to get the final scores my formula were produced as close as I can to the scores
computed by U.S. News and World Report.
So my inputs are the categories, and my output is the final score.
Earlier for the Olympic data, the input was the year, and the output was the time.
I often mention in class that I look at it this way.
What information will I be given, and what do I hope to know how to compute?
I will be given the U.S. News and World Report data, and I want to know the final score they
compute.
So I have it all in a table, and again I can turn to software to produce the weights.
I used mathematical software called MATLAB this time.
Earlier I used Excel.
What did I find?
Look at the top five weights.
Look at that huge positive weight.
It's more than twice the size of the next highest weight.
It's indeed the category for academic reputation.
So does this settle the matter?
No, but it gives insight.
I often tell my students that now you have an opinion on and from the data.
This certainly supports the President's claim.
This is the power of regression.
Once you have the data in place, a regression can quickly be done.
And even when you have data in more than three dimensions, this shows how quickly a picture
can be obtained that gives insight.
We see the size of that weight instantly.
Regression is used in various fields, with economics being one of the leading areas.
One reason for this is because regression enables us to artificially change one variable
while holding all the others constant.
We saw this with our gold medal times.
Each year reduces the time by about one hundredth of a second.
This can be quite helpful in business.
Suppose you have data on sales, prices, and promotional activities.
Regression can give you insight as to what would happen if sales prices were to increase
by five percent.
What if promotional activities were increased by ten percent?
This helps marketing.
In their very popular book Freakinomics, Stephen J. Dubner and Stephen D. Levitt look
at regression as their tool to bust some of the myths, for example, about parenting.
Listen to some of their findings.
They use data to see what factors correlate to test scores.
They're calculating correlation coefficients and seeing which factors have the highest
r-squared value.
What's correlated?
Well, remember, correlation can be positive or negative.
Here are two that help.
The child has highly educated parents, positive correlation.
The parents have high socioeconomic status, positive correlation.
These aren't all that surprising.
That can be the case with data analysis.
Sometimes the results are what we expect.
They should be.
We shouldn't always be seeing what we don't perceive or expect in our world.
But we can also get new insights that isn't what we expect.
This can be the case for some of the factors Dubner and Levitt found that don't correlate,
not positive correlation, but not negative correlation either.
Here are two.
The child's parents recently moved to a better neighborhood.
That is, better neighborhood doesn't mean better test scores.
Disruption from moving doesn't hurt test scores, so no correlation.
What about this one?
The child's family is intact.
Sounds great, but didn't help test scores.
On the other hand, family not intact didn't hurt test scores, again, no correlation.
We often have variables like intact versus not intact, and they are just as easy to use.
In fact, it's important to note there is another form of regression we can use in such
cases.
It's called logistic regression.
Before learning this, let's review what we've done.
When we started, we had two variables, an x and a y.
X was the year of the Olympic Games, and y was the gold medal winning time from that
year in the men's 100-meter dash.
But we can also have more inputs.
Instead of just one variable x, the studies could have used 10, 20, 100, or even hundreds
of variables.
But still, all the x variables, however many, combine to produce just one y.
Now, in logistic regression, we still produce a y and have various input variables.
But with logistic regression, the x values take only zeros and ones.
They are on and off.
Intact family or not, male or female, just two values.
One application is digit recognition.
Can a computer recognize your handwriting?
Well, that depends on your handwriting, but regression can help.
We talked about this in the lecture about training data, and we used simple averages
to sort through all the pixels for a given number.
Now we can do the same thing, but we can bring in the power of Lee Square's regression.
That will help us do a better job bringing outliers into our model, and handwriting is
an area filled with outliers.
So, where's the regression?
Exactly.
That is what you as a data analyst would need to figure out.
So let's try it together.
Consider just determining if you wrote the letter B. You'll scan your handwritten B into
a computer and place it into a grid of dots.
The dots in the grid are filled or not, depending on whether your B passes through that grid
point.
A variable is 1 or 0, depending if the corresponding dot in the grid is filled or not.
Then I take lots and lots of handwritten B's.
I use logistic regression to say, when the dots are filled, it's a B.
In another case, when the dots were filled, it was again a B. But in another case, when
a group of dots were filled, it was not a B.
So I get your handwritten B. It isn't any of the ones I've seen.
I look at which dots are filled and not filled.
Regression gives me a formula.
My variables will be 1's and 0's depending on which grid points were filled in your handwritten
B. Then I get a value out.
The value is almost never exactly 0 or 1.
Instead it is a value in between.
This can be thought of as giving me a probability of whether you wrote the letter B. This is
part of computer vision and classification, an important field in data analysis.
The United States Postal Service began researching how to have a computer read addresses in 1983.
The first prototype was deployed during the Christmas season in 1997 to read handwritten
addresses.
In that system, the Postal Service used in 1997 rejected 85% of envelopes.
It correctly identified the address in only 10% of those it read.
It doesn't sound all that great.
But it was already considered a success given the difficulty of this type of problem.
And it already saved the post office several thousand dollars.
Today, the large majority of letters are read and sorted entirely by computer.
Current reading success rates are above 90%.
So neither snow nor rain nor scribbled zip code can keep the Postal Service and their
computers using data analysis from delivering the mail.
Regression is a powerful tool in data analysis, whether saving big time dollars for the Postal
Service or aiding policymakers.
Regression can give us a better sense of what is correlated.
And a regression model does not have to be linear.
You could do least squares regression on a parabolic model, an exponential model and
so on.
Software to do this for you is getting better all the time.
The important thing is to understand what kind of correlation you are looking at.
Remember, correlation doesn't tell us why.
That can be its own challenge in step and data analysis.
Once we have a correlation, sometimes we use hypothesis testing to figure out why we have
a correlation.
Yet, regression is also a great tool all by itself, enabling us to model the past to
understand the future.
Listen and read news carefully.
You'll see how frequently regression is used in data analysis.
It's surprising that a well-placed line can so often unlock insight on our world.
