The 2004 indie comedy, Napoleon Dynamite, became a well-known case for this during the competition.
For example, it drove Len Bertoni crazy. He was a semi-retired computer scientist living outside Pittsburgh.
In 2007, his sister-in-law emailed him news of the Netflix million-dollar competition.
A year and a half later, he was still working a lot, often 20 hours a week.
When he had an idea, he'd write a computer program and then test it. His results improved.
When he got above 8.5% better than Cinematch, which was Netflix's algorithm, his progress was at a crawl.
His competitors indicated their progress was stalling too.
Why? Briefly, Napoleon Dynamite, an indie comedy from 2004, that achieved cult status and went on to become extremely popular on Netflix.
Will you like it? It's really, really hard to say.
On a regular hit like Meet the Parents, Bertoni could usually get within 0.8 of a star of what someone would rate it.
With Napoleon Dynamite, his algorithm would produce a prediction. It will predict, but on average, it would be off by 1.2 stars.
That might not sound like a big difference, but that prediction is 50% worse.
Napoleon Dynamite is a polarizing movie. It contains a lot of ironic humor, including a famously kooky dance during a student council election.
It's a movie someone tends to love or hate. The Netflix ratings reflect this.
The movie's been rated a million thousands of times in that Netflix database, and the ratings are disproportionately 1 or 5 stars.
Even worse, if you and I have similar tastes on most films, we may not agree on Napoleon Dynamite.
In 2008, when Bertoni's work had stalled at 8.8% improvement over Cinematch, we computed that his single movie was causing 15% of his remaining error rate.
Napoleon Dynamite caused the most trouble, but it wasn't the only such polarizing film.
Other hard-to-classified films included Lost in Translation, Fahrenheit 9-11, Kill Bill Volume 1, and Sideways.
So what led to the final push that allowed a team to win?
Remember, the Netflix prize took years of work. As such, it is rather amazing that the final first-place winner crossed that digital finish line of 10% merely minutes ahead of the competitors.
The team was Bellcore's Pragmatic Chaos.
Interestingly, it was only at the award ceremony when the winning team of computer scientists, electrical engineers, and statisticians were actually in the same physical place at the same time.
Even more striking, and possibly more important to us as data analysts, is that the first-place winner and the second-place winner, called the ensemble, were amalgamations of teams which started off competing separately for the million-dollar prize.
It's when separate teams joined forces with other teams that the final leap beyond the 10% was made. It was by combining teams and algorithms into one complex algorithm that those final advances were made.
Interestingly, the ideas farther away from the mainstream proved to be the most helpful at making the final improvements that won the prize. For example, what about the number of movies rated on a given day?
This information didn't predict much on its own, but movies rate differently on the day they are seen compared to movies reviewed long after. And it turned out that how many movies were reviewed at once could be used as a proxy for how long it had been since a given viewer had seen a movie.
In a sense, the prize-winning algorithm was a meta-algorithm that combined weights for a variety of simpler algorithms.
Recommendation systems appear all over the internet. Amazon recommends movies and books based on the one you're looking at.
A different kind of example is Pandora, which has over 70 million active listeners each month. Pandora's success is rooted in an idea that was a commercial failure, the Music Genome Project.
