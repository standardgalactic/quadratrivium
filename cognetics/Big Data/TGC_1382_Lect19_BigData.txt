Data compression is powerful. It allows us to store, access, and use far more images.
Think of the Mars rover, for example, sending back many more images thanks to compressed images.
But the mathematics inside data compression is even more powerful and can be used in other ways, too.
In particular, we will see that the same methods for throwing out data can be adapted to improve an online recommendation system.
It's almost incredible. A technique for using less data helps create a recommendation that's even better.
Recommendations are now big business, at least for organizations that do them well.
But how do they do it? How might we do it?
The latter, how would we do it, was a question Netflix asked when they put together a million-dollar competition just to improve their recommendation system.
It's a great example. To enter the Millionaires Club, you needed to do much more than recommend a film to me on the couch.
In a sense, you had thousands and thousands of people sitting on the couch telling you the films they like and how much they liked them.
It came via the Netflix ratings data. To win the cash, your computer algorithm had to do a better job of predicting the Netflix existing recommendation system at the time called Cinematch SM.
Your goal, take Netflix's dataset of users' ratings, which are integers from 1 to 5 for all movies.
From that, predict how a user would rate other films.
Note, this is harder than just recommending. You are actually trying to predict how much I'd like a film.
In the competition, Netflix supplied data on which you could test your ideas.
When you thought you had something, you test your recommendation method by predicting ratings of movies and users in another dataset.
Your recommendation system had to predict future ratings.
So that second dataset had actual ratings of movies and you test your predicted ratings against the real ratings of movies.
If your predictions were at least 10% better than the recommendation system Netflix used, Netflix would write a check for a million dollars.
In data analysis, you need to know the nature of the data.
Netflix gave you users and movies. One way to store this is in a table.
A column is one user's ratings for all the movies, with a zero being a movie that wasn't rated.
So a row contains all the ratings for one movie.
We connect a user and movie with a line if the user rated the movie.
We associate a number with the line where the number equals the rating of that user for the movie.
This type of diagram is called a bipartite graph.
The lines are called edges.
For this application, the numbers or weights represent the number of stars a Netflix user gives it.
For another application, the weights could be a one for a thumbs up or a negative one for thumbs down.
So you have all this training data in the form of preferences.
Remember, the goal is to use that data to predict ratings.
So I'm on a couch by the TV and suppose I mentioned enjoying Braveheart.
What can you recommend?
You have a slew of data on me and other users in movies.
One approach is to find the users whose opinion are closest to mine.
What did that person like?
Recommend that movie to me.
If you are trying to predict my rating, what rating did that person give?
That makes sense, but how do we figure out who in the data is most like me?
There are a few ways to do this.
One way is to simply treat the row as a data point in higher dimensional space.
So if you have two data entries per row, then you might have a point like one five and four five.
This puts it in two dimensional space because you have two points.
If you have five entries in a row, then you find the distance between those points in five dimensions.
For our earlier example with two points, you find the distance in two dimensional space.
So let's return to five dimensions.
Suppose you rate movies as four, five, four, two, one.
I rate them as four, three, two, four, one.
Our friend Eric rates them as four, one, four, two, four.
Then you find the distance between your data and mine.
We subtract for each movie, square the result, add up those squares,
and take a square root to get the square root of four minus four squared plus five minus three squared
plus four minus two squared plus two minus four squared plus one one minus one squared,
which is about 3.46.
If you find the distance between my data and Eric's, then you would find the distance to be five.
Another way to measure distance is known as jacquard similarity.
This equals the number of preferences in common divided by the total number of things.
Then the jacquard similarity between you and my moving ratings is two-fifths,
since you and I rated the first and last movies the same.
But the data for you and Eric has a jacquard similarity of three-fifths since you have three ratings the same.
So if you choose someone else to go to a movie with, it would be Eric.
Note that this captured that you and Eric rated two movies with fours and one with a two.
So you have things you like and dislike.
Note, these two distant measures are different.
So you'd want to think carefully about what they both mean in terms of the question you're asking.
For instance, jacquard only considers when ratings are exactly the same.
Any other ratings are ignored, whether they're close or not.
Measuring distance like this can have another problem.
Suppose we are looking at data that has age and political stance.
Imagine that as you get older, you become more conservative.
If that's true, then when we count agreement in two people by age and by politics, then we'd be double counting.
This leads to overfitting in bad performance.
To help with this, we need to reduce what is called the dimension of the problem.
We need to get rid of such redundancies.
Before we do though, let's see how hard this can be.
Remember, Netflix was willing to pay a million dollars to improve an existing system.
So it isn't all that surprising that this can get tricky.
So here are some potential hurdles.
Some features are more informative than others.
Maybe my age has little or nothing to do with whether I like a particular item.
Maybe being male correlates a lot.
Generally, there is a lot of missing data.
For example, the Netflix data set had nearly 18,000 movies,
but the average number of reviews per person was only 200 movies.
So most movies are going to have a lot of missing values.
People who are very similar might have very few overlapping items.
Measurement error.
People may lie.
Remember our earlier lecture on complexity in computation.
If we have a great method, but it takes a month,
it won't help on problems that need an answer in minutes or seconds.
Preferences may change over time.
Research on changing preferences help the winners earn the Netflix prize.
First, viewers tend to rate movies differently depending on the day of the week.
Some users are in good moods on Sundays, for example.
Second, some movies rate better immediately while others rate better in hindsight.
When the Netflix prize competition was announced,
initial work quickly led to improvement over existing recommendation systems.
It didn't reach the magic 10% at that point,
but the strides were impressive nonetheless.
The key was using linear algebra, specifically a technique called the singular value decomposition.
This is often called the SVD.
It helps to understand how this method works and particularly why it works.
It is helpful also to see the SVD in the context of data compression.
The idea in data compression is expressing a lot of data by less.
Let me just say up front that even many practicing researchers using the SVD
haven't studied the underlying mathematics.
It's often not covered even in advanced courses on advanced data analysis.
But here's the basic idea.
We take our matrix of data and split it into a product of three different matrices.
There are many different ways we could do that.
What is great about the SVD is that it's a method where all three of its component matrices
retain valuable information.
Even more specifically, the information is spread across three matrices
in a way that we need to pull a little information from each to get a good result.
The result is almost like magic.
The actual decomposition of the SVD is a bit like factory numbers.
You remember that, right? The prime factors of 315 are 3 squared times 5 times 7.
We can say that we decompose the number 315 into its prime factors.
When we decompose a matrix, the feel is similar, but the result is much more complicated.
We don't get only prime numbers.
Instead, the numbers in a matrix decomposition can be any numbers.
So when you see the result of a decomposition, it's not possible to see at a glance that you've made any progress at all.
But you've made a lot of progress.
The outputs can look a little strange, and the math underlying those outputs is too complicated to explain right now.
But the results are worth it.
Just trust me on this one.
The SVD is a powerful, powerful tool.
It even has interesting theoretical properties to it.
But what's important is to be able to see it and to use it, not necessarily to understand everything about how it's derived or computed.
So we'll assume we have a matrix calculator that has an SVD button.
Given a matrix, hit that SVD button, and it gives you the singular value decomposition.
So what is this decomposition?
The SVD is defined by three matrices and can be found for any matrix.
It does not have to be a square matrix, such as 3 by 3 or 4 by 4.
Suppose we have a matrix with four rows and three columns.
Let's call it B.
Then the program that computes the SVD would return three matrices.
The first is U, which has the same number of rows and columns as B.
Four rows, three columns.
The second matrix is called sigma.
This is the singular values, and that's where they're located.
Here, the sigma matrix would have three rows and columns and be zero everywhere except on the main diagonal.
Those diagonal elements are called the singular values.
They'll be directly important to us a bit later.
Finally, there is V.
It would also have three rows and three columns.
It's a decomposition, so the product of U times sigma times V equals B.
Okay, we have the singular value decomposition, but what do we do with it?
Remember, one application is data compression.
So let's compress a grayscale image.
A grayscale image can be represented as a matrix.
If an image is 600 by 400 pixels, then it can be stored in a matrix with 600 rows and 400 columns.
Each element of the matrix holds the grayscale value of the corresponding pixel in the matrix.
We'll consider a matrix A that has 648 rows and 509 columns.
The elements of the matrix correspond to the grayscale pixels of the Renaissance engraving.
Melancholia won by the German artist and amateur mathematician, Albrecht Duerre.
So we end up with a matrix with grayscale values, in this particular case, ranging from 1 to 128.
In general, those values will range between 1 and 255.
So we get our singular value decomposition.
Now we are ready to compress.
We can do this by taking only a few numbers from each of our three new matrices.
Let's create a very, very compressed version of this picture.
It will give us a sense of how this is done.
We simply take the first column of U, the largest singular value of sigma, and the first row of V.
We multiply them together using linear algebra and we get a matrix that's the same size as our original matrix.
The compression is easily seen in the storage.
The original matrix stores 648 times 509, which equals 329, 832 numbers.
Our compressed image requires storing one column of U, which is 648.
One singular value and 509 numbers from the first row of V.
So that's 648 plus 1 plus 509, which equals 1,158 numbers.
So this compressed image takes only 0.35% of the storage of the original image.
The reason I'm laughing is look at what we get when we use this compressed image.
Not that compressive.
That's because we compress too much.
Let's try the first 10 columns of U, the 10 largest singular values of sigma, and the first 10 rows of V.
Now the image looks like this.
This takes about 3.5% to store compared to the original image.
Remember, the image has over 600 rows and 500 columns.
So let's try 50 columns of U in the 50 largest singular values and 50 rows of V.
We are reducing the dimensions of our data.
And yet this image looks pretty much like the original.
If you look closely, you're more likely to spot the image that's compressed.
But note, this compressed image only takes 17.5% of the storage.
And if you have an image using only 50% of the original, it is very, very hard to spot the differences.
This gives us a sense of how image compression can be done.
Compressed too much and you get a fuzzier picture than the original.
If you compress less, it can be very difficult to see a difference between the original and the approximation.
Now you can't zoom into the compressed picture as much.
But you can look at a lot more images before deciding which ones might be of interest.
Now, what we've done with image compression can be done with any array of numbered data.
In image compression, we identified significant components of the data and represented the larger data by a smaller set.
We can do essentially the same thing with non-visual data.
So let's represent our recommendation data in compressed form and use that to do our analysis.
Suppose we have Sam, Mel, Mike and Fred recommending six movies.
Here are their ratings.
We make this table of data a matrix with six rows and four columns.
Then we produce the SVD.
Again, U has six rows and six columns.
Sigma has six rows and six columns and V has four rows and four columns.
Then we reduce this to a two-dimensional problem by taking only the first two columns of U, two singular values and two rows of V.
Now we can treat the first row of V as coordinates for Sam.
So we can plot Sam's position as the point negative 0.5710 and negative 0.2228.
We do the same thing for Mel, Mike and Fred.
Remember, these points are consolidating all their movie ratings into two numbers each.
This is the same as compressing an image down into an image with much less storage.
The difference is that we've not only got lower resolution in our data,
we actually make it easier for our recommendation system to work.
Now, who is the most similar? Fred and Sam.
Let's go back and look at the data itself.
Consider the ratings for Fred and Sam.
There is indeed a lot of similarity.
The key here is that we never had to look at the data.
For large data sets, we can't absorb the numbers.
Further, we use mathematical measures of distance rather than looking at the graph
since we needed to go all the way down to two dimensions to see things in this case.
Remember with the picture, we were best using 50 or more columns of U.
Same will be true here. If we had 600 ratings, we would need much more than two columns of U.
In the picture, this would have been a very, very blurry image.
We don't have enough resolution to reach meaningful conclusions.
Now what? Suppose we get a new user, Jess.
We can compress her information down to two points and plot them in the plane.
We find who she is most similar to. Suppose it's Mel.
Suppose Jess hasn't seen Movie 1 or 3.
Then, we recommend Movie 1 and then Movie 3 since Movie 1 got a higher rating from Mel than Movie 3.
It's helpful to realize that we can also treat the columns of U as coordinates for the movies and find movies that are similar.
In this way, you create computational genres.
Now, there are a few more issues here. First, how low should our dimension go?
There isn't a hard and fast rule here, but the trick is using the singular values.
Let's assume we have 100 user ratings, about 60 movies from 600.
So, A will be a matrix with 600 rows and 100 columns. How many columns of U do we take?
The question is easier to pose as how many singular values do we want, and the key there can be seen if you graph them.
Note, the really quick drop-off. This is characteristic of singular values.
This is why we generally don't need a lot of singular values.
This is why, for example with the Durer image, earlier we got a reasonable image with only 50 singular values.
If this algorithm is so straightforward and creates good results, why did the Netflix prize take time?
Well, had the competition been to improve the algorithm by 5%, it would have been over quickly.
But it wasn't. Those last percentage points took more work.
As not everything is easily predictable. Remember the issue of time. How you rate movies can depend on when you rate them.
Further, some movies are simply hard to predict.
The 2004 indie comedy, Napoleon Dynamite, became a well-known case for this during the competition.
For example, it drove Len Bertoni crazy. He was a semi-retired computer scientist living outside Pittsburgh.
In 2007, his sister-in-law emailed him news of the Netflix million-dollar competition.
A year and a half later, he was still working a lot, often 20 hours a week.
When he had an idea, he'd write a computer program and then test it. His results improved.
When he got above 8.5% better than Cinematch, which was Netflix's algorithm, his progress was at a crawl.
His competitors indicated their progress was stalling too.
Why? Briefly, Napoleon Dynamite, an indie comedy from 2004, that achieved cult status and went on to become extremely popular on Netflix.
Will you like it? It's really, really hard to say.
On a regular hit like Meet the Parents, Bertoni could usually get within 0.8 of a star of what someone would rate it.
With Napoleon Dynamite, his algorithm would produce a prediction. It will predict, but on average, it would be off by 1.2 stars.
That might not sound like a big difference, but that prediction is 50% worse.
Napoleon Dynamite is a polarizing movie. It contains a lot of ironic humor, including a famously kooky dance during a student council election.
It's a movie someone tends to love or hate. The Netflix ratings reflect this.
The movie's been rated a million thousands of times in that Netflix database, and the ratings are disproportionately 1 or 5 stars.
Even worse, if you and I have similar tastes on most films, we may not agree on Napoleon Dynamite.
In 2008, when Bertoni's work had stalled at 8.8% improvement over Cinematch, we computed that his single movie was causing 15% of his remaining error rate.
Napoleon Dynamite caused the most trouble, but it wasn't the only such polarizing film.
Other hard-to-classified films included Lost in Translation, Fahrenheit 9-11, Kill Bill Volume 1, and Sideways.
So what led to the final push that allowed a team to win?
Remember, the Netflix prize took years of work. As such, it is rather amazing that the final first-place winner crossed that digital finish line of 10% merely minutes ahead of the competitors.
The team was Bellcore's Pragmatic Chaos.
Interestingly, it was only at the award ceremony when the winning team of computer scientists, electrical engineers, and statisticians were actually in the same physical place at the same time.
Even more striking, and possibly more important to us as data analysts, is that the first-place winner and the second-place winner, called the ensemble, were amalgamations of teams which started off competing separately for the million-dollar prize.
It's when separate teams joined forces with other teams that the final leap beyond the 10% was made. It was by combining teams and algorithms into one complex algorithm that those final advances were made.
Interestingly, the ideas farther away from the mainstream proved to be the most helpful at making the final improvements that won the prize. For example, what about the number of movies rated on a given day?
This information didn't predict much on its own, but movies rate differently on the day they are seen compared to movies reviewed long after. And it turned out that how many movies were reviewed at once could be used as a proxy for how long it had been since a given viewer had seen a movie.
In a sense, the prize-winning algorithm was a meta-algorithm that combined weights for a variety of simpler algorithms.
Recommendation systems appear all over the internet. Amazon recommends movies and books based on the one you're looking at.
A different kind of example is Pandora, which has over 70 million active listeners each month. Pandora's success is rooted in an idea that was a commercial failure, the Music Genome Project.
Pandora was launched in 2000 by Tim Westergren and a small team of entrepreneurs. They wanted to create a database of musical characteristics for a given song in order to identify other songs with similar characteristics.
They were, at least metaphorically, going down to the DNA level of music. Instead of starting with listener ratings, they built up from data about the music itself.
How do you find songs that have similar attributes?
Now, computer algorithms could do this, and they still can. But the Music Genome Project team believed such identification required a human touch.
So they hired musicians who knew music theory, for instance, to listen to each song. Then they broke the song down by as many as 450 predetermined attributes, giving each a numerical value.
They didn't get much response to license the music recommendation data, so in 2005, Tim Westergren co-founded Pandora.
It uses Music Genome Project data to generate the custom playlist for its users, and yes, it still uses real live people to listen and evaluate music.
This is very different from what happened to the in-house reviewers at Amazon, by the way, who were dropped very early in Amazon's history.
There is another human factor in the process, the user's interaction with the program. You can skip a song to hear a new one, or if you like a selection, click a thumbs up, or if you don't, click thumbs down.
Pandora tracks user interactions, thumbs up, thumbs down, or skipping a song immediately, affect what is played next for you. But they don't all affect it to the same degree.
Skipping a song carries less weight than a thumbs down. Skipping a song doesn't have as clear of a meaning. You might skip a song because you've heard it too much, or simply aren't in the mood at the moment.
But it also depends how often you use a feature. If I rarely skip and have been using Pandora for some time and then do skip, that says a lot more about that interaction.
Yet another mechanism is to use the wisdom of the crowd to filter and evaluate material. This is used by Reddit, which is a social news and entertainment website.
On it, registered users submit content in the form of links or text posts. Users then vote submissions up or down to rank the post and determine its position on the site's pages.
In 2013, there were over 100,000 subscribers.
This type of site uses what is called collaborative filtering. It filters large amounts of information by spreading the process of filtering among a large group of people.
Rather than having one main editor or group of editors like a newspaper or magazine might have, the collaboratively filtered social web has its entire set of subscribers as editors, which also encourages participation.
There are two basic principles involved in collaborative filtering. First, there is the wisdom of the crowds and the law of large numbers.
According to this, as communities grow, they make better decisions. In fact, this same idea is behind YouTube. People submit videos and the wisdom of the crowd enables the best videos to bubble to the top.
The second principle of collaborative filtering depends on the community being large enough with enough data on individual participants and how the individual participants collaborate or correlate with each other.
The second principle suggests that we can make predictions about what these users will like in the future based on their tastes, what their tastes have been in the past.
That is, we can include collaboration when we create recommendations.
So the next time you're shopping around for a movie to watch, in fact doing any kind of shopping, think about how the mathematics of data reduction can make it possible to decompose a recommendation based on thousands of people.
Of course, the recommendations work best if you offer some ratings yourself. Data analytics can then predict not just if you'd like a particular movie or not, but what score you might give it. Even Napoleon Dynamite.
