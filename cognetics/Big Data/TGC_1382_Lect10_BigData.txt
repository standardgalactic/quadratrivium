Statisticians have been conducting election polls for many decades.
But such polls aren't always accurate, especially for close elections.
But recent data analytics does much better than ordinary polling.
The secret is combining multiple polls.
It turns out that clever aggregation of multiple data sources produces much more accurate predictions.
And that also transforms political campaigns.
To see this, let's go back to the 1920s.
Political predictions at the time were notoriously unreliable and people were looking for new
ideas on how to forecast the upcoming presidential election between Herbert Hoover and Alfred Smith.
Now, a popular magazine of the time was Literary Digest.
And the Digest employed a powerful new tool called a poll to accurately predict Hoover's
landslide victory in 1928. Their poll was also accurately predicted Hoover's demise in 1932.
Then came the next presidential election.
The Republican nominee in 1936 was Kansas Governor Alf Landon.
He was running against first term president Franklin D. Roosevelt.
The Literary Digest predicted Landon would receive 57 percent of the popular vote to
Roosevelt's 43 percent. They came to this conclusion after sending out 10 million surveys
and getting 2.4 million back. FDR, our future four-term president,
won with 62 percent of the popular vote.
So what happened? Why was the Digest so wrong in predicting this election,
especially when it was so accurate in previous ones?
Well, this election occurred in 1936, which put it deep in the shadow of the depression.
As a result, many people were rapidly eliminating luxuries from their lives.
But the Literary Digest got their mailing list for their surveys from telephone directories,
automobile registration records, and their list of subscribers.
And here's the key. Each one of these activities, whether we're talking about using a telephone,
driving a car, or subscribing to magazines, each one of these was considered an unnecessary luxury
during the depression. What does this mean? Well, what it means
is that the Digest method analyzed how the wealthy population of the United States would vote.
But this isn't what they thought they were predicting.
They thought they had accurately predicted the election. Now, the story gets even better.
There was another young pollster of the time. He recognized the errors in the Digest data
and made his own poll. He surveyed only 50,000 voters compared to the 10 million voters
surveyed by the Digest. But here's the key. He made his set of voters more representative
of the voting population. And as a result, this new poll predicted FDR's win with only half a percent
of the data used by the Digest. Now, here is what may be the most impressive part.
This new pollster saw the Digest error and was able to compute what their prediction would be
within one percent. The new pollster was named George Gallup. And yes, we're talking about
the beginning of the Gallup poll we see used everywhere today. George Gallup's work in this
election launched his career and gave the Gallup poll its initial credibility.
Okay, now let's spring forward almost a hundred years to today. In the 2008 and 2012 election,
Nate Silver emerged in a way similar to George Gallup as a new force in polling. Silver is a
statistician and writer who initially made his name analyzing in-game baseball activity in the
realm of sabre metrics. Then he turned that statistical toolbox toward elections.
What do baseball and elections have in common? Both have an exceptional amount of data about past
performance that can be used to predict future performance. Professional baseball players,
more in most other professional sports, are expected to play almost every day during the
season. For elections, past performance data comes from polls. And during election season,
for presidential race, there can be a political poll somewhere appearing not only every day,
but almost every hour. That's because these days, there are many ways to gather polling data,
from cell phones to landline phones to email to webpages. As such, there is considerable
technology available to collect data regarding voters' preferences.
Taking advantage of all this, in the 2008 presidential election, Silver established
his website, 538.com. At first, he didn't reveal his identity. Soon after, he did. He began to appear
in various media outlets as an electoral and political analyst. In 2008, Silver correctly
predicted the winner of 49 of 50 states in the presidential election. The only state he missed
was Indiana, which went for Barack Obama by only one percentage point. It's important to note that
in that same election, he correctly predicted the winner of all 35 U.S. Senate races. Now,
it wasn't hard to predict some of the results. That was easy. What was impressive was coming so
close to giving correct predictions for all the results. By the next April, he was named
one of the world's 100 most influential people by Time Magazine. Just before the 2012 presidential
election, Silver released his book, The Signal and the Noise. It reached the New York Times best-seller
list for nonfiction and was named by Amazon.com as the number one best nonfiction book of 2012.
On the morning of the 2012 presidential election, Silver posted the final update of his model
at 10.10 a.m., giving President Barack Obama a 90.9% chance of winning a majority of the 538
electoral votes. By the end of the day, Mitt Romney conceded to Barack Obama.
Silver was also a big winner as he had correctly predicted the winner of all 50 states and the
District of Columbia. Silver creates a poll by combining the results of multiple pollsters.
He was not alone in this approach. Josh Putnam, for example, of Davidson College, where I teach,
also correctly predicted the outcomes of the nine swing states. Both exactly predicted the
electoral college totals for the 2012 election. It is important to note that individual pollsters
were less successful. Rasmussen reports missed on six of its nine swing state polls. How did
Gallup do? They had among the worst results. In late October, their results consistently
showed Mr. Romney ahead by about six percentage points among likely voters. This differed
significantly from the average of other surveys. Gallup's final estimate of the 2012 popular vote
narrowed with a final prediction that Romney would lead with 49% of the popular vote and
Obama would trail with 48%. But Gallup also said that the margin of error was plus or minus two
percentage points for each candidate's estimate. So the Gallup prediction claimed very little
certainty at all. In reality, President Obama won the popular vote by 3.85 percentage points over
Mitt Romney. Interestingly, this result gave Gallup three poor election predictions in a row.
In 2008, their polls overestimated Mr. Obama's performance. In 2010, they overestimated how
well Republicans would do in the race for the United States House.
How can so many specialists dedicated to the science of predicting elections vary and even
struggle? First, just like Roosevelt's second election, a major issue is polling a representative
group. Gallup's polls typically sample the opinions of a thousand national adults with
a margin of error of plus or minus four percentage points. If Gallup's creating a poll to gauge
public opinion about a national issue, how can the opinion of a thousand people represent the
opinion of millions? That's a key issue in the science of polling. Think of a pot of homemade
soup. If the soup is well mixed, then you only need to take a spoonful to know the taste. However,
if the soup isn't well mixed, then it might taste very different at the top than at the bottom.
In polling, just like we saw with the literary digest, you must have a representative group.
Leaning on this analogy of soup, we must have a well-mixed sample of people. Not only should
they be mixed, but in a way the mix of the group should look the same as the large group.
As mentioned early, an issue in this is that you must contact them. This alone creates an issue.
In the 2012 election, some polls were done with live interviews, others with automated telephone
interviewers, and even others via the internet. Of these three modes, automated polls had the
largest average error of five points, having a Republican bias for that selection.
Another issue is whether you call cell phones. Keep in mind that there are legal restrictions
regarding automated calls to cell phones. Yet in 2012, it was reported in July on Business Insider
that Android and iPhone users preferred Obama to Romney, 49% to 31%, of course, according to a poll.
As seen in the time of the 1936 literary digest poll, this can be tricky. How you contact poll
respondents affects who you reach. Now, let's dig a bit deeper into the methodology of Nate Silver.
Many of his most inner workings are essentially trade secrets, but he has presented many ideas
that lay out his overarching approach, which can help us not only in politics, but also many
other areas of data analytics. Remember that Silver adapted his work in baseball to politics.
The underlying problem is how to aggregate data from multiple sources. First, Silver indeed has the
data from multiple sources to bring together. He reported having a database of 4,670 distinct
polls from 264 distinct pollsters covering 869 distinct electoral contests. With the data brought
together, he's ready to analyze. We will take it step by step. Step one, collect all the recent
polls that you can find from within a state. We'll label them P1, P2, P3, and so forth.
Gathering all this information from various sources and getting it into the form you need
is called data scraping. Keeping a steady supply of current data fed to your model
can be the most time-consuming step in the whole analysis. Step two, figure out the poll weights.
This is part of the secret sauce in many cases. We know generally how this is approached,
but not specifically. But the general factors that go into weightings can be agreed scientifically.
Old information is worth less than new information. Bigger polls deserve more weight
than smaller polls. The quality of the poll matters a lot, so we need a measure of that.
Otherwise, it's just garbage in, garbage out. So each poll gets a weight that is made up of
three different values. The recency of the poll, the sample size of the poll, and the
pollster rating for the company that did the poll. Silver said he modeled recency
with exponential decay. That's fast, as we know. But Silver was working for a news organization.
He wanted his model to be able to respond strongly to recent events, such as a presidential debate.
For example, if a poll from today was given a value of one and tomorrow given a value of one
half and the next day of one fourth, this would be an example of exponential decay.
My colleague Davidson, by contrast, has used a slower rate of decay. So his model doesn't try
as hard to capture, to capture real but momentary changes. Instead, it gives more weight to past
knowledge. Either way, the older a poll gets, the lower the recency number gets. The faster this
decays, the more influenced your poll is by recent results. Recency also depends on whether the same
polling company releases a newer poll. If so, then the recency score for the older poll goes down.
For sample size, larger sample sizes are weighted higher. Note, however, that poor methodology
doesn't mean large sample sizes increase accuracy. We saw this in the Literary Digest example versus
Gallup's polling. The pollster rating requires the use of that massive database of historic
polling data. So this is another important place where we do an aggregation of data from many sources
and then keep updating as new results come in. The result is we can calculate how accurate
each pollster is. Better pollsters get higher weights. Of course, how a polling organization
does in the past helps indicate how they will do in the future, especially when one sees a trend.
Interestingly, membership in certain organizations like the American Association
for Public Opinion Research and the National Council on Public Polls are also positive
predictors of accuracy in polling. So this can be a piece of a pollster's rating. Further,
the type of poll, internet, live questions via telephone or automated questions via telephone
play into this as well. With this in place, we have the weights and are ready to combine
the three factors. Step three, folds the factors together. This is one of the parts that Silver
doesn't detail. Essentially, he has a formula in which he combines these factors. The goal is for
the weight to be proportional to its anticipated predictive power in upcoming elections. Keep
in mind we have three weights, so a very old poll from a strong pollster may not be as significant
as a very recent poll from a mediocre pollster. Similarly, sample size matters, but it can easily
be trumped by recency and or quality. Some polls are dropped from consideration entirely. For example,
a poll coming from an internet survey completed by people who already use a particular website
we won't know anything about the people in the sample and we may not even know anything about
how consistent it is from poll to poll. The sample of people surveyed isn't derived randomly,
so it's like the literary digest problem of non-representative sampling or even worse. We may
have no idea which way the sample is biased, so we may have no way to create an adjustment factor.
So, such a result may simply be dropped. Step 4. Now, three additional types of adjustments are
also made. There is a trend line adjustment for an estimate of the overall momentum in a
national political environment. A major event such as a debate can shift the momentum of an
election toward one candidate and away from another. So, we look at the size of change in
the most recent polls and use that to change how much we weight the given recency. Second,
there is house effects and a house effect adjustment. Some pollsters may tend to favor
one political party. This step adjusts for that effect. Finally, there is the likely voter adjustment.
Polls vary in whether they are voting or polling all American adults, only registered voters,
and or what we often call likely voters. There can be predictable differences between these groups.
For example, in 2010, polls of likely voters were about four points more favorable to the
Republican candidate on average than those of registered voters. After the 2012 election,
Gallup found that their particular way of calculating likely voters needed adjustment.
One indicator was, quote, thought given to the election. Sounds plausible,
someone who has thought about the election could be more likely to vote in the election.
But it turned out that this indicator hurt rather than helped their prediction. Gallup also gave
more weight to behavior in past elections than did other models. In 2012, it turned out that
likely to vote didn't necessarily mean voters did what they did in a previous election,
nor did it mean thinking about the election a lot before a poll.
Step five, regression. Sometimes a race doesn't produce very much good polling,
or it's biased in one direction or another. Other information, such as incumbency and
stature, can play a factor in a race. So linear regression attempts to predict a candidate standing
according to such non-poll factors. Step six, combined snapshot. We now have two pieces.
We have the regression result from non-poll factors and the adjusted polling average.
Now the prediction from regression is treated as a poll and given a weight in a re
and recombined with the other polls. This is intended to provide the most comprehensive
evaluation of the candidate's electoral standing at the present time.
Step seven, election day projection. But a future debate or political convention
may help or hurt a candidate. The election day projection captures no events between now
and the election. This step takes the data and extrapolates to election day. This step folds
in the undecided voter who may flip and flop throughout the election process.
Step eight, error analysis. Polls are forecasts of anticipated results in elections. Just like
weather forecasts, they are a prediction and good forecasts have a probability tied to them.
For example, Silver stated on the day of the 2012 election that Barack Obama had a 90% chance of
winning by day's end. Certain factors have been found to be important. For instance, the error
is higher in races with fewer polls. The error is higher in races where the polls disagree with
one another. The error is higher when there is a larger number of undecided voters. The error is
higher when the margin between the two candidates is lopsided. The error is higher the further
one is from the election day. Given this, a candidate running for the Senate in Colorado
and another running for the House in Michigan may both be predicted to win by six points.
But one candidate may be perceived to win with 90% likelihood and the other with only 60%.
Step nine, simulation. This step keeps in mind that results in one election may be tied to results
in another election. For example, Senate results can be tied to results from the presidential level.
So factors are varied on the national and local level and then an election outcome is computed
on a computer. Thousands of different variables for the parameters are inputted. The results are
averaged to see who wins and with what consistency. Further, you begin to see the likelihood in
presidential elections of different states going Democrat or Republican.
In the end, the process produces robust results as seen in 2008 and 2012, but you don't have to do
all of this to get great results. The colleague I mentioned earlier at Davidson, Joshua T. Putnam,
did extremely well with fewer steps. He was one of just a few people in the United States, probably
fewer than half a dozen, who correctly predicted every vote in the Electoral College.
We tend to forget, but it's actually the Electoral College that elects a U.S. President.
And Josh's method is simpler. He takes state polls, adds weights for recency, aggregates,
and out pops his prediction. I'm oversimplifying a bit, but he says the biggest challenge is keeping
the data updated, scraping it from the sources and getting that into a form he can analyze.
For someone who wants to get started making predictions, it can be even easier. Keep it
simple. Just start with one state. Assign weights to each poll from that one state. Come up with
your prediction. Update your prediction as more polls come in. That's it.
And what's the core tool in all of this? In an address at the Joint Statistical
Meetings, which is the largest gathering of statisticians held in North America,
Silver noted that the average is still the most useful statistical tool. It's a cornerstone
of all the methods that are making election predictions so much more reliable.
Up to now, we have looked at predicting the result of an election,
but these methods also affect the running of the campaign itself. Now, let's see how big data can
help the candidates run their campaign. Using data analysis, campaigns can better understand
the electorate. In particular, they are able to better identify who they want to communicate with.
In the 2012 election, the Obama campaign used data analytics to know what demographic to reach
for votes. Then they could study what media markets tend to reach that group. They'd promote their
candidate there rather than buying time on media outlets that reached a large demographic. This
approach was much cheaper and in ways much more valuable. Political campaigns have always tried
to identify and mobilize their voters. And campaigns have been amassing more sophisticated
files on potential voters since the 1990s. Some experts credit this approach to the 1996 Bill
Clinton campaign. They focused on winning swing votes rather than the entire electorate.
George W. Bush narrowed the focus further by concentrating resources on swing voter Republicans.
Makes sense. But to do this right, you needed to figure out who these people were, where they lived,
and what they cared about. Just jump to the 2008 Obama campaign and its well-executed web
campaign. They raised about half a billion dollars online. At the same time, they gathered a lot of
data around 13 million email addresses and 5 million friends across social media platforms.
Email addresses combined with information from voter registration records help the campaign uncover
which potential voters they should reach with rides to polling places or phone calls addressing
specific points raised online. And once campaigns have a digital profile of voters
they'd like to target, they can also develop very customized political advertising.
Campaigns can use online advertising techniques to be sure they present their message to their
desired audience. Online ads also offer quick feedback about how well they're working. Did you
click the ad? How long did someone engage in the pre-roll video before skipping it and getting to
the featured video? In the last weeks of an election this kind of rapid feedback can be
especially valuable, but it also helps a campaign stay more on track throughout an election.
A final observation and comment. What makes the approach we've been discussing especially new
and powerful is the effort to grab and use all relevant data about voters. It's perhaps
worth remembering there had been an entirely different approach to political prediction
over the years that just ignored polling data. A model from that approach might have tried to
predict voting by looking at something else. How is the economy doing? Is there a popular or
unpopular war? The idea was if you had a model explaining why voters will vote one way or the
other then maybe you don't need polling data tracking how they say they will vote. Data analytics
has transformed national politics in part by taking the opposite approach. Instead of hoping
for master variables to explain overall election get lots of data about voters. See how they vote
and see how they say they'll vote. As long as there is plenty of voter data available to aggregate
this will be more speculative kinds of prediction hands down. As we've seen this approach works
well when there is a lot of data. U.S. Senate and governor's races in the U.S. involve frequent
polling so those have plenty of data. So that's a good place to start when you'd like to do some
of this analysis yourself. But the less data you have the less confidence you can have in this kind
of analysis. U.S. House of Representatives races attract much less polling than Senate races.
Seats in state legislatures give even less polling. And yet the basic approach remains valid even for
smaller elections. There's just more room for error. If you want to know how people will vote
then focus on data about the voters. You may need to focus on other types of voter data such as
who they voted for previously, changing demographics of the district, political contributions.
Especially for small races it may also become important to have very granular metrics about
candidates themselves especially how they interact with voters. Remember in baseball it was new
statistics about players that raised analytics to a higher level. What if politicians had something
like say an in-person voter conversion percentage or a conversion rate for TV appearances? What about
a metric for who has the highest perceived fit with the interest in values of the district?
In any case there is a more general lesson in all of this. Data aggregation is not just for politics.
Waiting and aggregating data can work well for any messy complex field where no single variable
explains everything. We can look for an answer by gathering as much data as possible about that
answer. The data can include prior answers, predicted answers, and promised answers. Then we can
carefully aggregate all that data about answers toward a single best estimate of the answer we're
seeking. It's a lot of work but the more we bring together the better our answer will be.
