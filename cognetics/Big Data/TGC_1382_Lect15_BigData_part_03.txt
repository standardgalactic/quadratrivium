so.
In many cases, it is much better to have fewer variables than more.
For example, if you threw Super Bowl data into a model for the stock market, it might
come back saying, look at who won the game.
The idea of striving for fewer variables and less theory in our model connects to the principle
called Occam's Razor, which is attributed to the 14th century logician and Franciscan
friar William of Occam.
The principle states that, quote, entities should not be multiplied unnecessarily.
Many scientists have adopted or reinvented Occam's Razor.
For example, Isaac Newton stated the rule, we are to admit no more causes of natural
things than such as are both true and sufficient to explain their appearances.
A more current way of saying this is, if you have two competing models, each making the
same predictions, go with the simpler one.
This can be seen in physics.
A pair of physicists, Lawrence and Einstein, both studied the space-time continuum and
both concluded that the closer we get to moving at the speed of light, the more time slows
down.
Same predictions, but Einstein and Lorentz had different models, different explanations.
Lorentz explained this by changes that take place in the, quote, ether.
The problem was, other scientists weren't finding any evidence that ether exists.
Adding ether to the model offered no additional insight.
In fact, Einstein and Pawn-Carr√© recognized that the ether could not possibly be detected
according to the equations of Lorentz and Maxwell.
So Einstein's work was revolutionary, but it was also simpler.
Einstein's model did not reference ether.
In his explanation actually won out over Lorentz's.
Einstein put it this way, everything should be made as simple as possible, but not simpler.
No, though, Occam's razor is especially useful as a heuristic in the development of theoretical
models, when it attributes, when it arbitrates between published models that may be a sign
that the theory being rejected wasn't ready for publication after all.
The pressure to publish or perish probably drives a lot of the rush into overfitting,
not only in academic writing, but also in popular publications.
There is also the opposite problem called underfitting.
As the name suggests, underfitting is where things become too simple, so simple that they
don't adequately describe the phenomenon.
And here lies the difficulty.
One must make a model complex enough that it can predict both the data you have and future
data you have yet to see.
But it must not become so complex that it performs really well on current data to the
degree that it does not perform well on future data.
