Sometimes data analysis is too good to be true, or it is too good to be truly useful.
We call this overfitting.
Let's look at the ways this can happen and what we can do about it.
In 2014, Warren Buffett offered a billion dollars to anyone who can perfectly predict
winners of just over 60 games in the NCAA's March Madness Tournament.
So what if I come along offering to sell you a method that goes like this?
It's a system of linear equations, one equation for each game teams have already played.
I used the data available from the last 10 years and I create a perfect system that
predicts all those games over all those years.
How much would you be willing to pay for my method?
Sound amazing or too good to be true?
We'll see in the next lecture that techniques that sound very similar do offer genuine insight.
But hopefully this particular promise sounds no more valuable than my having a bridge to
know where you can buy or maybe some prime swampland.
Here's why the method I described won't work.
If I include the tournament that I hope to predict in the data, then I already have information
that I'm trying to predict.
If my method knows its job is to predict the tournament and that that data is already included,
it is actually possible for some methods to simply look at those games and predict the
winner that way, since it already knows the outcome.
In other words, if I know the outcome of a game I need to predict, then I simply won't
pay attention to the outcome of any other game.
We discussed this in an earlier lecture as an example of the need to separate training
data from the data we test and predict.
This is an example of overfitting data.
Here, it is pretty obvious something went wrong.
It is too good to be true.
In terms of what we said in an earlier lecture, we haven't kept our training data separate
from the data we use to predict.
This is one way that overfitting happens in data analysis.
Here's another example from my research computer lab at Davidson.
In 2013, three undergraduates were conducting research into sports ranking methods.
One morning, I walked into the lab and asked how things were going.
They said, ah, it's really tough today.
I said, so the method isn't working?
They commented, oh, no, it's working great.
In fact, it's working too great.
We're trying to figure out what we're doing wrong.
It took a couple of days for our team to see what we were, in a subtle way, doing wrong.
We were including future information.
We were predicting outcomes of games day by day using all the data to that point.
We used a number of statistics.
One could only be calculated with the entire season's data.
We were using that information to know which teams were similar.
But just by knowing this piece of information based on all the data, we were able to predict
much, much better, too much better.
We had to be sure every piece of the algorithm was using data only available at that point
in the season and not in the future.
It took time to remove that assumption.
We had to think of another way to calculate the similarity of two teams.
But once we removed that assumption, the method still performed well.
But no, not like before.
And my students were proud.
Why?
They knew they had made strides as data analyst researchers.
Knowing when you've done something wrong, something too good to be true, can be as important
as doing something right, maybe more important.
Let's try another example.
Suppose we have a database of retail purchases.
We know the item bought, the purchaser, and the date and time of purchase.
We can construct a model that will perfectly describe who has bought what.
The method can pick up that the date and time of a purchase can predict the other attributes.
This is the same thing I could do in my earlier model for March Madness.
If I had past tournament data included, then I could perfectly predict who wins games that
have already been played.
That's what happens with the retail purchases too.
The model will not generalize at all the new data because the model is overfitted to past
times that will never occur again.
There are two issues here.
I included a variable that while predictive on the training set hurts me later with any
test set.
By cheating, by cheating to include too much information about the past, I'm getting worse
results at any future time.
I'm better off not having it as a variable at all.
Over including the past isn't the only problem.
Overfitting can also happen if we include a variable that really has no insight for the
analysis at all.
Did you know that statistics can show a strong correlation between who wins the Super Bowl
in economic performance?
Between 1967 and 1997, the first 31 years of the Super Bowl, the stock market gained an
average of 14 percent for the remainder of the year when a team from the original National
Football League, NFL, won the game.
However, when a team from the original American Football League, AFL won, it fell by almost
10 percent.
This economic indicator was correct 28 of 31 times over that 30 year period.
Statistical formulas can pop out the statistical significance of something, and this would
reveal that there is only about a one in almost five million possibility that this relationship
emerged from chance alone.
What an amazing find.
It's striking in its simplicity.
Forget the Super Bowl ads.
Just see who wins to determine whether to buy or sell.
Yes, that's true in 28 of those 31 years, but there isn't such a predictive connection.
The next few Super Bowls showed this.
As we have seen before, it is easy for us to see correlation in think causation.
Again, this variable, while correlated with past data, doesn't mean it will have predictive
value in the future.
However, if you allow this to enter your data analysis, it could end up looking like a highly
predictive variable.
Remember, how we often find patterns where they may not really be.
Well, if we threw such patterns into our model without thinking, the model may mistakenly
tell us it's actually happening.
Let's look at another example of throwing in an unrelated variable.
This one throws a sports variable in with elections and political data.
With so much attention and interest, especially in presidential elections, there is a lot
of curiosity about ways to explain the election through something totally unrelated.
Like what?
Well, how about football again?
Did you ever notice that if the Redskins won their last home game before the election,
the incumbent party would hold the White House?
This has been true in 16 of the past 18 elections.
2012 was an exception.
The Redskins lost to the Carolina Panthers on November 4th, but Barack Obama won the
election for his second term.
So one must be careful.
Too many variables can lead to great results on predicting past data, but poor performance
for future data.
Again, we are overfitting the past, said another way we are trying too hard to predict the
past when our real goal is to predict the future.
In the cases we saw before, we had variables that weren't predictive, but could be seen
so.
In many cases, it is much better to have fewer variables than more.
For example, if you threw Super Bowl data into a model for the stock market, it might
come back saying, look at who won the game.
The idea of striving for fewer variables and less theory in our model connects to the principle
called Occam's Razor, which is attributed to the 14th century logician and Franciscan
friar William of Occam.
The principle states that, quote, entities should not be multiplied unnecessarily.
Many scientists have adopted or reinvented Occam's Razor.
For example, Isaac Newton stated the rule, we are to admit no more causes of natural
things than such as are both true and sufficient to explain their appearances.
A more current way of saying this is, if you have two competing models, each making the
same predictions, go with the simpler one.
This can be seen in physics.
A pair of physicists, Lawrence and Einstein, both studied the space-time continuum and
both concluded that the closer we get to moving at the speed of light, the more time slows
down.
Same predictions, but Einstein and Lorentz had different models, different explanations.
Lorentz explained this by changes that take place in the, quote, ether.
The problem was, other scientists weren't finding any evidence that ether exists.
Adding ether to the model offered no additional insight.
In fact, Einstein and Pawn-Carr√© recognized that the ether could not possibly be detected
according to the equations of Lorentz and Maxwell.
So Einstein's work was revolutionary, but it was also simpler.
Einstein's model did not reference ether.
In his explanation actually won out over Lorentz's.
Einstein put it this way, everything should be made as simple as possible, but not simpler.
No, though, Occam's razor is especially useful as a heuristic in the development of theoretical
models, when it attributes, when it arbitrates between published models that may be a sign
that the theory being rejected wasn't ready for publication after all.
The pressure to publish or perish probably drives a lot of the rush into overfitting,
not only in academic writing, but also in popular publications.
There is also the opposite problem called underfitting.
As the name suggests, underfitting is where things become too simple, so simple that they
don't adequately describe the phenomenon.
And here lies the difficulty.
One must make a model complex enough that it can predict both the data you have and future
data you have yet to see.
But it must not become so complex that it performs really well on current data to the
degree that it does not perform well on future data.
This inherent tension is ever present.
This can sound hopeless, but we do have predictive models.
They save lives.
Here's an example where the challenges of underfitting were largely overcome.
In 2012, the National Hurricane Center predicted the approach of Hurricane Sandy toward New
York and New Jersey.
Schools closed, subways shut down, and low-lying coastal areas were evacuated.
The warnings came as early as five days in advance of the storm's arrival.
Their predictions saved lives as well as millions of dollars.
Superstorm Sandy was more complicated than a standard hurricane.
A model subject to underfitting would have been more likely to miss this.
Usually, a hurricane like Sandy would have blown out to sea, but a high-pressure area
over Greenland steered Sandy back toward shore, where she crashed into a cold front.
Even with these unusual circumstances, underfitting was avoided.
Sandy's path was predicted two days ahead of time to within 50 miles.
This included fairly good predictions of the storm's intensity, wind gusts up to 80 miles
per hour, 12 inches of rainfall, and up to 11 feet of inundation from the storm surge.
This aligned closely with the actual storm.
Hurricane forecasting overcomes the problem of underfitting with lots and lots of data.
Please collect information about hurricane's position, wind movement, and the atmosphere's
temperature and moisture levels.
This information is improving all the time.
Aircraft fly into the storm and collect data.
They also drop probes that measure data.
Boys and floats collect data about the ocean and important interactions between the sea
and the atmosphere.
And then there is land-based radar also tracking the storm.
This data is used to calculate temperature, pressure, and humidity changes, usually in
30-second intervals at points on a grid.
The grids generally consist of one million east-west points by one million north-south
points by 100 vertical points.
The vertical points are going up into the air.
So that's a grid of 100 trillion points.
That's a lot of computing.
And that's what makes it possible to predict a path two or more days down the line.
Modern data in computing have dramatically improved modern hurricane prediction.
Underfitting in hurricane predictions is no longer the problem it once was.
In the 1970s, five-day hurricane forecasts were off by an average of 400 miles.
In the 1990s, it was down to about 200 miles.
For 2010 and 2011, five-day forecasts aired by an average of just 100 miles.
There is still room for improvement.
Underfitting is still a large issue for forecasts about intensity, which was not significantly
better than a few decades ago.
In fact, you may notice that fields where there is still plenty of room for improvement
will leave more room for error by referring to forecasts rather than predictions.
In a sense, the worst thing about underfitting is how it leads to overfitting.
When we don't have enough data, we overfill the gaps.
We may have some shiny gems of data in the mix, but that dazzles us into thinking we
know more than we do.
Overall, it's hard to be fooled by underfitting.
Your training data and your test data give poor results.
Your model doesn't work, and you immediately know it doesn't work.
But one must be more careful to avoid overfitting.
Overfitting can occur from more than one direction.
Over sometimes, an entire variable or model can cause overfitting.
We might call this misfitting.
It misses the boat entirely, as we saw the first years of the Super Bowl correlated with
the stock market.
It would have appeared that one had a great model.
But these sorts of chance correlations are going on all the time.
Instead of focusing on that one unusual correlation between sports and the economy, think of all
the thousands of things going on in politics, sports, and the economy all at the same time.
If we have all that data to choose from, the chance of finding five or ten or twenty data
points that are correlated somewhere is actually really high.
It's like coins being tossed.
So a rare correlation that's weirdly compelling when seen in isolation isn't really so surprising
after all.
And we definitely don't gain any deeper insight by letting that into our prediction model.
Second, another culprit is not appreciating the place of noise in real data.
Whenever data is collected, for example in a science lab, there is always some measurement
error.
We might even know that there is some underlying law or principle.
There are scientific laws that follow a line.
Increase the force and the resistance increases linearly.
But that doesn't mean that when you actually go into the lab and measure this that you
will definitely see nothing but that relationship.
You are more likely to see data that comes close like this.
We can fit a line through the data using a least squares line as we learn.
We saw this when we were looking at Olympic medalists in the men's one hundred meter in
an earlier lecture.
If we fit a line, we get the type of chart that you see.
Notice though that the data isn't matching at every point.
The line isn't passing through every point.
So we aren't predicting the points we measured perfectly.
We are off on those values.
This is because in a sense we are assuming the presence of that measurement error or
noise.
What if we didn't?
What if we wanted 100% accuracy for the points that we measured?
One possibility, bad, is just to change the data.
Gregor Mendel, the famous 19th century researcher into heredity is an example.
He published data about heredity and peas that was much later found to be too perfect.
Decades later, a statistical analysis of Mendel's data by R.A. Fischer found there was too little
noise in Mendel's results.
Fischer's comment was, quote, I have no doubt that Mendel was deceived by a gardening assistant
who knew only too well what his principal Mendel expected from each trial made.
Regardless of who done it in this case, we always want to avoid overfitting data, even
if it's for a cherished model.
But there's also the reverse problem.
Another way to strive for 100% accuracy is to overfit the model itself.
We could leave the data alone and make too many adjustments to the model.
We could measure with a very complicated curve rather than a line.
Here we see this.
Notice how this data is dipping at the end.
This data will have a much lower value if we extend the plot to the value of 8 on the
x-axis than we would have had if we used the line.
This type of difference could be significant as we are about to see.
But first, remember, data is generally not exact and can contain spurious components.
It may help to think of static in a phone line.
When someone calls and has static in the connection, you try to hear the person talking, so you
work to listen to the voice within the static of the call.
In the book, The Signal and the Noise, Nate Silver lays out a case for the Fukushima Nuclear
Disaster as a dangerous outcome of overfitting.
Earthquakes continue to be unpredictable.
Even with today's supercomputers, modern geophysicists are only marginally better than simple historical
means.
In hurricanes, we can collect data, lots of it.
With earthquakes, relevant data is much harder to collect.
We cannot directly measure stresses 20 kilometers or more underground, so the data is scarce
and underfitting is an issue.
In addition, the current data we do have is approximate, that is, it is also noisy.
Early, we talked about this as dirty data, so we are at risk of overfitting what we
do have.
So how do we forecast a random event?
Look for other data.
If we look over long geological time scales, it turns out that earthquakes are far from
random.
We can even fit the data well with an underlying function.
Then we can pick any point on the Earth and compute a future earthquake probability.
For example, what's the probability of Fukushima having a 9.1 earthquake, which was the magnitude
of the devastating earthquake?
It isn't zero, at least historically.
Archaeological data suggests tsunamis of the 130-foot height seen in 2011.
While the historical data said it could happen, it hadn't happened in so long that there was
no apparent reason to expect another, and so, or so, they seemed to think.
The Fukushima nuclear reactor was built to withstand an 8.6 earthquake.
This wasn't just a small, short gap.
Remember that the earthquake scale is logarithmic.
A 9.1 earthquake is 5 times larger than an 8.6 earthquake.
As Silver shows, a plot of recent earthquakes doesn't reflect this.
If we plot the magnitude of recent earthquakes versus their annual frequency, this gives
an estimate to the probability of such a magnitude.
But this graph of recent earthquakes stops at 8.0.
There were no earthquakes that large since 1964, so there is a drop-off of earthquakes
between 7.5 and 8.0.
How do we fit the data?
In a sense, what type of regression will we use?
What type of curve will we use to fit the data?
Align another curve?
If one uses a line, which fits a large amount of the data better, there is a 9.0 earthquake
approximately every 300 years.
A line will fit the points prior to a 7.0 magnitude earthquake pretty well.
That dip at the end?
Not so well.
A linear model may not appear to work, so one could choose a curve to better fit that
kink in the curve.
Remember did and found a probability of one about every 13,000 years.
If that were really true, it's really unlikely any of us see this happen again.
But remember, if you fit the data too closely, you are overfitting data.
And overfitting a graph with less than 50 years of earthquake data looks like a real
risk.
Especially if you're making a forecast about an earthquake 13,000 years in the future.
So bringing in geological data from deep in the past makes more sense than ever.
And it turns out that the geological data would have suggested the line continues all
the way, making that big earthquake much more likely.
So it all depends on what you're studying.
A hurricane may be predicted within hundreds of miles based on short term factors that
have become easy to measure.
Earthquakes by contrast depend on data occurring not only deep in the earth, but also spread
over decades if not centuries.
The kind of data needed is different in adjustments needed for lack of data are also different.
Now, noise isn't the only issue in overfitting.
It is possible not to have enough data.
It is possible to have too little noise.
In such a case, you may actually not have enough noise to accurately predict what is
happening.
If you have two points, you will fit the data exactly with a line.
Or if you have only one point, then you can draw any line at all.
But if you can get 10 to 20 points, then you are not overly waiting just one or two initial
measurements.
And if you can get a few hundred points or a thousand, then you are not overly waiting
a couple dozen points.
Again, the flip side is failing to consider how often unusual things can happen merely
by chance.
Suppose you're going to pick someone to help with your retirement funds that are invested
in the stock market.
To do this, you'll see how the person performed over a 10-year period.
It will suffice for the person to accurately predict year by year whether the market goes
up or down.
If the person guesses wildly, there is a one-half chance of guessing correctly.
So the probability of someone guessing correctly eight or more times in 10 years is just over
five percent.
One is the probability that the person does not get eight or more correct is 94.53 percent,
which we'll need in a moment.
So it is fairly unlikely that one analyst will get them right just by chance.
Now you decide to broaden your search and choose from 50 stock analysts.
You'll select the person who does the best.
You're still just looking for one person who gets eight or more selections correct.
Let's imagine that all 50 stock analysts just guess.
Yes, they are just flipping coins and hoping for the best and your business.
You can compute the probability that at least one of those 50 gets at least eight correct.
It equals 1 minus 0.9453 to the 50th power, which equals 0.9399.
In fact, broadening your search to 100 analysts makes it even worse.
You now have a 99.6 percent chance that someone will get at least eight correct just by flipping
coins.
Each analyst individually is unlikely to get at least eight correct.
But as a group, the probability that one person will do that becomes quite high.
To make matters worse, there is no guarantee that the selected person will continue to
be accurate.
Remember, the person was flipping a coin.
So once we zoom in on our apparent winner, that one person's chance of guessing correctly
collapses back to 5 percent.
Data analysts build models that take data to predict future outcomes or explain past
events.
In a sense, data analysts are storytellers.
We saw in our lecture on patterns that we as people have a great way of putting order
in our world through stories.
We can find some way to explain what we observe to date.
Remember that scientists reach farther.
It isn't enough to build a model that can describe all past earthquakes or produce the
path of all past hurricanes or perfectly predict the outcome of every past March Madness Tournament.
These results are impressive only if they enable us to better predict future earthquakes,
future hurricane patterns, and future March Madness Tournaments.
Can a model be extended into something we've not yet observed and make predictions?
This is the goal.
To do this, you usually have to be less predictive in the past.
Remember a certain amount of that past data has noise and randomness.
We don't need to predict the noise.
We need to find the trend within the noise and yet not overfit that trend.
If we can, if we can find that sweet spot between predicting the past and predicting
the future, then our data analysis is at its best.
It can improve our forecast and like the case with Hurricane Sandy, give us more time to
respond to the forecast.
