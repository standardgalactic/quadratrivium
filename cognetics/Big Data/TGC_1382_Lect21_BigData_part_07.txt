We did have one big choice, our initial centroids.
It turns out that different initial choices can lead to different clusters.
This can be a downside to K-means, but it is easy to run and quick to check.
That's a huge benefit.
One use of K-means is with down-sampling images, which is usually done to reduce the memory of an image.
Suppose we have a large picture which could take a lot of storage.
If we take K equals 16, we move to saving the image with only 16 colors.
This image becomes this.
We can find the value, thus improving the resolution in certain regions as desired.
Let's step back to get an overview of the family of clustering methods.
In particular, what are you doing and when might you want each?
It's a toolbox, and it can help you know when to pull each one out.
Globs.
That's what I think when I think of K-means.
This method is useful for point-wise data with distances.
Again, it minimizes distance to the center of the clusters, which is why it is good at data that looks for globs in space.
Graphs.
If I have a data that's a graph, vertices, and edges, then I think of spectral clustering, which is also looking for globs, but now in graphs.
Those globs were the dense pink blocks we were seeing in the songs matrix earlier.
Spectral clustering uses eigenvectors from linear algebra to get them more to find those dense connections in the sub matrix and not as much outside of them.
A nice attribute of spectral clustering is that you get a unique solution, and it even finds an optimal solution.
Now, the globs defined using K-means, you have to pick the number of means in advance.
With spectral graphs, you automatically get powers of two, which may or may not be what you want.
You can't tell it to find seven, for instance.
Finally, hierarchical is more flexible, but the results are not always as clear.
The method is really good at multi-level interpretation.
You can easily zoom in and out to find something like sub-genres, and you don't have to know how many clusters you want in advance.
But you do have to specify how to measure the distance between clusters.
It could be the distance between the centers of your clusters, but it also could be the two closest points between clusters, or even the two points farthest from each other in the two groups.
These are only some of the many common methods used in clustering.
Again, many more exist.
Regardless of what method you use, clustering is used as an exploratory method.
Peter Cheeseman and colleagues at NASA developed a Bayesian clustering algorithm called AutoClass that discovered two subgroups of infrared stars where no previous difference was suspected.
In marketing companies group customers into what are called segments, as in segments of a market, the company Claritus, now part of Nielsen, offers a segmentation that groups the United States population into 66 clusters.
Let's look at just one of their 66 categories, up and comers, which are upper income, middle age, or younger without kids.
This is further described as a stopover for younger upper mid-scale singles before they marry, have families, and establish more desk-bound lifestyles.
Such groups are often found in second-tier cities and consist of mobile adults, mostly aged 25 to 44.
This group contains a disproportionate number of recent college graduates who are into athletic activities, the latest technology, and nightlife entertainment.
They order from priceline.com, travel to South America, watch South Park, and drive a Nissan Altima Hybrid.
Is that supposed to be exactly true of everyone in the group?
