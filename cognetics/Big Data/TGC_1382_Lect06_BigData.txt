We've been looking at patterns and we've been looking at some of the tools of analysis.
Another key to consider early on is storing the data.
Now storing information is something data analysts have been doing electronically for
decades.
It's an old issue still worth asking.
Do you make the effort to save copies of your data in different places?
But today we are also dealing with huge data sets and in order to analyze them we have to
have a better understanding of how they can be stored and then how the data can be retrieved.
To get a sense of the amount of data today let's look more closely at the Library of
Congress.
It has a lot of data much of which has been kept for years in physical form.
It's the largest library of printed material in the world with more than 155.3 million
items on approximately 838 miles of bookshelves.
What makes up the Library of Congress?
More than 35 million books and other print materials, 3.4 million recordings, 13.6 million
photographs, 5.4 million maps, 6.5 million pieces of sheet music, and 68 million manuscripts.
The Library was founded in 1800.
Among its collections the Library has Thomas Jefferson's Personal Library of 6487 books
acquired in 1815.
The Library is still growing.
It receives about 15,000 items each working day and adds approximately 11,000 items to
its collection daily.
Between 2000 and 2010 the Library also archived 167 terabytes of web-based information and
much more after that.
That brings us to this lecture's topic, how to store all these items.
But keep in mind it isn't enough to store everything.
We also have to be able to find the data once stored.
In data analysis the data is often stored in digital rather than physical form.
Digital storage and reference isn't new.
Computers can store large data sets like the 155 million items in the Library of Congress
very, very quickly.
And we are now accustomed to such speed.
How many times have you said, wait a second, well these days that may actually be too long.
Google engineers have found that if search results are just a tiny bit slower even by
the blink of an eye people search less.
Remember there are multiple search engines and in the land of searching it is definitely
survival of the fittest.
The fastest to run out, get what you queried and returned is indeed the fittest.
Research showed that we will visit a website less often if it is slower than a close competitor
by more than a quarter of a second, 250 milliseconds.
It takes four tenths of a second, 400 milliseconds just to blink your eye.
So let's return to the Library of Congress.
It's archiving all of America's tweets.
Now this isn't all the content of Twitter but still that's a lot and depending on the
moment can be a whole lot.
We leave the borders of the United States for a moment to see how this is true.
The volume of tweets isn't uniform from moment to moment.
On Saturday, August 3rd, 2013 in Japan people watched an airing of Castle in the Sky.
They took to Twitter to share a key moment on a single word near the end of the movie.
In fact it seemed like everyone had the same idea at the same moment or at least the same
second.
In the evening at 1121 in 50 seconds in Japan Twitter hit a peak of 143,199 tweets in just
one second.
Let's look at the graph to see the spike in that volume.
Remember Twitter typically gets 500 million tweets a day for an average of about 5,700
tweets a second.
That spike of activity was about 25 times greater than the average rate of tweeting.
In fact Twitter's ability to handle this particular spike gave the company a chance to pat itself
on the back.
They had improved the efficiency and parallelism of their storage after problem surface back
when tweets had surged during the 2010 World Cup.
So when you hear that an organization like the Library of Congress is archiving something
like tweets, think about the amount of data they are storing.
Also about how fast the act of storage has to be during a spike.
Again think about the further challenges of making that archive accessible.
Users will want it to be stable, fast, and convenient and then multiply the hundreds
or thousands of terabytes just from Twitter across a lot of other organizations.
There are many, many data sets today that are very, very large.
For a data set to be meaningful and useful, it must first be stored and second be accessible.
With such amazing volumes of data, how can this be done?
In a nutshell, not with yesterday's methods.
I remember as a child going to the library and having a card catalog that I thumbed through
to find books for research projects in class.
We even had small workshops led by school librarians to help us learn to do this better.
Then the whole thing went digital.
In fact, I helped my school with some of the database work when I was in high school.
I remember the first time I entered an item into the school database.
It was a simple DOS program.
Almost instantly, in fact it felt instant to me, the query was returned with the book
I was looking for.
I was amazed.
How could the computer possibly look that fast?
Just like that card catalog became dated, so are some of the storage techniques that
worked with data sets that were considered large a decade ago.
We can see this further from two other fields.
A healthcare organization has patients records consisting of various tests from x-rays to
MRIs to sonographs to doctor's charts.
That's for every patient in that organization, which could be millions.
A legal firm may be sifting through piles of documents, which may include emails, electronic
calendars, and they need to index everything in relation to how the case is developing.
So they can find patterns and build their case over sometimes disparate pieces of information.
So there can be different approaches used to handle large data sets, especially very
large data sets.
However, size alone isn't the only determining factor in how to approach managing data.
Another issue is whether the data is in motion or at rest.
Given our discussion a few moments ago related to Twitter, let's see these ideas as they
relate to Twitter.
Data at rest would consist of analyzing past tweets about a product to get a sense of customer
satisfaction, at least on Twitter.
Data in motion could have the same goal, but the difference is immediacy.
Here, the company might keep track of tweets as they appear.
When a product is rolled out, what are people saying?
This could, for instance, allow one to quickly change some aspect of promotion or customer
service or even the product if that's possible.
There is also a cycle to data management.
Of course, you must collect the data, but what data you collect depends heavily on what
problem or question you're considering.
For example, I worked on a research project on discerning race time strategies in NASCAR
based solely on what was being tweeted during the race.
Could I know when drivers took pit stops if they changed any tires and if so, how many?
In another project, we worked on getting a sense of the popularity of restaurants, again
based solely on tweets.
Both projects ideally would occur in real time.
Can you see what questions we'd have to answer?
That's where you start in research.
What are we asking?
Then you start the cycle of data management.
First, does the information even exist?
Is there data to reach any conclusions?
And second, if it is there, how do we capture it and know what it is stating?
In the NASCAR study, we found that we could discern the status of a NASCAR driver from
tweets during the race.
In the restaurant study, we found that we could determine reviews on a diner via what's
being posted on Twitter.
But when we started, we didn't know.
We looked around a bit to see, but in the end, we dove in and began doing preliminary research
to figure it out.
As part of the research, we both collect data, which means storing it, and also grab the
data in order to analyze it.
Once we have the data collected, we must decide how to store it and how to organize it.
We may also need to bring together data from multiple sources.
This is called data integration.
If your data is in motion, it may be better to have data integration happening over and
over.
You may even want data integration in real time, which is the opposite of dumping everything
just at once into a static and fixed database.
An important aspect of data integration, even if you do not have data coming from disparate
sources, is preparing your data for analysis.
We'll consider that in a separate lecture.
After the data is integrated, you can finally analyze it.
For example, Amazon will look at past customer actions from all the data that's been integrated
in its system, and then they can make an action.
For Amazon, that may be recommending a book.
There are business examples that you may not as easily see as connected to digital information.
Walmart uses data analysis.
When they learned via social media that cake pops were popular with customers, the company
quickly responded by putting them in stores.
But notice, the cycle of data management goes back to capture.
This is something I impress on my students and class-oriented in our research teams.
Once we act, we need to capture more data and validate the action.
For NASCAR, did teams take pit stops when we said they did?
For eating out, does our sense of public sentiment match what is really happening?
For Amazon, if they keep recommending books that aren't of interest, then the feature
could even become a nuisance.
In the case of Walmart, did the fact that cake pops were popular on social media correlate
to sales, either in stores or on the Walmart website?
Here are the issues I teach my researchers to think about as we begin a project in data
analysis.
Fundamentally, we're looking at how data will be managed.
First, how much data will you have?
In working on the reviews for restaurants, we were working with tens of thousands of
tweets in the early stages of the research.
But we had to also keep in mind, what if what we did really hit the mainstream, which was
our goal?
What if suddenly a million people were using the app?
Does that make a difference?
If so, it might mean that you need a much more efficient algorithm, otherwise your
app may stall very, very suddenly with too many users.
For an app, how much of the data can you process live and how much will you need to process
offline prior to any search?
If you have millions of users, you simply may not be able to process all the requests
in the data live.
Google, for example, computes part of its rankings offline before you ever enter something
into the search engine.
Another important issue is the security of the data.
How secure must it be?
And one more, how precise should the data be?
Can we aggregate information?
Now, let's assume you are indeed using quite a bit of data.
Notice that you can't easily store it on your own computer or even with one external
hard drive.
At one time, you'd need a supercomputer to aid in this.
For most of us, that meant any such questions just couldn't be considered.
But today, that's changed.
The change came when companies like Yahoo, Google and Facebook turned their attention
to help offer storage mechanisms for the data that their services were helping produce.
An important part of storing and accessing data is data warehousing, which serves as
a central repository of data collected and then integrated from one or more disparate
sources.
They often contain both current and historical data, enabling one to look at current patterns
and compare them to past behavior.
This is an important feature.
Data warehouses generally are used to guide management decisions.
Note how Walmart decided to put cake pops on the shelves.
Now, a data warehouse is a relational database that is designed for query and analysis rather
than for transaction processing.
Let's dig a bit deeper into that sentence, as there is a lot of things going on there
that describe a data warehouse.
As we mentioned, a data warehouse often has multiple sources feeding into it.
For example, there may be multiple branches of a bank in several countries with millions
of customers in various lines of business from savings to loan.
Each bank's database may have been developed or tweaked internally with each application
designer making individual decisions as to how an application and its associated database
should be built.
As we mentioned, these sources are combined in a data warehouse.
A smaller subset of the data warehouse aimed at end users might be called a data marked.
The key, though, is that the data sources are also accessible.
That's the relational database part of it.
A relational database is a type of database that organizes data into tables and links
them based on defined relationships.
These relationships enable you to retrieve and combine data from one or more tables with
a single query.
Let's see this play itself out.
Amazingly, we don't have to have big data to see how a relational database can help.
Let's take a database of five people in an organization.
Here's our organization listed by their employee number.
This is easy enough, but what if we make changes?
If you remove the two employees from purchasing, Jim Harris and Abigail Smith, then the purchasing
department doesn't appear to exist, at least in the database.
But purchasing may simply be in transition, only temporarily without employees at the
moment.
Or what if marketing gets renamed advertising?
Every entry has to be changed.
Keep in mind, these are issues with storing five elements.
Remember the Library of Congress?
It has millions.
Relational databases would break this into separate tables.
We want to remove all repetitive information.
This can be achieved by creating a table for the department and another table for the employees.
The department table only has the department number and department name.
The table for the employees would contain the employee number and a name.
So we have two independent tables.
Now what?
We want to retrieve data, and so we want to have meaningful links between the tables.
We have a pretty small data set, so there the relationship or relation between the databases
is pretty clear.
The employees work in the different departments.
So we simply need a key that links data.
So our employee table becomes what we see.
Now if both Harris and Smith leave purchasing, the department isn't gone from the database.
Similarly, changing the name of a department is a very quick change.
To be sure we understand this type of approach, let's try another example.
We'll create a database of flight information of an airline.
So we will store flights and pilots for the various flights.
Each flight has a unique number, a departure city, a destination city, a departure and
arrival time.
Each pilot has a unique company ID, a name, and experience level.
So we will have two tables similar to what we did before.
We will have one for the flights and one for the pilots.
Now we think of the attributes for each data set.
For the flights, we have the flight number, a departure city, a destination city, departure
time and arrival time.
Now we ask an additional question.
Which entry should we search on?
The flight number is the unique attribute for flights.
So this becomes what is known as the primary key.
For the pilots, they have the company ID, name, and experience level.
The primary key is the company ID, since again it is unique to each pilot.
Right in our earlier example, the department number and employee ID were the primary keys.
Finally, for this example, we get one more table.
We need a way to link the tables.
Here we have an assigned to table.
Here we have each pilot's company ID, the flight number that was assigned to the pilot
and the date of the flight.
Now what if we had all the flights for say US Airways?
That's a lot.
How can one computing device possibly manage it?
It doesn't have to.
Parallel computing and parallel processing is one of the cornerstones behind modern-day
very large-scale data analysis.
In particular, Apache Hadoop is a free programming framework that supports the processing of large
data sets over multiple computers.
So what is Hadoop exactly?
Interestingly, the word Hadoop is named after the creator Doug Cutting's child's elephant
toy.
But the technology was initially sponsored by Google, so they could usefully index all
the rich textual and structural information they were collecting.
But here's the key.
They didn't want to just store it.
They wanted to present meaningful and actionable results to users.
Something like that existed, so they created it themselves.
This isn't a project of Google alone.
Yahoo has played a key role in developing Hadoop for enterprise applications.
Hadoop was designed to tackle problems where the data doesn't fit nicely into tables.
Again, this fit what Google was doing when they created it.
They were indexing the web and examining user behavior to improve performance algorithms.
Hadoop runs on a large number of machines that don't share any memory.
So buy a bunch of computers and put them on a rack.
Then be sure to install Hadoop on each one.
Then get your data.
It can be lots of it.
Hadoop will take care of it.
Break it into pieces and spread it across your computers.
Note, if you have a lot of data, those computers themselves may be servers.
That is, each computer is itself a large computer.
Where is a piece of data?
Hadoop knows.
How do you interface all the data since it's distributed across servers?
With Hadoop, a very nice feature is that data is in multiple places.
So if a server goes offline, things can be found elsewhere.
You can deal with big data because Hadoop spreads it out into smaller, manageable pieces.
Then you can ask your complicated computational questions because all of your processors work
in parallel and together.
Let's look at one of the prominent clients using Hadoop, Facebook.
Users share billions of pieces of content every day on Facebook.
That could potentially be a mess.
Facebook has no idea when you are going to log on.
When you do, your newsfeed with updated information on everyone you're connected to is posted.
Someone may have posted a picture or updated their status only moments ago.
And there you are with the results on your friends and their postings on your screen.
Exactly.
Even with so much data, it must be stored and then presented in a quick and relevant
way.
There is a lot of infrastructure supporting this and of course, there is all that data.
Have you ever had your digital camera run out of memory?
Well, the same thing can happen with Facebook.
What they do is move the data periodically to even larger data centers.
Think of the size of this problem.
If not done well, it could be a big data headache.
But this is something Facebook does seamlessly to the user.
And yes, they are using Hadoop to help.
In 2010, Facebook had the largest Hadoop cluster in the world with over 20 petabytes of storage.
The cluster grew to 30 petabytes by March 2011.
30 petabytes is 3,000 times the size of the Library of Congress.
What next?
They had to move to a larger data center.
Facebook had literally run out of power and space.
So what do you do?
One option was to physically move the machines.
Get enough hands on the job and it wouldn't take too long.
But remember, we get impatient if a search result isn't shorter than the blink of an
eye.
Making Facebook down for days was far from a good business decision.
So they wanted to set up a replication system that mirrors changes from the old cluster
to the new larger cluster.
Then when it was time to switch, everything would be redirected to the new cluster.
Sound easy?
Maybe, but remember the files that are being replicated aren't static.
They are in motion.
Files are created and deleted continuously.
So they decided they would migrate, but to do so, they had to create a new way to do
it.
And a key component of doing this, indeed, Hadoop.
The replication was created and successfully implemented.
If you were on Facebook, you probably didn't even notice that Facebook data moved.
That was the point.
And that's the characteristic of modern big data storage issues.
New techniques need to be created, and they are.
Ironically, we aren't always aware they happened as sometimes they are created so we never
see their impact.
The data world doesn't feel any bigger to the user.
We keep posting updates on Facebook, and Facebook is in a new big data center.
Ready to capture, analyze, and present it until, of course, they someday need to move
again.
The issue of data storage is one that comes up more and more in data analysis.
If your business is especially dependent on analyzing huge amounts of web data, then Hadoop
can be part of the solution.
But you don't need any of that to tap the power of relational databases, which are your
first step up when a spreadsheet is no longer enough to hold your data.
And if you have only megabytes or even gigabytes of data, you are unlikely to gain either speed
or flexibility from pulling your data into a Hadoop rack of computers.
So there are other options.
In the case of our studies on Twitter data on restaurants, the company Twizu used another
company, a data supplier, to pull down all the tweets with certain search terms.
Then they created their own non-relational database with MongoDB, which stands for Humongous
Database.
This allowed everyone to have an efficient database for tens of thousands of tweets and
for everyone to log in and grab data of interest, whether that person was in North Carolina
or London.
In our NASCAR studies using Twitter data, we stored the data on our laptops in large
text files.
We created Python-based computer programs to read and parse the files.
So there are a variety of approaches.
Part of the question is, what data do you need to collect?
How much of it do you need?
If you need quite a bit, then how will you store it and remember how will you access
it?
If you ever consider using this for a new business venture, keep in mind, as even Facebook
had to do, what happens if you run out of storage?
What will you do then?
These issues are ever present with today's large and growing data sets.
So always keep the underlying storage of data in mind.
When you have a lot of data, it's an important issue that, when done right, can seem almost
as simple as storing all the data on your laptop.
