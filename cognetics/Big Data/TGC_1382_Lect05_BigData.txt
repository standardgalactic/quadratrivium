Data sets are getting bigger, so a fundamental aspect of working with data sets today is
ensuring that you use methods that can sift through them quickly and efficiently.
In this lecture, we will talk about a core issue of computing, complexity.
Managing complexity is an important part of computer science and plays an important role
in data analytics.
We can see how important these issues are by looking at online purchases.
Billions of dollars are spent with credit card numbers flying through the worldwide
web to make online purchases.
When you make a purchase, you want to be on a secure site.
What makes it secure?
The data is encrypted.
We'll see what that means, and then it's decrypted by the receiver, so clearly it can
be decrypted.
Broadly speaking, encryption techniques are based on factoring really huge numbers.
How huge?
Really huge.
They may be 10 to the 75th power.
That's a 1 with 75 zeros after it.
How big is that?
The number of atoms in the observable universe is estimated to be 10 with 80 zeros after
it.
How do we know that someone isn't going to be able to factor such a number on their computer
or some large network of computers?
Just keep getting faster.
Maybe tomorrow there will be a computer that's fast enough and suddenly internet sales are
insecure.
This would make an interesting plot for a movie, but we are safe.
A computer simply cannot move at these speeds.
The problem would take hundreds of years to decode.
By then, your credit card number is pretty safe.
So having a computer even 1,000 times faster isn't going to be enough to crack the code.
The idea is that ensure the safety of such methods also determine how large a data set
someone can look at or if someone's data analysis technique can be done in real time.
To get a handle on this, let's see one of my favorite examples from class.
We can do it with a book of baby names.
At one time I used a big book of phone numbers in my office.
Today we'll use a copy of Le Miserable.
The book is among the longest written, over half a million words.
War and Peace is only a little bit longer.
Now say you chose a word on one of its approximately 1,500 pages.
How long would it take me to find it?
In math, it can help to look at a smaller problem.
Sometimes great insight is gained from smaller problems, enabling one to tackle and solve
a much larger problem.
Suppose rather than lay Ms, I take a copy of Good Night Moon, a classic book for young
children.
It has about 150 words.
Now you again will choose a word on a page.
Can I guess it?
There are only 32 pages, so I can start at the front and read along.
People tend to speak at a rate of 120 words per minute, which is about 2 words per second.
So Good Night Moon at an adult rate of speaking would take about 1 minute and 15 seconds to
read aloud, about 75 seconds.
If we did this many times and you picked your word randomly, then on average it would be
somewhere in the middle.
So this would tend to take just over 40 seconds.
But wait, this technique isn't as helpful with lay Ms, I, reading two words per second
means I'd need over 69 hours.
That's over two days if I read the entire book.
On average, I could find a word in lay Ms, in just over a day.
That's silly and impractical.
This isn't the only way to look for a word.
If you cut all the words out and jumble them up, and I needed to know which piece of paper
was the one you chose from, then I might need such an approach.
But the words are ordered on pages, and I can take advantage of that.
So I'm going to cut the book in half.
Is your word in the pages that I hold in my left hand?
Yes?
Then I look only at those pages, else I look in the other half.
I just reduced the problem in half.
When I was reading only one word at a time, I was reducing the problem only by one word
at a time.
Not a big deal with Good Night Moon.
It became a big problem with lay Ms.
Now I can use this trick again.
With the half I have remaining in my left hand, I can split it in half.
Is your word in the portion that I'm holding in my hand?
Yes?
I look there.
No?
I look in the other portion.
So how long would this take?
For lay Ms, I start with half a million words.
Then I go to a quarter of a million in one step.
Then I go to 125,000 words in two steps.
Just over 31,000 in four steps, and just under 1,009 steps.
At some point, you will have one page of words.
Now you can split the words on the page and continue the process.
And just under 20 steps, you'll be down to one word.
With Good Night Moon, it takes eight steps.
Note, lay Ms.
Arabla is over 3,000 times longer than Good Night Moon.
But it only takes two and a half times longer to find a word in a much, much bigger problem.
But I haven't quite answered our question yet.
I generated this example by having a human reading.
What if a computer does the searching?
Again, with large data sets, we can still have an issue.
Consider Twitter, where hundreds of millions of updates are posted per minute.
This is easily over a billion words per minute.
If you had to search this, you were going to have to have a good algorithm.
Doing what we did with lay Ms.
Would take 31 steps to sort through just one minute of Twitter data.
Is that still a good algorithm?
Computer scientists, long before the massive data sets of today, have studied how fast
algorithms work.
This is called complexity theory.
It can help you compare algorithms and know if one will work as problems grow.
In fact, it can tell you if what you are doing will work if your company suddenly spikes
in users.
If you go from 150 users to half a million, will things still be done efficiently?
Or like reading straight through lay Ms versus good night moon, do we get answers in days
rather than seconds?
So let's dive into this realm.
Again, it can help us know how big of problems we can analyze.
If we double the problem, is it going to take just a tad longer or twice as long or more?
To aid us in this, let's look at songs.
Let's see if you recognize this one.
I'll speak the words to spare us all from my singing.
So here's the first verse of our song.
In a cavern, in a canyon, excavating for a mine, dwelt a minor 49er in his daughter,
Clementine.
Oh my darling, oh my darling, oh my darling, Clementine, thou art lost and gone forever,
dreadful sorry, Clementine.
That's Clementine.
It has a total of five verses.
Those I can choose how many verses I'll read.
How many words I read depends on the number of verses I'll select.
With verse one, there are 40 words.
If I read two verses, there will be 80 words, 40 from the first verse and another 40 from
the second.
A total of three verses means 120 words.
What about five verses, 200?
Why?
There's a formula.
40 times the number of verses I'll read.
If the song had 20 verses, then I'd read 40 times 20 or 800 words.
In general, we can say the number of words in N verses is 40 times N.
If we plot the number of words for each verse, then we begin to see an important part of
its growth.
This is known as linear growth.
We can see it in our formula, 40N.
That's a line.
And that's exactly what we see in the graph.
Let's try another type of song.
What's sometimes called a cumulative song.
Again, I'll speak the words.
Here's verse one.
Old MacDonald had a farm, E-I-E-I-O.
And on his farm, he had some chicks, E-I-E-I-O.
You can probably take it from there.
So if we count E-I-E-I-O as a single word and don't repeat the first line at the end,
then that's 41 words.
Now let's do the second verse to be sure we remember how the song works.
Old MacDonald had a farm, E-I-E-I-O.
And on his farm, he had a pig, E-I-E-I-O.
With an oink, oink here and an oink, oink there.
Here an oink, there an oink, everywhere an oink, oink.
And with a cluck-cluck here and a cluck-cluck there, and so forth.
This is about 61 words.
So reading the first and second verses has 102 words.
Note, each successive verse is longer than the previous, of course.
Reading verses one through three would have 183 words.
One through four would have 284 words.
We can plot this, and it isn't a line.
But what is it?
Now sometimes people will hastily say this is exponential growth.
News stories often misuse the words, exponential growth.
They grow exponentially.
By that, they tend to mean something is growing quickly, but this is not exponential growth.
Then what is it?
Let's work it out.
Every verse starts with 21 words.
Old MacDonald had a farm, E-I-E-I-O, and on his farm he had some chicks, E-I-E-I-O.
Old MacDonald had a farm, E-I-E-I-O.
Then we have the portions related to the animal sounds, like with a cluck-cluck here and a
cluck-cluck there, here a cluck, there a cluck, everywhere a cluck-cluck.
That's 20 words.
For verse two, there are two animals.
We have 20 times two words for the animals, which for us was a chick and a pig.
Now how many total words for verse N?
It would be 21 plus 20 N.
Remember though, we are adding these up.
So verse three, reading from verse one through verse three would be 21 plus 20 plus 21 plus
20 times two plus 21 plus 20 times three.
Suppose I do this for 100 verses.
This would be 21 times 100 plus 20 times the quantity one plus two plus three plus four
all the way up to 100.
Now we can write the sum of one through 100 in a clean way.
How do we do that?
Well, we turn to a famous story that this problem was given to in primary school in the
late 1700s as actual punishment.
Think about doing this in elementary school, adding all the numbers between one and a hundred.
This definitely would give a teacher a break, except the young mathematician Carl Friedrich
Gauss was a child in that classroom.
He later became a prominent mathematician with work that is used today in many fields
of mathematics.
Gauss saw a pattern.
Take the numbers one through 100 and under them write the numbers 100 through one.
Now add each column of numbers.
So you are adding 101, which is 101.
When you add 99 and 2, which is 101, that's the key.
You will get 101 in every case, and there are 100 of them.
So the sum of one through 100 and 100 through one is 100 times 101.
That is twice what we need, so summing one through 100 is 50 times 101.
What if I add the integers 1 and 1 through 500?
This would equal 250 times 501.
The formula in general for adding the numbers between 1 and n is n times n plus 1 divided
by 2.
This gives us what we need for old McDonald.
If we read n versus, we will have 21 times n plus 20 times n times n plus 1 divided by
2.
This is a quadratic equation.
So the graph we generated earlier is a parabola.
This is much faster growth than linear.
You can call it parabolic growth, but it's not exponential.
If it isn't exponential, what is?
What does exponential growth look like?
Actually, exponential is difficult to demonstrate in songs, as the growth is so quick.
So let's make up a song, and we'll call it Make It Stop.
Here's the song.
Verse 1, 0, 1, 2, and you read through the number 9.
So you read the numbers 0 through 9.
Verse 2, read the numbers 0 through 99.
Verse 3, read the numbers 0 through 999.
We will assume one single digit number takes one word to say.
Two digits takes two words to say, and so forth.
So verse 1 has 10 words, the numbers 0 through 9.
Verse 2 has 100 or 10 squared words.
Verse 3 has 1,000 numbers or 10 cubed words.
So if we read verses 1 through 3, then it would take 10 plus 10 squared words plus 10 cubed
words.
Between things general again, what would it take to read N verses?
Well, it would be 10 to the first power plus 10 to the second power plus 10 to the third
plus 10 to the fourth plus all the way up to 10 to the nth power.
Now we're going to call this sum S. You add it all up, and it equals S. So again, S equals
10 to the first plus 10 to the second plus 10 to the third plus 10 to the fourth keeps
summing up until you get 10 to the nth.
Now we're going to do this little trick.
We're going to take 10 times S. What would 10 times S be?
Well, that equals 10 to the second.
Why?
Because we had 10 to the first times 10.
That's 10 to the second.
So we get 10 to the second plus 10 to the third plus 10 to the fourth plus all the way
up until at the end, we would have 10 times 10 to the n, which is 10 times n plus 1.
Now why did we do that?
The reason for doing that is we subtract S from 10 S. So we have 10 S, which again is
10 to the second plus 10 to the third plus 10 to the fourth all the way up to 10 to the
n plus 1 minus S, which equals 10 to the first all the way up through 10 to the n.
10 S minus S is 9 S, simple enough.
And then look at that right hand side.
Look at all of that cancellation that happens.
We do not cancel the last term on 10 S, which is 10 to the n plus 1, and we don't cancel
the first term on S, which is 10 to the first.
Remember, we had 10 S minus S. So this all simplifies to 9 S equals 10 to the n plus 1
minus 10 to the first power.
What does this look like carried out to seven verses?
It's exponential growth.
Note how even plotted on an exponential axis, this looks like nothing's happening for the
first three verses.
There is much happening, but it gets lost in the growth between the sixth and the seventh
verses.
This is characteristic of exponential growth.
Nothing much seems to happen, and suddenly there's a spike.
So this is growing fast.
If you sing 100 verses, you have more words than the number of atoms in the observable
universe.
100 verses can sound like a lot, but remember, if you ever sang 100 bottles of milk on the
wall or whatever other beverage you chose to use, that is possible with some songs.
It's not possible with this one.
The much faster growth comes from the variable in that exponent.
Rather than n to the tenth power, we have 10 to the nth power.
That's a huge, huge difference.
Early when I was searching for the word in Les Mis, I was using exponential decay to
get where I was going quickly.
I was able to search two to the nth power words in n steps.
So let's return to the issue of exponential growth and have it be a doubling similar to
how I search for the word in Les Mis.
Suppose a highly contagious virus hits.
It starts with one person.
The next day, that person and someone else is sick.
The next day, four people are ill.
Suppose the illness stays in Manhattan, which has about 1.6 million people.
Also let's assume it would take 10 days to create a vaccine.
On day one, 1 in 1.6 million people are ill.
Even after a week, only 64 people are ill.
After two weeks, about 8,000 people have been ill or half a percent of the population.
After three weeks or 21 days, you are at 1 million people.
And the next day, everyone's infected.
But note, on day 20, you only have 30 percent ill.
What does this mean?
If you wanted the vaccine available on day 20, when 30 percent of the population is infected,
then you needed to start it on day 10.
When 512 people or only .032 percent of the population are infected, that's really hard
to see.
Exactly.
Exponential growth can, to a certain extent, appear to be doing almost nothing and then
spike.
That's why paying attention to those small changes can be quite important.
Okay, so let's try another example.
This time, looking at the growth of photography.
In 1930, about a billion photos were taken.
Thirty years later, that number had grown to about 3 billion photos taken annually.
In 1970, about 10 billion photos were taken a year.
1980, about 25 billion.
1950, 57 billion.
And in 2086 billion photos were taken a year.
So what about the growth of photography at that point?
Was it linear, exponential?
Neither?
Well, it wasn't either.
It looks like parabolic growth, but with slowing toward the end.
But then, notice what happens as digital cameras became widely available.
By 2011, we took more than 380 billion photos in a year.
You may have deleted more digital photos than some people took in their entire lives.
So what about digital photography?
Has the growth become steep but linear?
Maybe not.
It also doesn't look truly exponential.
It's not like our unsingable make-it-stop song.
We would want more data to decide, but interestingly, the growth of digital photography looks like
a continuation of the parabolic curve that we saw for non-digital photography.
It's just that now we are on a steeper portion of that same curve.
Clearly, digital libraries of things like photographs are going to be a large and a very imposing
dataset.
So let's return to complexity.
Having a bigger and faster computer just isn't enough.
If you have an inefficient algorithm, it may take literally thousands of years to solve
certain styles of problems.
They are simply that difficult.
No quick algorithm is known.
In fact, some problems can be shown to be computationally intensive.
Our encryption problem for credit cards is that type of problem.
Earlier, we searched through Les Mis using a technique called binary search.
Searching is one big thing to do with data.
Another is sorting.
One way to sort is to put everything in a pile and take the items one by one and insert
them into their proper place.
It works, but for large datasets, that can be very slow.
Let's see another technique.
It is faster, which usually means it's a bit more complicated.
This method uses the idea of divide and conquer, which we were using to search through Les
Mis.
The idea is to split the array into two lists.
One list contains items less than some value, and the other list contains items greater
than or equal to some value.
Sort both lists and then recombine, which we will see is quite easy.
This is called, appropriately enough, quick sort.
It's quick.
Okay, so suppose our list, we'll start small, is 5, 3, 7, 4, 6.
So we pick the value that will determine what goes into the two piles.
We call this the pivot.
Let's pick 5, since it is the first element in our list.
Now, 3 goes in the first pile, since it is less than 5, as does 4.
So 5, 7, and 6 go in the other pile.
So we now sort the first pile into 3 and 4.
We sort the other pile into 5, 6, 7.
Note, when you combine, the left is sorted, and all the elements are less than those in
the right.
Combining is trivial.
If your list is bigger, then you simply apply that same idea to those two piles that you
created on the first step.
You again divide them into two pieces.
This is called recursion.
Keep using the idea to produce smaller and smaller versions of the same problem until
the problem is small enough, it's easy to do.
When you get down to, say, 10 items, it is quick and easy to sort, so just do it.
Then backtrack up the ladder of recursion and combine those lists.
I used this idea when I was a poll volunteer during a presidential election.
One job was sorting the numbered voting slips that voters placed in front of the voting
booth before exiting.
They weren't sorted and had to be to ensure that we got all of them back.
I have a PhD in math and teach computer science, so I was maybe naturally asked to do the sorting.
I looked at all those slips of paper and just stared at them for a moment.
What could I do?
Then it hit me.
I'd use quicksort, not on a computer, but by hand.
So I went to a big empty table.
The papers were all between 1 and 500, and I knew that, so I picked 250 as my pivot.
I tossed the papers quickly into the two piles.
Then I went to the pile on my left and used 125 as my pivot.
That broke that pile into two piles.
I kept dividing all the piles until I had about 10 slips in a stack, and then I'd order
them.
Toward the end, I literally had about 50 small stacks on the table.
I remember another volunteer came by as I was doing this and went, whoa, what are you doing?
Now I had the order of those 50 stacks in my head, but I was still sorting that last pile
of 10.
So I said, wait, wait, wait, wait, just wait.
I had all those stacks, and then suddenly, boom, boom, boom, boom, I picked them all up,
and I turned with the big stack and said, there, I'm done.
They were sorted.
It was as if I was some kind of genius tracking all those things at once.
Now I did know my path through the 50 stacks, but that was because I had built the path
in little steps that were easy to remember.
And I was able to do the sort because I knew the method was fast, and it would let me get
the big job done in little jobs of manageable size.
Try this the next time you need to sort through a pile.
I've used the same method to sort exams into alphabetical order.
Computers make this much faster, of course, but it's the algorithm that's the key.
The algorithm is what makes it easier to manage complexity, whether it slips of paper or big
data on a computer.
So keep this in mind.
Sometimes we can work with small data sets and gain great insight, but we also want to
keep in mind what can change when we work to large data sets, especially the even larger
data sets that might follow.
Else, what we do today may become obsolete tomorrow and beware of thinking that bigger
computers alone can make up the difference.
Imagine if Google's search algorithm didn't scale.
The number of web pages has grown at an alarming rate, a genuinely exponential rate.
If they could not find or develop scalable algorithms, then Google may have been great
with millions of web pages, but failed to keep up with billions of web pages, or failed
to keep up today when the number of web pages is estimated in the trillions.
By scaling up by a hundred or a million in size, the time needed could have suddenly
become problematic.
Imagine if, a year from now, given the growth in the size of the web, it suddenly didn't
take a second or less to get a search result from Google.
Now it took an hour, or overnight.
Google as we know it wouldn't be Google.
Part of Google's success was its ability to adapt its algorithms to the growing size
and complexity of the web.
Today's developers of applications and services must think ahead.
There may be 10,000 users now, but if things go well in today's age, there could be several
million users.
Can the current methods deal with huge, even nonlinear increases in scale?
Sometimes faster algorithms have a more complex design, with some details that enable increased
speed.
More complexity in the algorithm can mean simplifying what your computer needs to do
to finish the job.
Algorithms are the key to managing complexity.
It is algorithms that can make one person or company's intractable problem become another's
wave of innovation.
