way.
There is a lot of infrastructure supporting this and of course, there is all that data.
Have you ever had your digital camera run out of memory?
Well, the same thing can happen with Facebook.
What they do is move the data periodically to even larger data centers.
Think of the size of this problem.
If not done well, it could be a big data headache.
But this is something Facebook does seamlessly to the user.
And yes, they are using Hadoop to help.
In 2010, Facebook had the largest Hadoop cluster in the world with over 20 petabytes of storage.
The cluster grew to 30 petabytes by March 2011.
30 petabytes is 3,000 times the size of the Library of Congress.
What next?
They had to move to a larger data center.
Facebook had literally run out of power and space.
So what do you do?
One option was to physically move the machines.
Get enough hands on the job and it wouldn't take too long.
But remember, we get impatient if a search result isn't shorter than the blink of an
eye.
Making Facebook down for days was far from a good business decision.
So they wanted to set up a replication system that mirrors changes from the old cluster
to the new larger cluster.
Then when it was time to switch, everything would be redirected to the new cluster.
Sound easy?
Maybe, but remember the files that are being replicated aren't static.
They are in motion.
Files are created and deleted continuously.
So they decided they would migrate, but to do so, they had to create a new way to do
it.
And a key component of doing this, indeed, Hadoop.
The replication was created and successfully implemented.
If you were on Facebook, you probably didn't even notice that Facebook data moved.
That was the point.
And that's the characteristic of modern big data storage issues.
New techniques need to be created, and they are.
Ironically, we aren't always aware they happened as sometimes they are created so we never
see their impact.
The data world doesn't feel any bigger to the user.
