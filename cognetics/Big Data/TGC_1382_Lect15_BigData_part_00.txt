Sometimes data analysis is too good to be true, or it is too good to be truly useful.
We call this overfitting.
Let's look at the ways this can happen and what we can do about it.
In 2014, Warren Buffett offered a billion dollars to anyone who can perfectly predict
winners of just over 60 games in the NCAA's March Madness Tournament.
So what if I come along offering to sell you a method that goes like this?
It's a system of linear equations, one equation for each game teams have already played.
I used the data available from the last 10 years and I create a perfect system that
predicts all those games over all those years.
How much would you be willing to pay for my method?
Sound amazing or too good to be true?
We'll see in the next lecture that techniques that sound very similar do offer genuine insight.
But hopefully this particular promise sounds no more valuable than my having a bridge to
know where you can buy or maybe some prime swampland.
Here's why the method I described won't work.
If I include the tournament that I hope to predict in the data, then I already have information
that I'm trying to predict.
If my method knows its job is to predict the tournament and that that data is already included,
it is actually possible for some methods to simply look at those games and predict the
winner that way, since it already knows the outcome.
In other words, if I know the outcome of a game I need to predict, then I simply won't
pay attention to the outcome of any other game.
We discussed this in an earlier lecture as an example of the need to separate training
data from the data we test and predict.
This is an example of overfitting data.
Here, it is pretty obvious something went wrong.
It is too good to be true.
In terms of what we said in an earlier lecture, we haven't kept our training data separate
from the data we use to predict.
This is one way that overfitting happens in data analysis.
Here's another example from my research computer lab at Davidson.
In 2013, three undergraduates were conducting research into sports ranking methods.
One morning, I walked into the lab and asked how things were going.
They said, ah, it's really tough today.
I said, so the method isn't working?
They commented, oh, no, it's working great.
In fact, it's working too great.
We're trying to figure out what we're doing wrong.
It took a couple of days for our team to see what we were, in a subtle way, doing wrong.
We were including future information.
