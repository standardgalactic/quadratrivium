So how do we do this?
Let's assume we have our data in a table.
Each row represents a person's ratings of movies.
We can create the training set by selecting random rows, like 80% of the data set.
The remaining rows are the test set.
Did it work?
Again, we probably don't know, but things were created at random, so we can simply do
it again.
In this way, we can create a few training and test sets and see how we do.
This again tests the robustness of our method.
As long as we keep any given training set entirely distinct from its corresponding test set,
we're okay.
Keep in mind, this assumes that your data is ordered randomly or at least placed randomly
into training and test sets.
If that's not the case, if training data differs from test data in some way, then this approach
might not work.
Clearly, there is subtlety here, and what we do in those cases is still an area of active
research.
In many ways, this shouldn't be all that surprising.
For example, we have been trying to create representative samples in political polling
for years.
We have, at various times, created successful ways to do this, but when the technological
landscape changes or the issues change or the nature of the population changes, we can
need new ways to specify our training and test data, too.
Note, putting data into training and test sets is done prior to creating methods that
analyze them.
If the goal is to predict some phenomenon, then we process the data in this way prior
to creating our method.
This isn't the only method of pre-processing.
What else might we do?
Why should we?
Data can be what is called dirty.
If data is dirty, how can you trust your results?
Data can be incomplete, noisy, and inconsistent.
This can be due to human error, limitations of measuring devices or flaws in how the data
was collected.
It can be that nothing really went wrong, but the data isn't necessarily in the assumed
form.
For example, one person could have two entries in the data due to changing addresses.
