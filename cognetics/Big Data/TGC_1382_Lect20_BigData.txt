Have you ever looked back at the series of decisions that determine some course of action
for you?
In today's lecture, we'll see how a tool called decision trees can analyze many of
the variables far beyond what you might think of as decisions.
We'll see how we can combine research results from multiple studies to gain medical insight
or do a deeper dive into a single event such as what happened on the fateful night that
the Titanic sank or hunt for new customers.
Decision trees are one of the most transparent and powerful techniques in all of data analytics.
Let's start with the decision trees involving probability.
I'll use a medical example from the text, Staphs, Modeling the World by Bach, Veleman,
and DeVoe.
There are two studies.
First, a study by the Harvard School of Public Health called binge drinking on campus, results
of a national study.
They found that 44% of the college students engage in binge drinking, 37% drink moderately
and 19% abstain entirely.
A second study appeared in the American Journal of Health Behavior, and it reported that among
binge drinkers between the ages of 21 and 35, 17% were involved in alcohol-related auto
accidents, while 9% of non-binged drinkers in the same age group were involved in such
accidents.
Let's ignore the fact that college students are often a bit younger than the 21-35 year
olds of the second study.
Can we combine the results of these two studies?
That would let us determine the probability of a randomly selected college student being
a binge drinker and in an alcohol-related accident.
A decision tree diagram makes it easy to combine two studies and answer such questions.
We're interested in college students, so the first step is to branch out according to
college drinking habits.
So we have a branch representing 44% likelihood of a college student being a binge drinker,
37% of being a moderate drinker and 19% having abstained.
Here we have visualized the results of the first study about college students.
Now we fold in the second study about adult binge drinking.
First, we have that 17% of the binge drinkers were involved in alcohol-related auto accidents.
So we add this to the branch related to binge drinking.
Then we also know that 9% of non-binged drinkers were involved in alcohol-related auto accidents.
This gives us this.
Now let's look at our question again.
We're interested in the probability of randomly selecting a student and that person being
a binge drinker who's been involved in an alcohol-related accident.
We find this by first following the branch for binge drinking and then its branch for
an auto accident.
We can multiply the two probabilities along each path.
So we find 0.44 times 0.17, which equals 0.075.
In this way, we can fill out the entire tree.
We can also use the tree to find answers not already displayed on the tree.
For instance, if someone is a drinker involved in an auto alcohol-related accident, what's
the probability that the person was a binge drinker?
To find this probability, we are interested in a ratio involving the top branch, a binge
drinker who was involved in an accident, which was 0.075, and we divide that by the sum of
the branches involving an accident, which is 0.075 plus 0.033, or 0.108.
So we find this probability as 0.075 divided by 0.108, which equals 0.694, or about 69%.
Think about this.
We had two entirely separate studies, yet we combined their data to find a clear result.
The chance that a student who has an alcohol-related car accident is a binge drinker, and we found
that to be more than 69%.
Results like this may not be obvious from two studies, but the results become very clear
with tree diagrams.
Notice the kind of data analysis here.
We are combining the data of two studies to answer our own questions.
But you can, of course, also collect the data yourself.
I did this with Dr. Kevin Hudson and Dr. John Harris of Furman University.
It started with an example I gave in class.
I brought up the legendary Casey At The Bat story.
In it, Casey is so confident of his ability to score that he lets two strikes whiz by
without even trying.
What is the probability of his getting a hit on that last attempt?
It could seem like one-third if he hit like Ty Cobb with a batting average around .366.
But remember that a batting average is made on all pitches and not simply letting two
of three intentionally go by to raise the drama, then swinging just once and hitting
a sure thing.
So we used pitching data John and Kevin had already scraped off the Internet.
Here's the first branch of our tree, which is whether the batter swings at the ball or
just lets it go by without trying.
If a major league baseball hitter has two strikes and no balls, then we found that he
swings the bat 75% of the time.
So that's how often major league players do what mighty Casey did.
How do we know?
I didn't.
The data told us.
OK, let's ask a different question.
How often does a player go out on that very next pitch?
Again, we looked at the data.
From here, we see that the average professional batter with no balls and two strikes gets
out on the very next pitch .75 times .844 plus .25 times .6, which equals .783 or about
78% of the time.
That's what a player trying to showboat like Casey would be up against.
Keep in mind more pitches will come if the batter does not swing and if the pitch was
a ball, which happens .4 times .25, which equals .1 or 10% of the time.
But that's not what Casey did.
He took a swing.
And while Casey might have been a great hitter, intentionally taking two strikes gave a really
high likelihood of there being no joy in Mudville that day.
He should have looked at the probabilities.
Decision trees can enable us to combine results to answer new questions or use probabilities
from a large data set to study particular cases.
They can also uncover information that potentially tells us a story.
Let's use the data to see the story behind this headline.
Yes, the Titanic.
It set sail on April 10, 1912.
There were 2,224 passengers and crew that set sail on April 10 from Southampton, England,
headed to New York City.
Of that, 711 survived.
The Titanic was the largest ship in the world at over 882 feet in length and 92 feet in
breadth.
Her gross tonnage was over 46,000 tons.
Four days into the journey, about 20 minutes before midnight, the Titanic was sailing about
400 miles south of Newfoundland.
The ship received six warnings of sea ice, but was traveling near her maximum speed when
she sighted the iceberg, unable to turn quickly enough.
The ship suffered a glancing blow just before midnight.
The impact buckled the ship's starboard side and opened five of her 16 compartments to
the sea.
Now, the Titanic had been designed to stay afloat with four flooded compartments, but
not more.
Soon, the crew realized that the ship would sink.
Forty-five minutes after the ship hit the iceberg, Captain Smith ordered the lifeboats
to be lowered under the orders women and children first.
So passengers evacuated in lifeboats, though many were launched only partly loaded.
By 2.20 a.m., the Titanic broke apart and sank, with well over 1,000 people still aboard.
A distress call had been sent, but the closest ship to respond was 58 miles away.
It arrived about four hours later and brought aboard an estimated 711 survivors.
The sinking of the Titanic is an event about which new books are still being published.
And now, we turn to the data.
The British Board of Trade collected data in their investigation of the sinking.
Let's look at the passenger list of the 2,224 passengers and crew.
It's a list of all those passengers and crew, and for each person, we have four pieces of
data.
Number one, the sex of the individual.
Number two, age, which will be labeled as adult or child.
Number three, booking class labeled as first, second, or third class, or crew.
Number four, whether or not that person survived.
Let's take a moment and talk about class.
The three separate classes were determined not only by the price of a ticket, but also
the person's wealth in social class.
Those traveling in first class, the wealthiest passengers on board, were prominent members
of the upper class, including millionaire, philanthropist, and women's right activist,
Margaret Brown.
Second class passengers were middle class travelers and included professors, authors,
clergymen, and tourists.
Third class or steerage passengers were primarily immigrants moving to the United States and
Canada.
There actually isn't complete agreement among primary sources as to the exact number of
passengers on board, rescued, or lost.
Even so, what we can learn about this disaster, we can see in the data.
More complete lists also exist with names, places of origin and such, and we'll restrict
our analysis to show that we can do what we can do with four pieces of information.
First, recall Captain Smith order that women and children board the ships first.
What does the data say?
Well, 57 children survived, 316 women survived, 338 men survived, more men than women survived.
Were the orders not followed, what happened?
Remember, you must be careful how you interpret data.
It is true that more men than women survived, but be careful drawing conclusions about
the meaning of such numbers.
Percentages suggest a different story.
The overall survival rate for men was 20%.
For women, it was 74% and for children, 52%.
Why are the percentages so different from the totals?
There were many more men on board than women or children.
So we see that the order to fill the boats with women and children first was mostly followed,
though apparently less so for children than for women.
In fact, third class women were 41% more likely to survive than first class men.
Let's return to an earlier statistic.
The survival rate for women was 74% and 21% for men.
Now let's take males and split by adult male and child.
In the group of adult men, 20% survived.
The group of male children, 45% survived.
Now we'll partition the group of children again by class.
When we do, we see that 27% of third class children survived and 100% of first and second
class children survived.
Okay, well let's continue this and let's move to the group of women.
Let's split that group by class.
First, second and third and crew.
93% of those in first, second or part of the crew survived.
46% of the women in third class survived.
These statistics tell us the tale.
Women and children in first and second class had an extremely high survival rate.
Unfortunately, the same survival rate didn't extend into the third class, although even
there women and children were much more likely to survive than men.
What we've been doing is a sort of informal decision tree analysis.
We started with Captain Smith's order and let that guide our splits that we made.
But we would get the same results if we had a computer using an algorithm from decision
tree learning.
We could have split by men and women, adult and child, or a few different ways by class
and crew.
In all, there are a number of ways to split at that first level.
So which is better?
A decision tree algorithm splits such that we get the best predictor of whether someone
survived, which is our target value.
For the Titanic, the best indicator of survival if you take only one attribute is whether
someone was male or female.
Then you look at each of these attributes and decide where to split.
You do if you improve your prediction.
Between males by adult and child, we find that the survival rate from 21% for all males
to 20% for adult men.
Then if we split by class, we find that second and third class men survive at a rate of 14%.
An interesting result if we split yet again is that third class men were twice as likely
to survive as second class men.
Let's look at how these factors help us.
The newspaper headline said women and children.
But if sex and age alone determined the probability of survival, we would expect women in each
class to have just under an 85% survival rate.
But this split, by this split, children had about 52% chance of survival and men 20%.
We saw this would be misleading for important subgroups.
Over 97% of 144 women in first class survived, 86% of the 93 women in second class were saved.
So the overall figure was quite accurate for second class.
But only about 74% of the women in both third class and crews survived.
There were 165 women in third class and 23 in crew.
So adding in class, we find 93% of women in first and second class and the crews survived.
Note what a strong predictor this is of survival.
Children of either sex in first and second class survived at a rate of 100%.
Those on the other side of these three splits were much less likely to survive.
Women in third class survived only about 46%.
Children in third class survived at less than 28%.
So we have predictive variables of survival for the titanic disaster.
From this, we uncover using only the numbers, the tale of that fateful night.
Also notice how we essentially create a series of questions that one can answer.
Are you male?
We now know your likelihood of survival 21%.
If you're male, are you a child?
We again have a survival rate and that's 45%.
Are you in first or second class?
And so on and so on with those series of questions that come directly out of our decision tree.
This very same setup can be applied in many other contexts.
For instance, decision trees have been a powerful research technique in medical research
on heart disease.
Heart disease is the leading cause of death in the world.
A nice attribute of decision trees is that they produce a questionnaire a doctor or patient
can ask.
They produce essentially an if then else type structure.
If this, then ask this, else ask this other question.
Further at each step of the process, we have a probability of someone having heart disease.
What's really important to know is that you can try this.
There are nice but expensive programs, but there are also inexpensive add-ins to spreadsheets
like Excel.
I like to use Jump Software, JMP, which is not cheap, but you can literally click a button
in the data splits.
With only a few clicks of the button, you know the Titanic story, which we just discussed.
You feed the program a table of data where one column is what you are trying to predict
survival or death in the case of the Titanic.
The other columns contain what might be predictive of that outcome.
Then, you click to see what you might find.
Like any technique, it may not work as it is splitting on one variable at a time and
thereby missing something when two variables might happen concurrently.
Still, it can be interesting and is frankly a fun way to explore data.
It is important to note that decision trees are part of the larger field of classification.
We use data to classify.
Examples are many.
Classification methods help detect a spam email message from its header and content.
These can be classified based on their shape as spiral or elliptical and then split further.
Banks can take data to determine if they believe someone should be given a home loan.
In this case, you are predicting if someone might default on a loan.
The resulting probability indicates if someone could be seen as safe or risky for a loan.
Who visits each web page offers another example.
When you visit a web page, a digital trail of sorts is left.
Not a lot of information is available, but there is some.
For instance, a data analyst would be able to see that I use the Chrome browser to connect
with from my Mac with a particular IP address that I am on campus and that I am using an
EDU location.
You can also know if the person clicked the page by clicking a link on another web page
or if for instance they came in via a browser.
If not, they directly inputted the web page for the visit and you can see that as well.
This can be analyzed to determine the total number of visitors for a web page, how many
visitors are from .edu.com and .gov sites.
You can also tell what time they visited and what day of the week that people came.
We can use such a log to determine when we have a person visiting versus when it is a
web robot.
Web robots, often called a web crawler, automatically move through the web retrieving information
about web pages.
For instance, search engines use crawlers to see which web pages link to each other and
even some of the textual content on the page.
Suppose a business would like to know if people repeatedly visit a web site and if so what
products they view.
If someone does visit the same product more than once, do rebates or free shipping aid
in the customer making a purchase.
To analyze such things, we first must remove web robot movement through the web pages.
Rather than analyze such data immediately, we can combine the data in order to better
analyze it.
That is, we use the original data to construct a number of attributes not directly in the
original data.
They include the total number of web pages visited in a web session, the total number
of image pages retrieved in the session, the total time taken by the visitor, the depth
of the web search.
For example, visiting a web page has depth one.
If you click a link on that web page, then you reach the depth of two.
So clicking a link on a web page increases your depth by one.
In short, we are taking the data and essentially making a new data set that we can then analyze
to gain insight on that original data.
This approach was taken in the book Introduction to Data Mining by Tans, Steinbach and Kumar.
The resulting decision tree gives the following insights.
Web robots tend to go over a lot of web pages but with very little depth.
Web pages tend to have much more focused sessions.
They are narrow and much deeper.
Web robots tend not to retrieve image pages associated with web content.
Sessions with web robots tend to be longer and contain a large number of page requests.
Web robots also make repeated requests for the same documents.
Given the results of this decision tree analysis, one can look at a particular visitor and have
a good chance of knowing if it's a web robot and then remove such data.
By doing this, we can gather better information on how humans are moving through a website
and decide how rebates and free shipping, for instance, affect sales.
The decision tree allows us to clear a lot of the noise out of our data and do more meaningful analysis.
Decision trees have some other important advantages.
First, they are simple to understand and interpret.
You don't need to know much about data mining or even much about the data itself to understand
how to use the results.
Second, decision trees require little data preparation, apart from maybe combining pieces
of data, as we saw in the example of web logs.
Other methods may perform better, but preparing the data can take a much longer time.
Finally, decision trees are often called a white box method.
We saw this in the Titanic example.
We were able to take the results and see from the data why each split made sense.
Other methods, by contrast, give a black box result, meaning that the result is harder to see or explain.
Decision trees do have limitations.
First, decision trees aren't necessarily performing the best split.
Our method was making only one split at a time.
For instance, our first split was male versus female.
We could have split over adult and child, but we weren't splitting over male child versus female child.
If I wanted an analysis that could be varied by changing the splits, I would use other tools,
such as support vector machines.
That's a model from machine learning, and we actually won't cover that, but it's basically a more nuanced way to do such splits.
Regardless of the underlying method, we can't consider every possible split,
especially for larger datasets with more attributes.
So we choose some way of splitting and do the best we can.
Just keep in mind, the split may not be the best.
On the other hand, if you split too many times, you may have great results for the data you're looking at,
but it may not work well for future data.
This is called overfitting, which we discussed in an earlier lecture.
Another problem with splitting too much is that the rules become quite complicated.
Statistically, you may have a good descriptor of who to give a loan,
but it may become more difficult for the loan officer to even implement.
Businesses wondering where to look for new customers can also turn to decision trees.
Let's look at an example from the book, Data Mining Techniques for Marketing, Sales, and Customer Relationship Management by Linoff and Barry.
They share an example of how decision tree analysis helped one of the author's newspaper clients, the Boston Globe.
They were interested in estimating a town's expected home delivery circulation for that newspaper.
They had lots of data, indeed hundreds of possible parameters, from demographic information to geographic.
The question was, which of these help predict future customers?
Why?
This means you can find a potential untapped resource and knowing to whom you should promote your product is the holy grail of marketing.
So how did they predict future subscribers?
First, they had to discern that percentages rather than raw totals were the right kind of target to take from their existing customer data.
Why?
This takes size out of things.
You may want to find a smaller town or neighborhood that is untapped.
You may only end up with a couple hundred new customers, but you'll be much more efficient than you might have been attracting the same number in a larger market.
Then what?
Well, you need to define what your decision tree is deciding.
Remember the Titanic, where the decision tree had two groups, who survived and who did not.
In the search for new customers, they had three groups, good and bad and in between.
They actually removed those in the middle.
Data from the middle might be important for later sales efforts, but such data was not part of the training set.
So this simplified the model by training it only on data that was clearly from the good or the bad set.
With all this in place, it was time to split.
So how did it split?
The first split was on median home value.
The towns where most houses were worth less than $226,000, according to the 2000 census data, were poor prospects for that newspaper.
Other variables that turned out to be important included the average years of school completed, the percentage of population in blue collar occupations, and the percentage of the population in high status corporate occupations.
Again, how did decision tree analysis help?
They had started with hundreds of variables available via the US Census Bureau data in other sources.
The decision tree narrowed all that down to the few that were most predictive and important.
Similarly, if we had done the Titanic analysis with 10 to 15 pieces of information per person,
we'd likely get a smaller number of splits on the data telling most of the story.
In each case, a decision tree would indicate which pieces of information were most predictive and important.
But we do need a word of caution.
There could be subtle interdependencies in the data that a decision tree will not capture.
Even so, it can bring us down to a manageable number of variables.
Then we can turn to regression and neural networks to define the to refine the analysis.
If you include too many variables in regression or neural networks, the data gets memorized.
As such, you perfectly describe the data you have that's overfit, but you can't predict future behavior, except in the rare case, it already matches perfectly with past events.
Decision trees are a powerful technique.
Not only for decisions we make, but also for understanding all kinds of factors and probabilities contributing to a set of outcomes.
Personally, I always ask myself, can I use a decision tree for this whenever I look at data?
With decision trees, you create a huge sieve in the data deluge, keeping a lot of the good stuff and getting rid of a lot of the noise.
John Tukey famously got everyone to begin graphically displaying data to get familiar with it, true and important.
Decision trees offer a less recognized but equally valuable second layer of familiarity, one that can springboard your analysis.
Sometimes, like with the Titanic, we create a decision tree and we're done.
We have the desired insight.
Other times, decision trees are a first tool that perhaps offers the way to look at other methods.
Either way, decision trees can carve quickly through your data, offering insight and possibly predictions about the future.
So remember, decision trees are easy to use and very powerful.
In fact, a decision tree may be all the analysis you'll need.
