So let's cluster Italian cities of Bari, Milan, Florence, Naples, Rome, and Turin.
To begin, each city, again, is its own cluster.
Finding the distances between the cities we find Milan and Turin are the closest.
So we group them.
From there, we find the distances between this new cluster and the previous clusters.
After this, we find Naples and Rome are the closest.
So that's a new cluster.
Next, Bari groups with the Naples and Rome cluster.
Where are we now?
We have a cluster with Turin and Milan.
We have a cluster with Bari, Naples, and Rome.
And Florence is still its own cluster.
So our clusters at this point aren't well balanced.
Taking this another step, Florence joins the Bari, Naples, and Rome cluster.
Finally, everything becomes one cluster, and then we're done.
A helpful part of this process is seeing it in a dendrogram.
This method is based on the core idea of objects being more related to nearby objects than to objects farther away.
As seen in the dendrogram, these algorithms do not provide a single partitioning of the dataset,
but instead provide an extensive hierarchy of clusters that merge with each other at certain distances.
Keep in mind that clustering is often used on much larger datasets,
as we saw with the musical artists from the Million Songs database.
But sometimes the amount of data is because each object has more data.
But now we had six cities and we're looking only at the distance between each pair of cities.
Let's move up to 38 objects, but now we'll turn to medicine.
Let's identify a relationship between cancer patients and DNA gene expression.
Todd Gullib of Harvard Medical School and his collaborators looked at 38 leukemia samples
containing gene expression of 6,817 genes.
They formed two clusters.
From the patterns found in the genes, the clusters correctly place 24 of 25 samples into ALL leukemia
and 10 of 13 samples into AML leukemia.
The ideal algorithm would place all AML samples in one cluster and all ALL samples in another.
Clustering discerns structure in hidden relationships within the data.
Such methods can potentially help with diagnosis.
When a new patient with an unknown classification arrives,
their data can be compared with the data from the existing classified clusters
and a classification for this new patient can be determined.
Now let's learn another common clustering method.
This is called K-means.
This creates what I think of as globs of data,
where you choose the number of globs in advance.
