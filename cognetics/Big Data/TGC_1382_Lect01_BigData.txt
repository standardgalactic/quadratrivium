You
You
Your lecture is dr. Tim chartier
Dr. Chartier is an associate professor of mathematics and computer science at Davidson
College.
He holds a Ph.D. in applied mathematics from the University of Colorado Boulder.
Dr. Chartier chairs the advisory council for the National Museum of Mathematics and was
named the first math ambassador of the Mathematical Association of America.
He is the author of MathBytes, Googlebombs, chocolate-covered pie and other cool bits
in computing, and writes for the Huntington Post Science blog.
He also fields mathematical questions for ESPN's sports science program and has served
as a resource for the CBS Evening News, National Public Radio, The New York Times and other
major news outlets.
Today, right now, the field of data analysis relates to and impacts our world in unprecedented
ways.
Right now, millions, even billions of computers, are collecting data from smartphones and tablets
to laptops to supercomputers.
Data is an ever-present and growing part of our life.
Take the personal camera, put a few hundred of these together, and that's already enough
combined memory to store the contents of every printed book in the Library of Congress.
In the digital world, each picture can be worth a million words or more.
And we are also emailing at an amazing rate.
Who is the world like before email?
Definitely different, at least in terms of data.
Today, the amount of textual content in email in just ten minutes equates to the full text
of printed books again in the Library of Congress.
That's six times the Library of Congress every hour, 144 times the Library of Congress every
day.
By the time you see this, who knows, maybe the rate will have already doubled.
In fact, by some estimates, humanity is already producing a terabyte of new data per person
every year.
A good portion of that is transient data, meaning not stored or not stored yet.
That's things like digital phone calls, but this already amounts to a trillion bytes of
data per person.
So how much data is there in the world?
A 2012 digital universe study from IDC estimated that the global volume of digital data stored
and managed in 2010 was over one trillion gigabytes.
We need a bigger unit.
That was a billion terabytes, so less than one terabyte per person at that point.
A million petabytes, a thousand exabytes, and over one zettabyte.
That was in 2010, and the number was predicted to double every year, reaching 40 trillion
gigabytes by the year 2020.
Now those numbers are for all the data, but no one person or computer has all the data
that's distributed over all the computing devices everywhere.
Still, even individual data sets are huge.
NASA has deployed a series of Earth-orbiting satellites that continuously gather information
on our world.
In fact, so many applications are creating data sets that are so big that the ways we
traditionally have analyzed data sometimes do not work.
Indeed, the ideas we have today might not solve the questions we have for the data tomorrow.
Right now, data analysis is helping NASA ask a question like, how accurately can we predict
the beginning and end of a region's growing cycle?
As more and more data is collected, and as the technology we use to collect that data
changes, new questions will arise, which may mean we need new ways to analyze the data
to gain insight.
Data analysis is a fairly new combination of applied mathematics and computer science,
available in ways that would have been hard to imagine a couple decades ago, an inconceivable
100 years ago.
Why?
A lot of it has to do with data.
And this flood of new data is being organized, analyzed, and put to use.
On the Internet, Amazon recommends books for you to consider.
Netflix indicates its guests at what you might predict as a rating for a movie.
These are recommendation-based, not just for you, but on comparing your data with that
of millions of other people.
And suppose you give it a try if you take one of those recommendations, even if it isn't
the style of book or film you usually choose.
Well, sometimes we can be surprised with what we see.
Sometimes we like the recommendation, and we didn't even know we would.
Analytics or Amazon did know somehow.
Well, that's the somehow of data analytics that we want to explore through this lecture
series.
Online companies like Amazon, Netflix, Pandora, and so on are gathering rating after rating
after rating for millions of people, and then putting all of that data to use.
Other transformations are taking place in politics, sports, healthcare, finance, the
entertainment industry, science, industry, you name it.
Albert Einstein once said, we can't solve problems by using the same kind of thinking
we used when we created them.
This is definitely true today.
We are collecting data as never before, and that creates new kinds of opportunities and
challenges.
In fact, so many applications are creating data sets that are so big that the ways we
traditionally have analyzed data don't work.
Indeed, the ideas we have today might not solve the questions we have for the data tomorrow.
This is the idea behind the term big data, where the sheer size of large data sets can
force us to come up with new methods we didn't need for smaller data sets.
Big data is often defined as having three Vs, volume, velocity, and variety.
Let's see a few more examples of the volume of data in today's digital world.
Which would you say is bigger, the complete works of Shakespeare, or an ordinary DVD?
The complete works of Shakespeare fit in a big book of roughly 10 million bytes.
But any DVD or any digital camera, for that matter, will hold upwards of 4 gigabytes, which
is 4 billion bytes.
A DVD is 400 times bigger.
All the printed words in the Library of Congress would be 10 trillion bytes, 10 terabytes.
That's one very large wall full of DVDs.
But it's also about the size of a single high-end personal hard drive.
That is, you might carry all the books in the Library of Congress on a single device
the size of just one book.
In data is not merely being stored.
We access a lot of data over and over.
Google alone returns to the web each day to process another 20 petabytes.
What's that?
It's 20,000 terabytes, 20 million gigabytes, 20 quadrillion bytes.
How big do you want to go?
Google's daily processing gets us to one exabyte every 50 days.
And 250 days of Google processing may be equivalent to all the words ever spoken by humankind
to date, which have been estimated at 5 exabytes.
And nearly 1,000 times bigger is the entire content of the worldwide web, estimated at
upwards of 1 zettabyte, which is 1 trillion gigabytes.
That's 100 million times larger than the Library of Congress.
Of course, there is a great deal more that is not on the web.
But let's turn to the velocity of data.
Let's start a clock to see what this feels like.
Not only is there a lot of data, it's coming at very high rates.
High-speed internet connections offer speeds 1,000 times faster than dial-up modems connected
by ordinary phones.
Here are some things that are happening every minute of the day.
YouTube users upload 72 hours of new video content in the United States alone.
There are 100,000 credit card transactions.
Google receives over 2 million search queries, and 200 million email messages are sent.
Now, it can be hard to wrap one's mind around such numbers.
How much data is being generated?
Let's turn to Facebook.
In only 15 minutes, the amount of photos uploaded to Facebook is greater than the number of
photos stored in the New York Public Photo Archives.
That's every 15 minutes.
Now think about the data over a day, a week, or a month.
Now, finally, there is a variety.
One reason for this can stem from the need to look at historical data.
But data today may be more complete than data of yesterday.
The cost of a gigabyte in the 1980s was about a million dollars.
So a smartphone with 16 gigabytes of memory would be a 16 million dollar device.
Today, someone might comment that 16 gigabytes really isn't that much memory.
Exactly.
This is why yesterday's data may not have been stored or have been stored in a suitable
format compared to what we can store today.
Now, consider satellite imagery.
The images come in a large variety of aspect ratios.
While I know that a satellite image will contain pixels, I don't necessarily know what is in
the picture or not in the picture.
I don't necessarily know where to look.
I may not even know what to look for.
So we stand in a data deluge that is showering large volumes of data at high velocities with
a lot of variety.
With all this data comes information.
And with that information comes the potential for innovation.
Steve Jobs, charismatic co-founder of Apple, was diagnosed with a pancreatic cancer in 2003.
He became one of the first people in the world to have his entire DNA sequence, as well as
that of his tumor.
It cost him a six-figure sum, but now he had his entire DNA.
Why?
When doctors pick medication, they hope the patient's DNA is sufficiently similar to
the patient in the drug trial.
Steve Jobs' doctors knew his genetic makeup and could carefully pick treatments.
When one treatment became ineffective, they could move to another.
While Jobs eventually died from his illness, having all the data and all that information
added years to his life.
We all have immense amounts of data available to us every day.
Research engines almost instantly return information on what can seem like a boundless
array of topics.
For millennia, humans have relied on each other to recall information.
The Internet is changing that and how we perceive and recall details in the world.
Human beings tend to distribute information through what is called a transactive memory
system.
And we used to do this by asking each other.
Now we also have lots of transactions with smartphones and other computers.
They can even talk to us.
In a study covered in Scientific American, Daniel Wegner and Adrian Ward discuss how
the Internet can deliver information quicker than our own memories can.
Have you ever tried to remember something?
In meanwhile, a friend types it into a smartphone, gets the answer, and if it is a place already
has directions, in a sense, the Internet is an external hard drive for our memories.
So, we have a lot of data with more coming.
What works today may not work tomorrow, and the questions of today may be answered only
to springboard tomorrow's ponderings.
But most of all, remember that within the data can lay insight.
We aren't just interested in the data.
We are looking at data analysis, and we want to learn something valuable we didn't already
know.
So, let's look at some examples of this.
Now you don't need large data sets to pose computationally intensive problems.
And even at a small scale, such problems can be too hard to allow for optimal solutions.
For example, UPS must decide on a delivery route for packages to save time and gas.
Consider 20 drop-off points.
Which route is the best?
Seems simple enough, but checking all possible routes isn't that easy.
You have 20 choices for the first stop, 19 for the second, and so forth.
In all, there are about 2 times 10 to the 18th power.
How big is that number?
It has five times the estimated age of the universe.
I mean, clearly, we aren't checking that number of combinations on a computer each time a
driver needs a route.
Keep in mind, that's only 20 stops.
UPS has about 55,000 drivers every day.
Until recently, UPS drivers had a general route to follow.
It allowed for decisions on the part of the driver.
UPS now has a program called Orion, or on-road integrated optimization and navigation, to
help.
It uses math to decide on routes.
They can be counterintuitive, but save time in the end.
It doesn't find the best route, but a lot of research has been done to find good solutions
to this problem.
Keep in mind, UPS has a harder problem than simply finding a route to save time.
They must also consider other variables, like promise delivery time.
So how much can this save?
Under these two numbers, $30 million.
That's the cost to UPS per year if each driver drives one more mile each day than necessary.
$85 million.
The number of miles the analytics tools of UPS are saving per year.
Data analysis doesn't always involve exploring a data set that is given.
As questions arise, and data hasn't even been gathered, then the key is knowing what
question to ask and what data to collect.
As an example, let's join Oren Ezioni on a flight from Seattle to Los Angeles for his
younger brother's wedding.
Wanting to save money, Oren bought his ticket months before the idos were set.
During the flight, Oren asked neighboring passengers about their ticket price.
Most had paid less, even though many had bought their tickets later.
For some of us, this might simply tell us not to worry so much about choosing close
to the date of a flight.
But Oren was Harvard's first undergraduate to major in computer science.
He graduated in 1986.
To him, this was a problem for a computer to solve.
He'd seen the world this way before.
He helped build Metacrawler, which was one of the first search engines.
InfoSpace bought it.
He made a shopping comparison website also snatched up.
Another startup was bought by Reuters.
So, Oren gave 12,000 price observations grabbed by his computer programs from a travel website
over 41 days.
He ended up with something that could save customers money and not just by comparing
current prices.
It didn't know why airlines were pricing the way they did, but it could help predict whether
fares were more likely to go up or down in the near future.
When it became a venture capital-backed startup called Faircast, it began crunching 200 billion
flight price records.
Then, Microsoft bought it in 2008 for $110 million and integrated it into the Bing search
engine.
What made it possible to predict future fares?
Data and lots of it.
How big and what's big enough depends in part on what you're asking and how much data you
can handle.
And you can consider how you can approach the questions.
UPS can't look for the optimal answer, but they can save millions of dollars finding
much better answers.
Again, they can do this by asking questions only answerable with the data that is streaming
in and available in today's data explosion.
What other misconceptions can there be about data analysis?
Let's clear up some misconceptions I encountered about my work in the field.
In 2014, my work in sports ranking and bracketology received considerable national attention with
coverage by the New York Times, CBS Evening News, and USA Today, among quite a few others.
In the process of the interviews, I noticed several common questions that reflect certain
misconceptions about data analysis.
So let's talk about these.
We'll see and discuss them more in coming lectures.
But in order to get the big idea of data analysis, let's be sure we build that understanding
that can help us have a solid foundation.
First, data analysis gives you an answer, not the answer.
With March Madness predictions for college basketball, people often thought the work
would give everyone the same answer.
That could be true in a sense if I posted a definitive Tim Chartier bracket that everyone
used.
But even if we all correctly predicted the same result for one year, we might not do
so well the next year.
More likely is that a model might perform better than 90, 95, or even 99 percent of
millions of other brackets.
But that doesn't mean they predicted perfectly.
Data analysis generally cannot do that.
Instead, it might predict better than we usually can without it.
This is very, very important to remember.
So as you will see throughout this course, I teach you the ideas that enable you to see
that there is more than one answer.
Much of life is too random and chaotic to capture everything.
But it's more than that.
Unlike math you may have studied in school, data analytics does not get rid of all the
messiness.
So you create an answer anyway and try to glean what truths and insight it offers.
It's not the only answer.
The style in data analytics is more back of the envelope.
Even though we do have also lots of tools and technology, that is, rather than pages
of detailed proofs, you might gain enough insight from a handful of computations, something
that might fit on the back of an envelope.
This makes it easier to get started, and it also makes it a lot of fun.
Second, data analytics does involve your intuition as a data analyst.
You are not simply crunchy numbers.
If you build a model and create results that go against anything anyone previously has
found, everything, it is likely that your model has an error.
My data group at Davidson experienced this.
We built a model that initially made complete sense to us for sports ranking.
We were proud of the result and excited to see what results it would produce.
But then we ran it.
We didn't recognize any of the top teams.
What happened?
We couldn't just trust the results and leave it there.
Instead, we trusted our intuition and doubted the work.
So we looked carefully, very carefully, trying to see what happened.
Soon, we were able to show that we created a method where it was to your advantage to
lose.
So our method was giving high ranks to very, very weak teams that lost a lot.
So everything fell apart, and it no longer worked.
How did we figure this out?
Our intuition told us that we were seeing results that simply didn't make sense.
They simply didn't fit.
But the data analysis wouldn't know.
It will create an answer.
It's like an opinion, and our method was creating a very sketchy opinion at best.
Third, there is no single best tool or method.
In fact, many times, part of the art and science of data analysis is figuring out what method
to use.
And sometimes you just don't know.
But we'll learn there are some methods that are important to try before others.
They may or may not work, and sometimes you simply won't know, but you can learn things
about your data in viable paths to a solution by trying those methods.
It can be hard, and sometimes the needle you're looking for in the haystack of data may simply
not be there.
Other times it is there, but you have to have the right method to sift through the hay.
Fourth, you do not always have the data you need in the way you need it.
Just having the data is not enough.
Sometimes you have the data, but it may not be in the form you need to process it.
It may have errors, be incomplete, or be composed of different data sets that have to be merged.
And sometimes just getting data into the right format is a big deal.
I worked on a project once in which a one gigabyte data set was not in the form that
my computer algorithm needed.
It was non-trivial to simply reformat it.
I had student researchers trying to write codes, but they were writing codes and that
just took too much time.
They would take something like two months or more to reformat the data with the codes
that they wrote.
This would mean literally hitting the energy on a computer and not using it for other things
for several months.
Fortunately, I found another code that already existed.
It was designed for a completely different purpose, but if I used it on the data I had,
I could get my data into the form I needed in about 20 minutes.
So I ran that code just to reformat my data with a very, very fast algorithm.
Then I needed yet another program to get the reformatted data exactly ready so that my
final program could solve what we were interested in.
In the end, it took two years to figure out all those steps, but in the end, we produced
a method that could produce new medical imagery software.
The main step was literally getting the data I had into the form I needed.
This isn't uncommon in data analysis, especially on very large data sets.
Fifth, not all data is equally available.
True, some data sets are easy to find.
They already exist on the internet.
You can download them and immediately begin analyzing the data.
You can easily download sports data on players and teams, for example.
But other pieces of data, like the score during each game from one minute to one minute, may
not be as easily available.
It doesn't mean you can't get it.
It's out there, but you may need to figure out how to grab it.
Many times, there is too much data to grab by hand.
So unless a vendor or website offers the data, you have to write computer programs to grab
it off the internet.
And that isn't always easy to do, even if you're a computer programmer.
Sixth and lastly, at least for now, while an insight or approach may add value, it may
not add enough value.
Several years ago, along with my collaborator Amy Langville of the College of Charleston,
we developed a ranking method applicable to Twitter.
If you searched on a term like data analysis, it wouldn't just return tweets related to
that term.
It would sort them according to the influence or potential impact of the person who wrote
the tweet.
This is similar to how Google ranks webpages, something we'll talk about in a later lecture
on search engines.
A big company that does Twitter searches was very interested in our idea.
They commented that it would allow new value in better results.
But then they also said, we'll have to see if this would allow enough value to our customers
to justify the research and development resources this would demand.
Not every new and interesting insight is worth the time or effort needed to integrate it
into existing work.
And no insight is totally new.
If everything is new, then something is probably wrong.
We'll talk about other misconceptions throughout the course.
But these are pitfalls that can obscure the big idea of data analysis.
What makes data analysis so powerful are fundamental techniques we'll learn for looking at datasets
large or small.
For example, a technique we'll learn later called the singular value decomposition makes
it possible for recommendation systems to find products I'm not even aware I might like.
Creation of new but very simple analytics in sports helps determine when to draft or
trade a player and how to play.
Analytics to combine data from many political polls has changed political campaigns and
can affect who wins.
Some fundamental concepts inform everything we do in data analytics.
How do we test your data?
How do we look for patterns in data without imposing patterns that aren't really there?
There is a lot to discuss and to consider.
Exactly.
That's the nature of data analysis.
We have a set of data.
From it, we learn and gain insight.
Some things will be obvious and not very insightful or new, but then we see the unexpected.
For a business that can catapult it in front of its competitors, we'll see how looking
at the web as a network did this for Google.
It's a new world and one that data is continuing to change.
The trick is we don't entirely know how or where.
But data analysis enables us to predict with better accuracy and higher probability many
aspects of the future.
In the end, data analysis is a set of tools, existing and ever-developing, from regression
and partitioning to clustering and ranking.
But data analysis is also a mindset.
It's a way of improving our ability to ask questions and an expectation that data can
make possible new answers.
And much more than in the past when data sets were smaller, the mindset is also fueled by
simple curiosity about seeing the big picture and about being able to zoom in on any of
the details.
This is the world of data analytics, and it's also the world we live in every day.
Numbers surround us from sports scores to checkbooks.
Statistics fill the news from politics to education.
We will learn the tools to understand how data analysis is done, to check and do it yourself,
and we will see firsthand how much of an art and a science this is.
There is no one answer.
So be ready to get the data analytics itch.
Like an artist who keeps tinkering with a painting, you'll learn to see that the world
is your palette.
Analytics lets you sift through data to find information of value and paint a picture that
was always just a mathematical step away, and that's just one picture, only one way
of looking at the data, which can lead you to begin wondering, what can you do next?
Rather than reaching for something like a crossword puzzle in your free time, you may
find yourself reaching for data that you can begin tinkering with yourself.
It's a great way to exercise your mind.
Again, there isn't one answer, so each question is like a puzzle that could be played anew,
each time you look at it a new way.
And as I tell my students, if you find yourself visiting webpages of news sources just to
download their data, you may even end up creating your own news.
Data analytics are transforming our world in ways you can understand and appreciate.
It's like a spectator sport that's looking into all areas of life and inviting you to
participate.
