We are reducing the dimensions of our data.
And yet this image looks pretty much like the original.
If you look closely, you're more likely to spot the image that's compressed.
But note, this compressed image only takes 17.5% of the storage.
And if you have an image using only 50% of the original, it is very, very hard to spot the differences.
This gives us a sense of how image compression can be done.
Compressed too much and you get a fuzzier picture than the original.
If you compress less, it can be very difficult to see a difference between the original and the approximation.
Now you can't zoom into the compressed picture as much.
But you can look at a lot more images before deciding which ones might be of interest.
Now, what we've done with image compression can be done with any array of numbered data.
In image compression, we identified significant components of the data and represented the larger data by a smaller set.
We can do essentially the same thing with non-visual data.
So let's represent our recommendation data in compressed form and use that to do our analysis.
Suppose we have Sam, Mel, Mike and Fred recommending six movies.
Here are their ratings.
We make this table of data a matrix with six rows and four columns.
Then we produce the SVD.
Again, U has six rows and six columns.
Sigma has six rows and six columns and V has four rows and four columns.
Then we reduce this to a two-dimensional problem by taking only the first two columns of U, two singular values and two rows of V.
Now we can treat the first row of V as coordinates for Sam.
So we can plot Sam's position as the point negative 0.5710 and negative 0.2228.
We do the same thing for Mel, Mike and Fred.
Remember, these points are consolidating all their movie ratings into two numbers each.
This is the same as compressing an image down into an image with much less storage.
The difference is that we've not only got lower resolution in our data,
we actually make it easier for our recommendation system to work.
Now, who is the most similar? Fred and Sam.
Let's go back and look at the data itself.
