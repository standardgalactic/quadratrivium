All data is not created equal.
There can be errors and ambiguities in data, and we must determine what to include or exclude.
We need what is called data preparation.
In addition, all data does not have or should not have the same purpose.
That's because we often need to peel off part of the data to help us figure out how predictive
or effective our analysis is.
We divide our information into training data, which we use to first build our analysis,
and then test data, which we use afterward to check.
So in this lecture, we talk about these important matters of data preparation and data training.
Which play a fundamental role in the success of our data analysis.
Let's turn to an application that works well thanks to heavy use of data preparation.
The delivery of physical mail, not email, but regular mail.
While we do send a lot of correspondence via email, a great deal of mail is still sent,
especially during the holiday season.
Let's get a sense of the scale.
The United States Postal Service delivers more mail to more destinations than any other service in the world.
In 2012, they delivered over 160 billion pieces of mail to more than 152 million homes, businesses,
and post office boxes in every state, city, town, and borough in the United States.
And children still send letters to the North Pole.
In fact, the Postal Service's Letters to Santa program dates from 1912, when Postmaster General Frank Hitchcock
authorized postal employees and citizens to respond to letters to Santa.
New York City has its own Operation Santa program, where thousands of volunteers work with postal offices
to respond to more than half a million letters from children of all ages each year.
Timely delivery has been a goal of every postal system throughout history.
The U.S. Postal Service's famous motto came from a translation describing the ancient Persian system
of mounted postal carriers from around 500 BCE, as reported by the Greek historian Herodotus.
Neither snow nor rain nor heat nor gloom of night stays these carriers from the swift completion of their appointed rounds.
So now we have modern methods to help with this ancient goal.
An important part of getting the mail delivered is knowing where it goes.
Think about it.
You can write a letter to anywhere, and by putting the appropriate address on it, it's sent.
It's how all those letters are processed, that's the key.
And a main part of this is reading the addresses.
Today, much of the process is automated.
A computer reads the address, in particular the zip code.
How is this done?
Addresses are often handwritten, and there are lots of different typefaces.
So this looks like a hard problem.
So how do we even tackle this?
We need data, lots of it.
In particular, we will want a lot of handwritten digits.
For this, we'll turn to the Mixed National Institute of Standards and Technology Database.
We will use the set of 60,000 digits.
For instance, here we see four different examples of the number three from the data set.
Can we get a computer to automatically recognize that each of these is the digit three?
Let's develop a simple method that uses averages.
So you can write the number three.
What we'll do is we'll take that number and we'll convert it into a grayscale image.
In order to compare things, I'll make it a small image that's 28 by 28 pixels.
That's pretty small.
This corresponds to having a table with 28 rows and 28 columns.
That equates to 784 cells in all.
Each element of that table also contains a grayscale color that corresponds to a pixel
in the image.
0 is black and 255 is white, and all the numbers from 1 to 254 are all the shades in between.
I'm going to take that square table and convert it into a long table with one column.
So I'll have a table with 784 rows.
Now I'm going to find an average grayscale value for the number 0, 1, 2, and so forth.
Let's do it for the number zero first.
There are about 6,000 images of the number zero in our database.
For each of those zeroes, again there are close to 6,000, I have a table with one column
and 784 rows.
Now I take every element in the first row of every table and take the average of those
numbers.
I'm finding the average value among all the tables for that first pixel of the number
zero.
Pixel is short for picture element.
I'll do the same thing for every other element of zero.
Put together the average values for the 784 elements, tell me what an average zero will
look like.
For example, here I have 5 handwritten 3s taken from the dataset.
Then I calculate the average of each pixel over all 5 samples.
This produces the average 3 over the samples.
Here's what I get.
I do this for every number over all the samples in my database.
Here are the numbers from zero to nine.
Now I take your number and want to find which of these numbers it looks most like.
How?
I again will work with a simple idea.
We can easily find the distance between two points in the XY plane.
A point in two dimensions can be seen as a vector with one column and two rows.
We have a vector with one column and over 700 rows.
The same formula can be extended to the distance between vectors of that size too.
So we'll do that.
In particular, the smallest distance wins.
So I can take your handwritten 3, a new 3, not already in my dataset, and see which of
our average digits it looks most like by using distance.
Here we are using average pixel values to compute distances to the pixel values of your
handwritten sample.
Smallest distance across all the pixels tells us which average number is closest.
How did we do?
Remember the handwritten 3s we saw earlier?
I know these are 3s.
So this is the basic idea and it usually works out well, but more complicated algorithms
for digit recognition also exist for a reason.
Sometimes it is hard to recognize a handwritten digit.
Sometimes we write fast and some of us frankly have bad, really bad handwriting.
Let's see a digit our method won't recognize.
This is meant to be a 3, but the method we just created classifies it as a 5.
Do you see why?
There are elements of the digit 5 that are within it too.
So maybe this is good enough and maybe it isn't, but really we've only tested one number.
We need to get a better handle on it.
Wait, we have 60,000 numbers.
We do, but we just use those to create the method.
We need to use another set of data.
Indeed, we need to test our method on data it hasn't seen.
That's why it would be great to have you write some numbers.
That would be unknown to my method and test how it does.
What I'm describing is a very important part of data analysis.
That big set of 60,000 digits is called the training set.
This is the data we develop our idea on.
Then we need to test our idea.
But again, it often doesn't work to test the set I already had.
So I turn to another set of data.
This is called the test set.
The data we are using is separated in order to be used this way.
We were using the training set.
There is another set of 10,000 handwritten digits we initially set aside.
That is the test set.
It is very possible to design a method that does great on the training set, but fails
on the test set.
We'll talk about that more in a bit.
At one level, it sounds simple enough to have two sets of data.
But really, is it that simple?
Not always.
And the issue can be quite subtle.
In my own research, I remember my research group struggling for several months over
whether our test set really was suitably testing our method.
What did we decide?
Ironically, we ended up thinking of a more interesting application of our work and moved
in another direction without ever resolving that particular question.
Sometimes, it's that hard.
The issue of really testing an idea can be quite subtle, and it can become quite troublesome
when not done correctly.
In fact, a poor training set can doom data analytics.
There are a number of issues to consider.
First, the training set must cover the full range of values that the problem might present.
Suppose you're creating a method to predict housing prices.
You'd want expensive, inexpensive, big, and small one-story and two-story houses with
and without garages.
The more features that exist, the larger your training set should be.
There isn't, though, an easy way to know how large the set should be.
Do you want dozens, if not hundreds, or thousands of examples of each feature?
So let's return to an example doing really well on the training set, but not well on
the test set.
This is because your method has essentially memorized the training set.
This is like me knowing all the students in two classes at Davidson College, which is
maybe 60 students, and my saying that I then know the names of all students at the college.
Now an issue in training in test sets is that they often come from the same set of data.
You have access to only one set of data.
From it, you want to create a training and a test set.
Ideally, the training set would be representative of the whole set.
In the same way, without being identical, the test set would also be representative.
But you probably don't know in advance what exactly representative means, especially on
really large data sets.
So how do we do this?
Let's assume we have our data in a table.
Each row represents a person's ratings of movies.
We can create the training set by selecting random rows, like 80% of the data set.
The remaining rows are the test set.
Did it work?
Again, we probably don't know, but things were created at random, so we can simply do
it again.
In this way, we can create a few training and test sets and see how we do.
This again tests the robustness of our method.
As long as we keep any given training set entirely distinct from its corresponding test set,
we're okay.
Keep in mind, this assumes that your data is ordered randomly or at least placed randomly
into training and test sets.
If that's not the case, if training data differs from test data in some way, then this approach
might not work.
Clearly, there is subtlety here, and what we do in those cases is still an area of active
research.
In many ways, this shouldn't be all that surprising.
For example, we have been trying to create representative samples in political polling
for years.
We have, at various times, created successful ways to do this, but when the technological
landscape changes or the issues change or the nature of the population changes, we can
need new ways to specify our training and test data, too.
Note, putting data into training and test sets is done prior to creating methods that
analyze them.
If the goal is to predict some phenomenon, then we process the data in this way prior
to creating our method.
This isn't the only method of pre-processing.
What else might we do?
Why should we?
Data can be what is called dirty.
If data is dirty, how can you trust your results?
Data can be incomplete, noisy, and inconsistent.
This can be due to human error, limitations of measuring devices or flaws in how the data
was collected.
It can be that nothing really went wrong, but the data isn't necessarily in the assumed
form.
For example, one person could have two entries in the data due to changing addresses.
It could be important not to double-count the individual.
However, if we end up with that data that isn't descriptive, it can essentially be
garbage.
If this is the case, how can you expect meaningful results on the other side?
This leads to saying garbage in, garbage out.
This is a very common expression in computing.
One of its first uses was in a 1963 syndicated newspaper article about the first stages of
the computerization of the IRS.
Even though the expression became prominent in programming a few decades ago, the principle
was not new even then.
Consider the following quote of Charles Babbage, the English inventor of a design for the first
programmable computing device.
So, on two occasions, I have been asked, pray Mr. Babbage, if you put into the machine wrong
figures will the right answers come out?
Babbage continued, I am not able rightly to apprehend the kind of confusion of ideas that
could provoke such a question.
So, what does garbage look like?
How dirty is it?
Well, it may not look all that dirty to us.
Consider this simple question, are Bobby Miller from Charlotte, North Carolina and Bob Miller
also from Charlotte, North Carolina the same person?
You and I could guess it's the same person, but a computer would treat these as distinct
people.
These issues can be real.
Conversely, there can be two addresses for one person.
I grew up in a very small town outside Philadelphia.
It had around a thousand people total.
If someone asks where I grew up, I usually start with the town of about ten thousand
I grew up near, King of Pressure.
Heard of it?
Some have, it has a huge, huge mall.
If you know it, I can mention a smaller town near mine that is about three times the size
of my town.
Now often came to our home, not address to our town, but to King of Pressure.
As such, we were listed in some databases as living in King of Pressure.
At some point, it would be easy for such a database to have my family being from two
or even three different towns.
Easy enough.
But a computer without help from specialized software could easily see such entries as
being two or more separate people.
Maybe not a big deal by itself, but when merging corrupt or erroneous data into multiple databases,
the problem may be multiplied by the millions.
What's the point in having a comprehensive database if that database is filled with errors
in disputed information?
One way to deal with this is to buy software to clean up the data.
This isn't always all that easy, as can be reflected in the price of such software.
A general tool powerful enough to address your particular issues can cost between $20,000
and $300,000.
What is it fixing?
What could be going wrong?
One issue is duplicate data, like my family being listed twice.
A variety of issues can occur, and how to deal with them isn't always all that obvious.
To ensure this doesn't happen, data analysts generally look at the data to ensure it makes
sense.
To see how easily variation from different data sources can introduce small but serious
inconsistencies, consider a few cases.
We're postal codes recorded using a uniform format.
In the U.S. that means, do the zip codes all use the same five-digit format?
If addresses are encoded with something like bar codes, does the code have enough flexibility
to describe every type of address?
Are all dates the same?
If you have data coming from the U.S. and from Europe, they likely are not.
This is because we write dates as month-day-year.
However, in Europe, dates are often written day-month-year.
And other data sources may put the year first, a day like 12-12-12, can be ripe for confusion.
Is data in the same units?
If you're looking at currency, is the data global, and if so, are they all reporting
in their native currency or in some common currency?
Is everyone reporting values with the same accuracy?
If one source rounded to one decimal place and another to three decimal places, that
can be a problem.
If this data point is in millions of dollars, then the first and second decimal places represent
potentially significant amounts of money.
Is some data missing?
This is not unusual.
The data might not have been collected.
Maybe someone declined to report their age or weight.
It may be that some data isn't applicable.
For example, not everyone has a middle name.
Regardless of why the data is missing, what will you do when it isn't present?
There are several options.
First, you can eliminate data objects that have missing elements.
But then, you are throwing out data, and if the data is missing from an important subset,
you might be removing most or all of that set.
You could also delete an attribute that has missing values.
For example, maybe you no longer include middle names or ages.
Again, this removes data and should be done with caution.
Sometimes you can estimate a value.
Suppose you're looking at precipitation measures.
What if one collection device doesn't report a value on a given day?
Rather than remove that entire location from the data, you might be able to estimate that
value given certain surrounding areas amounts and their general ratio in location with the
missing value.
Or sometimes you can simply ignore that value is missing.
This depends on the analysis and the type of method being derived.
Let's look at another common attribute.
We can have inconsistent values.
Reading an address, maybe someone has a zip code that isn't contained in the listed city.
It could be that the person transposed two digits.
Sometimes this is easy to check.
Heights can't be negative in adult ages, shouldn't be single digits.
A zip code in California should start with a 9, not a 0.
Other times possible mistakes are not easy to check and can affect the results.
To see how unexpected errors can be introduced by new technology, let's look at weather data.
Data was collected for sea surface temperatures.
Originally they were collected from ships and buoys.
Technology changed and the data began to be collected by satellites.
With long-term behavior is needed in a study, data from both sources must be collected.
The data is studied and they are found statistically to be different.
It doesn't mean that they can't use the data together, but one would need to realize that
ship and buoy data from 1965 should be treated differently than satellite data from 1990.
To aid in dealing with these issues, data is often pre-processed or made clean, or at
least clean enough.
Data pre-processing comes in two main forms.
One, selecting data objects and attributes for analysis, and two, creating or combining
attributes to create new attributes more suitable for analysis.
In selecting data objects, you might sample the population or look at only a subset of
the available features of your data.
Other times, you might decide you need to add an attribute, some data you hadn't collected
before.
Other times, you combined data.
For example, in basketball, sports analysts often look at what percentage of your possessions
you have turnovers, rather than simply how many total turnovers you might have.
In his 2012 book, Best Practices in Data Cleaning, Jason W. Osborne stated that recent surveys
of top research journals in the social sciences reveal that many academic authors are not
suitably concerned about dirty data.
A prominent study of 17 educational and psychology journals by Kesselman and colleagues in 1998
found that often 10 percent or less of the authors were looking at issues of dealing
with dirty data.
A later study from 2008 showed, again, that authors were not indicating sensitivity to
issues in cleaning data.
For example, only 26 percent reported on the reliability of data being analyzed.
The main problem is that most statistical tests may not be robust or reliable with dirty
data, at least to the degree that researchers might hope.
It's the same confused thinking that Charles Babbage reported.
If you put into the machine wrong figures, will write answers come out?
No.
But even if you've done everything right initially, similar problems can pop up later.
To demonstrate how things can quickly and almost unexpectedly go astray, I'll close
with an example from my own work.
I've conducted research in sports ranking.
Given the scores of games, how should teams be ranked, in particular to predict the outcomes
of future games?
I've spoken to various audiences around the United States about this work.
One of my favorite things is to show an audience how to create their own ranking method.
They're learning how we do it, live, during the talk.
If I have enough time, I even download data right then, so we're using a ranking method
created right then by the group with data that is as current as possible.
I've done this many times.
It's fun for the audience and for me, so I was really looking forward to this when I
spoke to a group of New York City public school teachers.
There we were on a cold December Saturday morning.
We developed a method, decided on parameters that customized it for our personal ranking
system, and then I downloaded the data.
Now, we get to see how we've done, I said.
I pressed the rank button and the results were returned.
One of the worst teams, almost unknown, was ranked first.
Honestly, I'd never seen the method do that poorly.
Trying to recover, I said, well, let's compare this to the method that's usually, that we
usually use, that does well.
I put in those parameters.
Again, that same weak team was at the top.
This analysis was garbage, but more importantly, in front of this audience, I didn't know why.
Later, I looked closely at the data.
It had changed because the source of the data had changed.
The spot that had previously been for scores were now indicating whether games were home
or away for the teams.
So I was now ranking teams where the home team always won by two points.
So the team on top, that was a team that had more home games than anyone else.
They were ranked highest.
Pretty soon, I was able to change my computer program.
I could have changed the data, but since I'd be downloading it in the future, I changed
the program that read the data instead.
But what if the data was much, much larger?
What if you weren't doing something where you could see the problem in the results?
What if you simply didn't know what to expect in the results?
There are many other issues that can cause inconsistencies, changes, missing values in
other problems in data, creating results that are truly garbage.
Remember the other problem.
We have to create results that work on even good, clean data.
But our goal is generally not to simply do well on existing data.
That can lead to a problem called overfitting, which we'll discuss in a later lecture.
We want to create a method that can use that data to perform well, but also offer insight
into data that is yet to come.
Data analytics takes data, but how it takes the data matters too.
Working with the data can be and often is a science in itself.
Just remember to prepare your data.
And after you prepare your data, do your analysis with a training set, just a portion
of the data, leaving the rest of the data to test and validate your conclusions.
I call this training for success.
