Have a question?
Search engines like Google play a huge role in our world.
The importance of search engines also makes them big business.
So in this lecture, we learn the data analytics of search engines.
Let's get a sense of all this with a story from December 2010.
Suppose you're interested in buying a dress for a friend.
You decide to try some online shopping.
So you go to Google and type the word dresses into the search engine.
Then you decide to look at betting and put the word into Google.
Finally, you search on area rugs.
Wait, you pause.
I mean really pause.
Why?
One company, one, was at the top of each of those lists.
Think of the options for a moment of all the retailers of these products.
I quickly think of Belk, Macy's, Amazon, Nordstrom, Walmart, or
in terms of web pages, even a Wikipedia entry on the item.
Nope, one company topped each list, whether it be dresses, betting, area rugs.
So you start inputting more queries, skinny jeans, home decor,
comforter sets, furniture, tablecloths, grommet top curtains.
Same company on the top.
Who?
JC Penney's.
Now Penney's is a giant in American retailing.
They have over 1,000 stores and had over $17 billion in revenue in 2010.
Still, couldn't we find a query where they weren't beating millions of other retailer sites?
Okay, how about Samsonite Carry On Luggage?
There you go, type it in and indeed JC Penney.
Yes, JC Penney bested Samsonite.com on a search for Samsonite luggage.
What you probably wouldn't have known in all your web searching is that this had been true for months.
This steady performance caught the eye of the New York Times.
And they wanted an expert in online search to take a closer look.
So they contacted Doug Pierce of Blue Fountain Media.
Pierce's research suggested that Penney's had used knowledge of Google's underlying algorithm
to raise the profile of their website on all such queries.
Before we proceed, it is important to know that this isn't illegal.
But such companies risk being punished by Google because they don't like it.
We'll talk about that more a little later.
For now, let's concentrate on Google's algorithm.
How can it possibly be gained?
There are literally hundreds of billions of web pages.
Yet even hundreds of billions can be hard to get a handle on.
How big is that?
Let's put it this way.
If every person on earth was given 100 web pages, we'd still have billions of web pages left over.
The internet is that big.
So again, how can someone possibly manipulate a system of that size?
The key is knowing how Google analyzes the system to form its search results.
We're going to concentrate on the PageRank algorithm.
This method quantifies the quality of a web page.
The quality of a web page helps determine which is listed earlier in the results from a search engine.
Let's simplify this and consider only two web pages.
If they're equally relevant to a query that you input, the page with the higher quality is listed first.
The ability to quantify quality of web pages is part of what allowed a once struggling internet company, Google,
to overtake their competitors and become the most visited website in the world.
The founders of Google were Sergey Brin and Larry Page.
They were graduate students in computer science at the time they started creating what became Google.
When Google's algorithm was unveiled to the world, there was Google and its new algorithm and everyone else.
The internet before Google was a rather tangled mess.
Millions of web pages existed and the other search engines weren't as helpful.
You could go to a search engine, input your query, and just like today get a list of web pages.
But at that time, the web pages at the top of the list often weren't very useful.
It wasn't uncommon for a much better page to appear farther down the list.
This almost forced you to look at several pages of search results.
In other words, you might find a gem of a web page, but you weren't guaranteed at all that it would be at the top of the list.
How often do you click through pages of search results looking for a good one?
Most people don't offer it don't very often.
The Wall Street Journal reported that 39% of web users look only at the first search result returned from a search engine.
And 29% view only a few results.
That's why a company like Pennies wants to be at the top of that list and other companies were formed to help do that.
The way Google analyzed the worldwide web changed everything.
So, what did Google do that was different?
They looked at the connectedness of the web.
This was part of their billion-dollar idea.
They didn't just look at the content of web pages.
Google also looked at the structure of the web.
Who linked to who?
A fundamental idea in Google's search engine is the concept of endorsement.
Quality pages will tend to link to other quality pages.
To get a sense of how this works, let's consider for a moment endorsement in a job search.
Suppose you get down to your final two applicants.
One person has one recommendation, the other person has three.
Who do you hire?
Be careful, the number of recommendations may not be a determining factor.
Suppose you're creating a computer lab that will run Windows.
One applicant has a sterling recommendation from Bill Gates.
The other applicant has three recommendations from his elementary school children.
Who would you hire now?
What made the difference?
Why did your answer possibly change when I spoke of one versus three recommendations?
The issue was the quality of the recommendation.
The quality of a recommendation from Bill Gates is much different than elementary school children
when it comes to running a computer lab.
Suppose now that a recommendation comes from a former employer.
How do you know the quality of that recommendation?
One way is to get recommendations for that person.
But you may not, in fact, probably won't know their quality.
So you suddenly need recommendations from everyone at the same time.
What a tangled mess.
Seemingly, but not to bring in page.
This interconnectivity is exactly what they exploit.
First, let's look at a graph of recommendations between people.
The circles, called vertices, represent people.
I've drawn an arrow called an edge when one person recommends another.
As we see, we need to know everyone's quality to determine anyone's quality.
How does this translate into web pages?
An easy way to see it is by representing the web as a graph.
The vertices of the graph represent web pages,
and the edges are drawn from one web page to another
when a link appears on one web page to another.
There is an edge from vertex one to vertex four.
So web page one has a link to web page four,
which means if I were on web page one,
I'd find a link and be able to click it,
and it would take me to web page four.
A web page linking to another can be seen as a recommendation.
Web page one recommends web page four.
Again, the question is, what's the best web page?
While not answering the question,
an important insight is that higher quality web pages,
again, will tend to be linked by other high quality web pages.
I tend to think about it like high school popularity.
How cool are you?
It depends in part on who thinks you're cool.
If the coolest kids think you're cool,
then your cool factor just went up.
So how did Brynn and Page determine all this?
Their model can be seen as recommendations,
but also as a model of surfing the internet.
It's a model so it won't exactly replicate
how people surf the web.
In fact, it's entirely possible that no one
surfs the web according to the rules we're about to discuss.
Again, it's a model.
It doesn't have to be exact.
It only has to capture enough of the characteristic behavior
that it returns meaningful and reflective results.
So let's learn the model.
Since we are all different,
the model assumes surfing is random.
So a random surfer moves through the web
following very simple rules.
You can think of it almost as a game.
At each web page, the surfer rolls a die
to decide what web page to go to next.
Which link should the surfer follow?
You and I have different preferences
on which link to click.
So Google assumes you pick a link randomly on a web page.
So let's return to the small network we had earlier.
If we were on web page one,
then no matter what, we end up at web page four
as it is our only option.
Now at web page four, we have a 50-50 chance
of going to web page two or web page five.
But how does this produce a measure
of the quality of a web page?
Brinn and Page calculate the probability
of the random surfer being at any given web page
if this model is followed.
That probability is associated with the measure of quality.
If web page one's quality measure equals 0.07
and web page two equals 0.05,
then about 7% of the time,
you expect the random surfer to be at web page one.
And at web page two, about 5% of the time.
If these two web pages are both equally relevant
to the text you type into the search engine,
then web page one would be listed before web page two.
Remember, if web page one has nothing to do with your query,
then it will not appear in the search results.
It may be of high quality,
but again, it must also be related
to the content of your search.
You can try surfing the network you see on the screen
to determine which is the highest quality web page
under this model.
For web pages that have two links, just flip a coin.
Remember, if you have one link,
you are guaranteed to follow that link.
So, what do we do now that the model is set up
and we have the network of web pages?
You simply randomly surf the network following these rules,
but you must do it long enough that the results settle down.
For example, if I surf the network for 20 steps
and found web page four was visited the most,
well, then I surf the network another 20 times,
but this time I find web page one was visited the most.
So, which web page has the higher quality under this model?
We don't know yet.
We need to surf for a much, much longer time.
If you surf the network for 10,000 steps,
you would find web page one is visited the most.
This will most certainly be true
if you follow the network for 100,000 steps.
But how does Google know this is going to work?
Earlier, I found web page four was the highest quality
and then I found web page one if I surfed more.
I just said you most certainly would find web page one
had the highest quality if you surfed a million steps.
How did I know that?
Wait a minute.
How do I know that for any configuration of the web,
this idea is actually going to work
that eventually the numbers will settle down?
Could things simply end up being chaotic?
In different web pages are visited most
if you surf one million times versus two million times.
A nice feature of Brandon Page's algorithm
is that it will converge for any network,
anything at all, ever created.
We'll talk about this more in a bit.
We need to learn one more piece of the page rank puzzle.
For this, let's surf under this model on another network.
How about this one?
Now, what would you find?
Feel free to try it.
You may simply by looking be able to identify
that web page six causes a problem, a big problem.
There are no links from that web page.
Such web pages are called dangling nodes.
If you ended up at a web page with no links on it,
what would you do?
PDF files, movie files and music files
are often dangling nodes.
Well, if you're out that type of page,
you either input a new web address or hit the back button.
Currently, our model assumes you're stuck.
It's almost as if you give up
and simply close the browser as if there's nowhere to go.
We don't do that.
It's like we hit a PDF file and think,
oh my, I must close my browser
and start surfing all over again.
I hate it when this happens.
That's silly, it's ridiculous.
So Google, of course, assumes we go somewhere else.
Again, they need a model that represents what we do.
So where do we go?
Again, that is assumed to be a random choice.
You go anywhere on the internet with equal probability.
This relates to the final aspect of the algorithm.
We don't always follow links on web pages.
Sometimes we simply decide to go somewhere else.
Brennan Page called this teleporting.
You teleport either when you are at a web page,
possibly with outlinks,
or you teleport because you're at a web page
with no links from it.
How do we know when to do what?
You can again think of it as a game.
At each web page, you again roll a die.
If it comes up with one through five,
the surfer will click a link on the current web page
and follow that link to a new web page.
Each link, like before, is equally likely to be chosen.
But now, if the die comes up six,
the surfer will now go to any web page on the internet.
Again, each web page is equally likely to be chosen.
Finally, if you end up at a dangling node,
you teleport to any web page
with again all choices equally likely.
That's the model.
Yes, that's the model Brennan Page created
that started Google.
There is only one difference.
Their model assumed that you were 85% likely
to follow a link on a web page.
By rolling a die, we made that 83% likely.
Let's reflect on this model a moment.
Do you surf like this?
I don't.
I'm not equally likely to go to any web page.
It's estimated that there are more
than a trillion web pages.
The subset I would visit or want to visit
is very, very small relative to this size.
I'm not even sure that I follow links
on web pages 80% of the time.
Then how did Brennan Page know this model
would lead to good search results?
That's a very good question.
My guess is this was exactly the question
that other search engines asked about their algorithm.
Blue Chip venture capital firms like Yahoo,
Alta Vista, and many other major companies
were approached by Stanford with Brennan Page's algorithm.
They turned down the chance to buy Google search engine
system for $1 million.
David Filo, a Stanford alum and founder of Yahoo,
encouraged the pair to start their own company
and return when the product was fully developed.
So the two left their doctoral programs
and moved their computers into the garage of a friend.
So just to reiterate, in 1998, Brennan Page
were dropouts working out of a garage.
Before long, their company, Google,
became not just a business, but a verb
with people saying, I'll Google that.
By integrating the interconnectivity of web pages,
Brennan Page folded in a very important piece
of information that set their search engine apart.
By the way, that search system that
was on sale for $1 million in the 1990s
became the foundation of the Google company.
And by the summer of 2005, each of the founders
had a net worth of more than $10 billion.
Now let's return to our earlier question
as to how Google knows the algorithm will stabilize
or converge given enough time.
No matter how long we surf, could the number one site
change if we double the number of steps we've surfed?
Here we go a bit deeper with the math.
There is a math theorem called Perron's theorem
that guarantees that Google will always,
yes, always find a unique answer for any configuration
of the web.
How?
It turns out teleportation enables this.
Why?
If there is some non-zero probability
of surfing from any web page to any other web page,
then we are guaranteed to find an answer.
And for that answer to be unique,
we must have those non-zero probabilities for Perron's
theorem to hold.
We do, and bam, we have our unique answer,
guaranteed regardless of any configuration of the web.
That is a huge result.
And teleportation guaranteed that this happens.
The probability may be quite small.
But remember, it's non-zero, and that's all we need.
This tiny aspect of Google's model
guarantees the algorithm will always, always work.
Brinn and Page developed a powerful model
that created the company Google.
But with such prominence also comes much attention.
In particular, companies want to be at the top of web searches.
JC Penney did this, even for a search on Samsonite luggage.
We are now ready to have a sense of how this can be done.
Understanding PageRank can help raise a web page's spot
in a list of search engine results.
How?
Again, think back to endorsement.
And we can put it in the analogy that we had for high school.
Not feeling cool?
Get the super cool kid to say you're cool,
and your coolness shoots up.
If you can get web pages with high PageRank to link to yours,
then your PageRank rises.
So some companies go into business offering to help raise your PageRank.
One way is by developing pages with high PageRank.
Then, for a fee, they link to yours.
Link farms are created by what are called spammers
to essentially full search engines like Google
and raise the rank of a website.
Generally, a link farm has several interconnected websites
about a popular topic and with significant PageRanks.
The interconnected nodes then link to a client's page.
When Google figures out that a site sells links,
then their PageRank gets hit.
For instance, in 2007, Google decided
some web pages were selling links and lowered their rankings.
Keep in mind that PageRank was given a value from 0 to 10.
Now listen to the list.
WashingtonPost.com went from a PageRank of 7 to 5.
Forbes.com also went from 7 to 5.
StatCounter.com went from 10 to 6.
SunTimes.com went from 7 to 5.
Again, it isn't illegal to do this,
but if you're caught, Google can penalize you.
Is this fair?
It can have a major impact on your business.
That's exactly what created a legal stir in 2002 and 2003.
Searchking.com had high PageRank and was sharing it
with its clients.
Suddenly, in August 2002, it dropped from 8 to 4
and then from 2 to 0.
0.
Its clients' PageRanks dropped 2 and they moved on.
So on October 17, 2002, Searchking sued Google for $75,000
and for restoration of its and its clients' PageRank.
This case would set the precedent for such actions.
On May 27, 2003, the court ruled against Searchking.
The court ruled that PageRank is essentially
a matter of opinion.
If you think about it, we choose to listen to that opinion
or we don't.
Data is an opinion.
So yes, one can sell links or pay to improve a web page's
search engine ranking.
But you risk the wrath of Google.
Now, PageRank isn't the thing Google uses, the only thing
that Google uses to create search results.
So presumably, one can exploit other aspects of their search
too.
Again, a key is knowing what is being done.
To see this, we look to November and December 2003.
If you put miserable failure into Google,
the official White House biography of the president
was returned as the highest ranked query.
Somehow, this was happening, even though the words
miserable failure were nowhere on that web page.
How did this happen?
This is what became known as a Google bomb.
The architect was George Johnston,
who had started building this one about a month earlier.
Putting together a Google bomb was relatively easy.
It involved several web pages linking to the White House
biography of the president with an agreed upon anchor text.
This is the hyperlinked text that you
click to go to the linked web page.
In the case of the miserable failure project,
of the 800 pages that linked to the president's biography,
only 32 were part of the Google bomb
that used the phrase miserable failure in the anchor text.
But from November through December of 2003,
if you put the words miserable failure into Google,
the top search result was the White House biography
for President George W. Bush.
By January, a miserable failure query
returned results for Michael Moore, President Bush,
Jimmy Carter, and Hillary Clinton in the top four
positions.
It kept switching as the web savvy engaged
in this act of internet politics.
What part of the Google algorithm did this exploit?
Google wasn't just looking at the text on a website
to determine its content.
It treated the hyperlinked anchor text
that links to a site as a summary.
For example, if you link to the great courses,
you might link with the words the great courses
or the teaching company.
These words summarize the content.
By using such words, one can connect the web page
to words that possibly don't appear on the site.
Now, think again about J.C.
Penny coming up first for searches on dresses,
aerial rugs, and other products.
Between the selling of links and Google bombing,
this doesn't seem so insurmountable of a task.
Again, knowing how Google creates its web ranking
can be to one's advantage.
Of course, Google has filed patents
and taken other steps aimed at reducing
the impact of Google bombs, especially
in their 2012 Penguin update.
Speaking of updates, the history of changes
to the Google search engine offers more general lessons
of data analytics.
The core approach was very good, but Google
stayed on top by continuing to make the algorithm work better
and better.
Major updates have their own code names
and have included 2005 personalized search,
taking prior search history into account when giving results.
2007 universal search.
Results from text, news, video, and so on are all included.
Caffeine, 2010.
New indexing of the web gives 50% fresher results.
Panda, 2011.
Downgrading of sites with little content, mostly ads
or duplicates of other sites.
Penguin, 2012.
Pushback against Google bombs, keyword stuffing,
and other attempts to hijack the search results.
Hummingbird, 2013.
Conversational search became possible,
where complex questions and even follow-up questions
using pronouns became possible.
Google has stayed relevant from the time
when the internet had millions of web pages
to an era of trillions of web pages.
The reason the Google algorithm has continued
to work for more and more users is
because the algorithm itself adjusted to downgrade attempts
to hijack search results, while also finding more and even more
ways to deliver meaningful results.
Scalability is an important issue for any business or algorithm
that will work with data from the internet.
Google shows that scalability means far more than just
handling bigger quantities.
