No, but here lies a fundamental lesson about clustering.
Clustering is about grouping the items so they lose their individuality.
That description isn't entirely anyone I would know in the group, but parts of it might describe people that I would know.
We look for a method that groups each person more accurately than alternatives.
Let's try another example with clusters of the United States based on their presidential election voting records.
Red state and blue states entered popular conversation after the 2000 election, but we can look backward in time for more enduring trends.
New Mexico and Arizona entered the union in 1912, becoming the 47th and 48th states.
Thus, we have presidential election data for 48 states from 1912 through 2008, so we are clustering 48 states using data from 25 elections.
We first form A, where each row represents a candidate in a particular election.
Each election has the two major party candidates and often a fairly well-known third or fourth party candidate.
Some examples are Perot, Buchanan, Thurmond, Theodore Roosevelt in 1912.
Finally, we add a single leftover row for each election for all the remaining candidates.
This results in an 88 by 48 matrix, and we can run a clustering algorithm on that.
Along with Chuck Wessel, a professor from Gettysburg College, I did just that.
The technique we used is called the non-negative matrix factorization, and like spectral clustering, it uses linear algebra.
We'll just look at the results.
The United States has historically been dominated largely by two major parties, so choosing K equal to is a natural start.
We see a large cluster of states stretching across the south and west, joined by Minnesota and New Hampshire.
Now, when examining any of these maps, keep in mind that the clusters do not assign a label corresponding to a particular political party or philosophy.
So let's see what happens if we divide the same data into three clusters.
The new group consists of border states, the Pacific states, plus Michigan and Maine.
How about four clusters?
This is a small group of states, border in Canada plus Massachusetts.
With five clusters, what's new here is splitting the south at the Mississippi River in a new group centered on Texas.
Montana is clustered with Idaho and Wyoming, only when we use five clusters.
When I teach clustering, I always underscore one big issue over and over.
Data analysis or math does the grouping.
With the exception of decision trees, we generally can't immediately see why the groups turn out the way they do.
So once we get the results back, we have to look at them.
When I do clustering work, I often need to collaborate with someone who knows the data or application.
I need their expertise.
So we've learned to group items.
Many times, data analysts take tables of data and use software to cluster.
Many packages come with K-means and hierarchical clustering methods.
There are many more, but inherent to this work is deciding what to cluster.
Two things can go wrong.
One, the distance measure makes no sense.
Two, the clustering itself doesn't fit the application.
The easiest way to see why distance measures change is thinking of driving.
Do you want to know the distance between locations as the crow flies or how far away they are given the roads you have to drive?
