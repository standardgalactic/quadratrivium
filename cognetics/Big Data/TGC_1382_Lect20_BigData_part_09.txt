You may only end up with a couple hundred new customers, but you'll be much more efficient than you might have been attracting the same number in a larger market.
Then what?
Well, you need to define what your decision tree is deciding.
Remember the Titanic, where the decision tree had two groups, who survived and who did not.
In the search for new customers, they had three groups, good and bad and in between.
They actually removed those in the middle.
Data from the middle might be important for later sales efforts, but such data was not part of the training set.
So this simplified the model by training it only on data that was clearly from the good or the bad set.
With all this in place, it was time to split.
So how did it split?
The first split was on median home value.
The towns where most houses were worth less than $226,000, according to the 2000 census data, were poor prospects for that newspaper.
Other variables that turned out to be important included the average years of school completed, the percentage of population in blue collar occupations, and the percentage of the population in high status corporate occupations.
Again, how did decision tree analysis help?
They had started with hundreds of variables available via the US Census Bureau data in other sources.
The decision tree narrowed all that down to the few that were most predictive and important.
Similarly, if we had done the Titanic analysis with 10 to 15 pieces of information per person,
we'd likely get a smaller number of splits on the data telling most of the story.
In each case, a decision tree would indicate which pieces of information were most predictive and important.
But we do need a word of caution.
There could be subtle interdependencies in the data that a decision tree will not capture.
Even so, it can bring us down to a manageable number of variables.
Then we can turn to regression and neural networks to define the to refine the analysis.
If you include too many variables in regression or neural networks, the data gets memorized.
As such, you perfectly describe the data you have that's overfit, but you can't predict future behavior, except in the rare case, it already matches perfectly with past events.
Decision trees are a powerful technique.
Not only for decisions we make, but also for understanding all kinds of factors and probabilities contributing to a set of outcomes.
Personally, I always ask myself, can I use a decision tree for this whenever I look at data?
With decision trees, you create a huge sieve in the data deluge, keeping a lot of the good stuff and getting rid of a lot of the noise.
John Tukey famously got everyone to begin graphically displaying data to get familiar with it, true and important.
Decision trees offer a less recognized but equally valuable second layer of familiarity, one that can springboard your analysis.
Sometimes, like with the Titanic, we create a decision tree and we're done.
We have the desired insight.
Other times, decision trees are a first tool that perhaps offers the way to look at other methods.
Either way, decision trees can carve quickly through your data, offering insight and possibly predictions about the future.
So remember, decision trees are easy to use and very powerful.
In fact, a decision tree may be all the analysis you'll need.
