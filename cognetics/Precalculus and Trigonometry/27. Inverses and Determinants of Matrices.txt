Hello again. This is our third lecture on matrices and solving systems of linear equations.
We've seen that interplay between a system of linear equations and corresponding matrices,
and then we studied matrices by themselves for a while. We talked about how you can add
them, multiply them by scalars, and then the more complicated operation of matrix multiplication.
In this lecture, I'd like to focus more on matrices. I'd like to develop two more very
important ideas, one being the inverse of a matrix and the other being determinants
of matrices. Near the end of this lecture, I'd like to also show you an application of
matrix inverses to solving a linear system. I'll ask you the following question. Do you
like this new way of solving a linear system? Keep that question in the back of your mind
as we progress. But first of all, what do I mean when I say inverse of a matrix? First
of all, we'll be dealing with square matrices. In fact, this entire lecture will be with
square matrices. Let me illustrate the idea of a matrix inverse as follows. Let a be
minus one, two, minus one, one, a two by two matrix, and b, another two by two matrix,
one minus two, one minus one. I'm going to multiply a times b. Let's remind ourselves
how we do that because it's a non-trivial task. You set up the two matrices side by
side, and then you multiply the first row of the left-hand matrix times the first column
of the right-hand matrix, and that gives the upper left-hand entry of the product. So in
this case, it's minus one times one plus two times one, and that simplifies to one. So the
upper left-hand entry of the product is one. Let's do the next multiplication. Minus one
times minus two plus two times minus one. Oh, they cancel giving a zero. And then the
bottom left entry is also zero, and the bottom right entry, the entry in position two, two,
is minus one times minus two plus one times minus one, and that yields a one.
How interesting. The product of these two matrices, a, b, yielded that matrix 1001, which I called
an identity matrix. And in fact, if you were to reverse a and b and write it as b, a, you
would get the same answer. You'd get the identity matrix 1001. Here's a rare case where the
matrices actually commute. a, b equals b, a in this case. So what do we say in this case?
Imagine with numbers, we've kind of got a product of two numbers equal to one, like
seven times one seventh. We call those inverses of each other, don't we? Multiplicative inverses.
Well, the same thing here. a and b are inverse matrices of each other. So if a, b equals the
identity matrix, and b, a equals the identity matrix, we say that b is the inverse of a,
and in a similar fashion, a is the inverse of b. Some good news here. If the inverse exists,
that's a big if, but if it does exist, it's unique. There's only one inverse for a given
matrix if it has an inverse at all. And here's some bad news. The notation is not the greatest.
It's the matrix, the inverse of a is denoted a, and you put a minus one up here. We did
that earlier with inverse trig functions, didn't we? And I kind of pointed out that that's
a fairly misleading notation. You might think it's 1 over a, but 1 over a makes no sense.
We don't know how to divide matrices. So a inverse is that unique matrix b such that
a times a inverse or a times b equals the identity. So here's a great question. Given
a matrix, a square matrix, how do you find its inverse? What's the algorithm for finding
the inverse of a matrix? How do you, how do you do it? You know, Jen, I could do the following.
I could just tell you the algorithm right now. And for many students, that's fine. They
sort of tell me in class, Professor, don't confuse me with a lot of reasons. Just give
me the answer. How do you do it? But let me take a moment to try to motivate this algorithm
because I think it's kind of a surprising idea. And the algorithm by itself, it's kind
of no fun. It's more, it's better, I think, to see a little bit of motivation. So let
me spend a couple of minutes explaining how the algorithm arises. So here's what we'll
do. We'll take an example, say the matrix 1, 4, minus 1, minus 3. And I want to find
its inverse. So let's call the inverse b and give its entries x, y, z, w. So the game
is, given that matrix a, find the entries of the matrix b such that a, b, x, y, z,
b is the identity matrix 1, 0, 0, 1. So let's put them back to back. Here's a, 1, 4, minus
1, minus 3, times this unknown matrix, x, y, z, w. And let's do the matrix multiplication.
Okay. What's the upper left entry? x plus 4, z. The upper right entry, y plus 4, w.
The lower left, minus x, minus 3, z, and minus y, minus 3, w is the lower right. So I've
multiplied these two together. And what do I want the answer to be? I want the answer
to be the identity matrix 1, 0, 0, 1, don't I? So here I have a pair of 2 by 2 matrices
and they're equal to each other. That means the entries are equal to each other. So this
gives rise to four, four linear equations. The first equation is the equating the upper
left-hand entries. x plus 4, z equals 1. And then you have y plus 4, w equals 0, minus
x, minus 3, z equals 0, and finally, minus y, minus 3, w equals 1. But notice something
about these equations. In a sense, they've kind of split apart. The equations involving
x and z are over here, and then there's the equations involving y and w. They aren't
intertwined a whole lot. You've got really two systems of equations. One system involves
x and z, and the other system involves y and w. I have to solve two systems. Well, what
else do you notice about these systems? Check out their coefficient matrices. They're the
same, aren't they? Wow. The coefficient matrix is 1, 4, minus 1, minus 3, and that's the
same for both systems. So if I wanted to solve each of these systems, remember, I'm trying
to find x, y, z, w. So if I want to find x and z, I would solve the following system.
1, 4, 1, 1 would be the right-hand side on the constant term, and then the second row
would be minus 1, minus 3, 0. I would take that matrix and do our Gaussian elimination,
solve that system. Now, if I wanted to find y and w, I would set up another matrix, correlation
corresponding to that system, and it would have 1, 4, 0, minus 1, minus 3, 1. In other
words, the coefficient matrices are the same, but the right-hand sides are slightly different.
One of them is 1, 0, and the other one is 0, 1. Well, and I would row-reduce that matrix
and get an answer. But wait, can you do even better? The row reduction for the first matrix,
by row reduction, I mean those operations of multiplying one row and adding it to another
row, etc. Those operations for the first matrix are precisely the same ones you'll do on the
second matrix. So why not do them both at the same time? Isn't this cool? So what you
do is you write a wider matrix on the left is the common coefficient matrix, 1, 4, 3,
4, minus 1, minus 3. And then I like to put a vertical line here to distinguish that on
the other side, I have the right-hand sides. The first system had 1, 0 as its right-hand
side, the second system had 0, 1 as its right-hand side, and I'm going to glue them together.
And I'm going to solve this double system. Well, how do I solve that system? Or how do
I row-reduce that? I apply a Gaussian elimination to that. Well, it's the same old thing, a
multiple of one row added to another, blah, blah, blah. You do all that, and here's what
you get when you're finished. The left-hand side is the identity matrix, 1, 0, 0, 1.
And the right-hand side has been changed. It's now minus 3, minus 4, 1, 1. So magically
the row-reduction algorithm has produced on one side the identity matrix and on the other
side a new matrix. Let's now untangle all this. What does that system mean? Well, remember
we're solving two systems simultaneously here. The first system, if you peel off that first
column, the minus 3, 1 part, is 1, 0, minus 3, 0, 1, 1. That system says that x is minus
3 and z is 1 because that first system involved the variables x and z. The second system says
that y is minus 4 and w is 1. Whoa, I've solved the whole problem. I now know what the inverse
matrix B is. B equals minus 3, minus 4, 1, 1. So this is the algorithm. You can see
how it works. You take the matrix you want to invert. You put over here the identity
matrix 1, 0, 0, 1. And then you do row operations. You do Gaussian elimination, all those ugly
operations, all that arithmetic, until finally on this side, which began as the matrix A,
it will finish as an identity matrix and over here will be the inverse.
Not quite, of course. It may not get to the identity over here and I'll show you an example
of that in a minute. But let me first formalize this algorithm. It's a beautiful algorithm
and then we'll show an example or two. So here's the general algorithm for finding the
inverse of a square matrix. Okay, the matrix is A. Write down A and adjoin to it the identity
matrix. So this is now a much bigger, a rectangular matrix. It's now n rows but 2n columns. A
and the identity. Do row reduction. Do those elementary row operations that we learned
about when we studied Gaussian elimination. Keep doing them until over here on the left
you have the identity matrix and on the right will appear the inverse. Whoa, there it is.
There's your answer. However, it could be that you can't get the identity over here and then
you'll conclude that the matrix does not have an inverse. Alright, and I've kind of
written this in a formalized pattern. A vertical line I, row reduction to the identity matrix
to I vertical line A inverse. Let me show you an example. This example is a 3 by 3 matrix
1 minus 1, 0, 1, 0, minus 1, 6, minus 2, minus 3. How could you find its inverse if it has
one? Well the algorithm says adjoin to it on the right the identity matrix and now it would
be the 3 by 3 matrix. Do row operation. Now don't make me do it in front of you. This
would take by hand 20 minutes but a calculator can do it instantaneously. And you end up
with on one side the identity matrix and over there on the other side magically the inverse
has appeared. And there you see it. The inverse turns out to be minus 2, minus 3, 1, minus
3, minus 3, 1, minus 2, minus 4, 1. I invite you to check that A times that inverse indeed
yields the identity. Here's another example. Almost the same. Well another 3 by 3 example.
1, 2, 3, 4, 5, 6, 7, 8, 9 kind of a cute little matrix. What's its inverse if it has one?
Well you do the same algorithm. Plop the matrix here, the identity here, do those row operations
but look what happens here. When you're doing the row operations you end up with a row of
zeros on the left hand side. And that indicates that you can't find the inverse because it
doesn't have one. And when a matrix doesn't have an inverse we use the word singular to
describe it. It's sort of a bad matrix. It doesn't have an inverse. It's called singular
in the literature. Now if you're dealing with big matrices you have to use this algorithm.
With 2 by 2 matrices it's pretty simple to find the inverse. There's a little formula
that I'll show you and you can verify that it works. So given a 2 by 2 matrix ABCD, or
ABCD or random numbers, its inverse is the following. Well the matrix is D minus B minus
C A times a certain scalar, a certain constant. And that constant is 1 over AD minus BC. One
over the product AD minus BC. That product by the way is called the determinant of the
2 by 2 matrix. And a topic will come to in a moment. Let's do a quick example. Let's
let AB 3 minus 1 minus 2 2. What's its inverse? Well you could use that previous algorithm
if you want and row reduce things. Or we could use our new formula. Here's what it says.
The coefficient in the front is 1 over AD 3 times 2 minus BC minus 1 times minus 2. And
then there's a new matrix here. How does that new matrix work? Well the diagonal AD gets
flipped to DA. So it's now instead of 3 2 down the diagonal it's 2 3. And then the
other entries have minus signs in front of them. So instead of minus 1 minus 2 it becomes
1 2. And simplify. That quotient there turns out to be 1 4th times that matrix 2 1 2 3.
And if you want you could bring the 1 4th inside scalar multiplication. The inverse turns
out to be the 2 by 2 matrix 1 half 1 4th 1 half 3 4ths. And I invite you to verify
that indeed that is the inverse of A. So you be the teacher for a moment. Student comes
running up to you. You've just explained this fancy new formula. Student comes running up
and goes professor it doesn't work. I don't like your new formula. It doesn't work on
this matrix. Look teacher, the matrix 1 2 3 6. If I try to use your formula, you'll
see when I do AD minus BC. Remember that formula involved 1 over AD minus BC. When I plug those
in I get 1 times 6 minus 2 times 3. I get 0. And you told me teacher that I can't divide
by 0 in mathematics. What's going on here? How do you answer this student who claims
this formula is not working? Because of division by 0. Oh yeah, that matrix A doesn't have
an inverse. It's what we called a singular matrix. And if you try to use the other algorithm
on that matrix you'll discover you'll get a row of 0's and you cannot find its inverse
because it doesn't have one. Okay, remember this problem we did last lecture where we
fit a parabola through three data points. Let me show you how this inverse matrix can
be used to solve this problem a slightly different way. And then I'll ask you a question
I talked about at the very beginning of this lecture. Do you like this new method? So here's
the setup again. Three points, 2 0 3 minus 1 4 0. Three points in the plane. I want a
parabola that passes through all three of them. Let's call the parabola AX squared plus BX
plus C. Trying to find A, B and C. Okay, this gives rise to three equations, three unknowns
just like we did it last time. And remember that coefficient matrix was 4 2 1 9 3 1 16
4 1. The unknowns we wrote as a column matrix A, B, C and then the right hand side was 0
minus 1 0. And we looked at that and we solved it say with Gaussian elimination. This time
I'm going to do it slightly differently. Write that equation in the compact form AX equals
B. And think about it now. Gee, if that were an algebraic equation, if I forget about matrices
and I go back to algebra, and if I have an equation AX equals B, where A and B are just
numbers, what's X? Oh, it's B over A. You would multiply by A inverse, wouldn't you
to solve that? Well, we're going to do the same thing with matrices. So to solve the
equation AX equals B, multiply on the left, both sides by the inverse of A. And in the
A inverse times A will cancel. They'll give the identity matrix. And then the right hand
side is A inverse B. And so we have the equation, the identity matrix times X, which of course
is just X, equals A inverse B. Theoretically, this looks really nice. You have this linear
system, and it could be any old system with a square coefficient matrix. You find the
inverse of that coefficient matrix, and the answer is a simple equation, A inverse B.
That looks really simple, doesn't it? In this case, I won't bore you with finding that
inverse, but it turns out to be a 3 by 3 matrix. A does have an inverse. And if you
multiply that inverse times B, indeed you get the answer we had last lecture, 1 minus
6, 8. What do you think of this method? Is this a good method? Remember how it goes?
To solve a system, AX equals B, invert A, find the inverse of A, and then your answer
is nothing more than X equals A inverse B, a simple multiplication. What do you think
about it? Is this a better method than Gaussian elimination, that process we learned about
a few lectures ago? Well, I hope you see that, uh-uh, this is really not a very good method,
because to use it, you have to find A inverse, don't you? And that takes a lot of work.
A inverse requires all those row reduction, all those steps of Gaussian elimination. So
in theory, it looks like a nice method. One, whoa, very clean, X equals B, there's the
answer. In practice, this is not really a good method, and is never really used in real
life. In real life, in real applications, when you're confronted with a linear system,
everybody, I shouldn't say everybody, but most people will solve that linear system
with Gaussian elimination. Now, there's some other techniques for solving system, the Gauss-Jordan
method, there are certain what are called iterative methods, but the technique we've
learned in these lectures of row reducing, uh, that is the mainstream method for, for
most linear systems. Okay, let's move to determinants. And we saw what they were briefly
when we did that little formula for A inverse for two by two matrices. So let me define
what a determinant is. Uh, given a two by two mate, I'll start with two by twos, given
a two by two matrix, A, B, C, D, we define its determinant to be A, D minus B, C. It's
that cross-diagonal multiplication. As usual in math, sorry about this, there are lots
of notations for determinant. But one I prefer is D, E, T, determinant of A. But some books
put vertical lines around A, that really confuses me, that's like absolute valuing,
value of A. And then others will put the vertical lines around the entire matrix, A, B, C, D.
That also sometimes confuses me because I'm not sure if I'm looking at the matrix or it's
determinant. So let's try to use just determinant of A if, if we remember, but we should be
aware of all those notations. Okay, some quick examples. Um, A is two minus three, one, two.
What's this determinant? Oh, pretty easy. It's two times two minus, minus three times
one, four plus three is seven. How about two, one, four, two? Piece of cake. Two times two
minus one times four, that determinant is zero. Is that matrix we just looked at invertible?
Does it have an inverse? It doesn't. Remember, if the determinant is zero, you are singular,
you do not have an inverse. Okay. Now that was the definition of determinants for baby
matrices, two by two matrices. What about a three by three, or a four by four, or a ten
by ten, or a thousand by a thousand? Well, trust me, it's a non-trivial computation.
Again, fortunately, these are built into calculators. So nowadays, we don't really have to compute
determinants by hand very much. But I thought I'd, what's the word? I don't want to say
torture you here, but I thought I would show you a pretty complex calculation of a three
by three determinant, just to give you a taste of how tough it can be. Let's find the determinant
of a three by three matrix. And I'm just going to sort of show you the algorithm, but I'm
not going to explain where it comes from. Here's the matrix, one four minus two, three
two zero minus one four three. What's its determinant? Now the good news is, you could
plug this into your calculator or your computer, and it will spit out the answer without, and
you don't have to know how it did it. But I'm going to show you sort of the fundamental
way of doing it, and then I'll talk about it. So this is called expansion across the
first row. The determinant of that matrix, here's how you do it. Look at the first row
of the matrix. The first entry is one. And one is then multiplied by the determinant
of the matrix you obtain by deleting the row in the column where one lives. So that sub-matrix,
that smaller two by two matrix is two zero four three. You see how I got that little
bitty two by two matrix? One's in the upper left. I removed its row and column, and I'm
left with a two by two matrix. Then you step over to the four, and you multiply it by its
two by two matrix you obtain by deleting the row in the column where four lives. So that
leaves three zero minus one three. But there's a further kicker here. This is why this is
an unpleasant task. In front of the four you have to put a minus sign. There's kind of
a checkerboard pattern here where the signs go plus minus plus. And if it were a bigger
matrix it'd go plus minus plus minus. Okay. What about the third piece? Minus two times
the determinant of its sub-matrix three two minus one four. Then you find the determinants
of those little two by two guys. Those are pretty easy. Two by twos are easy. So it's
one times six minus zero minus four times nine minus zero, and then minus two times
twelve plus two. Three times four plus two times one. And then simplify that. You finally
get minus fifty eight. Ouch. Complicated algorithm. Multiply a number by the sub-matrix you obtain
by deleting the row in the column and remember that plus minus checkerboard pattern. But
you see what happens here? This three by three matrix gave rise to three two by twos. So
what if I'd given you a four by four matrix? Well you would expand across the first row
and that would give rise to four three by three matrices. And then you would do each
of those like I just illustrate. Ouch. What if I'd given you a five by five matrix? Starting
to get a headache probably. You'd have a five by five. You expand across the first row.
Remember the checkerboard plus minus plus minus plus. And each of those will give rise
to a four by four. So you'll have five four by fours. Each of which gives rise to those
three by threes. It's a terrible way to actually calculate a large determinant. And in fact
fortunately in real life there are completely different methods for calculating determinants
that are much much faster. And in fact they involve those same row operations that we
studied with Gaussian elimination. And again the good news is these are built into calculators
and computers and that's why I'm not going to spend too much time on them. In today's
world we tend not to ask students to calculate determinants very much because it's really
not necessary anymore. Alright. We've really I think had a good look
at linear systems of equations, matrices and their interaction. And then we've talked
about matrices as a standalone idea. You can add them, you can multiply them together,
you can find their inverses if they're invertible, you can find their determinants as all kinds
of things you can do with matrices. So what we're going to do in our fourth lecture and
final lecture on this topic, we're going to look at some real life problems. We're going
to apply our skills of linear systems and matrices to show some applications of these
ideas. So I look forward next time to seeing you when we talk about applications of linear
systems and matrices. Thanks.
