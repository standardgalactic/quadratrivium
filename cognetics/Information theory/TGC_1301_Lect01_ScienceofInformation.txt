You
your lecture is Dr. Benjamin Schumacher. Dr. Schumacher is professor of physics
physics at Kenyon College, where he has taught for 25 years. He received his PhD in theoretical
physics from the University of Texas at Austin. Dr. Schumacher is the author of numerous scientific
papers and books, and co-author of Quantum Processes, Systems, and Information.
As one of the founders of Quantum Information Theory, he established several fundamental
results about the information capacity of quantum systems. For his contributions to
Quantum Information Theory, Dr. Schumacher won the 2002 International Quantum Communication
Award and was named a Fellow of the American Physical Society.
If the science of information were a zoo, then any of the animals could be turned into
any other animal. This transformability of information, its capacity to shift from one
physical form to another, is perhaps the most fundamental principle of the science of information.
Here we have four familiar physical media for representing music. The phonograph record
represents musical sounds, as variations in the shape of a thin spiral groove that wraps
around the face of the disc from the outside to the middle. In a cassette, the sound is
represented as a series of magnetic bands on a narrow flexible tape wound up on spools.
The tape in one cassette would stretch the length of a football field, yet that is actually
shorter than one of those phonograph grooves. The CD represents sounds by a series of microscopic
pits, half a micron wide, read out by a tightly focused laser beam. But a lot of music these
days is stored on a computer, like other kinds of data. So here is a memory unit from a computer.
It looks nothing like, and it works nothing like, the record or the cassette or the CD.
Yet they all store the same piece of information, a piece of music. In a way, it's ironic
that we now use computers to store musical sounds. In the early days of computers, many
of them stored their data as sound waves, bouncing back and forth in long tubes of liquid mercury.
Sound is data, data as sound. Any animal into any other. Another lesson in the transformability
of information.
You and I live in a revolutionary age, the age of information. Never before in history
have we been able to acquire, record, communicate and use so many different forms of information.
Never before have we had access to such vast quantities of data of every kind. Never before
have we been able to translate information from one form to another so easily and quickly.
And so, never before have we had such a need to comprehend the concepts and principles
of information.
The historical roots of this new age of ours go way back, but the real revolution took
off in 1948 in New Jersey at the research branch of the old Bell System, Bell Labs.
And one of the main events that triggered it was the publication in the Bell System Technical
Journal of a long scientific paper by the American engineer and mathematician Claude
Shannon. That paper set forth the elements of a new science called information theory,
the science that became the new constitution of our revolutionary age.
It is impossible to exaggerate the importance of Shannon's ideas. His theory has had an
impact far beyond the technology of communication and computing. Information has proved to be
a profound, unifying idea in many branches of science. It plays a central role in contemporary
physics and biology, and even finance and economics.
And yet, what is information? Information is a paradox. On the one hand, information
is physical. Whenever we communicate or record information, we always make use of a physical
medium, sound, light, radio waves, electrical signals, magnetic patterns.
But on the other hand, information is also abstract. The messages carried by our physical
signals are not really identical to the signals themselves. Shakespeare's plays are not
just bits of ink on a page.
That dichotomy is the key to the amazing transformability of information, from letter to laser pulse,
from radio signal to sound wave. And yet, somehow the same.
But I think the greatest example of information transformability in all of human history is
the invention that gave us human history. In the first place, writing. Human speech is
a pattern of sound. Spoken words travel a little distance, and then they fade away.
But in writing, these spoken words are rendered as patterns carved or painted on a surface.
And in this form, they become durable rather than ephemeral. They can be carried from place
to place. Through writing, I can receive the words of someone a thousand miles away, or
a thousand years ago.
There are many different kinds of writing systems. The Japanese use three different kinds of systems.
Kanji is a logographic system. The symbols stand for entire words. There are thousands
of kanji. In the hiragana system, like the closely related katakana system, the symbols
stand for syllables that make up words. There are 46 characters in the hiragana system.
Finally, Japanese is often written using the 26 letters of the English or Roman alphabet.
Each one standing for a sound within a syllable. In Japan, they call this system romaji.
The fewer symbols you have, the more of them you need to use. Thus, the Japanese word mizu,
meaning water, requires one logogram in kanji, two syllables in hiragana, or four letters
in romaji.
Let me show you one of my favorite examples of writing. The old runic alphabet that was
common in northern Europe about a thousand years ago. These particular inscriptions, scratched
onto sticks and pieces of scrap wood, were dug up in the old city of Brigham, modern
day Bergen in Norway. The Brigham inscriptions give fascinating glimpses of daily life in
old Norway. They speak to us over the centuries. Some of the inscriptions are simply labels
to be attached to things. This belongs to Johann. Some are magic charms or prayers or
curses. Some of the sticks are simple text messages like we might send on a cell phone
today. Gita says you are to come home.
We can read the Brigham inscriptions because we still understand the runic alphabet. And
that's lucky. Writing represents speech, and in alphabetic writing, each symbol represents
a sound. But the rule of association between the symbol and the message, the code, is entirely
arbitrary. There is no necessary connection between one set of scratches and the sound
P. Writing works because both writer and reader know the code. Without the code, there is
no way to understand what the symbols mean.
We weren't so lucky with the Egyptian hieroglyphs. For thousands of years, ancient Egyptians
made inscriptions on stone and writings on papyrus using a beautiful script of stylized
pictures. And then around 1500 years ago, the last Egyptian scribe who knew the ancient
hieroglyphic code died. The hieroglyphs became incomprehensible. But not forever. 200 years
ago, a brilliant Frenchman named Jean-Fran√ßois Champollion rediscovered the code. How did
he do that? He made use of one of the most famous artifacts in all of archaeology, the
Rosetta Stone. I have a one-quarter scale reproduction of the Rosetta Stone that hangs
in my office, and I've brought it here to show you. The original is in the British
Museum. It's a large slab of granite covered with writing. The inscription on it is a religious
decree from the late history of Egypt after that country was conquered by Alexander the
Great, and so the decree is given three times in three different written languages. At the
bottom is Greek, written in the familiar Greek letters. Next comes an Egyptian inscription
using an everyday script that scholars call Demotic. The top is written in the mysterious
hieroglyphs. Champollion was able to compare the three inscriptions and analyze the correspondences,
recognize the Egyptian language as a relative of modern Coptic, and work out the hieroglyphic
code. It turns out to be a curious mixture of logographic and alphabetic systems.
Champollion's discovery opened for us the writings of ancient Egypt. I find the Rosetta
Stone to be an inspiration to me as a scientist. It's a testimony to the ability of the human
mind to unravel even the most difficult puzzles. But the Rosetta Stone is also an icon of the
transformability of information. The same abstract message is here represented physically
in three different forms, as different patterns carved into stone.
I've been telling you about systems of writing, both ancient and modern, to make a point. The
essential concepts of information, the concepts of message and symbol and code, are not new
in human experience. They are as old as language itself. What is new is the jaw dropping multiplicity
of uses to which we have put those concepts in the age of information. That is the legacy
of Shannon's revolution. Claude Shannon hardly seemed like a revolutionary. He was a lean,
quiet man, fond of tinkering, interested in puzzles and games. He juggled. He built robots
that juggled. But everyone who knew him knew that he had a brilliant and original mind.
The institutions that employed him had the good sense to leave him alone, to investigate
his own research problems his own way. Later in his career, when he was on the MIT faculty,
he only taught a single course in 20 years. It was a research seminar in the late 1950s.
Here is Shannon's approach to teaching. Every week, he would discover something new and
lecture on that.
I think Shannon's real gift was a genius for abstraction. He had an uncanny ability
to look at a complicated real-world subject and see through the technical details to the
essential logical structure beneath. And so that's the talent he called upon during
World War II and the years just after as he pondered the subject of communication.
What is communication? How does it work? What is information?
Like a detective, Shannon begins by figuring out what information is not, and his first
point is his most startling. Shannon realizes that the mathematical idea of information
is not the same thing as meaning. Remember, he is starting out by analyzing communication
systems. After all, he works for the American Telephone and Telegraph Company. The essential
communication process is the same, no matter what the message means. It could be a nursery
rhyme, a formula, a photograph, a weather report, or even just a string of random numbers. It's
all information. And in the same way, the idea of information is not about the value
or the significance of a message. Meaning, value and significance are obviously important
qualities, but they are not the key to understanding what information is.
What's left? What's left, Shannon says, is the distinction between different messages.
It's as simple as that. Every message that is communicated is one particular message
out of a set of many different possible messages. And so here's a pretty good definition of
information as Shannon sees it. Information is the ability to distinguish reliably among
possible alternatives. So consider two people, Alice and Bob. Bob has just asked Alice a
question, maybe a very simple one with a yes or no answer. But Bob does not know what the
answer is. It might be no, it might be yes. He cannot distinguish reliably which one of
the possible answers is the right one. He lacks information. Now Alice says yes. Bob hears
and understands her. He has gained information. If information is all about the distinction
between possible messages, then there is a fundamental atom of information, the binary
distinction between just two possible messages. The messages might be yes and no, as in Alice's
answer to Bob. They might be on and off for an electrical signal, or the binary digits,
one and zero. It doesn't matter what the messages mean. It only matters that there
are two possible alternatives. Such a simple two value message is called a bit, which is
short for binary digit. The term was coined by John Tukey, a mathematician and statistician
who was also at Bell Labs when Shannon was thinking out his theory.
Shannon pointed out that bits form a kind of universal currency for information. Our
basic alphabet only needs two distinct symbols. And because we can transform information, we
can freely switch from one code to another than any sort of information. Numbers, words,
pictures, audio or video can be represented by bits. Arrangements of binary digits, zeros
and one. So here's a concrete example. It's the code that was established for teletype
machines in 1930, a modernized version of one invented 60 years earlier by the French
Telegraph engineer, Emile Baudot. In a Baudot code, each letter of the alphabet is represented
by five bits. The letter A is one, one, zero, zero, zero. The letter B is one, zero, zero,
one, one. A space between letters is zero, zero, one, zero, zero, and so on. So a teletype
machine has a keyboard. The operator pushes the key for a letter and a set of five zeros
and ones are transmitted on wires via electrical signals. At the far end, another machine receives
the signal and types out the corresponding letter. Letters can also be stored using paper
tape. On the tape are tiny sprocket holes which help guide it through the machines. The other
larger holes represent the zeros and ones, arranged in groups of five in rows across
the tape. A hole means one and no hole means zero. To create a record tape, you simply
use a machine that makes holes in the right pattern based on the teletype signals. Then
you fold or spool the tape up and put it in a safe place. To read the tape, you feed it
into another machine that detects the pattern of holes, either with electrical contacts
or beams of light and turns them back into teletype signals.
Why did Baudot use five bits for the teletype code? Why not four or seven? The answer involves
a fundamental fact of information theory. A code represents different messages by a
series or string of symbols, the five binary digits in this case. This is the code word
that represents the message. But the code has to preserve the information that is the distinction
between messages, so no two different messages can be represented by the same code word.
And that means that the number of possible code words in the code can be no smaller than
the number of possible messages, which we will call M. That is, the number of possible
code words is greater than or equal to M. That's our fundamental fact about codes.
In the teletype, we're coding letters of the alphabet. There are 26 letters, and since
we also need a character for the space between words, we can suppose that M is equal to 27.
How many five-bit code words do we have? Well, there are two possible values for the
first bit, and for each of those two possible values, there are two possible values for
the second bit, and so on, so the total number of possible five-bit combinations is 2 times
2 times 2 times 2 times 2, which equals 2 to the fifth power, or 32.
That number of code words, 32, is greater than the number of possible messages, 27,
so we're okay. But if we tried to get by with only four bits, we would only have two
to the fourth, or 16 available code words, and that number wouldn't be enough.
Here is another 19th century binary code, in fact the first binary code, Braille, the
writing system for the blind, created by Louis Braille in the 1830s. Letters are patterns
of raised dots that a blind reader can scan by touch. The dots are in two by three blocks,
so there are up to six possible dimples in a block. Think of a dot as one, and no dot
as zero. An A has one dot, and five no dots. That makes Braille a six-bit code with two
to the sixth, or 64 possible code words. That's more than enough for letters, plus
many punctuation marks, and other symbols. Both the Baudot and Braille codes, by the way,
circumvent their own limited set of code words by a clever trick. Each has a special code
word that indicates that the following code words should be interpreted according to an
alternate rule. This is how each code represents numerals, for instance.
These days, most text information is stored in computers using a seven-bit code called
ASCII, the American Standard Code for Information Interchange, which was introduced in 1963.
There are two to the seventh, or 128 ASCII code words. That's more than enough to represent
any character on a keyboard, both upper and lower case. For instance, the capital letter
X is one zero one one zero zero zero. The comma is zero one zero zero one one one. Since
the bits in a modern computer are generally grouped into bytes of eight bits, one seven-bit
ASCII code word is usually assigned one byte of computer memory.
There is an even more modern 16-bit system called Unicode that allows computers to use
letters and symbols from many different languages around the world. Lots of computer systems
and computer applications are shifting to Unicode.
The use of computers introduces a whole new aspect of information. The bits stored in
the computer's memory might simply be data, input, output, memory records, scratch base
for calculations and so on. These are organized into identifiable blocks of memory, usually
called files. But the bits might also represent computer programs, groups of instructions
that tell the computer how to operate. When we use a program, we say that the computer
runs or executes the instructions. But a stored program is just a file like any other. It
can be moved, copied, changed or erased. So computer information has a two-sided nature,
both data and program.
One of the first people to understand this fact was the Hungarian-American mathematician
John von Neumann. There may be no single person who can be called the inventor of the modern
computer, but if we made a list of the main claimants to that title, von Neumann's name
would appear near the top. In the years following World War II, while Shannon was creating information
theory, von Neumann was thinking about computers and robots, both the technical issues of how
to build them and the basic principles on which they operate.
One of the questions that von Neumann considered was this. Would it be possible to build a
self-reproducing machine? So imagine a robot that lives in a warehouse full of machine parts.
The robot rolls around, grabbing components from the shelves and putting them all together,
and eventually it assembles an exact duplicate of itself. That is what von Neumann had in
mind. He asked, what would such a machine have to be like? How would it work?
Well, obviously a von Neumann robot would have to contain a complete set of instructions
for building robots, in effect it must contain a detailed picture or blueprint of itself.
But that raises a tricky question. Does the blueprint of the robot contain, as part of
the blueprint, the blueprint itself? We can imagine a picture that contains a miniature
copy of itself. We can construct a geometric shape, part of which is an exact copy of the
whole. That kind of self-similar shape is called a fractal, but self-similar mathematical
objects are always infinite in some way. They contain infinitely many points, so they
can have infinitely fine levels of detail. That's how you get a picture inside a picture,
inside a picture, and so on. But that won't work for our von Neumann robot. Its memory
is strictly finite. It cannot hold a blueprint that contains a little copy of the blueprint
itself, which contains another smaller copy, and so on to infinity.
von Neumann figured out that his machine had to work in a different way, a way that
relies on the two-sided nature of information. The blueprint is a program that tells the robot
how to construct its duplicate, but that program is also a kind of data file that can be copied
and transmitted. The last step in the robot building program might read, make a copy of
the robot building program in the new robot's memory. The self-reproducing program does not
have to be infinite, it just has to be able to refer to itself.
In thinking about his self-reproducing machines in the 1940s, John von Neumann was not seriously
proposing an engineering project. We might someday make von Neumann machines, but that
isn't the point. There already exist machines of that kind in nature. They are called living
things. All kinds of biological organisms acquire energy and materials from their surroundings
and produce offspring of the same basic design as themselves.
So von Neumann's insight tells us something significant about how information works in
living systems. We now know that the genetic information in an organism is contained in
its DNA, as described by Watson and Crick just a few years after von Neumann's speculations.
DNA is a long twisted molecule that has a sequence of chemical bases and that sequence
is a kind of blueprint for the organism. Von Neumann's thought experiment tells us
that the information in DNA must be used in two quite different ways. It must be expressed,
that is it must act as a program for the organism to build and operate itself, but it must also
be copied like the blueprint of the von Neumann robot during reproduction. And that is exactly
what we find in nature.
Notice what has happened. The idea of information begins with technology, communication, codes,
information storage, computers, robots. The same ideas apply in an important way to the
natural world as well. Information theory is an essential part of science as well as
engineering. And this is true not just in molecular genetics, but in many other fields
too, from neuroscience to thermodynamics. In my view, many of the most fascinating connections
between information science or information and science are found in the science of physics.
Physicists have discovered that information plays an essential role in the basic laws
of matter and energy. Why? One answer is that information is a general way of thinking about
cause and effect. When we communicate, the message that is sent causes the message that
is received through some medium of transmitted signal. And every sort of physical cause and
effect link can be regarded as a kind of information, a kind of communication.
In the past, physicists regarded nature as a complex interplay of forces or energy transformations.
Today, they increasingly regard nature as a complex network of information.
Just as the concept of information influences physics, so too our understanding of physics
influences the concept of information. And this is especially true in the domain of physics
called quantum information theory. Quantum physics, of course, concerns the very strange
and surprising laws of the microscopic world, matter and energy at the most fundamental level.
It turns out that Shannon's original theory is not quite adequate to describe the information
processes that go on in that world. Let me mention a specific example to explain
what I mean. In ordinary information theory, there is no principle that restricts our ability
to duplicate information. Given a string of bits, we can copy them as perfectly and as
often as we like. But perfect copying is generally impossible for quantum information. We cannot
simply duplicate a quantum bit, or qubit, without distortion. This is sometimes called
the quantum no cloning theorem. And in that respect, at least, quantum information is
very different from the kind of information that Shannon considered.
Information plays such a central role in physics that some people have wondered whether the
most fundamental laws of nature might really be laws about information. The physicist John
Wheeler summed up the idea in a pithy slogan, it from bit. Perhaps everything in the universe,
every it, actually emerges from an underlying information process. It's a fascinating
speculation. The science of information reaches out to the very edge of our understanding
of the universe. But it begins by thinking about technology. That is where Shannon began,
with the practical technological problems of communication and data storage. Therefore,
you will be talking a lot about information technology. Some of our examples will be familiar
in our everyday lives. But often the examples that we choose to discuss may seem a little
old fashioned. After all, when was the last time you stored data on a paper tape?
There are reasons for including old fashioned examples along with up to date ones. To begin
with, information technology is developing so fast that by the time you see one of these
lectures, even my up to date examples may seem a little dated. Moreover, the profound
and timeless ideas of Shannon's information theory can often be illustrated more clearly
by an older piece of technology. Furthermore, we are interested in how the science of information
has developed over time, an interplay of ideas and inventions that has changed the world.
Sometimes new ideas inspired new inventions. But sometimes things happen the other way.
The inventions triggered the discovery of the ideas.
In the Revolution of 1948 that launched our age of information, the idea and the invention
arrived together at the same time and the same place. The idea was Shannon's information
theory, the theory that taught us how to distill all kinds of messages to bits. The invention
was the transistor. The transistor was invented at Bell Labs by the physicist John Bardeen,
Walter Bretain and William Shockley. It was just a few months before Shannon's paper
was published. A transistor is a device by which one electrical signal can control another.
That sounds very simple. It wasn't a new idea in 1948. Electrical relay switches and
vacuum tubes were already in use for that same function. But the transistor is a solid
state device made out of a semi-conducting material like germanium or silicon. Unlike
the relay switch it has no moving parts and it operates very fast. Unlike the vacuum tube
it does not require high voltages and it can be made exceedingly small. And with a modest
number of transistors, about six it depends on the design, you can make a simple flip-flop
circuit. That's the basic memory module of a computer where one bit of information can
be written, stored indefinitely and then read back later.
The transistor was a breakthrough discovery, one for which its discoverers shared the Nobel
Prize in 1956. And in the decade since the revolution of 1948 the transistor has become
the basis for more and more amazing technologies, especially technologies for communication
and information processing. Those technologies have become almost unimaginably powerful and
complex.
One way to chart this progress has been the famous Moore's Law. Gordon Moore is a scientist
and businessman who is one of the founders of the modern electronics industry. He pioneered
technology for integrated circuits, in which many transistors are located together in a
single electronic unit. Moore observed that since the invention of the integrated circuit
the number of transistors on one device has approximately doubled every two years. That's
been increased by a factor of 32 every decade and the rate of growth has continued without
interruption for the last 50 years.
Almost every measure of information processing power, speed, size, cost, energy consumption
has followed a similar exponential line of improvement. The result has been the creation
of the technological world we live in, in this age of information. Today my phone contains
a billion transistors. In the laboratory next to mine there is a computer with many
trillion.
What can you do with a trillion transistors all exchanging tiny electrical signals very
very quickly? The answer to that question was actually foreseen back in the days of
relays and vacuum tubes, and the man who foresaw it was none other than Claude Shannon, in
his early days as a graduate student at MIT back in 1937. Even then he was honing his
uncanny ability to look at an old and complex problem in an entirely original way and to
see where the simple logical pieces that would in time become the building blocks of a new
world. Even then Shannon was writing what has been called the most important thesis of
the 20th century.
So next time that is where we will turn to the first stirrings of the age of information
in the surprising scientific collision between arithmetic, logic, electricity, and one bit
of information.
