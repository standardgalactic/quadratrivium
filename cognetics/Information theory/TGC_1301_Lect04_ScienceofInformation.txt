Messages are not all alike.
Some messages tell us more.
So far we have considered only a simple model for an information source.
The source produces one of M different possible messages.
Each possible message is equally likely.
Furthermore, any two successive messages are completely independent of each other.
To see what that means, imagine a machine that generates something like English text,
maybe like transmitting teletype signals in a Baudot code.
Our machine transmits one symbol at a time, any of the 26 possible letters or the space
between words, and those 27 symbols occur randomly with equal likelihood.
Each symbol is generated independently of the previous ones.
The output of the machine looks like this.
This does not seem much like an actual English text.
Letters that are rare in real English like J and Z occur far too often.
There are long and unpronounceable strings of consonants and there are far too many letters
between the spaces.
So this simple model of an information source is not very much like a real source of messages.
To take the next step in the science of information, we need to do better.
We want an idea of an information source that includes some of the patterns and regularities
of actual messages.
But how?
This was the problem that Claude Shannon pondered when he was creating information theory.
His approach was to use the concept of probability.
In real English, the different letters do not occur equally often.
X is less common than P, which in turn is less common than E.
The space symbol between the words occurs even more frequently, so we should design
our text-generating machine to produce different letters with different probabilities.
Probability is a concept that is simple but subtle.
The theory of probability was started in the 17th century by mathematicians and scientists
and gamblers as a way of reasoning in the presence of uncertainty.
We assign each possible event X with a measure of its likelihood called its probability P
of X.
The first rule of probability is that the probability P of X is always between zero
and one, so rule number one for any X, zero is less than or equal to P of X, which is
less than or equal to one.
Now if X is actually an impossible event, if it can never occur no matter what, then
P of X equals zero.
On the other hand, if X is absolutely certain to occur, then P of X equals one.
In other situations, P of X is somewhere in between, closer to zero if X is less likely
closer to one if it is more likely.
To take a familiar example, if we flip a coin it might come up heads or tails, and we ordinarily
assign a probability one half, fifty percent, to each outcome.
Notice by the way that if you add up those two probabilities, P of heads plus P of tails,
you get one.
And that is the second general rule about probability, the probabilities add up to one.
Let's state that a little more carefully.
Let capital X represent a random variable, which is a set of possible events.
That set is mutually exclusive and exhaustive, which means that exactly one of the events,
no more and no less, will happen.
For each event X, lowercase X, in X, capital X, there is a probability P of X.
Then we express our second probability rule using a neat mathematical notation.
Rule number two, for any random variable X with values little x, the sum over X of P
of X equals one.
That capital Greek letter sigma stands for sum, and it tells us to add up P of X over
all the possible values of X. If the possible values are A, B, C, then the sum just means
P of A plus P of B plus P of C.
In our text generating machine, the random variable includes all of the symbols that
the machine might produce, the twenty-six letters plus the space.
Each of those letters will be assigned its own probability.
An assignment of probabilities, by the way, is called a probability distribution.
How do we decide what probability distribution to choose to make realistic text generating
machines?
Our choice hinges on how we interpret probability numbers.
So imagine a great many similar situations, flipping a coin many times, say.
Those situations are different trials of the same coin flipping experiment.
In some trials, the event X does occur, the coin comes up heads, but in others it does
not.
So suppose N is the total number of trials, and N of X is the number of times X occurs.
Then we expect that in the long run, P of X, the probability of X, is equal to N of
X divided by N.
The right hand side is called the statistical frequency of X, the fraction of the trials
in which X occurs.
In the long run, I claim that the probability should be just the same thing as the statistical
frequency.
My coin comes up heads half the time.
This is a useful intuition, but it's also tricky.
This equation is not literally true, especially if the number of trials is small.
Flip a coin ten times, and we might get six heads, or only three, even if it is a perfect
fifty-fifty coin.
The best we can say is that probabilities and frequencies are very likely to be nearly
equal if we make many trials of our experiment.
So we will base the letter probabilities for our text-generating machine on the letter frequencies
of our reference text, a large sample of English that we can analyze, and I've chosen
a book, The Return of Sherlock Holmes by Sir Arthur Conan Doyle.
Not only does it contain some of my favorite Sherlock Holmes detective stories, but I can
easily obtain the text of the book in an ASCII-coded computer data file, and I can use a computer
to do all my analysis.
The Return of Sherlock Holmes contains about 580,000 characters, making up around 112,000
words.
That should be a pretty good sample.
In the book, the most common character is the space, followed by the letters E, T, A,
and so on.
Space occurs 19.3 percent of the time, E occurs 10.0 percent of the time, T occurs 7.3 percent
of the time, A occurs 6.6 percent of the time, and so on, all the way down to Z, which only
occurs 0.034 percent of the time.
There are fewer than 200 Zs in the whole book.
You may have heard of an old printer's rule that the most common letters in English in
order are E, T, A, O, I, N, S, H, R, D, L, U, E, T, O, N, Sh, R, L, U. That rule is almost
exactly correct for the Return of Sherlock Holmes, except that we find H to be slightly
more common than S.
Now we can assign reasonable letter probabilities for our text-generating machine.
So here's a sample of the new machine's output.
This looks somewhat better than before.
The rare letters are rare, and there are more short words.
None of them are actual English words.
The machine is not yet very realistic, but we are making progress.
Now we have devised an information source whose messages, the letters, are not equally
likely.
How much information is being produced by the source?
That's a hard question.
It is easier to think about a simpler example, an information source that produces only two
possible messages.
Let's consider a fire alarm.
At any given moment, the fire alarm sends a message that says, no fire, the alarm is
silent, or fire, the alarm is sounding.
This seems at first like a simple binary source of information.
At each moment we learn one bit of information, whatever happens.
But that conclusion feels wrong.
To us it seems as if we learn more when the alarm goes off than we do when the alarm stays
silent.
Why?
At any given moment, it is overwhelmingly more likely that there is no fire.
The probability P of no fire is close to one, and P of a fire is close to zero.
So when the alarm is silent, that is more or less what we expect, it isn't news.
When the alarm does sound, we are much more surprised.
The message has a bigger effect on our thinking.
The world for us becomes very different.
The more surprising message tells us more.
To measure the information content of a message, therefore, we need a measure of its surprise.
All the surprise of a message X, S of X.
This depends on the likelihood of the message.
A less probable message has more surprise to it.
In information theory, the measure of surprise is based on the logarithm.
The surprise of X is the logarithm of the reciprocal of the probability of X.
This definition has all the characteristics we want.
If P of X equals one, then S of X equals log two of one, or zero, a message that is certain
has zero surprise.
When the probability is less than one, the reciprocal is greater than one, and the surprise
is a positive number.
If P of X gets very close to zero, one over P of X becomes very large, and so the surprise
S of X gets big as well.
If P of X is equal to zero, the surprise just blows up mathematically.
A truly impossible event would be infinitely surprising.
But since an impossible event can never happen, we are never actually infinitely surprised.
Recall one of our old-style information sources in which the impossible messages are all equally
likely.
That means that each message must have a probability one over M, so that M of them will add up
to one.
The surprise of such a message, S of X, equals log two of one over M, or log two of M. That's
exactly the Hartley-Shannon entropy, which measures the amount of information that the
message source produces.
Now we can conclude several things.
To begin with, surprise is a measure of the information in a message.
It's measured in bits.
If all messages were equally likely, they would all have the same surprise.
But if the messages have different probabilities, the less likely messages convey more information.
So return to the fire alarm example.
Any given moment, suppose the fire alarm has a probability 0.001, one tenth of one percent
of sounding.
The probability of not sounding is 0.999, 99.9 percent.
The surprise of no fire is log two of 1 over 0.999, which is 0.0014 bits, not very much.
But the surprise of fire is log two of 1 over 0.001, which is 9.97 bits, a lot more.
How about the letters of the alphabet?
With 27 characters, the Hartley-Shannon entropy is log two of 27, or 4.76 bits.
That would be the information content of a letter if all letters were equally likely.
Based on our analysis, however, some characters are more surprising than others.
The space character occurs with probability 0.193, so S of the space is 2.37 bits.
The letter E occurs with probability 0.1, so S of E is 3.32 bits, and so on down to
the letter Z. Its probability is 0.00034, so the surprise is 11.5 bits.
When we think of all letters as equally likely, that's a zero-order letter-by-letter model
of English.
A text-generating machine like that wasn't very realistic.
When we take into account the different probabilities for the different letters, that's a first-order
letter-by-letter model.
The output of that machine that uses that model is only a little better.
For example, that Q near the beginning is followed by a C, but an English Q is almost
invariably followed by a U.
In other words, the letters in English text are not independent of each other.
Each letter tells us something about the letters around it.
If each letter is a random variable, we need to consider the relationships between random
variables, how they are correlated with each other.
It helps to think about a simple example.
The weather.
At a certain time of year, the sky might be either sunny or cloudy, and the temperature
might be either warm or cool.
So we have two variables, the sky condition S, which includes sunny and cloudy, and the
temperature T, which includes warm and cool.
The actual weather is a combination of the two variables, forming a joint random variable
ST, which includes four possible events, sunny-warm, sunny-cool, cloudy-warm, and cloudy-cool.
Each of these has a probability which we can put in a table.
The most likely weather is sunny and warm, and the least likely is cloudy and warm.
As usual, the sum of all the probabilities is exactly one.
But what if we don't care about temperature?
What is the probability that the weather on a given day will be sunny, whatever the temperature?
There are two distinct ways that can happen, sunny and warm, or sunny and cool.
The rules of probability tell us that we just add the probabilities for the possible alternatives.
The total probability for a sunny day is 0.6.
And in the same way, the probability of a cloudy day is just 0.1 plus 0.3, which is 0.4.
We get the probability distribution over random variable S just by adding up the rows of our table.
This is called the marginal distribution because we can write these probabilities over in the
margin of our table.
And we can do the same to find the probabilities for the temperature T, writing P of warm and
P of cool at the bottom of the columns.
Let's state our new probability rule more formally using our notation for summing things up.
Rule number 3, suppose we have a joint variable x, y, including possible joint events, x, y.
And the overall probability of x is just the sum of P of x, y over y.
And there's a similar rule to find P of y.
Suppose the weather is warm.
Given that information, what is the likelihood that the weather is also sunny?
We would like to express this as a probability, written like so.
P of sunny, vertical line, warm.
That vertical line means to assume that everything that follows is true, and find the probability
of what comes before.
We read this, the probability of sunny given warm.
It is called a conditional probability because it only applies under the condition that it
is in fact warm.
We know that it is warm 50% of the time, and within that 50% we find that it is both sunny
and warm 40% of the time.
Since 0.4 divided by 0.5 is 0.8, we can say that it is sunny 0.8, or 80% of the warm days.
Thus P of sunny given warm is 0.8.
And in general, for any two events, x and y, P of x given y is equal to P of xy divided
by P of y.
If we know nothing about the weather, the probability P of sunny is 0.6.
But if we know that it is warm, then probability of sunny given warm is 0.8.
Knowing the sky condition changes our probability for temperature.
This implies nothing about cause and effect, but it does tell us that the variables s and
t are not independent of each other.
Okay, in English the successive letters are not independent either.
The next letter depends to some extent on the last letter.
By studying pairs of letters in our reference text, we can match not only the probabilities
for the individual letters, but also conditional probabilities for the next letter given the
last letter.
This is a second order letter by letter model of English, and that kind of machine yields
an output like this.
That is a better simulation of actual English text.
It is almost pronounceable, and there are even a few actual words like wag and he and
of.
There is a neat trick for producing this kind of simulated English.
Back in the 1940s, when Claude Shannon was trying to do this sort of thing, he did not
have the services of a computer, so he figured out a shortcut to generate text like this
without actually doing all that counting and calculation.
Start by picking a letter at random from the reference text.
Suppose that letter is W. What's the next letter?
Shannon said open your reference text to a random place and then proceed until you get
to the first W you find.
Now write down whatever letter follows it.
Then repeat the same procedure with that letter to generate the next one, and so on.
That procedure generates a text with the right frequency of letter pairs.
Shannon also did this for triples of letters.
In a third order letter by letter model of English, all the three letter combinations
occur with the right probabilities.
The next letter depends on the last two letters.
If we make a text generating machine like that using the return of Sherlock Holmes as
our reference, the output looks like this.
That feels even more like realistic English text.
It includes several actual words, including some longer words like fiend and panda.
The statistical properties of English can help us communicate.
Let me introduce you to a computer program called Dasher, which was created by Professor
David McKay and his team at the University of Cambridge.
The idea of Dasher is to allow people with severe physical disabilities, people who cannot
use a regular keyboard, to create text on a computer or a smartphone.
All you need to do is steer a cursor on the screen.
The letters, spaces, and punctuation marks appear in boxes to the right.
And as you steer toward one box, your view zooms in.
Once you enter the box, that's your letter.
And within that box, there are other boxes for the next letter, and so you zoom in some
more.
In effect you zoom in on the word or phrase you want.
As you go along, the text you've already written appears at the top.
And here's the beautiful trick, the size of the next box.
It depends on the probability of the next letter given the letters you've already chosen.
The more likely the letter, the bigger the box.
And so the quicker and easier it is to steer yourself toward it.
Dasher is a little like one of our text-generating machines, but instead of producing the text,
the program's mathematical model makes it easier for you to produce the text.
It is also a little like cell phones or word processors that attempt to predict what word
you're trying to type before you finish it.
If the system guess is right, you can quickly skip ahead to the next word.
But Dasher is more sophisticated and flexible than those systems.
You always have all of the possibilities before you.
It just takes a bit more time to reach the less probable one.
So let me demonstrate.
My copy of Dasher happens to have been trained using our reference text, the return of Sherlock
Holmes.
My expectations about English are based on that book.
For that reason, it turns out to be very easy to enter the name Sherlock Holmes.
Going a little more slowly, I can continue to write a complete thought about him.
Sherlock Holmes solved the crime.
When you use Dasher over a period of time, two things happen.
First, you get much more practice at steering toward the right letters, and you can drive
a bit faster.
And Dasher itself analyzes what you write using that data to update its internal model
of the English language.
Both of these things make it quicker and easier to create text with the program.
I'm told that an experienced user can write more than 20 words per minute.
I myself have never quite gotten that fast, but it is fun to try.
The Dasher program is freely available, so I encourage you to look it up and experiment
with it.
A few minutes with Dasher can teach you a lot about the statistical properties of English.
Let's try something different.
Up to now, we have been thinking about English as a series of letters, one by one.
Suppose instead we think about English as a series of words.
Our text-generating machine will produce words, one by one.
When we analyze the return of Sherlock Holmes, we count slightly more than 8,000 different
words.
We also note that some words are a lot more common than others.
The most common words are about what you'd expect, small words that show up in many different
sentences.
Here is the top ten list, the, I, and, of, A, to, that, in, was, it.
In more formal writing, the personal pronoun I does not appear often, but aside from that,
this is a pretty typical list for English.
Our particular reference text is not entirely average, of course.
The 23rd most common word in the book is Holmes.
Watson shows up on the list at number 74.
Now here's something curious, something that was first observed by the Harvard philologist
George Kingsley Zipf in 1935.
Zipf observed that if you take a book and count the number of times each word appears,
then arrange that list by the rank of the word, the first most common word, the second
most common word, and so on, a mysterious pattern emerges.
Roughly speaking, the number of occurrences of a word is inversely proportional to its
rank in the list.
That means that the second most common word occurs about half as often as the most common
word, and the third most common word occurs about one-third as often, and so on.
Word number 100 in the list occurs about twice as often as word number 200.
This empirical fact is called Zipf's law, and variations apply to other languages also,
and to a surprising range of other sorts of data.
Lots of people have speculated why.
Whatever the reason, Zipf's law does work fairly well for the return of Sherlock Holmes.
For example, the word crime is number 218 on our list and occurs 60 times.
Twice as far down the list at number 436 is the word master, which occurs 28 times, about
half as often.
Or here's another example.
Murder shows up at number 327, occurring 37 times in the book.
Twice as far down the list at number 654 is the word times, which occurs 19 times, again
just about half as often.
Using these data, we can construct a first-order word-by-word model of English, a text-generating
machine that produces entire words with the observed frequencies.
Here is an example of the output of that machine.
As a simulation of real English, that isn't too bad, and we can make it even better.
Using Shannon's random lookup trick, the same trick we use for letters, we can also
create second-order word-by-word and third-order word-by-word models of English.
These models take into account the correlations among pairs of words, or triples of words.
Here are some samples of the output of text-generating machines like these.
First, the second-order word-by-word model.
Next, the third-order word-by-word model.
These are randomly generated, so of course they do not make sense, but information theory
is not about meaning, it is about the sort of message that might occur.
Each of our successive text-generating machines gives a recognizably better approximation
or simulation of a real English text.
Each of our models captures more of the patterns and regularities of actual language.
The information sources considered in Shannon's theory are sources of this kind.
They generate messages according to probabilistic rules.
That isn't exactly the way that you or I would produce English text, but as we've
seen, a probabilistic model can do a pretty good job, especially if we are willing to
consider second and third-order versions that take correlations into account.
So let's consider one of Shannon's information sources.
There is a set X of possible messages containing indifferent alternatives.
Message little x occurs with probability p of x.
Now to keep things simple, we will still pretend that successive messages are independent of
each other, that is, we have a first-order message source.
Previously, we said that the amount of information produced by the source was the Hartley-Shannon
entropy, which is just log 2 of m.
That measure of information was connected to everything from password security to the
number of bits we need in a binary code.
But now the different messages have different probabilities, and the amount of information
in each one is the logarithm of the reciprocal of the probability, the surprise.
That's different for different messages.
Yet we still need a measure of entropy, the overall amount of information in the source.
But how is the surprise of the message related to the entropy of the source?
Shannon's answer is simple and elegant.
Entropy is the average surprise.
To write down the formula, I need to tell you how to take averages.
As before, we start with a random variable x containing possible events x, but now each
event also has a numerical value called f of x.
For instance, we might roll a cubicle die and count how many dots appear on the top
surface.
That could be any whole number between 1 and 6.
So what is the average number?
And here's the rule, our fourth and last rule for probability.
Rule number 4, if the events x of a random variable x have values f of x, then the average
of f is the sum of the probabilities of each event times its numerical value.
Again, that sigma means summing things up.
So for the die, each phase has an equal probability 1-6.
So the average number of dots is three and a half.
Sometimes people carelessly say that this is what we expect when we roll a die.
But that's not right, we never see three and a half dots on top.
Still, that number should be the long-term mean value of the numbers that we obtain when
we roll the die a great many times.
What's the average surprise?
In source x, the message x occurs with probability p of x, and has surprise log 2 of 1 over p
of x.
So Shannon's formula for the entropy, the average surprise, is the sum over x of p of
x times log 2 of the reciprocal of p of x.
Let's try it out in an example.
Suppose our source x has three different possible messages, a, b, and c.
Each a is the most likely with p of a equal to one-half, that's a surprise of one bit.
Messages b and c are less likely with equal probabilities one-fourth, that's a surprise
of two bits.
The average surprise is easy to work out, it's just 1.5 bits.
If the same three messages had been equally likely, then each one would have had a probability
of one-third and a surprise of 1.585 bits, so that would also be the entropy.
The new Shannon formula for entropy agrees with the old Shannon-Hartley formula when
the probabilities are equal.
In other cases, the Shannon entropy can be less.
Shannon's formula for entropy is a beautiful equation.
It was this new definition that enabled Shannon to move far beyond the tentative ideas of
his predecessor, Hartley, and it will be many lectures before we have explored even a fraction
of the secrets of the Shannon entropy.
What does Shannon's formula tell us about English text?
If we take a letter-by-letter view, the old Hartley-Shannon entropy gives us 4.76 bits
per character.
If we take into account the letter probabilities that we estimated from the return of Sherlock
Holmes, we find a lower value, around 4.07 bits per character.
On the other hand, if we take a word-by-word view of English, taking into account the different
probabilities of different words, we end up with 9.25 bits per word, less than 10 bits.
The average English word is about five letters long, so this suggests that the real entropy
of English is less than two bits per letter.
What does two bits per letter actually mean?
Remember, we previously found a connection between entropy and coding.
Entropy is the answer to the practical question of how many binary digits we needed in our
code words to represent the messages.
Now Shannon has given us a new, more general definition of entropy.
Entropy is the average surprise of a message.
With unequal probabilities, that entropy is less.
Does that mean we can use fewer binary digits in our code words?
Could we represent English with a code that only uses two bits per letter?
Is there still a connection between entropy and coding?
There is, but it's a big enough story that we need another lecture to tell it fully.
