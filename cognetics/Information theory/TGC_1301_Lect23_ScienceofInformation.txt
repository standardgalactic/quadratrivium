John Wheeler was an American physicist famous for his bold ideas and insights about the
deepest problems in physics.
With Niels Bohr, he developed the first theory of nuclear fission and was a key figure in
the Manhattan Project and afterward.
In gravitational physics, he revived interest in Einstein's general theory of relativity,
introducing the term black hole into the physicist's lexicon.
And toward the end of his long and storied career, he became ever more fascinated with
the meaning of quantum mechanics and the profound connections between physics and information.
That's when I knew him.
He was my teacher.
I became his last Ph.D. student.
Wheeler had a way of posing impossibly big questions, crazy sounding questions, seemingly
unanswerable questions, yet they were questions that it was impossible not to think about.
It was the thinking he was after.
He used to say that we physicists have grown too comfortable with our theories.
We lack a vital sense of desperate puzzlement about the strange universe we live in.
Our theories are not necessarily wrong, but they are telling us things, fundamental things
that we have not yet heard and understood.
And one of those unheard messages was about information.
So here's our classic Wheeler conundrum.
What are the laws of physics?
To a physicist, they are mathematical equations.
So get a big piece of paper and write down the most beautiful, elegant mathematical theory
you can think of.
Ponder your equations for a while.
Now do it again.
On a new piece of paper, write down even better equations, more beautiful, more compelling.
Repeat this process as often as you like.
Now take your last version, the best and most wonderful mathematical theory imaginable.
Set it before you and say, fly.
Nothing happens.
The equations are just equations.
They do not become real.
Therefore, Wheeler said, even the greatest mathematical theory of the cosmos is missing
something, the principle by which it can become real.
What could that be?
Wheeler did not claim to know the answer, but he thought there were clues scattered throughout
the fundamental theories of physics.
And one important clue, he believed, came from quantum theory and the central role that
information plays in that theory.
And always ready with a memorable phrase, Wheeler called his idea, it from bit.
We can find Wheeler's clue in the two-slit experiment.
As we saw, depending on how we set up the experiment, we can either observe the quantum
interference or find out which slit the photon passes through.
Bill Wooters and Wojtek Zurek, about the time they were proving the no cloning theorem,
also analyzed the two-slit experiment.
They showed that there is a strict information trade-off, the more which-slit information
we obtain, the less quantum coherence leading to the interference effect.
It is no accident that the two of them were working for John Wheeler at the University
of Texas when they did their work.
Wheeler noted that our own choice of experiment helps to determine which phenomena become
real in the two-slit experiment.
We are not merely detached observers, but participants in the process.
If we do not observe the slits, quantum interference is real, but there is no answer to the question
which slit.
If we do observe the slits, that question does have an answer, but the interference effect
is gone.
And Wheeler pointed out something even more unsettling.
We can wait to decide which experiment we are going to do until the last moment, when
the experiment is 99 percent over.
It works like this.
If we want to see interference, we do the usual experiment as we have already described.
But if we want to determine which slit the photon went through, we can insert a lens
just before the screen.
The lens deflects the light waves and forms an image of the slits on the screen.
The photons now land in one of two spots corresponding to the two slits.
The experimental apparatus might be very long, and we can decide whether or not to insert
the lens at the very last nanosecond, and I do mean that literally one actual laboratory
realization of this used a fast electro-optical switch instead of a lens, and the switching
time between the two experiments was around a billionth of a second.
Wheeler said we can choose whether or not the photon passed through a definite slit
long after the photon went through.
He called this a delayed choice experiment.
The experiment is not over until the photon is registered by one of the detectors at the
screen that produces an electrical impulse, a click that can be recorded and studied.
Niels Bohr said that an irreversible act of amplification is necessary, bringing things
from the microscopic realm to the macroscopic realm so that the result of the experiment
can be communicated in plain language.
Until that happens, we cannot describe too much reality to the photon.
Wheeler put it this way.
No phenomenon is a phenomenon until it is an observed phenomenon.
The quantum information in the microscopic world, that private world of superposition
and interference and entanglement, must be converted into classical information, Shannon
information, information that can be copied and processed and shared with the rest of
the universe, only then can we say that things have become unambiguously real.
Wheeler called it the elementary quantum phenomenon, this chain of quantum events that leads to
the creation of unambiguous information, information expressible as 1s and 0s, yeses and noes.
The elementary quantum phenomenon need not be localized in space or time, it stretches
from the source of the photons to the click of the detector.
It seems like a small thing, but the world is made of a stupendous number of such elementary
phenomena, not particles and forces, not fields, not even space and time, but information,
it from bit.
If the universe is made of information, how much information does a piece of the universe
contain?
That's the question that Jacob Beckenstein asked.
Beckenstein was a student of Wheeler's at Princeton, so suppose we have a sphere one
meter in radius.
How much information can we stuff into that sphere?
However much it is, common sense suggests that the maximum storage capacity of a region
of space depends on its volume.
More volume means more room for particles to encode the information.
A sphere of twice the radius, 2 meters, should be able to store 8 times as many bits.
But Beckenstein said that this is wrong.
To store more and more information in the sphere, we have to add more and more energy.
If we put too much energy in, the equivalent mass of the sphere will be large enough to
form a black hole.
A black hole of course is a star or other large mass that has collapsed under its own
gravity until nothing, not even light itself, can escape from it.
It is surrounded by a mathematical surface called the event horizon that can only be
crossed by information inward, never outward.
So once we have made a black hole out of our data storage region, the information is lost
and that is what limits us.
Now Beckenstein had previously made an important discovery about black holes.
He found that they have a thermodynamic entropy.
The entropy is given by Boltzmann's formula, S equals K2 log 2 of M, but that number M,
instead of counting possible microstates, counts possible histories.
That is, it represents how many different ways, from how many different configurations
of matter and energy the black hole could have formed.
That is exactly the information that has been lost in the hole.
Beckenstein showed that black hole entropy is actually proportional to the surface area
of its event horizon.
That is, entropy of black hole is a constant times area of horizon.
The exact value of the constant was later worked out by the English physicist Stephen Hawking.
So if we think of black hole entropy as bits of lost information, each bit occupies is worth
an area of 7 times 10 to the minus 70th square meters.
A black hole the mass of our sun is three kilometers in radius, its lost information
is over 10 to the 77th bits.
Based on black hole entropy, Beckenstein concluded that the information storage capacity of any
region of space, that is the maximum number of bits we can put inside without forming
a black hole, is not limited by its volume, but by its surface area.
The limit is 7 times 10 to the minus 70th square meters per bit.
Now that is an almost inconceivably small area, so let's picture it this way.
Start with the surface area of the entire earth.
Now shrink the earth until it is the size of a uranium nucleus.
Now consider its surface area.
Now you will have to shrink it again by the same factor before the area equals the fundamental
area of one bit.
Because of a very faint quantum radiation that Stephen Hawking discovered, a black hole
in empty space will slowly, very, very slowly, lose mass and shrink.
It gradually evaporates away.
Eventually, and the exact details of the final stage are not well understood, eventually
the black hole is gone.
So imagine the life cycle of a black hole.
It starts out as a bunch of matter, a collapsing remnant of a massive star.
The whole forms, and all of the information encoded in that matter disappears behind the
event horizon.
After things settle down, the black hole emits radiation very feebly for a long time, and
eventually the hole is gone, and only some very sparse radiation remains.
So here is the question, where did the original information go?
It did cross the event horizon into the black hole, but eventually the black hole goes away.
There is some Hawking radiation around, but that seems to have nothing to do with the
material that went into the black hole.
This is called the black hole information problem, and it is an especially naughty problem
in theoretical physics.
There are two possible answers.
First, the information might simply be lost, really destroyed, in the black hole.
The leftover radiation is random and independent of the original material.
The second answer is based on the principle of microstate information, which you may recall
from Lecture 16.
That states that microstate information is never really lost.
At worst, we lose access to it.
This answer therefore says that the emitted Hawking radiation carries, perhaps in scrambled
form, all the details of the exact configuration of matter that formed the black hole in the
first place.
The information is never actually lost.
It goes into the black hole, only to be re-emitted in another form, quadrillions of years in
the future.
I should point out that this is an entirely theoretical problem at present.
We do not have any black holes in the laboratory to experiment with, and we have never actually
observed the very faint Hawking radiation, but it is exactly the sort of problem that
lets us test our fundamental concepts to the limits.
The black hole information problem was the subject of a famous wager, famous among black
hole physicists anyway.
The wager was between Caltech physicist John Preskell on one side, and Stephen Hawking
and another Caltech physicist, Kip Thorne, on the other.
Preskell bet on the principle of microscopic information.
He said that every bit of information that goes into a black hole eventually comes out.
Hawking and Thorne took the other side.
They bet that a black hole really destroys information.
By the way, I don't think Hawking ever actually believed his side of the bet.
He has a distinctive sense of humor, and he often hedges his bets by taking the more interesting
side rather than the more likely side.
For instance, he bet against the discovery of the Higgs boson.
Well, in 2004, after making his own study of the black hole information problem, Hawking
claimed to be satisfied and conceded his part of the bet.
Preskell received an encyclopedia of baseball as his winnings.
Kip Thorne thought Hawking's concession was premature, and so his part of the joint bet
remains unresolved.
Regardless of the outcome of the black hole information problem, that the connection between
information and area is very strange.
Why should the information content of a region of space be limited by its surface?
It's literally like measuring a book by its cover.
Well perhaps it indicates something about the nature of information itself.
We have spoken about information as if it were an intrinsic property of an object.
This photon carries one bit of information.
That computer contains such and such an amount of data.
But Wheeler's elementary quantum phenomenon suggests that information only becomes unambiguously
real when it can be communicated and shared with the rest of the universe.
Or again, consider quantum entanglement, which is not really communication, but rather an
information relationship between particles.
So perhaps the right way to think about information is as a relation, not an intrinsic property.
Two particles are related to one another.
One part of the universe is related to all the rest.
This relational idea accords with our intuitive concept of information.
We say that a message carries information not just because of its contents, but because
those contents are correlated with something in the outside world.
The message is about something.
So if information is fundamentally relational, then it makes sense that it is limited by
surface area.
The surface is the boundary between the inside and the outside, between a region of space
and the rest of the universe.
The information relationship must reach across that boundary, and there is only so much room.
It takes 7 times 10 to the minus 70th square meters per bit of relation.
The logical endpoint of Bekenstein's ideas is a speculative theoretical concept known
as the Holographic Principle.
This was proposed by the Dutch physicist and Nobel laureate Gerard Hoefd.
The Holographic Principle states that we can regard the information in any region of space
as existing on the boundary surface of that region.
So earlier in this course we discussed the information in a jar of air, microstate information
about the air molecules, add to that information about the light and gravitational fields in
the jar.
There seem to be a great many bits inside the jar, but Hoefd says it is equally valid
to think of those bits as existing on the jar's surface.
After all, any information exchange with the interior, when we look inside the jar and
so on, is always mediated by that surface.
Our picture of things in the interior is a reconstruction based on those exchanges.
It isn't that the interior of the jar is an illusion exactly.
The two ideas, information in the interior and information on the surface, are equally
valid, they are complementary pictures of the reality and they are equally valid for
any region of space.
The world is, like a hologram, a little less three dimensional than it appears.
We cannot yet say where the Holographic Principle will lead.
Many physicists today regard it as the key to a deeper kind of physics.
Wealer always regarded black holes as another clue in the missing something in physics.
If the elementary quantum phenomenon is the birth of information, the black hole, perhaps,
is its death.
No wonder Wealer liked to call black holes a source of enlightenment.
What about the entire universe?
What is the information content of that?
Well if information is truly relational, if it is about the relation of the inside to
the outside, then the Holographic Answer for the whole universe, which has no outside,
is zero.
Yet there is an interesting calculation to be made.
So just as a black hole is surrounded by an event horizon, so we, in our expanding universe,
are surrounded by a cosmic horizon.
Think of it as a tremendous sphere, billions of light-years in diameter, with us at the
center.
Now there is nothing special about us being in the center.
Anyone living in the universe would construct their own surface which would be centered
on them.
So events beyond that cosmic horizon are not yet visible to us, or not yet at least.
The universe has not existed for long enough for that information from those regions to
have reached us.
The horizon does grow over time.
We see more and more of the universe, but at any given moment the observable part of
the universe is finite, and the cosmic horizon is its boundary.
So if we divide the area of that surface by the tiny area for one bit, we get a huge number
of bits, about 10 to the 120th bits.
But what does a number like that mean?
Well in 2001, Seth Lloyd from MIT tried to figure that out.
He said we can regard the universe as a gigantic information processing system.
In effect, the universe is a computer, and its evolution is a computation.
We can try to figure out how big that computation is.
How many bits are involved?
How many basic logic operations?
This would be an interesting number, especially if you hold the rather fantastic idea that
our universe is a simulation running on a computer in some higher unimaginable realm.
A truly universal computer.
That calculation will tell us how big that computer has to be to handle the simulation.
The cosmic horizon number, 10 to the 120th bits, is possibly too large, since it should
give the maximum amount of information that can be contained within a volume the size
of our universe.
But the universe is mostly empty.
Its information content is less.
Lloyd said that we can estimate the number of bits in the cosmic computation by estimating
the total entropy of the contents of the universe.
And the lion's share of that entropy is actually not in stars or gas, but in the cosmic microwave
background, the faint radiation left over from the hot dense era soon after the Big
Bang.
The entropy of that radiation is about 10 to the 90th bits.
That's a lot smaller, but it is still gigantic.
Lloyd next turned to estimating the number of basic computer operations that the universe
has executed since the Big Bang, and now we're counting ops rather than bits.
And he noted that energy is required to do any computation.
That energy need not be lost as waste heat.
The computation can be thermodynamically reversible, but energy needs to be present
for the operations to take place.
And the more energy there is, the more rapidly those operations can happen.
So armed with the idea, Lloyd estimated that the total number of basic computer operations
on the 10 to the 90th bits in the universe has been about 10 to the 120th ops.
The same as the bit number for the cosmic horizon.
And in fact, Lloyd showed that this is no coincidence, but would hold true in any expanding
universe like ours.
The horizon bit number equals the cosmic op number.
This sort of calculation can be a little dizzying, and it raises some unsettling questions.
For instance, Lloyd's calculation suggests that the universe is fundamentally finite.
It contains a finite amount of information.
It has performed only a finite number of basic logic operations.
The numbers involved are large, but even a large number is very far from infinity.
Mathematics, on the other hand, is all about the infinite.
There are infinitely many points on the line.
The irrational number pi has infinitely many decimal places.
And every computation we do is done within the universe.
It is a small piece of the cosmic computation, and is therefore limited by the cosmic limits.
That being true, what does it actually mean to say that pi has an infinite number of decimal
places?
Almost all of those digits can have no meaning, no existence in the physical world.
The quest for an information basis for physics has recently gone in an interesting new direction.
The idea is to try to identify where quantum mechanics itself comes from.
Quantum mechanics has a complex mathematical machinery.
It's one of those beautiful sets of equations in Wheeler's story.
But where did all that complicated stuff come from?
Why is quantum mechanics the way it is?
What makes it so special?
Perhaps we can answer this question by figuring out the minimum logical basis for quantum
mechanics.
What is the smallest set of postulates of the simplest kind that would lead to the whole
quantum mathematical machinery?
Can we derive quantum mechanics from axioms about information?
We now have not just one answer to this, but several.
Several physicists have found a number of lists of axioms with an information flavor
from which you can derive the whole theory.
There are usually about half a dozen axioms in a set.
All but one of them are very straightforward.
They include the no signaling principle, the idea of data compression, the ability to assemble
two or more systems into one larger system, and so on.
The list varies.
The point is that all of these axioms are fairly intuitive.
They're also perfectly consistent with classical physics and ordinary Shannon information.
Then there is always one more axiom, the special sauce that turns everything into quantum mechanics.
For Lucian Hardy at the Perimeter Institute in Canada, that special sauce is the idea
that you can go continuously from one information state to another.
Not that isn't true in Shannon's theory.
You have to jump discontinuously to go from zero to one.
But a qubit has superposition states between zero and one, and that allows the change to
be continuous.
For the Italian physicist Giacomo Dariano, Giulio Cirabella and Paolo Peronotti, the
special sauce is the idea that any noisy state of a system is really just part of a pure
entangled state of a larger system.
In other words, entropy is always about entanglement.
And there are others.
The hope is that by identifying the essential information principle that determines quantum
theory we will gain an understanding of the information basis of the quantum world.
We will be on our way to it from bit.
Frankly though, I am not sure that Wheeler would entirely have approved.
He did not think of physics in terms of overly formal postulates and theorems.
Now don't get me wrong, John Wheeler was a superb mathematician who introduced some
audacious mathematical ideas into physics.
Yet he believed that where the deep questions are concerned, too much formalism is a blind
alley in the end.
He used to say, axiomatization is the last stage before rigor mortis.
We cannot at present give good answers to Wheeler's deep questions.
Indeed, we are not even sure that we have properly understood the questions.
But the connection between information and physics has indeed led us in some bind bending
directions.
The whole subject of information and physics really had two godfathers, two people whose
ideas and influence shaped the work of thousands of others over the last two decades.
They set the agenda and inspired the rest of us.
Those two were Rolf Landauer and John Wheeler.
Their approaches were very different.
Landauer said, information is physical.
He pioneered the idea of using the laws of physics to understand how information is acquired,
processed, stored, and erased in the physical world.
Wheeler on the other hand said, physics is informational.
He urged us to dig beneath the elegant mathematical laws to uncover the true foundation beneath.
He believed that that foundation was built from information.
My own work and that of my research colleagues has pretty much been firmly of the Landauer
kind.
We have tried to understand the concepts and laws of information that arise from quantum
mechanics.
The whole of quantum information science, quantum communication, entanglement theory,
quantum cryptography, quantum computing is exactly that quest.
And we have discovered some amazing things about the physics of information.
And a surprising number of us are motivated by Wheeler's vision.
It is not too fanciful to say that we have Landauer on our minds and Wheeler in our dreams.
For us, every new discovery from the delayed choice experiment to black hole entropy to
the holographic principle confirms the central place of information in the fundamental laws
of nature and brings us a little closer to the dream of it from bit.
