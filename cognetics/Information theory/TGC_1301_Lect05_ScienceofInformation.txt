Bits are cheap.
After 50 years of Moore's Law with its exponential improvement in computer information technology
it has become absurdly inexpensive to store information.
For instance, here is the portable storage drive I use to make backup copies of my own
computer data.
It's a couple of years old.
The equivalent item today stores 1 terabyte of data, 8 trillion bits, and can be purchased
for under $50.
To express how cheap that is, I need a new unit of money.
The nano-scent, one billionth of a penny, a bit of data storage today costs half a nano-scent.
By the time you hear this, storing a bit will probably be cheaper.
If bits are so darn cheap, why would we care about using them efficiently?
The truth is, we always seem to find the need for however much information we can afford,
and then some.
When kilobytes are cheap, we want megabytes.
When megabytes are cheap, we want gigabytes.
And simple data storage is not the only task we need to do, and money is not the only cost.
Information is useless unless we can move it from place to place, and our time is more
valuable than a few pennies.
A basic wireless computer link can move several million bits per second.
At that rate, it would take weeks to transfer that one terabyte of data.
And sometimes, the available bit rates are much lower.
The communication link with the Voyager spacecraft, 20 billion kilometers from Earth, runs at
less than 200 bits per second.
So it can be important to find codes that pack as much information into as few bits as possible.
Sometimes we need data compression.
Shannon's theory is really about tasks and resources.
We are given a task to convey messages from one place to another, or to store them for
later retrieval.
We have some resources available to us, binary digits, transmitted or stored as electrical
signals or holes in paper tape, or tiny magnetic domains on a computer disk.
The basic question that Shannon's theory aims to answer is this.
What resources are required to accomplish the task?
In the last lecture, we learned to measure information by surprise.
For an information source x that produces message x with probability p of x, the surprise
s of x is defined to be log 2 of the reciprocal of p of x.
This is the amount of information in the message measured in bits.
An unlikely message carries more information than a very common one.
According to the Shannon entropy of the source x, called h of x, that's the average surprise
of the messages in x.
I want to look again at a simple example we used to illustrate this idea.
Our source has three possible messages, a, b and c.
Message a is probability one-half, while b and c each have probability one-fourth.
So the surprise of a is one bit, and the surprise of b and c are each two bits.
The average surprise, the entropy, is 1.5 bits.
Now suppose you know all about the information source x.
You know the actual probabilities for the messages as we've described them.
But I do not.
I have some other, possibly mistaken, probabilities in mind, called them q of x.
Perhaps I believe all three messages are equally likely, so that q of x equals one-third for
all three.
And I would say that each message has a surprise of log two of three, or 1.585 bits.
Now we start receiving messages from the source.
Sometimes I will be more surprised than you are.
When a occurs, your surprise is one bit, mine is 1.585 bits.
Sometimes I will be less surprised than you are.
When b or c occurs, your surprise is two bits, but mine is still 1.585 bits.
So who is more surprised on average?
Perhaps you can guess the answer.
In fact, I am more surprised than you are on average.
In the example, my surprise is always 1.585 bits, while your average surprise is only
1.5 bits.
And this is a general fact for any message source.
Your surprise in mind for a given message x are log two of one over q of x, and log
two of one over p of x respectively.
When we calculate the average surprise, though, we must take the average using your correct
probabilities, p of x, not q of x.
Your average surprise is the entropy h of x.
So we can write down a mathematical inequality.
The fellow with the wrong probabilities, me, is always more surprised on average.
The two sides would be equal to each other, of course, if my probabilities were the correct
ones.
This mathematical fact shows up all through information theory, so much so that it is
called the information inequality.
We'll see it again.
If surprise is our measure of information, some messages carry more information than
others.
Does that mean that they require more bits, more binary digits, to represent them?
We've not yet really discussed codes in which different messages are represented by different
numbers of bits.
It's an important idea, and it goes back more than 100 years before Shannon to the invention
of the telegraph in the 1830s by the American Samuel F. B. Morse.
Morse was not the first person to dream of sending messages by electricity, but he was
the one who made it work.
He did not start life as an inventor.
He was a well-known painter whose works included portraits of famous men like John Adams and
James Monroe.
And he was already in his 40s before he started thinking about using electricity for communication.
The inspiration was tragic.
While away from his home in Connecticut on a trip to paint a portrait of Marquis de Lafayette,
he received word that his wife had taken ill.
He headed home at once, but the news had traveled so slowly that by the time he arrived, she
had already died and been buried.
Morse understood that a way of sending messages very quickly would be a boon to humanity.
A few years later, after a discussion with a scientist acquaintance on a long sea voyage,
he realized that electrical signals could do the job.
Morse succeeded where others failed because he kept it simple.
A single pair of wires connected the sender and receiver.
The signal was just the on-off state of the input key.
For long distances, relay switches could be placed every so often to boost the current.
Morse got help refining and improving his designs from his partner Alfred Vail and from
the American physicist Joseph Henry.
In addition to the hardware problems, however, there was also what we might call a software
problem.
How could a meaningful message be conveyed by on-off, on-off?
Well, at first Morse tried to use a code book.
In the book, different words were represented by different numbers.
The telegraph operator would then just tap out the numbers on the key, 7, 3, 2, and the
operator at the other end would look up the words in another copy of the book.
In actual practice, this idea proved to be far too cumbersome.
So in the 1840s, Morse decided to send his messages letter by letter.
Each letter would be represented by a series of electrical pulses.
Those came in two varieties, a short one, dot, and a longer one, dash, that was three
times longer than the dot.
In terms of electrical signals, the dot was on-off, and the dash was on, on, on, off.
There were longer pauses between letters and between words.
The code was simple enough that an operator could learn to use it without having to look
things up.
But what pattern of dashes and dots should correspond to what letters?
Here, Morse and Vail did something clever.
They decided to use shorter patterns, shorter code words, for the more common letters.
And to find out which letters were more common, they went down to the local print shop.
Printers in those days assembled individual precast metal letters in a large form which
they then used in a press to print pages.
Over time, they had learned that they needed to have a bigger supply of certain letters
in their type cases.
We've already seen the variations in letter frequency.
Morse and Vail found that the printers had lots and lots of E's, plenty of T's and
A's, and so on.
Only small supplies of X's and Z's were needed.
So in the Telegraph code, what came to be called Morse code, the E is a single dot,
T is a single dash, A is a dot followed by a dash, and so on.
The code was designed so that messages would be conveyed in the shortest possible time.
In Morse's original code, which became standard in the United States for several decades,
a few letters had extra long dashes or little pauses in them, the Germans later simplified
the code by eliminating these and the resulting international version is the one we use today.
But Morse's basic idea still remains enshrined in the international Morse code.
The more common the message, or letter in this case, the shorter we should make the corresponding
code word.
That is how we can communicate with the greatest efficiency.
So how efficient can we be?
That is a question that requires some discussion.
Instead of dots, dashes, and pauses of Morse code, we'll build our code out of bits, ones
and zeros.
So there are two possible code words of length one, zero and one.
There are four possible code words of length two, zero, zero, zero, one, one, zero, and
one, one.
There are eight code words of length three, and so on, and we can use any of these in
our code and we allow different lengths for different code words.
So consider again our three-message information source with unequal probabilities.
Probability of A is one-half, while both B and C have probability one-fourth.
Let's try the following code, A is zero, B is one, C is zero-zero.
It looks promising.
Each message has its own code word.
And the more common message has one of the shorter code words.
But there is a serious problem with this code.
Suppose we receive a whole series of messages one after the other, each in this code.
We get a stream of bits, maybe it starts like this, zero, zero, one, one, one, zero, zero,
zero, one, zero, et cetera.
Our job is to decode this in the ABC alphabet, but right away we have a problem.
The first bits are zero, zero, one.
Does that mean AAB, zero, zero, one, or just CB, zero, zero, one?
There's no way to tell.
Both AAB and CB are represented by the same binary digits, zero, zero, one.
In other words, if we use our code for a series of messages, it fails the first principle
of codes.
Different messages are represented in exactly the same way.
We never ran into this problem before because our previous code words all had the same length.
If that length were three bits, then we take the first three bits for the first code word,
the next three bits for the next one, and so on.
We would know how to chop up a series of bits into individual code words, but now that the
code words might have different lengths, we don't know how to chop it up.
Why don't we just put extra spaces between the code words?
That's okay, but then we are no longer using a binary code.
We're using three symbols, zero and one, and a space.
In an electrical signal, how do we distinguish between off, zero, and a space?
The code words themselves must tell us how to divide up the stream of bits.
As we receive the bits, we need to be able to tell when we get to the end of each code
word.
That's the whole problem with the code we have proposed.
A zero in our code might represent an A, or it might be the first part of a C. Therefore,
if we are going to be able to recognize the end of a code word when we get to it, our
code must satisfy a special condition called the prefix-free condition.
No code word can be the first part, the prefix, of any other code word.
A code that obeys this requirement, a prefix-free code, is always okay.
We can always figure out where the divisions are between code words in any long string
of messages.
So here's a new prefix-free code.
A is zero as before.
B is one-zero.
C is one-one.
So consider that same stream of bits as before, zero, zero, one, one, one, etc.
The first zero must be an A, as is the second zero.
Then comes a one, which might be the start of either B or C. The next bit tells us which.
The one, a C. The bit after that is a one, but this time it's followed by a zero, so
that's B. And in this way, the series of bits resolves in a unique way into a series
of code words, zero, zero, one, one, one, zero, zero, zero, one, zero, meaning A, A,
C, B, A, A, B, etc.
No code word can be the first part of any other code word.
That means if we have a short code word, there are many longer code words that we simply
cannot use, all the ones that begin with that short prefix.
And that tells us something about the possible lengths of the code words.
But what does it tell us exactly?
This is one of those places in mathematics where there is an idea, a picture, and a mathematical
relation that go together in a really beautiful way.
The idea is the prefix-free condition we've been talking about.
So here's the picture.
It's what mathematicians call a binary tree.
We start with a point or a node, and from it we draw two lines off to the right to two
new nodes.
Then we do the same for each of those nodes, and each one has two more nodes to the right
of it, and so on as far as we like.
We'll say that the first node is at level zero, and the next two nodes are at level
one, and the four after that are at level two, and so on.
At level L we have two to the L nodes.
Now imagine that these lines are a set of water pipes going from left to right.
One unit of water enters the pipe at the first node, and at each node the water divides into
two equal streams.
So we can label each node by the amount of water out of that one unit of water that reaches
it.
Each node at level L gets an amount of water, one over two to the L, or two to the minus
L power.
And that makes sense because the one unit of water is spread out equally among all the
nodes at that level.
Now here's another way to label the nodes.
The first node gets a blank label.
The next two nodes are labeled zero and one.
The next four nodes, the ones on level two, are labeled zero zero zero one one zero and
one one, and so on.
Each node at level L has its own L bit label.
The nodes label gives directions for reaching that node.
You don't need any directions at the starting point, but at each node you have to decide
whether to continue on up or down.
So we let zero stand for up and one for down.
The node zero one one is reached by going up, then down, then down again.
And if you go on from that node, all the subsequent labels will start with zero one one.
Now consider the codewords of a prefix free binary code.
Each codeword appears somewhere on the tree.
A codeword of length L appears on level L of the tree.
But as you travel the tree from left to right, once you encounter a codeword, you can never
encounter another codeword further along the same branch.
So far as the code is concerned, once you reach a codeword, that's the end of the tree.
So for a simple example, take a look at the three codeword prefix free code we gave a
minute ago.
I've shaded the codewords so that you can see what I mean.
We can draw the same kind of diagram for any prefix free code.
Here's another one with seven different codewords.
Two of them have length two, three have length three, and two have length four.
So go back to our picture of water flowing in a branching series of pipes.
One unit of water comes in from the left and divides in half again and again.
Each codeword of length L receives an amount of water two to the minus L because it is
on level L of the tree.
Furthermore that water, the water that reaches one codeword, does not flow on to any other
codeword because no codeword is a prefix of any other one.
So what happens to our one unit of water?
It is divided among the codewords.
There might be some left over because there could be branches that never reach any codewords.
Some water, in other words, might leak out, but no matter what, if we add up all the water
that reaches the codewords, the result cannot be greater than one.
We've actually proven something marvelous, and here is the mathematical relation that
goes along with the idea and the picture.
Suppose we have a set X of messages, for which we make a prefix free binary code.
The message X has a codeword of length L of X, then the sum over X of two to the minus
L of X is less than or equal to one.
We might call this the water pipe inequality, but it is actually known as the Kraft inequality
after Leon Kraft, who was a graduate student at MIT when Shannon's Papers on Information
Theory first came out.
The Kraft inequality is a requirement on the lengths L of X of the various codewords.
It tells us that we cannot have too many codewords that are too short.
Let's apply the Kraft inequality to the two codes we devised for our three-message source,
the both the bad code and the good one.
The first code, the bad one, A is zero, B is one, and C is zero-zero.
That means that the codeword lengths are one, one, and two.
So the Kraft sum is five-fourths, but that is not less than or equal to one.
Looking at the tree diagram for the code tells us the problem.
Since this is not a prefix-free code, some water flows through two codewords.
We count that water twice.
That's the only way we can get more than one in the Kraft sum.
So that sum, even without the diagram, tells us that the code cannot be prefix-free.
In the second code, A is zero, B is one-zero, and C is one-one.
The lengths are one, two, and two.
That is a prefix-free code.
And sure enough, the Kraft sum is just one, consistent with the Kraft inequality.
If a code is prefix-free, then the Kraft inequality follows.
If the Kraft inequality holds, on the other hand, that does not guarantee that the code
is prefix-free.
A code with codewords one, one, one, and one, one, one is not prefix-free, but its Kraft
sum is seven-eighths.
However, satisfying the Kraft inequality does guarantee that there is maybe another code
with codewords of the same length that is prefix-free.
If the Kraft sum is actually less than one, that is if some of the water in the pipes
spills out instead of reaching a codeword, then we can always shorten some of the codewords
to make that sum equal to one.
So let's suppose we do that, so that the Kraft inequality becomes an equation.
None of our water spills out in our tree diagram.
The sum of two to the minus codeword length equals one.
This reminds us of a rule of probability, that any probability distribution must add
up to one.
So we can pretend that those numbers, two to the minus L of x, are actually probabilities
Q of x.
They aren't the actual probabilities P of x, but some other cockeyed fictitious probabilities.
We're just making them up out of the codeword lengths.
From our made-up probabilities, we can calculate made-up surprises.
The length L of x of the codeword for x is just the surprise of the message x, according
to our fictitious distribution Q of x.
But notice, the information inequality tells us that the average Q surprise is at least
as great as the actual average surprise, the entropy.
That means that the average codeword length can never be shorter than the entropy of the
information source.
That's an important principle.
It relates our measure of information, entropy, to the number of bits we need to represent
messages.
We saw a version of the same statement before, back when all of our messages had the same
probability and all of our codewords had the same length.
Now we know that it applies for sources with unequal message probabilities and prefix-free
codes with unequal codeword lengths.
In fact, this principle tells us how we can make the average length close to the entropy.
Let's go back to that three-message source and the prefix-free code we devised for it.
Consider once again our three-message source, and we can summarize what we found in a table.
Notice that for each message, the codeword length equals the surprise.
So the average length equals the entropy, in this case 1.5 bits.
This doesn't always work so perfectly.
Let's suppose we have five messages, A through E, with probabilities and surprises that we
can give in a table.
The entropy is about 2.2 bits.
The codeword lengths have to be whole numbers, of course, so we simply round the surprise
numbers up to find some links for codewords.
Then we use the tree diagram to construct a prefix-free code with those links.
Our codewords are 00, 01, 100, 101, and 1100.
The average codeword length for this code is 2.5 bits, which is only a little more
than the entropy of 2.2 bits.
Then we can tweak our code to do even a little better.
The open-ended lines in our diagram mean we are spilling water unnecessarily, so we can
shorten some of our codewords to plug those leaks and still have a prefix-free code.
The most efficient way of doing this is called a Huffman code, after another MIT graduate
student named David Huffman.
The Huffman code is the best prefix-free code we can find for any given case.
For example, the Huffman code gives codewords 00, 01, 10, 110, and 111 in our example.
This code has an average codeword length of just 2.25 bits, only a tiny amount above the
entropy of 2.20 bits.
The final step is to remember a trick from Lecture 3.
Coding whole blocks of messages together made the codes even a little more efficient.
The same is true in this case.
We can bring the average codeword length per message even closer to the entropy h of x.
So we have arrived at a momentous fact.
It's called Shannon's first fundamental theorem, and we can state it this way.
The entropy h of x of a source, the average surprise, equals the minimum average number
of bits necessary to code messages from that source.
No code can be more efficient than this, but we can find codes that do the job with about
h of x bits on average.
Those highly efficient codes make use of the regularities in the messages from the source,
using shorter codewords to represent more likely messages just like in Morse code.
How important is this?
Well, it is the basis for a very important process called data compression.
And as we have already seen, even in a world where bits are cheap, sometimes we want to
use as few of them as possible.
How few can we use?
It depends on the kind of data.
Text information, for example, is often represented using the ASCII code, with one byte, eight
bits per letter.
But from our discussion in the last lecture, we concluded that the entropy of English is
much less than that.
By considering letter frequencies, we found the entropy to be about four bits per letter.
By analyzing word frequencies, we estimated the entropy to be about ten bits per word,
or two bits per letter.
So Shannon's first fundamental theorem, the data compression theorem, tells us that
we should be able to find a code that compresses English text more efficiently than ASCII.
There are computer programs, called data compression programs, that do just this.
Roughly speaking, such a program analyzes a data file for its regularities and patterns.
What pieces of data occur most commonly?
What pieces of data are statistically likely to occur together, like the letter U, following
a Q?
Based on the analysis, the compression program devises a code to represent the data more efficiently.
The program creates a new file using the new code, and including a dictionary for the
code so that the original data can be restored.
Let me tell you about an experiment I did with a popular data compression program.
It's called GZIP, and it is a compression technique that was developed in the 1970s by
the Israeli computer scientist Abraham Lempel and Jacob Ziv.
For my data file, I'm using my computer copy of the return of Sherlock Holmes, which was
our reference text for studying language statistics in the last lecture.
Since I prepared that file for counting letters and words, it contains the English text without
punctuation marks and so forth, but it's just an example.
The file is called return.txt, and it takes up about 583,000 bytes, about 4.7 million
bits of my computer's memory.
Now I apply the GZIP program, and call the resulting file return.gz.
If you try to open this file in a text editing program, the program does not even recognize
it as text.
It's only about 205,000 bytes long, about 1.6 million bits.
Not approximately one-third as long as the original, less than three bits per letter
in the original book.
That's not quite down to Shannon's entropy limit, but it's pretty good.
I can store or transmit the compressed version, using much less memory or time, and when I
want to see the original, the GZIP program can restore it exactly.
GZIP compresses data by finding and exploiting regularities and patterns.
If there are no regularities, there could be no data compression.
So to prove that, I created a completely random data file called random.txt.
When I apply GZIP, I get another file of about the same size.
Indeed, it's a few hundred bytes larger.
Why is it actually larger?
Remember, the compressed file also contains a description of the code it's using.
No data compression technique is perfect.
The Lempel-Zeef method is often used because it is fairly effective and it works quickly.
But the problem of compressing text data is not actually a very difficult one.
The patterns are easy to spot.
For instance, since ASCII is originally a 7-bit code, but now written in 8-bit bytes,
the first bit of every byte is a zero.
And all of the regularities of English text are present as well.
Also, the data files are usually not very large to begin with, even the longest book
is only a few million bytes of text data.
There are other types of data that are much harder to compress effectively.
Images, audio, video.
Each of these can use up enormous amounts of memory space and communication time.
It's fair to say that without extraordinarily effective ways to compress these kinds of data,
the Internet as we know it in the 21st century would be quite impossible.
Yet the problem is so difficult that Shannon's theory by itself cannot solve it.
We need another ingredient, and that ingredient is a better understanding of ourselves.
Shannon's first fundamental theorem sets the ultimate limits of data compression, but
that compression is assumed to be perfectly faithful.
We can always exactly reconstruct every bit of the original data from the compressed version.
For some purposes however, like images, audio and video, we don't always need that kind
of perfect accuracy.
The compressed version might only need to contain some of the original information.
But which information?
What must be included and what can be safely ignored?
The answer depends on the way we human beings perceive the world around us.
So next time we will turn to the role of human perception in data compression.
