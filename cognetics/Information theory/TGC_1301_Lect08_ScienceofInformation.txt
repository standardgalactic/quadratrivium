Optical disks for music and movies.
Digital computer operating systems.
Communication with spacecraft billions of miles away.
None of these things would be possible without the process of error correction.
It's one of the most amazing and most underappreciated stories in the science of information.
And it begins with human language.
Human language appears to be very inefficient, as we saw back in lecture four when we analyzed
the statistical properties of English text.
If we analyze that text letter by letter, we find that the letter entropy, the entropy
content of each letter is nearly five bits.
This is why the Baudot code used five binary digits per letter.
If we analyze the text word by word, however, taking into account that some words are very
common, we find that the word entropy, the information content of each word is less than
ten bits.
The average length of a word is about five letters.
So that means we are using around 25 bits, five letters times five, times five bits per
letter, to express a message that contains less than ten bits of information.
English in short is very redundant.
And the same thing is true of every other human language.
Every one of them uses many more symbols than the messages seem to require.
More letters of text, or more basic sounds, more phonemes of spoken language.
Here's a different way to state the same fact.
There are a lot fewer English words than we might expect.
With 26 letters, the number of five letter groups is 26 to the fifth power, which is
almost 12 million.
How many actual five letter words are there?
Well, one good source is the official tournament word list for the popular word game Scrabble,
which includes every word that is legal to play in the game.
That list contains around 9,000 five letter words, and 9,000 is a lot less than 12 million.
Many of those unused words, of course, are completely unpronounceable, like OKMBF.
But even if you only include pronounceable words, there are many more possibilities than
English makes use of.
Many of them are used in other languages.
Vorbi is not an English word, but in Romanian it means to speak.
So picture a vast space of possible words.
The space is vast because there are lots and lots of arrangements of letters.
The actual words used by English are scattered very sparsely through this space.
Why do we use so few of the words?
Or to ask the same question from the other direction, why don't we use fewer letters
for the words?
That would make the space of possible words not so vast, but there would still be plenty
of room to fit our vocabulary into it.
In fact, the redundancy of English is no flaw.
It serves a vital purpose.
It makes our communication much less subject to errors.
So consider that list of the 9,000 English five letter words.
Some of them are close together in that space of all possible words.
They might differ in only one letter like pride and prime.
So let's call words like that neighbors of each other.
They are just one step apart.
How many neighbors does a typical word have?
Well, if we are considering all possible five letter words, then each of the five letters
might be changed to any of 25 alternatives.
So the total number of words with a one letter difference from our starting word, the neighbors
of that word, is 125, five times 25.
Only a few of those will be actual English words.
For example, the word grove is about average with six neighbors.
Drove, glove, grave, grope, prove, and trove.
There is a nice word game based on this idea called doublets or word golf.
It was invented by Charles Dodgson, the 19th century writer and logician who, under the
pen name of Lewis Carroll, is most famous as the author of Alice in Wonderland.
The game starts with one word and then stepping only between neighbors that are actual English
words changes it step by step into another word.
It's called word golf because the idea is to reach the goal in the fewest number of steps.
For example, you can turn head to tail in just five steps.
Head to heel to teal to tell to tall to tail.
Most words only have a few neighbors.
That's what makes word golf more of a challenge.
And that is also what makes English resistant to errors.
So suppose someone sends us a five letter word, but for some reason, electrical noise
in the telegraph, sloppy handwriting on the page, an error occurs in one letter.
Since there are 125 possible changes, but only six on average are real words, the probability
is better than 95% that the error is obvious.
That's how computer spell checkers work.
If the word is part of a larger message, the likelihood is also very high that surrounding
context will enable us to figure out the right word in spite of the error.
So an error occurs in transmission, but we are able to correct the error as we read.
What is true for written text is also true for spoken language.
The potential for error in speaking and hearing, especially in a noisy and distracting sound
environment, is quite high.
Nevertheless, because English is so redundant, a few misheard phonemes are usually no problem.
The hearer corrects them automatically, unconsciously.
Error correction wouldn't work as well if English words were more crowded together in
that space of all possible words.
Mistakes in one letter or sound would be more likely to turn the correct word into some
other plausible word.
Misunderstandings would abound.
So it is wrong to conclude that English or any other human language is an inefficient
code for communication.
In the real world, error correction is important.
Redundancy is not a bug, it's a feature.
In the last lecture, we saw how Shannon's second fundamental theorem guaranteed that
error correction is possible, provided that the communication channel allowed some information
to get through.
The amount of information conveyed by a channel is measured by its mutual information, and
the maximum mutual information for the channel is its capacity, C. If we are clever about
doing, about inventing a code in our decoding procedure, then in the long run we can send
up to C bits per use of the channel, while making the overall probability of error as
small as we wish.
However, even though Shannon's theorem guarantees that error correction is possible, it does
not tell us how to go about it.
And so in the years since information theory began in 1948, any number of mathematicians
and computer scientists and engineers, and even a few physicists, have worked on designing
error correcting codes for various situations.
Error correcting codes are based on the same idea we encountered when discussing the redundancy
of English.
We will use longer code words, and the code words in our code will be scattered very sparsely
in the set of all possible code words.
The code words will in some sense be far apart from each other, and that will make it easier
to recognize and fix potential errors.
So let's start with the simplest possible error correcting code.
Alice wants to send a one-bit message to Bob, zero or one.
However, there is some possibility that an error is made when the bit is transmitted.
So Alice uses code words containing three bits.
She simply transmits the same bit three times.
She sends zero, zero, zero, or one, one, one.
Bob, however, might receive some bits incorrectly.
So he simply checks to see if there are more zeros than ones, and decodes accordingly.
So here's the first and most basic question.
Does this actually help?
I mean, sending more bits does make things more redundant, but it also increases the opportunity
for errors.
Will this method actually improve Bob's chances of decoding Alice's message correctly?
Well, let's suppose that each bit transmitted has an error probability E, which we will
take to be 5%.
That's the probability of error if Alice simply sends her one-bit message once without
using any error correction.
Or to put it another way, the probability that a bit is transmitted correctly is one
minus E, or 95%.
Now consider Alice's repeat three times code.
The likelihood that Bob receives all three bits correctly is one minus E times one minus
E times one minus E, which is 0.857.
But Bob will also decode the message correctly if just one of the bits is wrong.
There are three ways that can happen.
The first, second, or third bits could be the wrong one, and each one has a probability
one minus E times one minus E times E, two rights and one wrong, which gives us 0.95
squared times 0.05, or 0.045.
The total probability that Bob recovers the original one-bit message is therefore 0.993.
Bob has a better than 99% chance of getting Alice's message right.
Error correction does help after all.
We can visualize how the code works.
The eight possible three-bit words can be arranged on the corners of a three-dimensional
cube.
The three binary numbers are the coordinates of those points.
We can define how far apart two code words are using the hamming distance, named after
Richard Hamming, one of Shannon's colleagues at Bell Labs and a pioneer of error correcting
codes.
The hamming distance between two code words is the number of places in which they differ.
So 0, 0, 0 has a distance of 1 from 1, 0, 0, and it has a distance of 2 from 1, 0, 1,
and it has a distance of 3 from 1, 1, 1.
In the diagram, the distance between code words can be found by counting the number
of edges on the cube we have to travel to get from one code word to the other.
The two code words Alice uses are 0, 0, 0, and 1, 1, 1, which are separated by a hamming
distance of 3, and that's a significant number.
If Alice sends the code word 0, 0, 0, and error occurs in one bit, Bob receives something
like 0, 0, 1.
But with one error, the received three-bit word is still closer to the original 0, 0,
0, hamming distance 1 than to the alternative 1, 1, 1, hamming distance 2.
Because the code words are separated by hamming distance 3, any single error will still leave
us closer to the original than to anything else.
Thus, whatever Bob receives, he shifts it back to the nearest code word in Alice's
code before decoding the message.
The whole process looks like this.
In code, then send through the channel, then do error correction, then decode.
There are four important facts that we need to know about an error correcting code to
understand how it works.
The first is what kind of symbols are being used.
Alice's code uses bits, like most of the codes we will discuss, but you can design
error correcting codes for decimal digits, letters of the alphabet, and so on.
The next fact is N, the number of symbols used in the code words.
For Alice's repeat three times binary code, N is equal to 3.
The third fact is K, the equivalent number of symbols being represented by the code
word.
In Alice's code word, the code words represent a single bit, either 0 or 1, so K is equal
to 1.
She is using three symbols to code one symbol.
And finally, we need to know D, the hamming distance between the closest pair of code
words.
In Alice's code, there are only two code words, and they are separated by hamming
distance 3, so D equals 3.
Coding theorists often designate a code by its N and K values.
They would say that Alice is using a 3-1 code with a minimum distance D equal to 3.
And that minimum distance D tells us a lot.
It tells us what kind of error correcting properties the code has.
For instance, suppose D equals 1.
That means there are two code words that only differ in one symbol.
That means that a single error could turn one code word into another, could turn grove
into grave.
And that's a dangerous sort of error.
If Alice sends, I will see you in the grove, and Bob receives, I will see you in the grave,
he might get the wrong idea.
So suppose D equals 2.
We can illustrate this with a 2-bit code that lives on a square.
It's a 2-1 code, and the two code words are 0-0 and 1-1.
If Alice sends 0-0 and 1-bit picks up an error, Bob might receive 0-1.
And he cannot correct that error, because 0-1 is equally close to 0-0 and 1-1.
Alice might have sent either code word.
But Bob does know that an error has occurred, and sometimes that's enough.
He may be able to notify Alice that her message did not get through.
Transmission garbled, please send again.
We've already seen that having D equal to 3 enables us to correct any single error,
since a single error still leaves us closer to one code word than to any other.
So what about D equal to 4?
Let's imagine two code words separated by a distance 4.
We can correct any single error in the code words, since that error would only shift the
received message by a hamming distance of 1.
There is no ambiguity about which original code word is closest.
What if there are two errors?
Then we might wind up at a code word that is in the middle, a distance 2 from two different
code words.
We would not know how to correct such an error since neither code word is closest.
But we can detect that there have been two errors, and that's worth something.
If the code words were a little farther apart, D equal to 5, we would be able to correct
even two errors since we'd still wind up closer to the original code word than to any
other.
So we can summarize the error correcting power of a code just by the number D, the minimum
hamming distance between code words.
D equals 1 means no error correction or detection is possible.
D equal to 2 means that no error correction is possible, but we can detect one error.
D equal to 3 means that we can correct one error.
D equal to 4 means we can correct one error and detect two errors.
D equal to 5 means we can correct two errors, and so on.
If D is an even number, we can detect D over two errors, and we can correct one fewer.
That means a D equal to 8 code can detect four errors or correct up to three of them.
If D is an odd number, we can correct up to D minus 1 over two errors.
So if the code has D equal to 13, we can correct up to six errors.
Now let's take a look at one of the most famous error correcting codes, the hamming
7-4 binary code.
Hamming was of course well acquainted with the ideas of information theory as they were
being developed.
At one point he shared an office with Shannon, and this particular code was actually used
as an example in Shannon's original paper.
So what kind of code is a 7-4 code?
Well N equals 7, which means the code words are 7 bits long.
K equals 4, which means that the code words represent 4 bits of information.
That means we need 2 to the 4th or 16 different code words.
And as we will see for this code, D equals 3, so it can correct any 1-bit error.
We cannot draw a diagram of a 7-dimensional cube, but it is not too hard to understand
how the code works.
Each code word has four data bits, D1, D2, D3 and D4.
These carry the message, and in addition we add three more parity bits, P1, P2 and P3.
These are calculated from the data bits using the XOR function, which we remember from our
Boolean logic in lecture 2.
But just to remind ourselves, 0, XOR 0 and 1, XOR 1 are both 0, while 0, XOR 1 and 1,
XOR 0 are both 1.
The parity bits are calculated in this way.
P1 is D1, XOR D2, XOR D4.
P2 is D2, XOR D3, XOR D4.
P3 is D3, XOR D1, XOR D4.
There is a pattern here, but it's a little hard to understand, so let's draw a picture.
Start with three overlapping circles and label each of the seven regions with one of the
bits.
The data bits come from the message.
So here's the rule for the parity bits.
Each parity bit is the XOR of the three bits it shares a circle with.
You can think of this rule in an even simpler way.
The parity bits are set so that each circle contains an even number of ones.
So if the data bits are 1, 0, 1, 0, the parity bits are 1, 1, 0.
Since each data bit affects its own value and the value of two parity bits, the distance
between nearest code words is 3.
Therefore the hamming 74 code ought to be able to correct any one-bit error in either
data bits or parity bits.
How does that work?
Well, suppose one of the data bits is flipped, say D1.
That means that two of the circles in our diagram now contain an odd number of ones,
which tells us the problem must be in the overlap of those two circles, namely D1.
If D4 is flipped, all three circles now have an odd number of ones, and so we know that
the three-way overlap is to blame.
If one of the parity bits is flipped, that only affects the circle containing that bit.
And in any case, we know exactly where the error occurred and so we can correct it.
If errors are not too frequent so that double errors and triple errors are extremely rare,
then the hamming 74 code will do a good job of correcting them.
Suppose the chance of an error in one bit is one in a thousand.
In other words, our channel is binary symmetric channel with the error probability E equal
to one one thousandth.
If we are sending a four-bit message, the chance of an error is about four times as
big, one in two hundred and fifty.
But if we use hamming's code to send those four bits using seven bits, the overall chance
of making an error is a lot less, about two in a million.
Common error correcting codes are a lot more sophisticated than the hamming 74 code, but
they work in similar ways, and they are used everywhere.
For instance, consider the memory of your computer.
Errors occur when the stored information is corrupted.
The errors come in two types, hard errors, which are associated with tiny defects in
the memory chip itself, and soft errors, which are changes in the memory due to some outside
influence without any damage to the chip.
There are lots of possible causes, including radiation and cosmic rays.
Fast charged particles from radioactive elements or outer space can pass through the memory
chip of your computer and alter the information stored there.
If you think that sounds unlikely, you'd be right.
Chip-making companies generally don't advertise how sensitive their products are, but one
study from the 1990s suggested that over a single hour, each bit of memory has a soft
error probability of around 10 to the minus 14th, one in a hundred trillion.
So is that anything to worry about?
Well, at that rate, each gigabyte of memory in your computer will experience about one
radiation-induced error once a year.
And the consequences might be pretty serious, especially if the error occurs in some vital
part of the computer's operating system.
So computer memory uses a few extra bits to create an error-correcting code for its memory.
But the standard schemes can generally correct any single memory error and detect any double
error.
They are codes with D equal to 4.
And this whole process of error detection and correction is going on all the time, behind
the scenes as it were, helping to make our computers more reliable.
Their error rates need more advanced codes.
One of the most popular and powerful types are the Reed-Solomon codes developed over
50 years ago by Irving Reed and Gustav Solomon.
The codes themselves are based on an algebraic mathematical structure.
They're built out of polynomial functions, but we won't dig into the details.
What matters for us is how the codes perform.
Let me describe a very common Reed-Solomon code.
The basic symbols are not bits, but bytes, groups of 8 bits, considered as a single symbol.
The code words are 255 bytes long, and they represent 223 bytes of data.
It is a 255-223 code.
So what is D, the minimum distance between code words?
For Reed-Solomon codes, that's always N minus K plus 1, or 33, in this case.
The 255-223 Reed-Solomon code can correct up to 16 single byte errors.
One of the things that makes the Reed-Solomon codes so useful is their ability to handle
bursts of errors.
We've been talking about errors as if each error were an independent accident.
An error in one bit makes it no more or less likely that the next bit will also have an
error, but real errors are often different.
We might get a whole sequence of adjacent errors produced by the same blast of radio static.
The Reed-Solomon 255-223 code can correct 16 one byte errors, and a byte error could
be a mistake in one or all of the 8 bits in that group.
That means that the code can correct a burst of over 100 consecutive one bit errors in
the code word.
Let's recall the simple repeat three times binary code.
It can correct any single bit error.
Imagine a stream of such code words one after the other.
Even two successive errors might defeat the error correction and lead to a mistake in
the received information.
But now, let's take the code words and interleave them so that the bits of each code word are
spread far apart.
The first code word has its bits at location one, six, and eleven.
The second is at two, seven, and twelve, and so on.
Now the information in the code words is much more resistant to a burst of errors.
In fact, any burst of five or fewer errors can be corrected since no more than one error
occurs in any single code word.
Interleaving the code words adds extra steps in the decoding process, since we have to
undo the interleave of the bytes before we correct the errors.
Nevertheless, it can be worth the extra effort to defend against bursts of errors.
The music on a compact disc is stored using a rather sophisticated error correcting code.
The audio information is represented by a series of tiny shallow pits on the disc, covered
by a layer of transparent plastic.
Each pit is only half a micron wide, half a millionth of a meter.
The length of the pits varies, but it is also very short, typically a micron or two.
The disc is read by bouncing a laser beam off its surface.
As the disc spins, the laser spot falls on one pit, then between the pits, then the next
pit, and all along the track.
Because everything is so small, smudges or dust or tiny scratches on the disc will typically
affect many successive bits.
The errors in reading the data, in other words, will come in bursts.
So the audio data on a compact disc uses an error correcting code that is very good at
handling many errors in a row.
It starts with a read Solomon 28-24 code on bytes.
The code words for this are interleaved, so that the successive bytes for each code word
are widely separated.
And then the result is encoded again, using a read Solomon 32-28 code, and the resulting
code words again interleaved.
This double coding, called concatenation, adds another layer of defense against errors.
The only errors that the inner code needs to correct are the ones that slip through the
outer code.
The result is that the audio CD code can correct a burst of 3,500 successive one-bit errors
on a single track.
That is a stretch of errors of more than two millimeters long.
And that explains why a CD with a few small scratches and smudges can still be read.
And it also explains why radial scratches are much less of a problem than scratches that
go along a track.
The error correcting code can deal with a burst of a few hundred errors across the width
of a scratch, but not a burst of tens of thousands of errors along its length.
And so, if you need to clean the surface of a CD or other optical disc, you should use
a clean cloth and move it radially on the disc so that any small scratches you produce
might produce errors that are easier to correct.
For our final example of codes in action, we'll go a bit further from home.
A few billion miles further, the two Voyager spacecraft were launched by NASA in 1977 on
trajectories that would take them past the planets Jupiter and Saturn.
When Voyager 2 passed Saturn in early 1981, it embarked on an ambitious extended mission
to make flybys of both Uranus and Neptune.
But the extended mission posed challenges.
Saturn is roughly one billion miles away.
Uranus is two billion, and Neptune three.
Thus the radio signals from Voyager would become fainter and fainter with each new planet.
To combat this, the bit rate of the data transmission was slowed from 115,000 bits
per second at Jupiter to only 3,000 bits per second at Neptune.
So let's consider the problem of transmitting a picture.
A simple gray-scale image from Voyager had 800 by 800 pixels, with 8 bits per pixel.
That's about 5 million bits.
A color image was three times as much.
Voyager was designed to use a very simple error-correcting code that transmitted about
two bits for every data bit.
That kept the error rate low, but not zero.
NASA scientists had decided that image data could have an error rate of up to 1 in 200
and still be usable.
But when communication became so much slower, NASA decided to upgrade the spacecraft software.
They added a lossless data compression procedure to squeeze the image data down to around two
million bits per image.
But that introduced problems of its own.
An error rate that was acceptable in an uncompressed image, just a few bad pixels here and there,
produced terrible results on the compressed image.
NASA needed to reduce the error rate dramatically, not 1 in 200, but 1 in a million.
So they added a Reed Solomon 255-223 code, concatenated with the code they already had
built in.
And that was simple enough for the small, primitive 1970 era computers on board to handle the
coding process.
It did not add a great many more bits to the transmissions, but it reduced the final error
rate tremendously.
The images from Uranus and Neptune in the late 1980s were in many ways better than those
from Jupiter and Saturn a decade before.
And that is an amazing accomplishment.
Remember, there was no way to improve the hardware.
NASA was a billion miles beyond reach.
Instead, the scientists and engineers on the Voyager program improved the way that the
information from the spacecraft was represented.
They created codes that were more efficient and more robust against errors.
It was a remarkable example of how the science of information could make a vital contribution
to the science of the solar system.
However, that triumph of coding and error correction can only be part of the story.
The radio transmitters aboard the Voyager spacecraft are very weak, about 20 watts,
yet we are still in communication with them, 12 billion miles from the sun.
How can we get any information at all from such a weak signal?
To answer that, we need to explore more fully how a continuous signal, a radio wave, an
electrical voltage, a sound, can be used to transmit discrete bits of information.
And yet again, we will find that it was Claude Shannon who laid the basic groundwork.
It is a fascinating subject with implications that are both important and, as the tale of
the Voyagers tells us, far leach.
