In Shannon's information theory, the essence of information is the distinction between different
possible messages. The simplest possible distinction, the atom of information, is the bit, the binary
distinction between 1 and 0. Yes and no, on and off. As Shannon pointed out, every sort
of information, numbers, text, sound, images, video, can be communicated by means of bits.
When thinking about the power of bits, Shannon was following up an idea he himself had introduced
a decade earlier in his master's thesis at MIT. Shannon's thesis has been called the
most important master's thesis of the twentieth century. There really is something to that.
In it he laid the basic groundwork for an amazing technological advance. But the subject
was not communication, it was all about computation. In the old days, computer was the name of
a profession. Scientific and engineering projects often employed whole groups of people, usually
women, to do the difficult and painstaking work of numerical calculation. And that kind
of calculation is hard work. Therefore, since ancient times people have tried to find ways
of making it easier. After all, there were all kinds of machines to help with physical
labor, levers, pulleys, gears, motors. Could we invent machines to help us do calculations?
Here is an ancient calculation machine, the abacus. Various kinds of abacus have been
used for many centuries all over the world. We often associate them with China or Japan.
This particular model is a Japanese Soroban abacus still widely used today.
The digits of a number are represented by the positions of the sliding beads. On each
rod the upper bead can be in two different positions. The lower four beads can be in
five different positions. That means there are two times five equal to ten sliding bead
code words. Exactly what we need to represent the digits zero through nine.
The operations of arithmetic are broken down into simple steps which are reduced to manipulations
of the beads. With practice you can do them very quickly. On an abacus you can perform
addition, subtraction, multiplication, division, even taking square roots. Another idea to
save labor is to do as many calculations as possible ahead of time and store the results
in a table. The table is printed and published and then anyone can just look up the figures
they need. By the beginning of the 19th century there were all kinds of tables, astronomical
tables, navigational tables, tables of logarithmic and trigonometric functions, tables of compound
interest, actuarial tables and so on. Then along comes Charles Babbage, an English mathematician,
inventor and scientist. Babbage is interested in all kinds of mathematical subjects ranging
from astronomy to actuarial science, but he is unhappy about the difficulty of creating
useful tables and also about the many errors they contain. He even makes a study of the
errors in mathematical tables. They come in two types, errors in the calculations and
errors in copying and printing the results. Babbage dreams of a completely mechanical
calculation machine, a device that could do the calculation with perfect accuracy and
then print the result without mistakes. So in 1821 he proposes what he calls the difference
engine. In his design wheels with numbers are mounted on a series of shafts. The shafts
are rotated in turn and the gears cause numbers on one shaft to be added together on the next
shaft. That sounds simple, but this basic operation is enough to calculate any polynomial
function that is any sum of powers of an input variable like x cubed minus 2x plus 6. And
that turns out to be just what is needed to do the sort of calculation Babbage is interested
in. Babbage gets a government grant and starts work. Unfortunately, the engineering of his
day is not sufficient to build his design. He spends a lot of time simply trying to improve
machine tool techniques. He and his engineering partner, Joseph Clement, build a working model
of only one part of the difference engine before the funding is cut off. But meanwhile,
Babbage has begun to design two new, more sophisticated engines. The first of these is
a more elegant difference engine with fewer gears and wheels for the same calculation.
Difference engine number two was actually constructed, but not until 1991 to commemorate
the bicentennial of Babbage's birth. You can see it today in the London Science Museum.
It has several thousand moving parts and weighs three tons.
Babbage's second new engine is much more ambitious. In fact, he never completes its
design. It remains a grand research project. He calls it the analytical engine.
The analytical engine would be so huge that it would be powered by steam instead of muscle.
It contains a memory unit called a store and a processing unit called the mill. It is not
designed for one single type of computation, like the difference engine. Instead, the analytical
engine could be programmed to do any calculation at all.
The program of the analytical engine would be represented by cards, with holes punched
in them. This punched card idea had been invented at the beginning of the 19th century to control
the Jacquard loom, a wonderful machine built by the French textile manufacturer, Joseph
Marie Jacquard. Jacquard's loom could produce cloth woven in any pattern. The pattern was
coated into the holes in cards. Thousands of cards for really complicated patterns.
The cards were then attached together in a long chain and fed through the loom, controlling
its operation. It was much like the punched paper tape of the teletype system.
Babbage makes use of the same technology to control his analytical engine. In this way,
he can instruct it to operate in any way he chooses. He's invented the idea of a general
purpose computer.
One of the people who realizes the significance of this is Ada Lovelace, a countess, daughter
of the poet Lord Byron, and a mathematician in her own right, who is a friend and collaborator
of Babbage, and a collaborator on his computational projects. She realizes that the infinite flexibility
of the analytical engine opens up some astounding new possibilities. The numbers in the engine
might stand for letters or symbols. Its operations might be logical functions rather than mathematical
ones. For instance, she imagines as the analytical engine might be programmed to compose music.
The countess understands that a general purpose computer can do much more than arithmetic.
Babbage's dream remained only a dream during his lifetime, and for long after. His most
important ideas, like the analytical engine, were largely forgotten until they were reinvented
in a different form a century later. But the difference engine, though it was never completed,
inspired generations of mechanical calculating machines. One such was the differential analyzer.
Now, the name differential analyzer actually refers to several different machines built
during the 19th and early 20th centuries, all of them designed to solve what are called
differential equations. These are the governing equations in many branches of physics and
engineering. They are central to aerodynamics, ballistics, orbital mechanics, and so on.
The MIT machine was designed by American engineers Harold Hazen and Vannevar Bush in 1931. It
consisted of several tables covered with machinery and rotating shafts and gears and wheels,
all controlled by electrical circuitry. The rotations of the different shafts might represent
different variables. For instance, in calculating the flight of a projectile, the turning of
the master shaft represented the passage of time. This in turn controlled the rotations
of other shafts, representing the acceleration, velocity, and position of the particle. The
final output was a smooth curve precisely traced on graph paper by the machine.
These days, we would call the differential analyzer an analog computer. That is, it computed
relationships between continuous variables like time, velocity, and position. The disadvantage
was that the computer had to be reconfigured, almost rebuilt, for each different problem.
Also, if you wanted numerical output, you had to resort to making careful measurements
on the graphs or reading the angles of the rotating shafts. Even so, it was a valuable
scientific tool throughout the 1930s and 40s.
Claude Shannon. In 1936, he had just finished undergraduate degrees in math and engineering
at the University of Michigan. He comes to MIT for graduate school. He gets a job working
for Vannevar Bush as a technician on the differential analyzer. And as he's doing his job, he becomes
fascinated not with the shafts and gears and wheels, but with the electrical control system
that makes everything turn at the right time. And the young Shannon realizes that purely
electrical components, switches, and relays can do the calculations all by themselves.
That's the problem he takes up for his master's degree in engineering, how to make a computer
out of electrical circuits. Think about an electrical circuit. Here's just about the
simplest one you can imagine, a battery, a switch, and a light bulb. We may understand
what happens in this circuit in three different ways. First, the circuit is basically a flow
of electric charge. Electrons move in the wires when the switch is closed. Electric current
goes through the wires around the circuit. Electric charge really hates being bunched
up anywhere. The electrons repel each other. So, we need a closed path, a circuit, so that
the current can continue to flow. The second way to view an electric circuit is as a flow
of energy. The electrons that leave the battery have a higher energy than those returning
on the other side. The battery adds energy to them. They also lose energy passing through
the bulb. The filament heats up and the bulb emits light. So, energy enters the circuit
at the battery and leaves the circuit at the bulb. Energy flows from battery to bulb.
The third way to think about an electric circuit is as a flow of information. The bulb only
lights up when the switch is closed and the current can flow freely, carrying energy from
the battery to the bulb. The switch could be in one place and the bulb in another place,
far away. The bulb would indicate whether the switch was closed. So, you can send a message
by opening and closing the switch and the light would flash off and on at the other end.
Therefore, information flows from switch to bulb. What kind of information? In the simple
circuit we're considering, it is simply the binary distinction between off and on. Zero
and one, no or yes, one bit of information. That's the aspect of the electric circuit
that we care about, that Shannon cared about when he pondered how to make an electrical
computer. So, when we draw a diagram of a circuit, we will simplify quite a bit. We
will leave out the wires that let moving electrons return to their starting place. We'll also
leave out the battery. We will assume that the return paths and the energy sources are
present in the necessary places. Our diagram for the simple circuit that we talked about
looks something like this. Switch to bulb. That line does represent a wire, a physical
connection between switch and bulb. The input is on the left, our finger on the switch.
One bit of information travels along the wire from left to right, the output is on the right,
the bulb flashing on or off. In the late 1930s, information theory and the definition
of bit is still years in the future, so Shannon goes back to the work of a contemporary of
Babbage, the English mathematician George Boole. Boole's great achievement was to reduce logic
itself to a kind of algebra. Logical relations and arguments take the form of algebraic
operations and calculations. Shannon realizes that this kind of Boolean algebra is really
the fundamental basis for any computation. In Boolean algebra, the variables ABC and
so forth stand for logical statements, and these variables can take just two possible
logical values, true and false, which we can represent by 1 and 0. A Boolean variable in
other words is a way of thinking about a bit. Shannon identifies them with the one bit
messages traveling on the wires of an electric circuit.
What can you do with these messages? Boole had introduced several algebraic operations
on them and Shannon is able to show how each of these corresponds to a way of putting together
a circuit. For instance, the simplest Boolean operation is negation or not. If A is true,
then not A is false and vice versa. It's represented by a symbol, not. We can make
a table to show how the operation works. In an electrical circuit, this would be represented
by a kind of relay switch, a switch on one wire that is controlled by another wire. You
can connect this so that the relay switch is on when the input from the control wire
is off and vice versa. In our diagrams, this is drawn as a little box with one wire coming
in and one wire going out, marked with the symbol not. If a zero comes in from the left,
a one leaves to the right and vice versa. This is our first example of what we will call
a logic gate. A logic gate stands for both a Boolean operation and a piece of an electrical
circuit. Shannon's whole idea is that these are really the same things. One or more inputs,
one bit messages, arrive from one side and one or more one bit outputs emerge from the
other side. I should point out, by the way, that electrical engineers use their own special
icons for the various logic gates on circuit diagrams. I decided to skip all that and just
draw them as boxes with symbols on them. That's good enough for our purposes, but if you dig
deeper into electronics, you'll need to learn the usual icons.
Our next logic gate is the AND operation. Given two variables, A and B, then A and B
is true only when both A and B are true. The symbol is a wedge that sort of looks like
the letter A, and in table form we might have something like this. Notice that we need more
rows in our table this time. With two bits of input, we need to consider two squared
or four possibilities. Again, we can make a physical version of this. The easiest way
is to put two input switches on a single wire so that current can flow only if both switches
are closed. We will draw this as a little box with two input wires and one output marked
with the wedge symbol.
The next logic gate is the OR operation, which is a complement to AND. That is, A or B is
true if A is true or B is true or both. Its symbol is a V, and its table looks like this.
To make an electrical circuit like this, you could put two input switches on parallel wire
paths. Current can flow if either switch or both is closed. In the diagram, this looks
like you'd expect.
There is, of course, another natural definition of the OR operation, which is called the exclusive
OR, or XOR. A, XOR, B is true if either A is true or B is true, but not both. Its symbol
is a plus sign in a circle, and here's the table for it, and a diagram. XOR is slightly
trickier to realize as an electrical circuit, but you very likely have one or more of these
in your home. This is almost always how it works when two wall switches control the same
light fixture. When both switches are down, the light is off, but when either switch changes
position, the light also changes. That's XOR.
Shannon realizes that any mathematical operation can be reduced to a basic logical pattern,
which can, in turn, be translated into the circuit layout of an electrical circuit. By
using one-bit messages and wires and connecting them to logic gates, any calculation whatsoever
can be performed just by electrons moving around. That simple idea lays the foundation
for the modern electronic computer.
So, to get a sense of how it works, let's design a circuit that can add numbers together.
First, we have to learn how to represent numbers with only zeros and ones. The way to do that
is the base two numbering system. Now, base two is not hard once you see the idea. In
the familiar base ten system, there are ten symbols, the digits zero through nine, and
in a multi-digit number, the meaning of each digit depends on its place and the number,
each place representing a power of ten. So we have the ones place, ten to the zeroth
power on the right, the tens place, ten to the first power next, then the hundreds place,
ten to the second power, and so on. So the number 507, 507 means five times a hundred
plus zero times ten plus seven times one. In base two, there are only two symbols,
the binary digits zero and one. In a multi-bit number, the meaning of each bit depends on
its place, each place representing a power of two. So we have the ones place, two to
the zero, the twos place, two to the first, the fours place, two to the second, the eights
place, two to the third, and so on. So the number 1101 in base two means one times eight
plus one times four plus zero times two plus one times one, thirteen. What if we add a pair
of one-bit numbers together? The answer might be as big as two, which is written 10 in base
two, so we'll need two bits for the result. We can make addition tables with all the possibilities.
Now look at the last, look at the two bits of the sum in the last column. The leading
bit of the sum, which we could call S1, is only one if both A and B are one. The remaining
bit, S0, is just A, XOR, B. We can put those logic gates together and design a circuit
that adds one-bit numbers. It uses one AND gate and one XOR gate.
Notice that each one-bit input message is copied and sent to two places, and by the
way, when we cross the wires, we assume that they do not actually intersect, one wire runs
above the other. This circuit takes two one-bit input numbers on the left and computes the
sum, producing the output number in base two as a pair of one-bit messages set off to
the right. Now we know how to add up to one plus one. That isn't very impressive so far.
What we have designed is what engineers call a half-adder circuit. What then is a full
adder? A full adder adds up to three one-bit inputs. So let's design one of those. Since
the maximum sum of three input bits is three, which is one-one in base two, we still only
need two output bits. To make things simpler, we can use the half-adder circuit as a module
in the larger circuit. We don't have to draw in all the ANDs and XORs. We just draw a box
with two inputs and two outputs, and call it AJ. Now we assemble a full adder circuit.
It requires two half-adders and one OR gate. Take a look at the layout. It's useful to
try to trace out all the one-bit messages in this diagram. So suppose all three input
bits, A, B, and C, happen to be one. Then the first half-adder computes one plus one,
so it gives a one in the top output bit and a zero in the bottom bit. The next half-adder
thus has zero plus one, so its output is a zero in the top bit and a one in the bottom
bit. The two inputs of the OR gate are one and zero, so its output is one. In the end,
both S1 is equal to one, and S0 is equal to one. The answer is one, one, or three.
Try it for yourself for some other input bits. It always works. The full adder is made of
five basic gates, two ANDs and two XORs and a half-adders, plus one additional OR gate.
But we can think of it also as a single module, with three inputs and two outputs. We can give
it its own symbol, the plus sign. What have we achieved? The full adder can add one plus
one plus one and get three in base two, which does not sound much more impressive than before,
but that turns out to be just what we need in order to add any number to any number.
When we add numbers in base ten, we sometimes need to carry a digit from one place to the
next. Thus, when we add 47 and 35, the 7 and 5 add up to 12, so we write two in the
one's place and carry a one to the ten's place. We now need to add three digits in
the ten's place, four plus three plus one, one being the carry digit, to get the final
answer, 82. It works the same way when we add up multi-bit numbers in base two. The two
bits in the one's place might produce a bit that carries over to the two's place and so
on. The extra C input for the full adder is precisely what we need to handle the carry
bit.
So, let me show you the circuit for adding together two four-bit numbers in base two.
The input numbers are A3, A2, A1, A0 and B3, B2, B1, B0. The final answer might be
a number with up to five bits, which we'll call S4, S3, S2, S1, S0. The circuit that
does the addition is a cascading pattern of full adders.
We only need a half adder for the one's place since there is no carry bit, but from then
on the top bit from each addition is the carry bit for the next one. The same pattern can
be used to add binary numbers of any size.
Let's try something quite different, something that wasn't in Shannon's Master's thesis.
Suppose we look at the output of a knot gate and then we take it and feed back a copy of
that signal into the same gate as the input. This is a crazy looking snake swallowing its
own tail sort of idea. The results are kind of crazy too. The circuit can never settle
on a particular output value. A one becomes a zero, which becomes a one again and so on.
It's hard to know how that actually works out in practice. We might expect a rapid oscillation
between one and zero at the output.
Now let's put two knot gates in a row and feed the last output back as input. Now everything
is okay. A zero at input becomes a one and then a zero, which feeds back. But of course
it works just as well the other way. A one at input becomes a zero and then a one which
feeds back. Either way is self-consistent.
All we can say for sure is that whichever way the circuit starts out, that's the way
it will stay. In other words, this circuit acts as a primitive form of memory.
Now let's build something really interesting. We start with something called a NAND gate.
NAND stands for not and and it's just an AND gate followed by a NOT gate. It only produces
zero when both inputs are one. The symbol for it is an upward pointing arrow.
Now put two NAND gates together. Take the output of each gate and feed a copy back to
the other gate as input. We call the two external input bits, A and B, and the two output bits,
P and Q.
Consider a few possible situations. If A equals one and B equals zero, then there is just one
self-consistent way everything can work out. P equals zero and Q equals one. Then the upper
NAND gate has two one inputs leading to output zero and the lower NAND gate has a zero and
zero input leading to output one. In the same way, if A equals zero and B equals one, then
the only self-consistent output is P equal to one and Q equal to zero, just the opposite.
Notice by the way that in both of these cases, the P-Q outputs are just the opposites of
the corresponding A-B inputs. By adjusting the inputs, we can make either sort of output.
Now set both inputs to one. A equals one and B equals one. As with the double NOT loop,
we now have two self-consistent ways it can work out. Either P equals zero and Q equals
one, or P equals one and Q equals zero. Both of these are okay. So whichever way the two
output bits start out, that's the way they'll stay.
This is a flip-flop circuit, a one-bit memory unit. The outputs are either zero one or one
zero, our two memory states. We can set whichever one we want by adjusting the A and B control
inputs. That is, we can set both A and B to one, and the outputs then stay whichever way
they started. The memory bit retains a constant value. Later, if we want to change that value
again, we can fiddle with A and B.
There are many things worth noting here. First of all, every circuit is a combination of logic
gates, connected by wires carrying one-bit messages. The full adder is a communication
network linking 17 logic gates together. The flip-flop memory circuit used just two NAND
gates and a feedback network that let the gates talk to themselves.
We built things up by linking simple pieces together, creating modules with more and more
complicated functions. We can use these modules in larger circuits, and so on. These same
ideas very far extended are the basis for the design of modern computer processors.
A single processing chip might contain hundreds of millions of logic gates, each one consisting
of a few microscopic transistors connected by wires only a few hundred atoms wide, exchanging
one-bit messages billions of times per second.
That is an extraordinary level of complexity, but it is all built out of simple Boolean logic
gates. That's what Shannon understands when he writes his master's thesis. The bit, the
distinction between 1 and 0 is the fundamental atom of information. Every sort of information
can be represented by bits, and in the same way, one-bit messages and simple Boolean logic
gates are fundamental atoms of computation. Every sort of computation can be constructed
from them.
And Ada Lovelace's insight holds just as well for Shannon's electronic computer as
for Babbage's analytical engine. Once you have a general purpose computing machine,
it can perform operations that have nothing to do with arithmetic.
Our excursion into computation may have seemed like a diversion away from the concepts of
information theory, but I don't think it is. For one thing, Shannon's work on Boolean
operations and electronic computers was background for him in his later work on information theory.
Moreover, when we discuss the concepts of information theory, we will often describe
various kinds of information processing, coding, decoding, error correction and so on. Now
we have some idea how such things can actually be done.
As we will see, these ideas about computation will return in fundamental ways more than
once in our course. Logic gates and Boolean operations will be needed to analyze Maxwell's
daemon in lecture 17. The idea of a universal computer, a single machine that can be programmed
to do any sort of computation, forms the basis for an entirely different approach to information
in lecture 19.
But meanwhile, it is time to dive into information theory proper. And our first order of business
is to figure out how to measure information and how to encode it most efficiently. It
is time to make the acquaintance of a subtle and powerful quantity that seems to lurk
around every corner of the science of information, a mysterious function called entropy.
