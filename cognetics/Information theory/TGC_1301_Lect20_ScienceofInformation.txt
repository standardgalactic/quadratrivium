Shannon's information is based on codes and communication.
This new way of thinking about information brings with it new insights on many things,
the meaning of randomness, Occam's razor for choosing between hypotheses, even Maxwell's
demon and the second law of thermodynamics.
Yet algorithmic information is beset by a strange impossibility, one that connects this
new concept of information to the very foundations of logic and mathematics.
We'll start with a famous old conundrum of logic called the Berry Paradox.
The Berry Paradox was first posed by an Oxford librarian, G. G. Berry, who lived about a
hundred years ago.
Or so we learned from Bertrand Russell, the puckish English philosopher and logician who
named and published the paradox in 1906.
It's a paradox about the positive whole numbers, the integers, and we can describe
integers by phrases in English.
Four.
One plus one plus one.
The product of all the integers up to ten.
The largest prime number, less than one million.
Every integer has a description, if only of the one plus one plus one sort.
However, large integers usually need many words to describe them, and this makes sense.
After all, there are only a finite number of words in English.
This means there can only be so many different number descriptions with, say, twelve words
or less.
Now consider the following description.
The smallest integer that cannot be described in twelve words or less.
Well, there certainly are integers that cannot be described so succinctly, and so it follows
that there must be a smallest such integer.
Of course, here's the problem.
That phrase itself has twelve words.
Thus any integer that is described by that phrase cannot be described by that phrase.
That's the Berry Paradox.
The Berry Paradox is a warning.
It tells us that the concept of description is fraught with unseen pitfalls, yet in algorithmic
information theory the whole concept of information is based on the idea of description.
So as we explore that concept of information, we must be very careful indeed.
It is ironic that the subject of information, which is at root about what we know, brings
us face to face with what we can never know.
Our discussion is going to get a little technical.
With some close attention, you should be able to follow every detail.
Still, I don't want you to get lost.
Therefore, I will organize the discussion in four acts and tell you when we are moving
from one act to the next.
In Act 1, we will talk about computer programs that contain other programs, or that need
an additional input to run.
In Act 2, we will discuss the problem of deciding whether or not a program will ever halt.
In Act 3, we will use Berry's Paradox, suitably disguised, to prove that algorithmic entropy
is an uncomputable function.
Finally, in Act 4, we will explore some implications of uncomputability for physics, logic, and
mathematics.
So, Act 1.
We said that our binary string P counts as a description of another string S provided
that P is a program on a standard universal computing machine, a UM, that prints S as
output and then halts.
Among all the descriptions of S, there is a shortest one called S star, and the link
to that star is the algorithmic entropy K of S. This gave us a neat way to define randomness.
A string S is random if it has no description much shorter than itself.
Its entropy K of S is about equal to its own length.
That turns out that most strings of a particular length are random.
So consider strings that are 100 bits long.
There are two to the one hundredth of them, which is a pretty big number, it has 32 digits.
We'll say that S is random if K of S is at least 90.
That is, S star is at least 90 bits long.
So how many non-random hundred-bit strings can there be?
Well each of these is describable by a program less than 90 bits long, so the number of such
strings is limited by the number of such programs.
That number of programs is smaller than 2 to the 90th, which is also a big number, 28 digits long.
However, this is much smaller than 2 to the one hundredth.
In fact their ratio is about .001, so at least 99% of our strings with 100 bits are random
by this definition.
Almost all strings are algorithmically random.
The ones with patterns are the rare exception.
But a string S might or might not be random, but its shortest description S star is itself
always random.
Why?
Well, suppose S star were not random, then its own shortest description S star star would
be much shorter than S star, then we could make a new program called program R.
So program R, 1 run S star star, 2 run the output of step 1 as a program and print its
output, 3 halt.
Program R uses another program, program S star star as a module, a functional part of
the whole, and that's okay.
Computer programmers call that a sub-program.
Also R generates a sub-program in step 1, then runs in step 2, that's okay too, after
all a program or sub-program is just a binary string.
So program R is really just S star star plus a few extra instructions, so the length of
program R is about equal to the length of S star star, which is by our assumption much
shorter than S star.
But look at what program R does.
In step 1 it actually generates S star, then in step 2 it runs S star and produces S. In
other words R is a UM description of the original string S, but that means it cannot be shorter
than S star, which is the smallest such description, so our assumption is wrong.
S star star is not much shorter than S star, and therefore S star has no much shorter description.
It is random.
If that seems surprising to you, think of it this way.
The difference between the length of a string L of S and its algorithmic entropy K of S
can be thought of as a kind of algorithmic redundancy.
It's a measure of how many extra bits we use to write S directly, rather than S star,
its shortest description.
So when we go down from S to S star, we squeeze out all that redundancy, so there's no redundancy
left in S star.
That's what makes it algorithmically random.
The analogy to Shannon's information theory is pretty clear.
If we have an information source with some redundancy, we can use a code to represent
the output of the source with a smaller number of bits.
That's called data compression.
So replacing S by its minimal description S star is an algorithmic version of data compression.
In fact it's perfect data compression because no binary string shorter than S star could
possibly be a description of S.
A program that could do perfect data compression that could convert any string S into its minimal
description S star would be a very useful thing.
But as we will see, such a program is actually impossible.
When Alan Turing worked out the theory of universal computation, he realized that there
are things that even a universal computer can never do.
There are uncomputable functions.
Okay, what is a function?
Well a function is a mathematical rule that takes an input and produces an output.
We can design programs on our UM that do the same sort of thing.
We begin with a partial program F, a binary string containing some computer instructions,
but F by itself is not quite a complete program.
We have to add some additional bits as the input data for the program.
The complete program for the universal machine is something like this, FS, the bits of F
followed by the bits of the function input S.
When we run that on the universal machine it produces an output string which we will call F of S.
So how should you and I write down the program of a function?
How do we describe F itself apart from its input S?
We'll do it this way.
First we make a note that there will be an input to which we give a temporary name
Then we describe what to do with X.
For instance, suppose the input is a number and F computes the square of the number.
We'd write down the program like so.
Program F with input X.
Calculate X squared and print it, halt.
This is the human language version of some binary program F.
When the computer reads F followed by an actual input S, it applies this procedure to S
and then outputs F of S, in this example the square of S.
We can create function programs of all kinds and since the program and its input are both
just strings of bits, we can play some wacky games with the idea.
For instance, we can give the computer F F so that it will calculate F of F.
Now that's not as strange as it sounds.
Let me give you an example.
I have a program on my computer that determines how large a data file is, how many bytes it
occupies in memory.
But the program itself is a bunch of bytes, a data file.
So I can just instruct the program to determine its own size.
That's like the program F F, which computes F of F.
Act two.
Not all computer programs ever do come to a stopping point.
Some get stuck in an infinite loop and some functions can be like that too.
A function F might calculate output and halt for some inputs but get stuck for others.
I've often found myself in the following situation.
I have instructed my computer to do some task, maybe run a program that I have written.
I get it started and then I wait.
Pretty soon I start to wonder.
Maybe the computer is heading toward a final answer and if I am patient it will eventually
finish the job and everything will be okay.
Maybe the computer has somehow gotten trapped in a loop or some kind of endless procedure.
Should I just be patient?
Or should I intervene, pull the plug and try to figure out what went wrong?
It would be handy to know in advance whether or not a given program is one of those that
runs forever or whether it will eventually finish.
For the UM we would like to know whether program FS, function F acting on input S, ever halts.
As a logical fact, every program FS either will or will not halt.
The answer is one bit of information, yes or no.
So for every FS there is either a one or a zero depending on whether it halts or not.
That's the halting function and it certainly does exist mathematically.
And a function we would like to be able to compute.
So as a thought experiment Alan Turing imagined a program H that could calculate the halting
function.
Here's how H works.
H actually takes two inputs, the program for the function F and its input S. Then the
halting function H analyzes F and S very carefully.
It outputs 1 if the program FS will eventually halt and 0 if it will run forever.
And we assume that the program H never gets stuck itself and it always gives the right
answer.
So the halting function program H is represented by some binary string just like any other function
on the universal machine.
To run it on the computer we would make a complete program out of the program H than
the program F than the input S. That is we give the computer a complete program HFS.
Once we have the program H we can use it as a sub-program in a larger program.
For instance we could use it to prevent the computer from ever getting stuck in a computation.
It would see that coming and avoid it.
So here is program G. Program G which takes as input F and S.
Step one, compute H of FS.
Step two, if H of FS equals 1 then calculate FS and print the result.
Three, halt.
To compute a function F on input S we would run the complete program GFS.
Here's what G does.
Before trying to compute F of S the computer first checks to see if FS halts.
If so it does the calculation and prints the answer.
But if not it just quits right away.
The computer never gets trapped in an infinite loop.
That sounds pretty useful.
So let's consider a new program called T for Turing.
This takes an input X and then does something a little odd with the halting function.
So here's program T, it takes input X.
Step one, compute H of XX.
Step two, if H of XX equals 0, halt.
If H of XX equals 1, loop forever.
So program T is a function.
It takes as its input the program for some other function F.
The computer is actually running the complete program TF.
So this program uses the halting function program H to determine whether F will get
stuck if it is given itself as input.
That is whether the program FF halts.
If FF does not halt, then T does halt.
If FF does halt, T keeps running on in an endless loop.
In short, the program TF always does the opposite of what FF would do if one halts, the other
does not, and vice versa.
Now here is what Turing asks.
What happens when F is actually T itself?
What happens to the program TT?
Remember, since T is a binary string, TT is a perfectly sensible, complete program for
the computer.
But the program is designed so that TT halts, if and only if, TT does not halt.
It's a logical contradiction.
Actually, it's exactly like the old paradox of the barber.
That the barber lives in a village.
Some men in the village shave themselves.
They are self-shaving men.
The barber's rule is that he shaves exactly those men who are non-self-shaving.
He shaves all those and no others.
But who shaves the barber?
If he is self-shaving, then by his own rule he cannot shave himself.
But if he is non-self-shaving, he must shave himself.
In the same way, some functions F halt when they receive themselves as input.
They are self-halting.
The others are non-self-halting.
Program T is designed to halt for exactly those programs that are non-self-halting.
So is program T self-halting or non-self-halting?
Neither one is possible.
Where did we go wrong?
Turing says the only place we could have gone wrong is when we assumed that we had a program
H to compute the halting function.
Therefore, no such program can exist.
The halting function, which is well-defined mathematically, is not computable by any program.
If we tried to write a program to tell whether another program halts or not, no matter how
our program worked, it would have to fail sometimes.
Either it would make mistakes by giving the wrong answer, or it would sometimes get stuck
and be unable to decide.
That sounds rather like me staring at my computer screen, not sure whether or not my program
will ever stop on its own.
Turing's proof that the halting problem is an uncomputable function, that no universal
computer can solve it, is one of the greatest mathematical discoveries of the last century.
It would be no exaggeration to say that the whole modern field of computer science has
its origin in Turing's work on universal machines.
Act 3
Uncomputable functions are fascinating, but do they say anything about information?
And indeed they do.
One of those uncomputable functions is this whole basis for algorithmic information theory,
the algorithmic entropy K.
Let's see why.
Like Turing did for the halting function, we will assume for the sake of argument that
there is a program K that does calculate K of S. If we provide our universal machine
with program K followed by a string S, that is a complete program KS, then it will compute
K of S. Print that out and halt.
Program K itself has some length K.
Now that might be quite a long program, it could be 100 million bits long, but however
long it is we could come up with a number, call that N, that is more than a billion times
larger than L of K.
Next we must note that it is relatively easy to generate all binary strings, one by one,
starting from the shortest and going from there.
That is we start with zero, and then one, then zero zero, zero one, one zero, one one,
then zero zero zero, zero zero one, and so on.
First we get all the one bit strings, then all the two bit strings, then all the three
bit strings, and so on.
The procedure is not very complicated and if we keep it up eventually we will hit every
binary string of every length.
So now we can write a new program which we call B. B stands for Berry, the Oxford Librarian.
Here's program B. Step one, generate the next string S. Step two, compute K of S using
sub-program K. Step three, if K of S is less than N go back to step one, otherwise print
S and halt.
Well, how big is program B?
It's all pretty simple.
The only possibly complicated part is the sub-program K, which might be very large, so in rough terms
B is not much longer than K itself.
So what exactly does B do?
Well, it starts with the string zero.
This has a small value of K, the algorithmic entropy, so it tries again.
It generates a string one.
This two has a small K and so it tries again.
It generates the string zero, zero, and so on.
Program B runs for a very, very, very long time.
But eventually it will generate a binary string S whose algorithmic entropy K of S is at least
as large as the huge number N. It then prints S and halts.
So we've established three things about program B.
It is a description of that string S. That is, it eventually prints S and then halts.
The algorithmic entropy of S is at least N.
The length of B is about L of K, which is a lot less than N.
These cannot all be true.
If K of S is at least N, then the shortest description of S must be that long.
But B, which is much shorter, is also a description of S.
We have a contradiction.
Program B really is a computer version of the Berry Paradox.
In words, program B is a description for a binary string that is like this.
The first binary string that cannot be described in fewer than N bits.
But by assumption, B itself has far fewer than N bits.
Well, what's the problem?
The problem is that there can be no program K that can compute algorithmic entropy.
K of S is an uncomputable function.
Thus our fundamental measure of information based on computation turns out to be impossible
to compute.
Act 4
The conclusion we have reached is rather strange, and it has some strange consequences.
For example, let us recall Wojtek Zurek's formula for thermodynamic entropy.
Zurek considered a system that included Maxwell's demon, a kind of computer.
The demon possesses information stored in its memory.
The binary string M represents that memory record.
It has an algorithmic entropy K of M.
Given the demon's memory record, there are a great many microstates, M of them, consistent
with that information, the demon's idostate.
The missing microstate information is the Shannon entropy log 2 of M.
And according to ordinary information theory, the missing microstate information is the
Shannon entropy log 2 of M.
So, Zurek says, total thermodynamic entropy is the Boltzmann constant K2 times the sum
of the Shannon and algorithmic entropies.
As the demon acquires the disposals of information, each entropy term might increase or decrease,
but the total can never decrease on average.
That's the second law of thermodynamics as it applies to Maxwell's demon.
But now we know that algorithmic entropy is uncomputable.
Therefore, accepting Zurek's formula, the total thermodynamic entropy is uncomputable.
So in the second law of thermodynamics, there is actually an element that we can never, ever
compute.
Does this matter?
Almost not at all, but just maybe a little.
Suppose Maxwell's demon is at the stage where it wants to erase its memory record
M. Every bit that it erases will cost energy and produce an entropy K2 in the environment.
That's Landauer's principle.
Naturally the demon does not wish to use any more energy than necessary.
So before it erases its memory, the demon will do data compression on it so that it
erases a smaller number of bits.
This has to be a reversible computation of course, so the demon wants to find the smallest
bit string from which the original memory M can be reconstructed.
And of course that's just M star, the shortest description of M on a universal computer.
That M star is K of M bits long.
What the demon wants is a perfect data compression program, a program that can turn M into M star.
But no such program can exist.
If it did, we could compress a string S to S star and then just count the bits in S star.
That would compute the algorithmic entropy K of S. And since we know that K is uncomputable,
perfect data compression must be computationally impossible.
The actual data compression program used by the demon will compress M to M prime.
The length of M prime might be as small as M star, that is the data compression might
be perfect, but sometimes M prime will be longer than M star.
That means the demon will wind up erasing more bits than K of M. That will expel more
energy as waste heat and increase the overall entropy.
Let me state that as plainly as I can, the demon wastes additional energy because certain
mathematical functions are uncomputable.
Zurich calls this conjectured effect logical friction.
Uncomputability is also closely connected to a profound fact about the nature of mathematics.
That is Kurt Girdel's famous incompleteness theorem.
Girdel was an Austrian logician who later immigrated to the United States.
He proved that any formal mathematical system of axioms and logical rules, provided it is
as sophisticated as simple arithmetic, must contain statements that are neither provable
nor disprovable from the axioms.
In other words, there are undecidable propositions.
These are something like the uncomputable functions for a universal computer.
Indeed, the connection is more than an analogy.
A computer is itself a formal mathematical system, and a formal mathematical system can
be programmed on a computer.
Gregory Chaitin, one of the founders of algorithmic information theory, has taken this idea very
far.
He imagines a computer given a description of some mathematical system, say number theory.
All of the axioms about numbers and all of the logical rules are expressed as a string
of bits.
Call that string n.
Then the computer is given a proposition p about numbers.
Its job is to either prove or disprove p.
But when p is undecidable, the computer never halts.
Chaitin says the basic problem is that the axioms in the bit string n only constitute
a finite amount of information.
Some propositions actually entail more information than the axioms themselves, so they can never
be proven from them.
For Chaitin, Goethe's famous theorem about mathematics is actually a deep fact about
information.
And this is nowhere more strikingly confirmed than in the almost inconceivable properties
of the number that Chaitin calls omega.
Omega is actually pretty easy to define.
Once again, we imagine that our computer is being programmed by a monkey.
The monkey types a random string of zeros and ones, and the likelihood that the monkey
types a particular program p is 2 to the power of minus the length of p.
Some programs that the monkey types will eventually halt.
Others will run forever.
So omega is just the total monkey probability that the program halts, the monkey halting
probability.
In symbols, we just add up all the probabilities for the halting programs.
The number omega is somewhere between zero and one.
It has a binary expansion, an infinite series of bits, something like 0.00111100, etc., etc.
However, omega is an uncomputable number.
No program could produce the bits of omega one after the other as output.
Indeed, any initial segment of omega is algorithmically random.
The first million bits of omega, for instance, have no description less than about a million
bits long.
Now we can find an approximation, omega prime, that is smaller than omega, and with more work
we can push omega prime closer to omega.
Every time a program halts, that tells us one of the monkey probabilities in the omega
sum.
And if we add up the monkey probabilities for all the halting programs we know about
so far, we could call that omega prime.
Now since there are some other halting programs that we haven't identified yet, we know that
omega prime is a little less than omega.
The problem is that we can never know exactly how close omega prime is to omega.
The number omega contains an astonishing amount of useful information.
Even if we only knew part of omega, it would tell us a lot.
For instance, it turns out that knowing the first n bits of omega would allow us to solve
the halting problem for all programs up to n bits long.
That's because we could just test all the n bit programs at once and calculate an estimate
omega prime for omega by keeping track of which programs halted.
Eventually omega prime would match those n bits of omega.
That means omega prime would be within 2 to the minus n of the real thing and that would
mean that none of the rest of the n bit programs would ever halt.
We could stop testing them.
And once we solve the halting problem, we can do a lot more uncomputable things.
With n bits of omega, we can calculate the algorithmic entropy of any string up to n
bits long.
In fact, we can solve any solvable mathematical problem that can be stated in n bits.
The monkey halting probability omega is in fact a maximally compressed compendium of
all mathematical information.
But that compendium is forever beyond our reach.
Even our deepest mathematics has, as it were, only revealed the first few dozen bits.
And no one can predict what the next ones might be.
Gregory Chaitin says that this means that there can be no final theory of everything
in mathematics.
There is an infinite amount of information yet to be discovered.
The mathematical universe will never exhaust its surprise.
