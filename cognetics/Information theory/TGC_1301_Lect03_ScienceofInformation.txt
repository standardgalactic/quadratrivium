How do we measure information?
How do we tell whether a message involves a small amount or a large amount of information?
We do have an intuition about this, and the intuition is based on the length of the message.
The fortune in a fortune cookie is less information than a newspaper article, and that in turn
is less than a whole book.
Just counting the words tells you that.
More words mean more information.
But can we compare information of different kinds, like a newspaper article and a photograph?
Is it really true that our picture is worth a thousand words?
To compare these, we need to have a common way of representing the messages, and we already
know that this is possible.
All kinds of information can be represented by binary digits, by bits, so we can just
count the number of zeros and ones that we need for different messages.
When I examine the files on my own computer, I find that, in round terms, a good quality
photograph takes about the same number of bits, something like 8 million bits or one
megabyte, as one minute of music or the text of an entire novel.
A picture, it appears, is really worth a couple of hundred thousand words.
Yet, this comparison depends on the codes we use.
The teletype code uses 5 bits per letter, while an ASCII text file uses 8 bits.
A message of 100 letters could require either 500 bits or 800 bits.
Which one is the right measure?
For images and sounds, it's even worse.
The same picture can take up to 10 times as many bits in one format, one code for pictures,
then it does it another.
This comparison, however, the comparison of information depends on the code by which we
represent the information.
Our intuition is not wrong.
The number of bits does have something to do with the amount of information, but that
connection comes at the end of the story, rather than the beginning.
We must start somewhere else.
Let us imagine a source of information, something that produces messages.
The source might be a person.
A machine.
Or a natural phenomenon, like the weather.
The messages might be speech, or letters of the alphabet, or electrical signals, or the
daily high temperature.
In a few minutes, we will suppose that the source produces a whole series of messages
one after the other.
But for now, let's just consider one of them.
How many possible different messages might the source produce?
That depends on the source, naturally, and just as in lecture one, we will denote that
number by m.
Now, m is a significant number, because it does not depend on our choice of code.
However, for whatever code we choose, we will need at least m different code words.
According to Shannon, information is really about the distinction between possible messages,
so it makes sense that the amount of information depends on m, the number of possibilities.
If m is small, perhaps only two for a one-bit message, then the amount of information in
the message is small.
If m is huge, then we gain a lot of information when we receive the message.
A book contains a lot of information precisely because there are so many possible books.
Could we just adopt m itself as our measure of information?
That's an attractive idea, but it doesn't work out as we would like.
Suppose we consider two messages from the same source.
How many possible pairs of messages are there?
The first message has m possibilities, and for each of those, the second message also
has m possibilities, so the two messages have m times m, or m squared, joint possibilities.
The numbers multiply.
But our intuition says that two messages should have just two times as much information
as one message, not the square of the information.
To appease that intuition, we need to take the number of possibilities m, which is multiplicative,
for more than one message, and turn it into something that is additive.
And the right trick to do that is to use logarithms.
Logarithms were introduced into the world by the Scottish mathematician John Napier
in 1614, and they soon gained wide popularity as a tool for doing difficult calculations.
The genius of logarithms is that they turned multiplication and division into the easier
operations of addition and subtraction.
Here's how it all works.
We'll settle on a number b, greater than one, which is called the base.
We can raise the number b to various powers, b to the first power is b, b to the second
power is b times b, and so on.
The powers one, two, and so on are called exponents.
We can also have the zero power.
b to the zero is equal to one, and even negative exponents, b to the minus one power, is one
over b, and so on.
It either makes sense to raise b to a fractional power, b to the two-thirds power, or b to
the 1.2, 3, 4, 5 power.
What do fractional powers mean?
The answer comes from one of the basic rules of exponents.
Suppose you have two powers, x and y, then it is always true that b to the x times b
to the y equals b to the x plus y.
When you multiply powers of b together, then the exponents add up.
So consider that number b to the two-thirds.
If you multiply it by itself three times, that is, you cube it, the result is b to the
two-thirds times b to the two-thirds times b to the two-thirds, which is b to the two-thirds
plus two-thirds plus two-thirds, or b to the second power.
So b to the two-thirds is just the cube root of b squared.
And in the same way for any fraction m over n, b to the m over n is just the nth root
of b to the n.
So we can raise b to any power, positive, zero, negative, or fractional.
And here is a related fact.
Any positive number at all can be written as sum power of the base b.
That is, given a number z, we can always say that z equals b to the x for sum exponent
x.
Here is an example.
Suppose we choose b equal to two.
Then two equals two to the first, and four equals two squared.
But what power of two is the number three in between two and four?
We should be able to write three equals two to the something between one and two.
And as a matter of fact, the number we need is about 1.585.
That is, three is equal to two to the 1.585 power, approximately.
You can easily confirm that with a good calculator.
Now we're ready to define what a logarithm is.
In a nutshell, a logarithm is an exponent.
For a base b, there is a function log b, the base b logarithm.
And the two equations z equals b to the x power and x equals log b of z mean exactly
the same thing.
So choosing b equal to two again, we have log two of one equals zero, because two to
the zero power is one.
Log two of two equals one, because two to the first power is two.
Log two of four equals two, because two to the power two is four.
Log two of eight is three, because two to the third power is eight and so on.
Log two of three is 1.585, because two to the 1.585 is equal to three.
How is the logarithm of a number actually computed?
To give a real answer would take us a bit far afield into the realm of calculus.
For now, just accept that there is a way and that a scientific calculator or a computer
can easily do it for us.
Since a logarithm is just an exponent, all the basic rules of exponents are also rules
for logarithms.
So let's list a few of the logarithm rules.
Log b of x times y is log b of x plus log b of y.
In other words, the logarithm of the product of two numbers is just the sum of their logarithms.
Logarithms turn multiplication into addition.
And likewise, log b of x over y is equal to log b of x minus log b of y.
In other words, the logarithm of a quotient of two numbers is just the difference of their
logarithms.
Log b of x to the n is n times log b of x.
That's the logarithm rule for powers.
Log b of 1 over x is minus log b of x.
That's the rule for reciprocals.
Log b of 1 is 0, no matter what base b we choose.
And as a general fact, if x is greater than y, then it is also true that log b of x is
greater than log b of y.
The logarithm of a bigger number is bigger.
Well, what base number b should we choose?
Well it can be anything we like.
In most mathematics and science there are two usual choices.
The first is b equal to 10, which gives us the so-called common logarithms.
On a calculator, the key for finding that logarithm is usually just labeled log with
no base indicated.
The other usual choice is to set b to the irrational number e equal to 2.71828.
That sounds a bit arbitrary at first, but the logarithms with that particular base have
a number of very nice mathematical properties.
In fact, that number e is one of the basic constants of mathematics along with pi, so
the base e logarithms are called natural logarithms.
And on a calculator, the key for those is usually marked ln.
In information theory, we use the base 2 logarithm, log 2.
That leaves us with a practical problem however.
You won't find a log 2 button on almost any calculator.
How do we calculate it when we need it?
The answer is the first note that the natural logarithm of 2 is ln of 2.693.
Then it turns out that log 2 of x is natural log of x divided by natural log of 2 or natural
log of x divided by 0.693.
For large numbers, it turns out there is a quick rule of thumb to estimate the base
2 logarithm.
All you need to do is count the digits.
Consider the number 5 billion, 5 followed by 9 zeros.
That has 10 digits.
The rule of thumb says to multiply the number of digits by 10 thirds.
10 times 10 thirds is 100 thirds or 33.3.
The base 2 logarithm of 5 billion is actually closer to 32.2.
Not bad, try it for yourself.
Now finally we come to the point of the logarithm math.
Our information source produces M possible messages.
Shannon says that the amount of information contained in a message from the source is measured
by the entropy, which is defined this way.
Entropy H equals log 2 of the number of possible messages, log 2 of M.
An important definition like this always inspires lots of questions.
Let's take them one at a time.
First, why use the logarithm?
For the answer, remember what happens if we have a pair of messages from the source.
The total number of message pairs is M squared.
So the entropy of a pair is log 2 of M squared, which is 2 times log 2 of M, which is 2 times
H, where H is the entropy of one message.
So the entropy of the pair is just twice as much.
The logarithm in the entropy has made our measure of information additive, just as we
wanted.
What happens when M equals 1?
The entropy of such a message is zero, because log 2 of 1 is zero.
And that makes sense.
If there is only one possible message, then you already know what the message is, even
before you receive it.
The message itself carries no information.
Why should we use base 2 logarithms?
This is an arbitrary choice, and sometimes you see other choices.
But the base 2 logarithm makes things work out especially neatly in information theory.
See, if we have a one-bit message source, M equals 2.
So H equals log 2 of 2, 1.
In effect, using base 2 logarithms means our entropy is measured in bits.
Okay, why do we call this the entropy?
That's a familiar-sounding word.
It comes from physics.
From the outset, Shannon knew that there was an analogy between his measure of information
and a very important concept in thermodynamics, the science of heat and energy transformation.
In that field, entropy helps determine whether a particular energy transformation is possible
or not, and it can be calculated by taking the logarithm of a certain number.
In fact, as we will see in a later lecture, the connection between information and thermodynamics
is much more than an analogy.
The quantity we call H was first proposed as a way of measuring information not by Shannon
in 1948, but by another Bell Labs engineer named Ralph Hartley 20 years earlier in 1928.
Hartley's work was an important precursor to Shannon's.
He did not call his measure entropy, and he did not recognize its connection to thermodynamics,
and he only took the first few steps along the path toward real information theory.
But we should not forget Hartley, and perhaps it makes sense to memorialize him with that
letter H.
Now comes the most important question of all.
What does entropy mean?
According to Hartley and Shannon, we can interpret H in two different ways.
H is the amount of information we gain when we receive a message, that is a measure of
the content of the message.
H is also the amount of information we lack before we receive a message, that is a measure
of our uncertainty about the message.
That second idea is interesting, because there are many situations where the importance of
a message lies in the fact that someone doesn't know it.
Consider for example the idea of a password.
Almost everyone these days has to use one or more passwords for security.
For example, I need a password to read my email, or to get money from my bank, or to
access confidential records at work, or submit a paper to a scientific journal, or to buy
something online, or even to open my garage door.
And the whole point of a password is that only one person, or only a few people, know
what it is.
If an unauthorized person were to try to access my garage or my email and invade my privacy,
they would have to guess my password.
How hard would that be?
Well, it depends on how many possible passwords there are.
If the password is a four digit number between 0000 and 9999, then there are 10 to the fourth
or 10,000 possibilities.
But a computer might be able to test that many possibilities in a short time.
Or it might be able to make 10 guesses each at the passwords of a thousand different people,
and probably get lucky at least once.
The entropy of a password, the base two logarithm of the number of possible passwords, is a
good measure of the password's security.
It measures how much the potential intruder does not know, how much he has to guess.
So the entropy of one digit is log 2 of 10, or 3.32 bits, and therefore a four digit password
has four times that entropy, or about 13.3 bits.
So suppose instead that my password were an eight letter word.
Well, there are 26 different letters in the alphabet, which gives an entropy of log 2 of
26 or 4.7 bits per letter.
Eight letters yields 8 times 4.7, which is 37.6 bits, which is a lot more than 13.3 bits.
That makes the password a lot harder to guess.
If it takes one second for a computer to try 10,000 four digit passwords, it would take
eight months to try 26 to the eighth power different eight letter passwords.
Or would it?
The annoying thing about passwords is that they are hard to remember.
Most of those eight letter combinations are gibberish.
Many people would rather choose a password that is a recognizable word.
How many eight letter English words are there?
Well, most of us can recognize and define less than 50,000 words without context or
dictionary.
And of those, maybe 8,000 from Ardvark to Zucchini have eight letters.
If that is the actual set of possibilities, the password entropy is log 2 of 8,000 or
13 bits.
That's actually less secure than a randomly chosen four digit code, which is 13.3 bits
of entropy.
This is why many computer systems set special rules on the user passwords.
They have a minimum required length.
They are required to include different kinds of characters, uppercase, lowercase, numbers,
and punctuation marks.
Sometimes the user is simply assigned a random jumble of a password and has to memorize it
as best they can.
The whole idea is to increase the password entropy.
My own favorite method for creating a good password, one that is both secure and memorable,
is to use a random series of words rather than letters.
I might choose four nouns at random, say, respect, plain, laugh, title, and use that
whole phrase as my password.
It's long, 22 letters long, but it isn't hard to remember.
What about the entropy?
Well, even if I stick to fairly common nouns, there are perhaps 2,000 of those, which means
they have an entropy of log 2 of 2,011 bits each.
A four-noun phrase has a password entropy of 44 bits.
A computer that could guess a four-digit number in one second would take 50 years to guess
my four-noun phrase.
A four-noun password is easy to remember, but very hard to guess.
That's the best kind of password.
The Hartley-Shannon measure of information, of course, is about much more than password
security.
It also tells us something important about the codes we can use to communicate messages.
Again, suppose that our source generates M different possible messages.
To take a concrete example, let's let M equal to five, the message is one of the letters
A through E.
Now we want to create a code that represents the output of this source as a code word made
up of binary digits, a string of bits.
How many bits do we need?
The code has to have as many code words as there are messages, so that no two messages
have the same code word.
If we use code words that are in bits long, then there are two to the N code words available
from all zeros to all ones.
So our requirement is that two to the N must be at least as great as M.
But that relation about exponents can be recast as a relation about logarithms.
That is, N is at least as great as long two of M, which is the entropy H.
We have arrived at a very useful fact.
The number of bits we need to represent a message from the source must be at least as
great as the entropy of the source.
But we could use more bits than H to encode the message, there is no limit to how inefficient
our code might be, but we cannot get by with less than H bits per message.
In our example, the source that produces letters A through E, the entropy is log 2 of 5, which
is equal to 2.32.
Therefore we need at least three bits to represent the output of the source.
The code might look like this.
Message A is assigning the code word 000, B is 001, C is 010, D is 011, E is 100.
There are some possible code words like 111 that we never use, but that's okay.
Each message from the source can be represented by three bits, which we can send by electrical
signals or any other way that we choose.
However, suppose our source is producing a whole series of messages, one after the other.
Each message is independent of the last one.
We could then group the messages together.
We might choose to make a code not for individual messages, but for successive pairs of messages.
There are 25 possible pairs, AA, AB, and so on, all the way to EE.
The entropy of the pair is 4.64, just twice the entropy of one message.
But that should mean that I can code a pair of messages with a 5-bit code word, and indeed
it does.
The code might look like this.
A is 0000, AB is 00001, and so on, all the way down to ED, which is 10111, and EE, which
is 1100.
You have probably noticed that we are assigning code words simply by counting up in binary.
The pair code uses the numbers 0 through 24 in base two notation.
There are lots of ways to build a code.
We just chose an especially easy method.
We are using five bits to represent two messages, so in some sense we are using 5 over 2, or
2.5, bits per message.
Now if someone had asked us to devise a code that uses 2.5 bits per message, we might have
asked how do we use a fraction of a bit?
Now we know the answer.
We use five bits for two messages.
This approach is called block coding.
Instead of coding messages one by one, we create code words for entire blocks of several
messages.
We do our coding wholesale, many messages at once.
That lets us be more efficient, only 2.5 bits per message instead of three.
And that savings ought to let us communicate more quickly, or store more information in
a given space.
2.5 bits of information is still greater than the fundamental entropy limit, which is 2.32
bits per message.
That's as it should be, but we've gotten closer to that limit.
So how close could we get?
Let's try coding triples of messages.
There are five to the third power, or 125 possible triplets, from AAA up to EE.
The entropy of a triplet is three times the single message entropy, or 6.96 bits.
That means that a code of seven bits could accurately represent the triplets.
Triplet AAA would be 00000, triplet AAB would be 000001, and so on, up to EE, which is 1111100.
That works.
And using seven bits for three messages means seven thirds, or 2.33 bits per message, which
is very close indeed to the entropy limit of 2.32 bits per message.
Shannon drew a general moral to this story.
The entropy H of the message source yields a lower limit to the size of the binary code
words in any code that can faithfully represent the messages.
By coding whole blocks together, and by choosing an efficient code, we can approach this limit
as closely as we like on a per message basis.
Therefore, the entropy H of a source equals the minimum number of bits required to represent
a message from the source.
That's a somewhat informal statement, so in the back of our minds we need to be careful
about its interpretation.
H is always a lower limit to the number of bits we need, but we can approach that limit
as closely as we wish by taking the wholesale approach and coding many messages together.
So, the Hartley-Shannon entropy is everything we wanted.
It is a measure of the information in a message that does not depend on how the message is
represented.
It's an additive measure, the entropy of a pair of messages is just the sum of the
entropies of the individual messages, and finally, our measure of information is indeed
related to the length of the message, specifically to the number of bits the message requires
if we use an efficient binary code.
Here's another way of thinking about the meaning of entropy.
Each bit in a code word, each 1 or 0, can be thought of as the answer to a yes-no question.
That is, the answer to a yes-no question is just a bit of information, so the entropy
tells us how many yes-no questions we would need to ask before we could determine the
message.
The code is just a convenient way of organizing the questions.
So, there's a familiar parlor game called 20 Questions, in which one player is supposed
to deduce the identity of some unknown object or concept chosen by the other players, asking
no more than 20 yes-no questions in the process.
So, why is the game 20 questions?
Why don't we play 10 questions, or 30 questions?
The real issue is, how many different possible unknown objects might be chosen by the other
players?
That's a bit tricky to estimate.
Let's suppose, for example, that they chose a word from the dictionary.
They are trying to be tricky, so let's consider a large dictionary.
The Oxford English Dictionary has about a quarter of a million different words in it,
which yields an entropy of log 2 of 250,000, which is about 17.9 bits.
So if a player is clever enough, she should be able to guess any particular English word
with just 18 questions.
On the other hand, if we include other kinds of unknown objects, the number of possibilities
will be much larger.
We have to consider all the well-known places, animals, books, television shows, fictional
characters, abstract ideas, and so on that don't have entries in the dictionary.
As a guess, let's estimate the total number of possibilities by the number of different
articles on Wikipedia, the giant, crowdsourced internet encyclopedia.
That is, we'll suppose that the subject of each article constitutes a potential 20 questions
object.
Wikipedia is pretty huge.
At the time I'm speaking, it has about 5 million articles.
That gives us an entropy of log 2 of 5 million, which is 22.3 bits.
So a very skillful player would therefore need 23 questions to be sure to guess the right
one.
Our entropy estimates yield a range of 18 to 23 questions.
And that is why we play the game with 20 questions.
10 questions is far too few the player wouldn't stand a chance.
30 questions would make the game too easy.
With 20 well-chosen questions, the asking player has a fair chance, but not a certainty
of winning.
That's what makes a good game.
The Hartley–Shannon entropy tells us about everything from password security to efficient
binary codes to how to design a good guessing game.
Unfortunately, the Hartley–Shannon entropy also has some limitations, and Shannon needed
to go well beyond Hartley's basic idea as he developed information theory.
For instance, our definition of entropy, the logarithm of the total number of possible
messages, puts every message on equal footing.
In effect, it assumes that every message is equally likely.
But a real information source does not behave in that way.
We know that in a typical English text, for example, letters like E and T are much more
common than letters like Q and X.
Some words appear frequently, others are extremely rare.
And that fact might have practical consequences.
We could design a code whose code words have different lengths.
The shorter ones would be used for common messages, and the longer ones for rare messages.
In that way, we could hope to use many fewer bits, fewer on average anyway, than the Hartley–Shannon
entropy limit would seem to allow.
Would that break the link between the number of bits and entropy?
What we need is a more sophisticated way of thinking about message sources and a new definition
of entropy to go with it.
Essentially, entropy should be a measure of novelty, of how unpredictable the source
is.
A source might have a million possible messages, but if it sticks to the same two or three
messages almost all the time, it's pretty predictable.
The entropy ought to be low.
And in fact, Claude Shannon's real definition of entropy is built on a mathematical definition
of surprise.
So next time, we will take a deeper look into entropy and surprise.
