Suppose someone sends you a one-bit message.
What does it mean to receive half of it?
It's a strange sounding question.
A one-bit message seems like an irreducible atom of information, one or zero, yes or no.
For a longer message with many bits, you could imagine receiving the first half of them or
the last half of them.
But how could you receive half of a single bit?
To find out, let's go back to our original story about communication between Alice and
Bob.
Alice has been asked a question, a yes or no question by Bob, and he's waiting for
her answer.
The two possible answers are equally likely, so the entropy of the message is one bit.
That's how much information Bob lacks before Alice speaks.
That's how much he will gain when he hears her answer.
But Alice and Bob are standing in the middle of a noisy party.
In that environment, it's hard to make yourself understood.
So when Alice gives her answer, Bob is not entirely sure he hears it correctly.
He thinks she said yes, she probably said yes, but he isn't quite certain.
Perhaps Bob is 90% certain.
That is, after Alice says whatever she says, based on what Bob heard, the probability of
yes is 0.9, and the probability of no is 0.1.
If Bob later learned that she said yes, he would not be very surprised.
If he later learned that she said no, his surprise would be greater.
Even after Alice speaks, her message still has some entropy for Bob, some uncertainty
about her answer.
Applying Shannon's formula, the entropy remaining is the average surprise.
So Bob started out lacking one bit of information.
He still lacks about half a bit.
Or he must have received about half a bit.
At the very start of this course, we defined information as the ability to distinguish
reliably between possible alternatives.
Bob has certainly gained some ability to distinguish which answer is Alice's, but
he cannot yet do it with perfect reliability.
That's what it means to gain half a bit of information.
This produces ambiguity.
The message that we receive may not tell us everything about the message that was transmitted.
Some information has been lost.
The possibility of error has been introduced.
And noise is ubiquitous.
There is no medium for information transfer or information storage that is completely
immune from error.
The thermal motions of electrons produce tiny fluctuations in electrical signals.
Water tapes get torn or damaged.
Distant thunderstorms or sunspots produce radio static.
Even printed books have typos.
So how do we take noise and error into account?
How do we defend against them?
That is, how do we communicate in the presence of noise and still avoid errors?
These are some of the most important questions in the science of information, and they are
the very questions that Claude Shannon explored in his path-breaking paper in 1948 in which
he established the foundations of information theory.
The answers he discovered are remarkable and surprising.
Shannon introduced the concept of a communication channel.
This refers to any means by which information can be conveyed, anything with an input and
an output.
For instance, a long electrical cable can act as a channel.
An input signal is introduced at one end and an output results at the other.
A fiber optic line or a radio link or sound waves through the air could all be channels.
My desk drawer could be a channel.
I write a phone number on a piece of paper, stick it in the drawer, and then take it out
six months later.
So the concept of a channel is not only about carrying information from place to place, it
can also represent the storage and retrieval of information over time.
Your memory is a channel.
If the output were always exactly the same as the input, then the idea of a channel would
be a pretty trivial one, but that is not usually the situation.
What the channel does is establish the relationship between input and output.
For each possible input, the channel determines which outputs are possible, and with what
likelihood they may occur.
Thus, the way to analyze a channel is to use probability.
The input is an information source X whose probability distribution over the messages
is P of X.
The set of possible outputs is Y.
For some channels, the output is exactly determined by the input, but other channels act in a
less predictable way, and several possible outputs might result from a given input.
To describe the channel, we need to describe the rule by which the inputs lead to the outputs.
That rule is specified by conditional probabilities, P of Y given X, the probability that the
output Y is produced given the input X.
The big lesson here is that the communication process involves both an input and an output,
which are related to each other, but are not necessarily the same.
We know the probabilities, P of X, of the various possible inputs, and the channel determines
the conditional probabilities P of Y given X.
The conditional probability P of Y given X is just the joint probability of P of X
Y over P of X, and we can turn that around, and say that the joint probability P of X
Y is P of X times P of Y given X.
The probability of an input-output pair is just the probability of the input times the
probability of the output given the input.
The Shannon measure of information is the entropy, the average surprise.
The input has an entropy H of X, which is the total amount of information in the transmitted
message.
The output has an entropy H of Y. The combination of input and output has a joint entropy H
of X Y, which we can calculate from that joint distribution P of X Y.
And now, here is a useful fact about entropy.
For any two variables with any joint probability, the joint entropy is no larger than the sum
of the individual entropies.
Now, this makes intuitive sense.
The information in both variables cannot be any larger than the information in one, plus
that in the other.
And it might be less.
If the input is a one-bit message and the channel is perfect, the output is always exactly
the same as the input.
So H of X is one bit, H of Y is one bit.
But since knowing X automatically tells us Y, H of X Y is also one bit, which is much
less than the sum of H of X and H of Y two bits.
Can we ever have H of X Y, the joint entropy, equal to H of X plus H of Y?
Sure, provided the two variables are totally independent of each other.
In that case, the probability of X Y is just the probability of X times the probability
of Y for every pair of XY values.
So before any communication occurs in the channel, the receiver knows neither the input
X nor the output Y, neither what is sent nor what is received.
So the amount of information that the receiver lacks at the outset is H of X and Y.
During the communication process, the receiver learns the value of the output Y, so he gains
an amount of information H of Y, but that still leaves something left over.
The information that the receiver lacks about X, even though he knows all about Y, call
that the conditional entropy H of X given Y.
That's how much information the receiver does not learn about the transmitted message.
It's a measure of failure to communicate.
Now information theorists sometimes call the conditional entropy the equivocation of the
message.
It tells us how much the receiver isn't sure about X, even when he knows Y.
Now a more positive measure would be the amount of information that the receiver actually
does gain, which is the difference between H of X and H of X given Y.
This very important quantity is called the mutual information.
Okay, there's a lot to keep track of here, so let's use a heuristic diagram to sum it
up.
The diagram shows two overlapping circles like a Venn diagram.
The left circle represents H of X, the amount of information in the input.
The right circle is H of Y, the amount of information in the output.
The whole area is H of X and Y, the information in both input and output.
H of X given Y is the part of X that is not included in Y, the information still missing
about X, even when Y is known.
We can see that this area is just H of X, Y, the whole thing, minus H of Y, the right
hand circle.
H of Y given X is the same on the other side.
The overlap of the two circles is the mutual information I of X and Y.
That's the Y information, that is also X information, the amount that the output of
the channels tells us about the input.
That's how much information the channel actually conveys to the receiver.
These are rather abstract concepts, so it helps to have a simple example in mind.
We suppose that the input X is a bit, possible messages zero and one, each with equal likelihood
one half.
The output is also a bit, however the output might not agree with the input, the bit might
flip, change to the opposite value along the way.
We let the number E represent the probability that an error of this kind occurs, so here's
a diagram.
The probability that input zero will yield output zero is one minus E, while the probability
that input zero will yield output one is E, and so on.
This is called the binary symmetric channel, binary because its inputs and outputs are
just binary digits, symmetric because the probability of error is the same whether the input is
zero or one.
This is just like our example of Alice and Bob at the noisy party.
X is what Alice says, yes or no, one or zero, Y is what Bob thinks he hears.
That might be the same as what Alice says, but there is some probability E that he hears
it wrong.
The error probability is determined by the level of noise at the party.
What about our entropies?
H of X, the entropy of the input, is one bit, so H of Y, the entropy of the output.
H of XY, the joint entropy of the input and output, is a little trickier to calculate.
We first write down a table of the joint probabilities, and applying Shannon's formula to these probabilities
involves some algebra, and we have to keep in mind the properties of the logarithm.
We'll skip to the answer.
The result for the joint entropy is that H of XY is one plus H of E bits, where that
H stands for the binary entropy function of the probability E. The binary entropy function
is easy to explain.
The quantity H of P is just the entropy of a binary source with message probabilities
P and one minus P. So let's sketch this function as P varies from zero to one.
The binary entropy is greatest when P is one half, that is when the two messages from the
binary source are equally likely.
In that case, the entropy is one bit.
The entropy goes to zero if P is either one or zero.
In one case, message zero is certain, and in the other case, message one is certain.
But in either case, there can be no surprise, so the average surprise is zero.
The whole curve is symmetric.
As far as the entropy is concerned, it does not matter whether zero or one is a more likely
message.
Finally, note how the curve of the binary entropy function is very steep at either end.
Even if P is equal to 0.9, where 90% of the way over to one, the value of H of P is still
surprisingly high, 0.469 bits, almost one half.
So now that we know that the joint entropy is one plus H of E bits, we can find out other
parts of our Venn diagram.
For instance, we know that H of X given Y, the amount of information Bob still lacks
about the transmitted message X after he receives Y, is just H of E bits.
The mutual information, the amount of information that is actually conveyed by the channel,
is one minus H of E bits.
To help us understand all this, let's think about a couple of extreme situations.
First, suppose there's no noise at all, and E is equal to zero.
Then H of E is equal to zero.
Whatever Alice says, Bob hears.
The mutual information, the amount that Alice actually conveys to Bob, is one full bit.
But now suppose instead that the party is so noisy that Bob cannot tell at all what Alice
is saying.
He is just as likely to be wrong as right about her answer, an error probability of
E equal to one half.
Then H of E has its maximum value, one bit, that makes the mutual information zero.
No information is conveyed.
When the output of the channel is completely independent of the input, the mutual information
is zero, and the channel conveys no information at all.
That's what happens when Bob's error rate is one half.
With that much noise, Bob's guess about Alice's message will be no better after the communication
than before.
He's getting no information.
So now consider a rather weird situation.
Alice and Bob are not just standing near each other in a room, they're communicating
through some machine, and the machine distorts the words that are said.
In fact, whenever Alice says no, it comes out sounding like yes, and vice versa.
This is the case where E equals one, and at first this seems even worse than before, but
the binary entropy function is zero, so the mutual information is one bit.
How can that be?
Shannon would explain this in this way.
We always assume that Alice and Bob have a common understanding of how their communication
channel works, how it distorts the inputs.
So they know the value of E. They know what's happening.
So in this situation where we have the distortion machine, Bob knows that whenever he hears
yes, Alice must have said no, and vice versa.
He can take that into account.
In other words, this channel is just as good as a perfect one.
Information is, after all, about the distinction between messages, not how those messages
are represented.
The distorting channel still preserves the distinction between the messages, because
different inputs still lead to different outputs.
No information has been lost.
The code has simply changed.
For all the in-between situations where E is not zero or one-half or one, we simply
need to calculate H of E and apply the formulas we found.
So if the error probability E is one-tenth, then H of E is 0.469 bits.
The mutual information, the amount of information that gets through, is therefore 0.531 bits,
just over half of a bit.
That seems like a harsh assessment of the channel's performance.
A 10% chance of error destroys not 10% of the information, but almost half of it.
That's what our calculation shows.
This seems counterintuitive at first, but it's actually quite reasonable once we carefully
consider what an error really entails.
So I'm a teacher.
Suppose I give an exam.
The exam consists of 100 true-false questions.
In other words, each is a one-bit question.
The student who takes the test is effectively a communication channel.
Information goes in during the term, and later, on the exam, information comes out.
But the student sometimes makes an error, so there is some noise in the channel.
Mathematically, for each question we can say that the input x is the original fact, and
y is the answer on the exam.
So consider a particular student, Charlie.
Charlie typically gets about 90% of the questions correct.
He misses 10 questions on the 100-question exam, putting down false instead of true or
vice versa.
His error probability, E, is equal to 0.1.
Is Charlie missing 10 bits of information, or more than 10?
Here is the straightforward way to answer that question.
What sort of additional information, feedback information, would I have to give to Charlie
so that he could correct all his mistakes?
I have to tell Charlie where all his errors occur.
I have to say something like, Charlie, you missed questions 7, 21, 29, 46, and so on.
That's a good deal more than 10 bits of information.
Just how many bits is it?
We can estimate that.
I essentially must convey to Charlie a list of 100 grading marks, which are either x's
for wrong answers, or check marks for right ones.
Those grading marks are like the output of a binary information source, the error source,
with a probability 1 tenth, that's E, for an x, and 9 tenths, 1 minus E, for a check
mark.
The binary information function H of E is equal to 0.469 bits, and that tells us the
entropy per grading mark.
Shannon's first fundamental theorem, the data compression theorem, tells us that I
could represent that grading information using about H of E bits per message in the long
run.
So I can correct Charlie's mistakes by providing him with about H of E additional bits of information
per question, around 47 bits for the whole exam.
So compare Charlie to another student, Diane.
She also gets about 90% of the questions right.
However, she works a little differently from Charlie.
When she thinks she knows the answer, she invariably does answer correctly.
But when she doesn't know the answer, she just leaves the question blank.
On the exam, she leaves about 10 of the 100 questions blank.
Diane is a very different channel.
She has three different outputs, true, false, and blank.
We can call them 1, 0, and B. Her errors are always ones in which she responds with a B
instead of 0 or 1.
This is called a binary erasure channel, which has a slightly different diagram.
So the two inputs are equally likely, so the table of the joint distribution is easy to
figure out as well.
The input entropy H of X and the joint entropy H of Y are the same as before, but the entropy
for the three-way output H of XY is not the same.
The equivocation H of X given Y turns out to be just E bits, and the mutual information
is 1 minus E bits per question.
So Diane really is missing only one-tenth of the information on the test.
She knows when she doesn't know the answer to a question, which means she actually knows
a good deal more than Charlie, who thinks he knows the answer but is sometimes wrong.
And that makes it easier to correct Diane's errors.
I only need to give her 10 additional bits of information.
Diane, on the questions you left blank, the answers are true, false, false, true, and so
on.
Now, human nature being what it is, I'm not sure I would have the nerve to give Diane
a 90% of the exam at Charlie, only a 53%, even though each missed 10 questions out of
100.
There might be complaints.
But from the standpoint of information theory, it would be perfectly fair.
So let's sum up what we have found so far.
A communication channel takes an input X and yields an output Y.
The two might not always be the same.
We describe the channel by some conditional probabilities, the probability of output given
input.
And if we put that together with the input probabilities, we find the joint distribution
over inputs and outputs.
And that lets us calculate intrapies H of X, H of Y, and H of X and Y.
From these, we have to find two important new quantities, the conditional entropy H
of X given Y, which is H of X and Y minus H of Y, which is also called the equivocation.
It measures how much of the input information is not conveyed by the output Y.
We've also defined the mutual information, I of X, Y, which is H of X minus H of X given
Y, which measures how much input information is actually conveyed by the output Y.
These new quantities correspond to different areas in our Venn diagram.
For the binary symmetric channel, H of X given Y is H of E, the binary entropy function
of the error probability E, and that means the mutual information, I, is one minus H
of E.
It matters very much what sort of errors we have.
In the binary symmetric channel, like Charlie's true-false exam, the errors look no different
from the correct bits.
That means it takes more additional information to correct the errors.
In the binary erasure channel, like Diane's exam, the errors are self-announcing and it
takes less additional information to correct them.
Of course, the exam example is not the usual situation.
When I provide the students with error-correcting information, we are assuming that those messages
are perfectly conveyed.
In a diagram, there are two separate channels.
The original facts are conveyed to the exam by the student, who may be careless or have
an imperfect memory, and so is a noisy channel.
But the exam corrections are provided by a second channel, me, who makes no errors.
It's a thought experiment we can pretend.
The second channel may be a reasonable idea for our classroom situation, but in most kinds
of communication we don't have a second perfect channel to use for error correction.
When our spacecraft sends a weak radio signal from billions of miles away, there is no
noiseless second channel to help us fix the parts of the signal that have been lost or
distorted.
In a situation like that, what do we do?
What can we do about errors?
Let's consider another example.
A general sends an order to an army in the field.
Unfortunately, the channel through which the message is sent is very noisy and a text character
has a 50% chance of being lost and replaced by an asterisk.
So the general sends the message five times, using the same noisy channel each time, just
to be sure.
The five received messages look something like this.
No single message can be read very clearly, but when we compare them, the original meaning
emerges, attack enemy at once.
So what therefore appears that we can reduce the overall probability of error if we are
willing to send our message redundantly.
In effect, we send a longer message.
Different parts of that message help us correct errors in other parts.
No part is immune from error, but there is always a chance that we can make a mistake
anyway, but we could at least improve the likelihood that we will get the message right.
That improvement comes at a cost.
We use the channel many times to send a fairly simple message that uses a lot more resources
and it slows down our communication.
And to be even sureer to avoid error, it seems that we must be even more redundant, and that
means repeating the message more times.
That might take quite a while.
Does this mean that the only way to avoid error, to make the error probability negligibly
small, would be to communicate extremely slowly?
Is there an inescapable trade-off between the volume of information and the reliability
of information?
Claude Shannon thought very deeply about this problem, which he regarded as the key issue
in information theory.
And he solved it.
His solution is profound and surprising.
First, he asked, what is the greatest measure of information that a given communication
channel can convey?
That's called the capacity C of the channel, defined as the maximum of the mutual information
I of X, Y, over all choices of input message probabilities.
Now we've calculated the mutual information for the binary symmetric channel and the binary
erasure channel, and it turns out that those were actually the maximum values for each
channel.
We've already calculated their channel capacities.
Here is what Shannon discovered, what he proved mathematically.
As long as you don't try to exceed the capacity of your channel, you do not have to communicate
more slowly to get rid of errors.
That is, suppose you decide on an information rate R, a number of bits to send per use of
the channel, so that R is less than C, the channel capacity, then by coding many messages
together, block coding, and by designing the right code and the right decoding procedure,
we can achieve two results simultaneously, sending an amount of information R each time
we use the channel and making the overall probability of error go to zero as we use
larger and larger blocks in our code.
Shannon also showed that if we get greedy and try to send too many bits each time, more
than the capacity C, then in the long run the overall error probability goes to one.
We're sure to make mistakes.
This is called Shannon's second fundamental theorem, and it is a big deal.
Most people agree that it marks the birth of modern information theory, and it is very
good news.
The second fundamental theorem tells us that we can defeat noise without paying too high
a price.
There is no inescapable trade-off between information volume and information reliability.
The possibility of error does not force us to communicate extremely slowly.
We can do error correction efficiently.
The channel capacity tells us just how efficiently.
For instance, consider the binary symmetric channel with error probability one-tenth.
The capacity of that channel is 0.531 bits.
So we choose a rate less than that, 0.5 bits is convenient.
To send one bit, in other words, we will need to use the channel twice.
It won't really work quite like that, of course.
What we will really do is send a thousand bits by using the channel two thousand times.
And that may involve a complicated coding and decoding procedure, but there is a way
to do it that makes our overall probability of error the probability that there is even
one error anywhere in the whole thousand bit message negligibly small.
Perhaps there is a profound lesson here, even a moral.
The world is full of imperfections.
When we calculate, we make mistakes.
When we try to remember, we forget.
When we communicate, we are all too often misunderstood.
But these facts need not lead us to despair.
The science of information through Shannon's second fundamental theorem tells us that errors
can be conquered.
We can build reliable systems out of unreliable parts.
That's the deep significance of error correction.
Unfortunately, Shannon's second fundamental theorem does not actually tell us how to do
this.
It's what mathematicians call an existence theorem.
It merely establishes that there exist efficient and effective error correction techniques.
The way Shannon proved this is very clever and a little indirect, alas his proof did
not reveal what those efficient and effective techniques might be.
But knowing that a thing is possible is a giant step forward.
And in the years and decades since the second fundamental theorem, we have learned a great
deal about practical error correction.
That will be the next subject to which we will turn.
We will learn that error correcting codes are everywhere, from deep space communication
to DVDs, essential but often unseen elements of our age of information.
