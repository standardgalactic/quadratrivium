Encryption, the process of protecting secrets, and cryptanalysis, the process of uncovering
those secrets are two sides of cryptography that are continually at war.
The more sophisticated one of them gets, the harder the other one becomes.
And so innovations on one side necessarily drive innovations on the other.
The whole history of cryptography is a kind of arms race between encryption and cryptanalysis.
The first half of the 20th century saw some of the greatest triumphs of cryptanalysis.
Code breaking changed the course of history more than once, and also opened new vistas
of history to our eyes.
And that was also the era when Claude Shannon used information theory to show what secrecy
actually meant, and how and when it could be broken.
During World War I, codes of both sides were regularly broken.
The most famous example was the Zimmermann Telegram.
This was a diplomatic message from German Foreign Minister Arthur Zimmermann to the
German Embassy in Mexico.
It proposed a German-Mexican alliance against the United States, which was still officially
neutral.
British cryptanalysis broke the code and gave the plaintext to the American government.
President Woodrow Wilson released it to the press, and the public outcry led the US to
join the war against Germany.
So in the aftermath of the war, the German general staff realized that they needed better
codes.
They decided to adopt the most advanced cipher system available at the time.
That was the Enigma system.
Enigma was a cipher machine originally designed for commercial use.
With a few modifications, it became the standard cryptosystem for the German army, navy and
air force.
Enigma was the main German code that the allied cryptanalysts faced during World War II.
There were many Enigma variations, but here is the typical setup.
The machine has a keyboard of 26 letters.
There is also a light board with 26 bulbs, one for each letter.
The electrical signal from the keyboard to light board runs through three rotor wheels.
The wheels each have 26 positions.
Each wheel contains circuitry to scramble the electrical connections in a different way.
The rotors can be switched out or changed around.
Each machine comes with five rotors in all.
At the front of the case, there is a switchboard type plug arrangement called the Steckerboard.
Using cables, the operator can connect up to ten pairs of letters.
An Enigma key consists of three parts.
First, there is the choice of three rotor wheels out of the set of five provided with
the machine.
That's 60 possibilities in all.
Second, each rotor is given an initial setting, labeled by a letter for each rotor.
There are 26 cubed or 17,576 settings.
Finally the ten cables of the Steckerboard can be connected in 150 trillion possible
ways.
And three factors give a vast number of possible Enigma keys.
How vast?
60 rotor choices times 26 cubed rotor settings times 150 trillion Steckerboard settings equals
about 160 million trillion possible key combinations.
Here is how you would use the Enigma machine.
You hit a key, the plain text.
A bulb lights up, giving the ciphertext for that letter.
The rotor then advances, so that the next letter will be encypered with a different rotor configuration.
The rotor positions do not repeat even in a message of thousands of letters, and the
Germans limited their coded messages to a few hundred letters.
To make things even more secure, the message actually has two keys.
The first few letters of any message, called the indicator, are coded using the same key
that everyone else is using.
Those letters give a new random three-letter rotor key, unique to that particular message,
to be used for the rest.
The Enigma was an amazingly sophisticated cipher system.
However, even as the German military began to adopt it during the 1930s, the Polish Cipher
Bureau were already able to break it, at least occasionally.
With weeks of hard work, they could sometimes figure out a key setting, and read Enigma
messages encoded with that key.
This was an impressive achievement, but it was of little practical use as the war approached,
and the Germans began changing their keys every day.
In 1939, the Germans invaded Poland, and the Second World War began.
Many of the Polish cryptographers were able to escape to England, bringing with them their
hard-won discoveries about Enigma.
The British then launched their own cryptanalytic effort, led by the brilliant Cambridge mathematician
Alan Turing.
At Bletchley Park, an old Buckinghamshire estate, the British built a massive code-breaking
center, employing thousands of people.
Their main job?
Break the Enigma system.
Read the German military's secret communications.
The British attack on Enigma was based on two things.
The first was a careful mathematical analysis of the Enigma system, led by Turing, revealing
a few slight weaknesses.
The second was the development, also led by Turing, of a special-purpose computer called
a bomb that automated some of the work.
The Poles had built simpler versions of these machines earlier, but Turing's bombs were
far more sophisticated.
The first ones were a combination of electrical and mechanical units.
Later versions were fully electronic and very fast, for runners of today's computers.
So how do you go about breaking the Enigma?
Different variations in the Enigma system did require the use of some different techniques,
but we can identify a couple of important steps in the main effort.
First, note that the Enigma system exchanges letters.
With a given setting, if A is transformed to X, then X would be transformed to A. This
is a useful feature for Enigma users.
It means that decryption and encryption are the same procedure, with the right settings
if you type the ciphertext on the keyboard, the lights show the plaintext.
But it also means that no letter is ever encrypted as itself.
A might be encrypted as D, or J, or W, but never as A.
And this is true for all the different models of Enigma, regardless of the key settings.
That provides a tiny clue.
A letter in the ciphertext tells us a little about the plaintext, just a tiny fraction
of a bit, but something.
When we intercept a message, we often have some context for it.
We know something about who is sending the message, or who it is intended for, or the
overall situation in which it is transmitted.
Thus, we might have a good reason to believe that some particular word or phrase appears
in the message.
Maybe we think the message contains the word submarine.
This is called a crib, which is cryptography jargon for a possible piece of plaintext
that we can use to help our cryptanalysis.
So where might the word submarine appear?
To find out, we can just take that word and slide it along the ciphertext.
Often one of the letters in submarine matches a letter in the ciphertext.
Then we know that submarine does not appear at that location, because in Enigma, no letter
is encoded as itself.
If our crib is long enough, there will only be a few possible places where it could occur
in the message.
The next step was one of Alan Turing's great discoveries about Enigma.
Suppose we have our crib in a possible location within the ciphertext.
It can happen that the plaintext and the ciphertext form a loop.
So the plaintext letter A is encoded as R. The next place over, plaintext R, is encoded
as N.
Two places down, plaintext N is encoded as A. A to R to N and back to A makes a kind
of closed loop.
You might think this would never happen, but it does happen, surprisingly often.
So why do we care about loops?
Well Turing realized that this loop structure does not depend at all on the plug settings
of the stacker board.
Check the telephone switchboard business at the front of the machine.
If you were to re-encode the same message with different stacker board connections,
loops of the same kind would appear in the same places just with different letters.
Therefore the loop structure only depends on the rotor settings.
So by thinking about closed loops, in other words, you can separate the Enigma problem
into two pieces.
First you attack the rotor settings and then you can tackle the stacker board later.
Here's where the bombs come in.
With a computing machine you can try all of the possible rotor settings.
At one per second it only takes about five hours.
Each time you find a setting that makes the right kind of loop, you have a possible piece
of the Enigma key.
Sometimes no setting gives the right kind of loop.
That means your hypothetical crib was wrong.
You have to start over.
If you do find a rotor setting that makes the right kind of loop, it could still be wrong.
To be certain, you need to solve for the stacker board settings.
But the stacker board problem is much easier if you have everything else right.
It amounts to solving a very simple type of substitution cipher in which ten different
letter pairs are swapped.
You can do that quickly with pencil and paper.
Through most of the war, the cryptanalysts at Bletchley Park were decrypting Enigma
messages on the same day they were transmitted.
Even when the Germans changed their procedures or added a fourth rotor to the machine, Bletchley
was able to find new techniques to break the cipher.
That real-time military intelligence was of immense consequence in the war.
Some say it made the difference between victory and defeat, especially in the struggle against
German submarines in the North Atlantic in 1942 and 1943.
The unlocking of Enigma became one of the most closely guarded secrets of the whole
war.
Everyone involved was sworn to absolute secrecy and the secrecy held for decades.
The story was only revealed in 1973.
There were other successes of Allied cryptanalysis during World War II.
The American Navy, for example, won the Battle of Midway, largely because they had broken
the Japanese naval cipher.
Knowing the location of the Japanese fleet, they could attack it from the air and sink
it.
It was a turning point of the Pacific War.
During World War II, Claude Shannon worked on technical projects in the United States.
He was part of an effort to develop radar-guided anti-aircraft guns.
He also worked on cryptography, evaluating the security of the special telephone system
that Franklin Roosevelt and Winston Churchill used.
Shannon met Alan Turing, who was visiting Bell Labs.
They had lunch together.
Though both were working on cryptography, that work was too secret for them to discuss.
They chatted about the future of computing machines.
At this time, Shannon was working out his ideas about codes and communication.
He realized that information theory could provide a new approach to cryptography.
Shannon tackled the subject with his characteristic originality and insight.
Rather than merely analyze this cipher or that cipher, he addressed himself to fundamental
questions.
What is a cipher?
What makes it secure?
What does it mean to break a cipher?
When is that possible?
Shannon wrote a secret report on mathematical cryptography at the end of the war.
This work was finally published in 1949 under the title, Communication Theory of Secrecy
Systems.
It was a sort of sequel to Shannon's revolutionary information theory of the previous year, and
it became the most significant paper in the whole history of cryptography.
His analysis begins with entropy.
If we have m possible messages, we have an entropy h equal to log 2 of m, measured in
bits.
That's the amount of information we lack about the message.
Now turn the statement around.
Suppose we lack h bits of information about the message.
Then the number of possible messages, roughly speaking, is m equal to 2 to the h power.
The more we learn about the message, the smaller our missing information, h. So there
are fewer and fewer possible messages consistent with what we know.
Eventually we arrive at the point where we know everything, all the information, and
there is only one possible message, h equals 0, m equals 1.
Then we know what the message is, or to be more precise, we know everything that is required
for determining what the message is.
We might still need to do quite a lot of calculation to turn the message into an understandable
form.
But we do not need to receive any more bits.
We have all the basic ingredients.
So I suppose Alice is sending a secret message to Bob, while Eve listens in.
We'll call the plain text p, and the key k, the cipher text which Eve might intercept
is c.
Now imagine that Alice and Bob are using a substitution cipher.
Eve intercepts a short cipher text message, WBJKRTO.
What does it mean?
Eve literally cannot tell.
All she knows is that the message is 7 letters long and uses no letter twice.
Aside from that, it could mean anything, depending on the key.
It could spell trouble.
It could mean no sweat.
The plain text could be treason.
It could be Tuesday.
Eve's set of impossible plain text messages is vast.
That's what makes Alice's message to Bob a secret.
But a substitution cipher can be broken by frequency analysis, if the cipher text is
long enough.
What does the link have to do with it?
Well, here's what Shannon says.
For any message is sent, Eve starts out with no information at all.
She then sees the cipher text, so the amount of information she gains is h of c, the entropy
of the cipher text c.
Well, how much is that?
Think about it on a letter by letter basis.
Each cipher text letter adds log 2 of 26, which is 4.7 bits of data, for Eve.
We'll call that value of 4.7 bits per letter, little h sub c.
If the message is l letters long, the cipher text information h of c equals l times hc.
That's how much information Eve gets.
What information does Eve lack?
From the very beginning, Eve does not know the key k, so she starts out with missing
information h of k, the key entropy.
Once the message is sent, she also lacks h of p, the information content of the plain
text.
And again, think about this on a letter by letter basis.
Each plain text letter adds little h sub p bits of entropy.
So the message of length l has h of p equal to l times h sub p.
The total amount of information that Eve lacks is therefore h of k, the key entropy, plus
l times h sub p, the plain text entropy.
So make two graphs.
Each horizontal axis is the length l of the message.
Each vertical axis is bits of information.
The first graph shows the amount of information Eve has.
It starts at zero and then gets larger as the message gets longer.
The slope of the line is h sub c equal to 4.7 bits per letter, the entropy of one letter
of cipher text.
The second graph is the amount of information that Eve lacks.
It starts at h of k, the key entropy, and then gets larger as the message gets longer.
The slope of that line is h sub p, the entropy of one letter of plain text.
So what is h sub p?
Well it is not 4.7 bits per letter.
As we have seen, English is redundant.
Our experiments led us to estimate that the entropy of English text is less than two bits
per letter.
Shannon's estimate, based on his own experiments, was about 1.5 bits per letter.
So our 26 letter alphabet with 4.7 bits per letter introduces a lot of redundancy.
For each letter, 4.7 bits minus 1.5 bits is 3.2 bits of extra redundant information,
on average.
Now information theorists are wary of using the letter r to mean anything but an information
rate.
So the per letter redundancy of English is called d.
d is equal to the cipher text letter entropy minus the plain text letter entropy, or 3.2
bits per letter.
Redundancy is a useful feature in a language.
All human languages have it.
It's what gives them a natural error correcting ability.
But that redundancy also has cryptographic implications.
It means that the graph of Eve's missing information goes up at a shallower slope,
only about 1.5 bits per letter.
If we put the two lines on the same graph, we notice something.
The information that Eve gets from the cipher text is a steeper line.
It starts out below, but eventually it catches up.
They intersect.
Of course, Eve can never learn more than there is to know about the message.
The cipher text line can never go above the other one.
When the two lines get really close to each other, the cipher text line bends over and
they run together.
But here's the point.
The difference becomes negligible.
The difference is the net amount of information that Eve lacks, entropy of the key plus entropy
of the plain text minus entropy of the cipher text.
That quantity shrinks to zero eventually.
And when that happens, there is only one possible key, and only one possible plain text consistent
with the cipher text that Eve has.
That means that Eve can break the code.
When does this happen?
Well, it happens near the point of intersection between the two straight line graphs.
Where does this happen?
Let's solve for L.
At the intersection point L, L times hc equals hk plus L times hp, or hk equals L times
hc minus hp, which is L times d.
So Eve's information catches up when the number of letters, L, equals the key entropy
divided by the redundancy per letter.
L is equal to h of k divided by d.
Shannon called this the unicity distance.
Now that word unicity seems to suggest some sort of oneness, and so it does.
If the message is much longer than the length L, there is only one key k and plain text
p consistent with the cipher text c.
The cipher text alone provides enough information for Eve to break the code.
If Alice and Bob are using a substitution cipher, there are 26 factorial or 400 trillion
possible keys.
That's a key entropy h of k of 88 bits.
So using Shannon's estimated English redundancy, d equals 3.2 bits per letter, the unicity
distance is 88 bits divided by 3.2 bits per letter, or 28 letters.
A cipher text much shorter than this is not enough to break the code.
There will be many possible plain texts consistent with the cipher text.
That's like our 7 letter message with many possible meanings.
But if the cipher text is much longer than 28 letters, there is probably only one sensible
plain text consistent with it.
It ought to be possible to break the code.
Now the unicity distance is not an exact line.
It is an approximate rule of thumb.
But it is an excellent way to estimate how much data is needed to break a code.
Of course, it does not tell you everything.
It only tells you when a solution is possible, when a code can, in principle, be broken.
It does not tell you what you will have to do to break it.
To see what I mean, let's estimate the unicity distance for enigma.
The answer might surprise you.
First we have to determine the key entropy h of k.
For enigma, counting rotor choices, rotor settings, and steckerboard connections, there
are 160 million trillion possible keys.
2 of this gives us 67 bits of key entropy.
That's actually less than the key entropy of a substitution cipher.
And the unicity distance is correspondingly less.
L is 67 bits divided by 3.2 bits per letter, 21 letters.
So in principle, an enigma cipher text message, much longer than about 21 letters, has only
one reasonable solution.
All we have to do is find it.
In theory, we need less material to find the solution to an enigma message than a substitution
cipher.
Does that mean that enigma is actually easier to break?
Not at all.
In actual practice, enigma is a much harder problem of cryptanalysis.
Why?
The answer is that we can solve the substitution cipher little by little in steps.
Frequency analysis gives us a start.
We can quickly determine the enciphered versions of E, T, A, and so on.
From these, we begin to recognize words and phrases and work out the rest of the letters.
It's a high entropy mountain to climb, 88 bits high, but we can climb it in easy stages,
one letter at a time.
Enigma does not seem to have that same step-by-step quality.
Its entropy mountain is not quite so high, but the sides appear to be sheer cliffs.
That's what makes it impossible to climb, or so the Germans believed.
Turing discovered that the rotor settings and the stackerboard settings of enigma could
be solved separately.
The rotor problem was hard, but not quite impossible.
And if you had that licked, the stackerboard problem was a trivial substitution cipher.
Thus, instead of a single sheer cliff all the way to the top, Turing found a clever path
that involved scaling one much lower cliff, plus an easy step-by-step ascent the rest
of the way, thereby just possibly allonturing one the Second World War.
Not all cryptanalysis has to do with espionage and war.
So I want to conclude our discussion by describing one of the great achievements of cryptanalysis
from the years just after World War II.
That was the decipherment by Michael Ventress and John Chadwick of Linear B.
Now ancient systems of writing can be examples of inadvertent cryptography.
They were originally designed to communicate, but over the millennia the code has been lost.
That makes us like Eve, trying to unravel the secrets of a ciphertext.
Sometimes we are lucky enough to have a crib, a piece of known plain text on which to base
our analysis, like the Rosetta Stone for Egyptian hieroglyphs.
Usually we do not.
There are many ancient writing systems that remain unread to this day.
Linear B was a system of writing used on Crete and elsewhere in the Eastern Mediterranean
about 3,200 years ago.
Many of the examples we have are inscribed on old clay tablets, some of them preserved
by baking when the building containing them burned down.
Most of these tablets appear to be inventories.
So many jars, so many chariot wheels, so many arrows.
These are illustrated with tiny pictures of the items.
We can guess at the pictures, but the rest of the symbols are mysterious.
It was a problem that many, many people, both professionals and amateurs, tried to solve
in the decades after Linear B was discovered, around 1900.
Some of the proposed solutions were very imaginative.
The language of Linear B was theorized to be Etruscan, or Hittite, or Semitic language,
or some unknown ancestor of modern Basque.
None of the theories was very convincing.
The man who deciphered Linear B was not a professional archaeologist or a linguist,
but an English architect named Michael Ventress.
Long fascinated by the Linear B problem, he began after World War II to make a serious
effort to decipher the code.
He had some clues.
The little pictures in the inventory tablets gave clues to the objects being listed there.
Also, it was pretty clear what sort of writing system it was.
Besides the pictures, Linear B has 87 distinct symbols.
That's too many for an alphabet, but not nearly enough for the symbols to stand for
words.
Linear B is a syllabic system.
Each symbol represents a syllable.
No more clues came from the American scholar Alice Kober, who had closely studied the
huge number of known Linear B inscriptions.
She discovered some fascinating regularities.
Similar groups of symbols would often appear differing only at the end, as if the words
changed form depending on whether they were singular or plural, masculine or feminine.
Kober deduced some of the rules for these changes.
Alice Kober died in 1950, just as Ventress was getting started.
He followed her lead, finding even more patterns and regularities.
He began to figure out which syllables were phonetically related, like ka and ka.
At this point, of course, he still had no idea what any of the syllables actually sounded
like.
The regularities that Kober and Ventress found were signs of the redundancy of the underlying
language.
And redundancy is the information basis for cryptanalysis.
However, without knowing what the underlying language might be, it was hard to see how
to proceed.
So Ventress began to search the inscriptions for plausible place names.
Place names are often very old.
In the U.S., for example, many river names are derived from Native American languages.
The Linear B place names might be remembered in later history.
The place names that Ventress conjectured gave him a few syllables to go on.
For instance, konoso for konosos, the place where many of the tablets were found.
The place names he identified gave him a kind of crib to Linear B. Their syllables in turn
began to unlock the sounds of words in the inventory lists.
Gradually, Ventress was forced to consider a surprising hypothesis.
The language of Linear B looked a good deal like Greek.
Archaeologists had assumed that the Greek language arrived in the area many centuries
after the inscriptions were made, but maybe the Greek speakers arrived earlier.
The language of the inscriptions was not the classical Greek of Plato and Euripides, rather
it appeared to be a previously unknown and much older form of the language.
In fact, it resembled the archaic form of Greek that had been recently reconstructed
from linguistic evidence by the Cambridge scholar John Chadwick.
So, working together, Ventress and Chadwick were able to establish beyond a doubt that
the Linear B inscriptions were indeed written in this archaic Greek.
The inscriptions are a direct record of a Bronze Age Greek-speaking culture, the historical
reality behind the legendary world of the Homeric epics.
Thanks to redundancy, every new sample of a code narrows the gap between the information
we know and the information we lack.
If that gap closes to zero, the message can have only one possible meaning, if we can
discover it.
That's Shannon's picture of cryptanalysis.
But does the information gap always close to zero?
With enough ciphertext, can we always deduce the plaintext and the key?
Can every code eventually be broken?
Using information theory, Shannon proved that the answer is no.
There are indeed secret codes that can never be broken.
In principle at least, we can communicate with perfect privacy.
Yet such ciphers present unique difficulties of their own.
So that is where we must turn next, to the theory and the practice of unbreakable codes.
