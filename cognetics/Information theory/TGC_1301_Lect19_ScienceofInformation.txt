In the 1960s, an entirely new theory of information, a way to go beyond Shannon, was discovered
independently by the most famous mathematician in the world and a teenage undergraduate at
the City College of New York.
The famous mathematician was the Russian Andrei Nikolayevich Kolmogorov, one of those rare,
wide-ranging minds that seems to have no limits to their genius.
Fundamental ideas and theorems that bear his name are found in a dozen mathematical fields.
And at the time of our story, he was in his 60s and a dominant figure in the mathematical world.
The undergraduate was Gregory Chaitin, Argentinian by birth and a transplanted New Yorker.
Later he would become an eminent computer scientist and researcher at IBM, known for
his mind-bending work on the foundations of number, logic and computation.
At the time we're talking about, he was a kid.
But this is not the story about competition, about David and Goliath.
We won't be concerned over much with who did what.
First, Kolmogorov and Chaitin arrived at the same ideas quite independently, and both
of them were anticipated in some respects by one of the early pioneers of artificial
intelligence research, Ray Solomanov.
This is a story of a new and different way of thinking about information, and that is
a place where a famous mathematician and a teenaged undergraduate are actually on equal
footing, where old problems and old ideas are suddenly seen in an original and surprising
light.
Shannon had based his theory on communication.
A message is encoded and sent to a receiver.
Kolmogorov and Chaitin based their new theory on computation.
A program is executed and produces some output.
Their theory came to be called Algorithmic Information Theory.
Shannon's information theory made sense because communication is universal.
All systems of communication are, at root, alike.
Any sort of message can be sent over any sort of channel.
So is there a corresponding universal idea of a computer?
There is, and the one who realized this was the mathematician who led the quest to break
the enigma, Alan Turing.
Now we've mentioned several people who, in one way or another, helped to invent the
computer, Charles Babbage, Claude Shannon, John von Neumann.
Turing's name has to be on that list, and maybe at the very top.
This is not because of his cryptographic work inventing specialized code-breaking computers
during World War II, it's because in the years before the war, before actual computers
actually existed, Turing was the man who taught us what a computer is.
We can imagine all kinds of machines for doing information processing.
Even one of Shannon's Boolean logic gates is an extremely simple computer, using bits
of input to compute bits of output, but not all computers are equally powerful.
One logic gate cannot add numbers together, it is too primitive.
But a network of logic gates can, they constitute a more powerful computer.
But in the same way Babbage's analytical engine could have solved problems that his
earlier simpler difference engine could not.
The limit of computation, the line between what is computable and what is not, seems
to depend on which particular computing machine you use.
Turing however showed something quite surprising, simple computers are all different, they can
do different things, but once the complexity of the machine crosses a certain threshold,
all computers are the same.
What does that mean?
Clearly the analytical engine, a slow mechanism of gears and wheels and punched cards, does
not look much like my laptop, a dazzlingly fast system of billions of transistors.
Also, my laptop is highly interactive, user input and output mesh together in a continual
cycle.
You can't play video games on the analytical engine.
But Turing says we should not be deceived by superficial differences of speed and technology.
The two computers are, at root, alike.
They can do exactly the same tasks.
They are embodiments of exactly the same concept of computing.
His argument is based on simulation.
One computer can simulate another.
I can run a program on my laptop that is a simulation of the analytical engine.
It mimics in software all the moving wheels and gears, all the punched cards.
That proves that any computation that is possible for the analytical engine is also possible
for my laptop.
At worst the laptop can just simulate the analytical engine performing it.
Turing showed that the reverse is also true.
It is possible to design a program for the analytical engine that simulates my laptop.
It's a huge program with billions of punched cards.
It might take billions of years to run, but every operation in the processor of my laptop
is mimicked by the engine.
So any task that my laptop can do can also be done, in principle, by gears and wheels.
At worst we can just have the engine simulate a virtual laptop.
According to Turing, all sufficiently sophisticated computers are equivalent.
They embody the same universal idea of computation.
Also because all kinds of information are essentially the same, Shannon could use the
bit as a simple common basis for describing information.
In the same way, since there is only one universal idea of computation, we can describe computation
using one type of ideal computing machine.
Turing described such a machine, the simplest computer that is equivalent to all the others.
It's called a Turing machine.
Here's what a Turing machine is like.
There is a long tape on which symbols can be written.
Turing's original design had an alphabet of seven different symbols, but we can make
do with just two.
That is, the tape could be a sequence of bits.
This single tape is used for all of the information storage of the machine.
Program, input data, scratch space for calculations, final output, it's all on the tape.
The tape is as long as we need.
Sometimes people say the Turing machine tape must be infinite.
That is a little misleading.
It just means that we can add more tape whenever that's required.
In addition, there is a very simple device that moves along the tape.
This device can go left or right.
It can read bits from the tape.
It can write bits on the tape.
It has almost no internal memory, just a handful of bits.
Just four bits of internal memory turns out to be enough.
So the machine operates in a series of steps.
At each step, the machine reads the bit from the tape and consults its own memory, depending
on what it finds, it writes a zero or one to the tape, moves left or right, and changes
its own memory bits.
If the internal memory bits are ever all zero, the machine halts.
The computation is finished.
It might seem that such a machine would be too trivial to do anything interesting.
In fact, it is computationally universal.
A universal Turing machine can simulate any other computer.
So from here on out, we will assume that we've agreed on a particular universal computing
machine, a particular standard computer.
It might be a Turing machine.
It might be some other kind of computer.
We will designate it UM for universal machine.
For convenience, we're going to allow our UM a few extra features.
These do not change anything that the UM can do, but they make it easier to discuss how
it does it.
We will assume that the input to our UM, the program and any initial data, are provided
on a separate tape that the machine reads from left to right.
The output of the UM will be printed on its own tape, and everything else occurs inside
the machine on an internal work tape.
Finally, when we discuss a program for the UM, we won't try to describe it by zeros
and ones.
That would be incomprehensible to us humans.
Instead, we'll give a heuristic, high-level description that we can understand.
Computer programmers call this pseudocode, not an actual program, but a careful outline
for which a program could be written.
So now we're ready to talk about information.
In a communication system, a code word is a binary string that allows the receiver to
figure out the message.
It is a description of the message written in the code.
Decoding is a kind of computation.
The receiver takes the code word as input, and computes the message as output.
Suppose that our message is itself a binary string, maybe it is this one.
Actually S1 is a million bits long, I've only shown you the first few bits, and the
rest are just like that.
Our job is to describe this string as briefly as possible.
If the receiver is a very simple machine, then we'll just have to send the bits one
by one.
That seems very inefficient.
Intuitively, there is just not that much information in the string.
If the receiver is a computer, like the UM, we could describe the string by sending a
program, an algorithm for making the string.
The UM runs the program, and it produces the string as output, and then it halts.
Message decoded.
So the program might look something like this.
Step 1.
Print 0, 1, half a million times.
Step 2.
Note that this takes advantage of the pattern in the string.
The result of the program is 0, 1, 0, 1, 1, millions of times, but the program itself
is much less than a million bits long.
This is the fundamental idea of algorithmic information theory.
The description of a binary string is a program for a UM that prints the string.
The information content of the string is measured by the size of the program, a string like
S1 that has a very short program has a low information content.
Some other string without a short description has a high information content.
For example, suppose the million-bit string looks like this.
I might have generated those bits by flipping a coin over and over.
In fact, I did do something like that, except my coin was the fluctuating quantum electric
field in a laboratory at the Australian National University.
Some fluctuations are about as random as anything gets.
So, since there is no pattern in S2, there is no short description of S2.
The shortest program we could send would probably look like this.
Step 1.
Print, quote, 1, 0, 0, 1, 1, 1, 0, 0, etc., etc.
Step 2.
Halt.
This program contains the string itself.
It is about a million bits long.
We have to be a little careful though.
Sometimes the pattern exists but is hidden.
So consider this string.
Any statistical test that you apply to S3 will tell you that it is random.
Zeros and ones appear with equal frequency.
Each bit is uncorrelated with the last one.
You might conclude that S3 has no brief description and thus a high algorithmic content.
But in fact, S3 is the binary expansion of the number pi, the ratio of the circumference
to the diameter of a circle.
These bits can be calculated by an elegant formula, a short program.
Thus S3 has a relatively low algorithmic information content.
This actually has a practical side in a way.
Consider computer games.
Many games allow you to wander around in the landscape of a virtual world.
Each hill and tree and house has its own unique shape.
Some of these virtual worlds can be enormous, larger than the Earth, and in some games you
can visit billions of different planets.
So how can any computer store so much data?
It can't.
And it doesn't.
The games use something called procedural generation to create their vast worlds.
That detailed landscape is the result of a computation.
The underlying program is surprisingly compact, so the algorithmic information content is
far less than it appears.
Yet like the bits of pi, even a simple procedure can produce a lot of variety and unpredictability.
You can wander the game world for years and never see two places exactly alike.
Yet when you return to your starting point, the game recreates the same place you left.
We have our UM whose programs and outputs are binary strings.
If the program P produces the output S and then halts, we can symbolize that with an
arrow.
P produces S.
Now not every program produces an output, because not every program actually halts.
Some get stuck in an endless loop.
We will return to that point later.
But if P produces S, then P is a description for S, as far as our universal machine is
concerned.
So any binary string S, program or output, has a length L of S, the number of bits in
S. For example, remember our binary string S1 that was just 0, 1, written over and over
and over.
L of S1 was 1 million.
On the other hand, the simple program that generated it, called that P1, was shorter.
L of P1 was much less than 1 million.
For any string, there are many different programs that produce it as output.
So for string S, we'll let P of S stand for the set of all programs, P, for which P produces
S.
Now, some programs in P of S may be quite long and inefficient.
However, there must be a program in P of S with the fewest bits.
And that program is the shortest description of S. We'll call it S star.
The length of S star is a way to measure the algorithmic information content of the string
S. This is often called the Kolmogorov complexity.
I am not too fond of that term.
Complexity means too many different things.
For example, when we talked about public key cryptography, computational complexity
referred to the time a program required to run.
Here, we care about the size of the program in bits.
So the term I prefer in place of Kolmogorov complexity is algorithmic entropy.
We'll remember Kolmogorov by using the letter K for it.
Its definition is simple.
K of S is L of S star.
Algorithmic entropy is the length of the shortest description, the shortest program that prints
the right output and then halts.
This new entropy plays the same role in the new information theory that Shannon's entropy
played in the old.
But notice how different the two are.
Shannon's entropy does not even make sense for a particular string of bits S.
Entropy is a property of an information source.
There are many possible messages, each one with its own probability.
Entropy measures the size of that universe of possibilities.
The algorithmic entropy makes sense for any particular string of bits.
The strings themselves can have a higher or lower information content according to
whether they require longer or shorter descriptions.
Now the two entropies are related to each other.
If we think about a source that produces binary sequences, the Shannon entropy is approximately
the average of the algorithmic entropy, taking the average over all of the possible sequences
that the source might produce.
So Shannon entropy is a way to estimate the algorithmic entropy on average.
Oh, and by the way, many of our equations in algorithmic information theory will use
those wavy approximate equal signs.
The two sides are perhaps not exactly equal, but the difference between them is small.
What this means is that we're leaving out a few technical details about which universal
machine we are using.
However, because all universal computers are essentially equivalent, the approximate equal
signs are okay.
The algorithmic entropy K gives us a completely new way to define randomness.
The old idea of randomness had to do with probability, but probability, like Shannon's
information theory, is really not about particular events.
A random variable X imagines a universe of possible events, not one in particular.
So in algorithmic information theory we can consider a particular string and say whether
it is random or has a pattern.
If the string S has a pattern, even a subtle one like the bits of pi, then it can be described
by a program much shorter than S itself.
K of S, the length of the shortest program S star, is much less than the length of S.
On the other hand, a string is random if there is no short way to describe it.
Of course you can always describe a binary string just by listing it, the program that
says print S and then halt.
That program has about the same length as S itself, therefore S is random if there is
no shorter way than that to describe it.
K of S, in other words, is about equal to the length of S.
This definition of randomness has nothing to do with probability.
Indeed, Kolmogorov believed that the idea of information was more fundamental than the
idea of probability.
Now there is an amusing way of connecting algorithmic information to probability.
Recall the old idea of having a monkey bang away at a typewriter.
It hits the keys randomly with equal probability.
We can then calculate the likelihood, very, very tiny of course, that the monkey by chance
writes one of Shakespeare's plays.
Let's play that game a little differently.
Our monkey will be a programmer for our universal computer.
Its keyboard will just have two keys, zero and one.
It hits them with equal probability, one half.
Whatever the monkey types in is given to the U.M. as a program.
We run it and see what the computer does.
Now one point to note here.
We have assumed that the universal machine reads its input tape, where the program is,
from left to right.
So if the first hundred bits of the tape form a valid program, none of the rest of the bits
will even be read by the computer.
So if P is a valid program for the universal machine, then P followed by some additional
bits like P plus zero, one, one, zero, one, zero, zero is really just the same program.
It does the same thing.
Only the first L of P bits of the input tape would matter.
So what is the probability that our monkey programmer will write program P?
Well each bit in P has probability one half, so the overall probability is one half to
the power L of P, and that's two to the minus L of P power.
That's the monkey probability of program P.
Now the monkey is sure to type some kind of program.
The computer will do something.
True it might keep running forever and never stop, but there are programs like that.
So the sum of all the monkey probabilities is one, and we can write that fact using our
old sigma notation for sums.
We have seen this before, back in lecture five.
It's a version of the Kraft inequality about the lengths of code words in a prefix free
code.
The U.M. programs are a kind of prefix free code, since no program is an initial segment
of any different program.
On a binary tree diagram of programs, once we reach a valid program, there aren't any
other programs on the same branch.
So suppose we want the monkey programmer to produce the string S. S stands for Shakespeare.
The monkey does not have to type S itself.
What it needs to do is type some program P so that P produces S on the universal machine.
That might be easier, and there are certainly many ways to do it.
So how likely is it that we end up with S?
Call that the total monkey probability to produce S, or M of S. To calculate that, we
must add up the monkey probabilities for all the programs in P of S, the programs that
produce S and then hold.
However, one of those possible programs is much more likely than the others.
That's the shortest program, S star.
So let me explain why S star is so dominant.
Consider two programs, P and P prime, both of which produce S. P is 500 bits long, while
P prime is 510 bits long.
That does not seem like much of a difference in length, but it makes a huge difference
for monkey probability.
That 10-bit difference means that P prime is two to the tenth times less likely, about
a thousand times less likely.
So by far the most probable way for the monkey to produce S is for it to type program S star.
And this means that it is not all that unlikely for the monkey to produce a string of a million
ones.
The shortest program that produces that string is very short.
You could imagine, just barely imagine, a monkey typing that program by accident.
The total monkey probability for a string is therefore about 2 to the minus L times
S star, which is 2 to the minus K.
So we can take the logarithm of this equation and solve for the algorithmic entropy K of
S.
K is the log of the reciprocal of M.
That log of reciprocal probability is what we call surprise in the Shannon theory.
However it makes perfect sense to say algorithmic entropy equals the monkey's surprise.
This is more serious, a lot more serious than it sounds.
In fact, it can shed light on a centuries-old philosophical principle.
William of Occam was a 14th century English philosopher, a Franciscan monk who studied
at Oxford and got in trouble for his rationalist heterodox views.
He was most famous today for Occam's razor, a guiding principle for his philosophy.
According to Occam, when we are faced with a choice of hypotheses, we should choose the
simplest one.
He said it is useless to do with more, what can be done with less.
The simpler theory is not necessarily the true one, but it is more likely to be true.
650 years later, Ray Solomonov was thinking about artificial intelligence.
He wanted to teach computers to reason as we do, inductively starting with data and
choosing hypotheses to explain the data.
Solomonov said that data can be represented by a string of bits, S. A theory is essentially
a program P that describes the data, S. That theory could be and should be much more compact
than the original data.
If there is a lot of regularity and pattern in the data, if the sun comes up every day
and apples always fall to the ground when you drop them, then that will be reflected
in the fact that the description P can be made much, much shorter than S.
Any theory P is a program within P of S. And Occam's razor advises us that the computer
should choose the shortest one, which is S star.
That one is supposed to be more likely.
More likely how?
More likely in monkey probability.
If a monkey were to generate theories at random, banging on its binary keyboard, and
it happened to produce a theory that matched the data S, then it would most likely get
there by typing S star.
That's the sense in which S star is the most likely theory.
Of course if there are two or more theories that have exactly the same number of bits,
they will be equally likely in Solomonov's monkey version of Occam's razor.
The computer will have no basis to choose between them, but as we saw, even a few bits
one way or the other can make a significant difference in probability.
The monkey odds are strongly in favor of the shortest description S star.
Throughout this course we have seen that Shannon's information theory finds its way into all
kinds of surprising places, language, cryptography, genetics, even gambling.
What about algorithmic information?
Is it just a fun intellectual game, or can we apply it?
Can we use it to help us understand nature?
The answer is yes.
Algorithmic information has something to tell us about thermodynamics.
Specifically, it helps us understand Maxwell's demon.
Now Maxwell's demon acquires information about the microstate of a physical system.
It uses that information to guide the system, opening and closing a trapdoor between gas
containers, for example.
So the thermodynamic entropy of the system decreases.
But Bennett showed that the second law is safe.
At the end of the day, the demon must erase its stored memory record, and the thermodynamic
cost of that erasure, given by Landauer's principle, is enough to prevent any net decrease
in overall entropy.
Then perhaps that explanation leaves us a bit uneasy.
The second law is okay in the end, but what about in the middle?
So the demon has not yet dissipated any waste heat, its memory is full of the microscopic
information it observed, and the entropy of the gas is reduced.
Isn't that at least a temporary reduction in entropy?
That's the question that caught the interest of Wojtek Zurek of Los Alamos National Laboratory.
Wojtek has long been a very creative physicist making contributions to subjects from cosmology
to quantum computing, and he reasoned like this.
When the demon acquires its information, it reduces entropy elsewhere.
When the demon gets rid of its information, it increases entropy elsewhere.
So the demon's information must itself be a form of entropy.
Yet the memory record is some particular string of bits.
It is not missing microstate information like ordinary entropy, it's not missing at all.
The demon knows it, it's part of what the demon sees, part of its idostate.
How can that have entropy?
Zurek said it has algorithmic entropy.
The total thermodynamic entropy has to be the sum of two parts.
The first part is K2, our Boltzmann constant, times log 2 of m, the number of possible microstates.
That's K2 times the Shannon entropy of the microstate.
And all the available information.
But there is also a second part to the entropy.
That is K2 times K of m, the algorithmic entropy of the demon's memory record, little m.
So here is Zurek's formula.
The total physical entropy is the sum of Shannon entropy and algorithmic entropy.
When the demon's memory is empty, all zeros, K of m is tiny.
When the demon acquires its microscopic information about the gas, the algorithmic part of the
entropy increases enormously.
Zurek showed that the total entropy, including the algorithmic part, never goes down on average
at any stage.
As the demon acquires and uses information, ordinary entropy goes down, but K goes up.
As the demon erases its data, K goes down, but ordinary entropy goes up.
The second law holds true throughout.
Zurek's formula is an update on Boltzmann's original formula, S equals K2 times the log
2 of m.
It takes into account the information that we possess, as well as the information we lack.
Now we've never had to think of this before, because in ordinary circumstances the information
we possess is negligible.
After all that constant K2 has a value of 10 to the minus twenty-third joules per Kelvin
per bit.
Even the memory capacity of the human brain, around 10 to the sixteenth bits, makes only
a tiny thermodynamic contribution.
But Maxwell's demon teaches us something new.
The algorithmic entropy K, the Kolmogorov-Chitin information content of a memory record, has
a real meaning for the laws of physics.
There's only one problem with Zurek's formula.
The algorithmic entropy K of the memory record M is uncomputable.
I do not mean to say that we do not know how to compute K of M. I mean to say that we do
know for sure that there is no way to compute K of M. Even a universal computer can never
do it.
That is going to be an incredible story.
It is solid mathematics, but at times it may seem to verge on the mystical.
That's because it is the story of the inherent limits of computation and logic.
It's a story that takes the science of information to the edge of reason.
