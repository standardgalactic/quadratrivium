Every branch of science has its legends, favorite stories somewhere between truth and
myth about great people and great discoveries.
The science of information is no exception.
Let me tell you one.
When Claude Shannon invented his measure of information, so the story goes, he was not
quite sure what to call it.
So he talked it over with the great mathematician and physicist John Von Neumann, and Von Neumann
said you should call it entropy.
Entropy is a term from thermodynamics, the science of heat and energy transformations.
First of all, Von Neumann said your information measure is mathematically related to thermodynamic
entropy, so the name is correct.
Even better, nobody really understands entropy, so you will win every argument.
Shannon later said that the famous conversation never took place.
He himself recognized the link to thermodynamics and chose the name entropy because of it.
But the story is one that information scientists like to tell anyway, that's because it contains
a nugget of truth.
Physicists and chemists had been using a concept called entropy since the 19th century.
They defined it, measured it, analyzed it, pondered it, debated about it.
But before Shannon's revolution, they never really understood it.
I want to tell you how the concept of entropy was originally discovered.
At first it seemed to have nothing to do with information.
It was simply a way of determining whether an energy transformation was possible or not.
We however can look back at the story through information colored glasses and see entropy
for what it really is.
For thermodynamics is not just a practical and profound branch of physics, it is also
a place where the laws of nature and the laws of information meet.
So the first big idea in thermodynamics is the familiar idea of energy.
Every separate part of the universe, we call that a system, has a certain energy content
which we denote by E. The physics unit of energy is the jewel.
Roughly speaking, that's the energy of motion that a baseball acquires if you drop it a
couple of feet.
Energy can change forms and be transferred from one system to another, but the total
amount of it remains constant.
Energy cannot be created or destroyed, it's conserved.
That's the first law of thermodynamics.
Systems can exchange energy, and this happens in two distinct ways.
The first way is called work, W. This is energy transfer that is associated with a force
acting through a distance, like the force of gravity acting on the baseball after you
drop it.
Less obviously, work is also done when you charge up an electric battery, or when a material
becomes magnetized.
It's all work.
The second way that energy is transferred is called heat, that's Q. If you put two
bodies at different temperatures next to each other, there is a heat flow from the hotter
one to the colder one.
That energy transfer isn't work, since the objects are not exerting forces or being
moved, that's heat flow.
And according to the first law, a system's energy cannot just go up or down on its own.
There must be some kind of energy transfer to or from the system.
So the total change in the system's energy, delta E, is just the heat plus the work,
Q plus W.
And we should notice here that Q and W could each be positive or negative, depending on
whether the energy is flowing into or out of the system.
Now as far as the first law is concerned, any sort of energy transformation or exchange
is fine.
It does not matter, for instance, whether heat flows from hot to cold or cold to hot.
Either way obeys the law.
But in the real world, heat only flows from hot to cold and never the reverse, or take
another example.
We can always transform work into heat.
If I rub my hands together, they warm up.
Friction turns muscular work into heat.
The reverse is not always possible.
We cannot always turn heat into work.
Now we can do that to some extent.
That's how a steam engine works.
A heat from a fire boils water and the steam pushes a piston.
But not all of the heat is turned into work.
Some of it is released into the cooler surroundings as waste heat.
And try as we might.
We cannot build an engine that turns heat into work with 100% efficiency.
So there is another principle at work besides the first law.
And that's exactly what the German physicist Rudolf Klausius discovered in the mid-19th
century, the second law of thermodynamics.
And to express his new law, Klausius introduced a strange new quantity called entropy, s.
The word derives from the Greek word tropos, meaning transformation.
So what is this mysterious entropy?
It's a property of a system, like its mass or its energy content.
S can change as the system undergoes changes.
It may increase or decrease, and entropy moves from one system to another when heat is transferred.
Klausius' second law says that the total entropy can never decrease.
S can stay the same, or it can go up, but it can never go down.
So let's see how it works.
The amount of heat Q is transferred to a system at temperature T. Klausius says that that
means that its entropy changes by delta S equal to Q divided by T. And that's the absolute
temperature, by the way, the temperature above absolute zero.
It's measured using the Kelvin scale.
Room temperature is about 300 Kelvin.
So we can see that the units of entropy are energy divided by temperature, joules per
Kelvin.
So suppose some heat flows from system one to system two, from the hot coffee to the
cool cup.
The amount of heat that flows is Q. System one loses heat, so its entropy goes down by
Q over T1.
System two gains heat, so its entropy goes up by Q over T2.
The total change in entropy is delta S equal to Q over T2 minus Q over T1.
The second law tells us that this must be positive.
Total entropy cannot decrease.
So the first term must be bigger than the second, which means that T2 is less than T1.
The heat always flows from the higher temperature, T1, to the lower one, T2.
It cannot go the other way.
Or consider that steam engine.
Heat flows from the boiler, so the boiler's entropy goes down.
The engine produces some work, but work doesn't affect entropy.
To satisfy the second law, therefore, the engine must expel some waste heat to raise
the entropy of the surroundings.
That's why it cannot be 100% efficient.
A couple of years ago, I was in Zurich at ETH, the Swiss Federal Institute of Technology.
That's the place where Clausius discovered entropy, and I happened to be there for a
conference on thermodynamics.
And near the university, I was delighted to find myself on the Clausius Strasse, a street
named after him.
Unfortunately, it was not a one-way street.
That would have been more appropriate, because that is the great lesson of the second law.
You cannot always go back.
Once the overall entropy increases, it cannot be decreased.
Not every process can be reversed.
Now, we do not directly perceive or experience entropy, but by doing experiments and seeing
which processes happen and which do not, we can measure it.
And we learn that certain factors make the entropy of a system higher or lower.
A gas has a higher entropy than a solid.
The gas entropy is higher when it occupies a larger volume.
A system has a higher entropy when it is hot rather than cold, and when its parts all have
the same temperature.
Yet none of these observations quite reach the answer to the question, what is entropy?
That answer was discovered by the Austrian physicist Ludwig Boltzmann about a dozen years
after Clausius first introduced the entropy concept.
Boltzmann's idea about entropy was brilliant, perhaps his greatest achievement.
His equation for entropy is inscribed on his tomb in Vienna.
But this is the place where we need to put on those information-colored glasses, because
the real meaning of Boltzmann's discovery is that entropy is information.
Everything we see around us is macroscopic.
It is composed of huge numbers of microscopic particles, the atoms and molecules.
And all of those particles are in ceaseless motion.
People sometimes say that those particles are moving randomly, but that isn't quite
right.
They do not zoom here and there in a whimsical way.
The atoms move and exert forces on each other according to the usual laws of physics.
But there are so many atoms, and the motions and interactions are so incredibly complicated
that we know almost nothing about the details of which atom goes where.
In fact, about any macroscopic system, we only know a few bits of information.
We know the mass, the volume, the temperature, and so on.
That small amount of large-scale information that we possess is called the macrostate of
the system.
We do not know the microstate, the exact situation of every individual atom.
So there are many, many, many possible microstates that are consistent with what little we do
know with the macrostate.
So let's call that number of possible microstates M. It's a mind-bogglingly huge number.
And since there are M possible microstates, and we don't know which one we have, we are
missing log 2 of M bits of microstate information.
That's the Shannon entropy H of the microstate message.
That's how many binary digits we would need to record all of the details about all of
the atoms in the system.
So here is Boltzmann's discovery in information language.
The thermodynamic entropy of Clausius, S, is simply a constant times the microstate
entropy H. We'll call that constant K2.
S is equal to K2 times the log 2 of M, which is K2 times H. That's our version of Boltzmann's
formula.
Thermodynamic entropy and information entropy are really the same.
So what is K2?
Well K2 is the thermodynamic value of one bit of missing microstate information.
That value is very small, about 10 to the minus 23rd joules per Kelvin per bit.
So let's look at a simple example.
In this jar is a liter of air at room temperature.
The thermodynamic entropy of the gas turns out to be around 6 joules per Kelvin.
How many bits of information do we lack about the microstate of the gas?
Well, according to the Boltzmann formula, the Shannon information in bits is H equal
to S over K2, or 6 times 10 to the 23rd bits.
That's a vast amount of information.
It's comparable to the total amount of data of every kind on the entire internet worldwide.
That's how much we do not know about what is in this jar.
Boltzmann, of course, did not think about entropy as microstate information.
The concepts of information theory were decades in the future.
He regarded entropy as a measure of mixedness or disorder.
The larger the number of possible microstates, the more mixed up and disorderly the system
is.
But the information view is more fundamental.
So suppose we take our liter of gas and expand it to twice the volume, two liters while keeping
it at the same temperature.
How does its entropy change?
Well, here's how Clausius would calculate that change.
He said we could make the change by slowly expanding the gas from one liter to two liters.
Now, ordinarily, a gas would cool as it expands.
So we should add some heat to maintain the temperature.
Adding heat adds entropy.
The change in entropy is just q, the heat, divided by t.
So knowing some details about how gases behave, we can determine the answer.
We have to add about 90 J of heat as the gas expands and the temperature is 300 Kelvin.
So the change in entropy is 90 divided by 300 or 0.3 J per Kelvin.
So expanding the air increases its entropy about 5%.
Now look at the same problem through our information-colored glasses.
We would take a lot of bits to describe the microstate of the gas molecules in a one liter
volume.
How many more bits would it take to describe them if they occupied a two liter volume?
Well we can think of the bigger volume as being made up of two one liter halves.
To describe where the molecule is, we first have to tell which half it's in.
That's one bit of information.
But once we've narrowed it down to a one liter volume, we lack exactly the same information
about the molecule that we lacked before.
We still need to specify where it is within a one liter volume and how it is moving and
so on.
So in other words, by doubling the volume, we have added exactly one additional bit
of microstate information for each molecule.
So if we have n molecules, then delta H, the change in the missing microstate information
equals n bits.
And therefore the thermodynamic entropy has increased by delta S equal to K2 times n bits.
Now it turns out, in one liter of air, there are about three times ten to the twenty second
molecules.
So that works out to a thermodynamic entropy change of 0.3 Joules per Kelvin.
That is exactly the right answer.
The same answer that we found by keeping track of the heat added as the gas expands.
But thinking about information makes the meaning obvious.
If we double the volume, each air molecule has twice as many places it can be.
That means we lack one additional bit of microstate information for each molecule.
Here's another related example.
Suppose our gas is composed of two kinds of molecules in equal amounts.
Let's say we have 50% nitrogen and 50% oxygen in the air.
I could imagine two situations.
In one, the two gases are separate, each in half the volume, separated by a thin wall.
In the other, the two gases are all mixed together throughout the volume.
The mixed up situation has a higher entropy.
The difference is called the entropy of mixing, and it is easy to see why there should be
a difference.
If the two gases are separate, I know one more bit of information about each molecule.
I know that each nitrogen molecule is in this half and each oxygen molecule is in that half.
But when the gases are mixed, any of the molecules could be anywhere.
So with in molecules altogether, that's an in-bit difference in microstate information
or in thermodynamic entropy terms, K2 times in.
By the way, the mixing of gases is a nice example of the second law.
If the gases are separate and we remove the wall, then they mix together.
The entropy goes up.
But we never see the reverse occur.
Two mixed together gases don't spontaneously separate themselves.
We can separate the gases, but the separation process would have to generate entropy in
some other way, maybe releasing some waste heat.
You will notice that the Boltzmann formula is related to the Hartley-Shannon formula
for information entropy, the logarithm of the number of possibilities.
This assumes, as Boltzmann did, that all of the possible microstates are equally likely.
But there are situations in which some microstates are more probable than others.
For instance, think about one nitrogen molecule in this room.
Because of gravity, that molecule is slightly more likely to be near the floor than near
the ceiling.
That's why the density of air decreases with altitude.
So not all microstates are equally likely.
This formula for entropy was extended to these situations by the American physicist
Josiah Willard Gibbs.
Gibbs found that if the microstate M has probability P of M, then the thermodynamic entropy S
equals K2 times the sum over all microstates of P of M times log 2 of 1 over P of M.
Now when Shannon, thinking about coding and communication, discovered his measure of information,
he recognized the link to Gibbs's formula, and that's why he used the term entropy.
So what about the second law?
The whole importance of entropy for physics is that it tells us which processes are possible,
and which ones are not.
The total entropy can stay the same, or increase, but never decrease.
What does that have to do with microscopic information?
So to explain the connection, we'll need to draw some pictures.
First I want to imagine the collection of all conceivable microstates of a system.
That forms a kind of space, what physicists call the system's phase space.
In my picture, I'll draw that space as a big 2D rectangle, but for a real system like
a cup of coffee, it is an unbounded space with a gigantic number of dimensions.
My picture is symbolic, not literal.
Anyway, each point in the space is a microstate of the system.
Two different points are two microstates that differ in some way, the arrangement of the
atoms in space, or how they are moving around, and so on.
Even one small difference in one single atom will give you two different microstates.
When we talk about microstate information, we're talking about detailed coordinates,
just exactly where the system is located in phase space.
So what's a macrostate in our picture?
Well the macrostate of the system does provide some information about the microstate, but
it leaves out a lot of detail.
If the phase space is like a map of the United States, a macrostate is like Texas.
Knowing that you are in Texas does say something about where you are, but there are still a
great many different specific points within Texas where you could be.
So in our picture of phase space, a macrostate is a region containing many microstate points.
Some macrostates are smaller, like Delaware.
They include fewer microstates.
Some macrostates are larger, like Texas.
They contain more microstates.
The larger regions have a greater entropy, because you'd need more information to locate
the actual microstate within that region.
And thermodynamic entropy is K2 times the information that we lack about the microstate.
Now over time the microstate of a system changes.
It wanders around in phase space.
For instance, in a jar of air, the molecules move and collide with each other.
The macrostate might not change, overall the air might look about the same, but the microstate
continually changes in complicated ways.
And this is true whether we think of gas molecules as little balls that fly around and exert
forces on each other, acting according to Newton's laws of motion, or as quantum mechanical
particles that propagate and interact as waves, according to the quantum laws.
It is also possible that changes in the microstate lead to changes we can see, changes in the
macrostate.
The system state wanders out of one macrostate region and into another.
But the laws that govern those changes are still microstate laws.
However it happens, there is a remarkable and terribly important fact about the laws
that govern how microstates change.
It is one of the most important facts in all of physics.
We call it the principle of microstate information, and it goes like this.
As microstates change, microstate information is preserved.
Well, what does that mean?
Well, information has to do with the distinction between things.
So imagine two microstates.
They differ if only in the location of one particle.
They represent two possible ways the system could be at a given moment.
The laws of physics cause the microstates to change as the particles move and interact.
Each of the two microstates wanders around in phase space following those laws, but the
principle of microstate information tells us that they never wander to the same place.
They must always end up as two different microstates.
The distinction between them is preserved.
So suppose a system starts out in a macrostate, Delaware.
We do not know the microstate of the system.
It could be any point in Delaware.
For time, the microstates wander around according to the microscopic laws.
Let's look at the microstates later on.
The principle of microstate information tells us that they occupy a region exactly the same
size as the one they started in, a region containing the same number of microstates.
Imagine now that a system starts out in one macrostate and winds up in another.
For instance, we might start out with separate oxygen and nitrogen gases, but then we allow
them to become mixed.
In phase space, the system starts out in one region and then ends up in another.
Whatever happens, happens because of the laws governing the evolution of the microstates.
And so all of the different microstates in the initial region end up as different microstates
in the final region.
Therefore, the final macrostate, the region at the end of the process, must be at least
as big as the initial macrostate, and it might be bigger.
The points in Delaware could all end up in Texas, but Texas could not fit into Delaware.
So because of the principle of microstate information, the final macrostate can be
no smaller than the initial one.
And that means the entropy of the macrostate cannot decrease.
That's the second law.
This is an amazing connection between information and physics.
Because of the principle of microstate information, distinct microstates remain distinct as the
system evolves.
But we don't see the microstates, atoms and molecules are too small.
We only see the big picture, the macrostate.
Yet the principle of microstate information does have a consequence that we can see.
That consequence is that a bigger macrostate can never be squeezed into a smaller one.
Small entropy can never decrease.
If the principle of microstate information were not true, if different microstates could
evolve into the same microstate later on, then the second law would not be true either.
There would be no problem fitting all of Texas into Delaware, since many different points
in Texas could end up at the same point in Delaware.
Entropy could decrease.
Now what we have called the principle of microstate information goes by many names in
physics books.
It's called Liouville's theorem, or overall unitarity, or microscopic reversibility.
But it is really a law about information in the physical world.
It tells us that at the fundamental level, the universe does not discard information.
And the second law of thermodynamics, the law of increasing entropy, emerges from that
fact.
The first person to glimpse a connection between entropy and information was the brilliant
Scottish physicist James Clark Maxwell.
Maxwell was a 19th century contemporary of Clausius, Boltzmann and Gibbs.
Like them, he was a pioneer in the emerging science of thermodynamics.
He thought that the second law depended on the fact that we can only see the big picture.
We only have access to macroscopic information.
Equal atoms and molecules are very much smaller than we are.
We do not see them, and we cannot manipulate them one by one.
So Maxwell believed that the second law was really an expression of our own limitations.
And to illustrate this, he devised a famous thought experiment.
He imagined a tiny being, it came to be called Maxwell's demon.
The demon is so small that it can perceive and act upon individual molecules.
So let us imagine that we have a container of gas that is a mixture of oxygen and nitrogen.
There's a wall in the middle with a small opening, but the gases are mixed on both sides.
Now we station the demon at the opening and allow it to open or close a trapped door.
And the demon plays the following game.
If a nitrogen molecule approaches from one side, the demon lets it through.
If it approaches from the other side, the demon closes the door and it bounces back.
And it does the same for the oxygen molecules, except it lets them through the other direction.
After a long while, the demon's game will sort the molecules out, all the oxygen on
one side and all the nitrogen on the other.
The gas goes from mixed to separate.
The entropy decreases.
In other words, the action of the demon can contradict the second law.
The demon can violate the second law in other ways by playing the game differently.
If the demon lets every type of molecule through in one direction but not the other, it can
eventually cause the whole gas to occupy a smaller volume.
If the demon sorts the molecules by their speed so that the faster ones end up on one
side and the slower ones on the other, it can produce a temperature difference where
none existed before.
In any of these ways, the demon can engineer a decrease in the entropy of the gas.
In other words, Maxwell seems to say that if we could have access to the microscopic
information that we usually lack, then the second law is no longer a limit to us.
Any energy transformation would be possible.
We could make heat flow from cold to hot.
We could turn heat into work with 100% efficiency.
Was he right?
Is the second law really a law about us and our limitations, rather than nature?
For 100 years, one physicist after another tackled the puzzle of Maxwell's demon.
Most of them believed that Maxwell had left something out of his thought experiment, and
to be fair to Maxwell, his demon was an offhand illustration of a concept rather than a detailed
theoretical construct.
He did leave a lot of things out.
Perhaps if we really analyzed carefully how the demon might actually work, then we could
discover that it could not actually reduce entropy.
The second law would be valid after all.
Yet despite repeated analysis, Maxwell's demon proved difficult to exorcise in a definitive
way.
To solve the puzzle of Maxwell's demon, we must delve even more deeply into the link
between information and physics.
A real demon would have to be a physical system that interacted with its surroundings.
It would be a kind of robot or computer.
What does it mean for such a system to acquire, store, process and use information like the
demon does?
How does that affect the entropy of its environment as it operates?
Only by understanding these questions can we see the demon for what it really is, and
learn from it profound new lessons about entropy and the physics of information.
