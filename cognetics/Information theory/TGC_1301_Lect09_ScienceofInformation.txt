Decades ago, the Voyager 1 and 2 spacecraft flew past the outer planets of our solar
system sending back astonishing images and a vast trove of scientific data.
Now they continue to fly out into interstellar space.
They will never return.
Their instruments continue to make measurements of magnetic fields and the space environment.
The Voyagers have gone far.
Voyager 1 is now 20 billion kilometers away from Earth, over 12 billion miles.
Voyager 2 is almost as far.
Their radio transmitters are not immensely powerful, about as strong as a cell phone
tower on Earth.
A cell phone tower can only reach a few kilometers away, yet we continue to this day to receive
data from the Voyager spacecraft.
How that is possible, and how the same principles affect communications here on Earth, is one
of the most remarkable chapters in the science of information.
We must begin by recognizing that there are two kinds of information.
The kind we have mostly discussed so far is digital information, which is the information
in a finite number of discrete alternatives.
The 10 digits in our numbering system, the 26 letters of the alphabet, those express
digital information, and the fundamental unit of digital information is the bit, which has
just two possible alternatives, 0 and 1.
But a radio wave, like an electrical signal in a wire or a sound wave in the air, is analog
information.
So consider an electrical signal, a voltage.
At any moment that voltage can have any value, 1 volt, 2 volts, 3 volts, and also fractional
values like 1.4 volts and 2.718 volts.
In the same way the amplitude of the electromagnetic field in a radio wave, or the air pressure
in a sound wave, can take on a whole continuous range of values.
Actually we've seen this distinction before when we talked about computers.
Old fashioned analog computers like the differential analyzer worked with continuous variables
like time and velocity.
Computers built upon Boolean circuits on bits and logic gates process digital information.
The original broadcast media, ordinary radio and television, are analog information technologies.
In radio for instance, we encode the analog sound wave into the analog radio signal.
There are two familiar ways of doing this.
Amplitude modulation, AM, and frequency modulation, FM.
Each of them starts out with a carrier wave, which is a simple smooth radio signal with
only one frequency.
This signal is then modified, modulated, to add in the sound wave information.
In AM radio, the amplitude or strength of the carrier wave is varied to represent the
sound.
In a diagram, the waves get taller and shorter.
In FM, the carrier amplitude stays constant, but its frequency is varied.
The waves squeeze together or get farther apart in time.
Both techniques have their advantages.
AM is a bit simpler.
FM is more resistant to noise, that's because the external environment can readily cause
fluctuations in the strength of a received signal, but not its frequency.
Both AM and FM do have an effect on the carrier frequency.
The signal is no longer just one frequency.
To understand this, we need to recall Joseph Fourier, whom we mentioned in lecture 6.
Fourier showed that any signal can be regarded as a mixture of simple sine waves of different
frequencies, each with a different amplitude.
That's the Fourier transform of the signal.
The carrier wave is all one frequency, so its Fourier transform is just a sharp spike
at the center frequency.
Let's suppose that frequency is 1000 kHz.
Now let's use amplitude modulation to add a 1 kHz sound signal to the carrier.
The resulting wave picks up two new frequencies at 999 kHz and 1001 kHz, just 1 kHz below
and above the carrier.
Modulation spreads the frequency out on both sides of the carrier into regions of frequency
called side bands.
The whole signal now occupies a range of frequencies.
The size of that range is called the bandwidth W of the signal.
Our AM wave with a 1 kHz sound signal has a bandwidth of 2 kHz.
If we used FM to add the 1 kHz sound waves, there may be several additional frequencies
in the side bands at 999 kHz, 998, 997, and also 1001, 1002, 1003 and so on.
The ones farther out are much weaker, so we usually only worry about the first few.
With three important pairs of side band frequencies, the bandwidth W of our signal would be 6 kHz.
We can tune a radio receiver to pick up only a narrow range of frequencies, just one carrier
and its side bands.
So even if there are many radio stations, we can listen to just one at a time, provided
the carrier frequencies are far enough apart so that the side bands do not overlap.
Now we are all using the same radio spectrum, the same physical range of frequencies, and
every radio signal has a bandwidth, so it occupies a whole chunk of that real estate.
We have to share somehow.
That's why the government makes regulations assigning different frequency ranges to different
kinds of transmissions, or even to particular broadcast stations.
In the U.S., AM radio stations are assigned carrier frequencies between 540 and 1600 kHz.
The bandwidth of an AM station signal is about 8 kHz, so the station frequencies are set
at 10 kHz intervals, 540, 550, 560, and so on.
No two stations in the same area are assigned the same frequency.
FM radio stations are assigned higher carrier frequencies in a range near 100 MHz, and the
minimum interval between stations is 200 kHz.
Other countries have slightly different rules.
We can also separate signals in space.
If two radio stations are far apart and their power is not unlimited, then their transmissions
will not interfere with each other.
The fact that WTAM in Cleveland and KNZZ in Grand Junction both use 1100 kHz is no inconvenience
for listeners in either Northern Ohio or Western Colorado.
An even better example is the cellular telephone system.
For voice communication, my cell phone uses a band near 850 MHz.
But there are only a few dozen different cell phone frequencies in that band.
Luckily, I'm not sharing that frequency real estate with every other cell phone user in
the world, but only with those who are using the same local cell phone tower at the same
moment.
And indeed, the tower transmitters and receivers are somewhat directional, so that two users
on different sides of the same tower could use the same frequency.
The best example of separating signals in space has to be electrical signals in wires.
A single wire can carry several voltage signals separated by frequency, just like radio transmissions.
In addition, if the wires are properly shielded, the signals in one wire do not interfere with
signals in other wires.
And the same is true for light signals in different fiber optic cables.
This means that the practical problems of shared spectrum are much less acute for communication
over cables than for wireless communication like radio.
With money and time we can always add more cables if we need them.
But no amount of money or time will ever add more electromagnetic spectrum.
We can only learn to better use what we have.
The concept of bandwidth is important for electrical signals in wires, especially if
we are going to use the wire to send digital information.
So let me explain what I mean by thinking about a telegraph signal.
Just two voltages are used, on or off, which might be 1 volt and 0 volts.
If we plot a graph of the voltage over time, a message looks like this.
The signal is dot dash dash dot dash dot dot dot dot dot dot dot, which spells out the
word wave in Morse code.
A skilled human telegrapher can send a couple of letters per second, but a machine telegrapher
could send or receive much faster.
The same message could be sent in a shorter time, or several messages could share the
same line.
By the 1920s, most messages on long-distance telegraph lines were of this sort.
So is there any limit?
There is a limit.
That limit is set by the bandwidth limit of the transmission line.
So let's do an experiment.
Instead of just turning voltage on and off with a telegraph key, we will send a smooth
sine wave voltage at some frequency.
When the frequency is low, then the output signal is about the same as the input.
When the frequency is high, the voltage is oscillating extremely fast.
The electrical circuit simply does not have time to respond to the changes.
The amplitude of the output signal is very small.
Not all frequencies of signals are transmitted equally well.
There is a natural upper frequency limit determined by the physics of the circuit.
In other words, there is a natural bandwidth, W, for the transmission line.
We can think of the telegraph signal in the Fourier manner as a combination of frequencies.
But the frequencies higher than W will be lost in transmission.
What does the received signal look like?
It might look like this.
The sudden jumps in voltage have been replaced with smoother changes, and the voltage oscillates
a little around every jump.
But this is still perfectly understandable as a Morse signal.
Dot dash dash, dot dash, dot dot dot dash, dot, W-A-V-E.
But the more limited the bandwidth, the more the high frequency Fourier components are
quashed.
The voltage changes are even less sudden.
As the bandwidth is reduced, the distortion is worse and worse.
Eventually, the transmitted signal has no real resemblance to Morse code.
We cannot read the dots and dashes at all.
Now, we've been describing what happens if we send a Morse code signal through transmission
lines with a smaller and smaller bandwidth.
But the same thing would happen if the bandwidth were constant, but we tried to send the dots
and dashes faster and faster.
At some point, the voltage changes in the line would not be able to keep up.
The message would not get through.
So how fast is too fast?
The answer was discovered by two people, a Bell Labs research engineer named Harry Nyquist
and our old guide and companion Claude Shannon.
They asked precisely opposite questions, 20 years apart, and got the same answer.
In the 1920s, Nyquist posed the following problem.
To send a telegraph signal, we want to transmit a series of voltage values, the on-off values
of the dots and dashes.
So suppose the telegraph line has a limited bandwidth W. How many different independent
voltage values can we send along it per second?
In other words, how many different numbers per second can be represented by a signal
of bandwidth W?
Shannon posed exactly the reverse question.
How many numbers per second are needed to represent a signal of bandwidth W?
The answer is a beautiful one.
A signal with bandwidth W, measured in hertz, or waves per second, is exactly equivalent
to two times W, independent voltage values per second.
So imagine a transmission line with a very low bandwidth of 100 hertz.
Ways with a higher frequency than that do not get through, but lower frequencies are
fine.
Nyquist says a signal on that transmission line can describe up to 200 different voltage
values per second, and no more.
Shannon says that 200 voltage values per second can completely describe any signal
in the line.
That number of voltage values is a way of measuring the analog information in a voltage
signal.
If we have a signal that is T seconds long, the number of values is 2 times W times T.
The frequency 2W is called the Nyquist Frequency, or the Nyquist Rate, and the whole relationship
is called the Nyquist-Shannon Sampling Theorem.
That's why it won't work to send Morse code too quickly through the line.
If the dots and dashes come more quickly than the Nyquist Rate, the transmitted wave won't
contain enough different information.
The signal will be under-sampled.
Closing the circle in the sampling theorem, which Shannon did in 1949, is regarded as
one of his greatest achievements.
It is certainly an extraordinarily useful piece of mathematics.
In fact, we have already used it back in lecture 6.
There we discussed how musical sound is recorded on a CD.
Human hearing has a natural bandwidth limit.
We cannot hear sounds above around 20 kilohertz.
So we can capture all the sound we can hear if we sample the sound signal at twice that
rate.
That's why CDs use a sampling rate of 44.1 kilohertz.
If we measure analog information by counting numbers, the number of independent values
in a signal wave, then we have to face a paradox.
Suppose we have one number, the voltage value in a wire, or one electromagnetic field value
in a radio wave.
That's a continuous number.
It can have any value within some range.
Such a number has infinitely many decimal places.
It is in some sense equivalent to an infinite number of bits.
Does that mean that one number carries an infinite amount of information?
Could we in principle encode gigantic amounts of information, whole libraries of data, in
the amplitude of a single electrical pulse, or radio transmission, or sound?
Of course, we can't really do that.
As Claude Shannon showed at the very beginning of information theory, even a continuous analog
signal really only has a finite capacity for digital information.
It can only carry a finite number of bits.
The reason is noise.
Noise is inescapable.
It is always present, everywhere, though perhaps only at a very low level.
For instance, take a piece of wire.
The wire is not attached to anything, so you might expect that the voltage between the
two ends of the wire would be zero.
And you would be right, on average.
But at any given instant, the situation can be different.
Since the wire contains heat energy, the electrons are in ceaseless random motion.
Sometimes it happens that there are a few more electrons near one end than the other.
That produces a momentary, very small voltage.
If you make extremely precise measurements, you find the voltage in the wire is ceaselessly
fluctuating, positive and negative, in a random way.
This was noticed in 1926 by another Bell Labs engineer, Bert Johnson, and shortly afterward
Harry Nyquist deduced where the fluctuations came from.
The phenomenon is called Johnson noise, or Johnson Nyquist noise.
It affects every electrical circuit.
The only way to eliminate it would be to reduce the wire temperature to absolute zero, which
is impossible.
The same kind of thing happens with other kinds of signals.
Thermal motions of air molecules produce tiny random variations in air pressure, a faint
hiss that is part of every sound we hear.
Even in deep space, there is radio noise from distant galaxies and microwaves left over from
the early days of the universe itself.
The effect of noise is to make nearby signals impossible to tell apart.
Consider two electrical signals with voltages of just 1.000 volt and 1.001 volt, just one
millivolt apart.
These two signals represent different messages.
A is represented by 1.000 volts and B by 1.001 volt.
We can distinguish between A and B by measuring the signal very exactly.
But what if we add a little noise to the signal?
Suppose the voltage is randomly changed by as much as 5 millivolts up or down.
If we receive a signal, we measure its voltage and get something like 1.003 volts.
What was the intended message?
Was it A 1.000 volt with 3 millivolts of noise added, or B 1.001 volts with 2 millivolts
of noise added?
We cannot tell, no matter how precisely we measure the received voltage.
So 0 volts is at one point and positive and negative values of voltage are above and below
the zero.
An electrical signal is a point on that line, a particular value of the voltage.
Random noise effectively smears that point out into a taller splotch or smear, a whole
range of possible voltages that the signal could turn into.
Or we can imagine a whole bunch of signals, S1, S2 and so on, at different voltages.
Each signal spreads out into a vertical splotch due to the addition of noise.
If two noise splotches overlap each other very much, it is impossible to tell which
received signal is which.
In our picture, S2 and S3 have merged together in this way.
They are no longer distinguishable from each other.
The signals S4 and S5, on the other hand, are separated just far enough to tell apart.
So if we want to prevent errors, we need to use signals that are far enough apart so that
even with noise, their overlap is negligible.
If we want to send lots of different distinguishable signals, we will have to spread them over a
wide range of voltages, but that has a price.
High voltage signals cost energy.
To use bigger voltages, we will need more power for the signals.
And as a general fact, that power is proportional to the square of the voltage so it rises
pretty steeply.
A signal with 5 times the amplitude requires 25 times as much power.
So P stands for the average power of our voltage signal.
N stands for the power of the noise, a measure of the size of the noise splotch.
Their ratio, P divided by N, is the famous signal-to-noise ratio, sometimes abbreviated
SNR.
By making a simple assumption about the noise, the shape of the splotch, if you will, Shannon
was able to calculate the information capacity C of one noisy analog signal.
In fact, C equals one-half times the log two of one plus P over N.
That's the number of bits that the signal can convey.
As long as the noise isn't zero and we don't use an infinite amount of power, the capacity
is finite.
It depends on the signal-to-noise ratio.
Now, the SNR is often expressed in a logarithmic way using decibels.
A given number of decibels expresses a given ratio using base 10 logarithms.
0.0 decibels means a ratio of one.
What 10.0 decibels means a ratio of 10.
20 decibels means a ratio of 100.
Each time the ratio multiplies by a factor of 10, the number of decibels increases by
10.
You're probably familiar with decibels as a measure of sound.
The sound decibel level expresses the ratio of the loudness of a sound to that of the
quietest sound that a person can hear.
Decibels always express ratios.
So suppose your computer has a wireless internet connection.
If you have a very strong connection to the local access point, your signal-to-noise ratio
might be around 1,000 or 30 decibels.
Shannon's formula gives a capacity of around five bits per transmitted signal.
Sometimes however, your connection is not as good.
The signal is weaker, or the ambient radio noise level is worse.
Then your signal-to-noise ratio might only be a ratio of 30 or 15 decibels.
That allows only 2.5 bits per transmitted signal.
Wireless networks are designed to adapt when the signal-to-noise ratio is low.
The link still works, but it takes longer to send or receive a given amount of information.
We've now discussed two limitations on analog signals.
One is due to the bandwidth, the range of frequencies allowed in the signal.
The other is due to noise and the limited amount of power we can put into the signal.
We can now combine the results.
A signal wave with limited bandwidth W is equivalent to sending two W signal values per second.
For a given signal-to-noise ratio, each of those signal values can only carry a finite
number of bits.
From these two ingredients, we can find the maximum information rate in bits per second.
Since this is an information capacity, we will call it C, but we will write it in cursive
to remind us that this is measured in bits per second, an information rate, rather than
just bits.
C equals W times log 2 of 1 plus the signal-to-noise ratio.
This is called the Shannon-Hartley formula.
Shannon derived it based on some earlier work by Ralph Hartley, another of his predecessors
at Bell Labs, whom we met when we discussed entropy in lecture 3.
The Shannon-Hartley formula is the basic law that governs how much information can be sent
over a noisy transmission line or a radio link.
The formula is a little trickier than it appears.
For instance, at first glance it seems that we can always double the capacity C by doubling
the bandwidth W. However, when we open up our communication system to a wider range
of frequencies, we also open it up to more noise.
The noise power also depends on the bandwidth.
So let's work out a simple example.
We previously considered a transmission line with bandwidth W equal to 100 Hertz.
Let's suppose our SNR is 2.
Then the Shannon-Hartley formula tells us that the maximum information rate we can send
is 158 bits per second.
Now suppose we double the bandwidth to 200 Hertz.
This most likely doubles the power of the noise as well.
With the same signal power, we'll wind up with a signal-to-noise ratio of only 1 instead
of 2.
Then the information rate changes to 200 bits per second.
That's a little more, but not twice as much.
As we make our bandwidth wider and wider, we let more noise in, and that limits the information
capacity at a given power.
Even a bandwidth of 10,000 Hertz gives us less than 288 bits per second.
When the signal-to-noise ratio is 1, the maximum information rate equals the bandwidth.
In our example, 200 Hertz becomes 200 bits per second.
In fact, the word bandwidth is often used as a synonym for information rate.
I hesitate to call that a mistake.
It's standard terminology, but it can be confusing.
The Shannon-Hartley formula tells us that the information rate depends on both bandwidth
and signal-to-noise ratio.
Interestingly, we are using our analog signals to send digital data, and we do that increasingly.
This is the idea behind digital radio and satellite radio.
Recall that we have some extremely effective techniques for compressing digital audio data.
Perceptual coding, as in the MP3 code, can achieve excellent sound quality with only
a small fraction of the bits.
That lets digital radio technology make very efficient use of its frequency real estate.
In a digital radio system, several different streams of audio data are combined, compressed,
trapped in an error-correcting code like a Reed-Solomon code, and then sent over a fairly
high bandwidth high frequency radio signal.
That is broadcast either from a local transmitter or an orbiting satellite.
When you switch channels in a digital radio system, you may not be switching radio frequencies
at all.
Instead, you're just picking out a different program from the same combined data stream.
So let's go back to where we began.
The radio data link from the Voyager spacecraft 20 billion kilometers from Earth.
The Voyager radio transmitter power is about 20 watts.
The power that matters, of course, is the actual power that reaches the Earth.
If Voyager broadcasts those 20 watts equally in all directions, the radio signal on Earth
would be incredibly weak.
Each square meter would receive only 4 times 10 to the minus 27th power watts, far too small
to detect.
But Voyager does not send its radio signal in all directions.
The large parabolic dish antenna focuses the energy to a narrow beam less than one degree
wide.
This means that the spacecraft must keep this antenna aimed very precisely at Earth.
The narrow beam means that the radio power is greater in that direction.
How much greater?
That's called G, the gain factor for the antenna.
And for Voyager, that gain is around 65,000.
It is a high-gain antenna.
The received radio signal at Earth contains 2.6 times 10 to the minus 22nd watts in each
square meter, still quite small.
To gather as much radio power as possible, NASA uses the giant parabolic antennas of
the deep space network located in California, Spain and Australia.
Each one is 70 meters in diameter, with an area of almost 4,000 square meters.
The total power received from Voyager is about 10 to the minus 18th watts.
That's P, the signal power.
To have any hope of working, the system must somehow reduce the noise power in to extremely
low levels.
Now there are both internal and external sources of noise.
The internal noise is produced by the radio receiver itself.
A significant part of this is Johnson Nyquist noise in the electrical circuitry, which depends
on temperature.
So the receivers in the deep space network are cooled to within 20 degrees of absolute
zero.
What about external noise?
Some of that comes from terrestrial radio transmissions.
After all, someone else's signal is part of your noise.
However, Voyager transmits at X-band frequency 8.415 GHz, which is in a part of the radio
spectrum that the International Telecommunications Union has reserved for spacecraft communications.
The rest of the external noise comes from deep space, from sources within our own galaxy
and beyond.
Those cannot be reduced by regulations.
But they can be reduced by the receiving antenna.
The antennas of the deep space network are extremely high-gain antennas, so they only
accept radio energy from a narrow range of directions in the sky.
That reduces the received noise.
The gain factor of the receiving antenna, therefore, reduces the noise instead of increasing
the signal, but that still increases the signal-to-noise ratio.
For the big 70-meter antennas of the deep space network, the gain factor is more than
10 million.
Even so, the deep space network is just barely able to receive the Voyager data.
And as the two spacecraft get farther away, those radio signals diminish.
The signal-to-noise ratio goes down, which in turn reduces the possible information rate.
When the spacecraft visited Jupiter, less than a billion kilometers from Earth, data
was returned at 115,000 bits per second.
At Saturn, the rate was less than half that.
When Voyager 2 flew past Uranus and then Neptune, the rate was halved again.
Deep spacecraft to the far reaches of the solar system faces the same challenge.
The New Horizons probe, which flew by Pluto in 2015, transmits its data at only 1200 bits
per second.
Now Voyager 1 is more than five times farther away than Pluto.
Its data can be transmitted only at 160 bits per second, almost 800 times more slowly than
when it was by Jupiter.
Voyager is not designed to transmit at any slower bit rate.
Thus, at some point in the next decade, Voyager 1 will simply be too far away for us to hear
its messages.
Though, for a few years longer, radio telescopes will be able to find its carrier wave, shining
faintly in the radio sky like a star.
The deep space communication system that began with Voyager is an amazing engineering achievement.
It is testimony to our ability to shape information into forms that can be conveyed almost inconceivable
distances and understood amidst a cacophony of competing signals, natural and artificial.
The fastest earthbound computer networks communicate at just about a billion times faster than
Voyager, hundreds of gigabits per second.
And the total information traffic on the Internet is perhaps a thousand times greater
still.
Yet these extremes are governed by the same laws of bandwidth and signal-to-noise ratio
that are at work all around us in the communication systems we use every day.
