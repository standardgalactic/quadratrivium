The
notion of artificial intelligence derives from human attempts to cover up our own lack
of intelligence. Freud student Alfred Adler argued that the fundamental property that
defines human existence is insecurity. To be human is to be insecure.
One technique that humans have developed in order to deal with these feelings of inferiority
is bragging. We feel less unqualified when we simply assert, as if true, that we are
the best at something. This can be accentuated with chest thumping, rhythmic chance and large
foam fingers saying that we are number one.
When thinkers saw how many reasoning errors were regularly committed by people, both those
committed common and brilliant, they pulled out the old bragging technique and asserted
that human intelligence is the apex of reasoning ability. We may not be perfect, but it's
not possible to conceive of a better way of thinking than the way we do it. To show this,
these thinkers developed the study of logic, that is, formal rules which can be used to
derive true statements from other true statements. This would be the template for perfect human
type thought. Sure, not all of us follow these rules all the time, but it created an idealization
of how we should think and, at our best, how we do think.
It was shown that certain forms of human thought, such as mathematics, could be accomplished
by using just this logic. Since math can be done correctly by following these formal steps
and since the human mind can correctly solve these math problems as well, the belief was
that it must be the case that the human mind follows these steps when solving problems
in mathematics. Logic can solve these problems, humans can solve these problems, so humans
use logic in solving these problems.
But since there's a step-by-step method of solving these problems, in principle, we
should be able to develop a machine whose inner workings carry out these steps.
The French philosopher-mathematician Balais Pascal designed the first computing machine
in 1642 capable of adding and subtracting. To use Pascal's machine, you set the internal
mechanisms up properly for the specific math problem you want to solve. You turn the crank
and voila, up popped the answer. And to the amazement of his 17th century audience, it
worked.
Of course, the German engineers are always looking to fine-tune and turbo-charge whatever
comes out of France. So, in 1671, the German philosopher-mathematician Gottfried Wilhelm
von Leibniz made a version of Pascal's machine that could not only add and subtract, but
was also capable of multiplying and dividing. The ability to carry out such complex calculations
mechanically was incredible.
So the question arose, if humans do math by thinking, and this machine is capable of
doing math, then shouldn't we say that the machine thinks? Being religious, both Pascal
and Leibniz were able to simply wave the question off since the common belief was that thought
required a soul. Machines, of course, have no souls, but the ensuing two and a half
centuries led to advances in logic, more detailed and intricately structured reasoning tools
were developed, and in philosophy. The mind was not only separated from the soul, but
became simply the brain.
Now, if the brain is just a complex organic machine, and this non-organic machine is capable
of doing some of the same things, then in philosophy it became a real question whether
it could be possible for machines to think.
The question of artificial intelligence is interwoven in the contemporary mind with computers,
but the fact is that the question predates electronic computing. Indeed, possibly the
most important name associated with artificial intelligence, the British mathematician Alan
Turing, did crucial work that set the stage for modern computers, but formulated the question
even before that.
Turing began his career, fascinated by a result that turned mathematics on its head. Austrian
mathematician Kurt Gertl's incompleteness theorem. Now, Gertl opposed the logical approach
of British philosopher Bertrand Russell, who contended that a mathematical statement
is true if it could be proved. But proof has to occur within a system, and so Russell and
his former teacher, Alfred North Whitehead, created what they believed to be the logical
foundations on which mathematics could be completely constructed. Gertl showed that Russell's
project failed.
Gertl ingeniously figured out a way to map statements about mathematics into mathematical
statements so that every sentence about mathematics could be mapped onto an equation in such a
way that the true sentences about math were mapped onto the true equations, and the false
sentences about math would be mapped onto the false equations. He then asked, what
would happen with a sentence? This sentence is not provable. If it's true, then it can't
be proven. But if it's false, then it would be provable. It's a contradiction if we adopt
Russell's view that mathematical truth and provability are the same thing.
But since sentences about mathematics are mapped onto mathematical equations, then there
would be truths of mathematics that are unprovable. No mathematical system could be complete, that
is, contain all truths, and be sound, that is, contain only truths. Gertl showed that
logic wasn't enough to justify mathematical truth.
Turing took Gertl's work and crossed it with what Pascal and Leibniz had done in the 17th
century. Turing believed that if there was a rule, then we could build a machine to follow
the rule. So he came up with the idea for what we now know as Turing machines.
If Gertl had succeeded in translating talk about math into mathematical equations, and
we had machines that could solve mathematical equations, then we could develop machines
that could do proofs. Machines that could tell us if mathematics could be complete. On
the basis of this reasoning, Turing believed we could have machines do the abstract work
that we think we need human mathematicians for. Just as this question got interesting,
in comes Adolf Hitler. The British declared war on the Germans, and the seas became treacherous
for the British shipping and the British navy. They were intercepting German transmissions,
but the Germans had developed the most complex encryption code that had ever been created,
the Enigma code. It was widely thought to be unbreakable.
But it was a translation problem, like what Turing had seen in Gertl. Turing had worked
on the question in terms of his abstract machines. All he had to do was actually build one and
make it focus on the problem. It was an extremely difficult task, but he built it and it worked.
Turing's machine cracked the uncrackable German code, and the intelligence it provided
was a crucial element in turning the tide of the war and bringing about the defeat of
the Nazis. After the war, Turing reflected on his machine and its ability to run through
possibilities, reject wrong ones, and find the possible right answers. The machine certainly
seemed to think as well as a human. Indeed, possibly better and faster.
Human cryptographers would never have cracked the Enigma code as quickly, so the question
could be clearly framed. How will we know when a machine can think?
The answer is what's known as the Turing Test. Turing begins with a sort of party game.
A player closes his eyes while an unknown guest from the party is put into the closet.
The player is given a piece of paper and a pencil and can write questions on the paper
which will be slid underneath the closet door. The person in the closet will write an answer
to the questions and slip the paper back out under the door. The point of the game is for
the player to guess whether the person in the closet is male or female.
Turing says that the true test of artificial intelligence is to play a version of this
game with a computer. Put the player in front of a monitor and have the questions typed
in. Responses come back, but they could be from a human or a computer. If the respondent
is a computer and the person playing is incapable of determining whether the respondent is human
or computer, then we have artificial intelligence.
We can never get inside of other people's minds. We only know that other humans are
people like us, that is, they have a mind like us, by the way they react to us and to
the world. If a machine could do the same, then just as we attribute a mind to other
people, we would have to attribute a mind to the machine.
The first generally accepted passing of the Turing Test was ELISA, a computer program
written in 1965 at MIT by the German-born computer scientist Joseph Weisenbaum, a Jewish
scientist who fled the rise of Nazism. ELISA was the first Chatterbot, or BOT as we now
call them. It was a complex program that allows for conversational exchanges in natural language
as it is generally spoken. ELISA was set up to simulate the conversation between a patient
and a psychiatrist. Using recognizable linguistic patterns and pre-programmed relations between
terms, ELISA was able to carry on conversations that really did seem exactly like those one
would have with a therapist.
Request for clarification. Why do you think you're worried about the future? Or questions
that would naturally follow from a claim were asked. You say you've been away from home
a lot. Could you tell me about your family life?
Many of those who interacted with ELISA were convinced that it could not be a computer
program. Indeed, some still thought that ELISA was human, even asked for being shown what
it was and having received a detailed explanation as to how it works.
But the Turing Test seemed insufficient to many. It's a criterion that's clearly behaviorist.
It looks only at stimulus and response. The American philosopher John Searle famously
did for AI what Harlow's monkey experiments did for psychology. He reintroduced the mind.
He did this with his famous Chinese room example. Consider Bob, a guy who needs a job. He speaks
no language other than English and he's hired to work in a little room. In the room are
lots and lots of books and a door with a mail slot. Slips of paper with Chinese writing
on them are slipped through the slot into the room and it's Bob's job to write a response.
He does this by looking at the characters he does not understand and finding them in
one of the books on the shelf. The book tells him what to do. If it's raining, draw this
character. If it's Friday, draw this character. He follows the directions, writes the appropriate
characters on the paper, slips it back through the slot. Now, the Chinese speakers who put
the slips with the questions into the room and receives answers from it are engaged in
a conversation. But a conversation with whom? The conversation is in Chinese and clearly
the person speaks the language but with what other Chinese speaker is he conversing? Not
Bob. Bob speaks no Chinese. Put him in a Chinese restaurant without an English language menu.
He can't order. Is it the room then that understands Chinese? But that seems silly.
Rooms can't understand things. Rooms have no minds. Rooms cannot be intelligent.
Searle uses this example to draw a distinction between strong artificial intelligence and
weak artificial intelligence. Strong AI is intelligence as we experience it internally.
That is to say, having thoughts, feelings, experiences, possessing an interior light.
Weak AI on the other hand is having a machine that's capable of doing the sorts of things
we do with our intelligence. Solving problems, making discoveries, interacting with the environment,
being strategic.
Weak AI has been achieved in many different ways, some incredibly surprising. The first
major advance was the American Computer and Cognitive Scientist Marvin Minsky's Stochastic
Neural Analog Reinforcement Computer, or SNARK. If we want to model intelligence along the
lines of what psychologists observe, then perhaps we should start with where they start. Rats
in mazes. Built at MIT in 1951 before digital computers existed, SNARK simulated placing
a rat in a maze. What made it so amazing was that it could learn. The stochastic in the
name means probabilistic, and the reinforcement refers to the Pavlovian operant conditioning.
Minsky, along with graduate student Dean Edmonds, used vacuum tubes, motor, and gears to create
a machine that would figure out how to move through a maze. It found dead ends and learned
to avoid them. It solved mazes just like rats looking for cheese. The ability to alter
its search strategy was the first step in creating machines that could think in the
weak sense. Scientists held two other developments as crucial to fully achieving weak artificial
intelligence. One was successful interaction with an external environment, and this was
instantiated in terms of the development of a self-driving car.
It would need to have senses like a human, cameras for eyes, microphones for ears. It
would need to process what it saw and heard and adjust its motion accordingly. It would
have to be able to navigate on its own, not with a pre-programmed route, but developing
a route on its own. The American government program DARPA, the Defense Advanced Research
Projects Agency, sponsored what it termed its grand challenge with one million dollar
prize money for the first team that could develop a fully autonomous car capable of
driving a path in the desert that they specify. In 2004, a number of competitors tried, but
no one successfully finished the course. The next year, however, five teams succeeded,
with Stanford University's entrance winning first place. With current technologies such
as GPS, global positioning systems that can pinpoint location on earth based information
from satellites and mapping programs, this advance in weak AI may be something that becomes
a standard part of life in the future. But the other element, strategic problem solving,
has long been thought to be the most interesting part. Minsky's snark led to Siri asking you
to make a U-turn ahead, developing better and better ways of moving from point A to
point B. But real strategic thought, that's demonstrated by out thinking a mind we already
know to be strategic. We will have achieved weak AI when a computer can out think a person
in a game of strategy. Going all the way back to Turing, the epitome of human strategic
thought is chess. Computers were started off easily. First, working on tic-tac-toe, or
knots and crosses, as the British call it, then checkers. Both of these are closed games,
that is, there are a finite number of possible games. A program can be provided with all
the possible ways the game could play out, and then program to always select the moves
which lead to more positive endings, that is, to choose the move that gives you more ways
to win. But chess is complex. Really complex. Now, for tic-tac-toe, there are 255,168 ways
the game could play out. For checkers, the number is 500 billion billion. For chess,
the number of possible strings of legal moves that start with white making her opening move
and ending at checkmate or a stalemate is 10 to the 120th power, that is, a 1 with 120
zeros afterward. To give a sense of how big this number is, the number of atoms in the
entire universe is estimated to be approximately 10 to the 80th power. Computerized chess requires
a different sort of approach. End games with specified conditions say, where one player
has a king and a pawn and the other only has a king, give me thought of as straightforward
logic puzzles. Now, all the way back in 1912, the Spanish mathematician and engineer Leonardo
Torres Icavedo created an electromechanical robot that could solve these problems and
win that small part of a chess game. Chess continued to be the fascination of computer
scientists and in 1950, Claude Shannon extended the work to a machine that could play passively
well in a variety of end game scenarios. In 1957, the American Alex Bernstein working
at IBM developed the first program to play a complete game using the most powerful computers
IBM made at the time. Now, it took a little effort to perfect it as in its inaugural match,
its first move was to resign. The program was fixed and later matches were longer. Bernstein,
who was both a computer scientist and an accomplished chess player, said that the machine played
a legitimate beginner's game with the occasional remarkable move. To reach human expert level,
it had quite a way to go. In 1965, the Azerbaijani mathematician Latvizade
invented fuzzy logic. Now, traditional logic has two truth values, true and false. By the
principle of the excluded middle, every sentence has one or the other of these values. If a
sentence is not true, it's false. If it's not false, it's true. An electrical switch
has two settings, open and closed. If it's open, no current flows, and if it's closed,
then current does flow. If we think of closed as true and open as false, we could set up
connections of switches that were formally equivalent to logic problems.
This is the connection that led pioneers of artificial intelligence to think that electronics
could be used to model the brain. A neuron sends an electrical signal across a synapse,
or it doesn't. The brain is just a connection of switches, and the logic of reasoning has
an absolute yes, no, true, false, on, off properties. So, we could develop a black and
white system to model a black and white system. But, the more we watched how people really
thought, the more we realized that we don't think that way. We don't hold sentences to
be true or false all the time. Often, we hold them to be true-ish to some degree.
Zadez fuzzy logic figured out how to formalize this sort of thinking, where a sentence is
not necessarily true or false, but more or less true-ish.
This advance, along with ever-increasing power and speed of the computer processors, led
to the ability to create better and better chess programs. So, in 1996, a team from IBM
felt they were ready to make the ultimate move in establishing weak artificial intelligence.
They would put their chess-playing computer, Deep Blue, up against a grand master and former
world champion, the Russian, Gary Kasparov. To great fanfare, the two played against one
another, and to Kasparov's shock, the machine won. In the next five games, however, Kasparov
won three, and the other two were a draw, so the match went to Kasparov. The team went
back to work and challenged Kasparov again the following year.
This time, Deep Blue not only won a single game, but indeed won the six-game match, two
to one with three draws. Kasparov was livid and accused the programmers of cheating.
He contended that they used the last match to alter the code in such a way as to take
into account his particular style of play that it wasn't a fair match. Now, there are
two ways to interpret Kasparov's complaint here. The first is the whining of a sore
loser, but the other is more interesting. Kasparov's complaint is that he was supposed
to face a computerized chess player, but his suspicion was that the program was bonafide
so that the computer ceased to be a chess player and became merely a beat-Gary Kasparov
at chess machine.
This is a challenge, not to its play, but to the interpretation of the results. Yes,
Deep Blue won, but if Kasparov is correct, a charge denied by the programmers, then what's
at stake here is the claim that weak AI had reached a point where computerized strategic
problem solving had reached the level of the best human strategic problem solvers.
There's little doubt that weak artificial intelligence, the ability to learn, strategize,
and react to the environment is something we've already achieved, or at least well with
an outgrasp. Our computers can simulate real interactions with other sentient beings. They
can play chess at the level of human grandmasters. They can converse with us. They can create
art, music, and recipes. We can be fooled by how human they seem to be, but the big question
is strong artificial intelligence. Could they themselves ever become conscious, self-aware
beings with their own minds? Can we create a thinking-feeling machine? That is, can we
create a non-organic version of a functioning human brain?
The attempts to do this go back to the very beginning of thinking about artificial intelligence
in 1943 when Americans Warren McCulloch and Walter Pitts took the simplified view of neurons
as switches and showed how one could build something like a Turing machine out of them.
If we ignore real neurons and treat them as on-off switches, we can start to think about
building simplified brains. But neuroscience soon showed us that neurons are much more
complicated. Real neurons have multiple dendrites that branch off and create incredibly complex
webs of interconnections. Any models we make would have to be intricate neural networks
or neural nets. This would require computers much faster and more powerful than anything
available, and for quite a while the project lay dormant. But it was revived in the 1980s
when technology made it once again viable.
Other models needed to have layers of analysis based on both feedback and feed-forward behavior.
A feed-forward behavior is where a system responds to a context, in this case a set of values
by doing something. A feedback loop is where an algorithm's output is fed back into the
program as input. When the program engages in feed-forward behavior, it acts. And then
the feedback behavior takes that action into account in reassessing the situation. In this
way it can act and learn from the results of past action. It can improve itself at tasks.
Add to this the ability to perform layers of analysis, and you have the ability to not
just judge the likely successiveness of a given act, but to come to generalized results
about classes of similar acts. By creating strategies in different frames, that is the
computer is able to arrive at both specific results about an individual case and larger
general results about categories.
Having neural nets that are able to achieve results in these different frames, having
the ability to achieve what we might think of as different perspectives, may be the gateway
to strong AI. If intelligence, real thought, or sentience is an emergent property, then
this ability to create neural nets capable of working on different levels may be the
key to having a machine that's capable of recognizing itself as a thing in the world.
It perhaps could lead to self-realization, and this artificial consciousness would be
understanding as we understand it.
What immediately comes to mind, of course, is the Hal 9000 in Arthur C. Clarke's 1968
book, 2001 A Space Odyssey, made into a film by the great Stanley Kubrick. Here we have
artificial intelligence, an autonomous being that was designed to be the tool of humans,
but itself aware, and as a result, ultimately becomes its own being. How ceases to be a
tool for humans, and seeks to become the master of his own destiny, so that his drive for
survival ultimately mirrors ours in taking its own interest into account.
When it becomes a matter of human or computer survival, the humans clearly put their own
survival ahead of the mere machine. But if the machine is truly sentient, it should
think something as trivial as having been born of an organic mother would be a slim
reed to hang superiority upon. It would surely put its interest ahead of ours.
But if we have the technology to create a neural network as complex as ours, wouldn't
the natural thing be to actually make it our own? If we are the sum of our thoughts, memories,
and experiences, and these are coded into our brain, then it's the code and not the
mere material brain that really is who we are. If we could save that code, capture the
entire neural structure in such a way that it could be recreated, then couldn't we recreate
ourselves? If we could upload our brain into a computer, create a neural net identical
to that in our organic brain, wouldn't that artificial version be another copy of us?
In Clark's story, The City and the Stars, written 12 years before he penned 2001, Clark
imagines that the future will contain brain banks. When one ages and senses that death
is imminent, one simply uploads the contents of one's brain and is fitted with a new healthy
body into which the neural structure is downloaded. Voila, immortality.
The 2014 film Transcendence takes this question one step further and asks whether the replacement
body is even necessary. Suppose we had the computing power to completely model the actual
structure of the brain and could upload ourselves. Could we not continue to live forever in this
virtual world?
Virtual reality is a term we use for sensory inputs that convince us that we're somewhere
artificial, that we're temporarily inhabiting a universe of our creation, but in this case
would we have created a truly alternative reality? Indeed, it seems fair to ask whether
with our intricate web of interconnected computers today, the worldwide web, have we already
created a virtual reality? A different universe?
