Lecture 15, risk and risk aversion.
It's a fact, a fact that we ignore at our peril.
Risk and uncertainty are part of our lives.
Because we choose them, but sometimes they choose us.
In this lecture, I will return to the question of how economists think about risk and how
economists think about decision making in the face of uncertainty.
These issues are interesting and these issues are important.
They are the cornerstone of our understanding of equity markets, stock markets, and insurance
and derivative securities, all of which we will think about.
They help us to understand why we decide to insure ourselves against some hazards, but
not against other hazards.
And they help us to understand when bank regulations are likely to make banks safer
and when they may have unintended consequences and actually make banks riskier.
At the beginning of the lecture, we will return to our simple model of uncertainty.
Uncertainty generated by a simple random number generator, the physical properties of which
are well known.
And today we will use the simplest of the simple, the flip of a fair coin.
As we proceed, I will again extend the lecture to consider sources of risk in more realistic
settings.
It's useful to quickly review several ideas from the previous lecture.
Suppose a fair coin toss.
Recall that when describing a coin toss, fair means that the probability of a heads and
the probability of a tails are each 50 percent.
And that that outcome is in no way controlled by the person who flips the coin.
Let's consider a simple game.
If the outcome is heads, I pay you one dollar.
If the outcome is tails, you pay me one dollar.
That game is what we call a fair game.
Consider that when describing a game, fair means having an expected payout of zero.
This game is fair because the expected payout is zero.
To confirm that is simple.
The expected payout is simply minus one dollar multiplied by one half plus one dollar multiplied
by one half.
Of course that total is zero.
Suppose we modify the game slightly so that you receive one dollar and ten cents when
the outcome is heads and still pay one dollar when the outcome is tails.
Now according to our language, the game is better than fair.
And for a long time it was believed that any rational decision maker would always choose
to play a better than fair game.
It was also believed that faced with the choice of several games, a rational decision maker
would always choose the game with the highest expected value.
So traditional thinking would include the belief that a rational individual would always
be willing to play the coin toss game I just described.
Standard thinking changed when scholars confronted a famous puzzle called the St. Petersburg
Paradox.
Thinking was very important in the courts of Europe in the 17th and 18th centuries.
Some nobles studied the mathematics of gambling and they even hired experts to analyze games
of chance and gambling strategies.
From this connection came the St. Petersburg Paradox.
The resolution of the paradox gave rise to a way of thinking that is still widely in
use today.
The paradox begins with the St. Petersburg game.
Let me explain that game.
The game involves flipping a fair coin until the first tails appears.
If the first tails appears on the first toss, the game pays the player a prize of $1.
If the first tails appears on the second toss, the prize doubles to $2.
If the first tails appeared on the third toss, the prize would double again to $4 and so forth.
The prize continued to double.
What would you play?
What would you pay to play the St. Petersburg game?
Of course, everyone will pay at least $1 because $1 is the minimum payout for the game.
But how much more than $1 would you pay?
When faced with the opportunity to purchase a play of the St. Petersburg game, most experimental
subjects are willing to pay between $2 and $3 and no more.
But and this is stunning, the expected payout of the game is infinite as the following computation
quickly confirms.
Never expected payout is the probability weighted average of the payouts.
So with probability one-half, you get $1.
Add that to the fact that with probability one-fourth, you get $2.
Add that to the payout of $4 that you get with probability one-eighth and so forth and
so on.
The prize is doubling, but the probability of each prize is being multiplied each time
by half.
But when you do that math, you see that the expected payout, $0.50, the value associated
with tails on the first toss, $0.50 associated with the second toss, an entire stream of
$0.50 without limit.
Add those all together and you get an infinite number.
That's the paradox.
If rational decision-making means playing a game any time its expected outcome is greater
than zero, rational decision-makers should be willing to pay any amount to play the St.
Petersburg game.
Clearly, there appears to be a limit to what people are willing to pay to play the St.
Petersburg game.
I imagine yourself were thinking about what that limit was for you.
Now, are we to conclude that those folks, including me and probably you, are irrational?
Or should we use this evidence to rethink our theory of how people make decisions in
the face of uncertainty?
The resolution of the paradox is that most rational agents are also risk averse.
They dislike risk.
They will not play some better than fair games because those games are risky.
And they are willing to pay to have risk eliminated from their lives.
But to appreciate what this means, we first have to have a working definition of risk.
As an example, we will use three similar but different coin toss games.
In each game, there will be one and only one toss of the coin.
In each game, you will win if the coin shows heads and lose if the coin shows tails.
What makes the three games different is the size of the wager.
We'll call these games A, B, and C. In game A, if heads comes up, you receive $1.10.
If tails comes up, you lose $1.
That's an expected payout of that game of $0.05.
In game B, if heads comes up, you win $2.10.
If tails comes up, you lose $2.
Again an expected payout of $0.05.
And you've got the pattern now.
In game C, if heads comes up, you receive $3.10 and if tails comes up, you lose $3.00.
Once more, the expected payout is $0.05.
So let's be clear as to how to compare the games.
All three games have the same expected payout, $0.05, which means that all three games are
better than fair games.
But clearly, your intuition tells you that game C is riskier than game B, and game B
is riskier than game A. The standard measure of risk used by economists and financial analysts
is some measure of the dispersion of payouts.
To see what dispersion means, let's look at a graph of the probability-weighted payouts
of games A, B, and C. In the graph, we see that game A has its
two payouts depicted in yellow, minus $1 and $1.10.
They're both relatively close to the expected value of $0.05.
Game B has both of its payouts further away from the expected value.
Game B is pictured in orange, and minus $2 and $2.10 are further away from the expected
value.
And finally, in red, we see that the dispersion is greatest because game C's payouts are
furthest away from the expected value, minus $3 and $3.10.
The red lines are outside of the orange lines are outside of the yellow lines.
That is a pictorial representation of the fact that game C has a bigger dispersion than
game B, and game B's payouts has a bigger dispersion than game A's payouts.
Or put another way, game C is riskier than game B is riskier than game A.
Another way to understand why game C is the riskiest is to note that the probability-weighted
loss is greater for game C. That's one-half times $3 or one-and-a-half.
It's greater than the probability-weighted loss for game B, which is one-half times
$2 or $1.
And in turn, that is greater than the probability-weighted loss for game A, 0.5 times $1 or $0.50.
Some economists prefer to think of risk that way.
By that measure also, C is riskier than B is riskier than A.
For those who are familiar with statistics, a standard measure of dispersion is the standard
deviation of payouts.
But you know, don't worry about that.
The picture captures the idea that a game or any endeavor involving random payouts or
random events is riskier if the dispersion of the payouts is greater.
We're now prepared to understand the resolution of the St. Petersburg Paradox.
The paradox's resolution is generally attributed to Daniel Bernoulli, a Dutch-Swiss mathematician
who was for a time a professor of mathematics at the university in St. Petersburg.
And there are two essential components to the resolution.
The first component is the idea that the value to an individual of additional wealth falls
as wealth increases.
It follows that the satisfaction derived from an individual from gaining an additional
dollar is smaller than the satisfaction lost from losing a dollar.
Let's state that a slightly different way.
We are happier when we win a dollar than when we don't.
But the happiness we gain, according to Bernoulli, is a smaller increment than the happiness
we lose when we lose a dollar.
Bernoulli's first idea is embodied in the hypothesis that the utility that an individual
derives from his or her wealth is a concave function of wealth.
You can always recognize a concave function because it makes a little cave.
There's a mind-checked for you.
And an example of a concave function is given in the graph that you're looking at now.
The graph is shown for an individual who would start with a hundred dollars of wealth, arbitrarily,
and it's drawn so that the utility of $100 is zero, just to normalize it.
The concavity of the function implies that the utility gain from the additional gain
of a dollar is smaller than the utility lost from the loss of a dollar.
As I've already said, put another way, the marginal value of wealth falls to the person
whose utility is pictured.
And Bernoulli thought our utility was a lot like that pictured in the diagram.
Now, Bernoulli's utility of wealth function has a solid intuition behind it.
When an individual has little wealth, she uses it to acquire the things that she values most.
As her wealth increases, she can acquire new things that add to her utility, but it's likely
that they add less to her utility than the really important things that she's already
purchased.
The second component of the resolution of the paradox is a hypothesis that a rational decision-maker
maximizes expected utility rather than expected payout.
Expected utility is the probability weighted average of the utility of the wealth that
would result from each possible random outcome.
By way of example, let's ask whether a person who maximizes expected utility and whose preferences
are described by the pictured concave utility function would play a fair coin toss game
for $10.
The answer is no.
By not playing the game, the risk averse individual can keep his or her utility at zero, the normalized
starting point.
By playing the game, the individual would have a 0.5 chance, a 1.5 chance of winning $10,
and a 1.5 chance of losing $10, but the gain in utility associated with winning $10 is
smaller in size than the loss in utility associated with losing $10.
One can read the utility from the graph, and I have done so in a numerical example because
this idea is so important.
So let's go ahead and compute expected utility.
The decision to not play leaves us exactly with our $100 probability 1.
The decision to play leaves us with negative expected utility.
I've already explained why, let me explain why a second time.
Expected utility is 1.5 times minus 0.105 plus 0.5 times a smaller number, 0.095.
Where did I get this minus 0.105 and plus 0.095?
From the utility function, minus 1.05 is your utility if you lose that $10.
You started at zero, you would fall to minus 0.105.
Where did I get the 0.095?
The utility you would get if you won the $10, and your wealth became not $100, but $110.
So what happens is, as I have explained in words, that the utility of your gains is smaller
in size than the utility of your loss, therefore your expected utility from that fair game,
a 50-50 coin toss involving a prize of $10, is actually negative.
You will not play.
Let's be clear.
Why does the expected utility hypothesis resolve the St. Petersburg paradox?
A person with little wealth places a lot of utility weight on each dollar they must spend
for a ticket to play the St. Petersburg game.
The person places less utility weight on each dollar they would win if they were very lucky
and it took many coin tosses for the first heads to appear.
As a footnote, it's interesting that behavioral economists have a slightly different take
on the resolution of the St. Petersburg paradox.
They believe that most individuals act as if they will never experience a very, very,
very rare event.
In the context of the St. Petersburg game, that means that most individuals think that
the probability that it will take more than, say, nine tosses for a heads to show up is
zero.
In fact, it's a very small number, but they round it down to zero.
They don't think they'll ever see that in their lifetime.
As a result, experiential economists tend to resolve the paradox by saying that individuals
do not use actual probabilities.
They instead use a perceived probability and they underweight very, very rare events, believing
that they will never see them.
What do we learn from our friend Daniel Bernoulli?
Many, many decades later, we learn several very important things.
Most individuals do not make decisions in the face of uncertainty by considering only
the expected value associated with random events.
Bernoulli's resolution of the St. Petersburg paradox suggests that individuals are risk-averse
because they value the dollars they might lose more highly at the margin than they value
the dollars they might win.
In a later lecture, we will see that expected utility maximizers with concave utility functions
will willingly buy insurance to avoid risk.
They want to stay at a utility of zero and will pay money to get to stay there.
If they own a flip of the coin, if they've been endowed with a flip of the coin, they
will pay money to get someone to take the outcome away from them.
They'll happily pay for that service.
The expected utility hypothesis is not the last word on decision-making in the face of
uncertainty.
It is a puzzle for researchers to explain why individuals often both buy property insurance
and buy lottery tickets.
Think about that for a minute.
Buying property insurance only makes sense if the agent is risk-averse with the concave
utility function.
Buying lottery tickets only makes sense if the decision-maker is not risk-averse since
lottery tickets are a less-than-fair game.
We know they're profit-making schemes, so how do we square those two possibilities?
Well in a very, very famous paper, Milton Friedman and Leonard Savage hypothesized that
the typical household has a utility of wealth function that is concave in the vicinity of,
call it their normal wealth, but it would turn convex at much higher levels of wealth.
What's that mean?
Well, the idea is that people would pay something for the chance to win a prize so large that
it would substantially change the way that they could live.
It would take them to a whole new place on their utility of wealth function.
Another challenge and another puzzle involves offering an individual a choice between two
lotteries, each of which is equivalent to a coin toss game.
In lottery one, the individual will receive $1,000 a year for every year of their life
if heads comes up and nothing if tails comes up.
Let's be clear, one coin toss, if you win $1,000 a year for every year until you die.
Lottery two is very different.
In lottery two, the individual will repeat the coin toss every year.
In any year, if heads comes up, the individual will win $1,000 for that year and if tails
come up, they receive nothing that year, but the game goes on.
Future coin tosses are still available to them.
It's not hard to verify that the expected payout of both lotteries is the same.
It's not hard to verify that the expected utility of both lotteries is the same.
But ask yourself, which game would you prefer to play?
If you had a choice of buying either game at the same price, which game would you pay
more for?
If you are like most experimental subjects and if you were like me, you would very much
prefer lottery two and would pay more in order to get it.
At the end of the day, what do we learn from the St. Petersburg Paradox and from Daniel
Bernoulli?
We learn that most decision makers are risk averse.
They dislike dispersion of payouts.
They like situations where the payouts are close to the mean better than situations when
the payouts are further away.
They dislike, in other words, the prospect of losing.
Daniel Bernoulli changed thinking about decision making in risky situations forever when he
put forward the expected utility hypothesis.
It is such a simple but such a powerful idea.
We place greater value on a dollar we lose than on a dollar we gain.
That research into human behavior goes on precisely because newer paradoxes have replaced
older ones.
This research is of great value to us.
It helps us to understand, not casino gambling.
That's only interesting if it leads to greater insights.
It is important because it helps us understand the nature and value of insurance.
It helps us predict what individuals will pay in order to buy insurance and have someone
take the risk away from them.
Thank you.
