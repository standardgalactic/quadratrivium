Lecture 22 Perception of Language
Hi, welcome to Lecture 22.
In Lecture 21 we talked about the structure and function of the brain with special attention
being paid to the cerebral cortex.
In this lecture, Lecture 22, we are now going to take some of what we learned about the
organization of the brain and apply it to a specific perceptual ability, language perception.
First a little history.
Back in the 1950s, physiological psychologists had the belief that the brain was divided
up into specific little centers.
This belief came about because when they electrically stimulated a region of the brain
in the forebrain called the hypothalamus, they could make animals begin eating even
if they weren't hungry.
They got what they called stimulation bound eating.
This finding was so important and interesting that it kind of drove research in laboratories
around the country.
Everybody started sticking electrodes in the hypothalamus and seeing what behaviors they
could elicit.
Researchers found what they thought was the brain center for eating and the brain center
for drinking and the brain center for sex and the brain center for aggression and the
brain center for pup retrieval and female rats.
The belief was that the brain was divided up into specific little centers.
It turns out that this was due to a methodological error and a naive understanding of how the
brain worked.
You see, psychologists, when they design an experiment, like to have everything under
their control.
So if you want to look for this, the eating center, you put an animal in a cage with nothing
but a bowl of food and you simulate its brain and if it runs over and eats, you infer that
you found the eating center.
Well, it turns out psychologists all over the country had discovered the same center
in the brain, even the ones that were studying drinking or sex or aggression or pup retrieval
or what have you.
How could the same brain center control all of these motivated behaviors?
The answer was that what they were doing was short circuiting a general motivational system.
You see, the brain is not divided into specific little centers, but it is divided into systems
including a motivational system, which they were short circuiting by sticking a wire
in it and passing a current through it.
In order for the rat to know whether it was hungry or thirsty or sexy or feeling maternal,
would need information from very many other parts of the brain.
The way that the psychologists discovered this was after some of them had compared notes
and found they'd all found the same part of the brain except having different functions,
they put an animal in a cage with food and water and a piece of wood and a member of
the same sex and a member of the opposite sex and some baby rats and they stimulated
this particular part of the brain and they couldn't predict what the animal was going
to do.
Sometimes the animal would eat, sometimes it would drink, sometimes it would gnaw the block
of wood, sometimes it would shred the paper, if it was a female rat, sometimes she would
take care of the baby rats, if it was a male rat either he'd go fight with another male
rat or he'd run out and buy flowers for the female rat.
They couldn't tell what they were going to get, they got an explosion of motives.
That's when they began to realize the brain isn't divided into specific little centers,
there is a general motivational system that you're short circuiting when you just pass
a current through it and that other parts of the brain are necessarily involved in deciding
exactly what the motive is and how to satisfy it.
Well it's the same in humans.
Simply speaking the brain is not divided into specific centers.
Now that statement holds true 100% for rats and lower animals, it turns out that there
are a couple of exceptions in the human brain.
It turns out that there are two perceptual abilities that humans have that lower animals
don't have that are so important to humans that they do have dedicated regions of the
cortex devoted to them.
In other words there are two centers in the human brain which other animals do not have.
Lower animals do not have.
Well everything below us is a lower animal that humans have and no other animal has.
These two perceptual abilities that have dedicated centers in the human brain are one language
perception and two human face perception.
We are the only creatures that are skilled in these two perceptual abilities, the perception
and understanding of language and the ability to identify human faces.
We're the only animal that can do that.
Even animals with 20 times the visual acuity of people can't identify human faces the way
we can.
Okay, to be more accurate there are actually two speech areas in the human brain, two speech
centers if you will, one for the production of speech which is a motor function and one
for the reception or understanding of speech which is more of a perceptual function.
Now based on what you know from the previous lecture you can probably guess which cerebral
hemisphere has these two speech centers and if you guess the left hemisphere you're absolutely
correct.
You may also be able to guess in what part of the brain the speech center for the production
of speech is found and in what part of the brain the center for the understanding of
or reception of speech is to be found.
If you were to guess that the center in the left hemisphere for the production of speech
is found in the frontal lobe you would be absolutely correct because speech production
of course is a motor function and the frontal lobe is the motor center in the brain.
The production of sound depends upon the integrity of a region in the left hemisphere for the
vast majority of us in the frontal lobe known as Broca's area after the physician Paul Broca
who discovered this connection in 1861.
Broca did an autopsy on a deceased individual that he'd been treating for gangrene or something
and the other interesting characteristic about this patient is he hadn't uttered a word in
his entire adult life and in the course of examining the brain Broca found that this
guy had a big tumor at the base of the third frontal convolution in the left cerebral hemisphere
which has now come to be called Broca's area.
The speech center for understanding speech comprehension of speech or reception of speech
those terms are pretty much interchangeable is found along the left sylvian fissure which
was first pointed out by Wada as being longer in most left hemispheres in the left hemisphere
of most brains.
So the receptive speech area is found on the superior temporal surface of the temporal
lobe the superior temporal lobe in the region that has come to be called Vernike's area
for the same reason Broca's area is called Broca's area.
This is the gentleman who discovered it and identified its function.
Now damage to either Broca's area or Vernike's area results in a condition called aphasia.
Aphasia is the term used to apply to some disorder of the associative aspects of language.
Now there are two categories of aphasia depending upon whether you have damage in Broca's area
or Vernike's area.
Damage to Broca's area produces what we call expressive aphasia.
If there is total destruction of that center you're mute you can't talk at all.
With partial destruction of Broca's area you may be able to utter some sounds that in some
way shape or form resemble language but you're greatly impaired in language ability if you
have damage to Broca's area.
If the damage occurs in Vernike's area along the superior surface of the temporal lobe
in the area of, well, the Sylvian Fisher you have problems understanding speech.
The fact of the matter is people with damage in Vernike's area can still utter words but
the words are what has been referred to as words solid.
They don't express anything, they're just essentially sounds.
So we have these two categories of aphasia, Broca's area which is also called expressive
aphasia and Vernike's area which is called receptive aphasia.
There is another category of aphasia that is diagnosed when the brain damage is extensive
enough that it includes both Broca's area and Vernike's area.
In this case the term that is used is global aphasia.
Now, study of global aphasia patients has suggested that the conceptual system in the
brain is separate from the language system.
In other words, you may have aphasia but that doesn't mean you can't think or express
yourself symbolically, the thing is you just can't do it in language.
You might be able to do it in writing or you might be able to do it as was demonstrated
in 1975 in a study involving seven global aphasia patients, one of whom was 84 years old.
Researchers taught these folks sign language and they were able to communicate.
They could understand signing and they could in turn communicate by signing.
So this is why we say that the conceptual system is different from the language system
in the brain.
One more point before we stop talking about aphasia.
I want to clarify for you that for instance if a boxer gets punched in the throat and
has swollen vocal cords and can't talk, that's not expressive aphasia.
Aphasia refers to an inability to talk due to central factors in the brain.
Similarly if someone is deaf and can't understand language, that is not aphasia.
Aphasia is not deafness.
You can still hear when you have aphasia, it's just that words no longer have symbolic
value, they're just noise.
So these are the two categories of aphasia, Broca's and Vernike's.
When sounds acquire symbolic meaning, we call them words.
Damage through Vernike's area takes away that symbolic meaning and they simply become
sounds again.
Okay, now let's talk about speech and speech perception.
Since the focus of our 24 lectures together is more on perception rather than motor functions,
we will have no more to say about Broca's area or expressive speech.
Let's talk about the reception of speech.
The basic unit of speech is not the word but the phoneme.
We're going to do a little vocabulary here.
A phoneme is the smallest utterance that is distinguishable from other utterances.
It's not the same as a letter.
Phonemes are not the same as letters.
Let me give you an example.
The p sound in pin is a phoneme.
The p sound in thin is a phoneme.
The b sound in ban is a phoneme and the m sound in man is a phoneme.
So they're not the same as letters at all.
And different languages possess different numbers of phonemes, the basic distinguishable utterances
of a language, phonemes.
Now the number of phonemes in English has changed over the past few decades.
As new words come into the language and into usage in the language, sometimes the number
of phonemes has to be increased.
At the present time, the number of phonemes in English is 43.
As I said in earlier, the last time I paid attention it was 38, now it's 43.
The number of phonemes in other languages differs significantly.
For example, in the Hawaiian language, the number of phonemes is 15.
Now you can see that the number of phonemes in a language can significantly influence
the character of that language.
Let me give you some examples from Hawaiian with 15 phonemes as opposed to 43.
Kau kau means food in Hawaiian.
Mumu is a long shapeless dress.
Nene is the Hawaiian state bird, sometimes called the Hawaiian goose, nene.
The most famous of the Hawaiian kings is King Kamehameha.
The most famous of the Hawaiian queens is Queen Liliokalani.
The word for chief is alii.
One of Kamehameha's sons, who was himself king for a little while, was liho liho.
So what you see in a language with very few phonemes is that the phonemes are used far
more frequently than in a language with many phonemes.
So you hear that repetition, nene, mumu, kau kau in Hawaiian.
That sounds foreign to the English ear where we have so many phonemes that we don't need
to repeat them many times in the same word.
Neonatal infants, young babies, not a week old or two weeks or less than a month old,
in the course of their random babbling, neonatal infants are capable of uttering any phoneme
that has ever been a part of any language on the surface of the planet, new languages
or old languages.
Babies can replicate the phonemes for that language in their random babbling.
Now, by the first month of life, two processes are going on that constrain the range of phonemes
that an infant is capable of using.
The first constraint comes about from the phonemes that are spoken by the baby's caregivers.
What the baby hears is nothing but the phonemes and the language of the caregivers.
And the other constraint is that when a baby in the course of its random babbling makes
a sound that sounds like speech to its parents or primary caregivers, they make a fuss over
the baby.
And the baby is rewarded by this, and even though the baby doesn't know what he or she
may have said or almost said, that sound increases in probability of occurrence.
So for instance, here is a baby making random baby sounds, gi-gi-goo-goo, ga-ga-mama, and
the mother runs over, oh, you said mama, that's wonderful.
Or in the case of the father or the baby, does the random babbling and maybe says da-da,
again, a big fuss is made over the baby.
So by the end of the first month, what's happening through the baby hearing phonemes from the
primary caregivers and repeating some of these phonemes, the ones that have been rewarded,
is dedicated connections, dedicated neural connections begin to be established between
the hair cells in the baby's cochlea and neurons in Bernice's area.
Dedicated connections begin to be set up.
What is happening is that the beginning of a process called the formation of a phonemic
perceptual map is beginning to be established.
A phonemic map, a perceptual map in the baby's brain, so that on future occasions when the
same phoneme is heard by the baby, the same auditory hair cell receptors are going to
send information to the same cluster of neurons in Bernice's area.
Eventually, each phoneme in the baby's native language is going to be represented at a different
spatial location in this perceptual phonemic map.
Now this map is well on its way toward being established by the time the baby is six months
old.
Now the baby doesn't know words and the baby doesn't, this is a passive process that takes
place through what the baby is hearing.
But how do we know this?
How do we know that the phonemic perceptual map is discriminably different in children
hearing different native languages by six months of age from EEG studies, from electroencephalographic
studies that have been done, for example, on the brains of children hearing English from
their primary caregivers as opposed to children hearing Swedish from their primary caregivers?
At six months of age, what the baby is hearing and where it goes in the baby's Bernice's
area is different depending on if it's hearing Swedish or English.
So the primary, the perceptual map, phonemic perceptual map has been significantly established
by six months.
Evidence suggests that it is completed by the end of the first year of life, that all
of the relevant phonemes are now spread out on this phonemic perceptual map in the baby's
brain by the time it's one year old.
The fact of the matter is though that the brain of a human infant possesses something
that we've referred to as plasticity.
Remember we said if you were destined to be left hemisphere dominant but you had brain
damage from birth trauma, you might switch over and be right hemisphere dominant, that's
an example of plasticity.
Well that plasticity is also operative in the brain of a newborn baby forming its perceptual
map, phonemic perceptual map.
And so the plasticity remains for the first year of life, meaning that you can learn new
phonemes, it's just going to be a little harder.
It turns out that by the end of the first year of life the character of an infant's
random babbling is quite different, in fact it's no longer random anymore.
A child's babbling at one year of life has now acquired the sounds of its native language.
Babies still don't know words and they still don't know that they can communicate using
words, but they know that when they say things like mama, da da, and bye bye, somebody makes
a fuss over them.
Whereas if they say phoneme sounds that have no relationship to the primary native language
of the caregivers, nobody makes as big a fuss.
Babies are capable of learning and establishing phonemic perceptual maps for two or even three
languages simultaneously.
Because infants, they can form three, usually it's two, but they can form three perceptual
maps and they don't interfere with each other.
It turns out that there are enough neurons in Bernanke's area that these perceptual maps
do not intrude and interfere with each other.
The implication of this is that it is far easier for a child to learn two languages simultaneously
than it is to learn one and then later learn the other, because when you've established
a phonemic perceptual map for one language, for the phonemes in one language, this constrains
somewhat the formation of a second perceptual map for another language.
It's still doable, it's just more difficult.
It's easier to learn them for an infant, for a child, simultaneously rather than successively.
Acquiring a new language after age 12, and you may remember we said that this plasticity
in the brain has dwindled significantly by the time the myelin sheath is in place, the
insulation of the neurons from each other, around age 12, 12-ish.
After age 12, a child is unlikely, it's not impossible, but it's unlikely to ever acquire
a second language and speak it like a native.
You can speak it very well and you can speak it so that non-natives will think you're doing
it perfectly, but it's very difficult for somebody to acquire a language after age 12
and fool a native speaker that you are, in fact, a native speaker of that language.
Now, of course, there are many factors that contribute to this ability.
If the phonemic map of the first language, if the phonemes in that phonemic map overlap
with the phonemes in the second language, it's going to be easier.
For example, if you are a Spanish speaker and then later in life you choose to learn
Italian, that's going to be a relatively easy crossover because many of the phonemes are
the same in Spanish and Italian.
However, if you're a Spanish speaker and later in life you choose to learn Russian, that's
going to be a bit more of a challenge because of the difference in the phonemes between
those two languages, between Spanish and Russian.
The fact of the matter is, if you do not hear a phoneme from another language as a child,
you will be functionally deaf to that phoneme later in life.
There are examples of this, let me just share a couple with you.
In the English language, for example, the R sound in rip is sharp and clear and distinct
from the L sound in lip.
So in English, the R sound and the L sound are going to have spatially separate locations
on our phonemic perceptual map.
In Japan, however, the R sound and the L sound are merged.
So it's like R, instead of R, or L, R. As a consequence, the location of the R sound
and the L sound in the phonemic perceptual map of a Japanese child, there's going to
be a great deal of overlap and the neurons are going to be all tangled up and the Japanese
child, or adult and adult, they will have difficulty actually hearing the distinction
between the R and the L sound.
They will be functionally deaf to that phonemic difference.
The two phonemes are far apart in the American perceptual map or the English perceptual map
overlapping in the Japanese perceptual map.
Let me tell you what some of the practical consequences of this are, in fact, from personal
experience.
My wife and I were in Kyoto, Kyoto, Japan, and we were so proud of ourselves that we
had learned to ride the subway, so we were feeling more like trekkers rather than tourists.
And my wife wanted to go to a large Japanese department store and she went to the cosmetics
section to see what they had that she needed or that weren't available where she was.
And she needed some blush.
So she said to the young woman working behind the counter, I'd like to see some blush, and
the young woman said, brush, and my wife said, no, blush, and the young girl said, brush,
and my wife said, blush, and she said, finally my wife pointed, blush, and the girl goes
and gets it.
I don't remember if my wife purchased it or not, but I remembered the exchange.
It was clear that this young woman who was so anxious to be helpful was functionally
deaf to the distinction between the rough sound, the brush, and blush.
Now as we were walking away, I still heard the sales clerk behind the counter working
on it herself, brush, brush, and she never did get it.
Now I happened to encounter a business professor that I knew walking around Kyoto and he told
me about a trip he took to a Japanese industry with 30 business students because in many
areas of business we can now learn from the Japanese.
So there was this industrial site and they had a host and the host said, first we will
hear an introductory lecture from Professor Tanaka and he said, and in the afternoon we
will take you to see the rubber trees and my friend is thinking, why do we want to go
see rubber trees?
Well, of course, what the guy said was in the afternoon we will go see the laboratories,
but it sounded like rubber trees because of that merging of the r and the l sound.
Okay, now, I'm not picking on or teasing or making fun of the Japanese, we are functionally
deaf to their phonemes too.
And I discovered this also in the same trip to Kyoto because there was one temple, Kyoto
by the way is the city of temples, I highly recommend you visit the city of temples and
see all these beautiful temples.
There was one temple that was not near a subway station so we took the subway as far as we
could and then we went and we got hailed a taxi and I said to the driver, Rio Anji Temple
please and he said, what?
And I said, Rio Anji Temple and he leans again and he says, where do you want to go?
I said, Rio Anji Temple and I realized I wasn't making myself understood so I leaned forward
and I showed him the tourist map that I had and it said Rio Anji Temple and he said, oh
Rio Anji Temple and I thought, I thought that's what I said but then I realized it couldn't
have been what I said and the phonemes that this gentleman used to say that name, I was
functionally deaf to them.
So it works two ways my friends, they are having problems, they are functionally deaf
to some of our phonemes and we are functionally deaf to some of their phonemes.
This has also been demonstrated in the laboratory.
Again the Japanese speakers and English speakers were wired up with EEG electrodes to measure
the electrical potentials in different parts of their brains.
It turns out that the EEG record is indistinguishable from a Japanese speaker and an American English
speaker if we say pan and ban because those two phonemes exist in both languages and they
are at different sites on the phonemic perceptual map but if you say rip and lip the EEG records
are quite different from the English speaker to the Japanese speaker.
After we have created our phonemic perceptual maps and we have our phonemes now it's time
for the infant to get to work and start forming words from phonemes.
In this regard some very interesting research by a psychiatrist from the University of Chicago
named Jan Ellen Huttenlocker suggests that mothers who talk more to their children actually
facilitate those children understanding more words at an earlier age.
She compared two groups of mothers who were loquacious and talked a lot and mothers who
were more reserved and didn't talk a lot and the babies of the more talkative mothers
by age 20 months knew 131 more words than the babies of the mothers who didn't talk
to them too much.
Now this is not a sexist finding it could have worked out just as well I assume with
fathers doing the talking except she had a group of people for whom the mother was the
primary caregiver.
Now it also turns out that it doesn't matter whether these words are monosyllabic words
or polysyllabic words it's the important thing is that they hear a lot of words so
you can either read to your child from a Dr. Seuss book or for a Tom Clancy book it doesn't
appear to matter as long as they hear words how long the words are the child doesn't understand
these words but what's happening is the neural circuitry is being laid down for the absorption
and the storage of more words and it turns out that children are like a sponge they learn
words at an incredible rate well in four months the children whose mothers talk to them a
lot went from 131 more words than children who weren't talked to too much to 295 words
so they're just sucking up these words.
I have to share a personal experience as a proud grandfather some years ago I went to
visit my daughter and my youngest granddaughter was in the kitchen and they had two cats Suki
and Amy and Suki was the bigger older cat and Amy was the littler cat and the granddaughter
was watching the cats and Suki had cleaned her bowl and Amy was eating out of her bowl
and Suki comes over and she elbows Amy out of the way and starts eating Amy's food and
this three-year-old kid looks at the cat and she said Suki you're despicable.
I was just floored by that so later on I asked my daughter where did she hear that word despicable
Wednesday last time that you remember using that word and she thought and she thought the
only thing she could think of was a couple of days earlier she had read a newspaper article
about the recent shenanigans of some local politician and she used the word despicable
to describe his actions and this little child in one exposure to that word acclaimed it as
her own.
I saw that personally her older sister apparently pulled a similar feat I didn't see this one
but my daughter told me about it when this other granddaughter was less than three years
old my daughter was saying something about her stomach didn't feel so good and this little
baby says maybe it's acid reflux I mean they're just amazing the rate they learn words.
Okay language and language perception what happens to language and language perception
as we age well it's not that happy of a story I mean it could be worse there are declines
in both speech intelligibility and speech comprehension in other words we have trouble
understanding words and if we do hear a string of words sometimes we have a little trouble
comprehending exactly what the meaning of that message was where do these deficits in
comprehension and intelligibility come from some come from presbycusis the progressive
loss you're hearing for high frequencies as a function of age because some components
of sounds are in the high frequency range they also come about because of the general
decrease in auditory sensitivity due to that thickening of the eardrum and the atrophy
in the striavascularis kind of tuning down the volume on our sensitivity there are also
the generative changes that take place in the eighth nerve or at least the auditory part
of the eighth nerve making sounds travel more slowly and making it more difficult for us
to follow rapid speech as we age we have the most trouble following soft spoken people
who speak rapidly and my suggestion would be in those situations to politely but assertively
say please slow down speak up and face me when you speak because looking at somebody's
lips is helpful in interpreting their speech even if you're you're hearing is normal in
our next lecture we will talk about some other interesting aspects of human perception visual
perception abilities thank you
