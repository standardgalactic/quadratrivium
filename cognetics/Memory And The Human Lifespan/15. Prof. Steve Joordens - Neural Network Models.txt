Lecture 15 Neural Network Models
Imagine I am a neuron. I have many arms that reach out like limbs of a tree, and I use these
arms to get signals from other neurons. At any given point in time, a number of those
neurons are sending me excitatory signals, and others are sending me inhibitory signals.
And I essentially listen to those signals and integrate them. I add them up. And if
any time the number of excitatory signals outweigh the number of inhibitory signals
by enough of an amount, then I will fire. Okay. What that means is that I will then
send a signal to other neurons that I'm connected to. And depending on the sort of neuron I am,
I may be sending an excitatory signal or an inhibitory signal. Somehow, the entire complexity
of cognition arises from a network of these sort of interconnected neurons sending these
very simple signals to each other. Amazing. In the previous lecture, I described recent
research that links many of the memory systems we've been discussing with the brain areas
that are associated with each. And that research identifies what we call the neural basis of
cognition. In the current lecture, I want to highlight how findings within cognitive
neuroscience have supported the development of a brand new way of theorizing within psychology,
one that relies heavily on what are called biologically inspired neural networks. These
models try to computationally emulate the interaction of neuron-like processing elements
in an attempt to use such models to account for memory phenomenon. This work represents
the cutting edge of current memory research and our discussion of it will highlight how
it brings together issues of brain processes and issues of brain structure in an attempt
to provide a powerful and comprehensive theoretical framework for understanding memory. Now, the
embracing of neural network models in psychology is similar in some way to phenomenon that
happen in the other older sciences. So, for example, in their infancy, many sciences began
by testing hypotheses that are based on what we call simple verbal theories. And a verbal
theory is a theory that's so straightforward one can state it in a sentence or two. And
the predictions of that theory are so obvious that they follow naturally from that verbal
description. Simple theories are great as far as they go, but as scientists continue
to learn more about virtually any topic, they tend to realize that verbal theories can't
do everything. So, for example, when trying to understand the movements of heavenly bodies,
Galileo felt that verbal theories would not suffice and he began relying on mathematics
to describe the complex motions he was observing. Galileo was actually criticized for relying
as heavily as he did on mathematical theorizing. Many of his colleagues felt that the language
of math was just too elitist, too academic, and as such, it would take the study of the
universe too far away from the abilities of the common people to understand. Well, in
the scientific study of memory, we saw that this turned to mathematics, that was there
right from the beginning. Remember that Ebbinghaus not only conducted experiments, but he also
came up with mathematical formula to summarize the results in his famous forgetting curve,
for example. Moreover, memory phenomena are so complex that the kind of mathematics needed
to address most topics might be far more complex than even what Ebbinghaus had in mind. As
our tour of the brain and its relation to memory showed us last time, the behavior of
our memory systems results from complex interactions of brain systems. And in fact, I was actually
simplifying these interactions. When I presented the primary players involved in various memory
phenomena one by one, in reality, anytime we ask people to use their memory, we see
a number of different brain areas becoming active at the same time. The story we developed
last time highlighted only the most prominent player for each memory system we've studied
so far. So the universe in our heads is every bit as complex as the universe beyond our
atmosphere. If we really want to understand the complex links between the brain and memory
systems, then we may also need to embrace new theoretical tools capable of capturing
this complexity. Mathematics may be important, but more than simple formulas, it may be computer-based
simulations that are helpful. This is where neural network models and cognitive modeling
more generally come into play. Now these models represent current approaches to a field that
many know of as artificial intelligence. That is, they describe computer-based models that
not only try to simulate some sort of intelligent behavior that humans are capable of, but they
do so in a way that also simulates both the structures and the processes thought to underlie
human cognition. The goal then is to predict and account for human behavior, but the approach
is to do so by building a model of the brain systems that's assumed to underlie the behavior.
Thus these models become simplified versions of the brain systems that inspire them, and
they aim to reproduce the critical aspects of those brain systems in a manner that produces
an overall theory of cognitive processing. It's for this reason that we often use the
term biologically plausible in conjunction with these neural network models. They are
not verbal theories. They're not even mathematical theories, not of the type that Ebbinghaus
had in mind. Rather, they are computer-based models of the structures and processes that
occur in the brain. Somewhat like an artificial heart is a model of a real heart. These models
are assumed to be models of our brain systems. Of course, don't expect to order an artificial
brain anytime soon. I would love to say that psychology as a whole is moving in this direction,
but if I implied that it would be inaccurate. To model brain systems in this manner, a scientist
needs to understand biology, psychology, computers, and mathematics. That's a tall order, and
as a result, the scientists that perform this work actually make up a relatively small but
determined group who are literally pushing the envelope of research on memory in this
way. Their successes will likely be critical to making this sort of approach more common
in the future of psychological research. So let's enter their world, and let's get a
sense of what they do and how it can provide a better understanding of memory systems.
As you might expect, if I try to get too detailed, things are going to get messy fast. That is,
to use these systems in a research context, one needs to start using math, and sometimes
rather complex math at that. Rather than turn this into a lecture about mathematics and
computers, my goal is to give you a general sense of cognitive modeling. Those of you
who find this area interesting and would like to learn more, I direct you to the additional
readings that are listed in the booklet that came with this lecture series. So where do
we start then? Well, if we're going to model the brain, what's the basic building block
of these models going to be? As I highlighted in the beginning of this lecture, the most
basic computational system in the brain is the neuron. Neurons or nerve cells, as they're
sometimes called, they receive their input from other neurons, and based on their analysis
of that input, they either send information along themselves or they don't. When a neuron
sends information to another, then the first neuron is said to have fired. The information
they receive and send is actually very simple. Essentially, it all boils down to one of two
potential chemical signals. One neuron can either send a signal that encourages the next
neuron to fire, or it can send a signal that discourages the next neuron from firing.
That first kind of signal is called an excited-horror signal, the second inhibitory. When these
excitatory and inhibitory signals are sent in appropriate ways, the magic of cognitive
processing happens. It follows then that if we want to model this amazing feat, we want
our models to be based on some sort of neuron-like processing elements. And in this point, it's
important to highlight the distinction between the terms biologically plausible and biology.
Neurons in the brain are actually little sacks of chemicals that survive within a sea of
other chemicals, and they're only separated by a semi-permeable membrane. The separation
of positive and negatively charged chemicals results in things called chemical and electrical
gradients. And when I speak of signals being sent from one neuron to another, they're sent
by releasing chemicals that produce changes in these gradients in highly complex ways.
When we model these neurons, we model the process, but not necessarily the specifics
that give rise to the process. So neural network models, for example, do not try to replicate
the electrochemical basis of real neurons. Instead, they model the neurons in a manner
which information is sent and received. They typically do so by passing along positive
or negative numbers rather than positive or negatively charged chemical elements. Thus,
the building blocks of these models are neuron-like elements, often referred to as nodes, and they're
considered biologically plausible because they model the processes that neurons accomplish
while not modeling again the specifics of the biochemical system underlying these processes.
So let me give you a taste of how one can combine these simple nodes to produce a system
that makes simple decisions. So imagine two input nodes that might at any given time be
on or off. When they're on, we assign them a value of one. When they're off, we assign
them a value of zero. Okay. Both input nodes are connected to an output node, and the connections
between the input nodes and the output nodes are given a weight. Let's say that the connection
weight between each input node and the output node is set at 0.5. At any time, the input
coming to the output node will equal the sum of the state of each node multiplied by the
weight between it and the output node. Okay. So let's say now we want this little network
to be able to perform what is called the AND function. And what that means is we want
the output node to fire only when both node one and node two, the input nodes that is,
are both on or firing. All we need for this to work is to set the threshold of the output
node to some number bigger than 0.5 but less than one. So let's say my threshold is 0.8.
Okay. So now when both input nodes fire, the total activation coming to the output node
will be one times 0.5 the weight for each node. So when we sum the 0.5 plus the 0.5,
we get a sum of one. Given that one is bigger than the threshold of 0.8, the output node
will also fire, indicating that both input nodes are on. Now note, if only one input
node was on, the summed input would be just 0.5. The other would be 0 times 0.5 and that
would be 0. So that would not be enough to raise above the threshold and it would not
be enough to make the output node fire. Okay. That's the general idea of how these computations
are done. Weights are multiplied by activations and then these weighted activations are summed
across the input units. The sum is then compared to a threshold that we are modeling and basically
if it sees the threshold, then some very simple decision can be made or not. Now, these sorts
of very simple simulations were first carried out actually in the late 1950s and the networks
created at that time were something they called perceptrons. By the way, perceptrons hit a
snag and they hit a snag when trying to model something called the exclusive or function.
That is, you can try to do this yourself and see if you could set up numbers. How do you
set up weights to make an output node fire when just one of the inputs is active but
not both? So the output node must fire if either input one is on and two is off or vice-versa
but not fire if they're both off or both on. It turns out it's impossible to set up the
type of mathematics that will do that and this problem stopped neural network models
in their tracks for almost 30 years. Now, the solution came about when people began to
build larger networks. So we can connect sets of nodes together to model brain systems and
then we can connect up these brain systems to model performance. For example, one of
the models that rejuvenated an interest in neural network models was a model of reading
development that was originally proposed by Mark Seidenberg and Jay McClellan in the 1980s.
That model contained a set of nodes assumed to represent the spelling of words, that is
their visual features. Another set was assumed to represent the sound of those words, that
is their phonological features. The challenge of the model they were investigating was to
see if they could somehow train it to provide a proper sound representation for both regular
words like pill and exception words like yacht. And by exception I mean the sound isn't just
sounded out version of the spelling, that would be yacht. So they wanted to say the
right sound yacht. They wanted to trace the development of that ability within their model
and ultimately they wanted to relate their findings back to the development seen in childhood
reading skills. The spelling of a word was represented as a very specific pattern of
neural firing across the so-called spelling nodes. And these nodes would be seen as analogous
to the neural tissue that might exist in the occipital lobe, the lobe that deals with visual
input. So when representing a given word some of these nodes would be on analogous to a
neuron firing and some would be off. No single node is especially critical. It's the overall
pattern of firing that matters. In Seidenberg and McClellan's model there were 105 nodes
used to represent the spelling of just short of 3,000 monosyllabic words. There were 30
of these nodes that represented the onset of the word, 27 represented the vowel, and
48 represented the ending sound or what's sometimes called the coda. So then a given
word was represented by three nodes firing. One to represent the onset, one to represent
the vowel, one to represent the coda. The concept of a distributed representation can
seem a little complex at first, but when we think about it a little it's really not such
a foreign idea. For example, if you watch TV how are concepts represented on television?
Well, they're represented across a fixed number of so-called pixels or little points of light.
Many high definition televisions use a grid of 1920 by 1080 pixels, a total of just over
two million pixels in all. Each pixel lights up with some mix of three colors, a combination
of red, green, and blue, but here's the important point. No single pixel tells you anything
about what is being shown on the screen at any given time. The image that you see is
determined by the pattern of light across the entire set of pixels. Scientists are reasonably
certain that the brain represents the concepts in a similarly distributed manner, and they're
convinced of that for a couple of reasons. First of all, when people see some event or
remember some event, it's typical to see many different brain areas activated as they perceive
or remember. So if they see an apple, for example, that does not cause just a single
neuron to fire, it causes a whole distributed pattern of neural firing. Second, when people
suffer damage to their brain or when their brain ages and neurons die, for example, we
do not see them losing particular specific concepts. They don't suddenly forget what
a tree is or what a car is. They don't have that part of semantic knowledge just suddenly
gone forever. Instead, they tend to show a pattern of memory loss that's termed graceful
degradation, a general impairment to memory that is not specific to any concept.
Okay, so now think back to your TV and imagine that your TV somehow suffered damage to its
pixels. So let's imagine that one out of every ten pixels suddenly stops working and that
these defective pixels are randomly broken. What would be the result? Well, the picture
would not be as good as it was. It wouldn't be as clear. There's no doubt that the experience
would be somewhat degraded, but it would be a very general degradation. Your TV certainly
would not have lost its ability to display certain concepts. Instead, you could still
see what is being shown fine. Every concept would be a little degraded, but none would
be lost. And this is what's meant by graceful degradation. And it's a general characteristic
of distributed representations, one that seems to mimic the effects of damage on human memory.
Okay, so now we have models that represent information in a brain-like manner across
a set of brain-like notes. Where does cognition come in? Well, cognition is often described
as information processing, the altering of information into different forms as it is
processed in a cognitive system. So if we return to Seidenberg and McClellan's model,
for example, that model would begin with the representation of the letters of a word, and
then via connections between the nodes, that initial representation would cause changes
to the states of other nodes within the network. Specifically, this particular network had
three layers of nodes that were organized into what's sometimes called a feed-forward
structure. The bottom layer contained the nodes that represented the spelling of the
words. And as I mentioned before, there were 105 nodes in this layer. Every node in that
layer was connected to each of 100 nodes in a middle layer. There's sometimes called hidden
units. That layer was not associated with any specific kind of representation. Instead,
it performs sort of a remapping function, and these hidden units are very important.
The nodes in those middle layer, those hidden units, are then connected to each of 61 nodes
in the third or so-called output layer, and the layer of nodes here are assumed to represent
the sound of the word. So a word could be represented at the letter level. The pattern
associated with that word would pass activation to the middle layer, the so-called hidden
units, via the connections that weight those activations. So based on the input that each
node in the middle layer receives, it may or may not then pass activation up onto the
sound layer, with the activation again being weighted by the connections between the nodes.
Based on the activations they received, nodes in the sound layer would either become active
or not, and the overall pattern of active and non-active nodes would represent the sound
that the model produced when presented with that specific letter pattern. If this sounds
like the description I gave you earlier of neurons receiving input and then either sending
output to other neurons or not, well great. That's the idea. That's what we mean by biological
plausibility. Okay, so two things to now note about all of this. The first is the character
of the processing. Virtually all of these models use what is termed parallel processing,
what we mean by that is that the nodes are not considered one at a time, with each one
being changed or not at a single given point in time. Instead, the input to all of the
nodes is considered simultaneously, and based on the state of the nodes in the layer before
them, and then the nodes, they get that state, the activations, and then all the nodes in
a given layer change together, or don't change, again, depending on the input they receive.
So we have a mass changing of a number of nodes at once. That's what's called parallel
processing, and that's yet another example of attempting to mimic the sort of processing
that we believe to be true of the brain. Most computers that we use are serial devices.
They make changes to information in a one-at-a-time manner, and this is basically true even when
they have multiple processors. The brain, though, is a parallel computing device. It
changes lots of information at once in a single step. So once again, the models are
trying to simulate the kind of processing that occurs in the brain. So there are distributed
representations, and there is parallel processing, and that's why these models are often referred
to as parallel distributed processing or PDP models. Okay, so now on to the second noteworthy
point. I described information flowing from the word nodes to the hidden units in the
middle to the sound nodes, but how does the network know how to produce the right sound
for a given word? Well, this is really where the hard work comes in with respect to these
models. The models all need to be trained before they can work effectively, and in the
case of the model I described, training would mean presenting a word and initially letting
the model produce any sound it wished, so it would produce a sound in a rather arbitrary
way. But then we compute the difference between the pattern that it produced and the pattern
that we wanted it to produce, and that difference provides us a quantification of the error
in responding. We then adjust all the connection weights between the nodes, and we do so in
a way that will reduce this error. And there's different algorithms for doing this, and in
fact, those algorithms are so important that they themselves have become a little bit of
a sub-ary in this pursuit, trying to find the best algorithms. In all cases, though,
the concept's the same. It's like when little Sammy reads the word B-I-G as beg, and we
say no Sammy, not beg, big. The hope is that Sammy then adjusts something within his reading
system so that in the future the proper pronunciation will occur. In neural networks, the thing
that's adjusted is the weights, the connections between the nodes. Does that sound familiar?
Neuroplasticity, connections, forming and not forming. Now of course, all words use the
same set of weights, so when you change things to enhance the pronunciation of one word,
that will affect the pronunciation of other words. In brain terms then, when we adjust
the impact that one neuron has on another, we will affect all of the processing context
in which that pair of neurons play a role. What that means is that a change made to enhance
some specific processing simulation has more general implications. And as you can imagine,
then getting that algorithm right is a major area. As I suggested, it literally has spawned
an entire separate research effort that involves people trying to think up and compare different
algorithms to see which one produces, first of all, strong performance, but also one that
seems to map onto the sorts of algorithms that the brain may be using. The Seidenberg
and McClellan model that I've highlighted throughout this talk is a model of reading.
Previously, I've described reading as a dance of a sort, between procedural memory, the memory
we use to speak words, and semantic memory that ties words to their meanings. But what
about other memory systems, especially episodic memory? Can these sorts of models also describe
our rich memory for episodes? Well, probably the most advanced neural network model of
human memory is the model proposed by Randy O'Reilly, a professor at the University of
Colorado in Boulder. O'Reilly's model of the brain combines what he calls slow learning
systems and fast learning systems. The slow learning systems are intended to model association
cortex. So, according to this model, this is where the patterns that underlie durable
long-term memories reside, or better said, are recreated. The fast learning system is
identified much more directly with the hippocampus. So, this system is able to very quickly represent
new information, and during the process of consolidation, it's able to repeatedly represent
that information in a manner that essentially trains the slow learning cortical systems.
For something to become well integrated into the cortical system, it needs to first be
represented in the hippocampus, maybe even across multiple episodes. So, if I ask you
what the capital city of Florida is, and assuming that you know the answer is Tallahassee, you
are retrieving that information from your cortical system, but the information got into
your cortical system thanks to repeated encounters with it in your hippocampus. The system in
O'Reilly's model can also work in reverse. If you're recollecting some episodic memory,
the relevant information that is stored in the cortical system is loaded into the hippocampus
system. There it can be combined with other information to effectively recreate the initial
episode. So, the hippocampus is critical for episodic type memories, and the cortical system
is critical for semantic type memories. But of course, episodic memory utilizes information
from the semantic system, and things get into the semantic system via episodic memory. It's
a true network, and that's why network models hold such promise. So, O'Reilly's implementation
essentially performs the sort of processing that I told you about when I discussed episodic
and semantic memory in the mapping memory lecture. This is a realization of the promise of these
neural network models, and in this sense, the model is providing a working theory of both the
processes and the representations that give rise to memory phenomenon, and it does so in a way
that mimics the sort of processes and representations used by the brain. That said, the approach
that I've been highlighting in this lecture so far represents an attempt by psychologist to
produce a theoretical and computational model. The goal is to produce a model that's sufficiently
complex to capture very rich interactions between brain systems, the brain systems that give rise
to memory phenomena. In many ways, then, this is still a relatively pure academic pursuit. These
models are still just theories for the most part, theories that can make predictions we can test,
and that advance our understanding of memory systems. However, there are some relevant points
one can take away from this. For example, O'Reilly's work on fast versus slow learning systems
suggests the following. The fast learning system, the one that underlies episodic memory, it's
relatively prone to error, it's fragile. Sometimes that system simply fails to provide an answer to
some query. So for example, maybe we're retracing some route to an old friend's house and we get to
an intersection. As we look both ways, we simply cannot consciously remember going left or right.
Our episodic memory has failed us. What do we do? Well, according to O'Reilly's model, we should
literally trust our intuition. What sometimes used to be called women's intuition, or as
manly men like to say, we should go with our gut. In O'Reilly's model, the gut is the slow learning
system. It may have represented our past travels in a way that does not support episodic memory,
and yet does support a general sense that one direction feels right. When all else fails,
we should trust that sense. So if you want just one take-home point to remember from all
that's happening in neural network models, you could remember it like this. Use the force, Luke.
