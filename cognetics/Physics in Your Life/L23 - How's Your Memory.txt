Lecture 23. How's your memory?
Well, I'm not really asking about your memory.
I'm asking about your computer's memory.
In the previous lecture, we saw how computers can do useful things.
They can decide whether two binary bits are the same,
or whether two binary bits are not the same,
or whether and this one is one, and this one is one,
or this one is one, or this one is one,
or they can switch a one to a zero and vice versa.
And out of that, we built up really useful functions,
like comparing two 8-bit words, 8-bit symbols for a letter,
to see whether they're the same.
And we could see how that would provide a useful function,
say, in a word processing program.
But isn't enough that a computer simply do those operations?
It has to be able to put the results somewhere to be useful.
If you're searching for the word DOG in your computer,
and I described last time how that might in fact happen,
and you want to change it to the word CAT, that's fine too.
You just load the C in where there was a D and so on,
but where do you store that?
You've got to put that information somewhere.
Computers need a means of storing their information,
and that means of storing is called generically memory.
Although we tend to use the word memory more for the memory
that's internal to the computer itself,
when we don't think of things like CDs as memory, but they are.
And I'd like to talk about all different forms of memory
that a computer has, and then focus on the semiconductor memory
at the heart of the computer and give you a sense of how it works.
There's a whole hierarchy of ways of storing information in computers,
and as the technology changes and as economics vary,
what are the current ones being used vary with time.
For example, in the old days, computer memory
at the heart of the computer, the so-called core memory,
consisted of little tiny rings of magnetic material
with wires going through them in a two-dimensional plane.
The whole array of these things, they were painstakingly woven together
by hand to make maybe thousands of bits worth of memory
in an individual computer.
That was superseded probably in the 1970s, late 1970s
by semiconductor memory, and we'll talk a lot about that.
But computers also have other forms of memory
that tend to be slower, harder to access,
but able to store more information and cheaper.
And it's that equation, the economics of memory,
the amount of material that can store and the speed
that are all trade-offs, and that's continually changing.
So what's true of computers today and what kind of memory they have
will not be true tomorrow.
Ten years ago, computers didn't always have CD drives
because optical storage was a new thing then.
Now all computers have CD drives and probably most have DVD drives
and they can store information in DVDs,
which as we know, hold a lot more information than CDs,
and we understand why.
It has to do with the fraction limit
and the wavelength of the light used, and so on.
So I'm going to give you just a quick overview
of the kinds of memory that a typical computer might have
at the time we're making this course,
and that is going to change as time goes on.
So typically there's some kind of external storage.
So let's look at a picture of a typical desktop computer here,
a tower-type configuration.
At the top it's got a slot for an optical drive, CD or DVD.
Eventually that will also be Blu-ray, I'm sure.
So there's a recordable, removable medium of some sort,
a CD in this case, or CD, a writable CD,
on which I could store information
or on which I could have stored information brought from elsewhere
and enter that into my computer.
Now a CD can store about 640 megabytes of information.
A DVD can store somewhere on the order of not quite 5 gigabytes,
5 billion bytes of information.
That's a lot of storage.
But that storage is relatively slow to read.
You put a DVD or CD in your computer,
especially if it's got something like a movie on it,
it can actually take a long time to read out
so long that the movie kind of appears on your screen haltingly.
So optical storage on these plastic discs, CDs and DVDs,
is cheap.
CDs cost way under a dollar.
You can get them for 10 cents a piece if you buy them in bulk.
They're cheap.
Easy to write to, easy to read, but slow, relatively slow.
So a high volume storage medium, but slow,
vantage, it's external, it can be removed,
you can archive information and store it somewhere else.
The workhorse of your computer,
in terms of long term storage of information,
is its hard disk.
Almost all computers have hard disks.
The hard disks are ultimately magnetic.
The information is written as little domains
or magnetic material and similar to the way I described earlier
that one writes a tape, a VHS tape or an audio tape
by passing a current through a little tiny coil
that's near the magnetic medium.
The magnetic medium moves past the coil
and you impose a pattern of magnetization on it.
It used to be that that pattern would be read,
as I described a few lectures ago,
by electromagnetic induction as the disks spun,
the different changing magnetic regions would induce a current
and that would take the information off the disk.
Modern computers, the so-called magneto resistive effect is used
and what that does is change the resistance,
electrical resistance of a little sampling element in the head
as the disk spins underneath and that has the same effect,
though, to produce an electric current that varies
in a way that corresponds to the information stored in the disk.
So the second major piece of memory that a typical computer has
is its internal hard disk.
It might also have external hard disks,
which you can connect for extra storage, for backup, whatever.
I have an actual hard disk here.
It's out of a computer that's a few years old.
This hard disk is the size of the disk
that would be inside a desktop computer.
A laptop computer would have a disk,
maybe a fifth the size of this, a tiny little disk.
Inside here is the disk itself.
I'll take that apart in a minute.
On the bottom is the bottom end of a circuit board
that has a lot of integrated circuit chips on it
whose sole job is to read and write the information from the disk
and to transfer that stream of bits that's coming off the disk
into the form that can communicate with the rest of the computer.
In the case of computer hard disks,
there is a platter of metal coated with a magnetic material
and it's spinning at very high speeds,
thousands of revolutions per minute.
And there is a head that literally flies,
held aloft, if you will, off the disk by aerodynamic forces.
And the distance is on the order of a millionth of a meter.
And a disk crash is literally a crash.
It's a little bit like an airplane crash.
That flying disk, if something happens and it hits a particle of dust
or something and loses its aerodynamic forces,
it will crash into the disk and damage the disk surface and damage the head.
And there goes your PhD thesis if you haven't got it backed up
or all your pictures of grandma or whatever you had at that point on the disk.
And if it wipes out the directory of the disk,
which is what tells the disk where everything is,
even though everything else is still fine, you may have lost it.
That's why this hard disk has a cover on it.
I'm going to take the cover off and show you the hard disk.
This hard disk is actually a stack.
You can't see it very well, but it's actually a stack of four or five
of these shiny metal disks.
They spin very rapidly and there is a small projection here, which is the head,
and it can move in and out back and forth in an arc.
It can't do that right now because it's locked,
but it can move in an arc and read and write information off different tracks on this disk.
So this is an electromechanical device.
This disk is spinning at very high speed,
and that's where the information is stored.
If you turn on your computer, part of the noise you hear is the hard disk spinning.
Because in a desktop computer, it tends to spin all the time.
In a laptop computer, especially on battery power,
it may spin up and down to save battery,
but that obviously makes it slower to get information.
It's easy for this head to move back and forth.
In fact, you'll hear a clicking noise as your computer boots up
or as you read a big file off the disk,
and that's this head physically, mechanically moving back and forth
to different parts of the disk to read the information that's stored in different places.
As a disk gets fuller and fuller,
a single file, your term paper, your PhD thesis,
your picture of grandma,
they may be stored in little hunks in different parts of the disk,
and the head has to go around and find all those parts
and put them together to make a complete file.
What do we put on hard disks?
Well, we tend to store our documents, things that we've written,
pictures that we've taken, songs, music, whatever,
the kinds of things, our programs, that kind of thing, tend to be stored on hard disks.
In addition, we store software on hard disks.
When you double-click your word processing program,
your word processing program lives on the hard disk most of the time.
When you double-click it, it's loaded into the computer's internal memory,
which I'll get to in a moment, and it's executed from there because that's much faster,
but its permanent location is on the hard disk.
When you hit the Save button on your computer or issue the Save command,
what you're doing is taking information that's stored in the computer's temporary
but very fast internal memory and writing it onto the hard disk.
The beauty of the hard disk is it doesn't have to be spinning to remember.
When you shut down the computer and turn it off,
the hard disk sits there with its regions of magnetization,
and they don't go away any more than when you take your favorite movie out of the VCR.
It ceases to be on the videotape.
This is just like videotape or audio tape.
The information is stored there magnetically, and it's still there.
It's called non-volatile memory.
It doesn't disappear when you turn the power off.
So the hard disk is non-volatile memory.
It's easy to get very large hard disks at the date at which I'm recording these lectures,
and again, it's hard for me to talk about timing with computers
because of Moore's Law and everything doubling in 18 months.
But computer hard drives are approaching 100 gigabytes,
100 billion bytes of information on a good-sized computer hard drive,
and you can certainly buy hard drives that are much bigger.
A thousand gigabytes, a terabyte used to be the province of supercomputers only.
Now you can buy a terabyte of storage if you're something like a photographer
with zillions of digital photographs.
You might need a terabyte of storage.
Some of us can get by with 20, 30, 40 gigabytes.
Computers of 10 years ago, it was 20, 30, 40 megabytes.
Factor of a thousand increased since then.
There's Moore's Law at work.
So the hard disk storage is relatively cheap,
although the hard disk is a fairly complicated mechanism.
It's an electromechanical mechanism.
It's a very delicate sensitive mechanism.
It's cheap only because you can get huge, huge, huge amounts of storage
crammed into a relatively small place at relatively small cost.
But it's not terribly fast because, again, you have to move that head
physically, mechanically, a process that takes a fraction of a second
to get it to the right place.
And then as the rotating disk goes around,
you have to wait until the right information is under the head and read it off.
So the hard disk is not fast.
They're pretty reliable these days, but not perfectly reliable,
and you ought to back them up because that hard disk,
with 100 gigabytes or whatever,
is holding an awful lot of valuable information.
If you lose your hard disk and you've got no backup,
you've lost an awful lot.
Then there's a third type of memory inside the computer,
and that's the computer's internal random access memory, or RAM.
And I've got some RAM here.
I showed you this in the last lecture, too.
This is a RAM module.
It's got, in this case, nine individual memory modules on it,
each with a few hundred megabytes of megabits of information.
There are nine of them, eight of them are the eight bits that make up a byte,
and the ninth is for various processing and error checking purposes.
And I'll talk more in this lecture about how that random access memory works.
But this is random access memory.
A hard disk is somewhat random access.
A tape is called sequential memory
because you can't get information off a tape
until you've wound the tape all the way to the point where the information is.
Hard disk is pretty random.
Once you get to the part of the disk you want to be on,
then you have to wait until the right bits come around.
Kind of the same with a CD or DVD.
But this memory is truly random access.
You can get to any point in the memory.
You can get any information out in equal amount of time.
You don't have to wait for it to go past other amounts of memory.
And we'll talk about how that memory works in just a minute.
But today's computers typically have a substantial amount of computer memory,
of RAM memory, typically, oh, well, my laptop right here has a gigabyte.
That's still probably on the large side,
but that's because I use this for scientific research,
as well as normal computer uses that everybody uses.
But a 500 megabyte memory would be a typical amount of RAM.
256 is getting a little low.
You may notice those numbers tend to be sort of strange numbers,
and the reason is those numbers are powers of two.
And I'll show you in a minute why that has to be.
So semiconductor memory, memory made of semiconductors,
ultimately of the kinds of transistors we've been talking about,
is the fastest.
Compact, it has no moving parts,
and the only reason it isn't used as the dominant memory in computers
is because it's more expensive.
Well, partly that and partly that the best,
the fastest semiconductor memory is also volatile,
meaning it stores information only while the power is applied,
and if you take the power off, it's gone.
That's why if you forget to save your document
and your computer crashes or the power goes down,
you're lost because everything that was in the computer memory
is totally wiped out and gone.
You've got to hit that save button frequently
or you've got to have a program that auto saves,
and if there's a crash, then it can bring you back
everything that was there at least
until the last time it saved information.
It is possible to get semiconductor memory that's non-volatile.
I have an example of it here.
This is my little pen drive.
It's actually called a drive because it used to be,
I would buy a small hard drive
to store excess information on with my computer.
In recent years, semiconductor memory has become cheap enough
that we can build these little tiny pen drives.
This is a 256 megabyte drive.
It has a quarter of a gigabyte of information can be stored on it,
and I simply plug it into the back of my computer
and it becomes available as another storage medium,
and it consists of semiconductor memory which is non-volatile.
The trade-off for the non-volatile memory is,
although it retains its memory when the thing is off,
it is a lot slower.
So getting information on and off that is not nearly as fast
as getting it on and off my internal memory.
By the way, the memory in a digital camera is the same type
that's in that little USB pen drive that I have there.
But let's talk about semiconductor memory now and how it works
because we're trying to get here from atom to computer
and we want to relate this to what we know about atoms
and semiconductors and PN junctions and transistors and so on.
Well, we've built up beyond that.
In the last lecture, I introduced these symbols for logic circuits
and said we aren't going to think about the individual transistors.
They're in there. We understand that they're in there.
We understand how they work. We can go all the way back to atoms.
We're going to start with these symbols.
And here's a very simple memory built from a couple of knot gates.
This particular memory is called a flip-flop.
And the reason it's called a flip-flop is because it has
only two possible states that it can be in.
Only two possible configurations.
Let me show you.
Suppose the output of that right-hand knot gate is a 1.
And remember what a knot gate does.
A knot gate's truth table is if the input's a 0, the output's a 1.
If the input's a 1, the output's a 0. Very simple.
It's an inverter, a notter.
It takes the input and makes it whatever it wasn't.
OK. Well, these two knot gates are connected together in a funny way.
The output of the right-hand one goes around through that wire up above
and goes into the input of the left-hand gate.
And the output of the left-hand gate goes into the input of the right-hand gate.
So these two gates, these two logic knots are connected together.
The output of one to the input of the next,
the output of that one back into the input of the other one.
So what's going on? Suppose this is a 1.
That means in a typical computer circuit it's at a certain voltage.
5 volts is often the value.
0 volts would correspond to a 0.
And there's actually a range, quite a range of voltage,
all of which will be interpreted as a 1
and all of which will be interpreted as a 0.
That's what makes computer logic circuits digital on-off circuits so robust.
So hard for them to have mistakes.
So there's a 1 at the output of the right-hand gate,
and that means that 1, because the wire connects them,
is at the input of the left-hand gate.
And if the input of the left-hand gate is a 1, it's a knot gate.
It's output is a 0.
If it's output is a 0, that's the input to the right-hand gate.
The right-hand gate's output should be a 1.
So this is a perfectly logical situation for this circuit to be in.
It's a 1-bit memory.
Right now, if we think of the right-hand gate's output as the state of the circuit,
this circuit is in the 1 state.
It's storing a digital 1.
It's storing a yes, a 1, a binary number 1.
It's also possible for this circuit to be in a different state.
The output of the right-hand gate could be 0.
Then the input of the left-hand gate is also 0, because they're connected together.
Well, an inverter, a knot circuit, flips that to a 1,
puts a 1 at the output of the first gate, the left-hand gate,
and a 1 at the input to the right-hand gate should make a 0 at the output.
So that situation is also perfectly logical and allowed.
There's a situation, though, that is impossible.
It's impossible for the outputs of both gates to be 1s.
Why? Because that's inconsistent with what these knot gates do.
That's logically inconsistent. That's not possible.
So what I have in this 1-bit memory is a circuit that can exist in one of two possible states,
and that's all.
And once it's in one of those states, it will happily store that information,
as long as you keep the power applied, forever.
The trouble is, I can't easily change the state of that circuit.
So to show you how I can change the state,
I want to draw that circuit in a slightly different way.
Here is the same 1-bit memory circuit drawn in a different way,
a more symmetric way that two gates kind of enter symmetrically.
And just convince yourself a minute that that circuit is exactly the same as what I had before
when I started this thing out.
Two knot gates connected one output to the input of the next one, and so on.
And that is exactly what I have, again, when I do this, when I draw it like that.
It's exactly the same circuit just drawn differently.
And emphasize there's no connection where there's wires crossed at the beginning.
One is the output of the upper gate, going to the input of the lower gate,
output of the lower gate, going to the input of the upper gate.
But now I want to make a circuit in which I can change the state of the circuit somehow easily.
I can say, OK, let me put a 0 in there,
because I determined that the result of that operation was a logical 0,
and that's what I want to store in that place.
So how do I change the state of this simple memory circuit?
Well, I replace those knot gates with a more complicated gate
that has an extra input.
And in this particular case, I'm going to replace it with nor gates.
Again, convince yourself that that circuit and that circuit are essentially the same,
except that I have those extra gates in place.
And I'm going to call the output of the upper gate the stored bit,
and it could have the value 1 or the value 0.
So that's what I'm going to do there.
And now I want to figure out how I can change the state of the circuit.
Before I do, let me point out that what we typically do with this circuit
is normally keep both those extra inputs in the 0 state.
And if we do, if you look at the truth table for the nor circuit at the lower left,
if one of the inputs of a nor gate is 1 of the inputs of a nor gate is a 0,
that would be the first row, the second row, the third row.
The output could be either a 1 or a 0 depending on what the other input is doing.
In the first row, if one of the inputs is a 0, if they're both 0, the output is a 1.
If the first input is a 0 and the second one is a 1, the output is a 0, and so on.
But the point is this, if we're in the 0 state at both of those inputs,
this circuit basically behaves like the other circuit I just had.
These gates simply act as inverters.
Whatever the, if the first one is a 0, if one of the inputs is a 0,
the output is the opposite of what the other one is.
Take a look again at the first two rows in that truth table.
If the first input is a 0, input A is a 0, then whatever input B is, the output is the opposite.
So those two gates are behaving just like those inverters or not gates I had before.
So if we have those two extra inputs in the 0 state, then this circuit is exactly like the circuit we had before.
Nothing new and exciting happens.
And let's do a little analysis. Let me remind you that there's no connection again.
So here we are with these two extra inputs in the 0 state.
And let's suppose for a moment that the stored bit is a 1.
Let's convince ourselves that this is all consistent.
If the stored bit is a 1 because that output is connected to the input of the lower gate, that input is also a 1.
Take a look at the truth table. That's a 1 at the upper input, A, a 0 at B.
That's the third row in the truth table. That says the output should be a 0.
If the output of that gate, the lower gate is a 0, that puts the other input to the upper gate at 0.
Both inputs to the upper gate are 0.
Well, that's the first row in the truth table, 0, 0, 1.
The output of the upper gate should be 1.
It's all totally consistent and everybody's happy.
You can convince yourself in the same way that if the stored bit were a 0,
then there's two 0s on the lower gate.
We're in the upper row. The output of the lower gate is a 1.
That puts the other input of the upper gate a 1 and a 0 and a 1 at the inputs gives us a 0 at the output.
So that's perfectly consistent also.
So this device can be in either of these two states and everybody's perfectly happy
and it will simply stay in that state as long as it needs to,
which is basically forever as long as the power is applied.
Now let's try to figure out how to change the state.
So let's take one of those new spare inputs and temporarily change it.
Let's change the lower one, for example, to a 1.
So there it goes turning into a 1.
Now we look at the truth table and the truth table says if we have a 0 and a 1,
we ought to have a 0 on the output.
So that output of that lower gate has to change to a 0.
Now we have a 0 and a 0 on the upper gate.
So we look at the truth table again and we see that a 0 and a 0 gives us a 1 at the output.
So that means the stored bit has to become a 1.
And then we have a 1 and a 1 on the lower gate and that changes,
that requires that our output be a 0, which it was, so that was fine.
And now if we bring the lower gate back down to 0,
that still has a 0 in the output of the lower gate and so we're still perfectly consistent.
And so what we did by raising that lower input to a 1 temporarily is change the stored bit to a 1.
Then when we brought that lower input back down to a 0, the stored bit stayed a 1 and we've stored a 1 in there.
So that is the way we change the state and I'm not going to go through all the analysis,
but you could convince yourself that if we did the same thing with the upper loose input,
changed it temporarily to a 1, we would force the output to be 0,
regardless of whether it had been 1 or 0 before.
If it had been 0, it would stay 0.
If it had been 1, it would go to 0 and then we would have stored a 0 as the stored bit.
And then when we dropped that input back down to 0, the 0 would stay as the stored bit.
So now we have a circuit in which we can change what's happening.
So I would like to think of this circuit more as a little memory circuit that can store a 0
if I temporarily raise the first input, the upper input to a 1, or temporarily store a 1
if I raise the lower input to 1.
Don't you dare raise them both to 1 simultaneously, that gives inconsistencies.
And real memory circuits are more complicated than this to avoid that kind of inconsistency
and some other timing inconsistencies that can occur in these simple circuits.
So real memory circuits are a little bit more complicated.
Well, some are really a little bit more simple.
This kind of memory I've just described, built from flip-flops, is in fact the fastest kind of semiconductor memory we can make.
And some computers, like supercomputers, use semiconductor memory of this sort, flip-flop based memory.
But most of our personal computers use what's called dynamic random access memory.
The kind I've just showed you with the flip-flops is called static random access memory.
Once you've stored the bit, it stays there.
In dynamic random access memory, there's only a single transistor for each bit
and it dumps charge onto or reads charge off of a pair of closely spaced electrical conductors called a capacitor.
And the problem is the capacitor loses its charge on a time scale of, oh, less than a thousandths of a second, typically.
And so in a computer with dynamic random access memory, you have to go through a few thousand times a second and refresh this memory.
And that may sound very fast, but computers operate a billion times a second in their basic operations.
So having to refresh the memory a few thousand times a second is very, very long time intervals as far as the computer is concerned.
So that's not really a problem.
So your computer memory is probably dynamic random access memory and it works slightly different from this flip-flop based memory I've showed you here.
But this kind of memory is in fact used in computers also.
Whatever kind of memory is used, this simple circuit now is going to reduce,
whether it's this circuit or a dynamic random access memory circuit.
We'll reduce it to a single little cell and we add a few more gates to it that allow it to do the following.
We've got an output that corresponded to what our stored bit was.
That output is a 1 or a 0 and that's the stored bit.
We have an input where we put in what bit we would like the output to become.
We have a read-write control that tells us whether we're going to write a new piece of information to this cell,
change its input or change its output from a 0 to 1, change the stored bit from a 0 to 1
or whether we're going to read out the stored bit so we can either write to the memory cell or read out from the memory cell.
And the select line at the top says, OK, if I make that high, if I make that a 1, this cell is active.
It's the one that's going to get read from or written to.
If the select line is low, even if there are things connected to the other inputs, nothing happens because this cell isn't selected.
So that's a single individual piece of computer memory.
Well, where do we go from there?
Well, we take those individual cells and we build them up into an array, a rectangular array of memory cells.
And each of them has its read or write input.
All of them are connected together.
So when we put a logical 1, a high voltage, a 5 volts on that read-write line,
all the cells are ready to have information say written to them if a 1 means write and a 0 means read.
But only the ones that are selected are the ones for which that will happen.
And you'll notice the select lines, those were the lines at the top, are connected together for the top 4 memory cells
and they're connected together for the next 4 memory cells and they're connected together for the third row down
and they're connected together for the fourth row down.
And they all go to this block on the left marked address decoding logic.
And in this particular case, that has 2 input lines at the bottom and 2 input lines give us a 2 digit binary number
which could be any one of 4 values, 0 0 0 1 1 0 1 1, decimal it be 0 1 2 or 3, 4 possible values.
And those correspond to the 4 possible arrays of memory cells along here.
That first one is the first word. These are 4-bit words as opposed to the 64-bit words used in modern computers.
There would be 64 of these things going along.
That's word 2, that's word 3 and the last one is word 4.
And depending on what comes in on those address lines, those 2-bit address lines,
one of those words will be selected as the one to which the read-write operation will occur.
And depending on whether the read-write is set to read or set to write,
if it's set to write, whatever's on those 4 input lines, a 4-bit binary number, a half a byte,
will go in to that particular word.
If we're asking for reading, then whatever is in that particular 4-bit word will appear on the output lines that I show at the bottom.
So that's how an actual computer memory is made.
And nowadays, if you buy a piece of computer memory, it doesn't have 1, 2, 3, 4 words.
It might have 500 million words.
And it has more bits coming in on the binary addressing.
But any of those bits can be addressed equally well.
So that's what you're buying when you're buying computer memory.
When you're buying one of these simple little modules and installing it in your computer,
you're installing an array that looks like I've shown in that picture,
an array of memory cells with a very, very large number of cells.
And the computer sends a binary number to those address lines and selects which words are getting read or written.
And when you double-click your PhD thesis or your picture of grandma or whatever it is,
what happens is that thing is loaded into computer memory.
And then when the program that you're working with needs to read it, it sets the memory to read.
It finds the right addresses where that information is being stored.
And it reads it and processes it as you want it processed.
And then it writes it back out maybe in modified form with the right lines.
Again, one of the most important specifications for your computer is how much of this memory it has.
Because if it runs out of memory, it can't do something like process that big picture of grandma
that you took with your new 6 megapixel camera, which has many, many, many, many bits of information.
Next time we'll go through how digital photography works a little bit and we'll see exactly how much information is stored there.
So you need a lot of memory. You need a lot of memory to run big programs.
And that's why memory is so important.
The amount of RAM you have in your computer is such an important specification.
And again, these days, a few hundred megabytes, probably a few gigabytes a year or two from when I make this up tape,
that's the kind of memory you'll be having, whereas your hard disk is holding maybe a hundred times as much as that.
So you have a lot more in your hard disk than in your RAM.
But sometime, probably in the next 10 years, semiconductor memory will become so cheap because of Moore's law
that it will be cheaper to make semiconductor disk drives just as we've already done for these portable units.
And then we'll say goodbye to the mechanical, electromechanical hard disk.
So let me wrap up again with the hierarchical picture.
Computer memory is all put into these little memory modules.
I've showed you these modules and then we're ready to go from atom to computer memory.
We already went from atom to a simple logic circuit like an exclusive OR gate.
Now we've put together some of those simple logic circuits to make simple memories.
We've combined those with a little more logic to make memory cells.
We've joined those together to make an array of memory cells and package them onto one of these little strips with a bunch of integrated circuits.
And that's the memory module you buy and stick in your computer if you need extra memory.
