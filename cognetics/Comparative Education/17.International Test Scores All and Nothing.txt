Lecture 17, International Test Scores, All and Nothing.
We are inundated with data that we never use.
Information exists that never existed before.
The alphabet soup of testing bodies that has arisen to measure education and student
learning attest to this.
We have TIMS, PISA, PURLS, the ICCS, and ON and ON.
These standardized international tests do serve a purpose.
They do test certain things, like how much students know about their curriculum, or whether
students can use their existing knowledge to solve new problems.
But they're frequently decoupled from the learning that takes place in real classrooms.
We also have to realize that these tests aren't always serving the most obvious or even officially
stated purpose.
Sometimes, they're designed, in effect, to serve a nation's educational system and
even the political system as legitimacy tools.
Participating in international standardized tests, like the Trends in International Mathematics
and Science Study, or TIMS, and the Program for International Student Assessment, or PISA,
and others, sometimes lends legitimacy to educational systems.
And this legitimacy can be transferable to a system's government and economy.
Take South Africa as an example.
Not long after the official apartheid system was dismantled, South Africa began participating
in the Trends in International Math and Science Study, or TIMS.
This was a great way to demonstrate the resiliency of the South African nation, as well as make
a nod to international transparency through its education system.
South African students, not surprisingly, performed on average at the lowest level in
two successive cycles of TIMS.
They even performed behind all of the other African countries, like Ghana, that spent
far less on education than South Africa did.
At first, participating in TIMS was symbolic of South Africa's renewed membership in the
international community.
But after two cycles of extremely low performance, the country's leaders decided that being a
member of the international education assessment community wasn't enough to offset the bad
news that their results showed.
So in 2007, the South African Education Department, then under Naledi Pandor, decided that the
solution was to pull out of TIMS.
No news would be better than bad news, apparently.
And then in 2010, Angie Monshega, the basic education minister, reversed that decision.
And South Africa began participating in TIMS again to show its commitment to quality education
to its stakeholders, families, and children in South Africa, as well as to the international
political and economic community.
South Africa participated, quit, and then rejoined.
It realized there was more to the test than the obvious purpose of benchmarking and student
assessment.
Now, a point I frequently make is that education is the largest and only institution in most
countries where everyone is expected to participate for significant portions of their lives.
And in some countries, they're even compelled to do so by compulsory attendance laws.
So it's not surprising that both intra- and inter-national legitimacy for education systems
themselves, as well as for national political and economic systems, rest on highly visible
international assessments like TIMS.
Even though many countries, including the United States and Saudi Arabia, have locked
away their results in a bureaucrat's drawer early in their participation in TIMS, low
or disappointing performance is generally not enough to prevent countries from participating
in these large international standardized assessments.
So participation in international assessments of student achievement isn't solely about
assessing what students know and can do.
That much should be clear.
Participation in these assessments has a political and economic purpose that provides legitimacy
not just to educational systems, but also to nations more broadly.
Now let's take some time to see how we can really use the information from these international
tests to identify a problem in education that needs to be solved and then find a solution
using comparison with other countries.
The trick is in identifying a problem that can be solved by using our understanding of
how the world learns.
The most frequent problem identified in education systems worldwide has to do with either low
or underperforming students.
How to find what works to solve that problem is a balance between reforming school factors
and non-school factors from examples and models provided by educational systems worldwide.
This lecture gives us a framework for finding what works to improve student achievement on
standardized tests in countries worldwide based on international comparison and evidence-based
decision making.
In a recent cycle of the trends in international mathematics and science study, about 70 countries
and benchmarking communities participated with more planning to participate in the future.
This large number means that there are countries from all regions of the world, all levels
of economic development, all types of political systems, and all different types of educational
quality and organization.
Most of the public discussion about TIMS centers on students' average math and science scores
by country.
This takes the form of country rankings, providing us with information about which country students
did the best in math or science on average.
For example, in 2011, and the trends in international math and science study was administered again,
the highest scoring mathematics students were from Taiwan, followed closely by South Korea
and Singapore.
And in science, the highest scoring students were from Singapore, followed closely by
Taiwan, Japan, and South Korea.
You're probably noticing a trend in the countries that perform at the highest levels on TIMS
math and science tests.
But as you know, whenever there's a winner, there's also a loser.
The lowest scoring countries were Qatar and Ghana in math and in science.
So if we follow the logic of such concerns as achievement envy, accountability, and
access entitlement, we might make the argument that Taiwan, Singapore, and South Korea score
so well because their students worked the hardest, because their teachers taught the
best, because their math and science curricula was the strongest, and because they serve
all of their students well across all sectors and strata of society.
If we follow this logic, we might also argue that Qatar and Ghana fail when compared to
Taiwan, Japan, and South Korea, because their students are lazy and their teachers incompetent,
their math and science curricula weak, and because they don't equally or adequately serve
all of the students in the education system in each country.
Now I know those sound like over-dramatized conclusions, but those are real arguments
that the media, policymakers, and parents make when they interpret the rankings results.
There are certainly problems with education in all of these countries, and Qatar and Ghana
may have more obstacles to quality education than the highest performing countries do.
But context has just as much to do with the teaching, learning, and performance of students
on these international assessments as does the content of education.
Remember, education is never an isolated enterprise.
Finland, China, and Saudi Arabia bear witness to how context can impact education.
Stable, relatively wealthy, and culturally homogenous nations have a natural advantage
on these types of tests.
There's something more to know about these country rankings on TIMS, though, and it's
important.
In short, the rankings are incredibly misleading.
They're misleading because the data for each country that participates in TIMS, or any
of the other international assessments, is sample data.
That means that a group of participating students is randomly drawn from schools in each participating
country to take the test and answer the questionnaire.
But these tests do not have data for all of the students in any country.
This is important because it means that we have to use statistics to estimate the chance
of one country's average student achievement being significantly different from any of
the others.
And what we find is important.
In short, even though we can rank the countries from number one to number 64, the rankings
don't tell us which country's student score on average, any differently from any of the
other countries.
Instead, what we do is calculate the chances that one country's scores are really different
from another's.
And when we do that, we just end up with three big groups, or maybe I should say three groups,
because one is pretty small.
It's comprised of consistently high achievers like the students in Taiwan and Singapore
or Japan and South Korea.
We actually have another small group.
It's comprised of consistently low achievers like Ghana and Qatar, Botswana, El Salvador
and Saudi Arabia.
But the largest group of countries is in the middle.
The middle group has countries that aren't significantly above the international average
and they aren't significantly below it.
In other words, most countries participating in TIMS are just average.
And it is statistically impossible to say whether they are really any higher or lower
than any of the other countries in that middle group.
Now the United States is consistently in the middle group, even though the relative
ranking of the U.S. has slowly climbed upward over the years.
So if most countries in the world don't perform significantly differently from each other,
what does it mean if a country is in the significantly high group, like Taiwan, Singapore
and South Korea, or in the significantly low group, like Ghana and Qatar?
Does it mean that they have a great educational system that we should all copy if they do
really well, or that they have an educational system that we should avoid at all costs if
they do really poorly?
Well you might be surprised to know that as far as educational systems go, the policies
for high achieving and low achieving countries really aren't that different.
Countries in both the high and low achieving groups all have a focus on equitable access
and opportunity to learn, at least in the formal policy.
All of them in both groups have a focus on recruiting and training quality teachers,
at least formally.
And all have a focus on providing the best and most up to date curricula for students
to learn and teachers to teach, again, at least formally.
The differences come from context and culture.
And so it is factors outside of schooling that make the real differences in how well
students learn and perform and how well teachers teach.
Finnish schools are funded based on a formula, guaranteeing equal allocation of resources
to each school, regardless of its location or the wealth of its community.
By contrast, schools in the United States are funded based on their district's tax base.
That means that wealthy communities, or high tax-based communities, think industrial zones,
make more money for schools.
The Finnish education expert, Pasi Solberg, makes the argument that while Americans may
be able to copy what Finland does, it can't replicate it.
Why is that?
Solberg points out that all children in Finland have, by law, access to child care, comprehensive
health care, and a preschool in their own communities.
Every Finnish school has to have a welfare team that is responsible for advancing child
happiness in school.
Children in the United States have none of this, and schools that adopt this model, often
called community schools in the United States, are the exception rather than the rule in the
U.S.
Solberg also notes that all education, from preschool to university, is free of charge
for anybody living in Finland.
This makes higher education affordable and accessible for all.
So there are not the same obstacles to continuing education beyond high school that we have
in the U.S.
This changes the expectations about school and the impact that education can or can't
have on individuals and their transition from high school to college or the labor market.
In other words, the way to compare student performance worldwide is to find what is shared
among the school and non-school factors.
And then to take careful note about which of those contextual factors, both in and out
of schools, influence how education is intended, implemented, received, and then sustained
by the students.
The fact of the matter is that the policies and practices of the highest-performing countries
aren't that different from what's already happening in the United States and in most
other educational systems worldwide.
It's the contexts that vary, and as a result, the same policies and practices have significantly
different outcomes in different countries and communities worldwide.
So instead of using international education assessments to rank countries based on average
student performance, the international data on education, resulting from these standardized
assessments and surveys, should be used to support evidence-based decision-making within
each country or community context.
Now there are critiques and misconceptions about the development, administration, analysis,
and interpretation of international tests.
This of course contributes to the debates surrounding the impact of international testing on national
educational policymaking and classroom practice to a degree that is arguably unparalleled compared
to other influences.
First, some have criticized international testing for its tendency to aggregate and therefore
mask individual or regional-level variation, which has sometimes been referred to as reductionism.
Second, some see international testing as a tool used by communities, organizations,
and individuals with a strong power base to override the interests of marginalized or
disadvantaged groups, or to merely push through the agendas of large transnational organizations
without regard for local, regional, or otherwise contextualized concerns.
Third, there are others who are skeptical of international testing because of their
cause of the inherent problems in sampling, coverage, administration, and interpretation
of such large-scale data.
Now these are all valid criticisms of international achievement studies that can't be ignored.
The danger exists, however, of throwing away relevant and needed information coming from
these international studies, rather than constructively training researchers and policymakers
how to appropriately use the results and not only improve the way the studies are used,
but also the way data is collected and measured.
The extremely isolated cases which reside at the individual level are valuable and necessary
pieces of data for understanding the particular disadvantages and needs of, for example, girls
from marginalized racial or ethnic groups in otherwise well-functioning educational systems.
The point is that if unique cultural and contextual data is lost, the process of planning, implementing,
analyzing, and interpreting international achievement studies is compromised.
Even though a lost case may be extremely isolated and not representative of the larger sample.
So then there is a potential gap in the overall understanding of a particular situation and
the impact international achievement studies have on national education policy.
In the same way, valuable information is lost if only the most individualized data is used
to make decisions.
Now let's face it.
The ability to participate in an international community of educational systems, assessing
their students and their systems performance is a great advantage to international assessments
like TIMS and others.
If we couple this with the ability to actually compare one-to-one the performance, characteristics,
and expectations of students or teachers or school leaders and curriculum specialists,
then we can make a pretty strong argument that international achievement studies can
and should have a great impact on what educators, researchers, and policymakers know about
education in their countries and in their comparison groups.
But how does the general public, a parent or a community member get information about
international achievement studies and their results?
An under-investigated factor contributing to the broad and strong impact of international
achievement studies on national education policy comes not from the research reports
and results disseminated to policymakers or school administrators or educational researchers,
but it comes from widespread publicly disseminated media reports.
In particular, reports on the results of international tests make mainstream media outlets such as
national and international newspapers, television news, talk shows, internet news, and blogs.
And the ability of media communication to be both instant and informative to the widest
possible community of parents and community members, business owners, politicians, and
other leaders often impacts the perception and use of international test results more
than any research or policy agenda from a university-based research group or a national
ministry of education.
In order to manage this dissemination of information in mainstream public outlets, the International
Association for the Evaluation of Educational Achievement and the Organization for Economic
Cooperation and Development, the two organizations that administer the TIMS and the PISA tests,
for example, create press releases, they prepare highlight brochures, and they host public
announcements about the initial results of these large-scale international tests.
They enlist the assistance of experts in the field to present the information from each
round of testing within the context of scholarship and policy relevance as much as possible.
But in countries where students perform on average below the general public's expectations,
there's often a public reaction to the results of these international tests.
Now, the U.S. and Germany have had their disappointments documented in the media, and then through public
debate and educational reform, they've had this done more than any other country.
Often the impetus behind the media reports on the results of international assessments
like TIMS and others has more to do with reporting on the economic, political, or social competitiveness
of a particular country or region than it does with the actual educational value or
evidence resulting from the study. It can also represent a shift in blame or responsibility
for wider social, political, and economic problems to a nation's educational system.
Most educational systems around the world have four general goals that they aspire to.
Most unfortunately aren't able to achieve all of these goals, but they are aspirational.
They aspire to have highly resourced schools and classrooms, strong parent and community
support, motivated expert teachers, and consistently high-performing students.
If we look at the student performance aspect first, we can use evidence from TIMS to make
some internal comparisons. The differences in content and cognitive
domains of learning and performance are one place to start.
Now the content domains in math are numbers, algebra, geometry, and data enchants at the
eighth grade level. And the cognitive domains are knowing, applying, and reasoning.
Now remember that content domains are the material that eighth grade students are supposed
to know. It's supposed to be tied to the curriculum they learn in schools.
While cognitive domains represent the kinds of things students are supposed to be able
to do with that knowledge. In the general eighth grade student population
in Saudi Arabia, the TIMS data shows us that students perform at or near their own national
average in the individual content domains of numbers, algebra, and data enchants.
But they perform significantly below their own overall mean in geometry.
Likewise in the cognitive domain, these same students perform a little bit above their own
national average in knowing math, and a little below their overall average in math reasoning.
But they're significantly below the national average in the cognitive domain of applying.
As one of the lowest performing countries in eighth grade mathematics, the first step
of international comparison shows us that Saudi students are average in every domain
except in geometry and applying. This means that the student achievement
evidence shows us where there needs to be more focus on geometry and applying.
And it shows us that Saudi students don't seem to excel beyond their own overall average
in any of the individual cognitive or content domains.
Human makers and educators might take this evidence and use it to improve teacher training
and professional development in geometry, or to improve the curriculum when it comes
to geometry and the application of mathematics concepts to problem-oriented situations.
Or they might disaggregate the data even more. This means that they can further break down
the evidence into important groupings. Since Saudi Arabia has a national system
of single-sex education which separates boys and girls into different schools altogether,
looking at the gender differences in cognitive and content domains is very informative.
In the cognitive domains, the evidence shows us that girls outperform boys in both knowing
and reasoning by a large margin. Since these were the two areas that Saudi
students performed at or near their own national average, we know from these results that it
is the girls and not the boys who kept the overall average steady.
Without the girls' high performance in knowing and reasoning, then the Saudi average would
have dropped significantly below in both areas.
But even more interesting is that boys and girls both performed far below the national
average in applying. And there was no significant difference between boys and girls in that
domain of applying. This further highlights the fact that applying
is the weakest cognitive domain element of the Saudi curriculum, teaching, and learning,
and it needs to be addressed. We find something similar when we look at
Saudi Arabia's content domain scores by gender. Girls significantly outperform boys in every
content domain, but both boys and girls perform far below their overall national average in
geometry. So now that we know that the Saudi students
need to improve in geometry and applying math, we have to know a little more about what each
of these domains means. Well, quite simply, according to the organization
that administers the TIMS, the applying domain involves the application of mathematical tools
in a range of contexts. Each of these problems is expected to be sufficiently familiar to
students that they will just involve selecting and applying learned facts, concepts, and
procedures. The content domain of geometry is a bit more
specific. In this domain, eighth grade students should be able to analyze the properties and
characteristics of a variety of two and three-dimensional geometric figures, including links of sides
and sizes of angles, and to provide explanations based on geometric relationships. They should
be able to apply the Pythagorean theorem to solve problems, and the focus should be on
using geometric properties in the relationships. This is the kind of evidence that can be used
to develop new policies and be implemented as actual practical activities or changes
in how both teachers and students learn and implement both geometry and math applications.
So to quickly summarize, it's important that we look at internal comparisons. Comparisons
within countries across cognitive and content domains or across peer groups and other groups
within the country. It's important that we look for internal variation first as a way
to identify strengths and weaknesses. And we need to highlight the fact that student
performance is related to both what they know and how they use that information. Remember,
context is key. It's key to understanding internal variation, and it's also key to making
reasonable comparisons across educational systems.
