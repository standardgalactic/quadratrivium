I'm going to start this lecture on the personal computer, not with Steve Jobs or Bill Gates.
Both of whom will turn up later with an entrepreneur from an entirely different field, Howard Head.
Head was an aeronautical engineer who revolutionized downhill skiing by replacing wooden skis in the 1950s
with the modern laminated skis that are used today.
In 1969, Head sold the Head Ski Company to AMF and he retired and took up tennis as a new hobby.
Head was not very good at tennis and he soon realized that either he was going to have to practice a lot more
or the equipment for the game was going to have to get a lot better.
Head decided that he needed a better tennis racket and he signed engineers at Prince Manufacturing,
a company that you just purchased, to design for him a new racket.
The engineers did so by increasing the sweet spot on the racket for Head and soon he was winning games.
This point, Head turned to his engineers and more or less said,
OK, this racket works great for me, now let's optimize it so it works well for lots of people.
Head's engineers went to work and did so through a number of prototypes with the result that the Head Tennis Racket became the standard for the game.
My point in starting with this anecdote is that computers followed a similar trajectory.
Just as the first enlarged sweet spot rackets worked only for Head,
so computers only worked for their inventors and other cognizanty.
But just as Head's engineers worked through multiple iterations,
so computer inventors and entrepreneurs worked to make computers accessible and useful for larger and larger numbers of people.
Put another way, the most important and interesting story about computers is how they became personal.
And this we'll see is as much about software as it is about hardware.
To tell this interesting story, we need to start with an early notion of a computer,
namely a machine for performing mathematical calculations,
and then follow how it became accessible and useful to greater numbers of users.
The first step was that scientists and engineers discovered ways in which machines could process information.
For thousands of years, people have organized and processed information using lots of familiar tools,
pen and paper, account books, and devices such as an abacus.
During the 17th century, European mathematicians and philosophers began to wonder
if machines could be made to do mathematical calculations.
In 1642, Blaise Pascal devised the first adding machine that used a series of toothed wheels.
His machine was improved upon by the German mathematician Gottfried Leibniz,
who figured out how to make the machine multiply.
Along with calculus, by the way, Leibniz also invented a binary number system
which could express any number as a series of ones and zeros.
In the early 19th century, French mathematicians used Leibniz's ideas to develop a new set of logarithmic tables.
But the French made so many mistakes, they stirred an English scientist, Charles Babbage, into action.
To develop better tables, Babbage decided to build a mechanical machine
for calculating and performing complex calculations.
In the 1820s to the 1870s, Babbage designed first a difference engine and then an analytical engine.
Using thousands of gears and powered by steam, the analytical engine had all the basic components of a modern computer.
It had a memory here composed of giant stacks of cogs Babbage used a decimal system to save on the number of stacks he'd need.
It also had a main processing unit which could perform addition, subtraction, multiplication and division.
The analytical engine included an output drum printer and a plotter.
It even had audio output.
A bell would ring if there was some problem or if the machine required attendance.
To feed information into his analytical engine, Babbage used punch cards, an idea that he borrowed from another inventor,
a Frenchman named Joseph-Marie Jacquard.
Jacquard had used punch cards to control a loom which automatically wove complicated patterns into textiles.
Studying Babbage's machines, the English mathematician Augusta Ada Bryan, the Countess of Lovelace,
suggested that punch cards could be not only used to feed information,
but they could also be used to provide the machine with step-by-step instructions.
Thus, the Countess of Lovelace introduced the idea of computer programming.
While Babbage was focused on how his engine could produce numerical results, Lovelace was a visionary
and she saw that the machine could do more than just mathematics
because the numbers could represent other things, letters, symbols, or in her favorite example, musical notes.
In the meantime, another important leap came from mathematicians and American businessmen in the late 19th century.
As we discussed in several lectures on the Industrial Revolution,
companies got ahead by pursuing economies of scale and speed, doing things bigger, doing things faster.
But to determine whether they were actually achieving those economies,
managers needed to look at large amounts of data concerning the costs of the raw materials and the labor that they were using.
To help manage that data and use it for calculations, Herman Hollereth founded the Computer Tabulating Recording Company.
Hollereth started this company to manufacture and market a punch card tabulator that he had developed
so the U.S. Census Bureau could speed up its analysis of data collected in the 1890 census.
In 1924, Hollereth's company merged with several other firms to create the International Business Machines Corporation, or IBM.
IBM did a big brisk business producing a range of machines for collecting and processing business data.
As we noted in passing in lecture 30, IBM encouraged Howard Akin, a researcher at Harvard,
to experiment in the 1930s with a large-scale calculator, the Mark I, that relied on electromagnetic relays.
With the outbreak of World War II, scientists and engineers in Britain, Germany, and the United States
investigated building large electronic calculators to crack the German code called Enigma,
Alan Turing and a team of mathematicians in Britain built in 1943 an electronic machine called Colossus with 1,500 vacuum tubes.
Other early efforts were prompted by the fact that many of the weapons developed during the war, the atomic bomb, the V-2 rocket,
and jet fighters all required numerous complex mathematical calculations.
Prior to the war, these calculations were made using desktop adding machines which were operated by women workers,
and those workers were called computers.
As the new electronic machines came in and replaced the women, so the new machines were dubbed computers.
In particular, the U.S. Army wanted to develop more reliable ballistic tables, and they needed those to guide the firing of artillery.
Rather than employing dozens of women to do the calculations, the Army asked two electrical engineering professors at the University of Pennsylvania,
John Mockley and Jay Presper Eckert, if they could develop an electronic calculator using vacuum tubes to crunch the numbers.
Mockley and Eckert built ENIAC, that, as we've seen, used over 17,000 vacuum tubes.
When ENIAC was announced in 1946, it was heralded in the press as a great brain, and it had a speed of 1,000 times faster than other electromechanical machines.
In order for ENIAC to perform all of these calculations, though, operators had to use a series of patch cords and a switchboard to connect different parts of the machine.
It's been said that ENIAC was not so much programmed as configured.
In other words, ENIAC really only made sense to its designers and the team of operators who worked with it.
Like the first racket that helped head wind games, so ENIAC was limited to its ability to solve problems only for its designers.
Not impressed with that complicated cable and switchboard, John von Neumann, a mathematician from Princeton,
proposed that the instructions, known as the program, be stored in the computer's memory.
To further speed things up, von Neumann also proposed borrowing an old idea from Leibniz.
Rather than store and process numbers using 10 digits, 0 through 9,
von Neumann suggested that computers would run a lot faster by expressing numbers only using two digits, 0 and 1.
As we'll see in the lecture on the Internet, this paralleled Claude Shannon's information theory
that all information can be expressed as a series of bits, either on or off.
Walklian Eckert pursued the idea of a stored program in several additional computers,
both EDVAC that they built at Penn and UNIVAC, a commercial machine that they produced at their own short-lived company.
UNIVAC was soon purchased by Remington Rand.
Several other stored machine programs were also built and tried at this time at universities such as the University of Manchester in England and Iowa State.
As promising as these early machines were, however, they still only made sense to a small number of computer scientists who worked closely with them.
These were special purpose machines, not general computers, that could be used to solve a variety of problems for a variety of people.
Moreover, these computers didn't always get reliable results, even when they were just performing calculations.
Because the trouble could be almost anywhere in the program or in the machine itself, debugging was incredibly difficult.
And logbooks from von Neumann's computer project at Princeton conveyed just how frustrating this was.
There, one of his colleagues commented that when confronted by inconsistent answers from the computer, he wrote in the notebook,
I have now duplicated both results. How will I know which one is right, assuming one result is correct?
Part of all this was a function of unreliable components.
Memory was stored on the Princeton machine in cathode ray tubes that had to be repeatedly refocused, and the machine also used unreliable vacuum tubes.
However, even if these issues could be resolved for computing to be viable outside the laboratory, the machine's operation would need to be significantly simpler.
What was needed was a way to make sure that machines were performing reliably and not require every user to have to get involved with the deepest parts of the machine in order to be sure that it was doing its job.
And the person who wrestled most effectively with these issues was Grace Hopper.
Hopper studied mathematics at Basser and Yale, and during World War II, she was commissioned as an officer in the Navy, and the Navy assigned her to work with Aiken on the Mark I up at Harvard.
After the war, she became the senior mathematician at the Eckert-Maukley Computer Corporation.
An ambitious pioneer in software development, Hopper may be best remembered for coining the phrase,
sometimes it's easier to ask for forgiveness than get permission.
Working with Maukley and Eckert on their machines, Hopper came to understand the need to distinguish between machine function and data processing.
Hopper knew that in order for the limited memory machines of the day to run efficiently, instructions originally composed in a high-level programming language
needed to be rewritten into a compact, low-level machine code.
The process for doing this was inefficient and prone to error.
As a solution in the early 1950s, Hopper developed the first computer compiler that converted source code from the programming language into binary machine code.
Most people thought this was an impossible task, and the Navy refused to authorize the project.
Nevertheless, Hopper went ahead and did it anyhow, and hence her signature phrase.
And even after that, many refused to use it even after the compiler was proven.
Most importantly, by simplifying programming, Hopper made computers much more efficient and much more flexible.
Huge steps away from the machines that only made sense computer scientists and steps toward computers where hardware would be generalized and software specialized.
But more than that, Hopper helped develop the programming languages Coball and Fortrand, which may generalize computing viable.
And later in life, Hopper lobbied the Defense Department to adopt standards for hardware and software, standards designed to foster distributing computing,
access to central depositories, the very model of today's computing.
The first commercial computer to truly exploit Hopper's vision of a general-use machine, which could run software, was the IBM System 360, released in 1964.
The 360 designation was intended to imply that the machine could be utilized to perform a full array of functions covering 360 degrees of the business universe.
The concept and the product were so successful that IBM sold more than a thousand machines in the first 30 days was on the market.
While the success of the IBM 360 was based primarily on the desire of corporations to impose centralized control over their operations,
the machine had a larger impact of establishing the notion that computing should be a routine part of business life.
In terms of the head tennis racket metaphor, the System 360 was like getting to the point where head rackets were being put into the hands of professional tennis players.
The question now became, how could computers get into, say, the hands of college and high school tennis players?
In building the System 360, IBM first used ceramic modules that included several discrete electronic components, and it then moved to using integrated circuits.
But integrated circuits, particularly in the form of Intel's microprocessors, set the stage for a significant departure from large-scale computers and towards personal computers.
When Intel introduced its first computers on a chip, it was concentrating on developing a product for industrial customers, and Intel really didn't give any thought to how people might build their own computers.
The assumption was that computers were something used by large organizations, companies, government agencies, universities, computers were not used by individuals.
But once it was possible to put an entire computer on a chip, several innovators in the early 70s saw an opportunity to make a personal computer.
For some early developers, the personal computer was a radical counter-cultural departure to the mainframe computers controlled by big business and the government.
The personal computer was seen as a machine that would give everyone access to the power of computing.
Notably, this power-to-the-people notion still permeates Silicon Valley's rhetoric, even as the pioneering companies in that region come to be corporate giants themselves.
The first PCs were fashioned by electronics enthusiasts who freely shared their plans with each other in hobby magazines and through clubs such as the Homebrew Computer Club in Northern California.
Several hobbyists went into business manufacturing computer kits, and Bill Gates of Microsoft got his start by writing programs for an early computer kit, the Altair 8800.
Building on what they learned through the Homebrew Club and by consulting with engineers at Intel, Steve Wozniak and Steve Jobs introduced the Apple personal computer in 1976.
To raise money to build that first computer, Jobs sold his Volkswagen Microbus and he and Woz worked in his parents' garage.
The computer they came up with offered expanded memory, data storage, a keyboard and color graphics, all for $1,290.
Envious of Apple's popularity, IBM entered the personal computer market in 1981 with a machine that ran on a standard disk operating system that had been developed by Microsoft.
It was called MS-DOS. MS-DOS was critical because it now meant that data and software could be exchanged easily between individuals and their machines.
Up to this time, each personal computer, Apple, Altair, Radio Shack or Compact, ran on its own proprietary operating system.
The IBM PC was introduced in 1981 with a price point below $1,500 and within two years, IBM was selling their millionth machine.
IBM shrewdly shared the details of the machine's new architecture with software developers who then proceeded to develop thousands of programs that could be run on IBM machines.
Doing so resulted in the development of early killer apps, apps that included spreadsheet applications like Viscical, Lotus123 or Excel, as well as word processing applications such as Microsoft Word or WordStar.
By the end of the 1980s, IBM Compatible Machines and Microsoft operating systems had come to dominate the PC market.
Now the availability of a wide range of software applications for the IBM PC meant that users for the first time could configure their computers to suit their own needs.
The computer was getting a little bit closer to being something that everyone could use.
In terms of the tennis racket analogy, IBM put the racket within reach of lots of people since it figured out how to manufacture huge quantities of this new device.
As IBM's share of the personal computer market grew in the early 80s, Steve Jobs at Apple did not stand idly by but began casting about for new ideas to stay competitive.
Jobs found an answer in the graphic user interface, GUI, and that in my opinion is the most important piece of the story.
Up to this time, all personal computers still had to be programmed, meaning that you had to enter line commands and to make them work, it helped if you could do a bit of your own programming, typically in a language called basic.
I remember well being intimidated by the programming manual that came with my first personal computer, the TRS-80 Model 3.
I really wanted the Model 3 to work, but was I going to have to write programs?
It was if you could only have that fancy head tennis racket provided you were willing to string it yourself.
As the name suggests, GUI allows you to interact with your computer using images, graphics, rather than text commands.
GUIs are now used everywhere in computers, on cell phones, mp3 players, gaming devices, household appliances, and industrial equipment, and so it's probably hard to imagine that they once didn't exist.
Graphic user interface wasn't invented by Apple, but rather came from another company in Silicon Valley, Xerox Park, and to tell this story you need to go back in time a little.
Drawing on the invention of photocopying in the 1930s by Chester Carlson, I'm sorry, I'm from a different branch of the Carlson family, Xerox had made a fortune with its copying machines in the 1960s.
In 1970, the company opened its Palo Alto Research Center, or PARC, your Stanford University, and staffed it with some of the company's brightest minds.
Charged with investigating a range of technologies, PARC invited a number of visiting scientists from Stanford's Artificial Intelligence Laboratory, including Jeff Raskin.
Raskin had worked on a graphic user interface as a graduate student, and he found fertile ground to pursue his ideas at PARC.
Within three years, Raskin had a GUI up and running on a new prototype called the Alto, and the Alto also included another interesting new device, a mouse, which was an input device developed a few years earlier by another Stanford researcher, Doug Engelbart.
Although an interesting machine, Xerox was not inclined to convert the Alto into a commercial product since the company had lost money when it tried to enter the computer market a few years earlier.
As a result, Raskin left PARC and joined Apple as one of its first employees.
Over the course of the next three or four years, Apple struggled to develop the Lisa, another computer, which was named after Job's girlfriend at the time.
All through this period, Raskin lobbied for the next Apple product to be an Alto Plus and to have a graphic user interface, and eventually, Jobs agreed to go to PARC in 1980 to see for himself what PARC had come up with.
There, PARC engineers showed Jobs an interface that consisted of graphical elements such as windows, menus, radio buttons, and icons, all items that we take for granted on our computer screens.
Anxious to get a closer look at the GUI, Jobs got so close that one Xerox engineer running the computer recalled that he could feel Jobs' breath on the back of his neck.
Jobs became incredibly excited by what he saw and he started jumping up and down.
He did so because he realized that the GUI represented the way to make computers that everyone could use.
Jobs raced back to Apple and charged his team with developing a new low-cost computer, the Macintosh, with a GUI and a mouse, and it was introduced in 1984.
The Macintosh was the first machine in which all the user had to do was to point the mouse at an icon and click on it.
At this time, IBM PC users were still having to type commands.
In response, IBM countered by having Microsoft introduce its own graphic user interface, a system that was called Windows in 1988.
Thanks to GUI, computers were finally personal. They could be used by business people and housewives, young people, and old people.
The computer was now the equivalent of Head's Racket, optimized for use by a huge audience.
But to reach that huge audience, Jobs and Apple had to overcome that enormously established by IBM with its PC.
And to do so, they used an unforgettable TV commercial that invested their product, the Macintosh, with a whole new set of meanings.
There during the 1984 Super Bowl game, the Apple commercial opens in an dreary industrial setting in blue and gray tones,
showing a long line of people trudging in unison through a tunnel monitored by a string of telescreens.
This is in sharp contrast to the full-color shots of a female runner wearing an athletic uniform,
bright orange shorts, running shoes, and a white tank top with a cubist-inch image of a Macintosh computer on her chest.
She's carrying a large brass sledgehammer, and as the runner is chased through the tunnel by police officers, presumably the Thought Police,
she races towards a large screen with an image of Big Brother.
And the Big Brother figure is giving a speech about how information and purification directives are improving the thoughts of people.
Now close to the screen, the runner hurls the hammer towards it, just as Big Brother concludes, we shall prevail.
In a flash of light and smoke, the screen is shattered, shocking the people watching the screen.
The commercial concludes with a portentous voiceover.
On January 24th, Apple Computer will introduce Macintosh, and you'll see why 1984 won't be like 1984.
There, they're meaning, of course, George Orwell's novel.
For this commercial, Apple framed the Macintosh not just as another electronic gizmo,
but rather as the device that would set you free from IBM's corporate enslavement.
To complete the invention of the personal computer, jobs in Apple not only had to get the hardware and the software right,
they also had to get their creation tied to powerful meanings, meanings about power, authority, and freedom.
Again, we see how invention is about both the technical and the social.
Since the 1980s, we may be free to do our own thing with computers, but we have come highly dependent on them.
Personal computers continue to evolve at breakneck speed, with their users connecting their machines to the internet
to send emails, surf the web, to run complex software, and play games with elaborate graphics.
In the 90s, laptops became commonplace, and people became accustomed to taking a computer with them everywhere,
supplemented by smartphones, and then most recently by tablet computers.
Much of the time, we're caught with thinking about this new hardware, and we're excited about questions such as,
is it faster? Does it have more memory? Does it have more bells and whistles?
Now, while we're attempting to think that it's the hardware, the chips, the memory storage, the peripherals,
that shaped the history of the personal computer, I would argue that it's been the evolution of programming that's at the heart of the story.
Through programming and software, especially the graphic user interface,
it became possible to convert a calculating machine into a device that most of us can't live without.
To borrow the title from Tracy Kitter's fine book about early computers, software was,
and will continue to be, The Soul of the New Machine.
