What do you do when you have annoyed the most powerful man on Wall Street?
This question was very much on the mind of Nikola Tesla in January 1902.
We first met Tesla in lecture 20 when he invented the alternating current motor and again in
lecture 27 when Tesla was competing with Marconi by using radio waves to broadcast power through
the earth.
Challenge facing Tesla in 1902 was that although Morgan had given Tesla $150,000 to build a
station in order to send power and messages across the Atlantic, Marconi had beaten Tesla
to the punch.
In December of 1901, Marconi announced that the Morse code signal for the letter S had
been transmitted from Cornwall, England and received at St. John's, Newfoundland.
Marconi, not Tesla, was the new Wunderkin of radio.
So what did Tesla tell his patron Morgan?
Along with complaining of course that Marconi had stolen his circuit designs, Tesla proposed
to Morgan in 1902 a plan for a world telegraphy system in which a number of transmitting
stations would collect news and broadcast them to customers via individual receivers.
As Tesla boasted to Morgan, the fundamental idea underlying this system is to employ a
few power plants, preferably located near large centers of civilization and each capable
of transmitting a message to the remotest regions of the globe.
These plants are to be connected by wires, cables and any other means with the civilized
centers nearby and as fast as they receive the news they pour them into the ground through
which they spread instantly.
The whole earth is like a brain as it were and the capacity of this system is infinite
for the energy received on every few square feet of ground is sufficient to operate an
instrument and the number of devices which can be so actuated is for all practical purposes
infinite.
You see Mr. Morgan, the revolutionary character of this idea, its civilizing potency, its
tremendous money making power.
Tesla believed that he and Morgan would make money by manufacturing receivers and he envisioned
several versions.
By far Tesla's most imaginative idea for a receiver was a handheld device connected
to a vertical wire in a short pole or even a ladies parasol so it could pick up voice
messages anywhere in the world.
As Tesla promised in 1904, an inexpensive receiver, not bigger than a watch, will enable anyone
to listen anywhere on land or sea to a speech delivered or music played in some other place,
however distant.
Here in the opening years of the 20th century we see Tesla conjuring up a vision of a device
much like a transistor radio or a cell phone with the promise of providing instantaneous
information anytime, anywhere.
Although Tesla certainly was not thinking about the computers, the software, the packet
switching that were necessary to create the World Wide Web, Tesla's fundamental idea
that all news should be collected and disseminated around the world is very much what the internet
and the World Wide Web came to be in the 1990s.
The World Wide Web, noted media scholars Noah Wardrip-Froon and Nick Montfort, was developed
to be a pool of human knowledge which would allow collaborators in remote sites to share
their ideas in all aspects of a common project.
So how did the idea of the internet and the World Wide Web go from being a fanciful concept
put forward by Tesla in 1902 to being an incredibly important invention that it was a hundred
years later?
How has the Web come to have, as Tesla predicted, a revolutionary character, civilizing potency
and tremendous money making power?
As we'll see, the journey from Tesla's world telegraphy system to the Web takes us from
Bell Labs to DARPA to computer science departments at universities.
And it's a journey that involves some of the most powerful inventions of the 20th century,
the idea of digital information and packet switching.
So let's get started.
You may remember that back in lecture 19 on the camera, telephone and phonograph, we
talked about how these inventions were examples of analog communications.
In analog communications, information is stored or transported from one place to another by
a representation that serves as an analog of the message.
In the case of the telephone, the sound waves of the voice are converted into electric current
waves, the analog, that can be sent over a wire.
In the same manner, the camera records a scene, that's the message, and it does so via a negative
image on film, that's the analog, and that can be reproduced as a photograph.
And in the case of the phonograph, sound waves from the voice or music are stored by duplicating
them in the ups and downs of the wave in hills and valleys in the groove of the record.
Equally, radio and television work by converting sound images into radio waves that are sent
over the air from the transmitter to the receiver.
Tesla too thought in analog terms.
He anticipated that he would send messages through the earth by assigning a unique frequency
to each message.
Of course, this meant that he would soon use up all the frequencies in one portion of the
electromagnetic spectrum and would have to push out to higher or lower frequencies in
order to send all his messages.
These limits were also something that engineers at AT&T, RCA and other companies worried about
in the middle decades of the 20th century.
And to deal with the exploding volume of messages, telephone calls, telegrams, TV programs in
the news, these engineers experimented with both parsing the spectrum into smaller and
smaller channels, as well as expanding the spectrum by transmitting at higher and lower
frequencies.
But the means to overcome the fact that the electromagnetic spectrum is a finite resource
came not from an engineer, but rather a mathematician at Bell Labs, a man by the name of Claude
Shannon.
During World War II, the U.S. government asked Bell Labs to come up with a way to encrypt
telephone conversations between the United States and Britain that the Germans could
not intercept and decode.
To solve this problem, Bell engineers came up with what they called pulse code modulation
or PCM.
Instead of sending a waveform down the wire, the engineers instead took thousands of samples
per second of the value that the wave had, converted those into on-off pulses, and then
sent the string of on-off pulses down the wire.
Done properly, the encoding and decoding could happen so quickly that the users on either
end of the telephone conversation would never notice any processing delay.
While Bell Lab engineers got busy after the war converting PCM into a means for improving
transmission quality, Shannon saw PCM as a way for him to rethink the nature of communications.
In July 1948, the same month that Bell Labs announced the transistor, Shannon published
a long mathematical paper in the Bell System Technical Journal, a paper that introduced
the modern theory of information.
Now up to that point, communications engineers had focused on the form that messages took,
that a telegram was different than a telephone conversation, and that was different than
the TV program.
Shannon instead argued that engineers should think not about the form, but rather the information
embodied in a message.
Moreover, Shannon suggested that information could be measured in terms of bits, and that
bits could be expressed in numbers, that is, digitally.
The bits could be expressed in dots and dashes, as on-off pulses in PCM, or even in binary
code, as ones and zeros.
As a string of ones and zeros, it became incredibly easy to speed up sending messages over a wire
or across the airwaves.
But Shannon went further and suggested that you could also compress messages by leaving
out a portion of the message, provided that both the transmitter and the receiver knew
the rules by which portions were being left out.
That is to say, both sides had a digital key.
For those of us living in the 21st century, Shannon's notion that communications is information
may seem obvious.
But when he first proposed it in 1948, his colleagues found it startling, truly as one
of his associates recalled, a bolt out of the blue.
In information theory, particularly the idea that all messages can be converted into a
string of ones and zeros underlies the further development of computers, cell phones, and
as we'll see here, the Internet.
In describing Shannon's achievement, one of his colleagues at Bell Lab, Bob Lucky, wrote
later, I know of no greater work of genius in the annals of technological thought.
As Shannon pointed out, information can take any number of digital forms.
But engineers soon settled on using binary code, ones and zeros.
Either something was on, a one, or it was off, a zero.
Binary code was particularly useful because it allowed computer engineers to build more
sophisticated computers since they could simply be huge aggregations of switches, on or off.
But because nearly all electronic computers operated on some level of binary code, it
opened the possibility of getting computers to talk to one another.
Once again, just as we saw in lecture 26 on aviation, it took specific circumstances
to convert this possibility into a reality.
The circumstances here were the fact that there were too many big computers and not enough
research going on.
In the 1960s, many universities and government agencies had purchased large mainframe computers,
like IBM's All-Transistor System 360.
They soon discovered that they did not have enough work at hand to keep this big machine
busy and hence warrant the initial cost of the computer.
In response, some universities developed time-sharing arrangements, whereby several institutions
shared the capacity and the cost of a mainframe.
To do so, researchers had to develop a file transfer protocol, or FTP, whereby they could
send programs and data over telephone lines from their home institution to the computer
at another location.
These procedures, known as packet switching, consisted of breaking the files down into
manageable digital chunks, that's the packet, and assigning an address to each packet.
The packets could then be sent individually over the telephone line and reassemble at
the receiving computer.
packet switching has an important advantage in that it doesn't have to tie up an entire
information channel for a long time to send a big file.
The computers can send a few packets at a time, some over one channel, some over another,
and thus optimize the overall use of the information network.
Now almost as an afterthought, the designer of these time-sharing systems added a feature
that permitted users to send text messages to confirm the receipt of electronic files
or to send instructions to the people operating the mainframe.
For example, two graduate students, Tom Van Vleck and Noel Morris, added an email feature
to MIT's time-sharing system in 1965.
However, computer researchers soon discovered that this afterthought was a real convenience
and they began to use it on a regular basis to communicate with one another.
Hence this afterthought evolved into email, and the first program for sending messages
between computers on a network was put together by Ray Tomlinson in 1972.
At the same time that universities were buying mainframe computers and setting up time-sharing
schemes, the U.S. military invested heavily in computer research at many American universities
via the Defense Advanced Research Projects Agency, or DARPA.
Anxious to encourage researchers to share their work and to make efficient use of the
equipment they had purchased for their universities, DARPA took the next step of linking these
computers into a formal network known as ARPANET in 1969.
Other government agencies, groups of universities, and associations saw the advantages of ARPANET
and created their own computer networks in the 1970s.
To link all these networks together, two computer scientists, Vinton Surf and Bob Kahn, formulated
a cross-network protocol that allowed packets to move from network to network.
This internet-working of networks came to be known in computer lingo as the internet.
By 1981, 213 mainframe computers were connected to the internet.
By 1990, there were 800 computer networks linked to the internet with about 160,000 computers
attached to those networks.
20 years later, by 2010, there were an estimated 5 billion devices connected to the internet,
which included desktop computers, laptops, cell phones, tablets, e-book readers, internet
TVs, and digital cameras.
In the 1980s, the internet was used primarily by university researchers and computer specialists.
But in the 1990s, several developments permitted ordinary people to start using it.
First, there was a tremendous increase in the capacity of the telephone network over
which internet traffic moved.
For decades, AT&T had been increasing capacity of its telephone network by introducing new
technologies such as coaxial cable and microwave transmission.
Anticipating that people would want to send not only voice messages, but also data and
images over telephone lines, AT&T had further expanded the capacity of its network and introduced
the picture phone in the late 60s and early 1970s.
Now while consumers rejected picture phones for being too expensive, and consumers were
also worried about having to answer the picture phone in their underwear, the project left
AT&T with capacity that could be used to carry the growing internet traffic.
In the 1980s, the capacity of telecommunication networks was further enhanced by the introduction
of fiber optics.
By using light waves to carry signals down a hair-like glass fiber, telecommunications
companies could greatly increase the speed and the capacity of their networks.
Using cable developed by Corning Incorporated, MCI started building its first fiber optic
network in 1982.
A third important development was that although the US government had funded the development
of the internet to support scientific research, the government decided in 1995 to privatize
management and the operation of the internet.
This decision allowed companies to develop a variety of internet-related services and
software as well, which in turn permitted millions of individuals and companies to start
using the internet for email and other activities.
Companies estimate that there were 61 million internet users worldwide at the end of 1996,
148 million by the end of 1998, and over 600 million by the end of 2002.
In 2011, it was estimated 35% of the world's population.
Along with email, another important development was the creation of the means by which people
could locate and access information from computers anywhere in the network.
To do this, several software engineers, but most notably Tim Berners-Lee at the Center
for Particle Research CERN in Geneva, Switzerland, developed what came to be known as the World
Wide Web.
To create the web, Berners-Lee and others invented three things.
First they came up with a language, which is called Hypertext Markup Language, or HTML,
and that's used for assembling text, pictures, and files into a web page.
Next, they produced a system of codes that gives an address to every website and every
page in the trade that's known as a universal resource locator or a URL.
And finally, they produced a protocol, HTTP, which stands for the Hypertext Transfer Protocol,
for moving web files from their source, the server that they're on, to the user's machine.
Combining words, pictures, sounds, and even video into a convenient and flexible package,
the web has proven to be attractive to numerous individuals and businesses.
Web access was improved by means of browsers, developed first at the University of Illinois
by Mark Andreessen, and his product was known as Mosaic.
Andreessen subsequently commercialized Mosaic by launching Netscape, and now millions of
people use that browser in order to surf the web.
While there were 150 websites in 1993, by the end of 1997, there were 24.5 million,
and in December 2012, there were at least 8.5 billion web pages.
With millions and billions of possible places to look for a specific piece of information,
it became apparent to several entrepreneurs that people needed some way to sort through
and find what they wanted on the World Wide Web.
The first tool for searching on the Internet appeared in 1990 called Archie.
They came up with the name Archie, not from the comic strip, but taking the letter V out
of archive.
Archie was created by three computer science students at McGill University in Canada, Allen
Emitage, Bill Healan, and J. Peter Deusch.
Archie downloaded the directory listings of all the files located on public anonymous
FTP sites and then generated a searchable database of file names.
Rather than work from what was available on public lists, though, computer scientists
next invented web crawlers, sometimes known as spiders, that browse the entire web in
a methodical fashion.
Matthew Gray at MIT built one of the first web crawlers in 1993.
Based on web crawlers, several search engines appeared in the 1990s, including Lycos, Excite,
and Yahoo.
But the most successful of these web startups has been Google, created by two Stanford computer
science PhD students, Larry Page and Sergey Brin.
While Yahoo and its rivals used web crawlers to collect data about websites, they were
still relying on humans to index the data, and it was this data that was used to answer
search inquiries from users.
As graduate students, Page and Brin realized that they could significantly improve the
search process by taking several steps.
First, they decided to use web crawlers to collect and store all or part of as many-wage
web pages as possible, as well as information about who was viewing those web pages.
Next, they made an assumption that the most useful or reliable pages are those that are
viewed the most often and linked to other pages.
Based on this assumption, they created an iterative algorithm that generates a page
rank, and page ranks are used to organize the listings of sites that a user sees when
they do a search on Google.
As more and more people use the web, and move from website to website, Google Counts
sees hits, feeds them back into the algorithm, and updates the page ranks.
Page and Brin came up with the basic idea behind their algorithm in 1996 and launched
a search engine under the name of Backrub.
Fortunately, Page had the insight that they needed a better name, and he suggested Google.
According to Page, it was fun, it was short, it was reasonably easy to spell.
The full search engine ran first on several servers crammed into Page's dorm room, and
there were so many computers in his room that he had to keep fans running all the time to
keep the servers cool.
To expand server space, Page and Brin took to hanging out on the loading dock of the
CS department, watching for deliveries of new equipment.
When equipment was not picked up immediately and moved to a researcher's office, Page and
Brin would borrow the computer until the researcher realized that he and she had not gotten everything
they had ordered.
Neither Page nor Brin were especially keen on launching a business, as they intended
to get their Ph.D. in computer science and become researchers.
However, when Andy Bektoshim, a founder of Sun Microsystems and a Stanford alumnus, gave
them a check for $100,000 made out to Google Incorporated, they figured that they ought
to go on ahead and create a new company.
In 1998, Page and Brin raised an additional million dollars from Silicon Valley venture
capitalists and they launched Google, working initially out of the garage and house they
rented from Susan Wozzycki, who later became Google's director of product development.
By 2000, Google had indexed a billion web pages, making it the largest search engine
on the web.
At the end of 2010, Google had 84% of the market share in the search engine business.
Since 2000, Google has added a number of features.
You can download maps and driving directions, search books from the libraries of Harvard
or Oxford, and watch videos via YouTube, and all of these features are free.
Since these features cost money to design and maintain, you might reasonably ask a couple
of questions.
How does Google make money?
How is free profitable for them?
The answer is advertising, but here too, Google has been an innovator.
While other search engines made money by running banner ads, which many users found annoying,
and by listing advertisers higher up in the search results, Google has pursued two different
revenue streams.
Page and Brin realized that if people were going to enter a term in the search box, they
wouldn't be doing that by accident.
They might well be looking for a product to buy, hence they would be open to reading
ads for items specifically related to the search they were conducting.
This led Google to create first ad words.
When you conduct a search, you not only get the results from the search engine, you also
get a selection of ads.
Companies arrange with Google to place those ads, and they only pay Google when the ad
is shown to a user.
A typical agreement along these lines allows several views of a company's ad for less than
a penny.
Moreover, because some words or products are especially popular, Google just doesn't let
anyone sign up for those spaces, but rather auctions them off to the highest bidder, and
that generates even more income for Google.
And if that's not enough, Google also has come up with AdSense.
Suppose you teach yoga classes and have a website to attract customers.
You can then sign up with Google who will pay you to run ads for maths and yoga related
gear on your website.
As the owner of the yoga site, you don't pay anything, but rather the manufacturers
of the yoga gear again pay Google each time their ad is dropped onto your site.
Google reports that it earned $36 billion in advertising revenue in 2011, of which 70%
came from AdWords and the rest from AdSense.
Of course, to make sure that we keep coming back and look at those ads, Google needs to
keep us attentive, and that's why they continue to introduce new free applications.
Thanks to Google, the Internet and the World Wide Web have lived up to two of the three
things that Tesla predicted in 1902 for its world delivery system.
Both the Internet and the Web are revolutionary, in a sense they have created the amount of
information with which people in the industrial world deal with every day, and these inventions
have radically altered how we work and play.
Google Amply reveals that the Web has tremendous money making power, and that leaves only civilizing
potency.
In an interesting term, we will look at this idea in our next lecture on social media and
revolution.
