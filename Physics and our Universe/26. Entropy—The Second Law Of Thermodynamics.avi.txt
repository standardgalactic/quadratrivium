Welcome to lecture 26, the first of two lectures dealing with a really big important idea in
physics.
That idea is the second law of thermodynamics.
Unlike the other laws of physics we've looked at that say what is going to happen, Newton's
law says you exert a force, you'll get an acceleration.
The second law is different.
The second law talks about what is likely to happen or more importantly about what is
unlikely to happen.
It's got a probabilistic side to it that none of the laws we've dealt with so far have.
It's crucially important in understanding the workings of the world not only in physics
but in biology, in chemistry, in geology, in evolution, in all kinds of things like that.
It's also crucial in understanding the technological world we live in and particularly technological
problems we face as we try to use ever more energy.
So I'm going to devote two whole lectures to the second law of thermodynamics.
I know of no other statement of how important the second law is than a beautiful quote from
C.P.
Snow from his book The Two Cultures from 1959.
C.P.
Snow was a British physicist, novelist, civil servant.
He was a baron and he was eventually knighted.
And he lived in the middle of the 20th century and he was concerned that the scientists and
the humanities people really weren't talking to each other enough and his two cultures
is about those two cultures and how different they are.
And here is what he has to say about the second law of thermodynamics.
And I take from this the title of the inaugural lecture I gave for my endowed chair at Middle
bury College like a work of Shakespeare.
So let me read you this quote from C.P.
Snow's The Two Cultures.
A good many times I have been present at gatherings of people who by the standards of the traditional
culture are thought highly educated and who have with considerable gusto been expressing
their incredulity at the illiteracy of scientists.
Once or twice I have been provoked and have asked the company how many of them could describe
the second law of thermodynamics.
The response was cold.
It was also negative.
Yet I was asking something which is about the scientific equivalent of have you read
a work of Shakespeare's.
Well, I feel a little bit like I have gone from the sublime to the ridiculous.
One minute I am reading an erudite 20th century quote from a British scholar and the next
minute I am on a cooking show.
Actually, I love to cook and there is an awful lot of good physics in cooking.
I have given a whole lecture on physics in the kitchen, for example.
And the second law of thermodynamics is very much with us in the kitchen because the second
law is about heat and it is also about organization.
And I want to give you an example of the second law.
So you are in the right place, we are doing physics here, we are doing it in the kitchen.
So I am going to take an egg and I am going to scramble it.
Egg, yolk and white.
There they are.
There are two parts of the egg but I want scrambled egg.
Okay, that looks good, I can throw it on the pan and cook it.
But I don't want to.
I want a fried egg instead or maybe a poached egg.
So I am going to unscramble it.
So I am going to take my whisk and I am going to reverse carefully every motion I did.
Keep doing that, exactly the right motions.
And eventually as I stir, the yolk should reassemble and the white should all get together
around it and I should have my egg ready to poach or fry but it isn't happening.
Maybe if I keep going a little longer it will happen.
Let me just keep stirring.
Surely there is a chance that eventually the egg white molecules will all end up surrounding
the egg yolk molecules and I will have my unbeaten egg again.
Well there is a chance of that but I could beat this thing for longer than the age of
the universe and it is very, very, very unlikely that that would ever happen.
That's what the second law of thermodynamics is about.
It's about processes that are so rare, so improbable that in our real world they simply
never happen and beating an egg is a beautiful example of that.
I simply cannot unbeat the egg.
That's a process that is completely irreversible.
You beat an egg, you can't reverse that process.
Let's get more scientific about that again now.
Okay let's look at some other examples of processes that are essentially irreversible
or in fact let's look at one process that's reversible and another that isn't.
So here I have a couple of movies playing simultaneously.
In the upper movie you see a ball bouncing, very simple thing.
Ball comes in, bounces off the floor, goes back up.
If I were to play that movie backwards it would make perfect sense.
That process is perfectly reversible.
The lower movie is showing a block of wood sliding along the floor, frictional forces
are dissipating its kinetic energy and turning it into quote heat actually as we know into
internal energy as the block eventually comes to a stop and I've stuck a thermometer in
the block and you see the thermometer rising as the block slows to a stop indicating that
we've turned the directed kinetic energy of the block into the random, still kinetic energy
but the random kinetic energy associated with the thermal motion of the molecules.
If I play that one backwards it makes no sense at all.
We never see a block sitting on the table at a relatively high temperature and somehow
all the random thermal motions of that block conspire all of a sudden to have a component
in the other in one direction and the block suddenly starts to move and its temperature
drops.
Now I want to emphasize in both these movies in principle the processes could be reversed.
There is nothing that violates any fundamental law of physics, there's nothing that violates
Newton's laws, there's nothing that violates energy conservation.
The ball is bouncing and its energy is conserved.
The block is sliding and its energy is conserved as we know now from the first law of thermodynamics
it's just been converted to the form of internal energy, the random energy of the molecular
motions making up that block.
And when I play that movie in reverse the amount of energy doesn't change.
The block has initially a lot of internal energy after it accelerates it's cooler so
it has less internal energy but now it's got some directed kinetic energy.
So there's no violation of conservation of energy but that second movie played in reverse
just makes no sense because that wouldn't happen.
Why wouldn't it happen?
Because it is so dramatically improbable that all those molecules would get together in
the same state and that's the essence of what the second law of thermodynamics talks about.
The second law talks about this tendency of systems to get into states that are more randomized
and more chaotic.
And why does that happen?
It happens because out of all the possible states that describe a physical system.
The states that represent order are much, much fewer.
Take my egg.
There are many ways to arrange the molecules of that egg so all the yolk molecules are in
one place and all the white molecules are around them, there's zillions of ways.
But that number, big as it is, is tiny, tiny, tiny compared with the number of ways to organize
those molecules or arrange those molecules when the egg is scrambled.
And so it's the sheer improbability as determined by the sheer number of organized states being
much smaller that is at the essence of the second law of thermodynamics.
Let me give you another practical example.
If you don't do anything much to your house but live in it, it gradually degenerates into
chaos.
Books end up on the floor, not shelved where they're supposed to be.
Dog hairs are all over the place instead of on the dog, dirty dishes are in the sink.
Once you take active steps to reorganize, to wash the dishes, to put them back in the
cabinets, the house degenerates into chaos.
Why?
Because there are simply more, many, many more chaotic states.
This bowl belongs in this place on the shelf.
That's where it should be if the house is organized.
But it could be here on the floor, it could be here in the sink, your kid could have left
it on the table over there, it could be lots of places and have a disorganized state.
There are many, many more states that are disorganized than there are states that are
organized.
So just given random happenings, take a system that starts out organized, it's likely to
end up in a disorganized state sooner or later.
The best it could do is to stay organized, but it's more likely if anything happens to
deteriorate into one of these chaotic states.
Organized states are rare compared to disorganized states.
Once the system gets into a disorganized state, there are many, many other disorganized states
it could transit into, but there are very few organized states, and so it's unlikely
to transit into a more organized state.
So this tendency toward chaos, this tendency toward disorder, is at the heart of the second
law of thermodynamics.
And there's a physics word for this tendency to chaos, and it's called entropy.
Entropy is a measure of this disorder.
Let's look at entropy.
It's a measure of disorder.
The more entropy you have, the more disorder, and in common language, the word entropy
is kind of prepped into everyday language, if people understand that it means disorder,
it means chaos, it means things are unorganized.
Disorder tends to increase over time.
Things tend to get more chaotic.
Egg got more chaotic.
As I beat it, it didn't get more organized.
The contents of my house get more disorganized, get more chaotic.
Entropy tends to increase.
And here's another important point about entropy that's a little less obvious.
An entropy increase in a system is accompanied by a decrease in the ability of that system
to do useful work.
This is not a statement about energy necessarily.
It's a statement about, or not about energy amounts.
It's a statement that says, I may have the same amount of energy after an entropy increase,
but I can't do useful work with it.
I can't do mechanical work, and we'll see much more about how this manifests itself
in our energy systems in the next lecture.
But that's essentially what energy is, and a little later on, we will actually look
quantitatively at how to talk about entropy.
Let's look at a particular situation where an increase in chaos, an increase in disorganization,
an increase in entropy causes us to lose the ability to do work.
So I'm going to imagine a situation in which I have a closed box.
It's insulated from its surroundings, so no heat can flow in and out.
So to use a word from the previous lecture, anything that happens in this box is an adiabatic
process.
And in the box, there is a partition, and on one side of the box, there's a gas, and
the gas has some volume and some pressure and some temperature, and the volume of the
gas is exactly half the volume of this box.
On the other side of the box is vacuum.
And I'm going to remove the partition and ask what happens.
And it's pretty obvious what happens in that case.
If I remove the partition, the gas spreads itself evenly throughout the entire box pretty
soon.
Gas molecules that are on the one side go rushing over to the other side, and eventually
we just have a random hodgepodge of molecules rushing all around.
Now the box has not expanded, so its volume hasn't changed.
So the gas has done no work.
It's still got the same amount of internal energy.
Its temperature is still the same.
And interestingly, by the way, this process is both isothermal.
The temperature hasn't changed.
And adiabatic, that may sound like a contradiction of the previous lecture's distinction between
adiabatic and isothermal processes.
But this process is a free expansion, and it is not a reversible process.
It's not something that can be characterized by a path in the PV diagram, because the minute
I take that partition away, chaos reigns.
And I don't have a well-defined temperature and a well-defined pressure or even a well-defined
volume of the gas until it's settled down into this new state.
So we have an adiabatic free expansion that is also isothermal.
Now as I say, the energy of the system has not changed.
Same internal energy, same temperature, we have not lost any energy.
But I claim we've lost the ability to do work.
Let's look at why.
Here's what I could have done.
I could have taken a little paddle wheel and put it on the vacuum side of that partition.
And instead of just removing the partition, I could have opened a little hole and I could
have let the gas stream out and turned the paddle wheel, and I could have connected the
paddle wheel to an electric generator or something else in the outside world that carried away
useful energy in the form of either mechanical work or electricity, both of which are very
high-quality forms of energy, as we'll see shortly.
I could have done that, and when I was all done, I would have had the gas, again, distributed
uniformly throughout the volume.
It wouldn't have been able to do that work anymore.
And now it would have less energy because I would have taken some of the energy away
as the gas did work turning that paddle wheel.
In either this state or in the original state where I didn't have the paddle wheel, once
I had taken the partition away and the gas had expanded and filled the whole compartment,
I can't extract energy from it, at least by this paddle wheel method.
That's what I mean when I say an increase in entropy is accompanied by a decrease in
the ability to do work, because in this system with all the gas on one side of the box and
the other side empty, that's a very organized state.
That's like the state of the egg with all the yolk molecules in one place and all the
white molecules surrounding it.
The state in which the gas is spread uniformly throughout the box is less organized, just
like the state in which the egg has been beaten is less organized.
Okay, well, that's a lot of talk about organization and words.
Let's get quantitative about this and understand why this occurs in a little more mathematically.
Let's go to our big monitor and let's look at this situation in a little more detail.
I'm going to take a statistical look at what entropy means.
Here's the question.
Once the box got into that state in which the gas was spread uniformly throughout it,
why doesn't it spontaneously go into that state?
Or why doesn't the air in this room suddenly all decide to have all of its molecules to
go to one half of the room and suddenly I'm breathing air and at higher pressure and the
people on the other side of the room are asphyxiating because they got no air?
Why doesn't that happen?
We don't worry about that happening, so it must not be possible.
Well it is possible.
It's just very unlikely.
So I want to ask the question, why doesn't this in fact happen when there is no rule
to prevent it?
Again, relevant rule, energy conservation is not violated if that event happened.
So I want to define two things.
I want to define what I call a microstate, a specific arrangement of individual molecules.
I give every molecule in that gas a name and a microstate is where is molecule A and where
is molecule B and where is molecule C and if I interchange molecule A and molecule C,
that's a different microstate.
It's a specific arrangement of individual molecules.
A macrostate is characterized by simply saying how many molecules are on each side of the
box.
It's an important distinction and we'll see how that distinction develops.
So let's consider the utterly simple, well maybe not the simplest, the simplest gas would
have one molecule but that's no fun.
Let's consider a gas with only two molecules, crazy, but let's consider it.
There are four possible microstates.
I could have a microstate in which the two molecules, and I've distinguished them by
color here, the pink molecule and the green molecule, one possible microstate has them
both on the left side of the box.
That could happen.
Another microstate has the green one on the left and the pink one on the right and still
another microstate has the pink one on the left and the green one on the right and by
our definition of microstate, a specific arrangement of the particular molecules, those two states
are different.
They're different microstates.
Now what about macrostates?
Macrostates just care about how many molecules are on each side of the box.
So there are two molecules in the left side and none in the right side for this molecule
on the left side and none in the right side for this macrostate, which corresponds to
that microstate.
There are two molecules on the right and none on the left in this macrostate, which corresponds
to this microstate.
But this macrostate has one molecule on the left and one molecule on the right and it
doesn't distinguish between these two microstates.
So these two microstates correspond to the same macrostate and you begin to see a pattern
already.
How many microstates are there?
We had two molecules in the gas, there were one, two, three, four, that's two squared.
How many macrostates there are only three?
One fewer, not significant, but one fewer.
Why?
Because there are only two plus one, there are the states where all the molecules are
together on one side and where they're together on the other side or where they're split evenly.
So there are more microstates than there are macrostates.
That means if this gas gets itself randomly into some state, which state are we most likely
to find it in?
Well, we're a little bit more likely to find it in that macrostate because there are two
corresponding microstates.
In fact, half the time we will find it in this state, a quarter of the time we'll find
it in this state and a quarter of the time we'll find it in that state if we just randomly
arrange those molecules and that's what's happening because they're moving around in
random motion.
So this macrostate where there are equal numbers on both sides is slightly more probable.
In fact, it's twice as probable as the other ones already.
That's only a two molecule gas.
What about a four molecule gas?
For a four molecule gas, there are 16 microstates.
Wow, it begins to be difficult to draw them all.
There's the microstate in which all of them are on the left and there's the microstate
in which all of them are on the right and then there are combinations.
These are all microstates in which three of the molecules are on the left, but they're
different microstates because they have different arrangements of the individual molecules and
one on the right.
Here's the opposite, there's one on the left and three on the right.
So there are one, two, three, four of those, there's only one of those and here are the
states in which half the molecules are on the left and half are on the right.
One, two, three, four, five, six.
There are 16 microstates, that's two to the fourth, that's the number of molecules we
got four molecules, 16 microstates, two to the fourth.
There are only five macrostates, that's four plus one.
There are the ones where all of them are on the left, they correspond to individual microstates,
there's the one where all of them are on the right corresponding to that microstate, but
this situation, three on the left and one on the right, corresponds to one, two, three,
four microstates, so does the situation with one on the left, three on the right, and the
one where they're divided equally, two and two, has one, two, three, four, five, six.
Out of all those possible states we could get into, this one again is the most likely
because there are more microstates that make that up.
So I'm likely, when I randomly arrange these molecules to get this state, I'm likely actually
six out of 16 times I'll get that state.
Only three out of 16 times I'll get that state, only one out of 16 times I'll get that state,
so that state is rare.
We're unlikely to see this situation, even with 16 molecules, we'll see it only one
out of 16 times, if we do this experiment over and over again.
So that's what happens with 16, with four molecules.
Well gases have a lot more molecules, suppose we had a hundred molecules, well you can see
the pattern, we'd have two to the hundred microstates, that's about ten to the thirtieth,
that's a huge number.
But we would only have one more than the number of molecules macrostates, that was the pattern
we saw established with two and then four, hundred and one microstate, macrostates.
And if I were to plot now, I'm not going to draw them all out, if I were to plot as a
function of how many particles there are on say the left side of the box, and the opposite
number of hundred minus that number on the other side, I would find a very strong tendency,
a great number of microstates that concentrate near fifty-fifty, not exactly, but near fifty-fifty.
So we are very likely to find this system of a hundred molecules in a state where there
are almost equal numbers on both sides, very unlikely to find these remote states like
the one where all the hundred molecules form on the left side or the right side.
And finally, let's look at a real gas which might have typically something like ten to
the twenty-three molecules, there are two to the ten to the twenty-three microstates
crazy number, there is a big number of macrostates also, but it's a lot smaller, it's ten to
the twenty-three plus one, and if I were to do that same graph, it's essentially a spike
with half the molecules in one side and half the molecules on the other side, and it looks
like this.
Maybe there's slight variations, maybe there's a few hundred more on one side or a few thousand
more or a million more on one side, but it doesn't matter, those numbers are so tiny
that essentially it's fifty-fifty equal, and it is just so improbable that it's never
going to happen that we get into the situation where they're all in one side.
So that's the statistical interpretation of entropy.
Okay, let's move on and get a little bit more quantitative.
Let's talk about entropy and how we would define it quantitatively.
Entropy is defined, not so much entropy itself, but the change in entropy is defined for a
reversible process, a process that has well-defined pressure, volume, and temperature as the heat
flow over the temperature at which this process occurs.
If the temperature is varying, you've got to use some calculus, but we'll stick with
this.
That's how entropy is defined.
The change in the system's entropy, the system's temperature, and the heat that gets added
to the system.
Let's express the second law of thermodynamics in terms of entropy.
It says the entropy of a closed system can never decrease.
At best it can stay constant.
In practice, there are irreversible processes, things like friction, and they need entropy
increases.
And we can generalize that process to the entire universe.
If we talk about that, we say the entropy of the universe can simply never decrease.
And let me do an example that shows this quantitative definition of entropy and talks about entropy
increasing.
So let me do a simple example.
I'm going to imagine I have hot water and cold water.
I'm going to put them in a closed insulated container, and I'm going to let them come
to some equilibrium temperature as we've seen before.
So here they are, hot at temperature Th, cold at temperature Tc, and let's let them come
to some equilibrium temperature slowly.
They come to some intermediate temperature.
It's in between the hot temperature and the cold temperature.
It's warm, and I'm going to call it temperature T.
What's happened to entropy?
Well, first, what's happened to temperature?
They originally, hot water, H, started at temperature Th, and it cooled toward temperature
T, the final temperature that they both share.
The initially cold water at temperature Tc, its temperature rose.
So the temperature rose to this final temperature T.
And here's the important point.
The temperature of the warm water was changing all the time, so my definition of entropy
required there to be a fixed temperature, or else I have to go use calculus.
But I can kind of avoid that by saying, look, there's some average temperature that the
warm water was at, and it certainly lies somewhere between the original high temperature and this
final temperature that they get to.
So I don't know exactly where the temperature is.
I could calculate it.
It's sort of the average temperature.
But what I do know is that that temperature, I'm going to call it T1, lies in between the
hottest temperature of that hot water and this final lukewarm temperature that they
both come to.
Similarly, for the cold water, which started out at T sub-cold, T sub-c, and rose toward
this final temperature, there's some intermediate temperature, which I'm going to call T2.
And I don't care what its value is.
All I care is it must lie somewhere between Tc and T, and I can use these temperatures
if I pick the right values as if they were a fixed temperature in that expression for
entropy.
And that's what I'm going to do.
So the entropy change of the warm water is minus Q over T1, because in that definition
of entropy, delta S is Q over T. Q is the heat that flowed into a system.
The warm water lost energy.
There was a heat flow to the cold water.
And so energy flowed out of the warm water as a flow of heat.
And so the heat Q is negative in the case of the warm water.
And again, I've picked this temperature T1 so that that does correctly give me the entropy
change.
It's not important that I get the calculation exactly right.
What's important is that that temperature T1 lies between the final temperature T and
the initial temperature H.
The entropy change of the cold water is plus Q over T2, the temperature that sort of characterizes
the average for the cold water.
Why plus Q?
Because heat flowed in to the cold water from the warm water.
Why the same Q in both cases?
Because I've got the whole thing in an insulated container, and so there was no energy lost.
Whatever energy left the warm water ended up in the cold water, so it's the same Q.
And now I'm ready to calculate the entropy change for the entire system.
By the way, you will notice that the entropy of the warm water decreased and the entropy
of the cold water increased.
There's nothing about the second law that says entropy can't decrease.
It says the entropy of a closed system or of the entire universe can't decrease.
It doesn't say an individual thing can't have its entropy decrease.
What will happen if something's entropy increases is, or decreases, it won't be part of a closed
system and something else in the system will experience an entropy increase.
And I guarantee that the increase will be greater than the decrease, and that's what
we're about to show.
So here's the change in the system's entropy.
It is Q over T2, that's a positive number, plus the entropy change of the warm one, and
that's minus Q over T1.
And here's the deal.
That quantity, I guarantee, it was greater than zero.
How do I know?
Because T2 is lower than T1, and that's all that we needed to know.
T2 lay between the cold and the final temperature.
T1 lay below the final temperature.
T1 lay above the final temperature.
T1 is greater than T2.
So this number, this fraction, because T1 is bigger than T2, this fraction is smaller
than this fraction, and this subtraction leads us a positive number.
And there's a clear, quantitative description in a simple system of how it is that entropy
increases.
Now, let me talk a little more about this possibility of systems that undergo entropy
decreases.
Closed systems don't.
We're always pointing out, well, here's a system that seems to contradict the second
law.
Let me give you an example.
The Earth was a more random system before life arose and evolved.
Living beings, in fact, take random molecules from their environment and make them very
organized.
I'm a very organized system.
You can see that physiologically.
My brain is organized with all kinds of ideas about physics.
I've got a circulatory system.
Wow, am I organized?
I have much less entropy than the molecules when they were randomly around in the environment,
which I've made.
Individual living things, that's true of.
Human society and civilization, that's true of.
The distribution of ink on the page of a book is much more organized than the ink when it
was in the ink bottle, or the distribution of lady-meeting diodes on a computer screen
when it gets an organized picture is much more organized.
Where did this organization come from?
The whole planet Earth, for example, is quite an organized system.
And it's not a closed system.
And there is something supplying that system with energy from outside.
In the case of the Earth, that thing is the sun.
And there are processes going on in the sun, which actually generate more entropy than
the decrease in entropy associated with, for example, the rise of civilization on Earth.
Any time you see a process that looks like it's decreasing entropy, and it may be, I
guarantee you, you're not dealing with a closed system.
Because if you have a closed system, any entropy decreasing process is offset by an entropy
increasing process.
So if I take the Earth, and I make a closed system that surrounds the Earth and the sun,
to the extent that that's closed, then the entropy of that entire system must be increasing.
Similarly, think about a refrigerator.
You put stuff in the refrigerator, and things get colder than they were already.
The opposite of what sort of ought to happen in a case like this.
I could take this system and put the warm water in the refrigerator, and the other warm
water out the back, and the warm water would get hotter, and the other one would get cooler.
But the refrigerator is plugged in.
And if I include the power plant and all the processes that go into generating the electricity
that runs the refrigerator, I've got a closed system, and its entropy will always increase.
Okay, let's wrap up lecture 26.
Systems tend naturally toward disordered states.
Why do they do that?
Because there are so many more disordered states than our ordered ones.
You can look at that in terms of the egg.
You can look at that in terms of the mathematics we went through with the partition box.
There are far more disordered states, so it's simply more likely that you will be in a disordered
state.
Entropy is a measure of that disorder, and entropy is a well-defined property of thermodynamic
systems.
The second law put in its grandest form says that the entropy of a closed system can never
decrease.
And as we move toward the next lecture, we'll be looking at some really practical implications
and some additional statements of the second law of thermodynamics.
