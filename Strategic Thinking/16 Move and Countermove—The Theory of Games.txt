When people interact with each other, they're playing games.
Whether lovers in a romantic game, drivers maneuvering in a traffic game, sports teams
playing an overt game with clearly defined rules or politicians vying for votes in a
political game, game theory offers a mathematical approach to crafting strategy to win games
involving two or more players.
Whether game theory is actually useful in practical matters depends on whom you ask.
Game theorists are adamant that their theory of games is not only useful to crafting strategy,
but many believe that game theory is strategy.
As with any theory, game theory has positives and negatives, and it can illustrate and predict
outcomes when people do act rationally and even find ways to cooperate.
Here's an example of how the average person, without much information at all, can find
ways to cooperate with other people, so everyone wins.
It's a game that I call Let's Meet in New York.
It demonstrates how, with almost no information, we can coordinate with each other people for
mutual gain.
In fact, this absence of information provides us with our most creative and analytical opportunities.
Without information, we're compelled to focus and to use our powers of reason and logic.
This example comes from Thomas Schelling.
Schelling was a game theorist who penned the powerful work of game theory and nuclear strategy.
In 1960, it was called The Strategy of Conflict.
Let's play the game.
Suppose I promised to give you, and another person, a thousand dollars a piece for just
telling me where you plan to meet each other in New York City.
Now you don't know this person, and you cannot communicate with this person.
This seems a relatively easy task until the rules are spelled out.
I give you only two pieces of information, New York City, and a specific date.
Now you give me the time and place of your meeting in 30 seconds.
That's it.
I play this game with students in my classes to demonstrate the power of analysis over mere
information.
In the absence of information, we are forced to think.
It's as if our minds are forced off the familiar track we normally traverse.
We are forced by necessity beyond the usual modes of thought and into unfamiliar territory.
The answers come in, and they are surprisingly the same every time I conduct this exercise.
90% of people list the place of meeting as Times Square or Grand Central Station, or
in a tip of the hat to the classic film in a fair to remember, The Empire State Building.
The overwhelming majority list the time as noon.
But there's more to this experiment.
That happens if I give the same task to a group of students, but with this changed condition.
I provide them with a five-page tip sheet full of facts about the city and the unknown
person you're expected to meet, and you get a full three minutes to come up with an answer.
That's 600% more time and approximately 2,000% more information.
Are the results better?
Is there more coordination?
Is there more of a consensus as to where and when the meeting will take place?
No, of course not.
The clarity of analytical thinking is dulled in the glut of information.
As the information supply goes up, the clarity of analytical thinking goes down.
The range of different times and places that students answer is far greater than in the
absence of information.
Why is that?
This is the world of the intelligence analyst whose occupation is making sense of the world
both without information and with information overload.
The intelligence analyst tries to anticipate the actions of a few select adversaries in
the presence of overwhelming information, the vast majority of which is useless or even
misleading.
This is not the normal state of affairs for most of us.
Similarly, we make arrangements with information we have or we do something else.
Only on occasion are we forced into situations where we must take into account another person's
actions in situations when we cannot communicate, and yet we very much desire a specific outcome.
For instance, the common happenstance of a husband and wife getting separated in a busy
mall at Christmas time.
The two of them will find each other, not through gathering of information, but through
logical analysis.
Life is a series of games.
When you drive in heavy traffic and you jockey for position against anonymous competitors,
you're playing a game.
When you go talk to your boss to ask for a raise, you're playing a negotiating game.
And when people interact in ways designed to achieve a goal, they're playing a game.
Game theory can demonstrate how rational people reason when presented with dilemmas within
complete information.
Game theory contends that it can provide the best rational strategy for a host of games
that we play, games that can be mathematically modeled.
Here we learn how game theory contributes to our understanding of how people reason
about problems, and how to utilize this contribution to help us pursue the best strategy in our
own games more often.
Games are situations in which there are several goal-oriented decision makers interacting
with each other.
The outcome of the interaction depends not on your actions alone, but on the actions
of the other player.
Moreover, your actions are contingent on each other's responses, and so in playing the game,
you must take into account what the other person might do.
This is what makes the game strategic.
Game theory looks at this type of situation mathematically.
It attempts to bring mathematical precision to human decision-making in game-type situations.
Concentrated development of game theory began in the 1920s.
Innocently enough, it involved one of one man's efforts to analyze a tactic in the game of
poker.
The technique of bluffing.
John von Neumann wanted to understand the process and discover if there might be a mechanism
for it.
And almost 20 years later, in 1944, von Neumann and his collaborator, Oscar Morgenstern, were
claiming that game theory could form the basis for the science of economics.
So we can understand what von Neumann and Morgenstern were talking about.
Let's go back to poker.
Like other games in which the pieces or arguments are clearly visible to every player, poker
incorporates an element of mystery.
You can't see the other player's cards, and those players can misrepresent what they
have in their hands through facial expression, body language, and by actions in the game.
Even a series of actions throughout a series of hands can be a strategic ploy intended to
create a false pattern to lull an opponent into a false sense of knowledge.
Creating an opposing player to misapprehend what you have in your own hand and to mistake
your intentions is called a bluff.
Von Neumann elected to study this game to determine if there might be a scientific or
mathematical way of modeling the situation in poker and determining the best way to
bluff.
Game theorists create mathematical models consisting of variables and equations that
represent a variety of games we encounter in the real world.
These equations, they purport to demonstrate the best strategy to pursue in the real world
situations they represent.
This all sounds like a good idea, is it?
Like all good theories, this one comes with assumptions about how the world works and
how people behave in that world.
One assumption is that of economic rationality.
Economic theory assumes that each of us is a rational actor, which means that we will
act rationally in interacting with others.
Rational behavior means that in goal-directed interaction with others, we seek to maximize
our utility, and this means we act to receive the highest payoff of the options available
to us, or we seek to minimize the damage that is done to us, its business-type cost-benefit
analysis applied to the human mind.
It is this assumption of rationality that vexes game theory and its ability to actually
predict human behavior.
One of the major contributions of game theory to the world of strategy came during the period
1950 to 1991, a time of nuclear standoff between the Soviet Union and the United States.
Game theorists recognized that the nuclear arms race had all of the characteristics of
a two-person strategic game.
In fact, the nuclear game closely resembled the dynamic at work in a game called the prisoner's
dilemma, which is one of the most widely known games utilized by game theorists.
There's a reason that it is the most widely known game.
It captured the dynamic in arms races, and as such, it had applicability for the foreign
policy establishment.
Most precisely, the prisoner's dilemma highlighted the dangers involved during the nuclear arms
race during the Cold War.
The game theory folks thought that they might be able to contribute to the nuclear debate
by ascertaining what the best possible strategy in such a dangerous game might be.
Prisoner's dilemma demonstrated how the nuclear game, as it was structured, would lead to
complete nuclear destruction if only there was a timeliness step by either side.
Now briefly, here is the prisoner's dilemma.
The game is structured with two players in a situation where each does not know the move
of the other, and they have the options to cooperate with each other or to defect from
the game.
Interviewed separately, the prisoners are given the option of implicating the other
prisoner in a crime or of remaining silent.
If both cooperate to protect each other by remaining silent, they both receive a beneficial
payoff of a light one month sentence.
If they both defect to implicate the other, they both receive a payoff of a three month
sentence.
The interesting part of the game is when the prisoner's decisions differ from each other.
If one cooperates while the other defects, the cooperator gets the sucker's payoff of
a one year sentence while the defector goes free.
In this situation, the dominant strategy is to defect.
There is just too much risk attached to cooperation, and this is the dynamic we saw during the
Cold War arms race.
Game theory showed that the incredible danger in the nuclear standoff was the dominant strategy
of defection.
The game gave the advantage to the state that launched a successful nuclear first strike.
Game theory contributes to our thinking in a number of ways.
Game theory gets us to consider not just our own plans, but the responses of others who
have an interest in the outcome of the game.
The responses from other players can have a tremendous impact on the situation we're
dealing with, changing it radically.
So we must anticipate responses, and then understand and select the best winning strategy
that can maximize our payoff.
It's the way it's supposed to work, but our opponent's response can sometimes be irrational
or unpredictable in our judgment.
We can sometimes misjudge the payoff of a game other people might be playing for different
stakes, or for higher, or for lower stakes.
So what appears irrational could very well be rational to our opponent.
Moreover, an opponent might feign his own irrationality to achieve a strategic result.
Another way of putting it is that a player acts irrationally because he believes that
it is rational for him to do so.
We must incorporate this concept of rational irrationality in our own calculations.
In a multiplayer game, it sometimes becomes the goal of one player to defeat another by
preventing him from winning, regardless of the cost to himself.
This type of rational irrationality is on display in my classes every semester as I
have my students play an auction game.
When I teach international economics, I introduce the subject to my students with an auction.
This is an auction game created by Max Baserman.
The auction game works this way.
I tell them I'm auctioning off a $20 bill.
Bidding starts at $1 and proceeds in $1 increments.
There are only two rules.
First is that the winner pays the winning bid to receive the $20 bill, of course.
Second, the person who came in second in the auction must pay the losing bid.
So for instance, if the winning bid is $4, the winner pays $4 to receive the $20 bill.
Second, the person who came in second pays $3.
This seems like an easy way to make $20, but people tend to believe what they want to believe.
People assume that they know the rules of an auction and they ignore the rules of this
one.
This is partly because we have mindsets that assimilate new information only with difficulty.
As the auction progresses, bidding is brisk until it reaches $9 or $10.
At that point, students quickly drop out until only two are left.
These two students suddenly find themselves playing in a game entirely different from
the one they thought they were playing.
The bidding usually escalates well beyond $20, and the flurry of bidding activity is
amusing to the rest of the class.
Instead of an open auction with many participants and a chance to steal $20 from the absent-minded
professor, they find themselves in a two-person game locked in a bidding war, feverishly bidding.
The payoff for the game has dramatically changed.
The bidding is no longer to win $20 on the cheap, it's to minimize losses.
The players will often go well beyond the original $20 to limit their losses, and the
bidding has a ratchet effect as the stakes get higher and higher, with no retreat possible.
I've run this auction with students outside the U.S. as well, with similar results.
In pursuit of the elusive $20 bill, students appear ready to spend as much as it takes
to keep from losing.
Who knows what drives the dynamic?
Ego?
The excitement of the crowd egging them on?
An internal calculus that operates differently than for those of us who merely observe the
proceedings?
Regardless, the exercise demonstrates that quite often we are not rational in our decision-making,
even in the realm where we would expect to find the most rational decision-making of
all our personal economics.
Our assumption of rationality can lead us to mis-specify the payoffs for a particular
game for another player.
We may assume that everyone in the game is motivated by the same incentive.
In work, for instance, some people are motivated by money, some by free time, others by awards
and recognition from the crowd, still others from quiet recognition from the superior,
and others simply by the permission to exercise authority within a particular sphere of activity.
This is a form of misperception.
One of its most prevalent forms is the fallacy of mirror imaging.
We assume that an opponent or fellow worker shares the same values and incentive system
as ours.
We may be wrong, but we base our actions on this assumption and we anticipate a response
based on this assumption, only to be surprised by an irrational response.
Former Defense Secretary Robert McNamara discovered this fallacy for himself in a way that cost
the United States dearly in blood and treasured during the Vietnam War in the 1960s.
McNamara was playing a coolly rational game of carrots and sticks in a limited war.
He assumed that the Vietnamese would recognize that they could not win such a war against
the United States.
The Vietnamese were, however, playing a different game, a total war of national liberation.
In retrospect, McNamara recognized that rationality will not save us.
Long after the war, McNamara acknowledged his mistakes in playing the wrong game.
He said this.
We viewed the people and the leaders of South Vietnam in terms of our own experience.
We totally misjudged the political forces within the country.
We underestimated the power of nationalism to motivate a people, to fight and die for
their beliefs and values.
Our misjudgments of friend and foe alike reflected our profound ignorance of the history, of
the culture and politics of the people in the area and the personalities and habits of their
leaders.
Closer to home, let's look at a game that all of us play at one time or another.
A game in which our rational, independent actions and our own self-interest can lead
to an undesirable result in the long run.
This game that illuminates economic issues for us is derived from a famous allegory called
The Tragedy of the Commons.
The Tragedy of the Commons is a dilemma that arises when individuals who pursue their own
rational self-interest yield a group result that is suboptimal.
The name refers to a situation described by ecological scientist Garrett Hardin in a famous
1968 essay called The Tragedy of the Commons.
In the scenario, herdsmen graze their cows on a common parcel of land.
Since there is no individual incentive to restrict grazing, and in fact there is a great incentive
to graze as many cows as much as possible, the parcel of land is quickly damaged and
is made useless by overgrazing.
The key to the dilemma is that the herder receives all the benefits from grazing his
cows while the damage is shared by the group.
So it's perfectly rational for each farmer to graze his cows as much as possible, even
as this depletes the shared and limited resource.
Let me quote Hardin here.
The rational herdsman concludes that the only sensible course for him to pursue is to add
another animal to his herd, and another, but this is the conclusion reached by each and
every rational herdsman sharing a commons.
Each man is locked into a system that compels him to increase his herd without limit in
a world that is limited.
Ruin is the destination toward which all men rush, each pursuing his own best interest
in a society that believes in the freedom of the commons.
Hardin portrayed our national park system as a modern day commons.
The parks themselves are limited in extent, for instance.
There's only one Yosemite Valley, but our population seems to grow without limit.
The values that visitors seek in the parks are steadily eroded.
This pursuit of private gain and shared cost is a hidden dynamic that Hardin uncovered
in the economic system we know as private enterprise or capitalism.
This is a game that companies play, and it has a name.
Game theory models this as the CCPP game.
Commonized costs privatize profits.
Hardin developed this game, and he claims with some justification that it captures much
of the wealth creation pattern in a free economic system.
The businessman devises a silent way to commonize his costs while privatizing his profits.
He claimed that many fortunes have been built in this way.
The private industry pursues its profit-making venture and, say, dumps waste into a river
or pollutes the air.
This is a cost shared by all, while the profits accrue to the business.
Commonly this is called an externality or an external cost.
Hardin claimed that this is a sleight of hand that masks the actual CCPP game.
Another similar situation is a common problem in economics, which is called the free rider
problem.
It can affect all of us, and it probably has affected all of us at some point.
It can be infuriating.
With the free rider problem, a common resource is shared, whether individuals pay forward
or not.
In the macroeconomy, we find our examples everywhere, national defense, or clean air.
But we also find examples in our daily lives.
Many of my students are familiar with the phenomenon of the late-night keg run.
The party is at its height, and the keg runs dry.
The hat is then passed for contributions to buy another keg.
Here is where the rational person gives pause.
Do I contribute or not?
Regardless of whether I contribute, the keg will arrive soon, and I can enjoy its benefits
without having contributed.
This is the free rider.
And yet, somehow, money is collected from people, and the keg arrives.
What distinguishes the contributors from the free riders?
What does this do to our assumption of rationality?
Ironically, it's probably rational for everyone to have made their respective decision.
It all depends on where you stop in the chain of reasoning.
Some people contribute because they believe everyone else will contribute.
Some don't contribute because they believe everyone else will contribute.
This is the problem with rationality and rational choice theory, and ultimately, the problem
with game theory.
John Elster is a brilliant scholar who has grappled with rational choice and game theory
for many years.
Elster attempted to construct a game theoretical foundation for Marxism based on rational choice
theory, the economic notion that people act based on the expected benefits and the choices
others are likely to make.
John Elster would eventually cease striving, having lost faith in rational choice.
He made the most obvious observation.
Do real people act on the calculations that make up many pages of mathematical appendixes
in leading journals?
I do not think so.
There is no general, non-intentional mechanism that can simulate or mimic rationality.
So this leads us to calculate how other people actually do behave.
If we understand that rational calculation might lead to different sorts of behavior,
perhaps we can construct new rules of the game that will take these differences into
account.
The free writer problem that is illustrated by the Kegg Party example is an enduring problem
in economics, and is called a collective action problem.
How can you get people to work together to achieve a collective goal when individuals
may be motivated by different incentives, and may be motivated not at all by a group
goal?
In a modern, diverse workplace, personnel are often motivated in different ways, some
by money, some by holidays, some by private praise, some by public acknowledgement, some
by group rewards.
The late economist Mansor Olson suggested one solution to the collective action problem.
He suggested the creation of selective incentives to get people to respond properly, that is,
to respond the way you want them to.
When group incentives are not sufficient to get people to contribute, then a different
incentive with an individual payoff might work.
I use this principle of selective incentives in a real-world situation, and I can say it
does work.
My problem was simple enough.
How could I get people to wear name tags over the entire two-week period of a conference?
You know as well as I do that most people stop wearing a name tag after the first day.
Experience told me that my exhortations to wear name tags for the vague goal of increasing
goodwill among the participants would be futile.
They would not wear name tags to please me, nor would they wear name tags for the group
goal of goodwill.
The payoff simply was not high enough for each participant to wear a name tag all two
weeks.
The task was to create an individual incentive for each person to wear his name tag every
day, all day.
They would wear name tags in their own self-interest, not the group interest.
The solution?
Specially created name tags became their ticket to all of their meals.
Not wearing a name tag?
No food.
But I'm not wearing my name tag because you want me to, said one young man who was philosophically
inclined.
I know, I answered, but that's irrelevant, and that's the beauty of selective incentives
designed to achieve collective goals.
In this game, I altered the payoff structure so that it modified the behavior of the participants
in accord with a larger objective, but which satisfy their individual needs.
So, what can we conclude about game theory?
Like so many other theories, it offers us insight into how people reason.
It forces us to consider the notion of a payoff and what that payoff is.
It compels us to consider the stakes of a game and whether all participants view the
stakes and payoff in the same way.
We also recognize the weaknesses of game theory as applied in the world of daily living.
We can see that the artificial notion of rationality expressed in behavior could cause problems
for us if we expect people to act in accord with it.
Even if we could conclude that people are rational in some universal sense, and we cannot.
This rationality is not always expressed in behavior.
As we've seen, sometimes a rational person may act irrationally because it's rational
for him to do so to achieve his goal.
The uncertainty principle tells us that the presence of an observer changes the behavior
of what is being observed, and that is a characteristic of any theory.
Once the theory's precepts and predictions of behavior become part of the information
available, this knowledge then alters the behavior as players incorporate the theory
into the strategic calculations.
It should be obvious why this reduces the usefulness of the theory.
As we've seen, game theory is not a panacea.
It cannot reveal the future.
People oftentimes do not act rationally, and rationality is the basic assumption that
must be fulfilled for game theory to function.
There may be deception, and the participants may not agree on which game is being played,
even if there is no deception, and even if both sides are acting rationally.
But game theory does offer us insight into understanding human interaction, especially
when people do act rationally, and it also offers a structured way to understand cooperative
alternatives to pure conflict.
What's attractive is that sometimes unnecessary destruction can be avoided, and outcomes can
be maximized for all sides.
