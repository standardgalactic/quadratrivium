Let me tell you something new, something you won't find in the textbooks yet.
The second law of thermodynamics that Clausius discovered a century and a half ago turns
out to be exactly equivalent to the following statement.
No process can have as its only result the erasure of information.
Every time you delete an email or erase a pencil mark on a piece of paper, that process
is accompanied by some change, some other change in the world.
Well, why is the erasure of information so important?
What does it have to do with the second law?
Well according to the principle of microstate information, the laws of physics for atoms
and molecules preserve information about their state.
If we imagine two different initial microstates for a system, states that differ in some way
and in where the molecules are or how they are moving, the states will evolve over time
into two different final microstates.
But in the everyday world in which we only see macrostates, things work differently.
Macroscopic information is not preserved, it can be lost.
So we see a book, sitting on a table.
It is at rest now, but a couple of minutes ago perhaps it was moving.
It might have been sliding from one side or the other, but friction slowed it down to
a stop.
This kinetic energy, energy of motion, was converted to heat.
So two different initial macrostates, book moving this way or book moving that way, both
lead to the same final macrostate, a slightly warmer book sitting at rest.
The macrostate has forgotten which macrostate it came from.
Well how did that happen?
In phase space, the abstract space in which each point is a microstate, macrostates correspond
to large regions containing many microstates.
We have a region we'll call L, which corresponds to the book moving left.
And another called R, corresponding to the book moving to the right.
And there's also a macrostate region called zero, corresponding to the book at rest.
It's larger because the book is slightly warmer.
More internal energy means more possible ways for the book's atoms to be vibrating.
So the microstates move around in phase space.
Microstates that start out in either the L or R regions end up in the zero region.
That can happen because the final region zero is larger than the other two.
There are more microstates consistent with that macroscopic information, and therefore
the entropy is higher.
We can learn many things from this.
In the microscopic world of individual atoms and molecules, there is no friction.
Friction is a big picture thing, a phenomenon of systems with trillions of molecules.
Friction can cause the loss of macrostate information, the distinction between a book
moving to the left or to the right, and it can also produce an increase in entropy.
And as we will see, those two effects are inextricably linked.
The most famous demon is a hypothetical entity that uses microscopic information to control
the behavior of a system, that the classic version has the demon operate a tiny trap
door between two containers of gas, and in this way it can achieve effects that at least
appear to violate the second law and reduce the system's entropy.
And over the years, lots of physicists have tried to find a loophole.
They thought about the demon not as a supernatural being, but as a physical being of some kind.
Operating according to the laws of physics.
By carefully considering how such a demon would function, they hoped to show that it
could not work as Maxwell supposed.
They tried to exorcise the demon from thermodynamics.
The Hungarian Leo Chalard and later the French Nobel Laureate, Leon Breuang, analyze a demon
as a system that acquires and uses information.
Indeed Chalard anticipated some of the ideas of Shannon's information theory by almost
20 years.
Starting from quantum physics, they argued that the demon must expend energy to acquire
its information about gas molecules.
It has to bounce photons off of them maybe.
That takes energy.
The energy is dissipated as heat, leading to an increase in the entropy of the surroundings.
This offsets the decrease in entropy achieved by the demon, so overall the second law is
vindicated.
They were almost right.
A real Maxwell's demon, a physical entity that makes use of microstate information,
cannot in the end violate the second law.
But Chalard and Breuang were wrong about the reason why.
It isn't the acquisition of information that creates extra entropy.
It's what happens to the information afterward.
One of the people who read Breuang's work was a physicist at IBM named Rolf Landauer.
It set him thinking.
What do the fundamental laws of physics tell us about systems that process information?
What principles govern the physics of computers?
Landauer's own research was about how electrons move inside materials, the basic physics,
in other words, of modern electronic computers, but Landauer's question was wider.
Regardless of the technical details, what can we say in general about computation as
a physical process?
We know that computers produce waste heat as they operate.
They produce so much heat that getting rid of it is a real challenge in computer design.
Take a real-world example, Intel Corporation's Ivy Bridge processor produces around 100 watts
of heat while it is operating, that's 100 joules per second.
All of that is being generated in a very tiny space, a thin layer half the size of a postage
stamp where a billion transistors are crowded together.
Without some pretty effective heat management, heat conduction, and cooling fans and so on,
the Ivy Bridge processor would glow red hot.
In fact, it would not work.
We are asked why, at a fundamental level, why does computing produce any waste heat at all?
Well, one reason is speed.
It produces more heat to go fast.
Let me give you an oversimplified picture of one bit in a computer.
The bit is essentially a capacitor, which is an arrangement of two metal plates next to
each other.
Some electrons can be shifted from one plate to the other.
That makes one plate positively charged and the other negatively charged.
From one side has the positive charge, the bit is zero, when the other side does, it's
a one.
Now, let's suppose we want to change the bit from zero to one.
Then we have to run the electrons quickly around from one side to the other.
And we need a wire for that.
And the wire has some electrical resistance, electron friction.
That's represented by the zigzag line in the diagram.
Friction produces waste heat.
But now suppose we slow things down.
We shift the charge from one side to the other only one-tenth as rapidly.
The electric current in the wire is only one-tenth as big, but it flows for ten times as long.
Here's the interesting point.
Heat dissipation is proportional to the square of the electric current.
So waste heat is now generated at only one-one-hundredth the previous rate.
So everything takes ten times as much time, of course, but that still means that the total
heat generated is only one-tenth as great as before.
So we can do the same operation, moving the charge to change at zero to a one with only
one-tenth of the waste heat.
The I.D. Bridge processor does billions of computational steps per second, but it pays
for that speed with a hundred watts of waste heat.
If you try to run it even faster, that's called overclocking the processor.
It generates considerably more, and your heat removal problems are that much worse.
But Landauer found that even if you design a computer perfectly and are content to run
it very, very slowly, there would still be some waste heat.
Entropy would be increased.
The process would be thermodynamically irreversible.
Why?
The reason computation is thermodynamically irreversible is that it is logically irreversible.
What does that mean?
Suppose we have a computer and in its memory are two numbers, three and four.
The computer calculates their sum and stores the sum in the same memory location.
Before you had three and four, afterward you have seven.
That computation is logically irreversible.
From the final memory state, seven, you cannot deduce the initial memory state.
It might have been three and four, it might have been five and two.
The computer has forgotten exactly how it started out.
And whenever that happens, Landauer said, there is always an increase in entropy somewhere.
If nothing else, the computer must release waste heat into its surroundings.
He illustrated this by considering the simplest possible irreversible bit operation, erasure.
When we erase one bit in memory, we start out with a bit that might be either zero or
one, and then we reset it to a particular value, say zero.
The information is not stored anywhere else, the computer cannot unerase it.
Once erased, the bit has been lost.
Now the computer and its surroundings form a physical system with a phase space.
Each microstate of the system corresponds to a point in that space.
So initially the memory bit of the computer is either zero or one.
Each of these corresponds to a whole region in phase space, a macrostate.
That's because there are many different detailed arrangements of atoms of the computer consistent
with either bit value.
So the microstate starts out in one of two possible regions which we will label zero
and one, and each region contains m microstates.
In turn, the memory is zero, but there might be some other changes in the computer.
So we will have another macrostate which we'll call zero prime.
In the process of erasing the bit, all of the possible initial microstates wind up there.
But by the principle of microstate information, that new region must be at least as large
as the first two combined.
m prime is the number of microstates in region zero prime, then m prime is at least two times
m.
Now I want to clear up a possible point of confusion.
The memory bit, the physical thing that stores the zero or one, might be very tiny.
It might even be a single atom.
I've referred to zero and one as distinct macrostates, but the physical difference between
them might not be very macro.
That doesn't matter.
The important thing is that they are distinct.
So I've proposed a new word to capture the idea.
The word is idostate.
It comes from the Greek word idos, meaning to see.
The idostate encompasses all of the information that is available to the computer, all of
the data that the computer can presently see and use.
And that includes macroscopic information like its temperature or battery level, and
it also includes the zeros and ones in memory.
Therefore two different memory records will correspond to different idostates, even if
the memory units happen to be microscopic.
The computer, like us, lacks information and maybe quite a lot about which atom is where
and what they are all doing.
So each different idostate is consistent with a huge number of possible microstates.
It's a whole region and phase space.
So thermodynamic entropy is just k2 times the log 2 of that number, the number of bits
of microstate information that the computer lacks.
The idostates zero and one, each containing m microstates, both evolve into the same idostate
zero prime containing at least two m microstates.
It's exactly the same situation as when we slid the book across the table.
The system forgets where it started out.
So the final idostate has to be larger, it must have a greater entropy.
How much greater?
Well, we can calculate the change in entropy.
Delta S is S final minus S initial.
S final is k2 times the log 2 of m prime, which is at least as big as k2 log 2 of 2m.
Therefore, delta S is at least k2 times log 2 of 2m minus log 2 of m.
The difference of two logarithms is just the logarithm of the ratio, that's one of the
basic properties of the logarithms.
So we have k2 log 2 of 2m divided by m, or just k2 log 2 of 2, and the log 2 of 2 is
1.
Therefore the entropy of the whole system, computer plus surroundings, must have increased
by at least k2, around 10 to the minus 23rd joules per Kelvin per bit.
Now that entropy increase is q divided by t, where q is the waste heat and t is the ambient
temperature.
Therefore the waste heat q is at least k2 times t.
Now at room temperature, 300 Kelvin, that isn't much, 3 times 10 to the minus 21st
joules per bit erased.
When your own computer erases information, it generates much more waste heat than this.
But this is the absolute minimum, and this fact is known as Landauer's principle.
Erasing one bit of information produces a waste heat of at least k2 times t.
Rolf Landauer proposed this way back in 1961, and at the time he was not quite clear which
computer operations had to produce waste heat.
Racing a bit, yes, but what about the others?
Years later, another IBM scientist, Charles Bennett, took the next step.
Bennett realized that every computer operation could in principle be made logically reversible.
A computer could operate in principle with no waste heat at all.
Erasure does produce entropy, but computation does not require erasure.
When Bennett first explained this to Landauer, he didn't quite believe it at first.
The idea of a fully reversible computer, a computer that can run backward as easily as
it runs forward, is a stretch of the imagination.
A computer of that kind would not operate much like the ones we have today.
But Bennett's arguments were sound, and Landauer soon agreed.
To see how a reversible computer might be designed, let's go back to one of Claude
Shannon's ideas, the idea that any computation can be done by a complex network of one bit
messages connecting simple Boolean logic gates.
Unfortunately, those logic gates are not very reversible.
Consider the AND operation, represented by a little wedge.
In the diagram, the AND gate is a little box with two input lines from the left and one
output line to the right.
Each line carries a bit, either 0 or 1.
The output bit is 1 if the input bits are both 1, but otherwise the output is 0.
That's not a reversible operation.
We cannot always reconstruct the input from the output.
If the output is 0, the input bits might have been 0, 0, or 0, 1, or 1, 0.
The AND gate forgets its input.
And so it turns out to be with the OR gate and the XOR gate.
Each of them destroys some information about the input bits.
One of our basic logic gates is reversible, the NOT gate.
It has one input and one output.
If the input is 0, the output is 1, and vice versa.
That's a reversible operation.
Just apply a second NOT gate.
The original bit value is perfectly restored.
We want to do a reversible computation, we need to use reversible logic gates.
So how do we design those?
Well, there are a couple of rules to keep in mind.
Call them the rules of reversible information processing.
The first rule is that the logic gate must have exactly as many bits of output as there
are bits of input.
If there is one input bit, there is one output bit, like the NOT gate.
But if a gate has two input bits, it must also have two output bits.
If it has three inputs, it must have three outputs.
The second rule is that the logic gate must preserve the input information.
Any two distinct inputs must lead to two distinct outputs.
So let's illustrate that rule by describing a very useful two-bit reversible logic gate
that's called the controlled NOT gate, or C-NOT.
The two bits, input bits, played different roles.
One of them is the control bit, C.
Nothing happens to the control bit.
It stays unchanged in the output.
The second bit is called the target bit, T. If the control bit is zero, then the target
bit is left alone.
If the control bit is one, then the target bit is negated.
Zero turns into one, and vice versa.
What we're really doing is replacing the target bit with C, X, or T.
You can see that the gate has the right number of output bits, so the first rule is okay.
To verify the second rule, you can make a table of input and output values.
The output bits considered together are different in each row of the table.
Thus, no two different inputs lead to the same output.
The gate preserves the input information.
The C-NOT gate is so useful that there is a special symbol for it in diagrams.
We put a dot on the control bit, and then connect that with a line to an XOR symbol on
the target bit, like so.
Now, arrange for that target bit to start out as zero.
That input is a fixed constant.
What happens?
Well if the control bit is zero, both bits end up zero.
If the control bit is one, both bits end up one.
Either way, we end up with two copies of the control bit.
This is how we could copy information in a reversible way.
We need to start out with a target bit of zero, some blank memory in which to create
the copy.
Then, we use the C-NOT operation, and we can also uncopy the reverse of copying.
What does that mean?
We start out with two copies of the same bit.
They are either zero, zero, or one, one.
If we apply the C-NOT, the control bit is unchanged, but the target bit is always set
to zero.
Notice that this does not count as erasing information, it's really just data compression.
We start out with only one bit of information, stored redundantly in two binary digits.
At the end, we still have the same bit, but now it's stored more compactly in one binary
digit.
Consequently, we have freed up some blank memory, the other bit is now zero.
That bit is available for storing some other information.
In ordinary non-reversible Boolean gates, two bit gates were enough.
When we build, we can build any complex mathematical operation out of a network of just those.
In reversible logic, however, it turns out that we need at least one three-bit gate.
There are several choices, but my favorite is the Toffoli gate, invented by the computer
scientist and physicist, Tomaso Toffoli, now of Boston University.
The Toffoli gate is sometimes called a controlled-controlled-NOT, or CC-NOT.
It has two control bits, C1 and C2.
They don't change.
The target bit is negated only if both C1 and C2 are equal to one.
We can diagram that like the C-NOT gate, but with two control bits.
We can use the Toffoli gate to construct reversible versions of our old standard Boolean gates.
For instance, suppose we arrange that the target bit input is always zero.
That bit changes to one only if both control bit inputs are equal to one.
So the target bit is now the AND of the other two bits.
We have a reversible AND gate.
It's reversible because it keeps the two input bits around and so no information has
been lost.
We can also make reversible versions of the OR and XOR gates.
Well can such reversible gates actually be built?
Toffoli, along with Edward Fredkind of MIT, proposed an idealized model.
It is less of a practical blueprint than a proof of concept.
The idea is to do your computing with billiard balls.
The balls move along various possible paths, corresponding to the bits in our diagram.
If there is no ball in one path that's a zero, if there is a ball that's a one.
The balls bounce off of fixed obstacles to change direction and logic operations are
performed by precisely timed collisions between the balls.
Since there is no friction and the collisions are perfectly elastic, everything is reversible.
Send the balls backward on their paths and the computation would run backward.
There is no waste heat.
The billiard ball idea sounds more like a vast pinball machine and it isn't a very
practical computer, but the Fredkind Toffoli billiard ball computer illustrates a point.
A real physical computation is actually performed by the laws of physics in the billiard ball
computer by Newton's laws of motion, and that is, in its deepest sense, what a computer
is.
It is a piece of nature that we have arranged so that its natural evolution encodes some
mathematical calculation.
Nowadays there are more serious proposals for making reversible logic gates using superconducting
circuits and so forth.
And also, Bennett's ideas about reversible computing have inspired engineers to find ways
of reducing the heat dissipation of computer operations in regular computers.
Computers today dissipate a million times less heat energy per operation than the ones
that were around when Bennett first dreamed of reversible computing.
That's a huge increase in efficiency.
If you had somehow been able to build Intel's Ivy Bridge processor using the technology of
40 years ago, that is, if you had put the same number of components in the same space
and then run the system just as fast, then it would not have glowed red when you turned
it on.
It would have exploded.
The Toffoli gate achieves its reversibility by keeping around some extra bits at the
output end, and this is no accident.
Bennett's reversible computer does not have to generate waste heat, but it does produce
waste information, extra bits stored in memory that are not part of the final answer and
that we may not care about.
If we want to get rid of them, we will have to erase them, and that means we will have
to pay Landauer's price, K2T, per bit.
That is the only point in the whole business where waste heat or entropy increase is necessary.
Erasure is the only truly irreversible thing you can do to information.
Okay, this all started from thinking about Maxwell's demon.
Does the demon provide a loophole to the second law of thermodynamics?
Bennett said, let's think about Maxwell's demon as a reversible computer, reversible
to avoid unnecessary waste heat.
As it operates, it acquires information about the molecules in a gas.
It uses that information to sort the molecules out.
The demon does not produce any waste heat, and the entropy of the gas appears to decrease.
The second law seems to be contradicted.
However, and this is crucial, something else has changed.
The demon's memory is now full of data it acquired about the molecules.
That's a huge amount of information, and the demon cannot continue to operate once
its memory is full.
In other words, the demon has consumed a resource, blank memory, to accomplish its task.
Bennett argued that if we want to unambiguously contradict the second law, the demon must
operate in a closed cycle.
It must return to its own initial state.
That way, it can continue to function.
But that means the demon must eventually erase its memory record.
Erasure is not reversible.
The demon must dissipate at least K2T of heat per bit erased.
And that cost of information erasure and the entropy produced in the surroundings is exactly
enough to save the second law.
Social art and BreWang were wrong.
The problem with the demon is not the cost of acquiring information.
A cleverly designed demon can do that without generating any entropy.
The problem is when the demon gets rid of the information.
Because of Landauer's principle, that cannot be done for free.
And now we arrive at that new statement of the second law with which we began the lecture.
No process can have, as its only result, the erasure of information.
This seems related to Landauer's principle, though it sounds less specific.
Both say that the erasure process is always accompanied by some other net change in the
world.
The demon does not specify what the cost might be.
But let's call it the Landauer form of the second law.
The Landauer form is truly equivalent to the Clausius form of the second law.
Each one implies the other.
Put another way, any machine that can violate one form of the law could be used to violate
the other form.
So imagine something that violates the Landauer form of the law.
There is some way to erase information for free without any cost of any kind.
We turn Bennett's reversible Maxwell's demon loose on a sample of gas.
It drives the gas entropy down while filling up its memory with useless data.
Now the demon exploits our hypothetical free erasing process to empty its memory.
It returns to its initial state.
The only net result is that the entropy of the gas has gone down, a clear violation of
the Clausius form of the second law.
First suppose we had an engine that violated the Clausius version, making entropy go down.
It might do this in one of several different ways.
For example, imagine the engine could extract heat from a body and turn 100% of that heat
into work.
Energy in the form of work can be used as we like.
For instance, we can use it to erase some bits of information.
The waste heat from that process could be returned to the body from which the heat was originally
extracted.
The net result, the body has returned to its original state.
Nothing else has changed except that we erased some bits, a violation of the new Landauer
form of the second law.
So they really are the same thing.
Maxwell's demon is a famous thought experiment, but increasingly it is becoming a real laboratory
experiment.
As we gain increasing control over nanoscale systems, we can effectively build devices
that act on microscopic information.
Consider for example an experiment done in 2011 by Mark Reisen's group at the University
of Texas.
They began with a cloud of ultra-cold atoms confined in a magnetic trap.
This type of atoms could exist in two stable internal states, which we may think of as
red or green.
Across the middle of the volume is a laser beam that would act as a barrier to red state
atoms.
Initially all the atoms are in the green state, so they pass through.
Just to the right of the barrier is another laser, call it yellow.
This is the demon laser.
The laser in a sense finds out where the atoms are and manipulates their state accordingly.
When a green atom passes through this beam, it absorbs and emits photons and then it turns
into a red atom.
Or any atom that crosses the middle barrier is soon switched to red and stays on the right
side afterward.
Eventually the whole cloud is there.
Reisen's experiment does not violate the second law, but the photons carry away entropy
from the color switching process.
But it does provide a way of compressing that cloud of atoms without heating it up.
That's new.
There may be applications for this to everything from medical imaging to isotope separation.
We haven't so much exorcised the demon from thermodynamics as made a home for it in science
and in technology.
Maxwell's demon has become our teacher and our guide.
It suggests a way to think about the second law more deeply, not simply as a rule about
energy transformations, but as a profound principle of information.
