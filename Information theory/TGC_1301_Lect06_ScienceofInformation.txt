Claude Shannon's first fundamental theorem, the principle of data compression, tells us
how far we can squeeze information into a binary code. The number of bits per message
can be made as low as the entropy of the source, but no lower. Yet sometimes, we need to go
lower, and for some kinds of data, we can.
Let's begin with a different approach to data compression, and let's look at a picture
I've drawn, a cartoon really. This picture, like all computer pictures, is just a two-dimensional
array of pixels, tiny square dots. This one is 200 pixels wide and 200 pixels high. Let's
zoom in so that you can see the pixels.
The pixels have just two colors, black and white, so we can encode each pixel as one
bit. Black is zero, white is one. To represent this picture, we list all the pixel bits,
starting in the upper left corner and going row by row down the picture. Since there are
200 times 200, or 40,000 pixels, we can represent this picture by 40,000 bits.
It actually takes up rather more bits than this on my computer, because the computer
needs to store images of many different kinds. So the computer needs to record the width
and the height of this particular image, the kinds of colors it has, and so on. All of
that extra information gets coded into the first part of the image file, which fills
it out to around 45,000 bits. But if we agree in advance that all of our pictures are going
to have 200 by 200 black and white pixels, then 40,000 bits is all we need.
When we look closely at those 40,000 bits, we find that there are long stretches of pixels
that have the same value. The first several lines are nothing but zeros. Even further
down, we find dozens of zeros and dozens of ones together. That's a pattern. And it
is a common kind of pattern in simple cartoon pictures. Nearby pixels are often alike. If
a picture varied randomly from pixel to pixel, it wouldn't look like anything. So here is
a method for representing the same picture data more efficiently. It's called run length
encoding. Instead of writing down all the zeros on the first line, we just write down
the number of zeros, 200. And for each line, we write down the number of zeros, then the
number of ones, then the number of zeros, and so on, and so on until we get to 200, the
end of the line. Some lines only need one number. Some lines need several. For this picture,
it turns out that we need just 900 numbers to do the job. Now each of the numbers can
be written using 8 bits, since 200 is 8 bits long in binary. That means that we can represent
the image with 8 times 900 or 7,200 bits, less than one-fifth of the number needed if we
code the image pixel by pixel. Run length encoding is just one of the techniques that
have been used to represent image data more efficiently. It compresses simple line drawings
and cartoons to a fraction of their original size. It does not work so well on photographs
or other images with fine textures and lots of subtle variations in color. The methods
we generally use to compress those images are quite different, not only in detail, but
also in their basic philosophy, and thereby hangs a tail.
To tell that tail, we'll start with sound. Sound is a simpler kind of information. What
we see is made up of different colors and intensities of light coming from different
directions, but sound waves reach the ear as small, rapid variations in air pressure
over time. We could represent a sound perfectly by a graph of pressure versus time. That graph
could also be the voltage put out by a microphone, which acts as a transducer, a device that
turns signals of one kind, sound waves, into signals of another kind, electrical voltage.
Now we need to turn that curve into digital data, data that we can represent by bits.
That involves two processes, sampling and quantization. In sampling, we only record the value of the
sound signal at certain discrete points in time. How many points in time? Well, telephone
systems generally sample about 8,000 times per second, which is enough to capture intelligible
human speech, but the resulting quality is not very good. For music, we want more. Now
our hearing can only detect sound waves with frequencies less than 20,000 Hz, or 20 kHz.
One hertz is one cycle per second. As we will see in lecture 9, this means that we only
need to record the wave values 40,000 times per second, 40 kHz, just twice the frequency.
Most audio for music is sampled at 44.1 kHz. We represent the sound wave as a series of
discrete wave values, 44,100 values per second. Next comes quantization. This has to do with
how accurately we record the sound wave value at each moment. Just as we replaced the continuous
time axis with a discrete set of sampled times, now we are replacing the continuous
pressure axis with a discrete set of recordable pressures. For musical sound, we often use
16 bits of information to record the sound signal at each moment. That gives us 2 to
the 16th, or 65,536, different possible levels. So the original sound wave becomes 44,100
samples per second, each one of which consists of 16 bits of sound level information. That's
over 700,000 bits per second. And if we are recording in stereo, double that. One minute
of stereo music recorded in this way takes up more than 80 million bits, or 10 million
bytes. A compact disc, which can store hundreds of millions of bytes, can hold a little more
than one hour of music. Can we do better? Can we compress sound information? We think
about sound as a signal, air pressure, that varies over time. We describe it by a series
of signal levels at different times, but we can also think about sound as a combination
of frequencies. This fact was one of the most important discoveries in the history of science,
and it was due to the French physicist and mathematician Joseph Fourier in 1822. Fourier
showed that any wave form at all can be formed by mixing together sine waves, smooth waves
of a single well-defined frequency. We could describe the overall wave by telling how much
of each sine wave goes into the mix. Mathematically, we transform a wave from a function a of t,
giving the amplitude of the wave at each time t, to another function a hat of f, giving
the amplitude of the wave at each frequency f. That's called the Fourier transform.
If you have access to an acoustic piano, a grand piano works best, you can do an experiment
to demonstrate this. Open up the lid so that you can see the strings. Each string naturally
vibrates at its own frequency, which is how we play different notes on the piano. Now,
while holding the damper pedal open, make some kind of sound, say a word loudly, Fourier,
then listen. We'll hear a kind of musical hum, a sort of echo. That's because the various
frequencies in your voice have caused the corresponding strings to vibrate a little
bit, and their hum matches the tone of your voice. Speak in a high voice, and the hum
is higher in pitch. Speak in a low voice, and the hum is lower. That's because the
different sounds cause different sets of strings, different combinations of frequencies
to vibrate. If we could measure the amount of vibration set up in each one of the strings,
we would have a Fourier transform of the sound of your voice. Your sense of hearing
actually does this in an approximate way. There are tiny hair cells in your inner ear
that because of their different locations in the cochlea respond to different frequencies.
The signals from these cells give you your perception of pitch. So we can use our bits
to represent sound as a signal in time, or as a bunch of amplitudes for different frequencies.
Now that by itself doesn't save us anything. There are as many different frequencies as
there are samples in the original sound, so we need just as many numbers. But it does
allow us to make use of some of the oddities of human perception.
When we record and listen to music, we aren't really that interested in reproducing the
actual sound, the exact curve of air pressure over time. What we want to represent is the
human perception of sound. Only some of the information in the sound makes a difference
to our experience of it. So maybe we can use our bits to represent that information, the
information that makes a difference, and ignore the rest.
This is the idea behind perceptual coding, which was proposed in the early 1970s by the
German acoustical physicist Manfred Schroeder. It's a revolutionary idea. But it is very
much in keeping with Shannon's original insight into information. Information is about the
distinction between messages. Perceptual coding tries to preserve the distinction between
subjective perceptions, but not necessarily other distinctions.
For sound, perceptual coding is based on the phenomenon of masking. One sound can prevent
us from perceiving another sound, even if it is still present. For example, suppose we
have a high frequency sound like a repeating chime.
You should have heard seven chimes, each one at 1000 Hz, each one softer than the last
one. We can hear that sound even if it is quite soft. Now add in a lower frequency sound,
a tone at about 330 Hz.
When the higher pitched chime gets too soft, it is completely lost to us. This is a familiar
phenomenon. Anyone who has tried to have a conversation next to a noisy vacuum cleaner
or a busy highway or a loud waterfall will recognize it. But it is important not to misunderstand
what is happening. The chime has a completely different frequency from the tone. We should
be able to hear them as separate sounds, but we do not. One sound covers up the other one.
That's masking.
Masking can occur between different frequencies, and also between different times. A loud sound
can prevent us from perceiving a softer sound that occurs just after it, or indeed just
before it. So we have both frequency masking and temporal masking.
Here is how perceptual coding for sound works. We have a mathematical model of human perception,
including masking effects. We analyze the sound using the Fourier transform and we figure
out what parts of the sound will be perceived very clearly. These parts we code very carefully
using lots of bits, but other parts of the sound will be somewhat masked, so our code
represents those parts very crudely with only a few bits. It sounds complicated, and it
is, but the results are dramatic. One common example of this is the MP3 sound format, which
is a complex code for sound that is often used in portable music players. This code
for sound was originally developed in the 1980s as part of a scheme for compressing video
data. The MP stands for moving picture, as we will see a little later. MP3 uses perceptual
coding to achieve an amazing degree of data compression. Recorded music in its uncompressed
form uses about 10 million bytes, 10 megabytes per minute. An MP3 version uses about 1 megabyte
per minute. That means we can store 10 times as much music in the same memory space, or
transmit 10 times as much in the same amount of time.
We do understand, of course, that an MP3 file is not a perfect representation of the original
sound wave. An expert listener, under good conditions, can tell the difference between
uncompressed audio and MP3 audio. And the same is true of other perceptual coding systems
like the Advanced Audio Coding AAC format, but for the most part these systems do an
excellent job of efficiently coding, not the original sound, but our subjective experience
of the sound. Perceptual coding is also the key to compressing visual data. So let's
once again consider a picture, but instead of the simple cartoon we began with, we'll
look at a more complicated image. A good quality digital photograph contains millions of pixels.
My phone takes pictures with 5 million. And the information contained in one pixel is
much more than one bit. Each pixel can be any color, any brightness. So how much information
is in a pixel? Our color vision depends on the cone cells of the retina of our eyes.
These come in three different varieties, each one sensitive to a different range of light
wavelength. Thus every color that we perceive is a mixture of three basic colors, red, green,
and blue. To describe a visual color, therefore, we need to specify three numbers, the intensities
for the three basic colors. One very common system is to have these three numbers each
be 8 bits long, one byte, so that the red, green, and blue levels are between 0 and 255.
Black, for instance, is 0000, no intensity for any of the colors. Red is 255, 00, full
intensity for red, but not green or blue. Yellow is a mixture of red and green, so we
can use 255, 255, 0 for that, and bright white would be 255, 255, 255, with all three colors
at their maximum intensity. So we can represent color by three bytes, 24 bits of information.
This gives us two to the 24th power, or about 16 million possible colors, which is a lot.
Human vision can actually only distinguish about 10 million distinct colors and shades,
so the 24-bit system is sometimes called true color, and it's almost always good enough.
So 5 million pixels at three bytes per pixel is 15 million bytes, over 100 million bits
for a single image. Clearly, serious data compression would be very useful. To achieve
it, we once again must turn to perceptual coding. For visual data, masking is not the key factor.
More important is to understand how we perceive detail in the image. Again, the way to approach
that question is to use the Fourier transform. It will be a little easier to discuss images
that have just one number per pixel, the intensity of light at that point. These are pictures
that are made up of lighter and darker shades of gray, like an old-fashioned photograph.
If you like, think of a color photograph as being made up of three gray-scale photographs,
one for red, one for green, and one for blue. So an image is essentially a function, light
intensity, of two variables, the X and Y coordinates of the pixels. Within the picture, what does
it mean to take the Fourier transform of something like that? A sound wave is one-dimensional.
It is a function of one variable, time. That wave is made up of simple functions, sine waves,
each with a different frequency. The Fourier transform gives the amplitude for each frequency,
so that is another one-dimensional object, amplitude as a function of frequency.
We want to think of our two-dimensional image as made up of simple sine wave pictures, except
that we will need two frequencies, one for X and one for Y. So here are a few of those
sine wave pictures. If both frequencies are zero, the sine wave picture will just be a
uniform featureless gray color. If we go to a higher X frequency, then we see that the
image has a repeating pattern of bands from left to right. If we also add a Y frequency,
then there is also a repeating pattern from top to bottom. It turns out that any picture
whatsoever can be expressed as a combination of sine wave pictures. Each combination of
spatial frequencies will have its own amplitude, and that description of an image in terms
of its two spatial frequencies is the Fourier transform of the image. So suppose we start
with an image and examine its Fourier transform. What do we discover? The first thing we find
out is that for most meaningful images, not just random jumbles of pixels, the amplitudes
for the lower spatial frequencies are larger. That's because those low frequencies represent
the overall arrangement of the image. Our image may be lighter in this corner, and there's
a big dark area in the middle. The fine detail of the picture, though, lies in the high frequencies,
and that makes sense. In those low-frequency sine wave pictures, nearby pixels have nearly
the same value. To get differences between nearby pixels, what we need to make fine detail,
we must include the sine waves with higher frequencies. So what happens to the image
if we just leave out the high frequencies? That is, we take the Fourier transform of
the image, set all the high frequencies to zero, and then rebuild the original. What
that does is it simply makes the original image blurry. The overall arrangement is the
same, but all the fine detail has been lost. Sharp edges become smooth gradients. The fine
detail does affect our perception of the image, but it does not affect our perception as much
as the overall arrangement. So put it this way, if we see two images that differ in their
overall arrangement, we notice the difference at once. But if we see two images that differ
only in their fine details, then we might not notice any difference at all. Perceptual
coding is all about representing only the differences that we perceive.
So here's the essential idea. It goes back to the concept of quantization, which we introduced
when we discussed sound waves. In quantization, we turn a continuous quantity into a finite
number of bits. We decide how many different levels or possible values we will be able
to record. The more different levels, the more bits we will have to use to represent
that quantity. So we will use a lot of bits, very good accuracy, to encode the low frequency
amplitudes in our image. The overall arrangement of the image is represented exactly. With
a higher frequency amplitudes, on the other hand, we can get by with fewer bits. We can
allow ourselves to be more approximate with the details.
What I have just described is the essential idea behind the widely used JPEG format for
digital images. JPEG stands for the Joint Photographic Experts Group, which was an international
body that introduced the JPEG format in the 1990s. It applies the idea I have just described,
not to the entire image, but to individual 8x8 pixel blocks, after which it compresses
the resulting data for the whole image using a Huffman code. It's a very flexible system,
and we can adjust the quality of the coded image in effect. We can decide how accurately
to represent the finest details. Once again, the degree of data compression achievable
is astounding. A JPEG version of a photograph may be 10 times smaller than the uncompressed
image, one-tenth the number of bits, but be essentially indistinguishable to the human
eye. And it is possible to go further, compressing by a factor of 20, or 50, or 100, but the
resulting images have noticeably lower quality.
JPEG compression, as its name suggests, is designed for photographic images, and for
that purpose, it is hard to beat. In fact, many digital cameras do not even store an
uncompressed version of the pictures they take. Keeping only the JPEG version means
that the camera's memory can store hundreds or thousands of images at a time, a whole
vacation trip worth of photos.
If data compression is important for images, it is utterly indispensable for video. A single
frame of high definition video contains about two million pixels, each with three bytes
of color information. Thirty video frames are displayed per second, at least in America,
and Europe the standard is 25 frames. That comes out to 180 million bytes of information
per second. That is dozens of times faster than my own home internet connection. At that
bit rate, even the most advanced optical discs could only store a few minutes of video data.
Without powerful compression techniques, communicating and storing video data would be entirely impractical.
That was the challenge faced by another international standards body, the Moving Picture Experts Group,
formed a few years after the JPEG organization. They designed a series of powerful and flexible
image coding formats, MPEG-1, MPEG-2, and so on, that make video data part of the information
revolution. The MPEG group also addressed audio data compression. The MP3 format was
originally just the audio part of the MPEG-1 video standard. MPEG-4, Part 10 Advanced Video
Coding, also known as the H.264 format, is the usual standard these days for streaming
video and Blu-ray discs. What you are seeing right now has probably been encoded along
the way using either that or one of the other MPEG codes.
What tricks can we use to compress video data? First of all, the frames of a video are pictures,
so we can use the perceptual coding techniques we've already discussed to compress them.
And actually, there is a video compression method called motion JPEG, which simply renders
each frame as a JPEG image. But that is not really efficient enough to do the whole job.
Video data does have one important characteristic. It is highly repetitive. Each video frame
is different from the one before, but the differences tend to be small. The objects move and the
lighting changes only a little bit. So the first strategy to compress video data is,
only store the differences between frames. Every so often we have a key frame, an image
that we encode completely, but for the frames after the key frame, we only need to store
the difference between the key frame and that new one. Most of those differences are zero
or do not affect our perception much, so we can represent the important differences using
many fewer bits. All of this illustrates a fundamental principle about coding and information.
The more we can predict, the less we have to encode. Remember our last lecture when
we discussed how to code the messages from a source into binary code words. In an efficient
code, the length of the code word is related to the surprise of the message. A more likely
message has a lower surprise and can be represented by fewer bits.
So suppose somehow we can improve our ability to predict the next message. That means that
one or a few messages now have a much higher probability and all the other possibilities
become much less likely. The entropy of the next message, the average surprise, becomes
less. And so we can use fewer bits on average to record what message actually does occur.
And this is true whether the messages are the letters of an English text or the frames
of a video.
Using all of these techniques and a few more, we can compress video data by a factor of
20 or more, with almost no noticeable degradation or loss of quality. Of course, there is that
word noticeable. Perceptual coding is a huge advance in data compression, but it works
by changing the problem. Rather than faithfully representing the original message, a sound
an image, a video, we identify which parts of the original information really matter
to us that actually affect our subjective experience. We put our efforts and resources
into conveying that part and discard the rest.
We have seen two different concepts of data compression. One concept is like Huffman coding
for the letters of the alphabet or one length encoding of the simple black and white cartoon.
From the compressed version, we can exactly recover the original. Nothing is lost. This
is lossless data compression, and sometimes that is exactly what you need. If you want
to compress data from a scientific instrument or a piece of computer software, you really
do want to make sure that the bits you recover are exactly the right bits.
But that leads us to the other kind of compression. When we compress sound using MP3 encoding
or an image using JPEG, we have only represented part of the information. The exact original
can no longer be recovered. We may recover something that is like the original in the
ways that matter to us, but something has been lost. This is lossy data compression.
One way to see this is to apply JPEG compression to a kind of image for which it is not well
suited. A diagram consisting of thin lines on a blank background or perhaps with some
small lettering often comes out very poorly after JPEG compression. The reason is that
the thin lines and the letters are details that are only preserved approximately in the
compression procedure. For such pictures, other image formats are better. We might, for
example, use the PNG format, PNG standing for Portable Network Graphics. That uses lossless
data compression and it works pretty efficiently for line drawings. In lossy data compression,
as we've seen, information is discarded deliberately as the price we are willing to pay for great
increases in efficiency. But information can also be lost in other ways. When information
is stored or communicated, errors can occur that make the original message difficult or
impossible to recover. Next time we'll tackle those issues head on. How can we measure the
loss of information in storage or transmission? And how can we use coding to defend against
the errors that result? The answers to these questions involve some of Claude Shannon's
most profound and original insights, concepts that lie at the heart of the science of information.
