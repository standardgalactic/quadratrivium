When John von Neumann was invited to give the Silliman Lectures at Yale University, he could
have chosen any number of topics, besides being one of the great mathematicians of the
20th century. He was a pioneer in sciences from quantum physics to economics. He had
invented more fields than some of us ever learned. The subject he chose for the lectures
was the computer and the brain. Computers and brains are alike in many ways. They have
inputs and outputs. They store and process information. Von Neumann did point out some
differences. The basic components of the brain, the neurons, are much slower and more imprecise.
than the electronic components of a computer. On the other hand, the brain has billions of
neurons, all working at once. Also, the brain consumes a lot less energy. The human brain uses
about 15 watts to operate. The early computer ENIAC, which von Neumann had helped to develop,
used 150,000 watts. The differences are significant. Yet thinking of the brain as a computer suggests
some interesting questions. How many bits of information do our senses provide us? What
is the neural code for the electrochemical signals among the neurons? How is memory stored
and what is the memory capacity of the brain? Von Neumann never gave his lectures. He died
in 1957. The manuscript he prepared, still incomplete, was published posthumously. Since
then, computer technology and neuroscience have made breathtaking advances. Yet Von Neumann's
little book holds up pretty well. Von Neumann was not a neuroscientist any more than Schrodinger
became a biologist when he asked what is life. But his goal was the same as our goal in this
lecture, to try to map out some connections, to learn what the principles of information
can tell us about the workings of the brain. There are two different ways to approach this
subject. Call them top down and bottom up. The top down approach is to begin with the
behavior of a subject, an animal or a human being. Do experiments on them and see how
they respond. The results are clues, indirect and often ambiguous, about what happens to
information in their brains. The bottom up approach starts with a single neuron. Ask
how does it work? What signals does it transmit? Study how neurons interact with each other.
Perhaps you monitor the activity of an individual neuron inside a live subject, testing how
changes in the outside stimulus changes the signals. These days there are other techniques
as well, EEG's, functional MRI's and so on. The point is that the top down and bottom
up approaches are complementary. They teach us different things.
Let me give you an example. My neuroscientist friend Andy studies the auditory system in
animals. Some kinds of data can be gotten by monitoring the nerve impulses sent from
the ears to the brain. But Andy is interested in how the animals perceive sounds and that
process happens within the brain. We have already encountered this idea when we discussed
perceptual coding in lecture six. To reproduce only the subjective perception of a sound
or an image we can make do with a lot fewer bits. The techniques are based on careful
experiments about our perceptions of sound and sight. So Andy spends a lot of time training
his animals to respond to auditory signals. When they hear a sound they learn to press
a lever and get a reward. When the same sound signals are given to them in different situations,
in a noisy environment or after a partial hearing loss, they will indicate whether the information
has been processed into a perception. That's a top down piece of data that would be hard
to get in a bottom up experiment. One of the earliest top down studies of information
in the brain was a classic 1956 paper by the Harvard psychologist George Miller. The magical
number seven plus or minus two. Some limits on our capacity for processing information.
Miller describes experiments by many scientists in which human subjects were taught a collection
of different possible stimuli. For example there might be some tones with different pitches.
Then one of these tones chosen at random was briefly played for the subject and he or she
would try to identify which one it was. If the number of possible tones is low, three
or four, then they got the right answer all the time. But with more tones, ten or twelve,
they made mistakes. Things become very interesting if you treat this as a problem of information
theory. The test subjects are a noisy communication channel. The tone that is played, the stimulus
S, is the transmitted message. The subject's judgment, the response R, is the received
message. Mistakes are errors in the channel from S to R. Now Shannon's theory tells
us how to measure information. The input information is the entropy H of S. That's log two of
NS where NS is the number of possible stimuli, the tones. The amount of information conveyed
by that channel is the mutual information I of S and R. That's H of S minus the conditional
entropy of S given R. So this is how much information gets through. We try the experiment
many times with different numbers of tones, estimating I of SR from the results. Then
we summarize the results by plotting I of SR versus H of S. That is the amount of information
that actually gets through versus the total amount of information in the tone. Now for
small values of NS, the subjects never make mistakes, and so I of SR equals H of S. As
H of S increases, errors creep in, and so I of SR is less than H of S. The curve bends
over and runs horizontally. It maxes out at around 2.5 bits. After that, using more different
tones just leads to more errors. And that's exactly what we would expect if the information
capacity of the channel, the respondents ability to distinguish tones, were about 2.5 bits.
2.5 bits is about log two of six, so we can reliably distinguish about six tones. More
than that, and we start making mistakes. That seems shockingly low. Surely our ears
are providing us with more information than that, but something about the way we process
that information in this simple task limits our capacity. People with a perfect sense
of pitch would presumably have a higher capacity, but for the rest of us, 2.5 bits is about
it. You can do this experiment with other types of stimuli. Maybe you make the tones
differ in loudness rather than pitch. Exactly the same kind of curve emerges, except the
plateau, the capacity is around 2.3 bits. That's around log two of five. We can reliably
distinguish about five levels of loudness. And in another experiment, they used visual
stimuli, the placement of a dot between two markers on a horizontal line. The different
stimuli were different locations for the dot. We are evidently better at visual information.
That capacity is around 3.25 bits, a little more than log two of nine. Each of these involves
a one-dimensional stimulus, a stimulus that varies in only pitch or only loudness or only
horizontal location. And now we understand the title of Miller's paper. In each of these
one-dimensional experiments, we can distinguish between five and nine, seven plus or minus
two, distinct stimuli. That's our capacity. And in general, the same story applies to
all kinds of sound, vision, taste, and touch experiments.
What about more dimensions? We might ask our subjects to distinguish points in a square.
The points differ in both horizontal and vertical position. The capacity then goes up to around
4.6 bits, more, but not quite twice as much.
Now the simple information capacity picture that George Miller painted has become far
more complicated over time with more experiments of different kinds. Yet it does appear that
our brains are well adapted to make rapid judgments involving only a few bits of perceptual
information. Our neural system allows us to quickly and reliably distinguish about seven
things, plus or minus two.
These experiments do not tell us how the brain processes information. For that, we need to
switch from a top down to a bottom up approach. So let's zoom in and start thinking about
neurons. Neurons are the basic units for information processing and communication in the nervous
system. They're found in pretty much every type of animal except sponges.
The general layout of a neuron is like this. There is a main body called the soma. It has
a bunch of little branching appendages called dendrites. There is a long fiber called an
axon that stretches out. By the way, when I say long, I could mean very long indeed.
In peripheral nerves outside the brain, like the motor and sensory nerves for your hands
and your feet, the axon of a single neuron might be a meter long. Most neurons are much
shorter.
So down the axon are some more branches called synaptic terminals, and these touch the dendrites
of other neurons.
Also here's how the system works. The dendrites pick up chemical signals called neurotransmitters
from other neurons. Some of these signals stimulate the neuron, others inhibit it. But
when the overall effect crosses a threshold called the action potential, an electrochemical
pulse is produced and travels down the axon, causing those synaptic terminals to send signals
to other neurons.
The soma, which puts together all the inputs from the dendrites, does that kind of information
processing as it determines whether or not a pulse is produced. The axon is a transmission
line for the output signal to be carried to other neurons.
That axon signal is not an electrical signal. It does involve changes in electric potential
so we can detect it electrically, which is convenient. But it is actually a propagating
change in the chemical concentrations of sodium and potassium ions across the outer membrane.
It's like a stadium wave at a sporting event. One person stands up, the next person stands
up, the person after that stands up. By that time maybe the first person is already sitting
back down. But the wave, the region where people are standing up, goes zooming through
the crowd. Nobody is actually running along with it. Only the information is traveling.
The information speed in the axon ionic stadium wave is not that fast, maybe tens of meters
per second. The nerves that carry pain information are tuned to go a little faster so we can
snatch our hand back quicker from that hot stove.
This means that neuron signals are roughly a million times slower than electrical signals
in wires. If we were to place ourselves at one point along the axon and measure the electric
potential over time, it would look like this. Each of those spikes represents a pulse going
past. They only last for a millisecond or two. The interesting thing is that the details
of the amplitude and shape of the spike do not appear to matter. In fact, those details
might change for our signal from one end of the axon to the other. But as far as neural
information is concerned, a spike is a spike is a spike. So that information is surprisingly
binary. There is a spike, or there is not a spike. The only question is when the spikes
occur. And even there it appears that timing details finer than about one millisecond are
usually unimportant.
Therefore we can regard information transmitted down the axon as a discrete series of bits.
Each bit represents one millisecond of time. When that millisecond contains a spike, that's
a one. Otherwise, we have a zero. Furthermore, since a neuron generally rests for a few milliseconds
between pulses, the ones in this binary string are usually separated by several zeros.
So now we know what neural information looks like. It's a stream of binary digits, mostly
zeros. They are generated at around a thousand bits per second, and they travel down the
axon. But what do those bits mean? What is the neural code?
Well, the first clue to this actually goes back a long time to 1926 when Edgar Adrien
and Ingva Zotterman at Cambridge did an experiment on muscle tissue taken from a frog. They monitored
the impulses from a single sensory nerve fiber in the muscle, while they pulled at the tissue
with small weights. They found that the rate of pulses was determined by the amount of
pulling force. Let's say a bigger force produced more frequent spikes. A one-eighth
gram weight produced about 20 pulses per second. A half gram weight produced about 50, while
a three gram weight produced more than 100. If a steady force is applied, the pulse frequencies
decrease over time. This is called adaptation. A constant stimulus produces a diminishing
sensory response. But the main point remains. In a sensory nerve, the rate of the pulses
represents the strength of the stimulus. That idea is called rate coding, and it appears
to be the basic neural code by which many kinds of neurons, including many within the
brain, communicate. So rate coding is interesting because the individual spikes do not carry
the neural message. The message is encoded in a whole train of spikes. So the typical
time interval required for a neural message has to be fairly long. The information is
encoded in the number of spikes that occur in that interval. That means that these two
code words are equivalent because they each have nine ones. That is, nine pulses in the
sequence. How long is the time scale for an actual neural message? The time for a single
rate-coded piece of information to be conveyed? Well, it can't be too long because we know
that we can receive and act upon sensory data in a fraction of a second. And it can't
be too short because many pulses have to be received for the rate to be measured. So
let's suppose that this time scale is around 200 milliseconds, one-fifth of a second. The
pulse rate for a neuron almost never exceeds 100 pulses per second. That's 10 milliseconds
per pulse. So the number of pulses we expect to find in 200 milliseconds is between zero
and 20. Those 21 possibilities correspond to 21 different measured pulse frequencies,
the code words in our rate-based neural code. 21 code words is enough for, let's see,
log 2 of 21, about 4.4 bits per message. At 5 messages per second, that's 22 bits per
second. However, that number is probably too high. The pulses come along somewhat randomly,
the intervals between them are irregular. This means that by chance, in a given interval
of time, we might have more or fewer pulses. If the expected rate is 9 pulses, the interval
might actually contain only 6 pulses, or 11. We physicists call this variation in the number
of discrete random events, shot noise. Shot noise means that two similar pulse rates,
9 pulses per interval, verses 8, say, cannot be reliably distinguished. Therefore, because
of noise, the information capacity of the neuron channel is lower, probably 10 bits
per second or less. Now several things mitigate this. For one thing, if you have several neurons
carrying the same information, producing pulses at the same expected rate, then by combining
the signals from all of them, you can make a faster and more accurate determination of
that rate. And also, different neurons can carry complementary information about the
same thing. We saw this in lecture 6 when we described hearing. Because of their locations
within the inner ear, the different hair cells are sensitive to different frequencies, so
the pulse rate for a particular auditory neuron will be large for one frequency, but much
lower for frequencies, higher or lower than that. That's called the tuning curve for that
neuron. But different neurons will have different tuning curves that will peak at different
sound frequencies. Even if each neuron only provides a few bits of information, each piece
of information is about a different frequency range. The combination of all of them provides
a pretty detailed description of the sound. Notice how much of this is parallel to the
perceptual coding of audio data, which we discussed in lecture 6. In the MP3 code, for
example, the sound signal for a short interval of time is broken up into frequency bands.
Some frequencies are coded with only a few bits. Our auditory system evidently has its
own internal MP3 code for sound. Neuron tuning curves show up in some surprising
ways. For instance, the visual cortex is the part of the brain that processes visual information.
So suppose you are looking at a picture of parallel stripes. There are cells in the primary
visual cortex that respond to a particular angle of the stripes. Those neurons combine
data from many parts of the retina to compute a simple function, and other cells respond
to different angles, or to edges, or to movement, or to shapes. There are millions of such cells
in the visual cortex, and they are a big part of how we integrate data from our eyes into
visual perception.
Now we come into more controversial territory. Is rate coding the whole story about the neural
code? The alternative is called temporal coding. In temporal coding, the actual times of particular
pulses, or more precisely, the times between adjacent pulses, make a difference in the
message. So take a look at those two binary pulse sequences we saw earlier. Both have
nine pulses. In rate coding, they are equivalent code words, but the pulses occur at different
times, so they are distinct code words in temporal coding. Therefore, the neural information
rate would be higher if neurons use temporal coding.
Now there are several pieces of data to suggest that precise timing does matter in the brain.
For instance, suppose I shut my eyes and listen to a sound from different points. I can tell
which direction the sound comes from. Now some of that is based on different loudnesses
in my two ears, but experiments prove that most of that sense of sound localization comes
from the difference in arrival time of the sound waves at my ears. The difference in
arrival time is exceedingly small, much less than one millisecond, so tiny timing variations
must somehow give rise to information in my brain.
Here's another more indirect point. The nervous system transmits pulse timing information
very exactly. The pulses do not get closer together or farther apart as they travel.
This is not evidence that this timing information is used in the neural code, but if it isn't
being used, why does the nervous system preserve it so well?
Also, if the exact timing of pulses was literally meaningless, then there would be no reason
for different neurons to fire at the same moment. They could produce their pulses independently.
They might occasionally produce simultaneous pulses by chance, but that would represent
nothing special in the neural information network. However, there are many experiments
showing that neurons can become synchronized. They fire simultaneously, much more often
than coincidence allows. Once again, that does not prove that synchronization is part
of the neural code. But if not, why does it happen so often?
There are many other observations and experiments of the same sort, all tending to show that
precise timing makes a difference. So the present consensus is that temporal coding
does play a role in the neural code at least in certain places. The question is, in how
many places? And how does the temporal coding work? Neuroscientists are still figuring that
out. But in any case, temporal coding, in which messages can be carried by individual pulses,
greatly increases our estimate of how much information is being exchanged by the neurons
in the working brain.
The brain does not just process and transmit information. It also stores it sometime for
years and decades. That is the phenomenon of memory, which raises a great many additional
information questions about the brain. What is memory in the brain?
This doesn't have a single answer. The brain appears to have several different memory systems.
They store different kinds of information in different ways for different periods of time.
The first is sensory memory, which puts together the fragmentary moment-by-moment inputs from
the visual and auditory systems into unified impressions. It stores a lot of detail, a
lot of bits, and it does not appear to be selective. In other words, it does not matter
whether or not you are paying attention to something. If that something affects your
senses, it goes into sensory memory. But sensory memory only lasts a moment, and then it is
gone.
The next system is sometimes called short-term memory. It is linked to your attention, and
it lasts much longer than sensory memory, perhaps 15 seconds. Short-term memory is very
useful for performing tasks, which is why it is also called working memory. If you give
me a phone number, I can write it down using my short-term memory. But five minutes or
five hours from now, I won't recall much about it.
Finally, there is long-term memory. Information that is stored by this system can be recalled
days or years later. Indeed, barring actual brain damage, this information may last a
lifetime. Sometimes we may not be able to access a particular memory right away. Memory access
is a whole different and interesting question, but long-term memory storage involves a permanent
change to the brain.
There appears to be an information filter at each stage. Only some sensory memory information
goes into short-term memory. Only some short-term memory information goes into long-term memory.
Also the very different time scales over which the systems operate indicate that the physical
mechanism is different in each case.
Short-term memory is very likely stored in self-sustaining patterns of neural signals
within the sensory areas of the brain. These patterns are very transitory and are almost
immediately changed to new patterns based on new sense data.
Short-term memory appears to be the business of the prefrontal cortex, the executive center
at the front of the brain. That is where our conscious attention arises, which makes sense
given the connection between attention and short-term memory.
How is the information represented there? One leading theory is that patterns of nerve
impulses lead to patterns of localized depletion of neurotransmitters. Remember, neurotransmitters
are the chemical signals that bridge the gaps, the synapses, between one neuron and the next.
So if a particular neuron has been involved in a lot of activity, its supply of those
molecules will be temporarily reduced. Therefore, the pattern of pulse activity leaves behind
a chemical footprint. That seems to be the way short-term memory is represented.
And after a short while, the cells do get resupplied. So unless it is renewed by additional
activity, that memory trace fades away.
That brings us to long-term memory. Long-term memories of events in our lives or of abstract
concepts seem to rely on the hippocampus, a pair of curved structures deep inside the brain.
The name, which means seahorse, gives you an idea of what these structures look like.
The hippocampus contains a lot of neurons, with a lot of connections, and it is linked
to the prefrontal cortex and to many other parts of the brain.
Information is represented in the connections between neurons. And like muscles, those connections
are strengthened with use. That process is called long-term potentiation. So repeated
activation of a given set of connections will cause them to be permanently reinforced.
That's long-term memory.
We know for sure that this happens in the hippocampus. Some memories are actually stored
there, like our memories of spatial information. Other memories appear to be located somewhere
else, but the hippocampus somehow assists in their formation.
The hippocampus does not seem to be involved in storing other kinds of long-term data,
like emotional responses, or how to ride a bicycle. But this information is also encoded
in the strengths of synaptic connections in various parts of the brain.
Now when we recall those memories, we use those connections. But notice, that's how
we form memories in the first place. So in our memory system, the information is read
using the same process by which it was written. Every time we access a memory, we are rewriting
it, tracing the pen over the letters on the page. But in that process, sometimes the memory
information can be altered. This is why it is possible, and even fairly common, to have
false memories, memories of events that did not happen quite the way we remember. Our
continually rewritten memory records can be very imperfect.
So how many memories can we store? What is the information capacity in bits of our memory
system? The psychologist Thomas Landauer at the University of Colorado did some experiments
on memory in human volunteers. We would classify these as top-down experiments. He exposed
the volunteers to various kinds of information, and then much later asked them some true-false
questions about it. As we saw in lecture 7, a true-false test is a good way to measure
information.
Based on his experiments, Landauer concluded that humans assimilate information into long-term
memory at a rate of about 2 bits per second. That's all. It doesn't sound like much.
30 years equals a billion seconds, so a person might have 2 billion seconds of waking life.
According to Landauer's estimate, that's only 4 billion bits of memory data. That's
less information than is stored on a single audio CD.
It seems certain that Landauer's estimate is much too low. His experiments were very
artificial and only tested a particular type of memory.
At the other extreme, John von Neumann made a wildly generous estimate of memory capacity.
He accepted the idea, which was common in his day, that we never actually forget anything.
Every sight, every sound, every thought in our head is recorded perfectly. So he calculated
that a complete record of the brain's neural impulses for a lifetime would constitute about
3 times 10 to the 20th bits of information.
We know better now. We know that our memory is not just a data recorder for the whole
brain. Memories are constructed, and only a portion of our experience is stored. A better
answer than either of these would be a bottom-up estimate based on the number of synaptic connections
in the brain. Those connections are the basic underlying mechanism for memory.
There are about 10 to the 10th neurons, each of which connects to around 1,000 other neurons.
That's at least 10 to the 13th connections, a terabyte of data. But the varying arrangements
and strengths of those connections multiply that number considerably. One recent estimate
of the information content of our brain network is on the order of 10 to the 16 bits, in fact
2.5 times 10 to the 16th bits. That's 2.5 petabytes, or more than 2 or 20 million billion bits.
That is an enormous information capacity. And in fact, there is no evidence that even
in a long human lifetime, we ever approach any limit to our ability to record new information
in our memory system. The memory difficulties some people experience always have particular
physiological causes. They are never about the underlying information capacity. And that
is an inspiring thought. You and I always have room to learn a little more.
There are still a great many unanswered questions about this magnificent information system
of ours, the brain. How can we bridge the gap between the top-down and bottom-up approaches,
the world of perception and experience and external behavior on the one hand, and the
intricacies of neural codes and long-term potential on the other? With models and experiments
and with ideas drawn from the science of information, the next couple of decades of
neuroscience research may tell.
