Hi, welcome back. We've been talking about problem solving, and in the previous lecture
we covered how diverse perspectives, the representations, could improve problem solving
in two ways. They could be transcendent. Remember Pope's characterization of Newton
turning darkness to light, and they could also expand the set of the adjacent possible,
providing more incremental improvements to solutions. In this lecture we're going to
consider how once someone has a perspective on a problem, how they go about trying to
find improvements within that perspective. These are what we're going to call heuristics.
Now formally heuristics are algorithms, techniques, and rules of thumb. They're the tricks that
we use to solve and find better solutions to problems. Now over the course of our lives
we're going to accumulate hundreds, perhaps even thousands of heuristics that we then
take and apply to problems as they arise. Now these heuristics, along with our perspectives,
comprise a big portion of what you can think of as your cognitive toolbox in terms of what
you apply to solving problems. Now in this lecture there's five parts. There's a lot
going on. First what I want to do is introduce some rudimentary heuristics and show how
diverse heuristics also expand the set of the adjacent possible. Second I want to demonstrate
the two ways that heuristics produce what some people metaphorically refer to as out-of-the-box
thinking, or outside-the-box thinking. And third I'm going to describe two sophisticated
computational heuristics. The first is called simulated annealing, and this mimics the annealing
process that's used to make metals and glasses. And the second I'm going to call drowning
ants, which should borrow some ideas from evolutionary theory. All right, fourth, again
a lot going on, I'm going to demonstrate how organizations and societies use heuristics
that share many of these same properties, the same ones that the sophisticated computational
heuristics have. And finally last, I'm just going to take a moment to comment on the relative
abilities of people and computers to solve problems. And I'm going to conclude that when
it comes to heuristics that well maybe computers have a leg up, but fear not, we've got one
big advantage and I'll get to that. I'm going to begin by describing some simple
heuristics. These are sometimes called local heuristics. So remember back two lectures
ago when I was discussing the routes through Rome. In that example, switching the order
of the locations in the route was the simple heuristic. So what you could do is you could
think of characterizing a solution as just an ordering of the various locations. So remember
the original route was the Forum, the Colosseum, the Pantheon, the Castle St. Angelo, the Temple
of Hadrian, and then concluding with the Arch of Constantine. Well we can write this as
a list. So here it is, this is just a simple ordering of the places we visit. And so when
we switch the order of the routes, what we're doing is sort of we can see on the map how
that makes the route either shorter or longer. Now notice that this simple switching algorithm
can be applied to any problem for which the solution can be written as an ordered list.
So for example, suppose you want a business that makes small wooden coasters for coffee
cups. Now these coasters entail gluing together small leftover pieces of wood cut to size.
So your process could be written as a list as follows. Cut, glue, sand, and then varnish.
But you could apply our simple heuristic and you could switch the order. It could be cut,
then sand, then glue it together, and then varnish. And you might find that this new
order saves you time and money because in the past the sanding might have had to wait
until the glue dried, right? So this seems like it's an improvement and so this little
heuristic of switching the order works. But it's not always going to work. So suppose
you'd apply it again. Now you might get the order cut, varnish, glue, sand. Well that's
not going to work at all because you're going to be sanding wood that's already varnished.
So heuristics, like perspectives, sometimes they work and sometimes they don't and that's
what's really useful to have a whole bunch of them. Now computer scientists classify
the switch the order heuristic as a local heuristic. Remember I just introduced this
as a local heuristic because what it does is it introduces a new solution that's near
the existing solution. It's in what's called the neighborhood of the existing solution.
There's a whole bunch of local heuristics. These include what are called greedy algorithms.
What a greedy algorithm does is it looks at solutions near the current point and chooses
the very best one. It then looks at all the solutions near that new point and chooses the
best one among those. Let me show you how it works. Let's suppose we're on a rugged landscape.
Now we're sitting right here at the bottom of this hill and what we're going to do is
we're going to look in a neighborhood of that point. So we're going to look sort of in this
little neighborhood of that point like this and we want to ask what's the best way to
go. There's two directions up. We could go this way or we can go this way to the left
or to the right. But it looks out to the left is a little bit steeper so we're going to
head in this direction. So a greedy heuristic is going to go left. Now greedy algorithms
can be contrasted with improving algorithms. An improving algorithm is going to move to
a new solution if it's better but it's not necessarily going to go to the best solution.
Okay now you might ask why wouldn't you always move to the best solution? Why would you just
take any old improving solution and you could take the best one? Okay ignore the fact of
that old line don't settle for anything less than the very best. That's an advertisement.
Let's look at the logic on its merits. You might not move to the very best point that's
nearby. You might not be greedy using the language of computer science because it takes
too much time. If you find an improvement take it. Why bother wasting time proving that
it's the best one? By the time you've proven it's the best one you might have climbed even
further just by taking a series of modest improvements. The second reason can be best
explained with a picture. Let's suppose I've got a landscape that looks like this and I'm
at a current solution that's at the bottom of this hill. Well now if I use a greedy algorithm
I'm going to climb the steeper hill which is going to be to climb this one but that's
the smaller hill. In general you have no guarantee that the steeper hill is going to be taller
in the end result. So for example in this graph this has this peak has a more gentle
slope but it's also taller. So what we want to think about is any algorithm will work
in some cases and it might not work in others. Why not? Well suppose you and I are standing
in Ann Arbor where I live and I say hey I'll race you to Chicago. If you followed a greedy
algorithm what would you do? Well you'd break out in a fast sprint heading west because
that's the fastest way to get closer to Chicago. But in fact you might do better to walk to
your car which might be east of Ann Arbor and then drive to Chicago. Even better once
you've gotten your car you should probably drive east in the opposite direction of Chicago,
go to metro airport in Detroit and then fly to Chicago. Okay I know that's a silly example
but I want to make clear that what's locally best just heading off toward Chicago need
not be the part of the best solution. Let's return to this general idea of problem solving.
Now to put a little bit more structure on the process let's suppose that our solution
has multiple dimensions or attributes and we're trying to come up let's say with a new web
application. So these attributes could be the links on the main page or if we're designing
a new Swiss Army knife these could be tools, blades, openers, scissors, etc. What we can
do is we can think of the status quo solution as some sort of vector, you know x1, x2 up
to xn. What a local heuristic does is changes only a small number of those attributes it
might only change them a little bit. So remember from our last lecture those one bit mutations
that's a local algorithm that's a local heuristic. Making these small changes attribute by attribute
is sort of the canonical local heuristic. So if I had a jackknife that had the attributes
big knife, little knife, screwdriver, scissors, opener I could change the scissors maybe to
a wine opener that's a local heuristic. Now I can use this simple mathematical formalism
to describe two types of what people call outside the box thinking and each of these
is going to rely on a different construction of the box. Let me do an example suppose we
have a jelly company and we're trying to improve the sales of our grape jelly. Our current
product can be thought of as a list of attributes, sugar, content, chunkiness, etc. Or it can
be thought of as a recipe in order to list of instructions for making the jam. We might
be all sitting around trying to think about how do we make some small change in the product
to improve its taste or make it more marketable. Just get a bigger market share. Then someone
might say hey I know let's make our grape jelly green. After all grapes are green. Fantastic.
That's out of the box isn't it? Well maybe it's only out of the box if for someone who's
reason we had ruled out changing the color of the jam. If color was off limits. If so
the box which we're thinking of didn't contain the green jam it's outside the box. Most likely
it's the case that color became off limits because we stopped thinking about it. Remember
a few lectures back to talk about green ketchup how Heinz came out with green ketchup. Again
that seemed way out of the box but maybe it wasn't. Tomatoes can be green or red but
green became out of the box so to speak because people stopped thinking about color. Well here's
how we can represent that. Our current solution has a long list of attributes x1, x2, x3, x4
so on right like I talked about. Well what we can do is we can say look the first ten
of these are inside the box. These are the things we're thinking about but maybe the
last five are outside the box. These are the attributes we forgot we could change like the
color of the jelly and the color of the ketchup. So if anybody thinks about changing them they
seem really innovative. They seem outside the box. Why are we calling it outside the box anyway?
There's actually a geometric reason. Let me explain. Suppose there's three dimensions so
we've got three dimensions to our problem that we're thinking about changing. So these could
be muffins and muffins might have fat on one axis. They might have sugar content on another
axis and they might have just a size. They could be big muffins or small muffins. So these
are three dimensions. Well we can represent those three dimensions as a box. Fats on one
axis, sugars on the other, size on the third. That creates literally a box. So if someone
says what about adding more fiber? Fiber's not part of our box. So if we've got this box
here, fiber's on the outside. It's outside our box. So many of the outside the box heuristics
that people use involve exactly that. Changing something that was thought to be fixed that
nobody ever thought about. Okay that's one type of outside the box heuristic and there's
another type and that relies on non-local heuristics. So these are heuristics that take
big jumps in the space. They jump outside of what people thought of as reasonable. So
remember in a local heuristic we moved really close. In a long leap heuristic what we're
doing is taking a giant step in the space of possibilities. Perhaps the most famous
long leap heuristic goes by the name do the opposite. In other words do the opposite of
the current solution. Not on every attribute necessarily but on some, some subset of attributes.
Now if you move one attribute a long way or a couple attributes a long way you're really
taking a huge leap. Well what are examples of do the opposite? Well think about how Google
took over search. Prior to Google what people did is they credit links that told people
where to search. Right so people did research and said this is where people should search.
What was Google's big idea? It was actually to use people's searches to tell experts where
to link. So it's the exact opposite. Google gathers information of what people think and
that tells the experts where to go. Here's another example. The online company Price
Line also does the opposite. How so? Well normally when you go to hotel they tell us
how much we're going to have to pay to spend the night in the room and we can either accept
it or refuse it. But when I go on Price Line the opposite occurs. I tell the hotel how much
I'm willing to pay and they can either accept or refuse. It's the exact opposite. Now the
most famous example of do the opposite occurred on a Seinfeld episode. George who's sort of
a continual failure realizes every decision he's ever made has been wrong. So he does
the opposite. What happens? Well at the end of the show he's got a job with the New York
Yankees and he's got a girlfriend. Not bad. He won by getting out of the box. Okay here's
one more. How do you open a banana? So here's a banana. Well you do it by opening on the
stem but it's not easy. Well let's try the opposite. Let's just pinch the bottom of the
banana and pull it apart. And guess what? It works a lot better. It's easier to open
the banana from the bottom. Oh by the way how do you think monkeys open bananas? That's
right by the bottom.
Within any profession there exists a vast collection of heuristics. Engineers have heuristics,
farmers have heuristics, so do hairstylists, nuclear physicists, even dog trainers. And
they develop these heuristics to fit the problems that they confront. For example mathematicians
have a whole bunch of heuristics they use to solve problems. And a favorite of mine is
adding zero. Now remember a couple lectures back when I was solving one of those really
complicated algebraic equations I added zero. Now why did I do that? I did that because
if I take any number plus zero and I add zero to it I get the same number back. Now how can
that help us? It turns out it helps us because it can simplify expressions. Let me show you
again exactly how this works. Let's suppose I'm taking an algebra class and I get this
fairly complicated expression. X to the fourth plus 4x cubed plus 6x squared plus 4x plus
6. What I want to do is I want to add zero to this, but a specific form of zero. What
I'm going to do is I'm going to subtract 5 and then add 5. So if I do that I'm going
to put a minus 5 here and then a plus 5. That's just zero, so I haven't changed this
in any way. Well if I do this I can take the first part of this expression and rewrite
it. It's just going to be x to the fourth plus 4x cubed plus 6x squared plus 4x. Now
here's the tricky part. I'm going to take the 6 minus 5 and just write that as 1 and
then I've got another plus 5 here at the end. Now why did I do that? I did that because
this first expression, x to the fourth plus 4x cubed plus 6x squared plus 4x plus 1, that's
just equal to x plus 1 to the fourth power. Now I bring down my plus 5 and I've greatly
simplified the expression. So this is much simpler, the thing at the bottom than the
thing I had at the top. Now this idea that mathematicians, people who play with numbers,
they use heuristics, that's not going to surprise anybody. But what I want to convince you of
that almost every profession does the same thing. Let's take dog trainers. Now you might
ask how can I represent what a dog trainer does using heuristics? Well first of all recognize
this, dog trainers are really people trainers. So to talk about heuristics first I've got
to think about the perspective that dog trainers have. So a dog trainer is going to represent
a person as a vector of behaviors. And each behavior is going to correspond to a domain
of interaction that you have with your dog. How do you interact with your dog when you
walk, when you play, when you're just hanging around the house, when you feed your dog,
when you call or that sort of stuff. Now one heuristic that dog trainers will have is consistency
because they'll recognize you have to act in the same manner to the dog in all settings
because this simplifies the learning environment for the dog and well for you. Now of course
it's the case that a dog trainer's problems are going to be different than a mathematician.
So a dog trainer is not going to think hey maybe I should add zero in some creative way.
That's not going to make any sense. So heuristics aren't always going to transfer from one domain
to another. Although they sometimes do and when they do we have something that's called
acceptation. So acceptation, that's when things like feathers evolve to keep birds warm but
eventually allow them to fly. Remember from the last lecture? With heuristics acceptation
occurs when the heuristic developed for one purpose gets applied to another and many famous
inventions arise through acceptation. This suggests one good way to get good at innovative.
Interact with people who are different from you. So if you want a new trick for solving
a problem you should interact with people who develop different tricks. So even though
there's no guarantee that this heuristic developed for a different profession is going to work
for you, it's got a chance. That's all we can ask. In fact think back to my first lecture
when I talked about people in cities are more productive because they bumped into more ideas.
What they're bumping into is heuristics. Here's one example from my life where I learned
a heuristic that I exacted for my own use. The people who built our house would take
regular breaks during which they would just sit quietly and think about what they'd done
and were about to do. Now why did they do this? Well they did this because these short
breaks prevented major mistakes. Give them a moment to sit back and look and make sure
everything was going okay. So I incorporated that same heuristic when I was writing these
lectures. I would write for about a half hour and then I take a regular break of some period
just to allow myself to think about what I'd written, what I was about to write and make
sure it all hung together. You can be the judge of whether or not that worked.
Okay, those are simple heuristics. Let's talk about sophisticated heuristics. So the
heuristics I've talked about so far, switching heuristics, greedy algorithms, improving algorithms,
doing the opposite, adding zero. These probably haven't blown you away with their sophistication.
Fair enough. Hey, but we're just warming up here, right? I'm now going to introduce
two very sophisticated heuristics. The first is called simulated annealing. Let's think
back a moment to our discussion of local heuristics and greedy heuristics and this
idea of climbing on rugged landscapes. Either type of heuristic is going to get stuck on
a little hill on these small hills. So I think of a rugged landscape that's going to have
lots of peaks. Because what does a greedy algorithm do? It just climbs up a peak and
then it's going to get stuck. Well that's not going to be effective if there's a lot
of little hills. So it would be better if we could somehow smooth out those peaks. So
how do you do it? Well one way is to allow errors. And this is the idea that underpins
simulated annealing. Here's how it works. So you have a possible solution X. You're sitting
at some solution. Then you define a neighborhood of solutions. So these are solutions that
are close to X. They're within the box, so to speak. So given a solution, you choose
some point in the neighborhood. And now we apply two rules. First rule is this. If the
new solution is better, just move there. It's better. No problem. Second rule, if the new
solution is worse, well then actually you move there with some probability. And this
probability depends on two factors. First factor is how much worse is the new solution.
Let's call this the loss. And then a second factor, or parameter so to speak, which is
called the temperature. Now the bigger the loss, the less likely you want to move to
the new solution. That makes sense. Because you wouldn't want to move to a terrible solution,
but we might want to move to a solution that's only a little bit worse because it might help
us get off a hill. So by accepting small mistakes, what we can do is climb over little bumps
in the landscape by just going down a little bit and then possibly going up higher. Now
at some point though, we'd like to stop moving downhill. We don't want to accept any solution
that's worse. So we'd like to stop on top of a hill. That's where this temperature parameter
comes in. So in real annealing, when they're making glass or metal, when the temperature
is really high, the molecules are all dancing all over the place. And eventually they cool
it. Now in simulated annealing, when the temperature is high, it basically means, ah, you're willing
to accept quite a few mistakes. But as you anneal or cool the temperature, you become
less likely to make mistakes. Now to make this formal, computer scientists use what
they call an annealing schedule. That tells us really how quickly to cool off the temperature.
So what they have is a mathematical expression that looks something like this. And this gives
the probability of accepting an error. So this looks complicated. So let's just relax
and I'll sort of walk through it. What we have is loss here. And loss represents how
much worse the solution is. And then we have this thing called temperature, which is how
hot the system is. And what this probability stands for is the probability of accepting
a new solution with a value that's lower by the amount of loss. So this looks complicated,
but it's really not that bad. What is this? Well, what is E here, first of all? The letter
E in this expression is Euler's constant. It's equal to 2.71828. So what we have is
1 over E raised to some power. So if I've got 1 over x, if x is big, that means there's
almost no chance that I'm going to do this, because I have like 1 over something big which
is 0. But if x is close to 1, then it means that I'm going to, you know, probably accept
6. I've got something like 1 over 1. So if you look at this expression, I've got 1 over
E raised to the power loss over temperature. If the temperature is really, really big,
what that means is I've basically got 1 over E to the 0. And E to the 0 is 1, which means
I'm likely to accept almost anything. As the temperature gets lower and lower and lower,
I start getting something that looks like 1 over E to some big power, which gets close
to 0. So what this is basically saying is when temperatures are high, I accept mistakes,
but when temperature is low, I don't accept mistakes. Now the trick or the challenge in
writing in a kneeling algorithm is figuring out how quickly do I lower that temperature.
Okay. Let's remind ourselves how a kneeling works. We start with a really high temperature.
This allows us to move all about the space of solutions pretty quickly. And we take
any improvement and we accept some losses and we're less likely to accept huge losses,
but we take, you know, most losses. Slowly then we cool the process. This means that
we accept fewer and fewer losses. Eventually the temperature gets so cold that at zero
temperature we only accept improving solutions and then we're going to stop on a hill and
probably a pretty good hill. Now the kneeling schedule gives us the rate that we let that
temperature fall. And if you look in books, you'll find that different types of problems
use different cooling schedules, different kneeling schedules. So this heuristic, which
computer scientists and statisticians use on a variety of problems, mimics real and
kneeling, right, the making of glasses and metals to get ordered structures. Now this
isn't a dog trainer solving a math problem, but it's close. It's a glass blower or metal
worker providing a heuristic that gets adapted to problem solving. Pretty interesting. All
right. Let's do a second sophisticated heuristic. This is what I'm going to call drowning ads.
It's a little bit simpler, but these drowning ants heuristic works as follows. It's what's
called a population heuristic. It's called a population heuristic because instead of
having one solution that we're moving up and up, we start with a whole bunch of solutions
simultaneously and try and find the best one. So here's how it works. You randomly guess
a whole bunch of solutions and then what you do is you just apply sort of a local improving
heuristic at each one. So each one of these ants is sort of climbing up. So metaphorically,
whole bunch of ants on hills climbing up a rugged landscape. We call it drowning ants
because we imagine a flood. So we represent the flood by rising waters. So these waters
are going up and up and up. Now as soon as an ants feet get wet, what it does is it picks
up its cell phone or calls out an alarm and a boat comes and rescues it. And over time
as the floodwaters are rising, fewer and fewer ants are left on the landscape. Eventually
only one ant is going to be left and that's going to be the best one and that's the solution
that our heuristic is going to give. So it's under the need algorithm and easy to program.
So these are two sophisticated heuristics, simulated kneeling and drowning ants. And
they're kind of like computer programs. It's fine, right? But you know what? We can also
see them as organizational and institutional heuristics. These are actually very similar
to routines that are followed by groups, people within an organization, sometimes even by
whole societies that are trying to find better solutions to a problem. Now I'm going to be
honest, you're not going to find organizations that explicitly describe their problem solving
process saying we do simulated kneeling. Here's our temperature schedule. But yet if
you look closely, you'll see the procedures they follow often pretty closely resemble
the annealing algorithm. For example, think of a typical brainstorming session. How does
it work? A facilitator announces a time limit, perhaps 20 minutes. During that 20 minutes,
people throw out ideas that the facilitator writes on a giant, you know, piece of paper
or a whiteboard. And it seems to me like color markers matter a lot in these settings. So
after the 20 minutes run out, people then winnow these dozens of ideas down to five or
ten. They're not super critical, but they winnow down to five or ten. These five or ten
then get further winnow down to two or three. And then the final two or three ideas get
serious evaluation and reconfiguring, and then one gets chosen. Well, let's think about
what's going on. In the first stage, when we're writing stuff down, there's no selection.
So it's like annealing with an infinite temperature. In brainstorming, no ideas get dropped, so
there's really no selection at all. After all the ideas are out, the temperature sort
of falls, and the number of solutions falls to ten. And then the temperature falls even
more, and we get down to one solution. Well, that seems a lot like annealing, doesn't it?
In fact, it is. We can think of brainstorming as exacted annealing. Okay, what about our
drowning ants? Let's think of these ants as business startups seeking solutions to the
problem of, say, making software for my personal computer. Initially, there's lots of ants
on the landscape, lots of little startups. But over time, the waters rise, and only the
fittest survive, because the nature of the market is going to be that some are going
to drop out, or they're going to drown in a sea of debt, in this case. Now, some businesses,
one might be named Oracle, are going to survive, right? Because the rising tide of competition
is going to weed out all the rest, and some company like Oracle, which makes maybe the
best software, is going to stay in. So market forces are acting sort of like our drowning
ants algorithm. And partly for this reason, for many problems, though not all, market
forces result in pretty good solution. They're partly to explain why we have, you know, such
amazing cars, furniture, computer software, even Chinese restaurants, because the rising
waters have wiped out the ineffective. But notice if we don't start out with a lot of
ants, we're probably not going to find as good a solution to our problem. So this tells
us why it's good to have lots and lots of different competitors to start out with.
Okay, we've discussed how individuals and organizations use heuristics. So to computers.
So a question we might ask is, who's better? People or computers? Well, probably computers.
There's a game I like to play with my students called Rush Hour. In this game, you have to
move a little red car out of a parking lot. And to achieve this goal, you've got to slide
other cars and trucks out of the way. And oftentimes, this requires some pretty complicated
sequences of moves. So as a homework assignment, I asked my students to write down heuristics
for how do you play the game. Now, these heuristics range from the simple to sort of slide all
the vehicles out of the immediate path to determine where the trucks have to go, because
the trucks are bigger in order to get the red car out. But once you write down these
heuristics, they can be followed by a computer. They can just be written as a formal instruction.
Move the red car forward if possible. If not, identify first car blocking its path. Move
that car. Well, if we can figure out how to write down all of our heuristics for Rush Hour,
we don't need to play anymore. We can just give it to our computer. A similar phenomenon
is occurring with jobs. So in the past, publishing houses had spell checkers looking for typos.
And these spell checkers used a heuristic. Look at the word. Is it in the dictionary?
If yes, go to the next word. If not, mark the word. Well, guess what? That heuristic
can be automated, so people no longer do it. Computers do. Well, if heuristics can be
taught to computers, does that mean we're going to become obsolete? No, not at all. And here's
a big reason. One of the biggest problems we face is what I call the problem of problems.
How to quote the poet Mary Oliver, do we spend our one wild and precious life? Do we build
beautiful art? Do we develop ways to make clean energy? Do we focus on curing diseases?
Finding a solution to that problem, the problem of problems, what do we do with our lives,
what does that mean? Well, it requires heuristics. And diverse ones at that. But those heuristics
must be based on the very essence of who we are. And computers won't know that. And they
probably never will. So people probably have an edge. We also have an edge in perspectives.
And what we've seen here is that heuristics, whether in the minds of people or in computer
code, enable us to find better solutions to problems. In the next lecture, we're going
to combine some of these insights. But we've learned here, and some of what we've learned
in the previous lecture on perspectives. And when we do so, we're going to see the power
of diversity in full force when it comes to problem solving. And we're going to see why
diversity can often trump ability. Okay, until next time. Thank you.
