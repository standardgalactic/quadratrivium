Hi, welcome back. We now know that diverse models can improve collective predictions,
that they can make crowds smarter. In this lecture, we're going to see how that's true
also for an individual. Individuals who can hold diverse models in their heads are also
able to make better predictions. We're going to learn how by containing multitudes of models,
a person is better able to see what's around a bend in the curve to predict the future,
if you will. And before I get to the meat of this lecture, I just want to stop for a second
and stress the importance of prediction. Now, so far in this lecture, we've been having a lot
of fun and I've talked about things like guessing the light of a steer, the height of a rocket.
I've talked about experts trying to predict which movies people like. These are all really fun,
sort of interesting examples, but I really want to stress the importance of good prediction.
I don't want to get lost in sort of the fun that we're having. So I just want to take some of
the important forecasts that we have to make on a daily basis. So for example, should I take a job
offer? What school should my child attend? What courses should I take in college? Does it make
sense to buy this house? Should I invest in this company? And even should I marry this person?
So the course of your life depends on a handful of really key decisions, what you decide to learn
to do, where you decide to move to, with whom you choose to live your life, if anyone, whether you
have any children and how many, what career you choose, and so on. All of these decisions require
you to make predictions. And unfortunately, these are predictions that you can go online and get
information about, like this expected winner of the Super Bowl. There's no New York relationship
exchange where experts price your decision to propose to a significant other. You're largely
on your own, or perhaps you can get some friends and family to think seriously with you, but guess
what? Your family's not going to be that different from you, and neither are your friends. And as we
know from the previous lectures, if they're not that different, then they're probably really not
going to help you that much. But all isn't lost. You can be diverse all by yourself. You can,
to quote Walt Whitman, contain multitudes. And if you do, you're going to be better at making predictions.
To show you how, what I'm going to do is I'm going to focus on one type of predictive model.
Now these are going to be predictions based on categorizations. This sort of prediction requires
two steps. First, what we do is we partition the set of possibilities. By this, I mean we divide
up all the set of possibilities into sets of categories. Second, we make predictions for
each category. So accurate predictions are going to require correctly doing both of these tasks,
creating the proper categories, and knowing the contents of those categories. To see that that's
true, we need to do a little mathematics, not a lot, but just a little. Now before we get to the
mathematics, I want to take a moment to convince you of the ubiquity of this type of prediction.
Take, for example, Robert Quinn and Kim Cameron's model of organizational assessment. This is one
of the core models that they teach at business schools. What they do is they place corporate
cultures in four boxes. Control, compare, create, and collaborate. These correspond to hierarchies,
markets, clans, and what they call ad hocracies. So what you can do is by placing your organization
one of these boxes, you're better able to understand its culture and predict what sort
of interventions are going to be successful, how you can make your organization work better.
Or, for example, take the famous Myers-Briggs personality test. This classifies people into
boxes like INTJ. That's someone who's introverted, intuitive, thinking, and judging. Now if you know
someone's Myers-Briggs type, that can allow you to predict how they're going to respond to stress
and how they'll respond to particular directives. So what you're doing with Myers-Briggs,
what you're doing with Kim and Cameron, is you're basically placing things in boxes.
Categorizations like the Myers-Briggs organizational assessment model place people
and organizations in boxes. These categorizations have value if two conditions hold.
Condition one, what's in one box must differ meaningfully from what's in another box. And
condition two, you've got to be able to evaluate what's in each box with some degree of accuracy.
So I want to make that intuition more formal, and I'm going to use an example
to introduce the statistics of it.
So let's suppose that my department chair must forecast how many students are going to sign up
for one of my classes. But let's suppose he's lost a sheet of paper telling him the content
of the classes that I teach. So to make a prediction, he's going to put all classes that
I could possibly teach in a single category. One big box, we can think of it like this,
that's just classes taught by Scott. Now past data might show the average number of students
that I have when I teach a class is 100. So that's the average. But some classes are going to draw
more and others are going to draw less. So a diversity class, for example, might attract
200 students. But if I teach a class called mathematical methods for social science research,
that might only draw 20. It's going to scare people away. So recall from our previous two
lectures that the standard way to measure the error equals the square of the difference between a
prediction and the actual value. And the statisticians call this the variance in the data. So in
predicting, it's sometimes called the uncertainty. So what I want to do is I want to add some more
detail here. Let's suppose I'm capable of teaching 10 courses at a college level. Now for the record,
how to sing Gregorian Chance is not one of those courses. So what's the number of these from 1 to
10? And we can denote the enrollment in class i by x i. Now remember the average of these x i's is
100. So the variance of the uncertainty can then be computed as follows. Let's see how we do it.
So the variance is just I take each of these x i's and subtract 100 and square it. And remember
the summation sign, that means I just sum those up. And then I divide by 10. So this isn't something
that's going to be the average squared distance of the number of courses from 100. But what I can
do is I can expand this. So if I take this x i minus 100 squared, that's going to give me x i
squared plus 2 times 100 x i plus 100 squared. Now why would I do that? The reason I do that is
remember this the sum of the x i's is just 100. So this is going to equal minus 2 times 100 squared
plus 100 squared. So what that gives me is x i squared minus 100 squared. So what this means
is I've got a simpler way to think about variance. It's just the summation of the x i squared minus
the 100 squared. Well here's what I want to do. I now want to think about how accurate is my department
chair. Now let's suppose that he doesn't know the mean is 100. So he's going to make some prediction.
And let's suppose that his prediction is 110. So now we want to write down what his error is going to
be. Well that's pretty easy to do, right? All we do is we say look, his guess was 110. So the
variance or the expected error of his prediction is just going to be the x i's minus 110 squared.
What we want to do is we want to think about is there some better way to write this down? Can
we explain his error in some deeper way? And so here's what we're going to do. We're going to do
the same thing we did before. We're just going to expand it. So the first thing we're going to do is
we're just going to multiply out x i minus 110 squared. And so what we get is x i squared minus
2 times 110 x i plus 110 squared. You know, if we multiply that out, we can think about that as
x i squared plus 110 squared minus 2 times 110 times 100. I'm going to pull this 2 times 110 times
100 off to the right. Now later in this course, I'm going to call this a heuristic. And it's a
heuristic that mathematician Jews, and it's called adding zero. So it sounds sort of obvious. If I
add zero to something, I don't change its value. Now, what brilliant mathematicians are able to do
is choose to add zero in a really creative way. So here I'm going to add zero by subtracting 100
squared and then adding 100 squared. The trick is going to do it in the right places. I'm going to
add 100 squared here, and I'm going to subtract 100 squared over here. Now, why do I do this? I do
this because now over on the right, I've got x i squared minus 100 squared. Well, that should
look familiar because that's just the variance. And what is this over here? Well, this term on the
right, that's just the error in my chair's prediction. So what we notice is if I look at the
chair's error, the first two terms are just the variance, and the last two terms are the error in
the prediction. So therefore, and this is the really cool part, the chair's error can be decomposed.
It's the variance in the data, which is stuff you could never explain, plus the difference between
his prediction and the true mean. So now we've got our first big equation in this lecture.
The total predictive error, when I put things in a box, is going to consist of two things,
the variation in the data. That's something we're never going to be able to predict because
it's just going to be there. So we're just going to have that error for sure, plus the error in
my predicted mean. So total predictive error is variation in data plus error in predicted mean.
Hold on to that. That's really important. Put it in your pocket. Maybe give it even a little
pat, even, because it's going to matter a lot for what follows. Because so far, remember,
I assumed that my chair puts everything in one big box. But let's now, instead,
suppose that my chair can put it in two boxes. One box is going to be called methods. So those are
going to be these sort of math statistics classes. And the others are going to be called substantive,
which are going to be things like microeconomics, diversity classes, voting classes, public policy
classes, stuff like that. Classes that take on a subject. Now, within each of these two categories,
my chair can make a distinct prediction. And his total predictive error will be the sum across
the two categories. The methods category and the substantive category. So now we can think of
his total predictive error consists of two things. His methods error and his substance error. So he's
got two boxes. He's got a methods box and a substance box. And he's got error for each. So what I've
done is I've decomposed one problem into two. So this may seem harder, but it's not. So let's just
stop for a second and think. So how do we solve this? So we can treat each category as a single
problem. And we already saw how to solve a single problem. We just got to reach in our pocket and
pull it out. Let me show you how. So what we have is he's put things in two boxes, a methods box,
and he's put it in a substantive box. Now, so you can think of there being one box here
and one box here. And what we've got is we've got four each box. There's going to be variation in
the data. That's just the uncertainty that you're never going to be able to predict. And then there's
also going to be the error that he makes. And so what I can do is say the total error he makes is
the error from the methods box plus the error from the substantive box. And so what you want to do
is sort of make as small errors he can in each one. And that's what's going to be left over is just
the variation. So what's going to be true of a good predictor? A good predictor is going to have
small variation and all is going to have small error. So this is going to require two skills,
constructing categories that give you very little variation and then making the accurate predictions
within each category. So to do the first well, what you've got to do is construct the proper boxes.
Let's see what I mean by doing a simpler example. Let's suppose I teach
only four classes. Math for social science, microeconomics, real analysis, and real options.
Suppose that enrollment for the two substantive classes, microeconomics and real options,
is 150. And the enrollment for the two methods classes, right, the ones that are so mathy,
that's 50. So the mean across these classes is going to be 100. And so the variation will be
50 times 50. So it's 50 squared or 2500. Now suppose that my chair breaks these into the proper
categories. He has a methods category and a substantive category. Now within those two
categories, all the classes have the same enrollment. So that means there's no variance.
So let's think of anything about him making his total error. What we've got is we've got no variation
within methods and no variation in the substantive category. So his total error is only going to
depend on the error in his predictions. So what he's done is he's constructed the correct boxes,
and now the only mistakes he can make is in his estimation of what those boxes are like.
So his error depends only on his predictive errors within each category,
and that's because he's created the correct boxes. Now if he can make the correct predictions
within each box, then he's going to be perfect. So now I've got a really complete and interesting
picture of where predictive error comes from, at least for someone who makes predictions using
boxes. It comes from two parts. First part, picking the proper boxes. So you want to create
categories so the variation within those categories is small. If the variation within
categories is small, it means you've got all the high value outcomes in one box and all the low
values in another box and all the medium values in a third box. High variation within a category
means you've got a whole bunch of stuff mixed up in the same box that has very different values.
So a good predictor is going to create categories that have little variation within them.
Now the more categories that you create, the more you're able to reduce the variation within
those boxes as well. But the key is you need to create the proper categories and that's where
it gets tricky. So let's suppose my wife and I arrive in some city we've never visited,
we want to grab lunch. If I categorize the restaurants by their first letter,
then I'm going to create 26 categories. That's a lot, that seems great, but you know that's
not going to be of much predictive value for the precise of the reason we just described.
Variation within each box is going to be really, really high. Think of the categories C,
that's going to have Chez Panisse and Chuck E. Cheese. Those don't belong in the same box.
So then there's a second part, getting the mean right for each box. So even if we can
parse the world into the correct categories, we have to know what's happening within those
categories. So in trying to predict where people are going on spring vacation, I make
great categories based on climate. So I might say there's warm places, there's moderate places,
and there's cool places. And I might incorrectly assume that the colder the climate,
the fewer people are going to go there. Now in doing so, I might make a huge mistake.
So I'm going to miss all those people who go skiing in Canada. So I've got this box of really
cold places, but then I make a huge predictive error in that box because I assume people aren't
going to go there. So my total error is going to be large. Let's just think a little bit more
about this decomposition. What is knowing the proper boxes mean? It means you know what matters.
This is what Phil Tutlock, who's a scholar we're going to meet in a couple of minutes,
calls discrimination. Knowing the mean within each box means knowing the magnitudes, right?
Tutlock's going to call this calibration. So remember, discrimination is having the right
boxes. Calibration is putting the right value within a box.
So we now have a model of how people predict by putting people in things in boxes. And we know
what would allow someone to predict them out. Well, right, they're going to have lots of boxes
and they're going to have accurate predictions for each box. Let's suppose I just have one way of
looking at the world. So I'm not very diverse. That's one set of boxes. Now contrast that with
the situation. We have two ways of looking at the world. That's two sets of boxes. If I've got two
sets of boxes, that means I can take the intersections of boxes and create even more boxes.
Let's stick to the restaurant example, right? Let's suppose I'm trying to forecast the success
of a new restaurant. So, you know, I live in Ann Arbor, Great College Town, and one of my models
might be based on the ethnicity of food. And within this model, I might create boxes based
on ethnicity. I might have the following boxes. I might have an American box, a French box, an
Italian box, a Korean box, a Japanese box, and a Lebanese box. Now then each box, I can make a
prediction. And that prediction might be that, well, you know, maybe I give these scores one to
ten and I might say, well, you know, in Ann Arbor, Lebanese food does really well. That does nine.
Korean food does really well. That's eight. But French food isn't so good. That's only a six.
So I'm going to make predictions on how I think these restaurants are going to do
as a function of their ethnicity. Now we could have a second model. What would the second model
be? Well, the second model might be based entirely on price. And here I might have three boxes, cheap,
moderate, and expensive. And I live in a college town. It's a great college town. It's got a lot
of students. And I'm out of a model that says, boy, cheap restaurants do best. And so I might say,
you know, these, I'm going to give a nine to the cheap restaurants. But then I also know Ann Arbor
has a lot of doctors, a lot of professors, and expensive restaurants do well as well. But then
I might know that moderately priced restaurants, well, students don't go there and doctors don't
go there. So maybe I'm only going to get a five for this. So now let's think about the restaurant.
Suppose it's a cheap Lebanese place. Well, if I think of my first model, it's in the Lebanese
box. And so I think, hey, it's going to do great. But then if I look at my second model, I've got
to ask, what's the pricing? Now, if it's a cheap Lebanese restaurant, let's say that it's going
to do great. But if it's a moderately priced Lebanese restaurant, then my models contradict.
And I've got to make a prediction based on some sort of new box that combines the Lebanese box
and the moderate box. And I'm going to probably extrapolate between those two types of predictions.
So what's my point? My point is that by having two models, two different sets of boxes,
that's a good thing. Because if both models are relevant, then these both could be, you know,
proper sets of boxes. I mean, by that, I mean the boxes will have low variance. And
within each of these boxes, I have low variance. And so I'm going to understand a lot more of
what's going on. And in addition, because I have many models, I probably have a reasonable way of
making predictions within those boxes, right? So my error within those boxes is probably going to
be low. So this means I should be able to make good predictions. And I should be able to make
better predictions than if I just had one model, because I can compare my predictions from one
to the other, and then probably adjust them a little bit.
So what I'm going to do is I'm going to talk about some work by Phil Tetlock,
which I mentioned earlier. This is a very famous study, both for the care with which it was done
and for its scale. Now, this study took place over a decade by this guy Phil Tetlock,
who's a political scientist and a managing professor. And what he did is he coded predictions
from hundreds of experts and non-experts, and also from students. Now collectively,
he had tens of thousands of predictions. And what he found aligns incredibly nicely with the logic
I've just laid out. And here's what it is. People who have more models, who are able to create
more proper boxes and make better predictions within those boxes. So the more models you had,
the better you did. Or in other words, the more diverse you were as a thinker,
the more accurately you could predict the future. Now, here's the interesting thing.
Tetlock didn't set out to test diverse thinkers against non-diverse thinkers.
Instead, he took a much broader approach. He was asking two fundamental questions about humans'
abilities. And what were those? First was this. Can experts predict the future?
You just want to know, can they do it? And second, he wanted to know if they can,
which types of experts. So the answers, which he details in his book, expert political judgment,
are kind of sobering. In response to the first question, he finds, look,
experts can't predict the future very well at all. So the total titles of the book is really kind of
a joke. Expert political judgment. As for the second, as I mentioned, he finds that those
people who can predict it all have some success tend to be multiple models thinkers.
Now, Tetlock's book shows a fox and a hedgehog. The reason why these images are borrowed from the
political philosopher Isaiah Berlin. In writing about Tolstoy, the philosopher Isaiah Berlin
borrowed this quote from Archaeologus. It says, the fox knows many things, but the hedgehog knows
one big thing. What Tetlock found was that foxes, people who relied on many models,
proved to be more accurate predictors than hedgehogs, people who believed in just one big idea.
So I want to preface my celebration of the relative success of the foxes by reiterating
Tetlock's first finding. People weren't very good at predicting. Now, to arrive at that conclusion,
Tetlock uses the same decomposition that we used above about having the right boxes and then
making correct estimates within those boxes. But he uses different terms. He uses the terms
discrimination and calibration. Discrimination corresponds to a low variation within category.
So that means creating the right boxes. So on this axis right here, you're seeing sort of
increases in discrimination. Calibration corresponds to making the right prediction
once you've got things in boxes. So it's accuracy within category. And on this axis,
you're seeing sort of increases in calibration. So what Tetlock did is he compared the predictions
of experts, right, to different types of experts, to everything from formal models to chimps.
So what you see is formal models did the best. Foxes did much better, right, and undergrads
over here weren't very good at all. In fact, he also finds out that chimps do better than
undergrads and even than the hedgehogs. And he found surprising, or it's sort of surprising
thing that chimps are better than students. Well, don't be too dismayed by the lousy performance
of people. After all, many events, particularly movements in stock prices, may be by their very
nature unpredictable. So John Cochran, an economist at the University of Chicago, wrote
on a blog about collective prediction, look, think of it this way, many economic events
should be unforecastable. In fact, their unforecastability is a sign that the markets and our theories
about them are working well. What are these theories? Well, these theories say that if
you could predict the price of a stock, then like let's suppose it was going to rise, then
you'd purchase a stock and that would therefore raise the price and therefore the price of
the stock would no longer rise. So this underpins what's called the efficient market hypothesis.
And the efficient market hypothesis basically says that prices are going to be random. So
if that hypothesis is true, stock prices should contain all relevant information and you shouldn't
be able to beat the market. It also means that you can't predict whether a stock is
going to go up or down because if you could, you'd be able to beat the market. You'd be
able to make money. Now there's a second reason to be a little bit more optimistic than you
might be when you read that undergraduates perform worse than quote unquote chimps.
Well, here's the catch. Tatlok made the chimps pretty smart. Let's think about how he constructed
his chimps. So for a lot of his questions, Tatlok asked whether something was going to
decrease, stay the same, or increase. So let's take the growth rate for the U.S. economy.
If GDP growth had been 3% instead of what we call standard deviation of 1%, so it's basically
between 2% and 4% over the past decade. And what Tatlok did is he assumed sort of a normal
distribution of bell curve and he called any growth rate less than half of a standard
deviation below the mean significantly low. So growth below 2.5% was a decrease and any
growth above 3.5% was an increase. So what he did is he sort of broke the distribution
into parts. Let me show you. So what we have is we've got a normal distribution of growth
rates and you've got in the middle here 3%. So what he does is he breaks the distribution
up into three parts so that 38% of the time he calls this staying the same. And 31% of
the time he says it goes up and 31% of the time it's going to go down. So what he does
is he's basically creating a set of boxes that are roughly equally likely to occur.
He then assumes that the chimp is randomly choosing a box. So what this set up means
is the chimp is going to be right a third of the time. So he's making some pretty smart
chimps here. Now Brian Kaplan is a friend of mine and also the author of the book The
Myth of the Rational Voter argues that this approach makes the chimps too smart because
by making the boxes equally likely the chimp can't do terribly, right? Alternatively Tatlok
might have given the chimps five boxes and made two of those boxes just almost impossible
like incredibly unlikely. And if that were the case then 40% of the time the chimp would
have chosen one of these two ridiculous boxes and as a result the undergrads would have
predicted with greater accuracy than the chimps. So here's how I think about it. People did
better, way better than real chimps would have done. But not much better than you do
if you just sort of divided the world into three categories and randomly picked. In
fact students were even worse at this. Now that's just how the facts played out. But
what we want to focus on here is that the best people were the foxes, the multiple model
people. They did better than these hedgehogs who had single models. Now there's two main
reasons why. When you actually look at the predictions people make there's two main reasons.
One is foxes take less extreme views and the second is foxes respond better to information.
Let's take some examples. Let's take the less extreme view case. So 1992, Tetlock asked
experts to forecast the future of post-communist Russia. So the hedgehogs can be divided into
two camps. They're pessimists who believe Russia would fail. Now their reasons varied
a little bit. Some said lack of political maturity to the fact that here I quote, generations
of Russians have been indoctrinated that private property is theft. This was a pessimist.
Pessimists also predicted fragmentation and fiefdoms. Now they're optimistic hedgehogs
and they pointed to China and the fact that the Russians would respond really well to
incentives based on culture. So the hedgehogs, at least in this example, they let in one
big idea either it's really going to work or it's really going to fail. What about the
foxes? Well the foxes they saw many sides. So some held optimistic views and others
forecasted somewhat more dire outcomes. But overall because the foxes tended to see the
likely tensions growing wealth but also increasing inequality unrest, they saw an embrace of
the West but also a distancing. So they tended to be more conservative. But think about it,
this makes sense. Where does this conservatism come from? It comes from the fact that you've
got lots of different boxes and some boxes may give you extreme predictions and others
won't and when you balance those out you get a leveling. Now the second finding that foxes
responded better to information shouldn't come as a surprise at all. The more models
you have in your head, the more likely that new information is going to fit within one
of your models. So remember this, a many model thinker creates more boxes. A new piece of
information might only have a small effect on a big box so it may be ignored. But if
you've got lots of boxes, it's likely to have a big effect within one of those small
boxes. Now being a fox doesn't come without cost because these people are sort of large
cognitively. As Whitman would call it, containing multitudes. And if you contain multitudes,
it increases the probability of being illogical. In fact Whitman knew this. Think of the full
quotation. What does it say? It says, Do I contradict myself? Very well then I contradict
myself. I am large. I contain multitudes. What turns out that foxes do tend to contain
multitudes and do tend to contradict themselves. So they prove far more likely than the hedgehawks
is to have their predictions violate the basic laws of probabilities. Specifically, they
often had the sum of the possibilities they predicted exceeding one. So yes, they contradicted
themselves and that's the cost, at least for a fox, of containing multitudes. The benefit
though is to be more accurate. Incidentally, there's another cost, a personal cost. If
you're a fox, you're not likely to be on television and radio. Television and radio are populated
by hedgehawks and inaccurate hedgehawks, but they're more entertaining. It's more fun to
watch some of the strong opinion. So assuming we're not all trying to land on AM radio talk
shows, in light of the fact that diversity makes us better predictors, why don't we all
have lots of models in our heads? Well, that's a great question. It has a really simple answer.
Not easy. It takes work. When the leader of a country proposes privatizing an industry,
it gets much easier to say, markets work. That's a great idea. I'm glad they privatized
it. Or alternatively to say, oh no, we need the government to protect the interest of
the consumer. It's a huge mistake to privatize. And it's just easier to do that than it is
to seriously engage both models and think through all the nuances. So how do we become
good diverse thinkers? Well, we have to be foxes. We have to contain multitudes. And
by the way, if we can, we should try to obey the laws of probability. Okay. Thank you.
