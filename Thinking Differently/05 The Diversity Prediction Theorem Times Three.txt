Hi, welcome back. In our last lecture we covered the diversity prediction theorem, and that
showed that the accuracy of collective predictions depends on equal measure on the accuracy of
the crowd's members and on their diversity. Equal measure, that was the surprising part.
Equal partners in predictive ability. And we also talked briefly, very briefly,
about the central role that forecasting plays in organizations. So not only does it matter for
sort of nuts and bolts operational issues, so how many televisions should we ship,
or how many citizens will take advantage of a new government healthcare program,
it also enables more effective design of products and policies. In order to give people what they
want, you have to know what they want, and that's going to involve some amount of forecasting
prediction. In this lecture we're going to talk about another application of forecasting,
namely using knowledge of a population to better serve that population. We're going to study a
particular case, a fun case, one that involves movies. So this case begins with a challenge. On
October 2, 2006, Reed Hastings, who is the CEO of Netflix, an online movie rental company,
began what was known as the Million Dollar Netflix Prize Competition. So in a nutshell,
here's how it worked. Netflix had developed this computer algorithm called Cinematch.
Cinematch predicts your movie preferences. So the contest rules were fairly simple. If you could
beat Cinematch by 10%, in other words, if you could be 10% better at predicting how much someone
would like a movie, you earn a million dollars. So let me talk first about how Cinematch worked,
and then how people tried to beat it. So suppose, for example, you'd rented the Sting,
which is a 1970s classic movie starring Robert Redford and Paul Newman. And then on the Netflix
website, you gave that movie a rating of five stars. So that means you just loved the movie. So
based on the nature of that movie, and perhaps on some demographic information, Cinematch would
then make some predictions. For example, it might predict that you would also give five stars to
Bush Cassidy and the Sundance Kid, because that also stars Redford and Newman. It's a buddy film,
and it's got intrigue and witty banter, right? So because you liked the Sting, you'd like Bush
Cassidy. Now, alternatively, suppose you rented Eight Mile, a gritty film about Detroit rappers.
Now, if you liked that film, Cinematch might predict that you wouldn't like Bush Cassidy and
the Sundance Kid, but you might like the movie Men in Black, which stars Tommy Lee Jones and Will
Smith, because Smith is also a famous rapper. Now, if you run Netflix, suppose you read Hastings,
an accurate Cinematch gives you what an economist is going to call market power,
because if you can predict the movie preferences of your members, let's say me, for example,
because I'm a longtime Netflix user, then that member, again, me, I'm going to be less likely
to abandon Netflix for some other movie rental site, even if they offer me a lower fee. Why?
Well, because when I go to click on a movie, Netflix is going to say to me either, hey,
five stars, good choice. I think we think, Cinematch thinks you're going to like this movie,
or it might say one star, which means I might want to think about whether I want to rent that
movie or not. Or it could say three stars, no great shakes, maybe yes, maybe no, but it's helping
me make choices so I don't waste my time. Now, I trust Cinematch because I think over time it's
learning that there's some things I do like. I like intrigue, I like suspense, I like witty
dialogue, and other things that make me sort of uncomfortable that I don't like, gruesome murders,
for instance. And there's things that make me just roll my eyes. Like, I hate these syrupy music
videos that they insert in the movies with protagonists in need of some sort of cathartic
reflection, goes for a car ride in the country, puts the CD in the car, the wind's blowing in his
hair, drives around, I can't stand it, spare me, I just want to watch the movie, I don't want a music
video. The Netflix figures that out. So the Netflix prizes this competition, and there's large
stakes, a million bucks. So as a result, it attracts some of the best and brightest scientists
in the world to try and figure this out, including some from the former Bell Laboratories, a place
called Bellcore. It also directed professors, postdocs, graduate students from many of the top
universities in the world. So in fact, this is like a competition to see who can construct the best
predictive model and may the best person win. Now, just to give away the plot a little bit,
that best person's not going to be a person. It's going to turn out to be a diverse team.
Let me give some background on this contest. So Netflix offers up a bunch of data. In fact,
over 100 million ratings by about half a million users who'd rented in total 18,000 movies. So 100
million ratings of 18,000 movies, that's a lot of data. So what the contest organizers did is they
divide that data in random weeks, this is incredibly important, into two sets. There's a training set
which is the one that the people worked on, and a testing set which only kept about three
million of these ratings to see how well people's predictions worked. So what Netflix did is they
put the training set out there on the web. So this was information about each movie, each
renter, and each rating. Individuals and teams then used this training set to construct predictive
models. And once you had a model, you could then take it and apply it to the testing set, that
other three million sets of data, you know, movie ratings, and to see how well you did. And this
allowed competitors to sort of work on models and at the same time test to see how other models
worked. Now the testing sets, it's pretty complicated, gets broken into three sets. So one of the people
could do sort of practice tests on. Another one, once they felt they were confident, used to do
formal rankings to keep a score of who's doing best. And then a third part that was kept aside for
final validation to determine who the ultimate winners are going to be. Now again, these testing
sets are small relative to the training sets. And each one, you know, only had about a little over
a million ratings. So small is relative here. Million ratings is a lot of ratings. So let me
summarize this format again. You've got how much people competing in here are trying to make predictive
models. They're using this training set data to build their models. And then once they built their
model, they go test it using the testing set data to evaluate their performance. And when they feel
like it's pretty good, they send it formally in and Netflix uses a second set of that testing data
to figure out which one is going to work best. Now you might say, why not use all the data?
Well, here's the problem. If you gave a good statistician all the data, they could construct a
model that matches every prediction pretty almost exactly. And statisticians call this
overfitting, because you're basically fitting all the data to itself. So by creating a separate
testing set, you can basically ensure that someone's model actually has predictive value,
they have an overfit to the data. So how did it work? Could people do it? Well, it turns out
beating Cinematch was easy. Within a week, there was a group that was called WXYZ,
and they'd beaten it by 5%. But it turned out beating it by 10% was pretty hard. And that's
what we're going to talk about in this lecture, how teams of forecasters, predictors, statisticians,
computer scientists, tried to beat 10%. So let me get some history on the competition.
Early on in the competition, there was a lot of diversity. Competitors were trying a whole bunch
of different approaches. And the modal method relied on something that are called nearest
neighbor methods. Let me give you a crude way to think about how these methods work.
So for each pair of movies, one creates a similarity measure. So Rocky II is actually
pretty similar to Rocky I, but not very similar to Gone with the Wind, like boxing movies aren't
that similar to Civil War movies. So if you want to think how much a person likes one movie,
it's going to determine by how much they like other movies that are sort of similar to those.
But after a while, things changed, and the best techniques relied on decomposition methods.
Let me talk about how these work. So let's take any movie. Let's take James Cameron's Titanic.
What we want to do is we want to break Titanic down into a long list of attributes. So attribute
one might be romance. Attribute two, drama. Attribute three, disaster death. Attribute four,
is it a huge budget movie? Attribute five, is it sort of a blockbuster in sales? Attribute six,
does it have sort of syrupy music videos, but maybe no car ride? And attribute seven,
is it historical? Things like that. So I'm going to be kind of geeky here and describe this the
way that a mathematician would. We can think of this as what we call the vector of attributes of
the movie. So to determine whether a person likes a particular movie, we need to know whether the
person likes those attributes. So what we're doing is in some sense decomposing a movie into a set
of attributes, and then estimating a vector of preferences for people over those attributes.
Let me make these preferences numbers between zero and ten. So a number zero for an attribute means
that the person doesn't like that stuff at all, and a number ten means they love it. So let's take
a hypothetical moviegoer, and they might have the following preferences over the attributes that
we just listed. So let's look at attribute one, which is romance. It could be that this person
likes romance. So on this attribute, they give it a weight of seven, so they care a lot about romance.
It could be that this person also likes drama, so she gives drama a rating of nine.
It could be not a big fan of disaster and death, so she gives it a three. Now, big budget movies.
Well, you know, maybe this person likes sort of quaint art films and doesn't like big budget
movies, so that gives it just a two. Now, what about blockbusters? Well, it could be that even
though this person likes small budget films, they like films that other people like, so they give this
an eight. And what about the syrupy music stuff? Well, you know, maybe sort of, maybe they give this
a four. And then last, what about historical stuff? Well, maybe this person is keen on history,
so she gives a preference of ten on historical ratings. Now, what we can do is we can think of
these are the numbers for a person, that we can think of how well they match a movie like Titanic.
Well, so all we want to do is we want to do one thing. We can say Titanic has these certain
features and this person likes certain features and see how well those things match. Now, we've
got to add in two other things. These are sometimes called dummy variables. The first dummy variable
is going to be a movie dummy. What this does is takes into account just how good the movie is
overall, the average rating of the movie by other people. So this variable is going to give
some sense of bonus for good movies and subtract something from bad movies. So let's take an
example. The movie Geely, which starred Ben Affleck and Jennifer Lopez. It should have been great
based on its attributes, but it totally flopped. Or maybe the most famous example here is Dustin
Hoffman and Warren Beatty in 1987, starting just a total stinker called Ishtar. I know I've seen
it, right? The movie dummy for that movie, dummy's the right word, it's going to subtract the vibe
in that movie. So even though Scott, these stars in it, it's going to basically say this is not a
good movie. Now, the other dummy that they added in was a person-specific dummy. Now, this corrects
whether a person tends to give high ratings or low ratings. Because there are some people,
they give, look, almost every movie, they give four stars, five stars, and there are other people
who may give mostly ones and twos. So the person dummy sort of corrects for that. So let's think
about then what these models look like, what a good model looks like. So basically you identify the
attributes of the movies that have sort of a high information content, and you're going to basically
look for things that explain why someone likes a movie and why they don't. Now, matrix decomposition
methods work similar to this, except an algorithm takes into account all this sort of data about
the movies in terms of the attributes. So in effect, what you basically do in these decomposition
methods that these people use is you put all the features of a movie in, such as those I just
described, and then a computer algorithm is basically going to construct latent factors.
Okay, now what are these factors? Well, these factors condense a whole bunch of features
into a single dimension. So let's go back to think about attributes of a movie. So let's have a movie
that has a car chase. That movie might also include things like suspense and bombs.
So what a factor model does, these matrix factor models do, is they take these features and put
them into a single latent factor, which might mean like action movies. So what these matrix
methods do is they want you to determine how many latent factors, how many attributes you should
consider to represent a movie. Okay, so that's the idea. Sometimes you have explicit attributes, like
is it a drama? Other things should come up with these sort of latent attributes.
So Bell Corp, some of the people from the old Bell Labs, used from 50 to 200 factors.
So they also allowed the people's preferences, right, the weight that they attached to each
one of these attributes and factors to change over time. So this will enable them to account for
children's tastes, differing as they age, people getting married, or someone just sort of liking
a new style of movie. So that's backed up. You've got these really smart people who figured out all
these sort of direct attributes and these latent factors. And the early leaders I mentioned in
this competition was Bell Corp. Now, the team was led by a guy named Robert Bell, who worked at Bell
Corp. And Bell Corp's best models, I mentioned, used approximately 50 variables. That's really
impressive when you think about it. Try just sitting down, you pause the movie, and try to think of
50 attributes for a movie. It's easy to think of 10, but when you get to 30, 40, it gets pretty hard.
Now, Bell Corp's best model could beat cinema action by 6.58%. That's good. That's great,
in fact, right? But it's only two-thirds of the weight of the million dollars.
But Bell Corp had more than one model. In fact, it had 107 models. That's right, 107 different models.
Now, think for a second, right? You've got 107 models that are diverse. And so what they could do
is create a crowd of models. And so when they combine these, they got an improvement of 8.43%.
Well, why is that? Why by combining they do better? Well, let's think back to our last lecture.
Remember the diversity prediction theorem? What did we see? We basically saw that by combining models,
we would do better. So remember we've got this theorem that says the crowd error equals the
average error minus diversity. As I mentioned, Bell Corp had a crowd of models. So by combining them,
Bell Corp could do better. So what you've got is, you know, maybe you have an average error of some
amount here, and then these models are diverse. And so when you subtract off that diversity,
that reduces the crowd error. So the crowd is going to do better. And that's what they exploited to
get from 6.58% to 8.43%. Now, at this point, I need to take on two technical issues. So first,
in the diversity prediction theorem, I'm talking about errors. So smaller errors correspond to
better models. Now, in this Netflix prize, I'm talking about percent improvement. So larger
percentage improvements imply better models. Now, in either case, the logic is the same. A
crowd of models will depend on those models being individually accurate and collectively diverse.
Now, second, in the diversity prediction theorem, I assume that all the models get
weighted equally. Now, in practice, you can do a lot better than that. Now, intuitively,
you can think, well, maybe I should put more weight on better models and less weight on weaker models.
And if I do that, the crowd should do even better, except putting more weight on more
accurate things. This gets kind of cool. So that's true, right? If I take weight off less
accurate models and put it on more accurate models, I'll do better. But we've got to be careful. We
can't push that intuition too far. Because in the extreme, that would say that the best model
would put all the weight on one thing, the very best model, the best individual model. But that's
not true because weighted averages typically do better. And in this case, it clearly wasn't true,
right? Because a weighted average of Bellcore's models could beat Cinemax by 8.43%, but their
individual model was just beating it a little over 6.5%. So each year, because actually, nobody
beat it in the first year. So at the end of the first year, Netflix gave out a small prize,
$50,000 to the team that was leading, and Bellcore was leading. So that's great,
but they were about 8.43%.
So this 8.4%, that's impressive. I mean, really it is. There's a lot of work on this, but
it isn't 10%. So it doesn't get the million bucks. So Bellcore had to get better. How do you do it?
How do you get better? Well, let's think back to what we know from our diversity prediction through.
One way they could do it is to become more accurate. They could become smarter.
But well, look, they were the best. They were the leader. No one could be doing better than they
were. So that's going to be hard. What else could they do? Well, they could get more diverse,
because another way to get better is by being more diverse. And getting more diverse isn't going to
be hard, right? You've got to find someone who thinks differently than you do, who's reasonably
accurate. So in 2008, they did exactly that. They got more diverse. And they joined forces
with a team called Big Chaos, which was two computer scientists from Austria. Now that team
had developed some really sophisticated ways to combine models using some fancy nonlinear
techniques. So rather than just put weight on the models and add up their predictions,
Big Chaos was using much more sophisticated techniques for aggregating. So they really
thought about how do we combine these models. So that helped, and it was good, but it wasn't enough.
So in 2009, Bellcore faced the same decision. You have to get smarter, or you have to get
more diverse. Well, again, getting smarter was hard, so they got more diverse, and they added a
group called Pragmatic Theory. Now this was a group of Canadians, and they had a different set of
skills. They were experts in modeling how preferences change. So now they have this
combined team that's called Bellcore's Pragmatic Chaos. Let me stop for just a second. Think
about what's happened. You've got this very smart group of people, and they combine with other people
with different tools, and collectively they were going to try and be better. So what they were doing
is seeking out diversity in the tools that people carried around with them to try and solve this
problem. So what happens? So now they combine their models, and they've got more than 800
variables. 800! Think about how hard that would be 800 variables for representing a movie. That's
just amazing. Now equally amazing, many of the attributes turned out to have the most explanatory
power for what we might have called indirect. So they weren't based directly on content. So content
would be something like music video. So what do I mean exactly? So content-based variables or
attributes correspond to features of the film. Who stars in the film? What's the film genre?
And so on, right? Indirect attributes are signals of the film's quality. So these are things like
the length of time the movie spent at theaters, the amount of time between the movie was first
released, and when it came out on video, these sorts of things. These indirect signals proved to be
far more informative than the content-based attributes as the contest moved on. Okay, so now
800 attributes is sort of amazing. And with more attributes to include in their models, it turns
out the members of Bell, Korn, Pragmatic, KS are able to build just better individual models. In fact,
they were able to build one individual model that was accurate as their previous blend of models.
So now they had a single model that was 8.4% better. So that's great. A single model is 8.4%
better. But again, not 10%. But when they blended their models, when they did this mixture of their
models to form a computer scientist form, they called it an ensemble of models. I was calling
a crowd of models, but they called it an ensemble of models. They did it. They broke 10%. So in June
26, 2009, more than two and a half years after this contest began, Bell Korn's Pragmatic cast
breaks 10%. They win, and a million bucks is going to go to public schools in New Jersey.
Totally happy story. But wait, not so fast. There's a problem. There's the problem. The contest rules
stated that once somebody breaks 10%, the contest is going to end in 30 days. Now, Reed Hastings,
when he came up with the contest, he included this extra period to ensure the contest would be
interesting and that he'd get a really good solution. Because when he announced this contest,
he's got no guarantee that someone doesn't improve on Cinemax by 10% in the first hour.
So some undergraduate student, she might have been interested in movies,
wrote a simple code and beat it by 11% within the first 20 minutes. He wanted to guard against that.
So the 30-day rule wouldn't prevent somebody from beating it, but it meant that there'd be at
least 30 days of competition. So 30 days to get the best possible model. And it could be that those
models improved on Cinemax by 20%, 30%. Who knew? Okay, mind you at this point, when the 30-day rule
kicks in, this contest has been going on for three years. They're not quite three years, but almost
three years. Nevertheless, rules are rules, and so Bell Korn's Pragmatic has chaos. They've got a
weight. I mean, they broke it, they've got a weight. Now, as the events transpired, Bell
Korn had to do more than weight. Because once they had formed, right, they appeared to the other
competitors to be the likely winner. And sure enough, they were, they got above 10%. So you've
got these 23 other teams, let's call them the laggards, right? Now, they're the talented bunks.
These people have PhDs, they have incredibly impressive resumes, you know, just like the
people on the Bell Korn team. But they weren't as good at cracking this problem. In fact, we have
two and a half years of evidence saying that Bell Korn was better. And no one's going to dispute
that. So why would they even try? What would these 23 other teams try and do, given that
they knew Bell Korn was better? Why would they even make any attempt? Well, there's a simple reason.
Diversity. These other teams weren't as smart. We know that. We've got two and a half years of data.
But they had lots of diversity. And so they tapped into that diversity. So what they did is they
created a team called the ensemble. So once Bell Korn's pragmatic case gets above 10%, the ensemble
forms. And there's only 30 days left in the contest. So the ensemble really couldn't afford
to try and develop new models and attributes and, you know, come up with something really novel.
But what they could do is they could figure out, how do we combine our models that we have?
So that's the strategy that they pursued. They said, okay, look, we've got all these models out
there. Let's try and blend them and see if we can figure out some way, right, by averaging
an aggregate in our models to come up with a good prediction. Let's see how they did that.
So I'm going to show you a simple graph. Each of these blue dots is a model that they had.
And what you see on this horizontal axis is the accuracy of the models. And so these are
very accurate. And these are less accurate. And what you see on this axis is the weight
that they end up giving those models. So some models get lots of weight, and some models get
very little weight, and some models even get negative weight. And in fact, some of the most
accurate models even get negative weight. This is sort of surprising. Now, as counterintuitive
as it may seem to subtract a prediction, the practice makes sense if you think of a model
as partially canceling out one another. So the part of another model that doesn't get canceled out
gets added too, and hopefully that improves the prediction. So what you've got is you've got
you're canceling out part of one model by subtracting another model. Now, the techniques that they
use, the ensemble used to come up with these weights were state of the art. And it led to a
bunch of academic papers on how we think about, you know, combining models in the best possible way.
So it turns out the ensemble proves a really worthy competitor. With just two days to go,
they beat Belchor's Pragmatic Chaos. They appear at the top of the leaderboard.
They've taken the lead. And how do they take the lead? Not because they were smarter,
because they had this extraordinary diversity, and that served them well.
So let me stop for a second. This is the third time we've seen the diversity prediction theorem.
First, we saw it within Belchor, because they had one model that's 6%,
when they combined it, they got to 8%. We saw it again when Belchor's Pragmatic Chaos formed,
and they got above 10%. Now, we've seen it a third time when ensemble forms, 23 teams,
that aren't as good as the other teams, but now they surpass Belchor.
So now, it's kind of exciting. We've got both teams above 10%, and the clock is ticking down.
So each submits its final model. On the end, it's anyone's guess who's going to win this.
And the final error is actually written in error terms. So root means squared error. That's the
same sort of errors that I described in the diversity prediction theorem, sort of how far
off you are. Now, keep in mind that smaller errors are going to be better. So what's the final score?
Belchor's Pragmatic Chaos, their error was 0.856704, and the ensembles was 0.856714.
So Belchor beat them on the fifth decimal point. So once again, they could pop open the champagne,
they won, won on the fifth decimal point, but they won. But wait, once again, these darn contest
rules come into play. The fifth decimal point hardly counts as significant in any sort of
statistical sense. It'd be like saying someone is taller than someone else on a particular day,
they were a few microns taller. Like that's basically the width of a human hair is about
100 microns. So it's just sort of ridiculous level of accuracy, right, we're talking about.
So for this reason, the rules of the contest said, look, we're only going to four decimal points.
So we have a tie. So what's the tiebreaker? The tiebreaker says who submits first? The team that
supports their model first is going to be the winner in the case of the tie. So who won? Well,
Belchor. Belchor submitted 22 minutes earlier. So after three years, they could finally pop the
champagne corks and the money could go to those schools. And that's it. That's the end of the story.
Where is it? So when I was a kid, one of my favorite books was Ten Apples Up On Top by Theo Lesig.
That's Theodore Geisel's name spelled backwards, Dr. Seuss, right? In this book, Ten Apples Up On
Top, there's two animals who compete by stacking apples on top of their heads. The number of apples
on top, the noggin of these silly animals grows from one all the way up to ten, right? We've now
seen the diversity prediction theorem once, twice, three times. To quote Theo Geisel, look,
see, I can do three. Well, here's a little quiz. Do you think you can do four? Four diversity
prediction theorems up on top? Well, here's a hint. Think of who's left standing at the end.
Got it? Okay. I thought so, right? Here's the answer. We've got two models. Each model aggregates a
whole bunch of diverse models. So these two models, Bellcor and the ensemble, predict equally well.
I mean, to the fifth decimal point. So they're equally accurate. But what else? They're different.
That means if we average them, what has to be true? That's right. We can do four.
The percentage improvement of Bellcor was 10.06. The percentage improvement of the ensemble
was 10.06. Well, what if I blend these two? What if I do a 50-50 blend? What should happen here?
I should do even better. And in fact, you get a 10.2% improvement. That's amazing. It's lots
of fun. But what we see is that Bellcor, even though they won, they didn't have the best model.
The best model was a combination of Bellcor and the ensemble.
Okay. Last week, I think this is just sort of some small stakes fun. I mean,
what can you get for a million bucks these days anyway? Let me give you an example from the
real world with even larger stakes. I was once talking with a CEO of a company that had a computer
algorithm that was predicting the outcome of value to some really high-wealth clients.
Now, to put some meat on this story, let's suppose that it predicts whether the IRS is going to
audit your tax return. And if the algorithm used sort of really only data on the return.
Now, let's suppose that this company spends a lot of money designing a new algorithm,
and this new algorithm looks at returns in a completely different way. It doesn't look at
the tax returns, but it maybe looks at changes in the returns from the previous year, not on
specific values in this year's return. Now, in this particular case, the new algorithm proved
to be just a tiny bit better than the original algorithm. So, I told this Netflix story to
the CEO, and he said to me, wait, are you saying that if I combine my two, I've got two models
that are equally good and very different, that if I combine them, I'd probably do better? Well,
standing next to the CEO was a statistician who worked for him, and she chimed in, and she says,
no, no, no, Scott's not saying that at all. And the CEO gave her a puzzled look, and then she said,
rather large smile on her face, he's not saying probably. It's definitely going to do better,
and it's true. I wasn't saying probably.
So, all fun aside, and all the intrigue and suspense of this Netflix prize, we shouldn't
lose track of the most important insight in this lecture. On predictive tasks, individual talent
and collective diversity matter equally. That's where you get collective accuracy.
Finding more talented people may often be more difficult than finding people of approximately
equal talent who think differently. So, this holds in our everyday lives as well. We should listen
to what other people think. Other people's opinions may be no more accurate than our own,
but if they're different, and if we combine them with our own, we'll predict more accurately.
We'll see a little bit further down the road. So, like the members of the ensemble,
we may not win a million dollars, but at least we'll have some fun trying. Thank you.
