Hi, welcome back. We've been talking about prediction, specifically the diversity prediction
theorem, and we've seen how collective accuracy depends on talent and on diversity. And we
just saw how that logic played out in the Netflix prize competition, where by combining
some diverse models, the predictions became more accurate. So at this point, we should
all have in our heads this idea that multiple models are better than a single model. However,
it also stands to reason that when I listen to multiple models, and some seem better,
sort of more reasonable than others, that shouldn't I pay more attention to the better
models? For example, suppose I bring together a group of people, and I hear their predictions,
and some of these predictions seem just more accurate, better than others. Shouldn't I
attach more weight to those predictions than the others? Logic seems to say that I should,
that I should take a little bit less weight from someone who's likely to be less accurate,
and put some of that weight on someone who seems more informed. Here's the problem,
if I take this logic to its conclusion, then I should place all the weight on the most accurate
person. But then that runs counter to the logic that we saw in the diversity prediction theorem,
where we wanted all these diverse ways of thinking. So what do we do? Do we go with the
diverse collection, or go with the person who seems best? Well, fortunately, we can answer
this question with some mathematics, at least partly, because what we have is less contradiction
really than opposing forces. So let's think of the diversity prediction theorem again.
What does it say? It says the crowd error equals the average error minus diversity. So as we
place more weight on better predictors, we're going to decrease the average error. So that's
great. But when we do that, we're going to reduce diversity. So here's the trade-off. If
the gain in error reduction exceeds the loss of diversity, then our prediction is going to be
better. But if not, we should then not place more weight on the better predictors, even though
they're better, because we're losing too much diversity. So let me slow down and try and make
more sense of this weighty issue, because it's a big one. The question of weighting
predictions can be consideredly as two separate questions. The first one is, who do we include?
Should we ignore the people likely to be included or not? And second, once we've decided who
belongs to the group, how much weight do we attach to each one? I'm going to take up the
second question first. And I want to think about having three predictors. So we're going to call
them Amy, Bell, and Carlos. And they've been making predictions for a number of years. And I've got
a lot of data on the errors that they've made. So let's suppose these are Amy's squared errors.
So what we've got is that she's made, let's say, six predictions, and we see these values. And
remember, these are squared errors. So this 49 here means that she was off by seven, and that's
seven squared. So let's think about how good she is on average. Well, all we have to do is take
these numbers and add them up. There's a quick way to add this up. We've got a 49 and a one,
that's 50. A 49 and a one, that's another 50. So that's 100. And then we've got a 64 and a 16,
that's 80. So if we add these up, we get a total of 180. And she's made six predictions. So that
means on average, she's off by 30. So what we can think of is this is her individual squared error.
Now, statisticians will call this her variance. And in the last lecture, when I considered each
prediction is coming from a different person, I called this the diversity of the predictions.
In that case, I did so because I wanted to highlight that these are distinct predictions
from different people. But here what I'm thinking is looking at one person, and I'm asking,
on average, how far off is Amy? What are her mistakes like? So I'm going to call that variance.
It's Amy's average squared error, and it's 30. I remember I've got three people. I've got Amy Bell
and Carlos. So let's suppose I've got the following data. So I've got Amy's average squared error is
30, Bell's is 15, and Carlos is what's a little higher at 60. What I want to do now is introduce
another statistical term, and this is called accuracy. Now, accuracy is a statistical term.
It's also a term that we use in our daily lives. The accuracy is just the inverse of the variance.
Let's think about what that means. We have really high variance. It means you've made a lot of
mistakes. So you have low accuracy. If you have low variance, that means you make few mistakes,
and you have high accuracy. If your variance were zero, you'd have infinite accuracy. So what are
the accuracies of these people? Well, all you have to do is take the inverse. So Amy's accuracy
is 1 over 30, Bell's is 1 over 15, and Carlos is whose least accurate is 1 over 60. Now,
these fractions can get confusing. I don't like working with 1s over 60s and 1 over 15s
and 1 over 60s. So what we want is just relative accuracy. So what we can do is we can multiply
all of these things by 60 so that we're now dealing with whole numbers. So that means Amy's
accuracy is going to be 2, Bell's accuracy is going to be 4, and Carlos' accuracy is going to be 1.
So what we've got is the same relative accuracies, but now they're essentially, you know, positive
weights as opposed to these small fractions. Now, here's what we can do. We can use some calculus,
and we can figure out how much weight we should put on people as a function of their accuracy. Now,
there's a lot of math involved with figuring this out, but I'm just going to give you the answer
here. So what we do is we think of Amy's accuracy is 2, Bell's accuracy is 4, and Carlos' accuracy is
1. So what we do is we add up and we get the total accuracy of everyone, which is 7. And then what you
can show is the optimal thing to do statistically is assign weights proportional to the relative
accuracy. So that means Amy's weight will be 2 out of 7, Bell's weight will be 4 out of 7, and finally
Carlos' weight will be 1 out of 7 because he's the least accurate. Now, intuitively, this makes a ton
of sense. The more accurate people, in this case Bell, who's the most accurate, should get the most
weight. The least accurate person, Carlos, should get the least weight. What the mathematics tells us
is exactly how much weight they should get, and it turns out it should be proportional to their
accuracy, and accuracy is just one over variance. So this is pretty cool. This is great. Okay, but
wait, you might say, what about diversity? That's a great question. These calculations assume that
the prediction satisfies a statistical condition called independence. Independence implies that
knowing one person's prediction tells you nothing, absolutely nothing, about another person's prediction.
So independence is assuming substantial diversity between the predictions, because it's saying that
one prediction contains no information about the other prediction. Now here, because we had three
people, Amy, Bell, and Carlos, the independence assumption implies that Bell's prediction is
no more like Carlos's prediction than it is like Amy's prediction. So all these predictions are very
diverse. Now in general, this isn't going to be true. One pair of predictions might be more
similar than another pair. So Amy and Bell's predictions might tend to be more like, and Carlos
might be different. Then it'll be the case that Carlos's prediction will be less correlated with
Amy's prediction than Amy's prediction is with Bell's. This notion of correlation captures
similarity. So if the correlation is positive, predictions are similar, and if it's negative,
they tend to be different. And if it's zero, they're independent. So independence, or what is
sometimes called zero correlation, is a convenient assumption, and it allowed us to make those
estimates we did about how much weight we should put on people. But as I mentioned, it usually won't
hold. So if we take into this correlation into account, then we can assign even better weights.
But how do we do it? Here's the logic. The less correlated one predictor is with the other,
the more diverse those predictions will be, and the more weight you should receive. Now the more
correlated a predictor is with the others, the less weight that person should get, and the reason
why is because it means they're sort of less diverse. Note the parallel to the diversity
prediction theorem. Their collective accuracy depends on individual accuracy and diversity.
Optimal weighting theory reinforces this exact same logic. More accurate predictors should
receive more weight, as should more diverse predictors. Think of it this way. By assigning
weights correctly, we can, in effect, both increase accuracy and increase diversity. And if we can do
that, then we increase collective accuracy in two ways. In effect, by shifting the weights, it's as
if we make the individuals both more accurate and more diverse. So it's win-win. Now there's a problem
here, and that that is reality is going to rear its ugly head. We've got a lot going on. So let
me stop for a second and summarize. So we have these two deep insights. The first is that collective
accuracy depends on individual accuracy and on diversity. Second, and this shouldn't be a surprise,
we should therefore attach weight to people based on their accuracy and on their diversity.
Diversity. More accurate people should get more weight, and so should more diverse people.
Now we've already seen that first insight in action. We saw that the diversity prediction
theorem always holds. What about our weighting results? Do these always hold? Well first,
these results do their mathematical identities, but unlike the diversity prediction theorem,
they rely on us being able to know the correlation of the predictions. We may not know that. In the
diversity prediction theorem, we need only know the individual predictions themselves, and those
are things we know for sure. And from there, we can compute the individual error and collective
diversity so the theorem always works. But as I just mentioned, for these weighting equations,
we need to know the correlations, and we may not know those. What we do is we have estimates of
those from past data. Now if we know those measures with a high degree of accuracy,
then we can do better by assigning weights. But if we don't know those statistics with much accuracy,
well that can be problematic, and here's where reality sort of rears its ugly head. Now to avoid
complicating matters too much, I'm going to consider just accuracy and leave the whole
diversity correlation thing out of the picture for a second. Let's suppose I have just two predictors,
Amy and Bell, and let's recall, remember Bell's accuracy was twice that of Amy's. So using our
accuracy weighting formula, Amy's weight should equal 1 over 1 plus 2 or 1 third, and Bell's
weight should equal 2 over 3 or 2 thirds. So we should place more weight on Bell, and in fact,
twice as much weight. However, it could probably be the case that in fact, Bell's no more accurate
than Amy. It's just that I don't have very much data, and Bell just randomly happened to predict
more accurately during those first six periods. That would mean by placing more weight on Bell,
we'd be costing ourselves some diversity, right? We'd lose diversity by attaching less weight to
Amy, but not gaining any accuracy. So Bell's not really any more accurate than Amy, she just was
fortunate on her first six predictions, and we're sort of messing things up by putting weights on
it. So if that's the case, by putting on those weights, we're going to do worse, right? We're
just attaching more weight to people who randomly happened to have done better early, and we're
going to make the collective prediction less accurate. So for this reason, the fact that
weighting can reduce diversity, and we can't be sure of the weights unless we have lots of data,
non-equal weighting suddenly looks a little bit less attractive. And in fact, J. Scott Armstrong
of Wharton Business School, who's a leading scholar of prediction, he spent his career
studying prediction, argues that we shouldn't do unequal weighting. And here, I'm going to quote
him exactly, he says, use equal weights unless you have strong evidence to support unequal weighting.
Well, what would be strong evidence? Well, it certainly wouldn't just be six predictions,
you'd run a lot of them. And what might an example be? Well, remember in the previous lecture,
we studied the Netflix Prize. Remember in that case, the models were based on millions of predictions,
millions. So it should come as no surprise, none, that in the Netflix Prize, the weights assigned
to each model were not equal. Let me show you exactly how unequal. So I'm going to show you a graph.
And this graph shows the model weights for the ensemble models. Remember the ensemble consisted
of 23 teams from 30 countries? They had 49 models that they combined to produce the model that ended
up tying Belchor. Now, two things stand out. First, there's incredible variation in the weights.
And second, some of these weights are negative, right? If you look at these weights,
down here, they're negative. More on that in just a second. A brief aside, before I move on,
determining which weights to apply to each model, this was a huge challenge, a big intellectual
challenge. And the members of the ensemble published papers in leading journals highlighting
the state-of-the-art techniques they used to develop these weights. Now, the theory that
we've just walked through said that the weight should depend on accuracy, with more accurate
models getting more weight and less accurate models getting less weight. That's if we don't
consider correlation. Well, let's see if that's true. Well, here's a graph that shows the weight
attached to each model. That's on this axis. And on this axis, you see the accuracy, where these are
sort of more accurate models over here and less accurate models to the right. Now, what you see
is the most accurate model in their group, this one right here, gets the most weight. And that
makes sense. Now, if you look across the board, you see there seems to be sort of a slight fall
off, right, as we move this way and less accurate models get less weight. But we see that correlation
is really, really loose. We also see that that accuracy really isn't accounting for much weights,
and some of these things, again, get negative weights. So what accounts for the fact that
accuracy isn't explaining all the weights in these models? Well, we know it's diversity.
Models that are like other models, even if they're accurate, get less weight. Think back to the graph.
It's basically saying that optimal weights depend much more on the model's diversity
than on its accuracy. That's why we see so little correlation. Diversity matters so much,
again, that some highly accurate models get negative weights. Now, how can that happen,
you might wonder? That's a really good question. And here's one possible explanation.
Let's step away from movies for a second and consider a different task. Suppose you work for
a ketchup company, and one of your competitors introduces green ketchup. This actually happened,
by the way. Heinz has green ketchup. Tastes the same. Let's further suppose this ketchup has
four uses, hot dogs, burgers, fries, and eggs. Those are the four things that people put ketchup on.
Now, the effect on your sales can really be understood by understanding what's the, quote,
unquote, market share of green ketchup going to be for each of these four uses. So that's what you
want to unpack. What's the effect of ketchup in the hot dogs, burgers, fries, and eggs market?
Well, let's walk through it. Let's think how this works. So what we want to do is we want to say,
here is the total effect of green ketchup on our markets for hot dogs, burgers, fries, and eggs.
Now, let's suppose we've got three predictors, three sort of market forecasters. I'm going to
call them Arun, Rebecca, and Cherise. Now, each of these people has a model. Now, Arun has a model
that considers hot dogs, burgers, and fries. So we're going to say that Arun's model is hot dogs
plus burgers plus fries. So he leaves off eggs. He doesn't think about eggs. Rebecca considers the
effects on the burger and fry market, right? So she thinks she has burgers plus fries. And she
thinks no reasonable person is ever going to put green ketchup on a hot dog. So it's going to make
the hot dog look sort of green, and no one's going to put green ketchup on an egg. Finally,
let's assume that Cherise also leaves out hot dogs, but she considers the other three. So she
has burgers plus fries plus eggs. So Arun and Cherise each consider three uses. Rebecca considers
only two. So it stands to reason that Arun and Cherise should be more accurate, and so we should
put more weight on their opinions. But let's look more closely at this using some mathematics.
Remember the truth here is right up here. This is the truth. h plus b plus f plus e. Now here are
the models of the people we have. Arun's is h plus b plus f. Rebecca's is b plus f. And Cherise's
is b plus f plus e. Okay, lots of figures here, lots of numbers here, but let's watch this.
If I attach a weight of plus one to Arun and plus one to Cherise and minus one to Rebecca,
what in a sense I'm doing is I'm adding up Arun plus Cherise minus Rebecca. And what I'm going
to get is I'm going to get h, and then I'm going to get plus b, but then minus b from Rebecca,
and then plus b from Cherise. So I'm going to get plus b, and then I've got a plus f, a minus f,
and a plus f. So plus f, and then I've got the eggs that Cherise considers. This adds up to h plus b
plus f plus e, which is the truth, the exact function we're trying to estimate. So getting it
exactly right in this case required putting negative weight on Rebecca. And the reason why
is because the models have to cancel out the duplication of the other models. So both Arun
and Cherise are considering b and f, just like Rebecca does. But if we add Arun and Cherise
together, we get two b's and two f's. And by subtracting out Rebecca, we subtract off a b and
an f, and then we get exactly the function we want. So notice that these, notice that these
weightings depend less on the relative accuracy of the models, and depend a lot more on the
diversity of the things they consider. That's why when we look at the Netflix prize data,
we see some positive and some negative. So how do we put this into practice? Well,
first, we've got to distinguish between two types of situations. In the first type of
situation, we don't have much data on past experience. And maybe some predictors seem
a little more accurate than others, and some don't. Here, I think it makes sense to heed
Scott Armstrong's advice. You don't have strong evidence, so equal weighting is not a bad choice.
Though it could be the case you want to abandon models that look like they haven't been useful
at all. So if somebody's been really bad, you may want to give them less weight. Second,
when you have lots and lots of data on past performance, then yes, you can take into account
ability, and you can take into account diversity and put more weight on the more accurate models.
But as the Netflix case, as well as the mathematics suggest, right, you can do better
if you think really carefully about how to balance these models out and take into account
both the accuracy and diversity. So how is this done? Well, this can be done by taking past data
and breaking it into two sets, just like they did in the Netflix prize, a training set and a
testing set. And you can only do this if you've got lots of data. So in the training set, you
explore different combinations of predictors to see which weights give you the most accurate
predictions. After you've come up with what you think are a good set of weights, then you check,
you verify that it proves more accurate on the testing set as well. What about cases where you
don't have past data? You've got people in a room making predictions and forecasts. You might work
for an advertising company and have two sets of images in front of you to put on a, let's say,
a box of college frosting flakes. So one might be the old Tony the Tiger instead of a classic box,
and another might be a sort of a hipper, slimmer Tony. How do you make those sort of predictions?
We've got 12 people in the room. Each one's giving you an opinion. How do you weight them?
So you've got to think, what are the proxies for accuracy I have? These might be past record.
These might be strength of the argument. Both of these might lead you to put more weight on
someone's opinion. Note implicit in what I'm saying, though, is what should not matter. Reputation
shouldn't matter. Status shouldn't matter. Charisma shouldn't matter. If someone's status
of reputation resulted from making good forecasts in the past, then sure, give them more weight.
But you need a reason. You need a reason to give someone more weight. It has to be strength of
argument. How might you take into account diversity in a situation like this? Well, think about how
different the models are. So when people explain their thinking, think about how different they are
and place more weight on models that offer compelling but distinct logic. So again, you
should be careful not to place too much weight on any one model because you're going to lose the
wisdom of the crowds. But you want to balance taking into account both accuracy or expected
accuracy and diversity. Now, one more point on diversity here. I mean diversity in the reasons
not in the numerical predictions. This is really important. So let me give an example here. So
imagine you're a publisher and you have predictions. Three different people make predictions for sales
of a new book. Two of your markers make predictions based on models based on the attributes of the
book and what consumers like. Think of this just like our Netflix models. So the first of these
two might predict the book is going to sell 10,000 copies. The other might say 11,000 copies. Now,
they're using similar models, but they differ in the attributes that they think are important and
they're assigning to the customers. Now, you've got a third publicist and this third publicist says,
I've got a totally different model. This is the author's fourth book. His previous three books
have sold 5,000, 7,000, and 9,000 copies in sequence. If this trend continues, the book should
sell 11,000 copies. He also shows you 20 other authors that he's sort of randomly chosen from
the pile who've written three books and found similar trends for each one of them. They've had
this sort of linear increase. So now you've got these three predictions, 10,000, 11,000, and 11,000.
You could just average them and say 10,666. If you think numerically, the 10,000 is the most
diverse. So you might say, oh, that's the diverse prediction, so I'm going to give it more weight
and make an estimate closer to 10,500. But that's not the right way to think.
It's the third model, the one based on trends, that's the most logically diverse. The other two
use the same reasoning, but just rely on different data. Equally important, the third model also seems
relatively plausible. It seems like it makes a lot of sense and there's data to back it up.
Therefore, because that third model seems both accurate, plausible, and diverse, right? It should
get more weight and maybe you should creep towards maybe 10,750, move more towards the 11,000 figure.
So in other words, you should focus on the diversity of the models,
not on the diversity of numerical predictions. So in thinking about trying to attach the correct
differential weight, it's going to require experience, practice, knowledge of the problem,
and understanding of how accurate and diverse you think the predictors are. Now you need to know
weighting's advantages in the pitfalls and it makes one better able to make judicious assignments in
terms of where you assign weights and how you assign weights.
Okay, so we've now covered how to weight opinions and predictions. Now we've got to go back
and address the first question and that is who belongs in the room? Whose predictions do you include?
Now we can answer this question at two levels. First at the most basic level, the diversity
prediction theorem tells us that we should add someone if their net effect on accuracy minus
diversity is positive, right? So that's easy. So a no-brainer addition is going to be someone
who can both increase accuracy and increase diversity. Those people are going to be rare though.
More often, someone who is either going to increase accuracy but decrease diversity
or decrease accuracy and increase diversity. Here's a funny thing though. The first type,
the higher, more accurate predictors, tend to be more acceptable to groups and teams.
Almost no one's going to complain about adding someone to a group who's better than the average
group member at the task. Nobody's really against adding talent, right? If you say,
let's invite Robin to our group, she's better at predicting than we are. Everybody's going to say
fantastic, let her come. But think of the second type, someone who's less accurate than the average
team member, but diverse. They can add just as much value, but they're going to be a harder cell
to the team. Here's what you've got to say to your team. Hey, let's bring in Scott here. He thinks
differently than us and he's less accurate than we are. The image of a lead balloon should come
to mind. And here's why understanding the mathematics matters. That's why we walked through that
theorem. If people know the diversity prediction theorem and you say to them, look, we're really
accurate given our type of model. Let's add someone who uses another type of model that may be a
little bit less accurate than ours, but who's likely to make very different predictions. Now,
keep in mind, collective accuracy depends on individual accuracy plus diversity. We've got
a lot of individual accuracy. We're only going to be compromising a little bit,
but we're going to add a lot of diversity. Well, now this might sell, and I hope it does sell,
because it's going to lead to better predictions. But that's just adding one person. Let's think
about the problem more generally. It's sort of fun to think about. So let's think about a case.
So suppose you're on a chain of retail establishments. These are electronic stores, restaurants,
bookstores. And on a regular basis, you need to forecast sales of new products. Now, for years,
you've relied on an in-house market researcher. We've read some books about the wisdom of crowds,
and you're watching this course thinking, hey, I should tap into diversity. So you have this idea,
why don't I have a crowd of market researchers, and my store managers make sales forecasts together.
So I've got the market researchers I've used before, but now I've got these other people,
these crowd, you know, this crowd that consists of store managers that can contribute as well.
Well, this is a great idea. So great that several retailers do it already. Best Buy, for example.
So here's the problem you've gotten. I've got two populations. The first population is the market
researcher. The second population is the store managers. Think about each in more detail. The
market researchers are probably going to be highly accurate because they, you know, use statistics
and things like that, but they're also going to be highly correlated. So there's going to be low
diversity within type. They're also going to be expensive. Now, the store managers, they're not
going to be as accurate. Well, you hope they'll be accurate, but they're not going to be as accurate
as the forecasters, but they're going to be diverse. They're going to be diverse because
they've got different experiences. They're regionally varied. They run different stores of
different sizes and they're cheap. So how do you form the crowd? Well, let's think about it.
If your crowd's going to be small, if you're going to have like three or four people,
go with all market researchers. They're more accurate. Even though they lack diversity and
they cost a lot, they're still more accurate. But if you're going to go with 100 people,
if technology lets you get bigger, then you still may only want three market researchers. And the
reason why is they lack diversity. So when you add a fourth, you're just not going to get much more
from the first three and they cost a lot. In fact, you might even want to cut back to two.
Because at the same time, since you can have 100 people, you want lots of store managers
because you can tap into their diversity by having a huge number of them. And so their lack of
accuracy won't matter as much. But if you only had a few, like a handful of store managers,
you'd get a buy it sample. So as a general rule, and PJ Lamberson, a former postdoc in
mind, and I published a paper on this topic, the bigger the crowd, the more you should value
diversity. The smaller the crowd, the more you should value accuracy. Again, this isn't necessarily
an intuitive point, but it's a logical point. Okay, one last really neat insight that I learned
from someone who runs prediction markets. You could think of this as a single crowd of 100 people,
three market researchers, and 97 store managers. That's why I've been thinking about it.
Or you could think of it as two crowds, a small crowd of market researchers, and a large crowd
of store managers. So why would that matter? Well, if the two crowds agree, that's great,
because you should feel really confident in the collective prediction. But what if they disagree?
Well, this gives you an opportunity to think, to think about why they disagree, to dig even deeper,
and to perhaps make an even better prediction. Because one thing you can do is once you've
got these two predictions, you set us as a starting point from which to think even deeper.
Let me give an example. There was a technology company that invented a new printer, and they
were using both market researchers and their sales force as a crowd to make predictions.
Now the market researchers thought, this is going to sell a lot based on price, size, speed, etc.
It's just going to sell. Now the equivalent of the store managers, the sales force,
thought it wasn't going to sell. So they've got this difference in the prediction.
So the executives did some nosing around, and when they talked to the sales force,
the sales force sort of said, well, it's ugly. This is an ugly printer.
Well, the people who did the formal market research, they didn't have some sort of ugly
variable in their regression model. There was no variable called ugly for the printer.
And so as you might expect then, their prediction proved to be too high. And so by digging more
deep way, you could figure out that in fact it was the crowd that was correct to the market
researchers, who probably weren't. Okay, I titled this lecture, The Waiting is the Hardest Part.
And I believe that it really is. Determining how much we listen to some person at the expense of
others requires really careful thinking. Accuracy matters. So yes, our natural predisposition
to wait people are more accurate makes sense. But we also have to think about
how we know that person is accurate. If we're not accurate and who we think is accurate,
that's going to make us worse off. But diversity also matters and it matters a lot.
So we must make sure that we recognize what your main diversity of thought exists in our group
and put more weight on people who are more diverse. Now finally, the waiting only takes
place after we decide who should be allowed in the room. And here, we should also benefit by
greater inclusion, by more diversity. And as we just saw, right, the more diverse the potential
contributors, the more useful they can be. Right, there's a lot to think about here. I feel like
we're just getting started. So let's move on. Thank you.
