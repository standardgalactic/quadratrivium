In this lecture, we examine more problems that can be described by graph theory, including
the shortest path problem, the traveling salesman problem, and the Hamiltonian cycle problem.
As we'll see, some of these problems can be solved efficiently, while other, similar
sounding problems are so hard that nobody's been able to find a fast algorithm to solve
them.
Now, what's an algorithm?
An algorithm is a systematic method for solving a problem.
Its only requirement is that it's solved the problem in a finite number of steps.
We've encountered algorithms in this course, such as the seed planting method for raising
numbers to powers, and the greedy algorithm for finding a minimum weight spanning tree.
But greed is not always good.
For example, in this simple shortest path problem, find the shortest path from vertex
one to vertex four, the cost of the shortest path from one to four is seven.
We go from one to three to four, which has a total cost of two plus five is seven.
But if you were instead, if you'd been greedy, and you said, hmm, what should that first
step be?
Should I take a step of weight one or a step of weight two?
If you took that step of weight one, you would have wound up with a cost of 11, which would
have been worse than seven.
So greedy algorithm doesn't work in the shortest path problem.
Now, that was a simple problem, but what would you do to find a shortest path in a
graph like this, or a graph like this, like your GPS tries to do?
Here's a more complicated shortest path problem, although it's still a very, very small problem.
People routinely solve shortest path problems with thousands, if not millions of vertices
and edges in the graph.
So here's a more complicated one.
Our goal is to find the shortest path from the vertex labeled A to the vertex labeled
B. And in the process, in the algorithm that we're going to come up with, we're not only
going to find the shortest path from A to B, we're going to find the shortest path from
vertex A to every other vertex in the graph.
Take a moment.
Go ahead and pause if you want to see if you can find the shortest path from A to B in this
graph.
I'll wait.
The algorithm that we're going to use is called Dykstra's algorithm.
And what's really cool about Dykstra is his name has a consecutive i, j, k in it.
At least I think that's kind of neat.
OK, so here's the algorithm in front of you.
We'll go through it step by step as we need it, starting with step 0, which is temporarily
assign c of A to B0, the cost of vertex A to B0, and the cost of everything else to
be infinite.
OK, so here's our graph.
c of x represents the current cost of getting to node x.
That's what c of x gives you.
None of these costs are permanent yet.
OK, next step one.
Find the node x, and I use the word node in vertex interchangeably.
Find the node x with the smallest temporary value of c of x.
If there are no temporary nodes or if c of x is infinity, then you're done.
The algorithm stops.
Node x is now labeled as permanent.
That means c of x and the parent of x will not change again.
OK, so let's see what that means in terms of this graph.
So we find the temporary node that's got the smallest label.
That's vertex A. It's got a cost of 0.
And we label that as permanent.
That 0 isn't going to change.
OK, now step two.
For every temporarily labeled vertex y that's adjacent to x, in this case x is vertex A,
make the following comparison.
If the cost of x plus the cost of going from x to y is less than the cost of y, then we
change things.
What do we change?
The cost of y is changed to that smaller number, the cost of getting to x plus the
weight of the edge from x to y.
And we assign y to have a parent of x.
I'll show you what that means in this graph.
OK, so our x here is vertex A. And we look at the three edges that come out of it that
have costs 4, 3, and 9.
All right, so I look at 0 plus 4, and I compare that with infinity.
Well, 4 is much less than infinity, so we change that infinity to a 4.
And we point back to vertex 0, saying that's the vertex that made me 4.
Then we look at the next edges, 3 and 9.
So the edge with 3, we see 0 plus 3 is less than infinity.
0 plus 9 is less than infinity, so we change those infinities to 3s and 9s, and they point
back to 0, saying that's what gave me my new cost.
And then step three, we return to step one.
We find the node with the smallest temporary value.
So now I see 4, 3, 9, and a bunch of infinities.
The smallest number out there that's temporary is the 3.
So we make that permanent, OK?
So that vertex, that 3 isn't going to change.
It's parent pointing back to the 0.
That's not going to change.
Now we look at all the edges leaving 3 that go to other temporary vertices.
So here I see 3, and I see the edge with a weight of 2, and 3 plus 2 is 5.
How does that compare with 4?
The 4 is fine, just thank you, being 4.
It doesn't want to change to 5, so we move on.
3 plus 1 is 4.
That's better than infinity, so we change that infinity to a 4, and we point back.
3 plus 3 is 6.
That's better than infinity.
3 plus 5 is 8.
Now look what happens here.
8 is less than 9.
So we're going to change that 9 to an 8, and we're going to point back.
Instead of pointing back to 0, we're going to point back to the 3 saying yes,
that's the vertex that made me 8.
OK, now we go and look for the cheapest vertex,
the temporary vertex with the smallest weight.
There's a tie.
There's a 4 and a 4.
Dijkstra's algorithm says you can choose either one with a tie.
So we'll choose this one.
We make that permanent.
There's only one edge that goes to another temporary vertex.
That's the 5, but 4 plus 5 is bigger than 4, so we move on.
The next vertex is 4, and so we make that permanent.
And we look at the edges that go to other temporary places.
So 4 plus 4 and 4 plus 6.
The 4 plus 6, that beats infinity, so the infinity becomes a 10 and points back.
The 4 plus 4 is 8, but that's worse than 6.
And the algorithm continues in similar fashion.
OK, so what do we do?
We find the cheapest vertex.
That would be the cheapest temporary vertex is 6.
We make that permanent.
We look at the edges that go out to other temporary vertices.
6 plus 5 is 11.
No thank you, says the 10.
6 plus 7 is 13.
That beats infinity, so that becomes a 13.
6 plus 1 is 7, so that's better than the 8.
So we change that 8 to a 7 and point back to the 6.
All right, next, the next cheapest one is the 7.
We see that 7 plus 5 is 12.
That beats 13.
Now notice that even though we've reached vertex B,
even though that was our goal, we can't stop the algorithm now,
because that 12 isn't permanent yet.
There's still a possibility that it's going to change.
OK, we go to 10.
That's the next cheapest temporary edge.
And we look at its vertex.
Ooh, look at that one.
10 plus 1 is 11.
That's less than 12.
So that 12 becomes an 11.
And finally, we go to the 11, we make it permanent,
and the algorithm stops.
By the way, did you find a shortest path that
had a total cost of 11?
What is the shortest path?
Right now, we have found the shortest path not just from A
to B, but we have an entire tree of shortest paths from node A
to all other nodes in the graph.
If you just want to find the shortest path from A to B,
you just start at B, and you backtrack to the vertex that
gave you the last cost.
So here we have 11 points to 10, and 10 points to the 4,
and 4 points to the 3, 3 points to the 0.
There's your shortest path from node A to node B.
So you may ask, how fast is Dijkstra's algorithm?
So suppose G has n vertices and m edges.
Let's analyze this algorithm.
Step 0 has time n.
That is, step 0 was the one where we assigned 0 to node A
and infinity to everything else.
We got to do one thing to each of the n vertices,
so it has time n.
Step 1 of finding the cheapest temporarily labeled
vertex, that's called at most n times.
And when you're looking on your list of all the cheapest
vertices, that takes you at most n steps.
So step 1, its total amount of time that it's going to be
used is going to be at most n times n, at most n squared.
I'm being very conservative here.
Step 2 is basically looking at each edge once a vertex
becomes permanent.
So each edge is going to be examined essentially twice.
Only one of those times are you really going to use it.
You'll examine it at most twice.
So this has a time of at most 2 times m, because I've got m
edges, each edge gets used at most twice.
Roughly speaking, if our graph has n nodes and m edges,
then Dijkstra's algorithm takes no worse than about n
squared plus 2m steps.
And since m is less than n squared, this is no worse
than 3n squared steps.
Computer scientists say this algorithm is on the order of
n squared.
They say it's big O of n squared.
What that means is it's on the order of at most n squared
steps times a constant.
And you could find a constant out there.
Like maybe it takes you like 10 n squared steps.
We call that big O of n squared.
Incidentally, a clever computer scientist can actually do
this problem in better than O of n squared time.
They could do it in time like m times the log base 2 of n
steps, where m is the number of edges, n is the number of
vertices, and that might be substantially less than the
order of n squared.
Now, given a problem of size n, an algorithm with a run
time of big O of n to the k for some fixed number k is
called a polynomial time algorithm.
So something that's of order of n cubed, that's a
polynomial time algorithm.
Order of n to the fourth, that's a polynomial time
algorithm, but it's a slower one.
And where n is the size of your problem.
Now, the size of a graph problem is typically like the
number of vertices or the number of edges.
If the size of your problem was just an integer, like you
were factoring or something, then it's how many bits does
it take to represent the integer?
Anyway, so if your input was just an integer n, the size
of that problem would be like the log base 2 of n.
Sometimes these algorithms that run in polynomial time are
called efficient, or sometimes we just simply
say that they're good.
We define the computational complexity of a problem to be
the run time for the fastest algorithm
for solving that problem.
Now, that can change over time as people find faster
algorithms for doing problems.
Now, if we don't know of a polynomial time algorithm,
well, maybe we're just not being clever enough.
Is there some way to measure that problem's complexity?
Let me answer that by looking at some examples.
So here first let's start off with some easy problems.
So the shortest path problem we've just seen can be solved
by Dijkstra in time on the order of n squared.
Since n squared's a polynomial, then that's considered
an easy problem.
The question is, g, a connected graph.
How could you tell if a graph's connected?
I know you can tell just by looking at one, but how does
your computer tell?
Well, one way is it can run Dijkstra's algorithm starting
at some vertex a and finding the cost of the shortest path
to every other vertex in your graph.
If when Dijkstra's algorithm finishes, if there's still any
vertices out there with a cost of infinity, then there are
no paths that get from a to that vertex, and your graph
would not be connected.
On the other hand, if every vertex has a finite cost, then
you know the graph is connected.
Because we know that any vertex can reach a.
So if I can get from x to a and from a to y, then we know
it's possible to get from x to y.
How about the question, is my graph Eulerian?
Well, you remember from Euler's great theorem that if a
graph is connected and every vertex has even degree, then
the graph's Eulerian.
We've just seen that it's easy to tell if a
graph is connected.
And to see if every vertex has even degree, we just look at
every vertex and we count the number of edges that go
through it.
That can be done very efficiently.
So yes, determining if a graph is Eulerian is considered a
easy problem.
How about is a graph too colorable?
Can I properly color the vertices of a graph using just
two colors?
As I mentioned in the last lecture, yes, you can do that
by a greedy algorithm.
You just assign a color to vertex to one vertex, all its
neighbors get the opposite color, all the other neighbors
haven't been touched yet, they get the opposite color, and
the opposite color, and a chain reaction occurs.
And when you're done, you can check to see if the graph has
been properly too colored.
So those were some easy problems.
Now let's look at some hard problems.
So no efficient algorithms have been found for any of these
problems.
We know there's an algorithm for finding the shortest path
in a graph efficiently.
But surprisingly, there is no efficient algorithm known to
find the longest path in a graph for any possible graph.
And I'm looking at graphs that I'm allowing my graph here to
have positive edges or negative edges.
So problems like that can create, negative edges can
create problems.
Is the graph three colorable?
That is, is there a proper coloring on your graph that
can use at most three colors?
Does the graph have a Hamiltonian path?
That is, is there a path that goes through every vertex?
These are questions that we don't have efficient answers
for.
Does the graph have a Hamiltonian cycle?
Just as hard.
A problem we'll talk more about, the traveling salesman
problem.
That is, find a Hamiltonian cycle in a graph with weights
on the edges so that the sum of the weights is as
small as possible.
Now, if g is a graph with n vertices, let's look at the
three colorable problem.
If I want to determine if it's three colorable, then one
way, an algorithm, an actual algorithm for the three
colorable problem, is try all possible colorings.
How many colorings, whether they're proper or not, can be
given to a graph with n vertices.
Got three colors for the first vertex, three colors for the
second, three colors for the third, and so on.
There are three to the n ways of doing it.
And for each of those three to the n ways, you could check
to see how long it, whether your coloring is proper.
To do that, that's going to require on the order of three
to the n steps.
And that is not polynomial.
Don't get three to the n confused with n to the three,
which is polynomial.
Likewise, the Hamiltonian path problem can be done in time
on the order of n factorial steps.
Just look at every possible circuit that goes through your
graph, or look at every possible arrangement of your
vertices, one through n, and they're n factorial, those.
And for each of those, say, is this a Hamiltonian path?
Nope, nope, nope, there's no edge here.
Is this a Hamiltonian path?
Is this a Hamiltonian path?
Just try that on each of them.
And again, that'll take your time order n factorial, but
that's even worse than three to the n.
These algorithms are called exponential time algorithms.
And by the way, you might say, I see why three to the n is
exponential.
Is n factorial really exponential?
Yes, it is.
One way of seeing that is to use something called
Sterling's approximation, which says that n factorial is
approximately equal to n divided by e, that's the 2.718
number, n over e raised to the nth power times the square root
of 2 pi n.
I just like to show that formula off, because I think
it's really pretty.
And when I say approximate, I mean in a very rigorous way.
The ratio of the left side and the right side is 1 as n
gets bigger and bigger and bigger, that ratio gets
closer and closer to 1.
In fact, if we could find an efficient algorithm to solve
any one of these hard problems, then this could be turned
into an efficient algorithm to solve thousands of other
hard problems.
These problems, all the ones I've mentioned to you, are
called NP-hard, and there is a $1 million cash prize for
anyone who finds an efficient algorithm for
any of these problems.
Now, why are these problems so special?
It has to do with complexity classes.
Computer scientists have defined, well, two complexity
classes, actually they define lots of them, but the two
biggies are called P and NP.
The complexity class P is the set of decision problems
that can be solved in polynomial time.
A decision problem is simply a question that has a yes or
no answer.
So a question like, is G Eulerian?
Yes or no?
Is G too colorable?
Yes or no?
Does G have a path from A to B with cost below 100?
Yes or no?
Those are decision problems.
Here's a question that's not a decision problem.
Find the shortest path from A to B. You can't say yes or
no to that, OK?
We'll talk about that later, but the others are decision
problems, yes or no answers.
The complexity class NP is the set of decision problems
where a yes answer can be verified in polynomial time.
So NP, by the way, stands for non-deterministic
polynomial.
That's a mouthful.
It doesn't stand for non-polynomial.
What that simply means is a problem where a yes answer
can be checked in a polynomial amount of time, polynomial
in terms of the size of the problem.
So the NP problems include all the problems that were in P
like, is G Eulerian?
Is G too colorable?
Does G have a path from A to B with cost below 100?
If the answer is yes, and I give you an Eulerian trail
or an Eulerian cycle, you can check that it works.
OK, let me see.
You say it's Eulerian.
Let me see your solution.
Yep, yep, yep, yep, yep, yep, that goes through every edge.
I'm happy.
Is it too colorable?
If I give you a proper two coloring, you can check that
no adjacent vertices have the same color.
So those can be checked efficiently.
But we can also find efficient solutions.
We can also check.
We can verify other problems that are hard.
That is, here are some other problems that are in the set NP.
Does G have a Hamiltonian path?
Well, I don't know how to find one efficiently,
but if you give me a Hamiltonian path,
I can check that it is indeed a Hamiltonian path
in an efficient way.
I just follow your path.
I see that I've contacted every vertex once.
I'm happy.
Is G three colorable?
Again, you give me a proper coloring that uses three colors.
I can check that no adjacent vertices have the same color.
Is there a Hamiltonian cycle with total cost below 100?
You give me the cycle.
I can add up the costs and check that it's
Hamiltonian in a polynomial amount of time.
These last problems are examples of what
are called NP complete problems, where
it's been shown that if any of these problems have
a polynomial time solution, then every problem in NP
has a polynomial time solution.
I'll say that again.
NP complete means if any of these problems
have a polynomial time solution, then all the problems in NP
would have a polynomial time solution.
This would imply that the set NP doesn't merely contain
everything in P, but is the same as P,
that everything could be done efficiently.
Let me give you a sense as to how you could even possibly
hope to show that.
The hardest problem in NP is called the satisfiability
problem, and it goes like this.
Given a logical expression like a or b, and not b or not c,
and c or not a, can we assign truth values, true or false,
to a, b, and c, so that this expression evaluates to true?
It's easy to verify that if I say, sure, let a equal false,
b equal true, and c equal false, that this expression
evaluates to true.
That is, each of those individual expressions, a or b,
not b or not c, c or not a, those all evaluate to true.
And when I hit each of them with and and and,
then true and true and true is still true.
Although it's easy to verify a solution,
you might ask, is there an efficient way to find a solution?
Now, we don't know of any.
But in 1971, Steven Cook proved that if we could find
an efficient solution to any satisfiability problem,
then it could be used to find an efficient solution
to any problem in NP.
It's a very deep result.
That problem is called NP-complete.
The satisfiability question was the original NP-complete
problem, since solving it would completely solve everything
in NP.
Then Professor Richard Karp came along with a list of several
other problems in NP, including the Hamiltonian cycle problem.
And he showed that if you could efficiently solve any of these
problems, then you could solve the satisfiability problem.
That is, you could turn the satisfiability problem into a
Hamiltonian cycle problem in such a way that the Hamiltonian
cycle problem size hasn't grown all that much.
And therefore, if you could solve the Hamiltonian cycle problem
in an efficient way, then you could solve the satisfiability
problem in an efficient way and therefore solve all the problems
in NP in an efficient way.
These problems are also called NP-complete.
And this led to thousands of other problems being classified
as NP-complete.
A polynomial time solution to any of them
would imply that P equals NP.
Now let's talk about the traveling salesman problem.
Given a graph G with weights on its edges,
the traveling salesman problem, abbreviated the TSP,
is to find a Hamiltonian cycle in the graph whose weight is
as small as possible.
For example, starting in Sacramento,
fly to each state capital and return home so that your total
miles was as small as possible.
This problem is not in NP because it's not a decision
problem.
You could say, well, here's the way to do it.
I can verify that you had a Hamiltonian cycle,
but I can't verify that it was the smallest one possible.
It's not a decision problem.
It's not a yes or no question.
Nevertheless, if a polynomial time solution could be found
for it, it would also imply that P equals NP.
A problem like this that's not in NP but still would give you
efficient solutions to all of NP,
those problems are called NP-hard.
Actually, everything that's NP-complete is NP-hard,
but there are other problems that aren't in NP
that are also NP-hard.
So does P equal NP?
Most computer scientists don't think so,
but nobody's been able to prove it or find an efficient
algorithm for any of the NP-complete problems.
There's a $1 million prize offered by the Clay
Mathematical Institute for a proof or disproof of the
theorem of the conjecture.
Well, the conjecture really is that P does not equal NP.
So a proof or disproof would win you a million dollars.
In practice, what do you do if someone asks you to solve
an NP-hard problem?
Do you just throw up your hands and say,
oh, I've taken this course.
I know there are no efficient solutions.
No, you just do the best you can.
Often there are efficient algorithms that can guarantee
a solution that's within a certain percent of optimality.
For example, the minimum spanning tree algorithm,
which can be done efficiently, can actually be adapted to
produce a Hamiltonian cycle, a solution to the TSP that's
at most twice as expensive as the minimum cost
Hamiltonian cycle.
More sophisticated methods exist that will,
with high probability, find a solution to a problem with
millions of cities to within 2% to 3% of optimality.
That's pretty impressive.
So long as you don't insist on a perfect solution,
you can still do well in practice.
So we've seen the graph theory problems can be hard.
I want to show you that combinatorics problems can be hard too.
For example, take this problem.
How many matches do we have?
No, not those matches.
I mean, yeah, matches like that.
Matches in a graph, like the graph K44 in the stable
marriage algorithm.
So it's the complete bipartite graph, four vertices on the
left, four vertices on the right.
How many matches?
It's easy to see there are four factorial ways.
Because the first vertex can be matched to four players, to
four vertices, the second one can be matched to three, then to
two, then to one.
No problem.
And so in general, KNN has n factorial matches.
But what if we throw away a bunch of edges?
Then the same combinatorics problem can get really hard.
Although there exist efficient algorithms that can tell us if
there is at least one match, the exact number of matches is
in a complexity class called sharp P complete.
That sharp is both the musical sharp symbol.
It's also the symbol for the number sign, right?
How many?
The sharp P complete complexity class are problems
where if you could actually count this problem exactly in
polynomial time, then you would prove that P equals NP, and
you'd win a million dollars.
Not only can combinatorics be hard, number theory can be
hard, too.
As we saw, the problem of factoring a large composite
number seems to be hard.
But it's not known whether that problem is NP complete.
But the fact that it's hard actually has its advantages.
It allows us to create some very secure
methods for cryptography.
On the other hand, prime detection, determining whether
or not a number is prime, is actually easy.
And that's a very recent result.
It was proved in 2002 by three mathematicians from India who
showed that the prime detection problem is in the set, is in
the complexity class P. And what's really cool about this is
that two of those three mathematicians were
undergraduates.
In our final lecture, we'll take a look back, put
everything together, glance beyond the bounds of this
course, and explore some truly magical applications of
discrete mathematics.
