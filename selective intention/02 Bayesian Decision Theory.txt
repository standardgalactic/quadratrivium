BAYESIAN DECISION THEORY 

J. Elder 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

Credits 

2 

Probability & Bayesian Inference 

  Some of these slides were sourced and/or modified 

from: 
  Christopher Bishop, Microsoft UK 
  Simon Prince, University College London 
  Sergios Theodoridis, University of Athens & Konstantinos 

Koutroumbas, National Observatory of Athens 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

3 

Probability & Bayesian Inference 

1.  Probability 
2.  The Univariate Normal Distribution 
3.  Bayesian Classifiers 
4.  Minimizing Risk 
5.  The Multivariate Normal Distribution 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Problems for this Meeting 

4 

Probability & Bayesian Inference 

  Problems 2.1-2.4 
  Assigned Problem: 2.2 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

5 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 1.  Probability 

Random Variables 

7 

Probability & Bayesian Inference 

  A random variable is a variable whose value is uncertain. 

  For example, the height of a randomly selected person in this 
class is a random variable – I won’t know its value until the 
person is selected. 

  Note that we are not completely uncertain about most random 

variables.   

  For example, we know that height will probably be in the 5’-6’ range.   

  In addition, 5’6” is more likely than 5’0” or 6’0”.  

  The function that describes the probability of each possible 

value of the random variable is called a probability 
distribution. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Probability Distributions 

8 

Probability & Bayesian Inference 

  For a discrete distribution, the probabilities over all 

possible values of the random variable must sum to 1. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Probability Distributions 

9 

Probability & Bayesian Inference 

  For a discrete distribution, we can talk about the probability of a particular score 

occurring, e.g., p(Province = Ontario) = 0.36. 

  We can also talk about the probability of any one of a subset of scores occurring, 

e.g., p(Province = Ontario or Quebec) = 0.50. 

 

In general, we refer to these occurrences as events. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Probability Distributions 

10 

Probability & Bayesian Inference 

  For a continuous distribution, the probabilities over all possible values of 

the random variable must integrate to 1 (i.e., the area under the curve must 
be 1). 

  Note that the height of a continuous distribution can exceed 1! 

S h a d e d   a r e a   =   0 . 6 8 3 

S h a d e d   a r e a   =   0 . 9 5 4 

S h a d e d   a r e a   =   0 . 9 9 7 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Continuous Distributions 

11 

Probability & Bayesian Inference 

  For continuous distributions, it does not make sense to talk about the 

probability of an exact score. 
  e.g., what is the probability that your height is exactly 65.485948467… inches? 

Normal Approximation to probability distribution for height of Canadian females 
(parameters from General Social Survey, 1991) 

p
y
t
i
l
i

b
a
b
o
r
P

0.16 
0.14 
0.12 
0.1 
0.08 
0.06 
0.04 
0.02 
0 
55 

µ =

5'3.8"

? 

s

=

2.6"

60 

65 
Height (in) 

70 

75 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
Continuous Distributions 

12 

Probability & Bayesian Inference 

 

It does make sense to talk about the probability of observing a score that falls within a certain 
range 

  e.g., what is the probability that you are between 5’3” and 5’7”? 

  e.g., what is the probability that you are less than 5’10”? 

Valid events 

Normal Approximation to probability distribution for height of Canadian females 
(parameters from General Social Survey, 1991) 

p
y
t
i
l
i

b
a
b
o
r
P

0.16 
0.14 
0.12 
0.1 
0.08 
0.06 
0.04 
0.02 
0 
55 

µ =

5'3.8"

s

=

2.6"

60 

65 
Height (in) 

70 

75 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
Probability Densities 

13 

Probability & Bayesian Inference 

Cumulative distribution (CDF) 

Probability density (PDF) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Transformed Densities 

14 

Probability & Bayesian Inference 

Observations falling within  x + !x

(

)  tranform to the range  y + !y

(

)

! px(x) "x = py(y) "y

! py(y) ! px(x)

"x
"y

Note that in general, !y " !x.

Rather, 

!y
!x

"

dy
dx

 as !x " 0.

Thus py(y) = px(x)

dx
dy

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
   
  
  
  
Joint Distributions 

15 

Probability & Bayesian Inference 

Marginal Probability 

Joint Probability 

Conditional Probability 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Joint Distributions 

16 

Probability & Bayesian Inference 

 Sum Rule 

Product Rule 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Joint Distributions:  The Rules of Probability 

17 

Probability & Bayesian Inference 

  Sum Rule 

  Product Rule 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Marginalization 

18 

Probability & Bayesian Inference 

We can recover probability distribution of any variable in a joint distribution 

by integrating (or summing) over the other variables 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Conditional Probability 

19 

Probability & Bayesian Inference 

  Conditional probability of X given that Y=y* is relative 

propensity of variable X to take different outcomes given that 
Y is fixed to be equal to y* 

  Written as Pr(X|Y=y*) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Conditional Probability 

20 

Probability & Bayesian Inference 

  Conditional probability can be extracted from joint probability 
  Extract appropriate slice and normalize 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Conditional Probability 

21 

Probability & Bayesian Inference 

  More usually written in compact form 

•  Can be re-arranged to give 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Independence 

22 

Probability & Bayesian Inference 

  If two variables X and Y are independent then variable X tells 

us nothing about variable Y (and vice-versa) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Independence 

23 

Probability & Bayesian Inference 

  When variables are independent, the joint factorizes into a 

product of the marginals: 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayes’ Rule 

24 

Probability & Bayesian Inference 

From before: 

Combining: 

Re-arranging: 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayes’ Rule Terminology 

25 

Probability & Bayesian Inference 

Likelihood – propensity for 
observing a certain value of 
X given a certain value of Y 

Prior – what we know 
about y before seeing x 

Posterior – what we know 
about y after seeing x 

Evidence –a constant to 
ensure that the left hand 
side is a valid distribution 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

End of Lecture 2 

Bayesian Decision Theory:  Topics 

27 

Probability & Bayesian Inference 

1.  Probability 
2.  The Univariate Normal Distribution 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 2.  The Univariate Normal Distribution 

The Gaussian Distribution 

29 

Probability & Bayesian Inference 

MATLAB Statistics Toolbox Function:   
normpdf(x,mu,sigma) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Central Limit Theorem  

30 

Probability & Bayesian Inference 

 The distribution of the sum of N i.i.d. random 
variables becomes increasingly Gaussian as N grows. 
 Example: N uniform [0,1] random variables. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Expectations 

31 

Probability & Bayesian Inference 

Condi3onal Expecta3on 
(discrete) 

Approximate Expecta3on 
(discrete and con3nuous) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Variances and Covariances 

32 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Gaussian Mean and Variance 

33 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

34 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 3.  Bayesian Classifiers 

Bayesian Classification 

36 

Probability & Bayesian Inference 

  Input feature vectors 

x = x1,x2,...,xl

!"

T

#$

  Assign the pattern represented by feature vector x 

to the most probable of the available classes 

  !1,!2,...,!M

That is, 

x ! "i : P("i | x) is maximum.

Posterior 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
 
   
   
Bayesian Classification 

37 

Probability & Bayesian Inference 

  Computation of posterior probabilities 

  Assume known 

  Prior probabilities 

P(!1),P(!2)...,P(!M )

   Likelihoods 

(
p x |!i

),

i = 1,2,…,M

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
   
Bayes’ Rule for Classification 

38 

Probability & Bayesian Inference 

(
p !i | x

) =

(
p x |!i
p x(

(
) p !i
)

)

,

where 

p x(

) =

M

"

i=1

(
p x |!i

(
) p !i

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
   
M=2 Classes 

39 

Probability & Bayesian Inference 

39    Given x classify it according to the rule 

If P(!1 x) > P(!2 x)   " !1
If P(!2 x) > P(!1 x)   " !2

  Equivalently:  classify x according to the rule  
)P !2
(
(
)P !1

) > p x |!2
(
(
) > p x |!1

(
If p x |!1
(
If p x |!2

)P !1
(
(
)P !2

) " !1
) " !2

  For equiprobable classes the test becomes 

(
If p x |!1
(
If p x |!2

(
) > p x |!2
(
) > p x |!1

) " !1
) " !2

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
   
   
  
Example: Equiprobable Classes 

40 

Probability & Bayesian Inference 

R

1

(

→

ω

1

)

and

R

2

(

→

ω

2

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
Example:  Equiprobable Classes 

41 

Probability & Bayesian Inference 

41    Probability of error 

  The black and red shaded areas represent 

(
P error | !2
  Thus 

) = p(x !2)dx

x0
$

"#

Pe ! P(error)
(
= P !2
x0
$

=

1
2

"#

(

(
) + P !1
)P error|!2
1
2

p(x !2)dx +

+#

$
x0

p(x !1)dx

) = p(x !1)dx

"

#
x0

and 

(
P error | !1

)P error|!1

(

)

  Bayesian classifier is OPTIMAL:  it 
minimizes the classification error 
probability 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
   
  
  
Example:  Equiprobable Classes 

42 

Probability & Bayesian Inference 

  To see this, observe that 
shifting the threshold 
increases the error rate 
for one class of patterns 
more than it decreases 
the error rate for the 
other class. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

The General Case 

Probability & Bayesian Inference 

  In general, for M classes and unequal priors, the decision rule 

43 

43 

P(!i | x) > P(!j | x)  "j # i $ !i

minimizes the expected error rate. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
   
Types of Error 

44 

Probability & Bayesian Inference 

  Minimizing the expected error rate is a pretty 

reasonable goal. 

  However, it is not always the best thing to do. 
  Example:   

  You are designing a pedestrian detection algorithm for an 

autonomous navigation system. 

  Your algorithm must decide whether there is a pedestrian 

crossing the street. 

  There are two possible types of error: 

  False positive:  there is no pedestrian, but the system thinks there 

is. 

  Miss:  there is a pedestrian, but the system thinks there is not. 
  Should you give equal weight to these 2 types of error? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

45 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 4.  Minimizing Risk 

The Loss Matrix 

47 

Probability & Bayesian Inference 

  To deal with this problem, instead of minimizing error 

rate, we minimize something called the risk. 

  First, we define the loss matrix L, which quantifies the 

cost of making each type of error. 

  Element λij of the loss matrix specifies the cost of 
deciding class j when in fact the input is of class i. 

  Typically, we set λii=0 for all i. 
  Thus a typical loss matrix for the M = 2 case would have 

the form 

L =

"
$
$
#

0 !12
0
!21

%
’
’
&

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Risk 

48 

Probability & Bayesian Inference 

  Given a loss function, we can now define the risk 

associated with each class k as: 

M

$

rk = !ki p x |"k
#
Ri

i=1

(

)d x

  Probability we will decide Class !i  given pattern from Class !k
  where Ri is the region of the input space where we 

will decide ωi. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
Minimizing Risk 

49 

Probability & Bayesian Inference 

  Now the goal is to minimize the expected risk r, 

given by 

r =

M

"

k =1

(
rkP !k

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Minimizing Risk 

50 

Probability & Bayesian Inference 

r =

M

"

k =1

(
rkP !k

)

where 

M

$

rk = !ki p x |"k
#
Ri

i=1

(

)d x

  We need to select the decision regions Ri to minimize the risk r. 
  Note that the set of Ri are disjoint and exhaustive. 
  Thus we can minimize the risk by ensuring that each input x 

falls in the region Ri that minimizes the expected loss for that 
particular input, i.e., 
(
!kip x | "k

Letting li =

)P "k(

#

)
,

M

k=1

we select the partioning regions such that
x $Ri if  li < lj   %j & i

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
   
Example:  M=2 

51 

Probability & Bayesian Inference 

  For the 2-class case: 
(
)P "1
(
l1 = !11p x | "1

(
) + !21p x | "2

(
)P "2

)

   and 

(
l2 = !12p x | "1

(
)P "1

(
) + !22p x | "2

(
)P "2

)

  Thus we assign x to ω1 if 
(
) p x | #2

(
)P #2

(
) < !12 " !11

(
) p x | #1

(
)P #1

)

(
!21 " !22
  i.e., if  
(
p x | !1
(
p x | !2

Likelihood Ratio Test 

)
) >

(
P !2
(
P !1

) "21 # "22
(
(
) "12 # "11

)
) .

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
   
   
   
   
Likelihood Ratio Test 

52 

Probability & Bayesian Inference 

(
p x | !1
(
p x | !2

? 

)
) >

(
P !2
(
P !1

) "21 # "22
(
(
) "12 # "11

)
) .

  Typically, the loss for a correct decision is 0.  Thus the likelihood 

ratio test becomes 
)
) >

(
p x | !1
(
p x | !2

(
P !2
(
P !1

? 

)"21
)"12

.

  In the case of equal priors and equal loss functions, the test 

reduces to 
)
) > 1.

(
p x | !1
(
p x | !2

? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
   
   
Example 

53 

Probability & Bayesian Inference 

  Consider a one-dimensional input space, with features 

generated by normal distributions with identical variance: 

)
p(x !1) ! N µ1,"2
)
p(x !2) ! N µ2,"2

(
(

    where 

µ1 = 0, µ2 = 1, and !2 =

1
2

  Let’s assume equiprobable classes, and higher loss for errors on 

Class 2, specifically: 

!21 = 1,  !12 =

1
2

.

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
    
 
 
Results 

54 

Probability & Bayesian Inference 

  The threshold has shifted to the left – why? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

End of Lecture 3 

Bayesian Decision Theory:  Topics 

56 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 
5.  The Multivariate Normal Distribution 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 5  The Multivariate Normal Distribution 

The Multivariate Gaussian 

58 

Probability & Bayesian Inference 

MATLAB Statistics Toolbox Function:   
mvnpdf(x,mu,sigma) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Geometry of the Multivariate Gaussian 

59 

Probability & Bayesian Inference 

  where ! " Mahalanobis distance from µ to x

Eigenvector equation:  !ui = "iui

MATLAB Statistics Toolbox Function:   
mahal(x,y) 

   where (u i,!i) are the ith eigenvector and eigenvalue of ".

  Note that ! real and symmetric  "  #i real.

See Appendix B for a review of 
matrices and eigenvectors. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Geometry of the Multivariate Gaussian 

60 

Probability & Bayesian Inference 

  ! = Mahalanobis distance from µ to x

where (u i,!i) are the ith eigenvector and eigenvalue of ".

   or y = U(x - µ)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
Moments of the Multivariate Gaussian  

61 

Probability & Bayesian Inference 

thanks to anti-symmetry of z  

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Moments of the Multivariate Gaussian  

62 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

5.1 Application:  Face Detection 

Model # 1: Gaussian, uniform covariance 

64 

Probability & Bayesian Inference 

Fit model using maximum likelihood criterion 

m face 

m non-face 

2

l

e
x
P

i

s  Face 

s  non-face 

Face ‘template’ 

59.1 

69.1 

Pixel 1 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
Model 1 Results 

65 

Probability & Bayesian Inference 

Results based on 200 cropped faces and 200 non-faces from 
the same database.  

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

)
t
i

H
(
r
P

How does this work with a 
real image? 

0.2

0.4

0.6

0.8

1

Pr(False Alarm) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Model # 2: Gaussian, diagonal covariance 

66 

Probability & Bayesian Inference 

Fit model using maximum likelihood criterion 

m face 

m non-face 

2

l

e
x
P

i

s  Face 

s  non-face 

Pixel 1 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
Model 2 Results 

67 

Probability & Bayesian Inference 

Results based on 200 cropped faces and 200 non-faces from 
the same database.  

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

)
t
i

H
(
r
P

More sophisticated 
model unsurprisingly 
classifies new faces 
and non-faces better. 

Diagonal 
Uniform 

0.2

0.4

0.6

0.8

1

Pr(False Alarm) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Model # 3: Gaussian, full covariance 

68 

Probability & Bayesian Inference 

Fit model using maximum 
likelihood criterion 

PROBLEM:  we cannot fit this model.  We 
don’t have enough data to estimate the full 
covariance matrix. 

2

l

e
x
P

i

Pixel 1 

N=800 training images 
D=10800 dimensions 

Total number of measured numbers =  
ND = 800x10,800 = 8,640,000   

Total number of parameters in cov matrix = 
(D+1)D/2  = (10,800+1)x10,800/2 = 
58,325,400  

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
 
 
Transformed Densities Revisited 

69 

Probability & Bayesian Inference 

Observations falling within  x + !x

(

)  tranform to the range  y + !y

(

)

! px(x) "x = py(y) "y

! py(y) ! px(x)

"x
"y

Note that in general, !y " !x.

Rather, 

!y
!x

"

dy
dx

 as !x " 0.

Thus py(y) = px(x)

dx
dy

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
   
  
  
  
Problems for this week 

70 

Probability & Bayesian Inference 

  Problems 2.7 – 2.17, 2.19 – 2.21, 2.23 – 2.27 are 

all good! 
  At least do problem 2.14.  We will talk about this 

Monday. (Hopefully one of you will present a solution!) 

  Also, MATLAB exercises up to 1.4.4 are good. 

  At least do Exercise 1.4.2.  We will talk about this next 

week. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

71 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 

5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 6.   
Decision Boundaries in Higher Dimensions 

Decision Surfaces 

73 

Probability & Bayesian Inference 

  If decision regions Ri and Rj 

are contiguous, deﬁne!
g(x) ! P("i | x) # P("j | x)
  Then the decision surface !

g(x) = 0

separates the two decision 
regions.  g(x) is positive on 
one side and negative on the 
other.!

(
Ri:  P !i | x

(
) > P !j | x

)

(
Rj :   P !j | x

) > P !i | x
(

g(x) = 0

+ 
 - 
)

73 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

!
   
   
  
  
Discriminant Functions 

74 

Probability & Bayesian Inference 

74   

If f (.) monotonic, the rule remains the same if we use: 

x ! "i  if:  f(P("i x )) > f(P("j x))  #i $ j

   

gi(x) ! f(P("i | x))

            is a discriminant function 

 

In general, discriminant functions can be defined in other ways, 
independent of Bayes.   

 

In theory this will lead to a suboptimal solution 

  However, non-Bayesian classifiers can have significant advantages: 

  Often a full Bayesian treatment is intractable or computationally prohibitive. 

  Approximations made in a Bayesian treatment may lead to errors avoided 

by non-Bayesian methods. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
  
   
End of Lecture 4 

Multivariate Normal Likelihoods 

Probability & Bayesian Inference 

  Multivariate Gaussian pdf 

76 

76 

p(x !i) =

1
!

(2")

2 #i

exp $

&
(
’

1
2

1
2

(x $ µ
i

)% #i

$1(x $ µ
i

)

)
+
*

µ
i

= E x,
-

.
/

,
#i = E (x $ µ
-

i

)(x $ µ
i

)%

.
/  

called the covariance matrix 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
77 

77 

Logarithmic Discriminant Function 

Probability & Bayesian Inference 

p(x !i) =

1
!

(2")

2 #i

exp $

&
(
’

1
2

1
2

(x $ µ
i

)% #i

$1(x $ µ
i

)

)
+
*

 ln(!)

        is monotonic.  Define: 
(
(
)P !i
(
gi(x) = ln p x | !i

)

) = ln p x | !i
(

) + lnP(!i)

(x ! µ
i

= !

1
2
where

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

Ci = !

!
2

ln2$ !

1
2

ln "i

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
   
Quadratic Classifiers 

78 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

  Thus the decision surface has a quadratic form. 
  For a 2D input space, the decision curves are 

0.5

quadrics (ellipses, parabolas, hyperbolas or, in 
degenerate cases, lines). 

0.3

0.4

0.2

0.1

0
10

5

0

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

−5

−5

−10

−10

5

0

10

J. Elder 

  
Example:  Isotropic Likelihoods 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

79 

79 

  Suppose that the two likelihoods are both isotropic, but with different means and 

gi(x) = !

variances.  Then 
1
2"i
gi(x) ! gj(x) = 0

2 (x1

2 + x2

  And 

2) +

1
2 (µi1x1 + µi2x2) !
"i

1
2"i

2 (µi1

2 + µi2

(
2 ) + ln P #i

(

)

) + Ci

 will be a quadratic equation in 2 variables. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
   
  
  
Equal Covariances 

80 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

  The quadratic term of the decision boundary is 

given by 

1
2

(
xT !j

"1

"1 " !i

) x

  Thus if the covariance matrices of the two 

likelihoods are identical, the decision boundary is 
linear. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
Linear Classifier 

81 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T " !1(x ! µ
i

) + lnP(#i) + Ci

 

In this case, we can drop the quadratic terms and express the 
discriminant function in linear form: 

T

gi(x) = wi
wi = ! "1µ

i

x + wio

wi0 = lnP(#i) "

T
! "1µ
µ
i
i

1
2

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Example 1: Isotropic, Identical Variance 

82 

Probability & Bayesian Inference 

T

x + wio

Decision  
Boundary 

gi(x) = wi
wi = ! "1µ

i

wi0 = lnP(#i) "

T
! "1µ
µ
i
i

1
2

! = "2I.  Then

T
w

(x # xo) = 0,  where

w = µ
i

# µ
j

,  and

xo =

1
2

(µ
i

+ µ
j

) # "2 ln

P($i)
P($j)

µ
i

# µ
j

2

µ
i

# µ
j

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Example 2: Equal Covariance 

83 

Probability & Bayesian Inference 

T

gi(x) = wi
wi = ! "1µ

i

x + wio

wi0 = lnP(#i) "

T
! "1µ
µ
i
i

1
2

T

gij(x) = w

(x ! x0) = 0

where 

w = ! "1(µ
i

" µ
j

),

(µ
i

+ µ
j

) ! ln

#
%
$

P("i)
P("j)

&
(
’

µ
i

! µ
j

µ
i

! µ
j

2

)!1

,

x0 =

1
2

and

x

T

)!1 * (x

1
2

) !1x)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
  
  
Minimum Distance Classifiers 

84 

Probability & Bayesian Inference 

  If the two likelihoods have identical covariance AND 
the two classes are equiprobable, the discrimination 
function simplifies: 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

gi(x) = !

1
2

(x ! µ
i

)T " !1(x ! µ
i

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Isotropic Case 

85 

Probability & Bayesian Inference 

  In the isotropic case, 

gi(x) = !

1
2

(x ! µ
i

)T " !1(x ! µ
i

) = !

1
2#2 x ! µ

i

2

  Thus the Bayesian classifier simply assigns the class 

that minimizes the Euclidean distance de between the 
observed feature vector and the class mean. 

de = x ! µ

i

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
 
General Case:  Mahalanobis Distance 

86 

Probability & Bayesian Inference 

  To deal with anisotropic distributions, we simply classify according 

to the Mahalanobis distance, deﬁned as!

dm = gi(x) = (x ! µ

(

)T " !1(x ! µ
i

)

)1/2

i

  Since the covariance matrix is symmetric, it can be represented as!

 ! = "#"T

    where!

the columns vi  of ! are the eigenvectors of "

    and where!

! is a diagonal matrix whose diagonal elements "i  
are the corresponding eigenvalues.

  Then we have   !
m = (x ! µ

d2

i

)T "T#!1"(x ! µ
i

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

!
!
!
  
  
  
  
General Case:  Mahalanobis Distance 

87 

Probability & Bayesian Inference 

d2

m = (x ! µ

i

)T "T#!1"(x ! µ
i

)

Let  !x = "Tx.  Then the coordinates of  !x  are the projections of x
onto the eigenvectors of #, and we have:

)2

(
!x1 " !µi1
#1

+ ! +

)2

(
!xl " !µil
#l

2

= dm

Thus the curves of constant 

Mahalanobis distance c have ellipsoidal form.

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
   
  
Example: 

88 

Probability & Bayesian Inference 

Given !1, !2 :   P(!1) = P(!2 ) and p(x !1) = N(µ
&
( ,   " =
’

&
( ,   µ
’

0
0

3
3

µ
1

#
%
$

#
%
$

=

=

2

&
(
’

1.1 0.3
0.3 1.9

#
%
$
&
( using Bayesian classification: 
’

,  "), p(x !2 ) = N(µ

2

,  "),

1

classify the vector x =

#
%
$

1.0
2.2

•  !-1 =

#
%
$

0.95 "0.15
"0.15 0.55

&
(
’

•  Compute Mahalanobis dm from µ1, µ2 :

d2

m,1 =

!
"

1.0, 2.2

$ % &1
#

!
’
"

1.0
2.2

#
( = 2.952, d2
$

m,2 =

!
"

&2.0, &0.8

$ % &1
#

!
’
"

&2.0
&0.8

#
( = 3.672
$

•   Classify  x ! "1.  Observe that dE,2 < dE,1

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
 
  
  
Bayesian Decision Theory:  Topics 

89 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 7. Parameter Estimation 

Topic 7.1 Maximum Likelihood Estimation 

Maximum Likelihood Parameter Estimation 

92 

92 

Probability & Bayesian Inference 

Suppose we believe input vectors x are distributed as
p(x) ! p(x;"),  where " is an unknown parameter.
Given independent training input vectors X = x1,x2, ...xN

{

}

we want to compute the maximum likelihood estimate "ML for ".
Since the input vectors are independent, we have

N
p(X;") ! p(x1,x2, ...xN;") = #
k=1

p(xk;")

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Maximum Likelihood Parameter Estimation 

93 

93 

Probability & Bayesian Inference 

N
p(X;!) = "
k=1

p(xk;!)

N
Let L(!) " ln p(X;!) = #
k=1

ln p(xk;!)

The general method is to take the derivative of L
with respect to !,  set it to 0 and solve for ! :

ˆ!ML :   

$L(!)
$(!)

=

N

%

k=1

$ln p(xk;!)
$(!)

= 0

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Properties of the Maximum Likelihood Estimator 

94 

94 

Probability & Bayesian Inference 

Let !0  be the true value of the unknown parameter vector.
Then
!ML is asymptotically unbiased:  lim
  N"#

E[!ML] = !0

!ML is asymptotically consistent:  lim
N"$

E ˆ!ML % !0

2

= 0

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Example: Univariate Normal 

95 

Probability & Bayesian Inference 

Likelihood func3on 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Example:  Univariate Normal 

96 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Example:  Univariate Normal 

97 

Probability & Bayesian Inference 

Thus !ML is biased (although asymptotically unbiased).

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
End of Lecture 5 

Example:  Multivariate Normal 

99 

Probability & Bayesian Inference 

  Given i.i.d. data                             , the log likeli-

hood function is given by 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Maximum Likelihood for the Gaussian  

100 

Probability & Bayesian Inference 

  Set the derivative of  the log likelihood function to zero, 

  and solve to obtain 

  One can also show that 

Recall:  If x and a  are vectors, then 

"
#$

!
!x

x ta(

) =

!
!x

a tx(

) = a

%
&’

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
