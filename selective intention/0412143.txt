5
0
0
2

b
e
F
5
1

2
v
3
4
1
2
1
4
0
/
h
p
-
t
n
a
u
q
:
v
i
X
r
a

Limits on Eﬃcient Computation in the Physical World

by

Scott Joel Aaronson

Bachelor of Science (Cornell University) 2000

A dissertation submitted in partial satisfaction of the
requirements for the degree of
Doctor of Philosophy

in

Computer Science

in the

GRADUATE DIVISION
of the
UNIVERSITY of CALIFORNIA, BERKELEY

Committee in charge:

Professor Umesh Vazirani, Chair
Professor Luca Trevisan
Professor K. Birgitta Whaley

Fall 2004

 
 
 
 
The dissertation of Scott Joel Aaronson is approved:

Chair

Date

Date

Date

University of California, Berkeley

Fall 2004

Limits on Eﬃcient Computation in the Physical World

Copyright 2004
by
Scott Joel Aaronson

Abstract

1

Limits on Eﬃcient Computation in the Physical World

by

Scott Joel Aaronson
Doctor of Philosophy in Computer Science

University of California, Berkeley

Professor Umesh Vazirani, Chair

More than a speculative technology, quantum computing seems to challenge our most basic
intuitions about how the physical world should behave.
In this thesis I show that, while
some intuitions from classical computer science must be jettisoned in the light of modern
physics, many others emerge nearly unscathed; and I use powerful tools from computational
complexity theory to help determine which are which.

In the ﬁrst part of the thesis, I attack the common belief that quantum computing
resembles classical exponential parallelism, by showing that quantum computers would face
serious limitations on a wider range of problems than was previously known.
In partic-
ular, any quantum algorithm that solves the collision problem—that of deciding whether
n1/5
a sequence of n integers is one-to-one or two-to-one—must query the sequence Ω
times. This resolves a question that was open for years; previously no lower bound better
(cid:1)
than constant was known. A corollary is that there is no “black-box” quantum algorithm
to break cryptographic hash functions or solve the Graph Isomorphism problem in poly-
I also show that relative to an oracle, quantum computers could not solve
nomial time.
NP-complete problems in polynomial time, even with the help of nonuniform “quantum
advice states”; and that any quantum algorithm needs Ω
queries to ﬁnd a local
minimum of a black-box function on the n-dimensional hypercube. Surprisingly, the latter
result also leads to new classical lower bounds for the local search problem. Finally, I give
new lower bounds on quantum one-way communication complexity, and on the quantum
query complexity of total Boolean functions and recursive Fourier sampling.

2n/4/n

(cid:1)

(cid:0)

(cid:0)

The second part of the thesis studies the relationship of the quantum computing
I ﬁrst examine the arguments of Leonid Levin, Stephen Wol-
model to physical reality.
fram, and others who believe quantum computing to be fundamentally impossible.
I ﬁnd
their arguments unconvincing without a “Sure/Shor separator”—a criterion that separates
the already-veriﬁed quantum states from those that appear in Shor’s factoring algorithm.
I argue that such a separator should be based on a complexity classiﬁcation of quantum
states, and go on to create such a classiﬁcation. Next I ask what happens to the quantum
computing model if we take into account that the speed of light is ﬁnite—and in particu-
lar, whether Grover’s algorithm still yields a quadratic speedup for searching a database.
Refuting a claim by Benioﬀ, I show that the surprising answer is yes. Finally, I analyze
hypothetical models of computation that go even beyond quantum computing. I show that

many such models would be as powerful as the complexity class PP, and use this fact to
give a simple, quantum computing based proof that PP is closed under intersection. On
the other hand, I also present one model—wherein we could sample the entire history of
a hidden variable—that appears to be more powerful than standard quantum computing,
but only slightly so.

2

Professor Umesh Vazirani
Dissertation Committee Chair

Contents

List of Figures

List of Tables

1 “Aren’t You Worried That Quantum Computing Won’t Pan Out?”

2 Overview

2.1 Limitations of Quantum Computers

. . . . . . . . . . . . . . . . . . . . . .
2.1.1 The Collision Problem . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.2 Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.3 Quantum Certiﬁcate Complexity . . . . . . . . . . . . . . . . . . . .
2.1.4 The Need to Uncompute . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.5 Limitations of Quantum Advice . . . . . . . . . . . . . . . . . . . . .
2.2 Models and Reality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Skepticism of Quantum Computing . . . . . . . . . . . . . . . . . . .
2.2.1
. . . . . . . . . . . . . . . .
2.2.2 Complexity Theory of Quantum States
2.2.3 Quantum Search of Spatial Regions
. . . . . . . . . . . . . . . . . .
2.2.4 Quantum Computing and Postselection . . . . . . . . . . . . . . . .
2.2.5 The Power of History . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Complexity Theory Cheat Sheet

3.1 The Complexity Zoo Junior . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Oracles

4 Quantum Computing Cheat Sheet
4.1 Quantum Computers: N Qubits
. . . . . . . . . . . . . . . . . . . . . . . .
4.2 Further Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

I Limitations of Quantum Computers

5 Introduction

5.1 The Quantum Black-Box Model . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Oracle Separations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iii

vii

viii

1

6
7
8
9
10
11
11
13
13
13
14
15
16

18
19
20
21

23
24
27

29

30
31
32

6 The Collision Problem

6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.1.1 Oracle Hardness Results . . . . . . . . . . . . . . . . . . . . . . . . .
Information Erasure . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.1.2
6.2 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
6.3 Reduction to Bivariate Polynomial
6.4 Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.5 Set Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.6 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Local Search

7.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Preliminaries
7.3 Relational Adversary Method . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4 Snakes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.5 Speciﬁc Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
7.5.1 Boolean Hypercube
7.5.2 Constant-Dimensional Grid Graph . . . . . . . . . . . . . . . . . . .

8 Quantum Certiﬁcate Complexity

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.1 Summary of Results
8.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.3 Characterization of Quantum Certiﬁcate Complexity . . . . . . . . . . . . .
8.4 Quantum Lower Bound for Total Functions . . . . . . . . . . . . . . . . . .
8.5 Asymptotic Gaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.5.1 Local Separations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
8.5.2
8.6 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Symmetric Partial Functions

9 The Need to Uncompute

9.1 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 Quantum Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10 Limitations of Quantum Advice

10.1 Preliminaries

10.2 Simulating Quantum Messages

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10.1.1 Quantum Advice . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10.1.2 The Almost As Good As New Lemma . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
10.3 A Direct Product Theorem for Quantum Search . . . . . . . . . . . . . . .
10.4 The Trace Distance Method . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10.5 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.2.1 Simulating Quantum Advice

10.4.1 Applications

iv

34
36
36
36
37
38
41
43
46

47
49
51
52
57
60
60
64

67
68
70
70
72
74
76
77
78

79
81
82
87

88
91
92
93
93
96
99
103
106
110

11 Summary of Part I

II Models and Reality

12 Skepticism of Quantum Computing

12.1 Bell Inequalities and Long-Range Threads . . . . . . . . . . . . . . . . . . .

13 Complexity Theory of Quantum States

13.1 Sure/Shor Separators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.2 Classifying Quantum States . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.3 Basic Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.4 Relations Among Quantum State Classes
. . . . . . . . . . . . . . . . . . .
13.5 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.5.1 Subgroup States
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.5.2 Shor States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.5.3 Tree Size and Persistence of Entanglement . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
13.6 Manifestly Orthogonal Tree Size
13.7 Computing With Tree States
. . . . . . . . . . . . . . . . . . . . . . . . . .
13.8 The Experimental Situation . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.9 Conclusion and Open Problems . . . . . . . . . . . . . . . . . . . . . . . . .

14 Quantum Search of Spatial Regions

14.1 Summary of Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.3 The Physics of Databases
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.4 The Model
14.4.1 Locality Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.5 General Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.6 Search on Grids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.6.1 Amplitude Ampliﬁcation . . . . . . . . . . . . . . . . . . . . . . . .
14.6.2 Dimension At Least 3 . . . . . . . . . . . . . . . . . . . . . . . . . .
14.6.3 Dimension 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.6.4 Multiple Marked Items . . . . . . . . . . . . . . . . . . . . . . . . . .
14.6.5 Unknown Number of Marked Items . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
14.7.1 Bits Scattered on a Graph . . . . . . . . . . . . . . . . . . . . . . . .
14.8 Application to Disjointness
. . . . . . . . . . . . . . . . . . . . . . . . . . .
14.9 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14.7 Search on Irregular Graphs

15 Quantum Computing and Postselection

15.1 The Class PostBQP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
15.2 Fantasy Quantum Mechanics
15.3 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

v

112

114

116
119

126
127
130
135
138
141
142
146
148
149
154
157
160

162
162
164
165
167
168
169
173
174
175
180
181
184
185
189
190
191

192
193
196
198

16 The Power of History

16.1 The Complexity of Sampling Histories . . . . . . . . . . . . . . . . . . . . .
16.2 Outline of Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.3 Hidden-Variable Theories
16.3.1 Comparison with Previous Work . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.3.2 Objections
16.4 Axioms for Hidden-Variable Theories . . . . . . . . . . . . . . . . . . . . . .
16.4.1 Comparing Theories . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.5 Impossibility Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.6 Speciﬁc Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.6.1 Flow Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.6.2 Schr¨odinger Theory . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.7 The Computational Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.7.1 Basic Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.8 The Juggle Subroutine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.9 Simulating SZK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16.10Search in N 1/3 Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
16.11Conclusions and Open Problems

17 Summary of Part II

Bibliography

vi

199
200
201
203
205
206
206
207
208
211
211
215
218
219
220
221
224
226

228

229

List of Figures

1.1 Conway’s Game of Life . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1 Known relations among 14 complexity classes . . . . . . . . . . . . . . . . .

4.1 Quantum states of one qubit

. . . . . . . . . . . . . . . . . . . . . . . . . .

7.1 A snake of vertices ﬂicks its tail . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 The coordinate loop in 3 dimensions . . . . . . . . . . . . . . . . . . . . . .

13.1 Sure/Shor separators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.2 Tree representing a quantum state . . . . . . . . . . . . . . . . . . . . . . .
13.3 Known relations among quantum state classes . . . . . . . . . . . . . . . . .

14.1 Quantum robot searching a 2D grid . . . . . . . . . . . . . . . . . . . . . .
14.2 The ‘starﬁsh’ graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.3 Disjointness in O (√n) communication . . . . . . . . . . . . . . . . . . . . .

vii

2

21

25

58
64

128
129
131

163
171
191

15.1 Simulating PP using postselection . . . . . . . . . . . . . . . . . . . . . . . .

195

16.1 Flow network corresponding to a unitary matrix . . . . . . . . . . . . . . .

211

viii

List of Tables

8.1 Query complexity and certiﬁcate complexity measures . . . . . . . . . . . .

68

10.1 Expressions for px,ijkl

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

109

12.1 Four objections to quantum computing . . . . . . . . . . . . . . . . . . . . .

116

14.1 Summary of bounds for spatial search . . . . . . . . . . . . . . . . . . . . .
14.2 Divide-and-conquer versus quantum walks . . . . . . . . . . . . . . . . . . .

163
165

16.1 Four hidden-variable theories and the axioms they satisfy . . . . . . . . . .

208

ix

Acknowledgements

My adviser, Umesh Vazirani, once said that he admires the quantum adiabatic algorithm
because, like a great squash player, it achieves its goal while moving as little as it can get
away with. Throughout my four years at Berkeley, I saw Umesh inculcate by example his
“adiabatic” philosophy of life: a philosophy about which papers are worth reading, which
deadlines worth meeting, and which research problems worth a ﬁght to the ﬁnish. Above all,
the concept of “beyond hope” does not exist in this philosophy, except possibly in regard
to computational problems. My debt to Umesh for his expert scientiﬁc guidance, wise
professional counsel, and generous support is obvious and beyond my ability to embellish.
My hope is that I graduate from Berkeley a more adiabatic person than when I came.

Admittedly, if the push to ﬁnish this thesis could be called adiabatic, then the
spectral gap was exponentially small. As I struggled to make the deadline, I relied on the
help of David Molnar, who generously agreed to ﬁle the thesis in Berkeley while I remained in
Princeton; and my committee—consisting of Umesh, Luca Trevisan, and Birgitta Whaley—
which met procrastination with ﬂexibility.

Silly as it sounds, a principal reason I came to Berkeley was to breathe the same air
that led Andris Ambainis to write his epochal paper “Quantum lower bounds by quantum
arguments.” Whether or not the air in 587 Soda did me any good, Part I of the thesis is
essentially a 150-page tribute to Andris—a colleague whose unique combination of genius
and humility ﬁlls everyone who knows him with awe.

The direction of my research owes a great deal as well to Ronald de Wolf, who
periodically emerges from his hermit cave to challenge non-rigorous statements, eat dubbel
zout, or lament American ignorance. While I can see eye-to-eye with Ronald about (say)
the D (f ) versus bs (f )2 problem, I still feel that Andrei Tarkovsky’s Solaris would beneﬁt
immensely from a car chase.

For better or worse, my conception of what a thesis should be was inﬂuenced by
Dave Bacon, quantum computing’s elder clown, who entitled the ﬁrst chapter of his own
451-page behemoth “Philosonomicon.” I’m also indebted to Chris Fuchs and his samizdat,
for the idea that a document about quantum mechanics more than 400 pages long can be
worth reading most of the way through.

I began working on the best-known result in this thesis, the quantum lower bound
for the collision problem, during an unforgettable summer at Caltech.
Leonard Schul-
man and Ashwin Nayak listened patiently to one farfetched idea after another, while John
Preskill’s weekly group meetings helped to ensure that the mysteries of quantum mechanics,
which inspired me to tackle the problem in the ﬁrst place, were never far from my mind.
Besides Leonard, Ashwin, and John, I’m grateful to Ann Harvey for putting up with the
growing mess in my oﬃce. For the record, I never once slept in the oﬃce; the bedsheet
was strictly for doing math on the ﬂoor.

I created the infamous Complexity Zoo web site during a summer at CWI in
Amsterdam, a visit enlivened by the presence of Harry Buhrman, Hein R¨ohrig, Volker
Nannen, Hartmut Klauck, and Troy Lee. That summer I also had memorable conversations
with David Deutsch and Stephen Wolfram. Chapters 7, 13, and 16 partly came into being
during a semester at the Hebrew University in Jerusalem, a city where “Aaron’s sons” were
I thank Avi Wigderson, Dorit
already obsessing about cubits three thousand years ago.

x

Aharonov, Michael Ben-Or, Amnon Ta-Shma, and Michael Mallin for making that semester
I also thank Avi for pointing me to the then-unpublished
a fruitful and enjoyable one.
results of Ran Raz on which Chapter 13 is based, and Ran for sharing those results.

A signiﬁcant chunk of the thesis was written or revised over two summers at the
Perimeter Institute for Theoretical Physics in Waterloo.
I thank Daniel Gottesman, Lee
Smolin, and Ray Laﬂamme for welcoming a physics doofus to their institute, someone who
thinks the string theory versus loop quantum gravity debate should be resolved by looping
over all possible strings. From Marie Ericsson, Rob Spekkens, and Anthony Valentini
I learned that theoretical physicists have a better social life than theoretical computer
scientists, while from Dan Christensen I learned that complexity and quantum gravity had
better wait before going steady.

Several ideas were hatched or incubated during the yearly QIP conferences; work-
shops in Toronto, Banﬀ, and Leiden; and visits to MIT, Los Alamos, and IBM Almaden.
I’m grateful to Howard Barnum, Andrew Childs, Elham Kasheﬁ, Barbara Terhal, John
Watrous, and many others for productive exchanges on those occasions.

Back in Berkeley, people who enriched my grad-school experience include Neha
Dave, Julia Kempe, Simone Severini, Lawrence Ip, Allison Coates, David Molnar, Kris Hil-
drum, Miriam Walker, and Shelly Rosenfeld. Alex Fabrikant and Boriska Toth are forgiven
for the cruel caricature that they attached to my dissertation talk announcement, provided
they don’t try anything like that ever again. The results on one-way communication in
Chapter 10 beneﬁted greatly from conversations with Oded Regev and Iordanis Kerenidis,
while Andrej Bogdanov kindly supplied the explicit erasure code for Chapter 13.
I wrote
Chapter 7 to answer a question of Christos Papadimitriou.

I did take some actual . . . courses at Berkeley, and I’m grateful to John Kubiatow-
icz, Stuart Russell, Guido Bacciagaluppi, Richard Karp, and Satish Rao for not failing me
in theirs.
Ironically, the course that most directly inﬂuenced this thesis was Tom Farber’s
magniﬁcent short ﬁction workshop. A story I wrote for that workshop dealt with the prob-
lem of transtemporal identity, which got me thinking about hidden-variable interpretations
of quantum mechanics, which led eventually to the collision lower bound. No one seems to
believe me, but it’s true.

The students who took my “Physics, Philosophy, Pizza” course remain one of my
greatest inspirations. Though they were mainly undergraduates with liberal arts back-
grounds, they took nothing I said about special relativity or G¨odel’s Theorem on faith.
If
I have any conﬁdence today in my teaching abilities; if I think it possible for students to
show up to class, and to participate eagerly, without the usual carrot-and-stick of grades
and exams; or if I ﬁnd certain questions, such as how a superposition over exponentially
many ‘could-have-beens’ can collapse to an ‘is,’ too vertiginous to be pondered only by
nerds like me, then those pizza-eating students are the reason.

Now comes the part devoted to the mist-enshrouded pre-Berkeley years. My
initiation into the wild world of quantum computing research took place over three summer
internships at Bell Labs: the ﬁrst with Eric Grosse, the second with Lov Grover, and the
third with Rob Pike.
I thank all three of them for encouraging me to pursue my interests,
even if the payoﬀ was remote and, in Eric’s case, not even related to why I was hired.
Needless to say, I take no responsibility for the subsequent crash of Lucent’s stock.

xi

As an undergraduate at Cornell, I was younger than my classmates, invisible to
many of the researchers I admired, and profoundly unsure of whether I belonged there or
had any future in science. What made the diﬀerence was the unwavering support of one
professor, Bart Selman. Busy as he was, Bart listened to my harebrained ideas about
genetic algorithms for SAT or quantum chess-playing, invited me to give talks, guided me
to the right graduate programs, and generally treated me like a future colleague. As
a result, his conviction that I could succeed at research gradually became my conviction
too. Outside of research, Christine Chung, Fion Luo, and my Telluride roommate Jason
Stockmann helped to warm the Ithaca winters, Lydia Fakundiny taught me what an essay
is, and Jerry Abrams provided a much-needed boost.

Turning the clock back further, my earliest research foray was a paper on hypertext
organization, written when I was ﬁfteen and spending the year at Clarkson University’s
unique Clarkson School program. Christopher Lynch generously agreed to advise the
project, and oﬀered invaluable help as I clumsily learned how to write a C program, prove
a problem NP-hard, and conduct a user experiment (one skill I’ve never needed again!).
I
was elated to be trading ideas with a wise and experienced researcher, only months after I’d
escaped from the prison-house of high school. Later, the same week the rejection letters
were arriving from colleges, I learned that my ﬁrst paper had been accepted to SIGIR,
the main information retrieval conference.
I was ﬁlled with boundless gratitude toward
the entire scientiﬁc community—for struggling, against the warp of human nature, to judge
ideas rather than the personal backgrounds of their authors. Eight years later, my gratitude
and amazement are undiminished.

Above all, I thank Alex Halderman for a friendship that’s spanned twelve years
and thousands of miles, remaining as strong today as it was amidst the Intellectualis minimi
of Newtown Junior High School; my brother David for believing in me, and for making me
prouder than he realizes by doing all the things I didn’t; and my parents for twenty-three
years of harping, kvelling, chicken noodle soup, and never doubting for a Planck time that
I’d live up to my potential—even when I couldn’t, and can’t, share their certainty.

1

Chapter 1

“Aren’t You Worried That
Quantum Computing Won’t Pan
Out?”

For a century now, physicists have been telling us strange things: about twins
who age at diﬀerent rates, particles that look diﬀerent when rotated 360◦, a force that is
transmitted by gravitons but is also the curvature of spacetime, a negative-energy electron
sea that pervades empty space, and strangest of all, “probability waves” that produce fringes
on a screen when you don’t look and don’t when you do. Yet ever since I learned to program,
I suspected that such things were all “implementation details” in the source code of Nature,
their study only marginally relevant to forming an accurate picture of reality. Physicists,
I thought, would eventually realize that the state of the universe can be represented by
a ﬁnite string of bits. These bits would be the “pixels” of space, creating the illusion of
continuity on a large scale much as a computer screen does. As time passed, the bits
would be updated according to simple rules. The speciﬁc form of these rules was of no
great consequence—since according to the Extended Church-Turing Thesis, any suﬃciently
complicated rules could simulate any other rules with reasonable eﬃciency.1 So apart from
practical considerations, why worry about Maxwell’s equations, or Lorentz invariance, or
even mass and energy, if the most fundamental aspects of our universe already occur in
Conway’s Game of Life (see Figure 1.1)?

Then I heard about Shor’s algorithm [219] for factoring integers in polynomial time
on a quantum computer. Then as now, many people saw quantum computing as at best a
speculative diversion from the “real work” of computer science. Why devote one’s research
career to a type of computer that might never see application within one’s lifetime, that
faces daunting practical obstacles such as decoherence, and whose most publicized success
to date has been the conﬁrmation that, with high probability, 15 = 3
5 [234]? Ironically,
I might have agreed with this view, had I not taken the Extended Church-Turing Thesis
so seriously as a claim about reality. For Shor’s algorithm forces us to accept that, under

×

1Here “extended” refers to the eﬃciency requirement, which was not mentioned in the original Church-
Turing Thesis. Also, I am simply using the standard terminology, sidestepping the issue of whether Church
and Turing themselves intended to make a claim about physical reality.

2

Figure 1.1: In Conway’s Game of Life, each cell of a 2D square grid becomes ‘dead’ or
‘alive’ based on how many of its eight neighbors were alive in the previous time step. A
In what ways is
simple rule applied iteratively leads to complex, unpredictable behavior.
our physical world similar to Conway’s, and in what ways is it diﬀerent?

widely-believed assumptions, that Thesis conﬂicts with the experimentally-tested rules of
quantum mechanics as we currently understand them. Either the Extended Church-Turing
Thesis is false, or quantum mechanics must be modiﬁed, or the factoring problem is solvable
in classical polynomial time. All three possibilities seem like wild, crackpot speculations—
but at least one of them is true!

The above conundrum is what underlies my interest in quantum computing, far
more than any possible application. Part of the reason is that I am neither greedy, nefarious,
nor number-theoretically curious enough ever to have hungered for the factors of a 600-digit
integer.
I do think that quantum computers would have benign uses, the most important
one being the simulation of quantum physics and chemistry.2 Also, as transistors approach
the atomic scale, ideas from quantum computing are likely to become pertinent even for
classical computer design. But none of this quickens my pulse.

For me, quantum computing matters because it combines two of the great myster-
ies bequeathed to us by the twentieth century: the nature of quantum mechanics, and the
It would be astonishing if such an elemental connection
ultimate limits of computation.
between these mysteries shed no new light on either of them. And indeed, there is already
a growing list of examples [9, 22, 151]—we will see several of them in this thesis—in which
ideas from quantum computing have led to new results about classical computation. This
should not be surprising: after all, many celebrated results in computer science involve
only deterministic computation, yet it is hard to imagine how anyone could have proved
them had computer scientists not long ago “taken randomness aboard.”3 Likewise, taking
quantum mechanics aboard could lead to a new, more general perspective from which to
revisit the central questions of computational complexity theory.

The other direction, though, is the one that intrigues me even more.

In my view,

2Followed closely by Recursive Fourier Sampling, parity in n/2 queries, and eﬃciently deciding whether

a graph is a scorpion.

3A few examples are primality testing in P [17], undirected connectivity in L [202], and inapproximability

of 3-SAT unless P = NP [224].

3

quantum computing has brought us slightly closer to the elusive Beast that devours Bohmi-
ans for breakfast, Copenhagenists for lunch, and a linear combination of many-worlders
and consistent historians for dinner—the Beast that tramples popularizers, brushes oﬀ
arXiv preprints like ﬂeas, and snorts at the word “decoherence”—the Beast so fearsome
that physicists since Bohr and Heisenberg have tried to argue it away, as if semantics could
banish its unitary jaws and complex-valued tusks. But no, the Beast is there whenever you
aren’t paying attention, following all possible paths in superposition. Look, and suddenly
the Beast is gone. But what does it even mean to look? If you’re governed by the same
physical laws as everything else, then why don’t you evolve in superposition too, perhaps
until someone else looks at you and thereby ‘collapses’ you? But then who collapses whom
ﬁrst? Or if you never collapse, then what determines what you-you, rather than the su-
perposition of you’s, experience? Such is the riddle of the Beast,4 and it has ﬁlled many
with terror and awe.

The contribution of quantum computing, I think, has been to show that the real
nature of the Beast lies in its exponentiality. It is not just two, three, or a thousand states
held in ghostly superposition that quantum mechanics is talking about, but an astronomical
multitude, and these states could in principle reveal their presence to us by factoring a ﬁve-
thousand-digit number. Much more than even Schr¨odinger’s cat or the Bell inequalities,
this particular discovery ups the ante—forcing us either to swallow the full quantum brew,
or to stop saying that we believe in it. Of course, this is part of the reason why Richard
Feynman [108] and David Deutsch [90] introduced quantum computing in the ﬁrst place,
and why Deutsch, in his defense of the many-worlds interpretation, issues a famous challenge
to skeptics [92, p. 217]: if parallel universes are not physically real, then explain how Shor’s
algorithm works.

Unlike Deutsch, here I will not use quantum computing to defend the many-worlds
interpretation, or any of its competitors for that matter. Roughly speaking, I agree with
every interpretation of quantum mechanics to the extent that it acknowledges the Beast’s
I would adopt
existence, and disagree to the extent that it claims to have caged the Beast.
the same attitude in computer science, if instead of freely admitting (for example) that
P versus NP is an open problem, researchers had split into “equalist,” “unequalist,” and
“undecidabilist” schools of interpretation, with others arguing that the whole problem is
meaningless and should therefore be abandoned.

Instead, in this thesis I will show how adopting a computer science perspective
can lead us to ask better questions—nontrivial but answerable questions, which put old
Let me give an
mysteries in a new light even when they fall short of solving them.
example. One of the most contentious questions about quantum mechanics is whether the
individual components of a wavefunction should be thought of as “really there” or as “mere
potentialities.” When we don our computer scientist goggles, this question morphs into a
diﬀerent one: what resources are needed to make a particular component of the wavefunction
manifest? Arguably the two questions are related, since something “real” ought to take less
work to manifest than something “potential.” For example, this thesis gradually became

4Philosophers call the riddle of the Beast the “measurement problem,” which sounds less like something
that should cause insomnia and delirious raving in all who have understood it. Basically, the problem is to
reconcile a picture of the world in which “everything happens simultaneously” with the fact that you (or at
least I!) have a sequence of deﬁnite experiences.

more real as less of it remained to be written.

4

Concretely, suppose our wavefunction has 2n components, all with equal ampli-
tude. Suppose also that we have a procedure to recognize a particular component x (i.e.,
a function f such that f (x) = 1 and f (y) = 0 for all y
= x). Then how often must we
apply this procedure before we make x manifest; that is, observable with probability close
2n/2 applications are
to 1? Bennett, Bernstein, Brassard, and Vazirani [51] showed that
necessary, even if f can be applied to all 2n components in superposition. Later Grover
So if we imagine a spectrum
[139] showed that
2n applications)
with “really there” (1 application) on one end, and “mere potentiality” (
on the other, then we have landed somewhere in between: closer to the “real” end on an
absolute scale, but closer to the “potential” end on the polynomial versus exponential scale
that is more natural for computer science.

2n/2 applications are also suﬃcient.

∼

∼

∼

Of course, we should be wary of drawing grand conclusions from a single data
point. So in this thesis, I will imagine a hypothetical resident of Conway’s Game of Life,
who arrives in our physical universe on a computational complexity safari—wanting to
know exactly which intuitions to keep and which to discard regarding the limits of eﬃcient
computation. Many popular science writers would tell our visitor to throw all classical
intuitions out the window, while quantum computing skeptics would urge retaining them
all. These positions are actually two sides of the same coin, since the belief that a quantum
I will show,
computer would necessitate the ﬁrst is what generally leads to the second.
however, that neither position is justiﬁed. Based on what we know today, there really is a
Beast, but it usually conceals its exponential underbelly.

I’ll provide only one example from the thesis here; the rest are summarized in
Suppose we are given a procedure that computes a two-to-one function f ,
Chapter 2.
and want to ﬁnd distinct inputs x and y such that f (x) = f (y).
In this case, by simply
preparing a uniform superposition over all inputs to f , applying the procedure, and then
) /√2, for some x and y
measuring its result, we can produce a state of the form (
i
|
such that f (x) = f (y). The only problem is that if we measure this state, then we see
either x or y, but not both. The task, in other words, is no longer to ﬁnd a needle in
a haystack, but just to ﬁnd two needles in an otherwise empty barn! Nevertheless, the
collision lower bound in Chapter 6 will show that, if there are 2n inputs to f , then any
2n/5 times.
quantum algorithm for this problem must apply the procedure for f at least
Omitting technical details, this lower bound can be interpreted in at least seven ways:

x
i

y
|

+

∼

(1) Quantum computers need exponential time even to compute certain global properties
of a function, not just local properties such as whether there is an x with f (x) = 1.

(2) Simon’s algorithm [220], and the period-ﬁnding core of Shor’s algorithm [219], cannot

be generalized to functions with no periodicity or other special structure.

(3) Any “brute-force” quantum algorithm needs exponential time, not just for NP-complete
problems, but for many structured problems such as Graph Isomorphism, approxi-
mating the shortest vector in a lattice, and ﬁnding collisions in cryptographic hash
functions.

6
(4) It is unlikely that all problems having “statistical zero-knowledge proofs” can be

eﬃciently solved on a quantum computer.

5

+

(5) Within the setting of a collision algorithm, the components

in the state
) /√2 should be thought of as more “potentially” than “actually” there, it
(
x
i
i
|
being impossible to extract information about both of them in a reasonable amount
of time.

x
i

and

y
|

y

i

|

|

(6) The ability to map

x
i
|
more powerful than the ability to map

to

|

, “uncomputing” x in the process, can be exponentially
f (x)
i

|
(7) In hidden-variable interpretations of quantum mechanics, the ability to sample the en-
tire history of a hidden variable would yield even more power than standard quantum
computing.

i |

|

f (x)
i

.

to

x

x
i

Interpretations (5), (6), and (7) are examples of what I mean by putting old
mysteries in a new light. We are not brought face-to-face with the Beast, but at least we
have fresh footprints and droppings.

Well then. Am I worried that quantum computing won’t pan out? My usual
answer is that I’d be thrilled to know it will never pan out, since this would entail the
discovery of a lifetime, that quantum mechanics is false. But this is not what the questioner
has in mind. What if quantum mechanics holds up, but building a useful quantum computer
turns out to be so diﬃcult and expensive that the world ends before anyone succeeds? The
questioner is usually a classical theoretical computer scientist, someone who is not known to
worry excessively that the world will end before log log n exceeds 10. Still, it would be nice
to see nontrivial quantum computers in my lifetime, and while I’m cautiously optimistic,
I’ll admit to being slightly worried that I won’t. But when faced with the evidence that
one was born into a universe profoundly unlike Conway’s—indeed, that one is living one’s
life on the back of a mysterious, exponential Beast comprising everything that ever could
have happened—what is one to do? “Move right along. . . nothing to see here. . . ”

6

Chapter 2

Overview

“Let a computer smear—with the right kind of quantum randomness—and
you create, in eﬀect, a ‘parallel’ machine with an astronomical number of pro-
cessors . . . All you have to do is be sure that when you collapse the system,
you choose the version that happened to ﬁnd the needle in the mathematical
haystack.”

—From Quarantine [103], a 1992 science-ﬁction novel by Greg Egan

Many of the deepest discoveries of science are limitations:

for example, no su-
perluminal signalling, no perpetual-motion machines, and no complete axiomatization for
arithmetic. This thesis is broadly concerned with limitations on what can eﬃciently be
computed in the physical world. The word “quantum” is absent from the title, in order
to emphasize that the focus on quantum computing is not an arbitrary choice, but rather
an inevitable result of taking our current physical theories seriously. The technical con-
tributions of the thesis are divided into two parts, according to whether they accept the
quantum computing model as given and study its fundamental limitations; or question,
defend, or go beyond that model in some way. Before launching into a detailed overview
of the contributions, let me make some preliminary remarks.

Since the early twentieth century, two communities—physicists1 and computer
scientists—have been asking some of the deepest questions ever asked in almost total in-
tellectual isolation from each other. The great joy of quantum computing research is that
it brings these communities together. The trouble was initially that, although each com-
munity would nod politely during the other’s talks, eventually it would come out that the
physicists thought NP stood for “Non Polynomial,” and the computer scientists had no
idea what a Hamiltonian was. Thankfully, the situation has improved a lot—but my hope
is that it improves further still, to the point where computer scientists have internalized
the problems faced by physics and vice versa. For this reason, I have worked hard to
make the thesis as accessible as possible to both communities. Thus, Chapter 3 provides
a “complexity theory cheat sheet” that deﬁnes NP, P/poly, AM, and other computational
complexity classes that appear in the thesis; and that explains oracles and other important

1As in Saul Steinberg’s famous New Yorker world map, in which 9th Avenue and the Hudson River take
up more space than Japan and China, from my perspective chemists, engineers, and even mathematicians
who know what a gauge ﬁeld is are all “physicists.”

7

concepts. Then Chapter 4 presents the quantum model of computation with no reference
to the underlying physics, before moving on to fancier notions such as density matrices,
trace distance, and separability. Neither chapter is a rigorous introduction to its subject;
for that there are ﬁne textbooks—such as Papadimitriou’s Computational Complexity [188]
and Nielsen and Chuang’s Quantum Computation and Quantum Information [182]—as well
as course lecture notes available on the web. Depending on your background, you might
want to skip to Chapters 3 or 4 before continuing any further, or you might want to skip
past these chapters entirely.

Even the most irredeemably classical reader should take heart: of the 103 proofs
in the thesis, 66 do not contain a single ket symbol.2 Many of the proofs can be understood
by simply accepting certain facts about quantum computing on faith, such as Ambainis’s3
adversary theorem [27] or Beals et al.’s polynomial lemma [45]. On the other hand, one does
run the risk that after one understands the proofs, ket symbols will seem less frightening
than before.

The results in the thesis have all previously appeared in published papers or
preprints [1, 2, 4, 5, 7, 8, 9, 10, 11, 13], with the exception of the quantum computing
based proof that PP is closed under intersection in Chapter 15.
I thank Andris Ambainis
for allowing me to include our joint results from [13] on quantum search of spatial regions.
Results of mine that do not appear in the thesis include those on Boolean function query
properties [3], stabilizer circuits [14] (joint work with Daniel Gottesman), and agreement
complexity [6].

In writing the thesis, one of the toughest choices I faced was whether to refer to
myself as ‘I’ or ‘we.’ Sometimes a personal voice seemed more appropriate, and sometimes
the Voice of Scientiﬁc Truth, but I wanted to be consistent. Readers can decide whether I
chose humbly or arrogantly.

2.1 Limitations of Quantum Computers

Part I studies the fundamental limitations of quantum computers within the usual model
for them. With the exception of Chapter 10 on quantum advice, the contributions of Part
I all deal with black-box or query complexity, meaning that one counts only the number of
queries to an “oracle,” not the number of computational steps. Of course, the queries can
be made in quantum superposition. In Chapter 5, I explain the quantum black-box model,
then oﬀer a detailed justiﬁcation for its relevance to understanding the limits of quantum
computers. Some computer scientists say that black-box results should not be taken too
seriously; but I argue that, within quantum computing, they are not taken seriously enough.
What follows is a (relatively) nontechnical overview of Chapters 6 to 10, which
contain the results of Part I. Afterwards, Chapter 11 summarizes the conceptual lessons
that I believe can be drawn from those results.

2To be honest, a few of those do contain density matrices—or the theorem contains ket symbols, but not

the proof.

3Style manuals disagree about whether Ambainis’ or Ambainis’s is preferable, but one referee asked me to
follow the latter rule with the following deadpan remark: “Exceptions to the rule generally involve religiously
signiﬁcant individuals, e.g., ‘Jesus’ lower-bound method.’ ”

2.1.1 The Collision Problem

8

}

{

to

1, . . . , n

1, . . . , n
{

Chapter 6 presents my lower bound on the quantum query complexity of the collision
problem. Given a function X from
(where n is even), the collision
}
problem is to decide whether X is one-to-one or two-to-one, promised that one of these is
the case. Here the only way to learn about X is to call a procedure that computes X (i)
given i. Clearly, any deterministic classical algorithm needs to call the procedure n/2 + 1
times to solve the problem. On the other hand, a randomized algorithm can exploit the
“birthday paradox”: only 23 people have to enter a room before there’s a 50% chance that
two of them share the same birthday, since what matters is the number of pairs of people.
Similarly, if X is two-to-one, and an algorithm queries X at √n uniform random locations,
then with constant probability it will ﬁnd two locations i
= j such that X (i) = X (j),
thereby establishing that X is two-to-one. This bound is easily seen to be tight, meaning
that the bounded-error randomized query complexity of the collision problem is Θ (√n).

What about the quantum complexity? In 1997, Brassard, Høyer, and Tapp [68]
n1/3
gave a quantum algorithm that uses only O
queries. The algorithm is simple to
describe: in the ﬁrst phase, query X classically at n1/3 randomly chosen locations.
In the
(cid:0)
second phase, choose n2/3 random locations, and run Grover’s algorithm on those locations,
considering each location i as “marked” if X (i) = X (j) for some j that was queried in the
ﬁrst phase. Notice that both phases use order n1/3 = √n2/3 queries, and that the total
number of comparisons is n2/3n1/3 = n. So, like its randomized counterpart, the quantum
algorithm ﬁnds a collision with constant probability if X is two-to-one.

(cid:1)

What I show in Chapter 6 is that any quantum algorithm for the collision problem
n1/5
queries. Previously, no lower bound better than the trivial Ω (1) was known.
for the following set comparison problem: given oracle
,

1, . . . , 2n

1, . . . , 2n

and Y :

n1/7

needs Ω
I also show a lower bound of Ω
access to injective functions X :
(cid:0)
decide whether

(cid:1)

(cid:0)

1, . . . , n
(cid:1)

{

} → {

1, . . . , n
{

} → {

}

}

X (1) , . . . , X (n) , Y (1) , . . . , Y (n)
}

{

has at least 1.1n elements or exactly n elements, promised that one of these is the case. The
set comparison problem is similar to the collision problem, except that it lacks permutation
symmetry, making it harder to prove a lower bound. My results for these problems have
been improved, simpliﬁed, and generalized by Shi [218], Kutin [161], Ambainis [27], and
Midrijanis [176].

for ex-
The implications of these results were already discussed in Chapter 1:
ample, they demonstrate that a “brute-force” approach will never yield eﬃcient quantum
algorithms for the Graph Isomorphism, Approximate Shortest Vector, or Nonabelian Hid-
den Subgroup problems; suggest that there could be cryptographic hash functions secure
against quantum attack; and imply that there exists an oracle relative to which SZK
BQP,
where SZK is the class of problems having statistical zero-knowledge proof protocols, and
BQP is quantum polynomial time.

6⊂

Both the original lower bounds and the subsequent improvements are based on
the polynomial method, which was introduced by Nisan and Szegedy [184], and ﬁrst used to
prove quantum lower bounds by Beals, Buhrman, Cleve, Mosca, and de Wolf [45].
In that
method, given a quantum algorithm that makes T queries to an oracle X, we ﬁrst represent

6
9

the algorithm’s acceptance probability by a multilinear polynomial p (X) of degree at most
2T . We then use results from a well-developed area of mathematics called approximation
theory to show a lower bound on the degree of p. This in turn implies a lower bound on T .
In order to apply the polynomial method to the collision problem, ﬁrst I extend the
collision problem’s domain from one-to-one and two-to-one functions to g-to-one functions
for larger values of g. Next I replace the multivariate polynomial p (X) by a related
univariate polynomial q (g) whose degree is easier to lower-bound. The latter step is the
real “magic” of the proof; I still have no good intuitive explanation for why it works.

The polynomial method is one of two principal methods that we have for proving
lower bounds on quantum query complexity. The other is Ambainis’s quantum adversary
method [27], which can be seen as a far-reaching generalization of the “hybrid argument”
that Bennett, Bernstein, Brassard, and Vazirani [51] introduced in 1994 to show that a
quantum computer needs Ω (√n) queries to search an unordered database of size n for a
marked item.
In the adversary method, we consider a bipartite quantum state, in which
one part consists of a superposition over possible inputs, and the other part consists of
a quantum algorithm’s work space. We then upper-bound how much the entanglement
between the two parts can increase as the result of a single query. This in turn implies a
lower bound on the number of queries, since the two parts must be highly entangled by the
end. The adversary method is more intrinsically “quantum” than the polynomial method;
and as Ambainis [27] showed, it is also applicable to a wider range of problems, including
those (such as game-tree search) that lack permutation symmetry. Ambainis even gave
problems for which the adversary method provably yields a better lower bound than the
polynomial method [28].
It is ironic, then, that Ambainis’s original goal in developing the
adversary method was to prove a lower bound for the collision problem; and in this one
instance, the polynomial method succeeded while the adversary method failed.

2.1.2 Local Search

→

Z, ﬁnd a local minimum of f —that is, a vertex v such that f (v)

In Chapters 7, 8, and 9, however, the adversary method gets its revenge. Chapter 7 deals
with the local search problem: given an undirected graph G = (V, E) and a black-box
function f : V
f (w)
for all neighbors w of v. The graph G is known in advance, so the complexity measure is just
the number of queries to f . This problem is central for understanding the performance of
the quantum adiabatic algorithm, as well as classical algorithms such as simulated annealing.
n, then previously Llewellyn, Tovey, and Trick [169] had
If G is the Boolean hypercube
shown that any deterministic algorithm needs Ω (2n/√n) queries to ﬁnd a local minimum;
and Aldous [24] had shown that any randomized algorithm needs 2n/2
o(n) queries. What
I show is that any quantum algorithm needs Ω
queries. This is the ﬁrst nontrivial
quantum lower bound for any local search problem; and it implies that the complexity class
PLS (or “Polynomial Local Search”), deﬁned by Johnson, Papadimitriou, and Yannakakis
[149], is not in quantum polynomial time relative to an oracle.

0, 1
}
{

2n/4/n

≤

(cid:0)

(cid:1)

−

What will be more surprising to classical computer scientists is that my proof
technique, based on the quantum adversary method, also yields new classical lower bounds
for local search. In particular, I prove a classical analogue of Ambainis’s quantum adversary
theorem, and show that it implies randomized lower bounds up to quadratically better

10

n

{

→

0, 1
}

2n/2/n2

Z. Not only does this improve on Aldous’s 2n/2

I then apply my theorem to show that
than the corresponding quantum lower bounds.
queries to ﬁnd a local minimum of a function
any randomized algorithm needs Ω
o(n) lower bound, bringing us
f :
(cid:1)
2n/2√n
closer to the known upper bound of O
; but it does so in a simpler way that does
not depend on random walk analysis.
In addition, I show the ﬁrst randomized or quantum
(cid:0)
lower bounds for ﬁnding a local minimum on a cube of constant dimension 3 or greater.
Along with recent work by Bar-Yossef, Jayram, and Kerenidis [43] and by Aharonov and
Regev [22], these results provide one of the earliest examples of how quantum ideas can help
to resolve classical open problems. As I will discuss in Chapter 7, my results on local search
have subsequently been improved by Santha and Szegedy [211] and by Ambainis [25].

(cid:1)

(cid:0)

−

2.1.3 Quantum Certiﬁcate Complexity

n

→ {

if f :

0, 1
}

0, 1
}
{

is a total Boolean function, then D (f ) = O

Chapters 8 and 9 continue to explore the power of Ambainis’s lower bound method and the
limitations of quantum computers. Chapter 8 is inspired by the following theorem of Beals
,
et al. [45]:
where D (f ) is the deterministic classical query complexity of f , and Q2 (f ) is the bounded-
(cid:17)
error quantum query complexity.4
This theorem is noteworthy for two reasons: ﬁrst,
because it gives a case where quantum computers provide only a polynomial speedup, in
contrast to the exponential speedup of Shor’s algorithm; and second, because the exponent
of 6 seems so arbitrary. The largest separation we know of is quadratic, and is achieved
by the OR function on n bits: D (OR) = n, but Q2 (OR) = O (√n) because of Grover’s
It is a longstanding open question whether this separation is optimal.
search algorithm.
In particular I
In Chapter 8, I make the best progress so far toward showing that it is.
prove that

Q2 (f )6

(cid:16)

R2 (f ) = O

Q2 (f )2 Q0 (f ) log n
n

(cid:17)

(cid:16)
0, 1
}
{

0, 1
}

. Here R2 (f ) is the bounded-error
for all total Boolean functions f :
→ {
randomized query complexity of f , and Q0 (f ) is the zero-error quantum query complexity.
To prove this result, I introduce two new query complexity measures of independent interest:
the randomized certiﬁcate complexity RC (f ) and the quantum certiﬁcate complexity QC (f ).
Using Ambainis’s adversary method together with the minimax theorem, I relate these
. Then, using the
measures exactly to one another, showing that RC (f ) = Θ
polynomial method, I show that R2 (f ) = O (RC (f ) Q0 (f ) log n) for all total Boolean f ,
Q2 (f ). Chapter 8 contains several other
which implies the above result since QC (f )
≤
results of interest to researchers studying query complexity, such as a superquadratic gap
between QC (f ) and the “ordinary” certiﬁcate complexity C (f ). But the main message
is the unexpected versatility of our quantum lower bound methods: we see the ﬁrst use
of the adversary method to prove something about all total functions, not just a speciﬁc
function; the ﬁrst use of both the adversary and the polynomial methods at diﬀerent points
in a proof; and the ﬁrst combination of the adversary method with a linear programming
duality argument.

QC (f )2
(cid:16)

(cid:17)

4The subscript ‘2’ means that the error is two-sided.

11

2.1.4 The Need to Uncompute

Next, Chapter 9 illustrates how “the need to uncompute” imposes a fundamental limit on
eﬃcient quantum computation. Like a classical algorithm, a quantum algorithm can solve
a problem recursively by calling itself as a subroutine. When this is done, though, the
quantum algorithm typically needs to call itself twice for each subproblem to be solved.
The second call’s purpose is to “uncompute” garbage left over by the ﬁrst call, and thereby
enable interference between diﬀerent branches of the computation.
In a seminal paper,
Bennett [52] argued5 that uncomputation increases an algorithm’s running time by only a
factor of 2. Yet in the recursive setting, the increase is by a factor of 2d, where d is the
Is there any way to avoid this exponential blowup?
depth of recursion.

To make the question more concrete, Chapter 9 focuses on the recursive Fourier
sampling problem of Bernstein and Vazirani [55]. This is a problem that involves d levels
of recursion, and that takes a Boolean function g as a parameter. What Bernstein and
Vazirani showed is that for some choices of g, any classical randomized algorithm needs
nΩ(d) queries to solve the problem. By contrast, 2d queries always suﬃce for a quantum
algorithm. The question I ask is whether a quantum algorithm could get by with fewer
than 2Ω(d) queries, even while the classical complexity remains large.
I show that the
answer is no: for every g, either Ambainis’s adversary method yields a 2Ω(d) lower bound
on the quantum query complexity, or else the classical and quantum query complexities are
both 1. The lower bound proof introduces a new parameter of Boolean functions called
the “nonparity coeﬃcient,” which might be of independent interest.

2.1.5 Limitations of Quantum Advice

Chapter 10 broadens the scope of Part I, to include the limitations of quantum computers
equipped with “quantum advice states.” Ordinarily, we assume that a quantum computer
starts out in the standard “all-0” state,
. But it is perfectly sensible to drop that
· · ·
assumption, and consider the eﬀects of other initial states. Most of the work doing so has
concentrated on whether universal quantum computing is still possible with highly mixed
initial states (see [34, 214] for example). But an equally interesting question is whether
there are states that could take exponential time to prepare, but that would carry us far
beyond the complexity-theoretic conﬁnes of BQP were they given to us by a wizard. For
even if quantum mechanics is universally valid, we do not really know whether such states
exist in Nature!

0
i

0
|

Let BQP/qpoly be the class of problems solvable in quantum polynomial time, with
the help of a polynomial-size “quantum advice state”
that depends only on the input
length n but that can otherwise be arbitrary. Then the question is whether BQP/poly =
BQP/qpoly, where BQP/poly is the class of the problems solvable in quantum polynomial
time using a polynomial-size classical advice string.6 As usual, we could try to prove an
oracle separation. But why can’t we show that quantum advice is more powerful than

ψni
|

5Bennett’s paper dealt with classical reversible computation, but this comment applies equally well to

quantum computation.

6For clearly BQP/poly and BQP/qpoly both contain uncomputable problems not in BQP, such as whether

the nth Turing machine halts.

classical advice, with no oracle? Also, could quantum advice be used (for example) to
solve NP-complete problems in polynomial time?

12

The results in Chapter 10 place strong limitations on the power of quantum advice.
First, I show that BQP/qpoly is contained in a classical complexity class called PP/poly.
This means (roughly) that quantum advice can always be replaced by classical advice,
It also means that we
provided we’re willing to use exponentially more computation time.
could not prove BQP/poly
= BQP/qpoly without showing that PP does not have polynomial-
size circuits, which is believed to be an extraordinarily hard problem. To prove this result,
is sent to the BQP/qpoly machine by a benevolent
I imagine that the advice state
“advisor,” through a one-way quantum communication channel. I then give a novel protocol
for simulating that quantum channel using a classical channel.
Besides showing that
PP/poly, the simulation protocol also implies that for all Boolean functions
BQP/qpoly
n
2 (f ) log Q1
,
0, 1
0, 1
f :
2 (f )
}
}
{
where D1 (f ) is the deterministic one-way communication complexity of f , and Q1
2 (f ) is
(cid:1)
the bounded-error quantum one-way communication complexity. This can be considered
a generalization of the “dense quantum coding” lower bound due to Ambainis, Nayak,
Ta-Shma, and Vazirani [32].

(partial or total), we have D1 (f ) = O

⊆
× {

0, 1
}

ψni
|

m Q1

→ {

m

(cid:0)

6⊂

The second result in Chapter 10 is that there exists an oracle relative to which
BQP/qpoly. This extends the result of Bennett et al. [51] that there exists an
NP
BQP, to handle quantum advice.
oracle relative to which NP
Intuitively, even though
the quantum state
could in some sense encode the solutions to exponentially many
NP search problems, only a miniscule fraction of that information could be extracted by
measuring the advice, at least in the black-box setting that we understand today.

ψni
|

6⊂

The proof of the oracle separation relies on another result of independent interest:
a direct product theorem for quantum search. This theorem says that given an unordered
database with n items, k of which are marked, any quantum algorithm that makes o (√n)
queries7 has probability at most 2−
In other
words, there are no “magical” correlations by which success in ﬁnding one marked item
leads to success in ﬁnding the others. This might seem intuitively obvious, but it does not
follow from the √n lower bound for Grover search, or any other previous quantum lower
bound for that matter. Previously, Klauck [155] had given an incorrect proof of a direct
product theorem, based on Bennett et al.’s hybrid method.
I give the ﬁrst correct proof by
using the polynomial method, together with an inequality dealing with higher derivatives
of polynomials due to V. A. Markov, the younger brother of A. A. Markov.

Ω(k) of ﬁnding all k of the marked items.

The third result in Chapter 10 is a new trace distance method for proving lower
bounds on quantum one-way communication complexity. Using this method, I obtain
optimal quantum lower bounds for two problems of Ambainis, for which no nontrivial lower
bounds were previously known even for classical randomized protocols.

7Subsequently Klauck, ˇSpalek, and de Wolf [156] improved this to o

√nk

queries, which is tight.

(cid:16)

(cid:17)

6
13

2.2 Models and Reality

This thesis is concerned with the limits of eﬃcient computation in Nature.
It is not
obvious that these coincide with the limits of the quantum computing model. Thus, Part
II studies the relationship of the quantum computing model to physical reality. Of course,
I therefore
this is too grand a topic for any thesis, even a thesis as long as this one.
focus on three questions that particularly interest me. First, how should we understand
the arguments of “extreme” skeptics, that quantum computing is impossible not only in
practice but also in principle? Second, what are the implications for quantum computing
if we recognize that the speed of light is ﬁnite, and that according to widely-accepted
principles, a bounded region of space can store only a ﬁnite amount of information? And
third, are there reasonable changes to the quantum computing model that make it even
more powerful, and if so, how much more powerful do they make it? Chapters 12 to 16
address these questions from various angles; then Chapter 17 summarizes.

2.2.1 Skepticism of Quantum Computing

Chapter 12 examines the arguments of skeptics who think that large-scale quantum comput-
ing is impossible for a fundamental physical reason. I ﬁrst brieﬂy consider the arguments of
Leonid Levin and other computer scientists, that quantum computing is analogous to “ex-
travagant” models of computation such as unit-cost arithmetic, and should be rejected on
essentially the same grounds. My response emphasizes the need to grapple with the actual
evidence for quantum mechanics, and to propose an alternative picture of the world that is
compatible with that evidence but in which quantum computing is impossible. The bulk
of the chapter, though, deals with Stephen Wolfram’s A New Kind of Science [246], and
in particular with one of that book’s most surprising claims: that a deterministic cellular-
automaton picture of the world is compatible with the so-called Bell inequality violations
demonstrating the eﬀects of quantum entanglement. To achieve compatibility, Wolfram
posits “long-range threads” between spacelike-separated points.
I explain in detail why
this thread proposal violates Wolfram’s own desiderata of relativistic and causal invariance.
Nothing in Chapter 12 is very original technically, but it seems worthwhile to spell out what
a scientiﬁc argument against quantum computing would have to accomplish, and why the
existing arguments fail.

2.2.2 Complexity Theory of Quantum States

Chapter 13 continues the train of thought begun in Chapter 12, except that now the focus is
more technical. I search for a natural Sure/Shor separator : a set of quantum states that can
account for all experiments performed to date, but that does not contain the states arising
In my view, quantum computing skeptics would strengthen
in Shor’s factoring algorithm.
their case by proposing speciﬁc examples of Sure/Shor separators, since they could then
oﬀer testable hypotheses about where the assumptions of the quantum computing model
break down (if not how they break down). So why am I doing the skeptics’ work for them?
Several people have wrongly inferred from this that I too am a skeptic! My goal, rather,
is to illustrate what a scientiﬁc debate about the possibility of quantum computing might

look like.

14

|

0
i

1
i
|

ψni
|

Most of Chapter 13 deals with a candidate Sure/Shor separator that I call tree
can be represented by a tree, in which each leaf is
states. Any n-qubit pure state
, and each non-leaf vertex is labeled by either a linear combination or
labeled by
or
a tensor product of its subtrees. Then the tree size of
is just the minimum number
of vertices in such a tree, and a “tree state” is an inﬁnite family of states whose tree size is
bounded by a polynomial in n. The idea is to keep a central axiom of quantum mechanics—
that if
—but to limit oneself
ϕ
ψ
i
|
to polynomially many applications of the axiom.

are possible states, so are

ψni
|

and α

ϕ
i

ϕ
i

i ⊗ |

and

ψ
|

+ β

ψ

i

i

|

|

|

The main results are superpolynomial lower bounds on tree size for explicit families
of quantum states. Using a recent lower bound on multilinear formula size due to Raz
[195, 196], I show that many states arising in quantum error correction (for example, states
based on binary linear erasure codes) have tree size nΩ(log n). I show the same for the states
arising in Shor’s algorithm, assuming a number-theoretic conjecture. Therefore, I argue,
by demonstrating such states in the lab on a large number of qubits, experimentalists could
weaken8 the hypothesis that all states in Nature are tree states.

Unfortunately, while I conjecture that the actual tree sizes are exponential, Raz’s
method is currently only able to show lower bounds of the form nΩ(log n). On the other hand,
I do show exponential lower bounds under a restriction, called “manifest orthogonality,” on
the allowed linear combinations of states.

More broadly, Chapter 13 develops a complexity classiﬁcation of quantum states,
and—treating that classiﬁcation as a subject in its own right—proves many basic results
about it. To give a few examples: if a quantum computer is restricted to being in a tree
state at every time step, then it can be simulated in the third level of polynomial hierarchy
PH. A random state cannot even be approximated by a state with subexponential tree
size. Any “orthogonal tree state” can be prepared by a polynomial-size quantum circuit.
Collapses of quantum state classes would imply collapses of ordinary complexity classes,
and vice versa. Many of these results involve unexpected connections between quantum
computing and classical circuit complexity. For this reason, I think that the “complexity
theory of quantum states” has an intrinsic computer-science motivation, besides its possible
role in making debates about quantum mechanics’ range of validity less philosophical and
more scientiﬁc.

2.2.3 Quantum Search of Spatial Regions

A basic result in classical computer science says that Turing machines are polynomially
equivalent to random-access machines.
In other words, we can ignore the fact that the
speed of light is ﬁnite for complexity purposes, so long as we only care about polynomial
It is easy to see that the same is true for quantum computing. Yet one of the
equivalence.
two main quantum algorithms, Grover’s algorithm, provides only a polynomial speedup.9
So, does this speedup disappear if we consider relativity as well as quantum mechanics?

8Since tree size is an asymptotic notion (and for other reasons discussed in Chapter 13), strictly speaking

experimentalists could never refute the hypothesis—just push it beyond all bounds of plausibility.

9If Grover’s algorithm is applied to a combinatorial search space of size 2n, then the speedup is by a

factor of 2n/2—but in this case the speedup is only conjectured, not proven.

15

More concretely, suppose a “quantum robot” is searching a 2-D grid of size √n

√n
for a single marked item. The robot can enter a superposition of grid locations, but moving
from one location to an adjacent one takes one time step. How many steps are needed to
ﬁnd the marked item?
If Grover’s algorithm is implemented na¨ıvely, the answer is order
n—since each of the √n Grover iterations takes √n steps, just to move the robot across
the grid and back. This yields no improvement over classical search. Benioﬀ [50] noticed
this defect of Grover’s algorithm as applied to a physical database, but failed to raise the
question of whether or not a faster algorithm exists.

×

Sadly, I was unable to prove a lower bound showing that the na¨ıve algorithm is
optimal. But in joint work with Andris Ambainis, we did the next best thing: we proved
the impossibility of proving a lower bound, or to put it crudely, gave an algorithm.
In
√n grid for a unique marked vertex in
particular, Chapter 14 shows how to search a √n
It also
only O

steps, by using a carefully-optimized recursive Grover search.

√n log3/2 n

×

(cid:17)

(cid:16)

shows how to search a d-dimensional hypercube in O (√n) steps for d
3. The latter result
has an unexpected implication: namely, that the quantum communication complexity of
the disjointness function is O (√n). This matches a lower bound of Razborov [199], and
improves previous upper bounds due to Buhrman, Cleve, and Wigderson [76] and Høyer
and de Wolf [146].

≥

Chapter 14 also generalizes our search algorithm to handle multiple marked items,
as well as graphs that are not hypercubes but have suﬃciently good expansion properties.
More broadly, the chapter develops a new model of quantum query complexity on graphs,
and proves basic facts about that model, such as lower bounds for search on “starﬁsh”
graphs. Of particular interest to physicists will be Section 14.3, which relates our results
to fundamental limits on information processing imposed by the holographic principle. For
example, we can give an approximate answer to the following question: assuming a positive
cosmological constant Λ > 0, and assuming the only constraints (besides quantum mechan-
ics) are the speed of light and the holographic principle, how large a database could ever be
searched for a speciﬁc entry, before most of the database receded past one’s cosmological
horizon?

2.2.4 Quantum Computing and Postselection

There is at least one foolproof way to solve NP-complete problems in polynomial time: guess
a random solution, then kill yourself if the solution is incorrect. Conditioned on looking at
It’s a wonder that this approach
anything at all, you will be looking at a correct solution!
is not tried more often.

The general idea, of throwing out all runs of a computation except those that
yield a particular result, is called postselection. Chapter 15 explores the general power of
I deﬁne a new complexity class
postselection when combined with quantum computing.
called PostBQP: the class of problems solvable in polynomial time on a quantum computer,
given the ability to measure a qubit and assume the outcome will be
(or equivalently,
I then show that PostBQP coincides with the
discard all runs in which the outcome is
classical complexity class PP.

1
i
|

0
i
|

).

Surprisingly, this new characterization of PP yields an extremely simple, quantum

16

computing based proof that PP is closed under intersection. This had been an open
problem for two decades, and the previous proof, due to Beigel, Reingold, and Spielman
[47], used highly nontrivial ideas about rational approximations of the sign function.
I
also reestablish an extension of the Beigel-Reingold-Spielman result due to Fortnow and
Reingold [115], that PP is closed under polynomial-time truth-table reductions.
Indeed, I
show that PP is closed under BQP truth-table reductions, which seems to be a new result.
The rest of Chapter 15 studies the computational eﬀects of simple changes to the
In particular, what if we allow linear but nonunitary trans-
p (suitably normalized)
2 to
= 2? I show that the ﬁrst change would yield exactly the power of PostBQP,
, and some
}

axioms of quantum mechanics.
formations, or change the measurement probabilities from
for some p
and therefore of PP; while the second change would yield PP if p
class between PP and PSPACE otherwise.

4, 6, 8, . . .

α
|
|

α
|
|

∈ {

My results complement those of Abrams and Lloyd [15], who showed that nonlinear
quantum mechanics would let us solve NP- and even #P-complete problems in polynomial
time; and Brun [72] and Bacon [40], who showed the same for quantum computers involving
closed timelike curves. Taken together, these results lend credence to an observation of
Weinberg [241]: that quantum mechanics is a “brittle” theory, in the sense that even a tiny
change to it would have dramatic consequences.

2.2.5 The Power of History

Contrary to widespread belief, what makes quantum mechanics so hard to swallow is not
indeterminism about the future trajectory of a particle. That is no more bizarre than a
coin ﬂip in a randomized algorithm. The diﬃculty is that quantum mechanics also seems
to require indeterminism about a particle’s past trajectory. Or rather, the very notion
of a “trajectory” is undeﬁned—for until the particle is measured, there is just an evolving
wavefunction.

In spite of this, Schr¨odinger [213], Bohm [59], Bell [49], and others proposed hidden-
variable theories, in which a quantum state is supplemented by “actual” values of certain
observables. These actual values evolve in time by a dynamical rule, in such a way that
the predictions of quantum mechanics are recovered at any individual time. On the other
hand, it now makes sense to ask questions like the following: “Given that a particle was
at location x1 at time t1 (even though it was not measured at t1), what is the probability
of it being at location x2 at time t2?” The answers to such questions yield a probability
distribution over possible trajectories.

Chapter 16 initiates the study of hidden variables from the discrete, abstract per-
spective of quantum computing.
For me, a hidden-variable theory is simply a way to
convert a unitary matrix that maps one quantum state to another, into a stochastic matrix
I list
that maps the initial probability distribution to the ﬁnal one in some ﬁxed basis.
ﬁve axioms that we might want such a theory to satisfy, and investigate previous hidden-
variable theories of Dieks [97] and Schr¨odinger [213] in terms of these axioms. I also propose
a new hidden-variable theory based on network ﬂows, which are classic objects of study in
computer science, and prove that this theory satisﬁes two axioms called “indiﬀerence” and
“robustness.” A priori, it was not at all obvious that these two key axioms could be satisﬁed
simultaneously.

6
17

Next I turn to a new question: the computational complexity of simulating hidden-
I show that, if we could examine the entire history of a hidden variable,
variable theories.
then we could eﬃciently solve problems that are believed to be intractable even for quan-
tum computers.
In particular, under any hidden-variable theory satisfying the indiﬀerence
axiom, we could solve the Graph Isomorphism and Approximate Shortest Vector prob-
lems in polynomial time, and indeed could simulate the entire class SZK (Statistical Zero
Knowledge). Combining this result with the collision lower bound of Chapter 6, we get an
oracle relative to which BQP is strictly contained in DQP, where DQP (Dynamical Quantum
Polynomial-Time) is the class of problems eﬃciently solvable by sampling histories.

queries, as opposed to O

Using the histories model, I also show that one could search an N -item database
N 1/3
√N
with Grover’s algorithm. On the other
using O
hand, the N 1/3 bound is tight, meaning that one could probably not solve NP-complete
problems in polynomial time. We thus obtain the ﬁrst good example of a model of com-
putation that appears slightly more powerful than the quantum computing model.

(cid:16)

(cid:17)

(cid:0)

(cid:1)

In summary, Chapter 16 ties together many of the themes of this thesis: the
black-box limitations of quantum computers; the application of nontrivial computer science
techniques; the obsession with the computational resources needed to simulate our universe;
and ﬁnally, the use of quantum computing to shine light on the mysteries of quantum
mechanics itself.

18

Chapter 3

Complexity Theory Cheat Sheet

“If pigs can whistle, then donkeys can ﬂy.”

(Summary of complexity theory, attributed to Richard Karp)

To most people who are not theoretical computer scientists, the theory of compu-
tational complexity—one of the great intellectual achievements of the twentieth century—is
simply a meaningless jumble of capital letters. The goal of this chapter is to turn it into a
meaningful jumble.

In computer science, a problem is ordinarily an inﬁnite set of yes-or-no questions:
for example, “Given a graph, is it connected?” Each particular graph is an instance of the
general problem. An algorithm for the problem is polynomial-time if, given any instance as
input, it outputs the correct answer after at most knc steps, where k and c are constants,
and n is the length of the instance, or the number of bits needed to specify it. For example,
in the case of a directed graph, n is just the number of vertices squared. Then P is the class
of all problems for which there exists a deterministic classical polynomial-time algorithm.
Examples of problems in P include graph connectivity, and (as was discovered two years
ago [17]) deciding whether a positive integer written in binary is prime or composite.

Now, NP (Nondeterministic Polynomial-Time) is the class of problems for which, if
the answer to a given instance is ‘yes’, then an omniscient wizard could provide a polynomial-
size proof of that fact, which would enable us to verify it in deterministic polynomial time.
As an example, consider the Satisﬁability problem: “given a formula involving the Boolean
, q (and, or, not), is there an assignment
variables x1, . . . , xn and the logical connectives
to the variables that makes the formula true?” If there is such an assignment, then a
short, easily-veriﬁed proof is just the assignment itself. On the other hand, it might be
extremely diﬃcult to ﬁnd a satisfying assignment without the wizard’s help—or for that
matter, to verify the absence of a satisfying assignment, even given a purported proof of
its absence from the wizard. The question of whether there exist polynomial-size proofs
of unsatisﬁability that can be veriﬁed in polynomial time is called the NP versus coNP
question. Here coNP is the class containing the complement of every NP problem—for
example, “given a Boolean formula, is it not satisﬁable?”

∧

∨

,

The Satisﬁability problem turns out to be NP-complete, which means it is among
the “hardest” problems in NP: any instance of any NP problem can be eﬃciently converted

19

⊆

into an instance of Satisﬁability. The central question, of course, is whether NP-complete
problems are solvable in polynomial time, or equivalently whether P = NP (it being clear
NP). By deﬁnition, if any NP-complete problem is solvable in polynomial time,
that P
then all of them are. One thing we know is that if P
= NP, as is almost universally assumed,
then there are problems in NP that are neither in P nor NP-complete [162]. Candidates for
such “intermediate” problems include deciding whether or not two graphs are isomorphic,
and integer factoring (e.g. given integers N, M written in binary, does N have a prime factor
greater than M ?). The NP-intermediate problems have been a major focus of quantum
algorithms research.

3.1 The Complexity Zoo Junior

I now present a glossary of 12 complexity classes besides P and NP that appear in this
thesis; non-complexity-theorist readers might wish to refer back to it as needed. The
known relationships among these classes are diagrammed in Figure 3.1. These classes
represent a tiny sample of the more than 400 classes described on my Complexity Zoo web
page (www.complexityzoo.com).

PSPACE (Polynomial Space) is the class of problems solvable by a deterministic

classical algorithm that uses a polynomially-bounded amount of memory. Thus NP
PSPACE, since a PSPACE machine can loop through all possible proofs.

⊆

EXP.

EXP (Exponential-Time) is the class of problems solvable by a deterministic
Thus

classical algorithm that uses at most 2q(n) time steps, for some polynomial q.
PSPACE

⊆
BPP (Bounded-Error Probabilistic Polynomial-Time) is the class of prob-
lems solvable by a probabilistic classical polynomial-time algorithm, which given any in-
stance, must output the correct answer for that instance with probability at least 2/3.
It is widely conjectured that BPP = P [147], but not even
BPP
Thus P
⊆
known that BPP

PSPACE.
NP.

PP (Probabilistic Polynomial-Time) is the class of problems solvable by a
probabilistic classical polynomial-time algorithm, which given any instance, need only out-
put the correct answer for that instance with probability greater than 1/2. The following
problem is PP-complete: given a Boolean formula ϕ, decide whether at least half of the
possible truth assignments satisfy ϕ. We have NP

PSPACE and also BPP

PP.

PP

P#P (pronounced “P to the sharp-P”) is the class of problems solvable by
a P machine that can access a “counting oracle.” Given a Boolean formula ϕ, this oracle
returns the number of truth assignments that satisfy ϕ. We have PP

PSPACE.

P#P

⊆

⊆

⊆

BQP (Bounded-Error Quantum Polynomial-Time) is the class of problems
solvable by a quantum polynomial-time algorithm, which given any instance, must output
the correct answer for that instance with probability at least 2/3. More information is in
Chapter 4. We have BPP

PP [55, 16].

BQP

EQP (Exact Quantum Polynomial-Time) is similar to BQP, except that the
probability of correctness must be 1 instead of 2/3. This class is extremely artiﬁcial; it
is not even clear how to deﬁne it independently of the choice of gate set. But for any
reasonable choice, P

BQP.

EQP

⊆

⊆

⊆

⊆

⊆
⊆

⊆

⊆

6
20

⊆

P/poly (P with polynomial-size advice) is the class of problems solvable by a
P algorithm that, along with a problem instance of length n, is also given an “advice string”
zn of length bounded by a polynomial in n. The only constraint is that zn can depend
only on n, and not on any other information about the instance. Otherwise the zn’s can be
P/poly. Since
chosen arbitrarily to help the algorithm.
the zn’s can encode noncomputable problems (for example, does the nth Turing machine
halt?), P/poly is not contained in any uniform complexity class, where “uniform” means
that the same information is available to an algorithm regardless of n. We can also add
polynomial-size advice to other complexity classes, obtaining EXP/poly, PP/poly, and so
on.

It is not hard to show that BPP

PH (Polynomial-Time Hierarchy) is the union of NP, NPNP, NPNPNP

, etc.
Equivalently, PH is the class of problems that are polynomial-time reducible to the follow-
ing form: for all truth assignments x, does there exist an assignment y such that for all
assignments z, . . . , ϕ (x, y, z, . . .) is satisﬁed, where ϕ is a Boolean formula? Here the num-
ber of alternations between “for all” and “there exists” quantiﬁers is a constant independent
of n. Sipser [222] and Lautemann [163] showed that BPP
PH, while Toda [228] showed
that PH

⊆
MA (Merlin Arthur) is the class of problems for which, if the answer to a given
instance is ‘yes,’ then an omniscient wizard could provide a polynomial-size proof of that
fact, which would enable us to verify it in BPP (classical probabilistic polynomial-time, with
probability at most 1/3 of accepting an invalid proof or rejecting a valid one). We have
NP

⊆
AM (Arthur Merlin) is the class of problems for which, if the answer to a given
instance is ‘yes,’ then a BPP algorithm could become convinced of that fact after a constant
number of rounds of interaction with an omniscient wizard. We have MA
PH.
There is evidence that AM = MA = NP [157].

P#P.

PP.

MA

AM

⊆

⊆

⊆

⊆

SZK (Statistical Zero Knowledge) is the class of problems that possess “sta-
AM. Although SZK
tistical zero-knowledge proof protocols.” We have BPP
contains nontrivial problems such as graph isomorphism [130], there are strong indications
that it does not contain all of NP [63].

SZK

⊆

⊆

Other complexity classes, such as PLS, TFNP, BQP/qpoly, and BPPpath, will be

introduced throughout the thesis as they are needed.

3.2 Notation

In computer science, the following symbols are used to describe asymptotic growth rates:

•

•

•

F (n) = O (G (n)) means that F (n) is at most order G (n); that is, F (n)
for all n

0 and some nonnegative constants a, b.

≥

a + bG (n)

≤

F (n) = Ω (G (n)) means that F (n) is at least order G (n); that is, G (n) = O (F (n)).

F (n) = Θ (G (n)) means that F (n) is exactly order G (n); that is, F (n) = O (G (n))
and F (n) = Ω (G (n)).

21

EXP

PSPACE

P#P

PH

AM

MA

NP

P

PP

BQP

EQP

P/poly

SZK

BPP

Figure 3.1: Known relations among 14 complexity classes.

•

F (n) = o (G (n)) means that F (n) is less than order G (n); that is, F (n) = O (G (n))
but not F (n) = Ω (G (n)).

The set of all n-bit strings is denoted

n

0 {

≥

0, 1
}

n, is denoted

0, 1
}
{

∗.

n. The set of all binary strings,

0, 1
}
{

S
3.3 Oracles

One complexity-theoretic concept that will be needed again and again in this thesis is that
of an oracle. An oracle is a subroutine available to an algorithm, that is guaranteed to
compute some function even if we have no idea how. Oracles are denoted using superscripts.
For example, PNP is the class of problems solvable by a P algorithm that, given any instance
of an NP-complete problem such as Satisﬁability, can instantly ﬁnd the solution for that
instance by calling the NP oracle. The algorithm can make multiple calls to the oracle, and
these calls can be adaptive (that is, can depend on the outcomes of previous calls).
If a
quantum algorithm makes oracle calls, then unless otherwise speciﬁed we assume that the
calls can be made in superposition. Further details about the quantum oracle model are
provided in Chapter 5.

∗

We identify an oracle with the function that it computes, usually a Boolean func-
tion f :
. Often we think of f as deﬁning a problem instance, or rather
0, 1
}
{
an inﬁnite sequence of problem instances, one for each positive integer n. For example,
n such that f (x) = 1?” In these cases the oracle string,
“does there exist an x
n, can be thought of as an input that is 2n bits
which consists of f (x) for every x

0, 1
}

0, 1
}

→ {

∈ {

0, 1
}

∈ {

long instead of n bits. Of course, a classical algorithm running in polynomial time could
examine only a tiny fraction of such an input, but maybe a quantum algorithm could do
better. When discussing such questions, we need to be careful to distinguish between two
functions: f itself, and the function of the oracle string that an algorithm is trying is to
compute.

22

23

Chapter 4

Quantum Computing Cheat Sheet

“Somebody says . . . ‘You know those quantum mechanical amplitudes you told
me about, they’re so complicated and absurd, what makes you think those are
right? Maybe they aren’t right.’ Such remarks are obvious and are perfectly
It does not do any good to
clear to anybody who is working on this problem.
point this out.”

—Richard Feynman, The Character of Physical Law [109]

Non-physicists often have the mistaken idea that quantum mechanics is hard.
Unfortunately, many physicists have done nothing to correct that idea. But in newer
textbooks, courses, and survey articles [18, 114, 175, 182, 235], the truth is starting to come
out: if you wish to understand the central ‘paradoxes’ of quantum mechanics, together with
almost the entire body of research on quantum information and computing, then you do
not need to know anything about wave-particle duality, ultraviolet catastrophes, Planck’s
constant, atomic spectra, boson-fermion statistics, or even Schr¨odinger’s equation. All you
need to know is how to manipulate vectors whose entries are complex numbers.
If that is
too diﬃcult, then positive and negative real numbers turn out to suﬃce for most purposes
as well. After you have mastered these vectors, you will then have some context if you wish
to learn more about the underlying physics. But the historical order in which the ideas
were discovered is almost the reverse of the logical order in which they are easiest to learn!
What quantum mechanics says is that, if an object can be in either of two per-
, then it can also be in a linear
. Here α and β are complex numbers
are called

fectly distinguishable states, which we denote
“superposition” of those states, denoted α
0
i
|
2 = 1. The asymmetric brackets
2 +
called “amplitudes,” which satisfy
β
|
“Dirac ket notation”; one gets used to them with time.

0
and
|
i
1
+ β
i
|

α
|
|

1
i

| i

|

|

|

+β

0
i

with probability

If we measure the state α
α
|
|

in a standard way, then we see the “basis state”
1
i
|
2. Also, the state changes to whichever
2, and
with probability
0
|
|
i
and then measure again, nothing having happened in the
outcome we see—so if we see
2 sum to 1, as they ought
interim, we will still see
|
So far, we might as well have described the object using classical probabilities—for
to.
example, “this cat is alive with probability 1/2 and dead with probability 1/2; we simply
don’t know which.”

. The two probabilities

2 and
α
|

1
i
|
0
i
|

0
i
|

β
|

β

|

|

24

The diﬀerence between classical probabilities and quantum amplitudes arises in
how the object’s state changes when we perform an operation on it. Classically, we can
multiply a vector of probabilities by a stochastic matrix, which is a matrix of nonnegative
real numbers each of whose columns sums to 1. Quantum-mechanically, we multiply the
vector of amplitudes by a unitary matrix, which is a matrix of complex numbers that maps
any unit vector to another unit vector. (Equivalently, U is unitary if and only if its inverse
1 equals its conjugate transpose U ∗.) As an example, suppose we start with the state
U −
, which corresponds to the vector of amplitudes
0
i
|

We then left-multiply this vector by the unitary matrix

1
0

.

(cid:21)

(cid:20)

which maps the vector to

and therefore the state

to

0
i
|

U =

1
√2 −
1
√2

1
√2
1
√2 #

,

"

1
√2
1
√2 #

,

"

U

0
i
|

=

1
√2 |

0
i

+

1
1
√2 |
i

.

with probability 1/2.
If we now measured, we would see
The interesting part is what happens if we apply the same operation U a second time,
without measuring. We get

with probability 1/2 and

1
i
|

0
i
|

1
√2 −
1
√2

1
√2
1
√2 # "

1
√2
1
√2 #

"

0
1

=

(cid:20)

(cid:21)

1
i
|

which is
with certainty (see Figure 4.1). Applying a “randomizing” operation to a
“random” state produces a deterministic outcome! The reason is that, whereas probabilities
are always nonnegative, amplitudes can be positive, negative, or even complex, and can
therefore cancel each other out. This interference of amplitudes can be considered the
source of all “quantum weirdness.”

4.1 Quantum Computers: N Qubits

The above description applied to “qubits,” or objects with only two distinguishable states.
Indeed, in
But it generalizes to objects with a larger number of distinguishable states.
quantum computing we consider a system of N qubits, each of which can be
. We
or
0
i
|
then need to assign amplitudes to all 2N possible outcomes of measuring the qubits in order
from ﬁrst to last. So the computer’s state has the form

1
i
|

=

ψ
|

i

N

0,1
Xz
}
∈{

z

αz |

i

25

1

0

+

1

2

0

, with α and β real, can be represented
1
Figure 4.1: Quantum states of the form α
0
i
i
|
by unit vectors in the plane. Then the operation U corresponds to a 45◦ counterclockwise
rotation.

+ β

|

where

2 = 1.

αz|
|

N

0,1
Xz
}
∈{

What was just said is remarkable—for it suggests that Nature needs to keep track of 2N
complex numbers just to describe a state of N interacting particles.
If N = 300, then this
is already more complex numbers than there are particles in the known universe. The goal
of quantum computing is to exploit this strange sort of parallelism that is inherent in the
laws of physics as we currently understand them.

x
i
|

The diﬃculty is that, when the computer’s state is measured, we only see one of
, not the entire collection of amplitudes. However, for a few speciﬁc
the “basis states”
problems, we might be able to arrange things so that basis states corresponding to wrong
answers all have amplitudes close to 0, because of interference between positive and negative
contributions.
If we can do that, then basis states corresponding to right answers will be
measured with high probability.

More explicitly, a quantum computer applies a sequence of unitary matrices called
gates, each of which acts on only one or two of the N qubits (meaning that is a tensor
2 qubits, and the operation of interest on
product of the identity operation on N
the remaining qubits). As an example, the controlled-NOT or CNOT gate is a two-qubit
gate that ﬂips a “target” qubit if a “control” qubit is 1, and otherwise does nothing:

1 or N

−

−

i → |
The unitary matrix corresponding to the CNOT gate is

i → |

i → |

|

00
|

,

00
i

01

,

01
i

10
|

,

11
i

11
|

i → |

.

10
i

1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0









.





26

Adleman, DeMarrais, and Huang [16] showed that the CNOT gate, together with the one-
qubit gate

4
3
5
5 −
4
3
5 (cid:21)
5

,

(cid:20)

constitute a universal set of quantum gates, in that they can be used to approximate any
other gate to any desired accuracy.
Indeed, almost any set of one- and two-qubit gates is
universal in this sense [94].

A quantum circuit is just a sequence of gates drawn from a ﬁnite universal set.
Without loss of generality, we can take the circuit’s output to be the result of a single
N with probability
2.
measurement after all gates have been applied; that is, z
αz|
|
(If a binary output is needed, we simply throw away the last N
1 bits of z.) It is known
that allowing intermediate measurements does not yield any extra computational power
[55]. The circuit is polynomial-size if both N and the number of gates are upper-bounded
by a polynomial in the length n of the input.

0, 1
}
−

∈ {

We can now deﬁne the important complexity class BQP, or Bounded-Error Quan-
n, ﬁrst a polynomial-time classical algo-
tum Polynomial-Time. Given an input x
∈ {
rithm A prepares a polynomial-size quantum circuit Ux. (The requirement that the circuit
itself be eﬃciently preparable is called uniformity.) Then Ux is applied to the “all-0” initial
n is in BQP if there exists an A such that for all
state
x,

N . We say a language L

0, 1
}

0, 1
}

0
i
|

⊆ {

⊗

(i) If x

∈
(ii) If x /
∈

L then Ux outputs ‘1’ with probability at least 2/3.

L then Ux outputs ‘0’ with probability at least 2/3.

By running Ux multiple times and taking the majority answer, we can boost the

−

probability of success from 2/3 to 1

2−

p(n) for any polynomial p.

BQP was ﬁrst deﬁned in a 1993 paper by Bernstein and Vazirani [55].1 That paper
marked a turning point. Before, quantum computing had been an idea, explored in pio-
neering work by Deutsch [90], Feynman [108], and others. Afterward, quantum computing
was a full-ﬂedged model in the sense of computational complexity theory, which could be
meaningfully compared against other models. For example, Bernstein and Vazirani showed
P#P: informally, quantum computers are at least as powerful as classi-
that BPP
cal probabilistic computers, and at most exponentially more powerful.
(The containment
BQP

PP by Adleman, DeMarrais, and Huang [16].)

P#P was later improved to BQP

BQP

⊆

⊆

Bernstein and Vazirani also gave an oracle problem called Recursive Fourier Sam-
pling (RFS), and showed that it requires nΩ(log n) classical probabilistic queries but only n
quantum queries. This provided the ﬁrst evidence that quantum computers are strictly
= BQP. Soon after-
more powerful than classical probabilistic computers, i.e. that BPP
ward, Simon [220] widened the gap to polynomial versus exponential, by giving an ora-
classical probabilistic queries but only O (n) quantum
cle problem that requires Ω

2n/2

⊆

⊆

1As a historical note, Bernstein and Vazirani [55] deﬁned BQP in terms of “quantum Turing machines.”
However, Yao [248] showed that Bernstein and Vazirani’s deﬁnition is equivalent to the much simpler one
given here. Also, Berthiaume and Brassard [57] had implicitly deﬁned EQP (Exact Quantum Polynomial-
Time) a year earlier, and had shown that it lies outside P and even NP relative to an oracle.

(cid:0)

(cid:1)

6
queries. However, these results attracted limited attention because the problems seemed
artiﬁcial.

27

People ﬁnally paid attention when Shor [219] showed that quantum computers
could factor integers and compute discrete logarithms in polynomial time. The security
of almost all modern cryptography rests on the presumed intractability of those two prob-
It had long been known [177] that factoring is classically reducible to the following
lems.
problem: given oracle access to a periodic function f :
, where R is
exponentially large, ﬁnd the period of f . Shor gave an eﬃcient quantum algorithm for this
oracle problem, by exploiting the quantum Fourier transform, a tool that had earlier been
used by Simon.
(The algorithm for the discrete logarithm problem is more complicated
but conceptually similar.)

1, . . . , R

1, . . . , R

} → {

}

{

Other results in the “quantum canon,” such as Grover’s algorithm [139] and meth-
ods for quantum error-correction and fault-tolerance [20, 80, 132, 159, 225], will be discussed
in this thesis as the need arises.

4.2 Further Concepts

This section summarizes “fancier” quantum mechanics concepts, which are needed for Part
II and for Chapter 10 of Part I (which deals with quantum advice). They are not needed
for the other chapters in Part I.
If

and

product, denoted
i ⊗ |
other. For example, if

Tensor Product.
or
ψ
ψ
|
|
= α

ϕ
i
ψ
i
|

|

ϕ
i

ψ
i
|
, is just a state that consists of
ϕ
i
+ β

are two quantum states, then their tensor
next to each
ψ
i
|
, then
1
i

ϕ
i
|

0
i
|

1
i
|

ϕ
i

and

and

= γ

+ δ

|

|

i |
0
i
|

and

ϕ
i
|

where
∗
such as

If

ψ
h

ϕ
i
|

ψ
|

ϕ
i

i |

= (α

0
i
|

+ β

) (γ

1
i
|

0
i
|

+ δ

1
i
|

) = αγ

00
i

|

+ αδ

01
i
|

+ βγ

Inner Product. The inner product between two states
is deﬁned as
1
= β1 |
i

N
+ βN |

· · ·

+

i

ψ
|

i

ψ
h

ϕ
i
|

= α∗1β1 +

· · ·

+ α∗N βN

+ βδ

10
|
i
1
= α1 |
i

+

|

.

11
i
+αN |

· · ·

N

i

denotes complex conjugate. The inner product satisﬁes all the expected properties,
ψ
h

= 1 and

ψ
|

i

+

) =
φ
i
|

ϕ
i
|
are orthogonal.

ψ
h

+

ψ
h

φ
i
|

.

(

ψ
|

ψ
h
|
and

ϕ
|
i
ϕ
= 0 then we say
i
|
General Measurements.
ϕ1i
{|
ϕji

ϕN i}
|
ϕj i|
ψ
|

, . . . ,

is

i

|

in which to measure a state

In principle, we can choose any orthogonal basis of
states
(Whether that measurement
i
can actually be performed eﬃciently is another matter.) Then the probability of obtaining
2. We can even measure in a non-orthogonal basis, a concept called
outcome
Positive Operator Valued Measurements (POVM’s) that I will not explain here. None of
these more general measurements increase the power of the quantum computing model,
since we can always produce the same eﬀect by ﬁrst applying a unitary matrix (possibly
using additional qubits called ancillas), and then measuring in a “standard” basis such as

ψ
|

|h

.

1
i

{|

, . . . ,

.

N

|

i}

28

|

0
i

+ β

1
i
|

Mixed States. Superposition states, such as α

, are also called pure
states. This is to distinguish them from mixed states, which are the most general kind
of state in quantum mechanics. Mixed states are just classical probability distributions
over pure states. There is a catch, though: any mixed state can be decomposed into
a probability distribution over pure states in inﬁnitely many nonequivalent ways.
For
with probability 1/2, then
with probability 1/2 and
example, if we have a state that is
|
) /√2 with probability
no experiment could ever distinguish it from a state that is (
0
i
|
) /√2 with probability 1/2. For regardless of what orthogonal basis we
1/2 and (
|
measured in, the two possible outcomes of measuring would both occur with probability
1/2. Therefore, this state is called the one-qubit maximally mixed state.

1
0
i
i − |

1
i
+

0
i
|

1
i
|

Density Matrices. We can represent mixed states using a formalism called
with itself, denoted
+
1
= α1 |
i
N complex matrix whose (i, j) entry is αiα∗j . Now suppose we have a
with probability
p. We represent the state by a Hermitian positive deﬁnite matrix ρ with trace 1, as

density matrices. The outer product of
, is an N
ψ
ψ
|
|
state that is
ϕ
i
1
−
follows:

N
+ αN |
0
= γ
i
|

with probability p, and

×
= α

1
i
|

1
i
|

φ
i

+ β

0
i

+ δ

· · ·

i h

ψ

i

i

|

|

|

|

αα∗ αβ∗
βα∗ ββ∗ (cid:21)

ρ = p

ϕ
|

ϕ
|

+ (1

p)

φ
|

= p

φ
|

−

i h

i h

δδ∗ (cid:21)
1. When
When we apply a unitary operation U , the density matrix ρ changes to U ρU −
is the jth diagonal entry
we measure in the standard basis, the probability of outcome
of ρ. Proving that these rules are the correct ones, and that a density matrix really is a
unique description of a mixed state, are “exercises for the reader” (which as always means
the author was too lazy). Density matrices will mainly be used in Chapter 10.

j
|

−

(cid:20)

(cid:20)

i

+ (1

p)

.

γγ∗ γδ∗
δγ∗

Trace Distance. Suppose you are given a system that was prepared in state ρ
with probability 1/2, and σ with probability 1/2. After making a measurement, you must
guess which state the system was prepared in. What is the maximum probability that you
will be correct? The answer turns out to be

1 +

ρ
−
k
2

σ

ktr

σ.

−

where
σ
are the eigenvalues of ρ

ρ
k

−

ktr is the trace distance between ρ and σ, deﬁned as 1

2

λi|
i |

where λ1, . . . , λN

Entanglement. Suppose ρ is a joint state of two systems. If ρ can be written as
, then we say ρ is separable;

ψ

a probability distribution over pure states of the form
otherwise ρ is entangled.
Hamiltonians.

|

i ⊗ |

ϕ
i

Instead of discrete unitary operations, we can imagine that a
quantum state evolves in time by a continuous rotation called a Hamiltonian. A Hamil-
N Hermitian matrix H. To ﬁnd the unitary operation U (t) that is
tonian is an N
eﬀected by “leaving H on” for t time steps, the rule2 is U (t) = e−
iHt. The only place I
use Hamiltonians is in Chapter 14, and even there the use is incidental.

×

P

2Here Planck’s constant is set equal to 1 as always.

29

Part I

Limitations of Quantum
Computers

30

Chapter 5

Introduction

“A quantum possibility is less real than a classical reality, but more real than

a classical possibility.”

—Boris Tsirelson [229]

Notwithstanding accounts in the popular press, a decade of research has made it
clear that quantum computers would not be a panacea. In particular, we still do not have a
quantum algorithm to solve NP-complete problems in polynomial time. But can we prove
that no such algorithm exists, i.e. that NP
BQP? The diﬃculty is that we can’t even
6⊂
prove no classical algorithm exists; this is the P versus NP question. Of course, we could
ask whether NP
= NP—but unfortunately, even this conditional
question seems far beyond our ability to answer. So we need to reﬁne the question even
further: can quantum computers solve NP-complete problems in polynomial time, by brute
force?

BQP assuming that P

6⊂

What is meant by “brute force” is the following.

In Shor’s factoring algorithm

[219], we prepare a superposition of the form

1
√R

R

r=1
X

r
|

g (r)
i

i |

where g (r) = xr mod N for some x, N . But as far as the key step of the algorithm is
concerned, the function g is a “black box.” Given any superposition like the one above,
the algorithm will ﬁnd the period of g assuming g is periodic; it does not need further
information about how g was computed. So in the language of Section 3.3, we might as
well say that g is computed by an oracle.

Now suppose we are given a Boolean formula ϕ over n variables, and are asked to
decide whether ϕ is satisﬁable. One approach would be to exploit the internal structure
of ϕ: “let’s see, if I set variable x37 to TRUE, then this clause here is satisﬁed, but those
other clauses aren’t satisﬁed any longer . . . darn!” However, inspired by Shor’s factoring
algorithm, we might hope for a cruder quantum algorithm that treats ϕ merely as an oracle,
n to an output bit ϕ (x) that is 1 if and only if x satisﬁes
mapping an input string x
0, 1
}
n such that
ϕ. The algorithm would have to decide whether there exists an x

∈ {

0, 1
}

∈ {

6
ϕ (x) = 1, using as few calls to the ϕ oracle as possible, and not learning about ϕ in any
other way. This is what is meant by brute force.

31

A fundamental result of Bennett, Bernstein, Brassard, and Vazirani [51] says that
no brute-force quantum algorithm exists to solve NP-complete problems in polynomial time.
In particular, for some probability distribution over oracles, any quantum algorithm needs
2n/2
oracle calls to decide, with at least a 2/3 chance of being correct, whether there
Ω
n such that ϕ (x) = 1. On a classical computer, of course, Θ (2n) oracle
exists an x
calls are necessary and suﬃcient. But as it turns out, Bennett et al.’s quantum lower bound
is tight, since Grover’s quantum search algorithm [139] can ﬁnd a satisfying assignment (if
it exists) quadratically faster than any classical algorithm. Amusingly, Grover’s algorithm
was proven optimal before it was discovered to exist!

0, 1
}

∈ {

(cid:0)

(cid:1)

A recurring theme in this thesis is the pervasiveness of Bennett et al.’s ﬁnding.

I
will show that, even if a problem has considerably more structure than the basic Grover
search problem, even if “quantum advice states” are available, or even if we could examine
the entire history of a hidden variable, still any brute-force quantum algorithm would take
exponential time.

5.1 The Quantum Black-Box Model

The quantum black-box model formalizes the idea of a brute-force algorithm. For the time
being, suppose that a quantum algorithm’s goal is to evaluate f (X), where f :
0, 1
{
}
state at any time t has the form

→
is a Boolean function and X = x1 . . . xn is an n-bit string. Then the algorithm’s

0, 1
}
{

n

α(t)
i,z |

i, z

.

i

Xi,z

}

∈ {

1, . . . , N

Here i
is the index of an oracle bit xi to query, and z is an arbitrarily large
string of bits called the “workspace,” containing whatever information the algorithm wants
to store there. The state evolves in time via an alternating sequence of algorithm steps and
query steps. An algorithm step multiplies the vector of αi,z’s by an arbitrary unitary matrix
It does not matter how many quantum gates would be needed
that does not depend on X.
, eﬀecting
to
to implement this matrix. A query step maps each basis state
the transformation α(t+1)
xi is the string z, with xi exclusive-OR’ed
⊕
into a particular location in z called the “answer bit.” The reason exclusive-OR is used is
that the query step has to be reversible, or else it would not be unitary.

i,z = α(t)

. Here z

i, z
|

i, z
|

xii

⊕

i,z

xi

⊕

i

At the ﬁnal step T , the state is measured in the standard basis, and the output of

the algorithm is taken to be (say) z1, the ﬁrst bit of z. The algorithm succeeds if

2

α(T )
i,z

2
3

≥

Xi,z : z1=f (X) (cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

∈ {

0, 1
}

n. Here the constant 2/3 is arbitrary. Then the (bounded-error) quantum
for all X
query complexity of f , denoted Q2 (f ), is the minimum over all quantum algorithms A that
succeed at evaluating f , of the number of queries to f made by A. Here the ‘2’ represents
the fact that the error probability is two-sided. One can compare Q2 (f ) with Q0 (f ),

32

or zero-error quantum query complexity; R2 (f ), or bounded-error classical randomized
query complexity; and D (f ), or deterministic query complexity, among other complexity
measures. Chapter 8 will deﬁne many such measures and compare them in detail.

As a simple example of the black-box model, let OR (x1, . . . , xn) = x1 ∨ · · · ∨

xn.
Then Grover’s algorithm [139] implies that Q2 (OR) = O (√n), while the lower bound of
Bennett et al. [51] implies that Q2 (OR) = Ω (√n). By comparison, D (OR) = R2 (OR) =
Θ (n).

The quantum black-box model has some simple generalizations, which I will use
n

when appropriate. First, f can be a partial function, deﬁned only on a subset of
0, 1
}
{
(so we obtain what is called a promise problem). Second, the xi’s do not need to be bits;
in Chapters 6 and 7 they will take values from a larger range. Third, in Chapter 7 the
output will not be Boolean, and there will generally be more than one valid output (so we
obtain what is called a relation problem).

5.2 Oracle Separations

“I do believe it
Against an oracle.”
—Shakespeare, The Tempest

Several times in this thesis, I will use a lower bound on quantum query complexity
to show that a complexity class is not in BQP “relative to an oracle.” The method for turning
query complexity lower bounds into oracle separations was invented by Baker, Gill, and
= NPA. Basically,
Solovay [41] to show that there exists an oracle A relative to which PA
they encoded into A an inﬁnite sequence of exponentially hard search problems, one for
each input length n, such that (i) a nondeterministic machine can solve the nth problem
in time polynomial in n, but (ii) any deterministic machine would need time exponential
in n. They guaranteed (ii) by “diagonalizing” against all possible deterministic machines,
similarly to how Turing created an uncomputable real number by diagonalizing against all
possible computable reals.
Later, Bennett and Gill [54] showed that a simpler way to
guarantee (ii) is just to choose the search problems uniformly at random. Throughout the
thesis, I will cavalierly ignore such issues, proceeding immediately from a query complexity
lower bound to the statement of the corresponding oracle separation.

The point of an oracle separation is to rule out certain approaches to solving
For example, the Baker-Gill-Solovay theorem
an open problem in complexity theory.
implies that the standard techniques of computability theory, which relativize (that is, are
“oblivious” to the presence of an oracle), cannot be powerful enough to show that P = NP.
Similarly, the result of Bennett et al. [51] that Q2 (OR) = Ω (√n) implies that there exists
BQPA. While this does not show that NP
an oracle A relative to which NPA
BQP,
it does show that any proof of NP
BQP would have to use “non-relativizing” techniques
that are unlike anything we understand today.

6⊂
⊆

6⊂

However, many computer scientists are skeptical that anything can be learned
from oracles. The reason for their skepticism is that over the past 15 years, they have
seen several examples of non-relativizing results in classical complexity theory. The most

6
33

famous of these is Shamir’s Theorem [215, 171] that PSPACE
IP, where IP is the class of
problems that have interactive proof systems, meaning that if the answer for some instance
is “yes,” then a polynomial-time veriﬁer can become convinced of that fact to any desired
level of conﬁdence by exchanging a sequence of messages with an omniscient prover.1 By
contrast, oracles had been known relative to which not even coNP, let alone PSPACE, is
contained in IP [117]. So why should we ever listen to oracles again, if they got interactive
proofs so dramatically wrong?

⊆

My answer is threefold. First, essentially all quantum algorithms that we know
today—from Shor’s algorithm, as discussed previously, to Grover’s algorithm, to the quan-
tum adiabatic algorithm2 [106], to the algorithms of Hallgren [141] and van Dam, Hallgren,
and Ip [232]—are oracle algorithms at their core. We do not know of any non-relativizing
quantum algorithm technique analogous to the arithmetization technique that was used to
prove PSPACE
If such a technique is ever discovered, I will be one of the ﬁrst to
⊆
want to learn it.

IP.

The second response is that without oracle results, we do not have even the be-
ginnings of understanding. Once we know (for example) that SZK
BQP relative to an
oracle, we can then ask the far more diﬃcult unrelativized question, knowing something
about the hurdles that any proof of SZK

BQP would have to overcome.

6⊂

The third response is that “the proof of the pudding is in the proving.” In other
words, the real justiﬁcation for the quantum black-box model is not the a priori plausibility
of its assumptions, but the depth and nontriviality of what can be (and has been) proved
in it. For example, the result that coNP
IP relative to an oracle [117] does not tell us
much about interactive proof systems. For given an exponentially long oracle string X, it
is intuitively obvious that nothing a prover could say could convince a classical polynomial-
time veriﬁer that X is the all-0 string, even if the prover and veriﬁer could interact. The
only issue is how to formalize that obvious fact by diagonalizing against all possible proof
systems. By contrast, the quantum oracle separations that we have are not intuitively
obvious in the same way; or rather, the act of understanding them confers an intuition
where none was previously present.

6⊂

⊆

⊆

1Arora, Impagliazzo, and Vazirani [36] claim the Cook-Levin Theorem, that Satisﬁability is NP-complete,
as another non-relativizing result. But this hinges on what we mean by “non-relativizing,” far more than
the PSPACE

IP example.

2Given an assignment x to a 3SAT formula ϕ, the adiabatic algorithm actually queries an oracle that
returns the number of clauses of ϕ that x satisﬁes, not just whether x satisﬁes ϕ or not. Furthermore, van
Dam, Mosca, and Vazirani [233] have shown such an oracle is suﬃcient to reconstruct ϕ. On the other
hand, the adiabatic algorithm itself would be just as happy with a ﬁtness landscape that did not correspond
to any 3SAT instance, and that is what I mean by saying that it is an oracle algorithm at the core.

34

Chapter 6

The Collision Problem

The collision problem of size n, or Coln, is deﬁned as follows. Let X = x1 . . . xn
, with n even. We are guaranteed that

be a sequence of n integers drawn from
either

1, . . . , n
{

}

(1) X is one-to-one (that is, a permutation of

(2) X is two-to-one (that is, each element of

1, . . . , n
{
1, . . . , n
{

}

), or

}
appears in X twice or not at all).

The problem is to decide whether (1) or (2) holds.

(A variant asks us to ﬁnd a
collision in a given two-to-one function. Clearly a lower bound for the collision problem as
deﬁned above implies an equivalent lower bound for this variant.) Because of its simplicity,
the collision problem was widely considered a benchmark for our understanding of quantum
query complexity.

I will show that Q2 (Coln) = Ω

, where Q2 (f ) is the bounded-error quantum
query complexity of function f . The best known upper bound, due to Brassard, Høyer,
(cid:1)
and Tapp [68], is O
(see Section 2.1.1). Previously, though, no lower bound better
than the trivial Ω (1) bound was known. How great a speedup quantum computers yield
for the problem was apparently ﬁrst asked by Rains [193].

n1/3

(cid:1)

(cid:0)

(cid:0)

n1/5

Previous lower bound techniques failed for the problem because they depended on
a function’s being sensitive to many disjoint changes to the input. For example, Beals et al.
, where bs (f ) is the
[45] showed that for all total Boolean functions f , Q2 (f ) = Ω
block sensitivity, deﬁned by Nisan [183] to be, informally, the maximum number of disjoint
changes (to any particular input X) to which f is sensitive.
In the case of the collision
problem, though, every one-to-one input diﬀers from every two-to-one input in at least n/2
places, so the block sensitivity is O (1). Ambainis’s adversary method [27] faces a related
In that method we consider the algorithm and input as a bipartite quantum
obstacle.
state, and upper-bound how much the entanglement of the state can increase via a single
query. But under the simplest measures of entanglement, it turns out that the algorithm
and input can become almost maximally entangled after O (1) queries, again because every
one-to-one input is far from every two-to-one input.1

bs (f )

(cid:16)p

(cid:17)

1More formally, the adversary method cannot prove any lower bound on Q2 (f ) better than RC (f ),
where RC (f ) is the randomized certiﬁcate complexity of f (to be deﬁned in Chapter 8). But for the

My proof is an adaptation of the polynomial method, introduced to quantum com-
puting by Beals et al. [45]. Their idea was to reduce questions about quantum algorithms
to easier questions about multivariate polynomials.
In particular, if a quantum algorithm
makes T queries, then its acceptance probability is a polynomial over the input bits of
degree at most 2T . So by showing that any polynomial approximating the desired output
has high degree, one obtains a lower bound on T .

35

To lower-bound the degree of a multivariate polynomial, a key technical trick is to
construct a related univariate polynomial. Beals et al. [45], using a lemma due to Minsky
) (where
X
and Papert [178], replace a polynomial p (X) (where X is a bit string) by q (
|
X
|

denotes the Hamming weight of X), satisfying

|

|

q (k) = EX
=k

X
|

|

p (X)

deg (p).

and deg (q)

≤

Here I construct the univariate polynomial in a diﬀerent way. I consider a uniform
distribution over g-to-one inputs, where g might be greater than 2. Even though the
problem is to distinguish g = 1 from g = 2, the acceptance probability must lie in the
interval [0, 1] for all g, and that is a surprisingly strong constraint.
I show that the
acceptance probability is close to a univariate polynomial in g of degree at most 2T . I then
obtain a lower bound by generalizing a classical approximation theory result of Ehlich and
Zeller [104] and Rivlin and Cheney [204]. Much of the proof deals with the complication
that g does not divide n in general.

(cid:0)

(cid:1)

n1/3

Shortly after this work was completed, Shi [218] improved it to give a tight lower
bound of Ω
for the collision problem, when the xi range from 1 to 3n/2 rather than
from 1 to n. For a range of size n, his bound was Ω
. Subsequently Kutin [161]
for a range of size n as well. By a
and Ambainis [29] showed a lower bound of Ω
(cid:0)
for the element distinctness
simple reduction, these results imply a lower bound of Ω
(cid:0)
problem—that of deciding whether there exist i
= j such that xi = xj. The previous best
(cid:1)
known lower bound was Ω
, and at the time of Shi’s work, the best known upper
bound was O
, due to Buhrman et al. [77]. Recently, however, Ambainis [30] gave a
novel algorithm based on quantum walks that matches the n2/3 lower bound.

(cid:1)
n2/3

n1/2

n3/4

n1/4

n1/3

(cid:0)

(cid:1)

(cid:0)

(cid:1)

The chapter is organized as follows. Section 6.1 motivates the collision lower bound
within quantum computing, pointing out connections to collision-resistant hash functions,
the nonabelian hidden subgroup problem, statistical zero-knowledge, and information era-
sure. Section 6.2 gives technical preliminaries, Section 6.3 proves the crucial fact that the
acceptance probability is “almost” a univariate polynomial, and Section 6.4 completes the
lower bound argument.
In Section 6.5
n1/7
for the set comparison problem, a variant of the collision
I show a lower bound of Ω
problem needed for the application to information erasure.

I conclude in Section 6.6 with some open problems.

(cid:0)

(cid:1)

(cid:0)

(cid:1)

collision function, RC (Coln) = O (1).

6
36

6.1 Motivation

In Chapter 1 I listed seven implications of the collision lower bound; this section discusses a
few of those implications in more detail. The implication that motivated me personally—
concerning the computational power of so-called hidden-variable theories—is deferred to
Chapter 16.

6.1.1 Oracle Hardness Results

The original motivation for the collision problem was to model (strongly) collision-resistant
hash functions in cryptography. There is a large literature on collision-resistant hashing;
see [201, 42] for example. When building secure digital signature schemes, it is useful to
, such that ﬁnding a distinct (x, y) pair with Hi (x) =
have a family of hash functions
Hi (y) is computationally intractable. A quantum algorithm for ﬁnding collisions using
O (polylog (n)) queries would render all hash functions insecure against quantum attack
in this sense.
(Shor’s algorithm [219] already renders hash functions based on modular
arithmetic insecure.) My result indicates that collision-resistant hashing might still be
possible in a quantum setting.

Hi}
{

The collision problem also models the nonabelian hidden subgroup problem, of
G,
which graph isomorphism is a special case. Given a group G and subgroup H
G,
suppose we have oracle access to a function f : G
f (g1) = f (g2) if and only if g1 and g2 belong to the same coset of H.
Is there then an
eﬃcient quantum algorithm to determine H? If G is abelian, the work of Simon [220], Shor
[219], and Kitaev [152] implies an aﬃrmative answer.
If G is nonabelian, though, eﬃcient
quantum algorithms are known only for special cases [105, 138]. An O (polylog (n))-query
algorithm for the collision problem would yield a polynomial-time algorithm to distinguish
= 2, which does not exploit the group structure at all. My result implies
H
H
|
|
that no such algorithm exists.

≤
N such that for all g1, g2 ∈

= 1 from

→

|

|

6⊂

Finally, the collision lower bound implies that there exists an oracle relative to
which SZK
BQP, where SZK is the class of problems having statistical zero-knowledge
proof protocols. For suppose that a veriﬁer V and prover P both have oracle access to
a sequence X = x1 . . . x2n, which is either one-to-one or two-to-one. To verify with zero
knowledge that X is one-to-one, V can repeatedly choose an i
and send xi to
P , whereupon P must send i back to V . Thus, using standard diagonalization techniques,
one can produce an oracle A such that SZKA

1, . . . , 2n

BQPA.

∈R {

}

6⊂

6.1.2 Information Erasure
m with m

Let f :
kinds of quantum oracle for f :

0, 1
}
{

0, 1
}

→ {

n

≥

n be a one-to-one function. Then we can consider two

(A) a standard oracle, one that maps

x

z
i |

|

⊕

f (x)

, or
i

x
|

z
i |

i

to

(B) an erasing oracle (as proposed by Kasheﬁ et al. [150]), which maps

eﬀect “erasing”

.
x
i
|

to

x
i
|

|

f (x)

, in
i

37

i

to

y
|

f −

1 (y)

Intuitively erasing oracles seem at least as strong as standard ones, though it is
not clear how to simulate the latter with the former without also having access to an oracle
that maps
. The question that concerns us here is whether erasing oracles
are more useful than standard ones for some problems. One-way functions provide a clue:
(cid:12)
(cid:12)
can be computed eﬃciently, but if
if f is one-way, then (by assumption)
f (x)
f (x)
x
i
|
, and hence f could be
f (x)
then so could
x
could be computed eﬃciently given
i
i
inverted. But can we ﬁnd, for some problem, an exponential gap between query complexity
given a standard oracle and query complexity given an erasing oracle?

given

x
i
|

i |

(cid:11)

i

|

|

|

In Section 6.5 I extend the collision lower bound to show an aﬃrmative answer.
Deﬁne the set comparison problem of size n, or SetCompn, as follows. We are given as input
.
two sequences, X = x1 . . . xn and Y = y1 . . . yn, such that for each i, xi, yi ∈ {
}
, and produces as output
A query has the form (b, i), where b
0, 1
∈ {
}
(0, xi) if b = 0 and (1, yi) if b = 1. Sequences X and Y are both one-to-one; that is, xi 6
= xj
= j. We are furthermore guaranteed that either
and yi 6
(1) X and Y are equal as sets (that is,

= yj for all i

1, . . . , 2n

1, . . . , n

and i

) or

∈ {

=

}

x1, . . . , xn}

{

y1, . . . , yn}
{

(2) X and Y are far as sets (that is,

1.1n).

y1, . . . , yn}| ≥

x1, . . . , xn} ∪ {
|{
As before the problem is to decide whether (1) or (2) holds.
This problem can be solved with high probability in a constant number of queries
using an erasing oracle, by using a trick similar to that of Watrous [239] for verifying group
non-membership. First, using the oracle, we prepare the uniform superposition

1
√2n

1,...,n
Xi
∈{

}

xii
0
(
i |
|

+

) .
yii
1
i |
|

We then apply a Hadamard gate to the ﬁrst register, and ﬁnally we measure the ﬁrst register.
) pair
If X and Y are equal as sets, then interference occurs between every (
z
1
i
i |
z
with certainty. But if X and Y are far as sets, then basis states
and we observe
b
0
i
i |
i
|
1
have probability weight at least 1/10, and hence we observe
1
with no matching
i
|
|
with probability at least 1/20.

z
0
i |
|

z
i |

−

b

i

i

,

|

|

In Section 6.5 I sketch a proof that Q2 (SetCompn) = Ω

; that is, no eﬃcient
quantum algorithm using a standard oracle exists for this problem. Recently, Midrijanis
not merely for the set comparison problem,
[176] gave a lower bound of Ω
but for the set equality problem (where we are promised that X and Y are either equal or
(cid:17)
disjoint).

(n/ log n)1/5
(cid:16)

(cid:0)

(cid:1)

n1/7

6.2 Preliminaries

Let A be a quantum query algorithm as deﬁned in Section 5.1. A basis state of A is written
, where xi is exclusive-OR’ed into some
i, z
i, z
|
|
speciﬁed location of z. Between queries, the algorithm can perform any unitary operation

. Then a query replaces each

xii

i, z

by

⊕

i

i

|

6
38

Let α(t)

that does not depend on the input. Let T be the total number of queries. Also, assume
for simplicity that all amplitudes are real; this restriction is without loss of generality [55].
after t queries when the input
i,z (X) be the amplitude of basis state
= h. Let P (X) be the
is X. Also, let ∆ (xi, h) = 1 if xi = h, and ∆ (xi, h) = 0 if xi 6
probability that A returns “two-to-one” when the input is X. Then we obtain a simple
variant of a lemma due to Beals et al. [45].

i, z
|

i

Lemma 1 P (X) is a multilinear polynomial of degree at most 2T over the ∆ (xi, h).

Proof. We show, by induction on t, that for all basis states

, the amplitude
α(t)
i,z (X) is a multilinear polynomial of degree at most t over the ∆ (xi, h). Since P (X) is
a sum of squares of α(t)

i,z’s, the lemma follows.

i, z
|

i

The base case (t = 0) holds since, before making any queries, each α(t)
i,z is a degree-
0 polynomial over the ∆ (xi, h). A unitary transformation on the algorithm part replaces
each α(t)
i,z by a linear combination of α(t)
i,z’s, and hence cannot increase the degree. Suppose
the lemma holds prior to the tth query. Then

α(t+1)
i,z

(X) =

α(t)
i,z
⊕

h (X) ∆ (xi, h) ,

h
X1
≤
≤

n

and we are done.

6.3 Reduction to Bivariate Polynomial

Call the point (g, N )

∈ ℜ

2 an (n, T )-quasilattice point if and only if

(1) g and N are integers, with g dividing N ,

(2) 1

g

≤

≤

√n,

(3) n

N

n + n/ (10T ), and

≤

≤
(4) if g = 1 then N = n.

{

1, . . . , n

For quasilattice point (g, N ), deﬁne

Dn (g, N ) to be the uniform distribution over
and range a subset
}
Dn (g, N ), we ﬁrst choose a set S

. More precisely: to draw an X from
}
with
S
|
|
xN from

all size-n subfunctions of g-to-1 functions having domain
of
1, . . . , n
{
X =
i
1
≤
b
b
b
chosen from

}
x1 . . .
n.
Let P (g, N ) be the probability that algorithm A returns z = 2 when the input is

⊆
n uniformly at random. We then choose a g-to-1 function
xi for each

to S uniformly at random. Finally we let xi =

≤
1, . . . , N
{

1, . . . , N

= N/g

≤

b

}

{

Dn (g, N ):

P (g, N ) =

EX
n(g,N )

∈D

X

[P (X)] .

We then have the following surprising characterization:

39

Lemma 2 For all suﬃciently large n and if T
q (g, N ) of degree at most 2T such that if (g, N ) is a quasilattice point, then

√n/3, there exists a bivariate polynomial

≤

P (g, N )
|

−

q (g, N )

|

< 0.182

(where the constant 0.182 can be made arbitrarily small by adjusting parameters).

Proof. Let I be a product of ∆ (xi, h) variables, with degree r (I), and let I (X)

0, 1
}
{

be I evaluated on input X. Then deﬁne

∈

γ (I, g, N ) =

EX
n(g,N )

∈D

X

[I (X)]

to be the probability that monomial I evaluates to 1 when the input is drawn from
Then by Lemma 1, P (X) is a polynomial of degree at most 2T over X, so

Dn (g, N ).

P (g, N ) =

=

=

X

X

EX
n(g,N )

∈D

[P (X)]

EX
n(g,N ) 

∈D

βI I (X)





2t

XI:r(I)
≤
βI γ (I, g, N )



for some coeﬃcients βI .

XI:r(I)
≤

2T

We now calculate γ (I, g, N ). Assume without loss of generality that for all

∆ (xi, h1) , ∆ (xj, h2)

∈

I, either i

= j or h1 = h2, since otherwise γ (I, g, N ) = 0.
Deﬁne the “range” Z (I) of I to be the set of all h such that ∆ (xi, h)
Z (I)
|

; then we write Z (I) =
|

I. Let
. Clearly γ (I, g, N ) = 0 unless

∈

S, where S is the range of

z1, . . . , zw(I)
X. By assumption,

(cid:8)

(cid:9)

w (I) =
Z (I)

∈

N
b
g ≥

n
√n ≥

2T

≥

r (I)

n
N/g

(cid:18)

(cid:19)

and, of these, the number that contain Z is

so the number of possible S is

w (I)

n
−
N/g

.

w (I)
Then, conditioned on Z

(cid:19)

−

(cid:18)
S, what is the probability that γ (I, g, N ) = 1? The
total number of g-to-1 functions with domain size N is N !/ (g!)N/g , since we can permute
the N function values arbitrarily, but must not count permutations that act only within
the N/g constant-value blocks of size g.

∈

1

j

≤

≤

Among these functions, how many satisfy γ (I, g, N ) = 1? Suppose that, for each
w (I), there are rj (I) distinct i such that ∆ (xi, zj)

I. Clearly

∈
+ rw(I) (I) = r (I) .

r1 (I) +

· · ·

6
Then we can permute the (N
r (I))! function values outside of I arbitrarily, but must not
count permutations that act only within the N/g constant-value blocks, which have size
either g or g

ri (I) for some i. So the number of functions for which γ (I, g, N ) = 1 is

−

−

40

(N

(g!)N/g

−

w(I)

Putting it all together,

r (I))!
w(I)

−

(g

i=1

Y

.

ri (I))!

−

γ (I, g, N ) = (cid:18)

n
−
N/g

w (I)

w (I)

−
n
N/g

(N

=

−

(cid:18)

(cid:19)
r (I))! (n
−
N !n! (N/g

(cid:19)

·

(g!)N/g

r (I))! (g!)N/g
w(I)

(N
−
w(I) N !

−

(g

ri (I))!

Y

i=1

−
(g!)w(I)

w (I))! (N/g)!

w (I))!

·

w(I)

i=1

(g

−

(N

=

r (I))!

−
N !

(n

−

w (I))!
n!

·

=

(N
−
N ! (n

2T )!n!
2T )!

−

qn,T,I (g, N )

w(I)

−

1

Y
N
g −

Yi=0 (cid:18)

ri (I))!

−
w(I)

i
(cid:19)

Yi=1

g




ri(I)

−

1

Yj=1

(g

j)

−





where

e

qn,T,I (g, N ) =

(n

−

w (I))! (n
(n!)2

−

e

is a bivariate polynomial of total degree at most

2T )!

2T

1

−

·

Yi=r(I)

(N

i)

−

w(I)

−

1

Yi=0

(N

gi)

−

w(I)

ri(I)

−

Yi=1

Yj=1

1

(g

j)

−

(2T

−

r (I)) + w (I) + (r (I)

w (I)) = 2T.

−

(Note that in the case ri (I) > g for some i, this polynomial evaluates to 0, which is what
it ought to do.) Hence

where

Clearly

P (g, N ) =

βI γ (I, g, N )

XI:r(I)
≤
(N
−
N ! (n

2T
2T )!n!
2T )!

−

=

q (g, N )

q (g, N ) =

βI

qn,T,I (g, N ) .

XI:r(I)
≤

2T

e

(N
−
N ! (n

2T )!n!

2T )! ≤

−

1.

41

(cid:27)

Since N

≤

n + n/ (10T ) and T

√n/3, we also have

(N
−
N ! (n

≤
2T )!n!

2T )! ≥

−

≥

2T

(cid:19)

n

n
N

(cid:18)
exp

−
−

2T + 1
2T + 1
1
5

n

−

−

(cid:26)
0.818

P (g, N )

q (g, N )

|

≤

−

1,

≤
< 0.182

(2T + 1) /n

≥
for all suﬃciently large n. Thus, since 0

P (g, N )
|

and we are done.

6.4 Lower Bound

We have seen that, if a quantum algorithm for the collision problem makes few queries,
then its acceptance probability can be approximated by a low-degree bivariate polynomial.
This section completes the lower bound proof by showing that no such polynomial exists.
To do so, it generalizes an approximation theory result due to Rivlin and Cheney [204] and
(independently) Ehlich and Zeller [104]. That result was applied to query complexity by
Nisan and Szegedy [184] and later by Beals et al. [45].

Theorem 3 Q2 (Coln) = Ω

n1/5

.

(cid:1)
Proof. Let g have range 1
in the rectangular region R = [1, G]
from Lemma 2, deﬁne

(cid:0)

×

g

G. Then the quasilattice points (g, N ) all lie
≤
[n, n + n/ (10T )]. Recalling the polynomial q (g, N )

≤

Suppose without loss of generality that we require

d (q) = max
∈

(g,N )

R

max

(cid:18)

∂q
∂g

,

n
10T (G

1) ·

−

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂q
∂N

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:12)
(cid:27)(cid:19)
(cid:12)
(cid:12)
(cid:12)

P (1, n)

1
10

≤

and P (2, n)

9
10

≥

(that is, algorithm A distinguishes 1-to-1 from 2-to-1 functions with error probability at
most 1/10). Then, since

P (g, N )
|
by elementary calculus we have

−

q (g, N )

|

< 0.182

d (q)

≥

max
2
g
1
≤
≤

∂q
∂g

> 0.8

−

2 (0.182) = 0.436.

An inequality due to Markov (see [82, 184]) states that, for a univariate polynomial p, if
b1 ≤

b2 for all a1 ≤

a2, then

p (x)

≤

≤

x

max
x
≤
≤

a[1]

a[2] (cid:12)
(cid:12)
(cid:12)
(cid:12)

b2 −
a2 −

b1
a1

≤

deg (p)2 .

dp (x)
dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)

R, there exists a quasilattice point (g, N ) for which

42

Clearly for every point

g,

N

(cid:16)

b

b

∈

(cid:17)
g
|

g

−

| ≤

1 and

N

N

−

G.

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

g

—or, in the special case

g = 1, take g = 2, since there is only one quasilattice
For take g =
point with g = 1. Furthermore, since P (g, N ) represents an acceptance probability at such
a point, we have

b

⌉

⌈

b

b

b

Observe that for all

g,

N

(cid:16)
10T G (G
b

n

(cid:17)
b
1)
−

0.182

−

−

(cid:18)

0.182 < q (g, N ) < 1.182.

−

R,

∈

+ 1

d (q) < q

g,

N

< 1.182 +

(cid:19)

(cid:16)

(cid:17)

b

b

10T G (G

n

(cid:18)

1)

−

+ 1

d (q) .

(cid:19)

, and note that the maximum-
1) d (q) /n in the N

For consider a quasilattice point close to

g,

N

magnitude derivative is at most d (q) in the g direction and 10T (G
direction.

(cid:17)

(cid:16)

b

b

−

Let (g∗, N ∗) be a point in R at which the weighted maximum-magnitude derivative
d (q) is attained. Suppose ﬁrst that the maximum is attained in the g direction. Then
q (g, N ∗) (with N ∗ constant) is a univariate polynomial with

for some 1

g

≤

≤

G. So

> 0.436

dq (g, N ∗)
dg

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2T

≥

deg (q (g, N ∗))

d (q) (G

1)

−

≥ s

1.364 + 2d (q) (1 + 10T G (G

1) /n)

−

= Ω

min

√G,

(cid:18)

(cid:26)

r

n
T G

.

(cid:27)(cid:19)

Similarly, suppose the maximum d (q) is attained in the N direction.

Then

q (g∗, N ) (with g∗ constant) is a univariate polynomial with

for some n

N

≤

≤

n + n/ (10T ). So

dq (g∗, N )
dN

0.436T (G

n

1)

−

>

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2T

≥ s

(10T (G

1) /n) d (q) n/ (10T )

1.364 + 2d (q) (1 + 10T G (G

−

1) /n)

−

≥

Ω

min

√G,

(cid:18)

(cid:26)

r

n
T G

.

(cid:27)(cid:19)

One can show that the lower bound on T is optimized when we take G = n2/5

√n. Then

43

≤

√n
√T n1/5

,

(cid:27)(cid:19)

T = Ω

min

n1/5,

T = Ω

(cid:18)
n1/5
(cid:16)

(cid:26)

(cid:17)

and we are done.

6.5 Set Comparison

Here I sketch a proof that Q2 (SetCompn) = Ω
son problem of size n as deﬁned in Section 6.1.2.
(cid:0)

(cid:1)

n1/7

, where SetCompn is the set compari-

The idea is the following. We need a distribution of inputs with a parameter g,
such that the inputs are one-to-one when g = 1 or g = 2—since otherwise the problem of
distinguishing g = 1 from g = 2 would be ill-deﬁned for erasing oracles. On the other
hand, the inputs must not be one-to-one for all g > 2—since otherwise the lower bound for
standard oracles would apply also to erasing oracles, and we would not obtain a separation
between the two. Finally, the acceptance probability must be close to a polynomial in g.

The solution is to consider κ (g)-to-one inputs, where

κ (g) = 4g2

12g + 9.

−

is a quadratic with κ (1) = κ (2) = 1. The total range of the inputs (on sequences X and
Y combined) has size roughly n/g; thus, we can tell the g = 1 inputs apart from the g = 2
inputs using an erasing oracle, even though κ (g) is the same for both. The disadvantage is
that, because κ (g) increases quadratically rather than linearly in g, the quasilattice points
to
become sparse more quickly. That is what weakens the lower bound from Ω
Ω
. Note that, using the ideas of Shi [218], one can improve my lower bound on
Q2 (SetCompn) to Ω

n1/7

n1/5

n1/6

(cid:1)

(cid:0)

.

3 an (n, T )-super-quasilattice point if and only if

(cid:0)

(cid:1)

Call (g, N, M )

(cid:0)

(1) g is an integer in

(cid:1)
∈ ℜ
1, n1/3

,

(cid:3)

(2) N and M are integers in [n, n (1 + 1/ (100T ))],

(cid:2)

(3) g divides N ,

(4) if g = 1 then N = n,

(5) κ (g) divides M , and

(6) if g = 2 then M = n.

from distribution
= 2N/g
S
|

For super-quasilattice point (g, N, M ), we draw input (X, Y ) = (x1 . . . xn, y1 . . . yn)
with
}
S with

1, . . . , 2n
2n uniformly at random. We then choose two sets SX, SY ⊆

Ln (g, N, M ) as follows. We ﬁrst choose a set S

⊆ {

≤

|

44

=

= M/κ (g)

SX|
SX|
|
|
κ (g)-1 functions
uniformly at random and independently. Finally we let xi =
1
b

, uniformly at random and independently. Next we choose
S
≤ |
|
SY
Y =
xN :
x1 . . .
} →
yi for each

{
xi and yi =

1, . . . , M

1, . . . , M

SX and

y1 . . .

yN :

X =

} →

b

b

{

i

≤

≤

b
n.
Deﬁne sets XS =
N = M = n; then by Chernoﬀ bounds,

x1, . . . , xn}

b

b

{

and YS =

y1, . . . , yn}

.
b

{

Suppose g = 1 and
b

(X,Y )

n(1,n,n)

Pr
∈L

XS ∪
[
|

YS|

< 1.1n]

≤

2e−

n/10.

Thus, if algorithm A can distinguish
YS|
XS ∪
at least 9/10, then it can distinguish (X, Y )
probability at least 9/10
equivalent lower bound for the former.

2e−

−

|

1.1n with probability
= n from
XS ∪
|
∈ Ln (2, n, n) with
∈ Ln (1, n, n) from (X, Y )
n/10. So a lower bound for the latter problem implies an

YS| ≥

Deﬁne P (X, Y ) to be the probability that the algorithm returns that X and Y

are far on input (X, Y ), and let

P (g, N, M ) =

We then have

(X,Y )

n(g,N,M )

EX
∈L

[P (X, Y )] .

Lemma 4 For all suﬃciently large n and if T
q (g, N, M ) of degree at most 8T such that if (g, N, M ) is a super-quasilattice point, then

n1/3/8, there exists a trivariate polynomial

≤

for some constant 0 < ε < 1/2.

P (g, N, M )
|

−

q (g, N, M )

< ε

|

Proof Sketch. By analogy to Lemma 1, P (X, Y ) is a multilinear polynomial
of degree at most 2T over variables of the form ∆ (xi, h) and ∆ (yi, h). Let I (X, Y ) =
IX (X) IY (Y ) where IX is a product of rX (I) distinct ∆ (xi, h) variables and IY is a product
of rY (I) distinct ∆ (yi, h) variables. Let r (I) = rX (I) + rY (I). Deﬁne

γ (I, g, N, M ) =

(X,Y )

n(g,N,M )

EX
∈L

[I (X, Y )] ;

then

P (g, N, M ) =

βI γ (I, g, N, M )

XI:r(I)
≤
for some coeﬃcients βI . We now calculate γ (I, g, N, M ). As before we assume there are
= h2. Let ZX (I) be the range of IX
no pairs of variables ∆ (xi, h1) , ∆ (xi, h2)
and let ZY (I) be the range of IY . Then let Z (I) = ZX (I)
,
ZX (I)
|
|
wY (I) =

I with h1 6
. By assumption
Z (I)
|

, and w (I) =
ZY (I)
|
|

ZY (I). Let wX (I) =

∪

∈

2T

|

N
g ≥

M
κ (g) ≥

1
4

n1/3

2T

≥

45

w (I)

so

Pr [Z (I)

S] = (cid:18)

⊆

2n
−
2N/g

w (I)

(cid:19)

.

−
2n
2N/g

(cid:18)

(cid:19)
S and ZY (I)

The probabilities that ZX (I)
be calculated similarly.

⊆

SX given Z (I)

⊆

SY given Z (I)

S can

⊆

⊆

so that

Then

Let rX,1 (I) , . . . , rX,w[X](I) (I) be the multiplicities of the range elements in ZX (I),

rX,1 (I) +

· · ·

+ rX,w[X](I) (I) = rX (I) .

Pr [IX (X)

ZX (I)

|

⊆

SX] =

(M

−

rX (I))!
M !

w[X](I)

r[X,i](I)

−

Yi=1

Yj=0

1

(κ (g)

j)

−

and similarly for Pr [IY (Y )

ZY (I)

SY ].

|

⊆

Putting it all together and manipulating, we obtain (analogously to Lemma 1)

that

where
Thus

γ (I, g, N, M )

qn,T,I (g, N, M )

≈

qn,T,I (g, N, M ) is a trivariate polynomial in (g, N, M ) of total degree at most 8T .

e

e

P (g, N, M )

q (g, N, M )

≈

where q (g, N, M ) is a polynomial of total degree at most 8T .
approximates P to within a constant is analogous to that of Lemma 2.

The argument that q

The remainder of the lower bound argument follows the lines of Theorem 3.

Theorem 5 Q2 (SetCompn) = Ω

n1/7

.

Proof Sketch. Let g

points (g, N, M ) all lie in R = [1, G]

(cid:0)
[1, G] for some G
∈

(cid:1)
[n, n + n/ (100T )]2. Deﬁne d(q) to be

n1/3. Then the super-quasilattice

≤

max
(g,N,M )
∈

R

max

×
∂q
∂g

,

n/100T
1)
(G

∂q
∂N

,

n/100T
1)
(G

∂q
∂M

.

Then d (q)

(cid:18)

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
δ for some constant δ > 0, by Lemma 4.
(cid:12)
(cid:12)
M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
R, there exists a super-quasilattice point (g, N, M )

(cid:12)
(cid:27)(cid:19)
(cid:12)
(cid:12)
(cid:12)

N ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

g,

−

−

For every point

≥

such that
g
−
−
b
deviate from [0, 1] by at most

| ≤

1,

g

|

(cid:16)
N

b

(cid:12)
(cid:12)
(cid:12)

N
b

b

(cid:17)
c
≤
(cid:12)
(cid:12)
(cid:12)

∈
G, and

T G3
n

O

(cid:18)(cid:18)

κ (G) . Hence, q

g,

N ,

M

can

(cid:16)

b

b

(cid:17)

c

M

−

(cid:12)
(cid:12)
(cid:12)

≤

M

(cid:12)
(cid:12)
c
(cid:12)

+ 1

d (q)

.

(cid:19)

(cid:19)

Let (g∗, N ∗, M ∗) be a point in R at which d (q) is attained.

Suppose d (q) is
attained in the g direction; the cases of the N and M directions are analogous. Then
q (g, N ∗, M ∗) is a univariate polynomial in g, and

46

8T

≥

deg (q (g, N ∗, M ∗))

= Ω

min

√G,

(cid:18)

(cid:26)

r

n
T G2

.

(cid:27)(cid:19)

One can show that the bound is optimized when we take G = n2/7

n1/3. Then

≤

√n
√T n2/7

,

(cid:27)(cid:19)

T = Ω

min

n1/7,

T = Ω

(cid:18)
n1/7
(cid:16)

(cid:26)
.

(cid:17)

6.6 Open Problems

(cid:0)

(cid:1)

n1/3

In my original paper on the collision problem, I listed four open problems: improving the
; showing any nontrivial quantum lower bound for the set
collision lower bound to Ω
equality problem; proving a time-space tradeoﬀ lower bound for the collision problem; and
deciding whether quantum query complexity and degree as a real polynomial are always
asymptotically equal. Happily, three of these problems have since been resolved [161, 176,
28], but the time-space tradeoﬀ remains wide open. We would like to say (for example)
that if a quantum computer is restricted to using O (log n) qubits, then it needs Θ (√n)
queries for the collision problem, ordinary Grover search being the best possible algorithm.
Currently, we cannot show such a result for any problem with Boolean output, only for
problems such as sorting with a large non-Boolean output [156].

Another problem is to give an oracle relative to which SZK

QMA, where QMA
6⊂
is Quantum Merlin Arthur as deﬁned in [239].
In other words, show that if a function is
one-to-one rather than two-to-one, then this fact cannot be veriﬁed using a small number
of quantum queries, even with the help of a succinct quantum proof.

Finally, is it the case that for all (partial or total) functions f that are invariant

under permutation symmetry, R2 (f ) and Q2 (f ) are polynomially related?

47

Chapter 7

Local Search

This chapter deals with the following problem.
Local Search. Given an undirected graph G = (V, E) and function f : V

N,
f (w) for all neighbors w

→

ﬁnd a local minimum of f —that is, a vertex v such that f (v)
of v.

≤

We will be interested in the number of queries that an algorithm needs to solve
this problem, where a query just returns f (v) given v. We will consider deterministic,
randomized, and quantum algorithms. Section 7.1 motivates the problem theoretically and
practically; this section explains the results.

First, though, we need some simple observations.

If G is the complete graph of
with
size N , then clearly Ω (N ) queries are needed to ﬁnd a local minimum (or Ω
a quantum computer). At the other extreme, if G is a line of length N , then even a
deterministic algorithm can ﬁnd a local minimum in O (log N ) queries, using binary search:
query the middle two vertices, v and w.
f (w), then search the line of length
2) /2 connected to v; otherwise search the line connected to w. Continue recursively
(N
in this manner until a local minimum is found.

If f (v)

√N

≤

−

(cid:17)

(cid:16)

{

0, 1
}

So the interesting case is when G is a graph of ‘intermediate’ connectedness: for
n, with two vertices adjacent if and only if they have
example, the Boolean hypercube
Hamming distance 1. For this graph, Llewellyn, Tovey, and Trick [169] showed a Ω (2n/√n)
lower bound on the number of queries needed by any deterministic algorithm, using a simple
adversary argument.
Intuitively, until the set of vertices queried so far comprises a vertex
cut (that is, splits the graph into two or more connected components), an adversary is free
to return a descending sequence of f -values: f (v1) = 2n for the ﬁrst vertex v1 queried by
the algorithm, f (v2) = 2n
1 for the second vertex queried, and so on. Moreover, once the
set of queried vertices does comprise a cut, the adversary can choose the largest connected
component of unqueried vertices, and restrict the problem recursively to that component.
So to lower-bound the deterministic query complexity, it suﬃces to lower-bound the size
of any cut that splits the graph into two reasonably large components.1 For the Boolean
hypercube, Llewellyn et al. showed that the best one can do is essentially to query all
Ω (2n/√n) vertices of Hamming weight n/2.

−

1Llewellyn et al. actually give a tight characterization of deterministic query complexity in terms of vertex

cuts.

48

n

D

D

D

D

→

0, 1
}

0, 1
}
{

. Taking

Llewellyn et al.’s argument fails completely in the case of randomized algorithms.
over functions
By Yao’s minimax principle, what we want here is a ﬁxed distribution
N, such that any deterministic algorithm needs many queries to ﬁnd a local
f :
minimum of f , with high probability if f is drawn from
to be uniform will
not do, since a local minimum of a uniform random function is easily found. However,
Aldous [24] had the idea of deﬁning
via a random walk, as follows. Choose a vertex
n uniformly at random; then perform an unbiased walk2 v0, v1, v2, . . . starting
v0 ∈ {
from v0. For each vertex v, set f (v) equal to the ﬁrst hitting time of the walk at v—
. Clearly any f produced in this way has a unique local
that is, f (v) = min
minimum at v0, since for all t > 0, if vertex vt is visited for the ﬁrst time at step t then
1). Using sophisticated random walk analysis, Aldous managed to show
f (vt) > f (vt
o(n) on the expected number of queries needed by any randomized
a lower bound of 2n/2
algorithm to ﬁnd v0.3
(As we will see in Section 7.2, this lower bound is close to tight.)
Intuitively, since a random walk on the hypercube mixes in O (n log n) steps, an algorithm
that has not queried a v with f (v) < 2n/2 has almost no useful information about where
the unique minimum v0 is, so its next query will just be a “stab in the dark.”

t : vt = v
{

However, Aldous’s result leaves several questions about Local Search unan-
swered. What if the graph G is a 3-D cube, on which a random walk does not mix very
rapidly? Can we still lower-bound the randomized query complexity of ﬁnding a local
minimum? More generally, what parameters of G make the problem hard or easy? Also,
what is the quantum query complexity of Local Search?

}

−

−

This chapter presents a new approach to Local Search, which I believe points the
way to a complete understanding of its complexity. The approach is based on Ambainis’s
quantum adversary method [27]. Surprisingly, the approach yields new and simpler lower
bounds for the problem’s classical randomized query complexity, in addition to quantum
lower bounds. Thus, along with recent work by Kerenidis and de Wolf [151] and by
Aharonov and Regev [22], the results of this chapter illustrate how quantum ideas can help
to resolve classical open problems.

The results are as follows. For the Boolean hypercube G =

n, I show that
queries to ﬁnd a local minimum on G, and any
any quantum algorithm needs Ω
o(n) lower bound of
queries (improving the 2n/2
randomized algorithm needs Ω
Aldous [24]). The proofs are elementary and do not require random walk analysis. By
comparison, the best known upper bounds are O
for a quantum algorithm and
O

If G is a d-dimensional grid of size N 1/d

for a randomized algorithm.

2n/4/n
2n/2/n2
(cid:0)

2n/3n1/6

2n/2√n

0, 1
}

(cid:1)
(cid:1)

{

(cid:0)

−

(cid:0)
· · · ×
Ω

N 1/d, where d
1/d/ log N
N 1/2

≥

×
3 is a constant, then I show that any quantum algorithm needs
queries to ﬁnd a local minimum on G, and any randomized algorithm

(cid:0)

(cid:1)

(cid:16)p
needs Ω
were previously known in this case.4

1/d/ log N

(cid:17)

−

queries. No nontrivial lower bounds (randomized or quantum)

(cid:0)
In a preprint discussing these results, I raised as my “most ambitious” conjecture
that the deterministic and quantum query complexities of local search are polynomially
related for every family of graphs. At the time, it was not even known whether deterministic

(cid:1)

2Actually, Aldous used a continuous-time random walk, so the functions would be from
3Independently and much later, Droste et al. [99] showed the weaker bound 2g(n) for any g (n) = o (n).
4A lower bound on deterministic query complexity was known for such graphs [168].

0, 1

n to R.

}

{

(cid:1)
−
N 1/2

49

and randomized query complexities were polynomially related, not even for simple examples
such as the 2-dimensional square grid. Subsequently Santha and Szegedy [211] spectacularly
resolved the conjecture, by showing that the quantum query complexity is always at least
the 19th root (!) of the deterministic complexity. On the other hand, in the speciﬁc case of
the hypercube, my lower bound is close to tight; Santha and Szegedy’s is not. Also, I give
randomized lower bounds that are quadratically better than my quantum lower bounds;
Santha and Szegedy give only quantum lower bounds.

In another recent development, Ambainis [25] has improved the Ω

quan-
tum lower bound for local search on the hypercube to 2n/3/nO(1), using a hybrid argument.
Note that Ambainis’s lower bound matches the upper bound up to a polynomial factor.

(cid:0)

(cid:1)

2n/4/n

The chapter is organized as follows. Section 7.1 motivates lower bounds on Local
Search, pointing out connections to simulated annealing, quantum adiabatic algorithms,
and the complexity class TFNP of total function problems. Section 7.2 deﬁnes notation and
reviews basic facts about Local Search, including upper bounds.
In Section 7.3 I give
an intuitive explanation of Ambainis’s quantum adversary method, then state and prove a
classical analogue of Ambainis’s main lower bound theorem. Section 7.4 introduces snakes,
a construction by which I apply the two adversary methods to Local Search.
I show
there that to prove lower bounds for any graph G, it suﬃces to upper-bound a combinatorial
parameter ε of a ‘snake distribution’ on G. Section 7.5 applies this framework to speciﬁc
examples of graphs: the Boolean hypercube in Section 7.5.1, and the d-dimensional grid in
Section 7.5.2.

7.1 Motivation

Local search is the most eﬀective weapon ever devised against hard optimization problems.
For many real applications, neither backtrack search, nor approximation algorithms, nor
even Grover’s algorithm can compare. Furthermore, along with quantum computing, local
search (broadly deﬁned) is one of the most interesting links between computer science and
Nature.
It is related to evolutionary biology via genetic algorithms, and to the physics of
materials via simulated annealing. Thus it is both practically and scientiﬁcally important
to understand its performance.

The conventional wisdom is that, although local search performs well in practice,
its central (indeed deﬁning) ﬂaw is a tendency to get stuck at local optima.
If this were
correct, one corollary would be that the reason local search performs so well is that the
It would thus be
problem it really solves—ﬁnding a local optimum—is intrinsically easy.
unnecessary to seek further explanations for its performance. Another corollary would be
that, for unimodal functions (which have no local optima besides the global optimum), the
global optimum would be easily found.

However, the conventional wisdom is false. The results of Llewellyn et al. [169]
and Aldous [24] show that even if f is unimodal, any classical algorithm that treats f as a
black box needs exponential time to ﬁnd the global minimum of f in general. My results
In my view, the practical upshot of these
extend this conclusion to quantum algorithms.
results is that they force us to confront the question: What is it about ‘real-world’ problems
that makes it easy to ﬁnd a local optimum? That is, why do exponentially long chains

50

of descending values, such as those used for lower bounds, almost never occur in practice,
even in functions with large range sizes? One possibility is that the functions that occur in
practice look “globally” like random functions, but I do not know whether that is true in
any meaningful sense.

The results of this chapter are also relevant for physics. Many physical sys-
tems, including folding proteins and networks of springs and pulleys, can be understood as
performing ‘local search’ through an energy landscape to reach a locally-minimal energy
conﬁguration. A key question is, how long will the system take to reach its ground state
(that is, a globally-minimal conﬁguration)? Of course, if there are local optima, the sys-
tem might never reach its ground state, just as a rock in a mountain crevice does not roll
to the bottom by going up ﬁrst. But what if the energy landscape is unimodal? And
moreover, what if the physical system is quantum? My results show that, for certain en-
ergy landscapes, even a quantum system would take exponential time to reach its ground
state, regardless of what external Hamiltonian is applied to “drive” it. So in particular,
the quantum adiabatic algorithm proposed by Farhi et al. [106], which can be seen as a
quantum analogue of simulated annealing, needs exponential time to ﬁnd a local minimum
in the worst case.

n

Finally, this chapter’s results have implications for so-called total function problems
in complexity theory. Megiddo and Papadimitriou [174] deﬁned a complexity class TFNP,
consisting (informally) of those NP search problems for which a solution always exists.
1 as a Boolean circuit,
For example, we might be given a function f :
and asked to ﬁnd any distinct x, y pair such that f (x) = f (y). This particular problem
belongs to a subclass of TFNP called PPP (Polynomial Pigeonhole Principle). Notice that
no promise is involved: the combinatorial nature of the problem itself forces a solution to
exist, even if we have no idea how to ﬁnd it.
In a recent talk, Papadimitriou [187] asked
broadly whether such ‘nonconstructive existence problems’ might be good candidates for
In the case of PPP problems like the one above, the collision
eﬃcient quantum algorithms.
lower bound of Chapter 6 implies a negative answer in the black-box setting. For other
subclasses of TFNP, such as PODN (Polynomial Odd-Degree Node), a quantum black-box
lower bound follows easily from the optimality of Grover’s search algorithm.

0, 1
}
{

0, 1
}

→ {

−

n

However, there is one important subclass of TFNP for which no quantum lower
bound was previously known. This is PLS (Polynomial Local Search), deﬁned by Johnson,
Papadimitriou, and Yannakakis [149] as a class of optimization problems whose cost function
f and neighborhood function η (that is, the set of neighbors of a given point) are both
computable in polynomial time.5 Given such a problem, the task is to output any local
η (v). The
minimum of the cost function: that is, a v such that f (v)
lower bound of Llewellyn et al. [169] yields an oracle A relative to which FPA
= PLSA,
by a standard diagonalization argument along the lines of Baker, Gill, and Solovay [41].
FBPP,
Likewise, the lower bound of Aldous [24] yields an oracle relative to which PLS
where FBPP is simply the function version of BPP. The results of this chapter yield the
ﬁrst oracle relative to which PLS
In light of this oracle separation, I raise an

f (w) for all w

FBQP.

≤

6⊂

∈

5Some authors require only the minimum neighbor of a given point to be computable in polynomial
time, which does not seem like the “right” idealization to me.
In any case, for lower bound purposes we
always assume the algorithm knows the whole neighborhood structure in advance, and does not need to
make queries to learn about it.

6⊂

6
admittedly vague question: is there a nontrivial “combinatorial” subclass of TFNP that we
can show is contained in FBQP?

7.2 Preliminaries

51

∈

≤

→
V such that f (v)

In the Local Search problem, we are given an undirected graph G = (V, E) with N =
,
|
N. The goal is to ﬁnd any local minimum of f ,
and oracle access to a function f : V
deﬁned as a vertex v
f (w) for all neighbors w of v. Clearly such a
local minimum exists. We want to ﬁnd one using as few queries as possible, where a query
returns f (v) given v. Queries can be adaptive; that is, can depend on the outcomes of
previous queries. We assume G is known in advance, so that only f needs to be queried.
Since we care only about query complexity, not computation time, there is no diﬃculty in
dealing with an inﬁnite range for f —though for lower bound purposes, it will turn out that
suﬃces. I do not know of any case where a range larger than this
a range of size O
|
makes the Local Search problem harder, but I also do not know of a general reduction
(cid:17)
from large to small range.

(cid:16)p

V
|

V

|

∞

The model of query algorithms is the standard one. Given a graph G, the
deterministic query complexity of Local Search on G, which we denote DLS (G), is
minΓ maxf T (Γ, f, G) where the minimum ranges over all deterministic algorithms Γ, the
maximum ranges over all f , and T (Γ, f, G) is the number of queries made to f by Γ before
it halts and outputs a local minimum of f (or
if Γ fails to do so). The randomized query
complexity RLS (G) is deﬁned similarly, except that now the algorithm has access to an
inﬁnite random string R, and must only output a local minimum with probability at least
2/3 over R. For simplicity, one can assume that the number of queries T is the same for
all R; clearly this assumption changes the complexity by at most a constant factor.
v,z,s αv,z,s |

,
i
where v is the label of a vertex in G, and z and s are strings representing the answer register
P
2 =
and workspace respectively. The αv,z,s’s are complex amplitudes satisfying
1. Starting from an arbitrary (ﬁxed) initial state, the algorithm proceeds by an alternating
sequence of queries and algorithm steps. A query maps each
,
i
i
⊕
denotes bitwise exclusive-OR. An algorithm step multiplies the vector of αv,z,s’s
where
Mf denote the set of
by an arbitrary unitary matrix that does not depend on f . Letting
2
3 . Then the
αv,z,s|
local minima of f , the algorithm succeeds if at the end
bounded-error quantum query complexity, or QLS (G), is deﬁned as the minimum number
of queries used by a quantum algorithm that succeeds on every f .

In the quantum model, an algorithm’s state has the form

αv,z,s|
f (v) , s

v, z, s
|

v,z,s |

∈Mf |

v, z, s

P
|

v,z,s : v

v, z

P

to

⊕

≥

2

It is immediate that QLS (G)

RLS (G)

DLS (G)

N . Also, letting δ be the

maximum degree of G, we have the following trivial lower bound.

≤

≤

≤

Proposition 6 RLS (G) = Ω (δ) and QLS (G) = Ω

√δ

.

Proof. Let v be a vertex of G with degree δ. Choose a neighbor w of v uniformly
Let f (v) = 2, and f (u) = 3 for all neighbors u of v
at random, and let f (w) = 1.
other than w. Let S be the neighbor set of v (including v itself); then for all x /
S,
∈
let f (x) = 3 + ∆ (x, S) where ∆ (x, S) is the minimum distance from x to a vertex in S.

(cid:16)

(cid:17)

Clearly f has a unique local minimum at w. However, ﬁnding y requires exhaustive search
√δ
among the δ neighbors of v, which takes Ω

quantum queries by Bennett et al. [51].

52

(cid:17)

(cid:16)

A corollary of Proposition 6 is that classically, zero-error randomized query com-
plexity is equivalent to bounded-error up to a constant factor. For given a candidate local
minimum v, one can check using O (δ) queries that v is indeed a local minimum. Since
Ω (δ) queries are needed anyway, this veriﬁcation step does not aﬀect the overall complexity.
As pointed out by Aldous [24], a classical randomized algorithm can ﬁnd a local
queries. The algorithm just queries √N δ
√N δ
minimum of f with high probability in O
vertices uniformly at random, and lets v0 be a queried vertex for which f (v) is minimal.
It then follows v0 to a local minimum by steepest descent. That is, for t = 0, 1, 2, . . ., it
queries all neighbors of vt, halts if vt is a local minimum, and otherwise sets vt+1 to be the
neighbor w of vt for which f (w) is minimal (breaking ties by lexicographic ordering). A
similar idea yields an improved quantum upper bound.

(cid:17)

(cid:16)

Proposition 7 For any G, QLS (G) = O

N 1/3δ1/6

.

(cid:0)

(cid:1)

Proof. The algorithm ﬁrst chooses N 2/3δ1/3 vertices of G uniformly at random,
then uses Grover search to ﬁnd a chosen vertex v0 for which f (v) is minimal. By a result of
D¨urr and Høyer [102], this can be done with high probability in O
queries. Next,
for t = 0, 1, 2, . . ., the algorithm performs Grover search over all neighbors of vt, looking
for a neighbor w such that f (w) < f (vt).
If it ﬁnds such a w, then it sets vt+1 := w and
continues to the next iteration. Otherwise, it repeats the Grover search log (N/δ) times
before ﬁnally giving up and returning vt as a claimed local minimum.

N 1/3δ1/6

(cid:1)

(cid:0)

The expected number of u such that f (u) < f (v0) is at most N/

=
(N/δ)1/3. Since f (vt+1) < f (vt) for all t, clearly the number of such u provides an upper
bound on t. Furthermore, assuming there exists a w such that f (w) < f (vt), the expected
number of repetitions of Grover’s algorithm until such a w is found is O (1). Since each
√δ
queries, by linearity of expectation the total expected number of
repetition takes O
queries used by the algorithm is therefore

(cid:1)

(cid:0)

N 2/3δ1/3

(cid:16)

(cid:17)

O

N 1/3δ1/6 + (N/δ)1/3 √δ + log (N/δ) √δ

(cid:16)

(cid:17)

N 1/3δ1/6

(cid:0)

or O
. To see that the algorithm ﬁnds a local minimum with high probability,
observe that for each t, the probability of not ﬁnding a w such that f (w) < f (vt), given
(cid:1)
that one exists, is at most c−
So by
the union bound, the probability that the algorithm returns a ‘false positive’ is at most
(N/δ)1/3

(δ/N )1/3 /10 for a suitable constant c.

(δ/N )1/3 /10 = 1/10.

log(N/δ)

≤

·

7.3 Relational Adversary Method

There are essentially two known methods for proving lower bounds on quantum query
complexity: the polynomial method of Beals et al. [45], and the quantum adversary method

of Ambainis [27].6 For a few problems, such as the collision problem [2, 218], the polynomial
method succeeded where the adversary method failed. However, for problems that lack
permutation symmetry (such as Local Search), the adversary method has proven more
eﬀective.7

53

How could a quantum lower bound method possibly be applied classically? When
proving randomized lower bounds, the tendency is to attack “bare-handed”: ﬁx a distri-
bution over inputs, and let x1, . . . , xt be the locations queried so far by the algorithm.
Show that for small t, the posterior distribution over inputs, conditioned on x1, . . . , xt, is
still ‘hard’ with high probability—so that the algorithm knows almost nothing even about
which location xt+1 to query next. This is essentially the approach taken by Aldous [24]
o(n) lower bound on RLS (
to prove a 2n/2
0, 1
}
{

n).
In the quantum case, however, it is unclear how to specify what an algorithm
‘knows’ after a given number of queries. So we are almost forced to step back, and identify
general combinatorial properties of input sets that make them hard to distinguish. Once
we have such properties, we can then try to exhibit them in functions of interest.

−

.

∈ A

B
∈ B

of 1-inputs, and an arbitrary real-valued relation function R (A, B)

We will see, somewhat surprisingly, that this “gloved” approach is useful for clas-
In the relational adversary method, we assume
sical lower bounds as well as quantum ones.
of 0-
there exists a T -query randomized algorithm for function F . We consider a set
0
inputs of F , a set
Intuitively, R (A, B) should be large if A and B diﬀer in only a few
and B
for A
locations. We then ﬁx a probability distribution
over inputs; by Yao’s minimax principle,
there exists a T -query deterministic algorithm Γ∗ that succeeds with high probability on
inputs drawn from
. Let WA be the set of 0-inputs and WB the set of 1-inputs on which
Γ∗ succeeds. Using the relation function R, we deﬁne a separation measure S between WA
and WB, and show that (1) initially S = 0, (2) by the end of the computation S must be
large, and (3) S increases by only a small amount as the result of each query.
It follows
that T must be large.

A

≥

D

D

The advantage of the relational method is that converts a “dynamic” opponent—
an algorithm that queries adaptively—into a relatively static one.
It thereby makes it
easier to focus on what is unique about a problem, aspects of query complexity that are
common to all problems having been handled automatically. Furthermore, one does not
need to know anything about quantum computing to understand and apply the method.
On the other hand, I have no idea how one would come up with it in the ﬁrst place, without
Ambainis’s quantum adversary method [27] and the reasoning about entanglement that led
to it.

The starting point is the “most general” adversary theorem in Ambainis’s original
paper (Theorem 6 in [27]), which he introduced to prove a quantum lower bound for the
problem of inverting a permutation. Here the input is a permutation σ (1) , . . . , σ (N ), and
N/2 and 1 otherwise. To lower-bound this problem’s
the task is to output 0 if σ−
query complexity, what we would like to say is this:

1 (1)

≤

Given any 0-input σ and any location x, if we choose a random 1-input τ that is

6I am thinking here of the hybrid method [51] as a cousin of the adversary method.
7Indeed, Ambainis [28] has given problems for which the adversary method provably yields a better lower

bound than the polynomial method.

‘related’ to σ, then the probability θ (σ, x) over τ that σ (x) does not equal τ (x) is small.
In other words, the algorithm is unlikely to distinguish σ from a random neighbor τ of σ
by querying x.

54

Unfortunately, the above claim is false. Letting x = σ−

=
τ (x) for every 1-input τ , and thus θ (σ, x) = 1. Ambainis resolves this diﬃculty by letting
us take the maximum, over all 0-inputs σ and 1-inputs τ that are related and diﬀer at x,
θ (σ, x) θ (τ, x). Even if θ (σ, x) = 1, the geometric mean is still
of the geometric mean
small provided that θ (τ, x) is small. More formally:

1 (1), we have that σ (x)

p

Theorem 8 (Ambainis) Let
F . Let R (A, B)
location x, let

≥

0 be a symmetric real-valued function, and for A

F −

1 (0) and

A ⊆

B ⊆

F −

1 (1) be sets of inputs to function
, and

, B

∈ A

∈ B

θ (A, x) =

B∗∈B

: A(x)

P

θ (B, x) =

A∗∈A
P

B∗∈B
: A∗(x)

=B∗(x) R (A, B∗)
R (A, B∗)
=B(x) R (A∗, B)
R (A∗, B)

,

,

A∗∈A
where the denominators are all nonzero. Then the number of quantum queries needed to
evaluate F with at least 9/10 probability is Ω (1/υgeom), where

P

P

υgeom =

A

, B

∈A

∈B

max

, x : R(A,B)>0, A(x)

=B(x)

θ (A, x) θ (B, x).

p

The best way to understand Theorem 8 is to see it used in an example.

Proposition 9 (Ambainis) The quantum query complexity of inverting a permutation is
Ω

√N

.

(cid:16)

(cid:17)

Proof. Let

A

the set of permutations τ such that τ −
if σ and τ diﬀer only at locations σ−
given σ, τ with R (σ, τ ) = 1, if x
θ (τ, x) = 2/N . So maxx : σ(x)

=τ (x)

= σ−

be the set of all permutations σ such that σ−

1 (1) > N/2. Given σ
1 (1) and τ −

1 (1)

be
≤
, let R (σ, τ ) = 1
∈ B
1 (1), and R (σ, τ ) = 0 otherwise. Then
1 (1) then

N/2, and

and τ

∈ A

= τ −

B

1 (1) then θ (σ, x) = 2/N , and if x

θ (σ, x) θ (τ, x) =

2/N .

The only diﬀerence between Theorem 8 and my relational adversary theorem is
that in the latter, we take the minimum of θ (A, x) and θ (B, x) instead of the geometric
mean. Taking the reciprocal then gives up to a quadratically better lower bound:
for
example, we obtain that the randomized query complexity of inverting a permutation is
Ω (N ). However, the proofs of the two theorems are quite diﬀerent.

p

p

Theorem 10 Let
needed to evaluate F with at least 9/10 probability is Ω (1/υmin), where

, R, θ be as in Theorem 8. Then the number of randomized queries

A

B

,

υmin =

A

, B

∈A

∈B

max

, x : R(A,B)>0, A(x)

=B(x)

min

θ (A, x) , θ (B, x)
}
{

.

6
6
6
6
6
6
6
6
Proof. Let Γ be a randomized algorithm that, given an input A, returns F (A)
,

with at least 9/10 probability. Let T be the number of queries made by Γ. For all A
B

, deﬁne

∈ A

∈ B

55

M (A) =

R (A, B∗) ,

XB∗∈B

M (B) =

R (A∗, B) ,

XA∗∈A

M =

M (A∗) =

M (B∗) .

XB∗∈B

XA∗∈A
DA be the distribution over A

∈ A

DB be the distribution over B

Now let
in which each A is chosen with probability
M (A) /M ; and let
in which each B is chosen with
probability M (B) /M . Let
DB. By Yao’s minimax
DA and
be an equal mixture of
principle, there exists a deterministic algorithm Γ∗ that makes T queries, and succeeds with
. Therefore Γ∗ succeeds with at least
at least 9/10 probability given an input drawn from
In other words,
4/5 probability given an input drawn from
letting WA be the set of A

D
DA alone, or from

on which Γ∗ succeeds, we have

and WB the set of B

DB alone.

∈ B

D

∈ A

∈ B

M (A)

4
5

M,

≥

WA
XA
∈

WB
XB
∈

M (B)

4
5

M.

≥

Deﬁne a predicate P (t) (A, B), which is true if Γ∗ has distinguished A
the tth query and false otherwise.
for which A (x)
function

by
(To distinguish A from B means to query an index x
, deﬁne a score

= B (x), given either A or B as input.) Also, for all A

from B

∈ A

∈ A

∈ B

S(t) (A) =

R (A, B∗) .

XB∗∈B

: P (t)(A,B∗)

This function measures how much “progress” has been made so far in separating A from

-inputs, where the

B

B

-inputs are weighted by R (A, B). Similarly, for all B

deﬁne

∈ B

It is clear that for all t,

S(t) (B) =

R (A∗, B) .

XA∗∈A

: P (t)(A∗,B)

S(t) (A) =

S(t) (B) .

So we can denote the above sum by S(t) and think of it as a global progress measure. The
proof relies on the following claims about S(t):

XA
∈A

XB
∈B

(i) S(0) = 0 initially.

3M/5 by the end.

(ii) S(T )

≥
(iii) ∆S(t)

≤

3υminM for all t, where ∆S(t) = S(t)

S(t

−

1) is the amount by which S(t)

−

increases as the result of a single query.

6
It follows from (i)-(iii) that

T

≥

3M/5
3υminM

=

1
5υmin

56

which establishes the theorem. Part (i) is obvious. For part (ii), observe that for every
WB, the algorithm Γ∗ must query an x such that
pair (A, B) with A
A (x)

= B (x). Thus

WA and B

∈

∈

S(T )

≥

≥

≥

WA, B
XA
∈

WB

∈
M (A)

XA
WA
∈
4
M
5

−

1
5

M.

R (A, B)

M (B)

−

WB
XB /
∈

It remains only to show part (iii). Suppose ∆S(t) > 3υminM for some t; we will obtain a
contradiction. Let

and let CA be the set of A

∈ A

∆S(t) (A) = S(t) (A)

S(t
−
for which ∆S(t) (A) > υminM (A). Since
∆S(t) (A) = ∆S(t) > 3υminM,

1) (A) ,

−

it follows by Markov’s inequality that

XA
∈A

CA
XA
∈
Similarly, if we let CB be the set of B

∆S(t) (A)

2
3

≥

∆S(t).

for which ∆S(t) (B) > υminM (B), we have

∈ B
∆S(t) (B)

2
3

≥

∆S(t).

CB
XB
∈

∈

CA, and at least 2/3 comes from (A, B) pairs such that B

In other words, at least 2/3 of the increase in S(t) comes from (A, B) pairs such that
A
CB. Hence, by a
CB with R (A, B) > 0 that are
‘pigeonhole’ argument, there exists an A
distinguished by the tth query.
= B (x), such
that the tth index queried by Γ∗ is x whether the input is A or B. Then since A
CA, we
have υminM (A) < ∆S(t) (A), and hence

In other words, there exists an x with A (x)

CA and B

∈

∈

∈

∈

υmin <

∆S(t) (A)
M (A)

B∗∈B

≤ P

: A(x)

B∗∈B

=B∗(x) R (A, B∗)
R (A, B∗)

Similarly υmin < θ (B, x) since B
P

CB. This contradicts the

∈

which equals θ (A, x).
deﬁnition

υmin =

A

, B

∈A

∈B

max

, x : R(A,B)>0, A(x)

min

θ (A, x) , θ (B, x)
}

{

,

=B(x)

and we are done.

6
6
6
6
7.4 Snakes

For the lower bounds, it will be convenient to generalize random walks to arbitrary distri-
butions over paths, which we call snakes.

57

Deﬁnition 11 Given a vertex h in G and a positive integer L, a snake distribution
Dh,L
(parameterized by h and L) is a probability distribution over paths (x0, . . . , xL
1) in G, such
that each xt is either equal or adjacent to xt+1, and xL
1 = h. Let Dh,L be the support of
Dh,L. Then an element of Dh,L is called a snake; the part near x0 is the tail and the part
near xL

1 = h is the head.

−

−

−

Given a snake X and integer t, we use X [t] as shorthand for

.
x0, . . . , xt}

{

Deﬁnition 12 We say a snake X
uniformly at random from
from

0, . . . , L
{
Dh,L conditioned on xt = yt for all t > j. Then

∈
−

Dh,L is ε-good if the following holds. Choose j
1) be a snake drawn
1
}

, and let Y = (y0, . . . , yL

−

(i) Letting SX,Y be the set of vertices v in X

we have

Y such that min

t : xt = v
{

}

= min

{

t : yt = v

,
}

∩

[X

Pr
j,Y

∩

Y = SX,Y ]

9/10.

≥

(ii) For all vertices v, Prj,Y [v

Y [j]]

ε.

≤

∈

The procedure above—wherein we choose a j uniformly at random, then draw a
Y from
Dh,L consistent with X on all steps later than j—will be important in what follows.
I call it the snake X ﬂicking its tail.
Intuitively, a snake is good if it is spread out fairly
evenly in G—so that when it ﬂicks its tail, (1) with high probability the old and new tails
do not intersect, and (2) any particular vertex is hit by the new tail with probability at
most ε.

(If X

I now explain the ‘snake method’ for proving lower bounds for Local Search.
Given a snake X, we deﬁne an input fX with a unique local minimum at x0, and f -values
Y = SX,Y ,
that decrease along X from head to tail. Then, given inputs fX and fY with X
we let the relation function R (fX, fY ) be proportional to the probability that snake Y is
= SX,Y we let R = 0.) Let fX and gY be inputs
obtained by X ﬂicking its tail.
with R (fX, gY ) > 0, and let v be a vertex such that fX (v)
= gY (v). Then if all snakes
were good, there would be two mutually exclusive cases: (1) v belongs to the tail of X, or
In case (1), v is hit with small probability when Y ﬂicks
(2) v belongs to the tail of Y .
In case (2), v is hit with small probability when X ﬂicks its
its tail, so θ (fY , v) is small.
θ (fX, v) θ (fY , v) and
tail, so θ (fX, v) is small.
minimum min
are small. So even though θ (fX, v) or θ (fY , v) could be
large individually, Theorems 8 and 10 yield a good lower bound, as in the case of inverting
a permutation (see Figure 7.1).

In either case, then, the geometric mean

θ (fX, v) , θ (fY , v)

p

∩

∩

Y

{

}

One diﬃculty is that not all snakes are good; at best, a large fraction of them are.
We could try deleting all inputs fX such that X is not good, but that might ruin some
So we would have to delete
remaining inputs, which would then have fewer neighbors.

6
6
58

xj+1=yj+1
6

1
x0

2

Large q (fX,v) 
but small q (fY,v)

3

5

4

3

2

1

y0

7

8

5

4
Large q (fY,v) 
but small q (fX,v)

9

10

11
11

xL-1=yL-1

Figure 7.1: For every vertex v such that fX (v)
= fY (v), either when snake X ﬂicks its
tail v is not hit with high probability, or when snake Y ﬂicks its tail v is not hit with high
probability.

those inputs as well, and so on ad inﬁnitum. What we need is basically a way to replace
“all inputs” by “most inputs” in Theorems 8 and 10.

Fortunately, a simple graph-theoretic lemma can accomplish this. The lemma (see
Diestel [98, p.6] for example) says that any graph with average degree at least k contains an
induced subgraph with minimum degree at least k/2. Below I prove a weighted analogue
of the lemma.

Lemma 13 Let p (1) , . . . , p (m) be positive reals summing to 1. Also let w (i, j) for i, j
1, . . . , m
{
there exists a nonempty subset U
rp (i) /2.

be nonnegative reals satisfying w (i, j) = w (j, i) and

such that for all i
P

i,j w (i, j)
U ,
j

≥
U w (i, j)
∈

1, . . . , m

⊆ {

∈
r. Then

≥

∈

}

}

P

Proof. If r = 0 then the lemma trivially holds, so assume r > 0. We construct
. Then for all t, if there exists an
1, . . . , m

U via an iterative procedure. Let U (0) =
i∗ ∈

U (t) for which

{

}

r
2

p (i∗) ,

w (i∗, j) <

U (t)
Xj
∈

then set U (t + 1) = U (t)
constructed is nonempty, observe that when we remove i∗, the sum
by p (i∗), while

. Otherwise halt and return U = U (t). To see that the U so
U (t) p (i) decreases
i
∈

i∗}

\{

U (t) w (i, j) decreases by at most
∈

P

i,j

P

w (i∗, j) +

w (j, i∗) < rp (i∗) .

U (t)
Xj
∈

U (t)
Xj
∈

So since
the procedure; hence U must be nonempty.

U (t) w (i, j) was positive to begin with, it must still be positive at the end of
∈

i,j

P
I can now prove the main result of the section.

6
Theorem 14 Suppose a snake drawn from
Then

RLS (G) = Ω (1/ε) ,

Dh,L is ε-good with probability at least 9/10.
QLS (G) = Ω
.

1/ε

59

(cid:16)p

(cid:17)

∈

∈

t : xt = v
{

; and for each v /
∈
}

Proof. Given a snake X
X, let fX (v) = min

Dh,L, we construct an input function fX as follows.
X, let fX (v) = ∆ (v, h) + L
For each v
where ∆ (v, h) is the distance from v to h in G. Clearly fX so deﬁned has a unique local
minimum at x0. To obtain a decision problem, we stipulate that querying x0 reveals an
answer bit (0 or 1) in addition to fX (x1); the algorithm’s goal is then to return the answer
bit. Obviously a lower bound for the decision problem implies a corresponding lower bound
for the search problem. Let us ﬁrst prove the theorem in the case that all snakes in Dh,L are
ε-good. Let p (X) be the probability of drawing snake X from
Dh,L. Also, given snakes
, let qj (X, Y ) be the probability that X ∗ = Y , if X ∗ is drawn
X, Y and j
from

Dh,L conditioned on agreeing with X on all steps later than j. Then deﬁne
p (X)
L

w (X, Y ) =

qj (X, Y ) .

0, . . . , L

1
}

∈ {

−

−

L

1

Xj=0

The ﬁrst claim is that w is symmetric; that is, w (X, Y ) = w (Y, X).
that

It suﬃces to show

p (X) qj (X, Y ) = p (Y ) qj (Y, X)

for all j. We can assume X agrees with Y on all steps later than j, since otherwise
Dh,L, let A denote the event that X ∗ agrees
qj (X, Y ) = qj (Y, X) = 0. Given an X ∗ ∈
with X (or equivalently Y ) on all steps later than j, and let BX (resp. BY ) denote the
event that X ∗ agrees with X (resp. Y ) on steps 1 to j. Then

p (X) qj (X, Y ) = Pr [A] Pr [BX|
A]
= p (Y ) qj (Y, X) .

Pr [BY |

A]

·

Now let E (X, Y ) denote the event that X
Y = SX,Y , where SX,Y is as in Deﬁnition
∩
12. Also, let fX be the input obtained from X that has answer bit 0, and gX be the
input that has answer bit 1. To apply Theorems 8 and 10, take
Dh,L}
. Then take R (fX, gY ) = w (X, Y ) if E (X, Y ) holds, and
Dh,L}
and
∈
with R (fX, gY ) > 0, and letting v
R (fX, gY ) = 0 otherwise. Given fX ∈ A
be a vertex such that fX (v)
Y . Suppose
X or v /
∈
the former case; then

and gY ∈ B
= gY (v), we must then have either v /
∈

gX : X
{

fX : X

A

=

=

∈

B

{

fX∗ ∈A

: fX∗
X

(v)

=gY (v)

R (fX∗, gY )

≤

fX∗ ∈A

: fX∗
X

(v)

=gY (v)

p (Y )
L

1

L

−

Xj=0

qj (Y, X ∗)

εp (Y ) ,

≤

since Y is ε-good. Thus θ (gY , v) equals

fX∗ ∈A

P

=gY (v) R (fX∗, gY )
R (fX∗, gY )

(v)

: fX∗
fX∗ ∈A

εp (Y )
9p (Y ) /10

.

≤

P

6
6
6
6
Similarly, if v /
∈

Y then θ (fX, v)

≤

10ε/9 by symmetry. Hence

υmin =

υgeom =

fX ∈A

, gY ∈B

fX ∈A

, gY ∈B

max

, v : R(fX ,gY )>0, fX (v)

=gY (v)

min

{

θ (fX, v) , θ (gY , v)

max

, v : R(fX ,gY )>0, fX (v)

θ (fX, v) θ (gY , v)

≤

=gY (v)

p

r

ε
9/10

} ≤
ε
9/10

,

60

,

the latter since θ (fX, v)

1 and θ (gY , v)

1 for all fX, gY and v.

In the general case, all we know is that a snake drawn from

≤

≤

probability at least 9/10.
: G (X)
fX ∈ A
{

and

B∗ =

}

Let G (X) denote the event that X is ε-good. Take
gY ∈ B

Dh,L is ε-good with
A∗ =
, and take R (fX, gY ) as before. Then since

: G (Y )
}

{

XX,Y : E(X,Y )

w (X, Y )

≥

XX

9
10

p (X)

9
10

,

≥

by the union bound we have

R (fX, gY )

XfX ∈A∗, gY ∈B∗

≥

≥

=

G(Y )
X
∧
1
10

1
10 −

X,Y : G(X)
9
10 −
7
10

.

w (X, Y )

p (X)

−

−

XX : qG(X)

XY : qG(Y )

p (Y )

E(X,Y )

∧

So by Lemma 13, there exist subsets
gY ∈

B

,

A ⊆ A∗ and
e

e

R (fX, gY ∗)

XgY ∗ ∈
B

e

R (fX∗, gY )

XfX∗ ∈
A

≥

≥

B ⊆ B∗ such that for all fX ∈
e
7p (X)
20

,

A

e

and

7p (Y )
20

.

So for all fX, gY with R (fX, gY ) > 0, and all v such that fX (v)
20ε/7 and υgeom ≤
20ε/7. Hence υmin ≤
20ε/7 or θ (gY , v)

≤

e

= gY (v), either θ (fX, v)
20ε/7.

≤

7.5 Speciﬁc Graphs

p

In this section I apply the ‘snake method’ developed in Section 7.4 to speciﬁc examples
of graphs: the Boolean hypercube in Section 7.5.1, and the d-dimensional cubic grid (for
d

3) in Section 7.5.2.

≥

7.5.1 Boolean Hypercube

n denote the n-dimensional Boolean hypercube—that is, the
Abusing notation, let
graph whose vertices are n-bit strings, with two vertices adjacent if and only if they have

0, 1
}

{

6
6
6
n, let v [0] , . . . , v [n
Hamming distance 1. Given a vertex v
bits of v, and let v(i) denote the neighbor obtained by ﬂipping bit v [i].
lower-bound RLS (

∈ {
n).

0, 1
}

0, 1
}
{
Fix a ‘snake head’ h

n) and QLS (
0, 1
{
}
n and take L = 2n/2/100.
0, 1
}

∈ {

I deﬁne the snake distribu-
Dh,L via what I call a coordinate loop, as follows. Starting from x0 = h, for each t take
with 1/2 probability. The following is

tion
xt+1 = xt with 1/2 probability, and xt+1 = x(t mod n)
a basic fact about this distribution.

t

−

61

1] denote the n
In this section I

Proposition 15 The coordinate loop mixes completely in n steps, in the sense that if t∗ ≥
t + n, then xt∗ is a uniform random vertex conditioned on xt.

One could also use the random walk distribution, following Aldous [24]. However,
not only is the coordinate loop distribution easier to work with (since it produces fewer
self-intersections), it also yields a better lower bound (since it mixes completely in n steps,
as opposed to approximately in n log n steps).

I ﬁrst upper-bound the probability, over X, j, and Y [j], that X

SX,Y is as in Deﬁnition 12).

Lemma 16 Suppose X is drawn from
Y [j] is drawn from

Dh,L, j is drawn uniformly from
Y = SX,Y ]

0.9999.

Dxj,j. Then PrX,j,Y [j] [X
Proof. Call a disagreement a vertex v such that

∩

≥

Y

= SX,Y (where

∩

0, . . . , L
{

, and
1
}

−

min

t : xt = v
{

= min

} 6

t∗ : yt∗ = v

.

}

{

If v is a disagreement, then by
Clearly if there are no disagreements then X
∩
the deﬁnition of
n. So by Proposition 15,
Dh,L we cannot have both t > j
−
either yt∗ is uniformly random conditioned on X, or xt is uniformly random conditioned on
Y [j]. Hence PrX,j,Y [j] [xt = yt∗ ] = 1/2n. So by the union bound,

n and t∗ > j

Y = SX,Y .

−

Pr
X,j,Y [j]

[X

∩

Y

= SX,Y ]

L2
2n = 0.0001.

≤

I now argue that, unless X spends a ‘pathological’ amount of time in one part of
the hypercube, the probability of any vertex v being hit when X ﬂicks its tail is small. To
prove this, I deﬁne a notion of sparseness, and then show that (1) almost all snakes drawn
from
Dh,L are sparse (Lemma 18), and (2) sparse snakes are unlikely to hit any given vertex
v (Lemma 19).

Deﬁnition 17 Given vertices v, w and i
steps needed to reach v from x by ﬁrst setting x [i] := v [i], then setting x [i
and so on.
exists a constant c such that for all v

(After we set x [0] we wrap around to x [n
−
n and all k,

, let ∆ (x, v, i) be the number of
1],
−
1].) Then X is sparse if there

1] := v [i

0, . . . , n

1
}

∈ {

−

−

0, 1
}

∈ {

t : ∆ (xt, v, t mod n) = k

|{

}| ≤

cn

n +

(cid:18)

L
2n

−

k

.

(cid:19)

6
6
Lemma 18 If X is drawn from

Proof. For each i

∈ {
i (mod n) is at most L/n. For such a t, let E(v,i,k)

Dh,L, then X is sparse with probability 1
0, . . . , L
, the number of t
0, . . . , n
be the event that ∆ (xt, v, i)

o (1).

1
}

1
}

∈ {

−

−

−

such that
k;

t

t
then E(v,i,k)
t

≡

holds if and only if

xt [i] = v [i] , . . . , xt [i

k + 1] = v [i

k + 1]

−

−

62

≤

(where we wrap around to xt [n
2k/2n over X. Furthermore, by Proposition 15, the E(v,i,k)
independent. So let

1] after reaching xt [0]). This occurs with probability
events for diﬀerent t’s are

−

t

µk =

L
n ·

2k
2n ;

then for ﬁxed v, i, k, the expected number of t’s for which E(v,i,k)
1 then
by a Chernoﬀ bound, if µk ≥

t

holds is at most µk. Thus

Pr
X

t : E(v,i,k)
t

> cn

µk

<

·

h(cid:12)
i
n
(cid:12)
for suﬃciently large c. Similarly, if µk < 1 then
(cid:12)

o(cid:12)
(cid:12)
(cid:12)

ecn
1
−
(cn)cn

(cid:18)

(cid:19)

µk

<

1
22n

Pr
X

h(cid:12)
n
(cid:12)
(cid:12)

t : E(v,i,k)
t

> cn

<

o(cid:12)
(cid:12)
(cid:12)

i

for suﬃciently large c. By the union bound, then,

1

ecn/µk−
(cn/µk)cn/µk !

µk

<

1
22n

t : E(v,i,k)
t
n

(cid:12)
(cid:12)
(cid:12)

cn

·

≤

(1 + µk)

= c

n +

(cid:18)

L
2n

−

k

(cid:19)

o(cid:12)
(cid:12)
(cid:12)

for every v, i, k triple simultaneously with probability at least 1
Summing over all i’s produces the additional factor of n.

−

n22n/22n = 1

o (1).

−

Lemma 19 If X is sparse, then for every v

∈ {

n,

0, 1
}
n2
L

(cid:18)

.

(cid:19)

[v

Pr
j,Y

∈

Y [j]] = O

Proof. By assumption, for every k

0, . . . , n

,
}

∈ {

[∆ (xj, v, j mod n) = k]

Pr
j

|{

t : ∆ (xt, v, t mod n) = k
L

}|

cn
L

n +

(cid:18)

L
2n

−

k

.

(cid:19)

≤

≤

 
Consider the probability that v

∈

Y [j] in the event that ∆ (xj, v, j mod n) = k. Clearly

[v

Pr
Y

yj

∈ {

n+1, . . . , yj}

−

] =

1
2k .

63

Also, Proposition 15 implies that for every t
So by the union bound,

j

−

≤

Then Prj,Y [v

∈

Y [j]] equals

[v

Pr
Y

∈ {

y0, . . . , yj

]
n}

−

≤

L
2n .

n, the probability that yt = v is 2−

n.

n

Xk=0 (cid:18)

Prj [∆ (xj, v, j mod n) = k]

∆ (xj, v, j mod n) = k]

·

PrY [v

Y [j]

∈

|

n +

L
2n

−

k

1
2k +

L
2n

(cid:19)

(cid:19) (cid:18)

n

≤

(cid:19)

Xk=0

= O

(cid:18)

cn
L

(cid:18)
cn2
L

(cid:19)

as can be veriﬁed by breaking the sum into cases and doing some manipulations.

The main result follows easily:

Theorem 20

RLS (

0, 1
}
{

n) = Ω

2n/2
n2

!

, QLS (
{

0, 1
}

n) = Ω

2n/4
n !

.

Proof. Take ε = n2/2n/2. Then by Theorem 14, it suﬃces to show that a snake

X drawn from

Dh,L is O (ε)-good with probability at least 9/10. First, since
Y = SX,Y ]

0.9999

[X

Pr
X,j,Y [j]

∩

≥

by Lemma 16, Markov’s inequality shows that

Pr
j,Y [j]

[X

∩

Pr
X

(cid:20)

Y = SX,Y ]

9
10

≥

≥

(cid:21)

19
20

.

Second, by Lemma 18, X is sparse with probability 1
sparse then

o (1), and by Lemma 19, if X is

−

[v

Pr
j,Y

∈

Y [j]] = O

(cid:18)

= O (ε)

(cid:19)

n2
L

for every v. So both requirements of Deﬁnition 12 hold simultaneously with probability at
least 9/10.

 
 
64

Dh,L moves a random distance left or
Figure 7.2: In d = 3 dimensions, a snake drawn from
right, then a random distance up or down, then a random distance inward or outward, etc.

7.5.2 Constant-Dimensional Grid Graph

In the Boolean hypercube case,
Dh,L was deﬁned by a ‘coordinate loop’ instead of the
usual random walk mainly for convenience. When we move to the d-dimensional grid,
though, the drawbacks of random walks become more serious: ﬁrst, the mixing time is
too long, and second, there are too many self-intersections, particularly if d
4. The
snake distribution will instead use straight lines of randomly chosen lengths attached at the
3. That is,
endpoints, as in Figure 7.2. Let Gd,N be a d-dimensional grid graph with d
1, . . . , N 1/d
1]), where each v [i] is in
Gd,N has N vertices of the form v = (v [0] , . . . , v [d
(assume for simplicity that N is a dth power). Vertices v and w are adjacent if and only
(cid:9)
if
= i (so Gd,N does
1
}
not wrap around at the boundaries).

, and v [j] = w [j] for all j

= 1 for some i

w [i]
|

0, . . . , d

v [i]
|

∈ {

−

≤

≥

−

−

(cid:8)

Take L = √N /100, and deﬁne the snake distribution

Dh,L as follows. Starting
from x0 = h, for each T take xN 1/d(T +1) identical to xN 1/dT , but with the (T mod d)th
coordinate xN 1/d(T +1) [T mod d] replaced by a uniform random value in
. Then
1 to lie along the shortest path from xN 1/dT to
take the vertices xN 1/dT +1, . . . , xN 1/dT +N 1/d
xN 1/d(T +1), ‘stalling’ at xN 1/d(T +1) once that vertex has been reached. Call

1, . . . , N 1/d

(cid:8)

(cid:9)

−

ΦT =

xN 1/dT , . . . , xN 1/dT +N 1/d

1

−

a line of vertices, whose direction is T mod d. As in the Boolean hypercube case, we have:

(cid:1)

(cid:0)

Proposition 21
then xN 1/dT ∗

Dh,L mixes completely in dN 1/d steps, in the sense that if T ∗ ≥

is a uniform random vertex conditioned on xN 1/dT .

T + d,

Lemma 16 in Section 7.5.1 goes through essentially without change.

Deﬁnition 22 Letting ∆ (x, v, i) be as before, we say X is sparse if there exists a constant
c (possibly dependent on d) such that for all vertices v and all k,

t : ∆

xt, v,

t/N 1/d

mod d

= k

(cid:16)

j

k

(cid:17)

(cid:12)
n
(cid:12)
(cid:12)

≤

o(cid:12)
(cid:12)
(cid:12)

(c log N )

N 1/d +

(cid:18)

L

N 1

−

k/d

.

(cid:19)

6
Lemma 23 If X is drawn from

Dh,L, then X is sparse with probability 1

−

o (1).

65

Proof. Similar to Lemma 18. Let ΦT be a line of vertices with direction i =
T mod d, and notice that ∆ (xt, v, i) is the same for every vertex xt in ΦT . Let E(v,i,k)
occurs with probability
denote the event that ∆ (xt, v, i)
N (k
are independent
events. So let

k for the xt’s in ΦT . Then E(v,i,k)

1)/d/N over X. Furthermore, if

T
and E(v,i,k)

d then E(v,i,k)

T
|

T ∗

≤

−

−

T

T

T ∗| ≥
N (k

µk = L

·

1)/d

;

−
N

then for ﬁxed v, i, k, the expected number of lines for which E(v,i,k)
Thus, by a Chernoﬀ bound, if µk ≥

1 then

T

Pr
X

T : E(v,i,k)
T

> c log N

1

ec log N
(c log N )c log N

−

·

µk

<

i

which is at most 1/N 2 for suﬃciently large c.
(c log N ) /µk,

o(cid:12)
(cid:12)
(cid:12)

holds is at most µk.

µk

!

Similarly, if µk < 1 then letting m =

T : E(v,i,k)
T

> c log N

<

h(cid:12)
n
(cid:12)
(cid:12)

o(cid:12)
(cid:12)
So with probability 1
(cid:12)

i
−

1

em
−
mm

µk

<

1
N 2

(cid:18)

(cid:19)

o (1) it holds that for all v, k, letting

h(cid:12)
n
(cid:12)
(cid:12)

Pr
X

for suﬃciently large c.
it =

t/N 1/d

mod d,

(cid:4)

(cid:5)

t : ∆ (xt, v, it) = k

|{

}| ≤

c log N

·
= (c log N )

(1 + µk)

·
N 1/d +

Lemma 24 If X is sparse, then for every v

(cid:18)

Gd,N ,

∈

N 1/d
L

N 1

−

k/d

.

(cid:19)

[v

Pr
j,Y

∈

Y [j]] = O

N 1/d log N
L

,

!

where the big-O hides a constant dependent on d.

Proof. As in Lemma 19, setting ij =

j/N 1/d

mod d we obtain that Prj,Y [v

Y [j]]

∈

equals

d

Xk=1

d

≤

Xk=1

= O

(cid:4)

Pr
j

[∆ (xj, v, ij ) = k] Pr
Y

[v

∈

Y [j]

(cid:5)

|

∆ (xj, v, ij ) = k]

L

1

N 1

−

k/d

(cid:19) (cid:18)

N (k

−

1)/d

+

L
N

(cid:19)

c log N
L

(cid:18)

N 1/d +

N 1/d log N
L

.

!

 
 
 
66

By the same proof as for Theorem 20, taking ε = (log N ) /N 1/2

−

1/d yields the

following:

Theorem 25 Neglecting a constant dependent on d, for all d

3

≥

RLS (Gd,N ) = Ω

QLS (Gd,N ) = Ω

N 1/2

1/d

−
log N !

,

N 1/2

1/d

−
log N 

.





s



 
67

Chapter 8

Quantum Certiﬁcate Complexity

S → {

0, 1
}

be a Boolean function with

This chapter studies the relationships between classical and quantum measures of
n, that takes
query complexity. Let f :
input Y = y1 . . . yn. Then the deterministic query complexity D (f ) is the minimum number
of queries to the yi’s needed to evaluate f , if Y is chosen adversarially and if queries can
be adaptive (that is, can depend on the outcomes of previous queries). Also, the bounded-
error randomized query complexity, R2 (f ), is the minimum expected number of queries
needed by a randomized algorithm that, for each Y , outputs f (Y ) with probability at least
2/3. Here the ‘2’ refers to two-sided error; if instead we require f (Y ) to be output with
probability 1 for every Y , we obtain R0 (f ), or zero-error randomized query complexity.

0, 1
}

S ⊆ {

Analogously, Q2 (f ) is the minimum number of queries needed by a quantum
0, 1
algorithm that outputs f (Y ) with probability at least 2/3 for all Y . Also, for k
}
let Qk
0 (f ) be the minimum number of queries needed by a quantum algorithm that outputs
= k.
f (Y ) with probability 1 if f (Y ) = k, and with probability at least 1/2 if f (Y )
Then let Q0 (f ) = max
If we require a single algorithm that succeeds
.
with probability 1 for all Y , we obtain QE (f ), or exact quantum query complexity. See
Buhrman and de Wolf [78] for a more detailed survey of these measures.

0 (f ) , Q1

0 (f )

∈ {

Q0

(cid:8)

(cid:9)

It is immediate that

Q 2 (f )

R 2 (f )

R 0 (f )

D (f )

n,

≤

≤

≤

≤

≤

D (f ). If f is partial (i.e.

R0 (f ), and that QE (f )

n), then Q2 (f )
that Q0 (f )
can be superpolynomially smaller than R2 (f ); this is what makes Shor’s period-ﬁnding
algorithm [219] possible. For total f , by contrast, the largest known gap even between
D (f ) and Q2 (f ) is quadratic, and is achieved by the OR function on n bits: D (OR) = n
(indeed R2 (OR) = Ω (n)), whereas Q2 (OR) = Θ (√n) because of Grover’s search algorithm
, while
[139]. Furthermore, for total f , Beals et al. [45] showed that D (f ) = O

0, 1
}
{

S 6

=

≤

de Wolf [244] showed that D (f ) = O

Q2 (f )2 Q0 (f )2

.

The result of Beals et al. [45] relies on two intermediate complexity measures, the

(cid:17)

(cid:16)

certiﬁcate complexity C (f ) and block sensitivity bs (f ), which are deﬁned as follows.

Deﬁnition 26 A certiﬁcate for an input X is a set S

1, . . . , n

⊆ {

}

such that for all inputs

Q2 (f )6
(cid:16)

(cid:17)

6
Deterministic Randomized Quantum

Query complexity
Certiﬁcate complexity

D (f )
C (f )

R2 (f )
RC (f )

Q2 (f )
QC (f )

Table 8.1: Query complexity measures and their certiﬁcate complexity analogues.

68

Y of f , if yi = xi for all i
certiﬁcate for X, and C (f ) is the maximum of CX (f ) over all X.

S then f (Y ) = f (X). Then CX (f ) is the minimum size of a

∈

=
Deﬁnition 27 A sensitive block on input X is a set B
}
B. Then bsX (f ) is the
f (X), where X (B) is obtained from X by ﬂipping xi for each i
maximum number of disjoint sensitive blocks on X, and bs (f ) is the maximum of bsX (f )
over all X.

such that f

1, . . . , n

⊆ {

∈

(cid:1)

(cid:0)

X (B)

Clearly bs (f )

C (f )

D (f ). For total f , these measures are all polynomially

related: Nisan [183] showed that C (f )
C (f ) bs (f ). Combining these results with bs (f ) = O

≤

≤

≤

bs (f )2, while Beals et al. [45] showed that D (f )

≤
(from the optimality of

Q2 (f )2

(cid:17)

Grover’s algorithm), one obtains D (f ) = O

Q2 (f )6
(cid:16)

(cid:16)

.

(cid:17)

8.1 Summary of Results

I investigate RC (f ) and QC (f ), the bounded-error randomized and quantum generaliza-
tions of the certiﬁcate complexity C (f ) (see Table 8.1). My motivation is that, just as
C (f ) was used to show a polynomial relation between D (f ) and Q2 (f ), so RC (f ) and
QC (f ) can lead to new relations among fundamental query complexity measures.

What the certiﬁcate complexity C (f ) measures is the number of queries used to
verify a certiﬁcate, not the number of bits used to communicate it. Thus, if we want
to generalize C (f ), we should assume the latter is unbounded. A consequence is that
without loss of generality, a certiﬁcate is just a claimed value X for the input Y 1—since
any additional information that a prover might provide, the veriﬁer can compute for itself.
The veriﬁer’s job is to check that f (Y ) = f (X). With this in mind I deﬁne RC (f ) as
follows.

Deﬁnition 28 A randomized veriﬁer for input X is a randomized algorithm that, on input
Y to f , (i) accepts with probability 1 if Y = X, and (ii) rejects with probability at least 1/2 if
f (Y )
= X but f (Y ) = f (X), the acceptance probability can be arbitrary.)
Then RCX (f ) is the minimum expected number of queries used by a randomized veriﬁer
for X, and RC (f ) is the maximum of RCX (f ) over all X.

= f (X). (If Y

I deﬁne QC (f ) analogously, with quantum instead of randomized algorithms. The
following justiﬁes the deﬁnition (the RC (f ) part was originally shown by Raz et al. [197]).

1Throughout this chapter, I use Y to denote the ‘actual’ input being queried, and X to denote the

‘claimed’ input.

6
6
6
Proposition 29 Making the error probability two-sided rather than one-sided changes RC (f )
and QC (f ) by at most a constant factor.

69

Proof. For RC (f ), let rY

V be the event that veriﬁer V rejects on input Y , and
let dY
V be the event that V encounters a disagreement with X on Y . We may assume
rY
rY
Pr
= 1. Suppose that Pr
=
V |
V
f (X). We wish to lower-bound Pr
= f (X). Observe that

ε0 if Y = X and Pr
≤
for all Y such that f (Y )
(cid:2)

ε1 if f (Y )

dY
V

−

≥

1

(cid:3)

(cid:2)

Pr

Hence for f (Y )

qdY

rY
V ∧
(cid:2)
= f (X),

V |

f (Y )

Pr

≤

rX
V ∧

qdX
V

= Pr

ε0.

≤

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

rY
V
dY
V
(cid:2)
(cid:2)

(cid:3)
(cid:3)

= f (X)

(cid:3)
rX
V

Pr

dY
V

Pr

rY
V

≥

Pr

−

rY
V ∧

qdY
V

1

ε1 −

−

≥

ε0.

(cid:2)
Now let V ∗ be identical to V except that, whenever V rejects despite having found no
= f (X),
disagreement with X, V ∗ accepts. Clearly Pr

= 0. Also, in the case f (Y )

(cid:3)

(cid:3)

(cid:3)

(cid:2)

(cid:2)

rX
V ∗

Pr

rY
V ∗

= Pr

(cid:2)

dY
V

(cid:3)
1

≥

ε1 −

−

ε0.

The result follows since O (1) repetitions suﬃce to boost any constant error probability to
any other constant error probability.

(cid:2)

(cid:3)

(cid:2)

(cid:3)

For QC (f ), suppose the veriﬁer’s ﬁnal state given input Y is

αY

z
z |

i

βY
0
z |
i

+ γY

1
z |
i

z
X

(cid:0)
is the accept state, and

0
i

is the reject state,

where
|
also that AX
is the probability of accepting. Then the veriﬁer can make AX = 1 by performing the
(cid:12)
(cid:12)
conditional rotation

= 1 for all z. Suppose
2

ε1 whenever f (Y )
(cid:12)
(cid:12)

ε0 and that AY

z γY
αY
z

1
i
|

P

≥

≤

−

+

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1

z

2

2

(cid:1)
βY
z
= f (X), where AY =
(cid:12)
(cid:12)

γY
z

on the second register prior to measurement.

AY =

2

αY
z

= f (X), this produces

γX
z −
βX
z

βX
z
γX
z (cid:19)
In the case f (Y )

(cid:18)

z βY
βX

z + γX

z γY
z

z
X
2

(cid:12)
(cid:12)

(cid:12)
(cid:12)
αY
z

(cid:12)
2
(cid:12)

βX
z

2

+

γY
z

z
X

(cid:12)
(cid:12)
2 (ε0 + ε1) .

(cid:12)
(cid:12)

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)

2

(cid:12)
2
(cid:12)

≤

≤

It is immediate that QC (f )

C (f ), that QC (f ) = O (Q2 (f )), and
RC (f )
that RC (f ) = O (R2 (f )). We also have RC (f ) = Ω (bs (f )), since a randomized veriﬁer
for X must query each sensitive block on X with 1/2 probability. This suggests viewing
RC (f ) as an ‘alloy’ of block sensitivity and certiﬁcate complexity, an interpretation for
which Section 8.5 gives some justiﬁcation.

≤

≤

6
6
6
6
6
6
6
70

(cid:17)

(cid:16)p

RC (f )

The results of this chapter are as follows.

In Section 8.3 I show that QC (f ) =
for all f (partial or total), precisely characterizing quantum certiﬁcate com-
Θ
plexity in terms of randomized certiﬁcate complexity. To do this, I ﬁrst give a nonadaptive
characterization of RC (f ), and then apply the adversary method of Ambainis [27] to lower-
bound QC (f ) in terms of this characterization. Then, in Section 8.4, I extend results on
polynomials due to de Wolf [244] and to Nisan and Smolensky (as described by Buhrman
and de Wolf [78]), to show that R0 (f ) = O (RC (f ) ndeg (f ) log n) for all total f , where
ndeg (f ) is the minimum degree of a polynomial p such that p (X)
= 0 if and only if
= 0. Combining the results of Sections 8.3 and 8.4 leads to a new lower bound on
f (X)
Q2 (f )2 Q0 (f ) log n
quantum query complexity: that R0 (f ) = O
for all total f . To my
knowledge, this is the ﬁrst quantum lower bound to use both the adversary method and
(cid:16)
the polynomial method at diﬀerent points in the argument.

(cid:17)

Finally, in Section 8.5, I exhibit asymptotic gaps between RC (f ) and other query
, and a sym-
I conclude in Section

complexity measures, including a total f for which C (f ) = Θ
metric partial f for which QC (f ) = O (1) yet Q2 (f ) = Ω (n/ log n).
8.6 with some open problems.

QC (f )2.205
(cid:16)

(cid:17)

8.2 Related Work

Raz et al. [197] studied a query complexity measure they called ma (f ), for Merlin-Arthur.
In my notation, ma (f ) equals the maximum of RCX (f ) over all X with f (X) = 1. Raz
et al. observed that ma (f ) = ip (f ), where ip (f ) is the number of queries needed given
arbitrarily many rounds of interaction with a prover. They also used error-correcting codes
to construct a total f for which ma (f ) = O (1) but C (f ) = Ω (n). This has similarities to
the construction, in Section 8.5.2, of a symmetric partial f for which QC (f ) = O (1) but
Q2 (f ) = Ω (n/ log n). Aside from that and from Proposition 29, Raz et al.’s results do not
overlap with the results here.

Watrous [239] has investigated a diﬀerent notion of ‘quantum certiﬁcate complexity’—

whether certiﬁcates that are quantum states can be superpolynomially smaller than any
classical certiﬁcate. Also, de Wolf [245] has investigated ‘nondeterministic quantum query
complexity’ in the alternate sense of algorithms that accept with zero probability when
f (Y ) = 0, and with positive probability when f (Y ) = 1.

8.3 Characterization of Quantum Certiﬁcate Complexity

, precisely characterizing quantum certiﬁcate
We wish to show that QC (f ) = Θ
complexity in terms of randomized certiﬁcate complexity. The ﬁrst step is to give a simpler
(cid:17)
characterization of RC (f ).

RC (f )

(cid:16)p

Lemma 30 Call a randomized veriﬁer for X nonadaptive if, on input Y , it queries each
yi with independent probability λi, and rejects if and only if it encounters a disagreement
with X.
na (f )

(Thus, we identify such a veriﬁer with the vector (λ1, . . . , λn).) Let RCX

6
6
be the minimum of λ1 +
.
Θ

RCX (f )

· · ·

+ λn over all nonadaptive veriﬁers for X. Then RCX

na (f ) =

71

(cid:0)

(cid:1)

na (f ) = Ω

Proof. Clearly RCX

RCX (f )
. For the upper bound, we can assume
that a randomized veriﬁer rejects immediately on ﬁnding a disagreement with X, and
(cid:0)
. Let V be an optimal
=
accepts if it ﬁnds no disagreement. Let
randomized veriﬁer, and let pt (Y ) be the probability that V , when given input Y
, ﬁnds
a disagreement with X on the tth query. By Markov’s inequality, V must have found a
queries. So by the union
disagreement with probability at least 1/2 after T =
bound

= f (X)
}

2 RCX (f )

Y : f (Y )

∈ Y

Y

{

(cid:1)

p1 (Y ) +

(cid:6)
+ pT (Y )

1
2

≥

· · ·
1, . . . , T

(cid:7)

∈ Y

. Suppose we choose t

uniformly at random and simulate the
for each Y
∈ {
tth query, pretending that queries 1, . . . , t
1 have already been made and have returned
agreement with X. Then we must ﬁnd a disagreement with probability at least 1/2T .
By repeating this procedure 4T times, we can boost the probability to 1
−
, let λi be the probability that yi is queried at least once. Then λ1 +
1, . . . , n
{
}
whereas for each Y

2. For i
+λn ≤

∈
4T ,

· · ·

e−

−

}

,

∈ Y

λi ≥

1

−

e−

2.

=xi
Xi:yi6

It follows that, if each yi is queried with independent probability λi, then the probability
that at least one yi disagrees with X is at least

1

−

=xi
Yi:yi6

(1

λi)

1

≥

−

1

e−
−
n

1

−

2

n

(cid:19)

−

(cid:18)

> 0.57.

To obtain a lower bound on QC (f ), I will use the following simple reformulation

of Ambainis’s adversary method [27].

n, let β be
Theorem 31 (Ambainis) Given a function f :
0, 1
}
a function from
be a relation such that
R (X, Y ) = R (Y, X) for all X, Y and R (X, Y ) = 0 whenever f (X) = f (Y ). Let δ0, δ1 ∈
(0, 1] be such that for every X

to nonnegative reals, and let R :

0, 1
}
0, 1
}
→ {

S → {
S

1, . . . , n

S ⊆ {

and i

with

S

2

∈ S

∈ {

,
}

β (Y )

β (Y )

≥

≤

1,

δf (X).

XY : R(X,Y )=1

XY : R(X,Y )=1,xi6

=yi

Then Q2 (f ) = Ω

1
δ0δ1

.

(cid:16)q

(cid:17)

I now prove the main result of the section.

6
72

Theorem 32 For all f (partial or total) and all X,

QC X (f ) = Θ

RC X (f )

.

(cid:18)q

(cid:19)

Proof. Let (λ1, . . . , λn) be an optimal nonadaptive randomized veriﬁer for X, and

let

S = λ1 +

+ λn.

· · ·

First, QCX (f ) = O
tion of basis states querying index i is within a constant factor of λi/S.
(cid:16)
n2 basis states.) Let

. We can run a “weighted Grover search,” in which the propor-
(It suﬃces to use
iterations

= f (X)

(cid:17)
=

√S

√S

, O

Y

Y : f (Y )
{

; then for any Y
}

∈ Y

suﬃce to ﬁnd a disagreement with X with probability Ω (1). Second, QCX (f ) = Ω
Consider a matrix game in which Alice chooses an index i to query and Bob chooses Y
(cid:16)
Alice wins if and only if yi 6
bility O (1/S), since otherwise Alice’s strategy would yield a veriﬁer (λ′1, . . . , λ′n) with

.
;
(cid:17)
∈ Y
If both players are rational, then Alice wins with proba-

= xi.

√S

(cid:16)

(cid:17)

Hence by the minimax theorem, there exists a distribution µ over

such that for every i,

Y

λ′1 +

· · ·

+ λ′n = o (S) .

Pr
µ
Y
∈

[yi 6

= xi] = O

1
S

.

(cid:19)

(cid:18)

. Also, let R (Y, Z) = 1 if and only
Let β (X) = 1 and let β (Y ) = µ (Y ) for each Y
if Z = X for each Y
. Then we can take δf (Y ) = 1 and δf (X) = O (1/S)
in Theorem 31. So the quantum query complexity of distinguishing X from an arbitrary
Y

and Z /

is Ω

∈ Y

∈ Y

∈ Y

√S

.

∈ Y

(cid:16)

(cid:17)

8.4 Quantum Lower Bound for Total Functions

The goal of this section is to show that

R0 (f ) = O

Q2 (f )2 Q0 (f ) log n

(cid:16)

(cid:17)

for all total f . Say that a real multilinear polynomial p (x1, . . . , xn) nondeterministically
represents f if for all X
= 0. Let ndeg (f ) be the
∈ {
minimum degree of a nondeterministic polynomial for f . Also, given such a polynomial p,
p if M2 contains every variable in M1. A
say that a monomial M1 ∈
monomial M is called a maxonomial if it is not covered by any other monomial of p. The
following is a simple generalization of a lemma attributed in [78] to Nisan and Smolensky.

p is covered by M2 ∈

= 0 if and only if f (X)

n, p (X)

0, 1
}

Lemma 33 (Nisan-Smolensky) Let p nondeterministically represent f . Then for every
maxonomial M of p and X
=
f (X), where X (B) is obtained from X by ﬂipping the variables in B.

1 (0), there is a set B of variables in M such that f

X (B)

f −

∈

(cid:0)

(cid:1)

6
6
6
6
Proof. Obtain a restricted function g from f , and a restricted polynomial q from
p, by setting each variable outside of M to xi. Then g cannot be constant, since its
representing polynomial q contains M as a monomial. Thus there is a subset B of variables
X (B)
in M such that g

= 1, and hence f

X (B)

= 1.

73

Using Lemma 33, de Wolf [244] showed that D (f )

C (f ) ndeg (f ) for all total
(cid:0)
(cid:0)
C (f ) deg (f ) due to Buhrman and de Wolf [78].
f , slightly improving the result D (f )
In Theorem 35, I will give an analogue of this result for randomized query and certiﬁcate
complexities. However, I ﬁrst need a probabilistic lemma.

≤

≤

(cid:1)

(cid:1)

Lemma 34 Suppose we repeatedly apply the following procedure: ﬁrst identify the set B of
maxonomials of p, then ‘shrink’ each M
B with (not necessarily independent) probability
at least 1/2. Shrinking M means replacing it by an arbitrary monomial of degree deg (M )
−
1. Then with high probability p is a constant polynomial after O (deg (p) log n) iterations.

∈

Proof. For any set A of monomials, consider the weighting function

ω (A) =

deg (M )!

A
XM
∈
Initially ω (S)

Let S be the set of monomials of p.
ω (S) = 0. The claim is that at every iteration, ω (B)
covered by some M
∈
deg (M ∗) = ℓ. Hence

B, but a given M

≤

∈

1

ndeg(p) deg (p)!, and we are done when
B is
distinct M ∗ with

e ω (S). For every M ∗ ∈

≥

deg(M )
ℓ

S

\

B can cover at most

(cid:0)

(cid:1)

ω (S

B)

\

≤

B
XM
∈

≤

≤

B
XM
∈
(e
−

deg(M )

1

−

Xℓ=0
(cid:0)
deg (M )!

(cid:18)

1) ω (B) .

deg(M )
ℓ

ℓ!

(cid:1)
1
2!

+

1
1!

+

· · ·

(cid:19)

At every iteration, the contribution of each M

B to ω (A) has at least 1/2
1)! (or to 0 if deg (M ) = 1). When
probability of shrinking from deg (M )! to (deg (M )
this occurs, the contribution of M is at least halved. Hence ω (S) decreases by an expected
amount at least 1

−

∈

4e ω (S). Thus after

log4e/(4e

1)

−

2ndeg(p) deg (p)!
(cid:17)
(cid:16)

= O (deg (p) log n)

iterations, the expectation of ω (S) is less than 1/2, so S is empty with probability at least
1/2.

I can now prove the main result.2

Theorem 35 For total f ,

R0 (f ) = O (RC (f ) ndeg (f ) log n) .

2The proof of Theorem 35 that I gave previously [4] makes a claim that is both superﬂuous for proving
I am grateful to Gatis Midrijanis for pointing this out to me.

the theorem and false.

74

Proof. The algorithm is as follows.

Repeat

Choose a 0-input X compatible with all queries made so far3

Query a randomized 0-certificate for X

Until f has been restricted to a constant function

Let p be a polynomial that nondeterministically represents f . Then the key fact
is that for every 0-input X, when we query a randomized 0-certiﬁcate for X we “hit” each
maxonomial M of p with probability at least 1/2. Here hitting M means querying a
variable in M . This is because, by Lemma 33, it is possible to change f (X) from 0 to 1
just by ﬂipping variables in M . So a randomized certiﬁcate would be incorrect if it probed
those variables with probability less than 1/2.

Therefore, each iteration of the algorithm shrinks each maxonomial of p with
probability at least 1/2. It follows from Lemma 34 that the algorithm terminates after an
expected number of iterations O (deg (p) log n).

Buhrman et al. [45] showed that ndeg (f )

2 Q0 (f ). Combining this with Theo-

rems 32 and 35 yields a new relation between classical and quantum query complexity.

≤

Corollary 36 For all total f ,

R0 (f ) = O

Q2 (f )2 Q0 (f ) log n
(cid:16)

(cid:17)

.

The best previous relation of this kind was R0 (f ) = O

, due to
It is worth mentioning another corollary of Theorems 32 and 35, this one

Q2 (f )2 Q0 (f )2

(cid:17)

(cid:16)

de Wolf [244].
purely classical:

Corollary 37 For all total f ,

R0 (f ) = O (R2 (f ) ndeg (f ) log n)

Previously, no relation between R0 and R2 better than R0 (f ) = O

R2 (f )3

was

known (although no asymptotic gap between R0 and R2 is known either [210]).

(cid:16)

(cid:17)

8.5 Asymptotic Gaps

Having related RC (f ) and QC (f ) to other query complexity measures in Section 8.4, in
what follows I seek the largest possible asymptotic gaps among the measures. In particular,
C (f )0.907
, as
I give a total f for which RC (f ) = Θ
(cid:17)
(cid:16)
RC (f )0.922
(cid:16)

3Clearly, as long as f is not a constant function, there exists a 0-input X compatible with all queries

. Although these gaps are the largest

well as a total f for which bs (f ) = Θ

QC (f )2.205
(cid:16)

and hence C (f ) = Θ

(cid:17)

(cid:17)

made so far.

of which I know, Section 8.5.1 shows that no ‘local’ technique can improve the relations
RC (f )2
. Finally, Section 8.5.2 uses combinatorial
C (f ) = O
designs to construct a symmetric partial f for which RC (f ) and QC (f ) are O (1), yet
(cid:17)
(cid:16)
Q2 (f ) = Ω (n/ log n).

bs (f )2
(cid:16)

and RC (f ) = O

(cid:17)

75

Wegener and Z´adori [240] exhibited total Boolean functions with asymptotic gaps
between C (f ) and bs (f ).
with an asymp-
totic gap between C (gt) and RC (gt). Let g1 (x1, . . . , x29) equal 1 if and only if the Hamming
weight of its input is 13, 14, 15, or 16.
(The parameter 29 was found via computer search
to produce a maximal separation.) Then for t > 1, let

In similar fashion, I give a function family

gt}

{

where X1 is the ﬁrst 29t
−
let

gt (x1, . . . , x29t ) = g0 [gt

−

1 (X1) , . . . , gt

1 (X29)]

1 input bits, X2 is the second 29t
−

−
1, and so on. For k

0, 1
}

∈ {

,

bs k (f ) = max
f (X)=k
C k (f ) = max
f (X)=k

bs X (f ) ,

C X (f ) .

Then since bs0 (g1) = bs1 (g1) = 17, we have bs (gt) = 17t. On the other hand, C0 (g1) = 17
but C1 (g1) = 26, so

C 1 (gt) = 13 C 1 (gt
1) + 13 C 0 (gt
−
−
1) , C 0 (gt
C 1 (gt
C 0 (gt) = 17 max
−
−
(cid:8)
22.725t

1) ,
1)

.

(cid:9)

. We can now show a gap between C

Solving this recurrence yields C (gt) = Θ
and RC.

Proposition 38 RC (gt) = Θ

C (gt)0.907
(cid:16)

(cid:1)

(cid:0)

.

(cid:17)

−

(cid:17)

C (gt)0.907
(cid:16)

1 (Xi) = 1
}

1 (Xi). Let I0 =

Proof. Since bs (gt) = Ω

, it suﬃces to show that RC (gt) = O (bs (gt)).
Let X be
and I1 =
1 (Xi) = 0
}
I1 uniformly at random; other-

The randomized veriﬁer V chooses an input variable to query as follows.
29
the claimed input, and let K =
i=1 gt
−
i : gt
{
wise A chooses an i

. With probability pK, V chooses an i

I0 uniformly at random. Here pK is as follows.
[17, 29]
1

∈
K [0, 12]
pK
Once i is chosen, V repeats the procedure for Xi, and continues recursively in this
= gt (Y ), then
= yj with probability at

manner until reaching a variable yj to query. One can check that if gt (X)
gt
−
least 1/17t, and RC (gt) = O

1 (Yi) with probability at least 1/17. Hence xj 6

i : gt
{

1 (Xi)

14
7
12

13
13
17

15
5
12

16
4
17

= gt

17t

P

∈

0

−

−

.

By Theorem 32, it follows that C (gt) = Θ
(cid:0)

. This oﬀers a surpris-
ing contrast with the query complexity setting, where the best known gap between the
deterministic and quantum measures is quadratic (D (f ) = Θ

QC (gt)2.205
(cid:16)

(cid:17)
Q2 (f )2

).

(cid:1)

(cid:16)

(cid:17)

6
6
RC (f )2
(cid:16)

(cid:17)

76

{

gt}

The family

happens not to yield an asymptotic gap between bs (f ) and RC (f ).
The reason is that any input to g0 can be covered perfectly by sensitive blocks of minimum
size, with no variables left over. In general, though, one can have bs (f ) = o (RC (f )). As
reported by Bublitz et al. [74], M. Paterson found a total Boolean function h1 (x1, . . . , x6)
such that CX (h1) = 5 and bsX (h1) = 4 for all X. Composing h1 recursively yields
C (ht)0.861
, both of which are the largest such
bs (ht) = Θ
gaps of which I know.

RC (ht)0.922
(cid:16)

and bs (ht) = Θ

(cid:17)

(cid:17)

(cid:16)

8.5.1 Local Separations

It is a longstanding open question whether the relation C (f )
is tight. As a ﬁrst step, one can ask whether the relations C (f ) = O

bs (f )2 due to Nisan [183]
and

≤

RC (f ) = O

bs (f )2

are tight.

In this section I introduce a notion of local proof in query

complexity, and then show there is no local proof that C (f ) = o

(cid:16)

(cid:17)

RC (f )2

or that RC (f ) =

bs (f )2

(cid:17)

(cid:16)

. This implies that proving either result would require techniques unlike those
o
that are currently known. My inspiration comes from computational complexity, where
researchers ﬁrst formalized known methods of proof, including relativizable proofs [41] and
natural proofs [200], and then argued that these methods were not powerful enough to
resolve the ﬁeld’s outstanding problems.

(cid:16)

(cid:17)

Let G (f ) and H (f ) be query complexity measures obtained by maximizing over

all inputs—that is,

G (f ) = max

X

H (f ) = max

X

GX (f ) ,

H X (f ) .

}

⊆ {

a minimal block on X if B is sensitive on X (meaning f

=
Call B
1, . . . , n
f (X)), and no sub-block B′ ⊂
(X)
(cid:1)
consist of X together with X (B) for every minimal block B of X. Consider a proof that
I call the proof local if it proceeds by
G (f ) = O (t (H (f ))) for some nondecreasing t.
showing that for every input X,

B is sensitive on X. Also, let X’s neighborhood
(cid:0)

N

X (B)

GX (f ) = O

t

H Y (f )

.

max
∈N

(X)

Y

(cid:18)

(cid:19)
(cid:1)(cid:9)
bs (f )2 is local. For each X,
As a canonical example, Nisan’s proof [183] that C (f )
Nisan observes that (i) a maximal set of disjoint minimal blocks is a certiﬁcate for X, (ii)
such a set can contain at most bsX (f ) blocks, and (iii) each block can have size at most
(X) bsY (f ). Another example of a local proof is the proof in Section 8.3 that
maxY
RC (f ) = O

QC (f )2

∈N

≤

(cid:8)

(cid:0)

.

(cid:16)

(cid:17)

Proposition 39 There is no local proof showing that C (f ) = o

o

bs (f )2

for all total f .

(cid:16)

(cid:17)

RC (f )2
(cid:16)

(cid:17)

or that RC (f ) =

6
77

√n
⌉

(f ) = n

+ 1, but RC0n

Proof. The ﬁrst part is easy: let f (X) = 1 if

(0n). For the second part, arrange the input variables in a lattice of size √n

denotes the
Hamming weight of X), and f (X) = 0 otherwise. Consider the all-zero input 0n. We
have C0n
(f ) = O (√n), and indeed RCY (f ) = O (√n) for all
√n.
Y
Take m = Θ
, and let g (X) be the monotone Boolean function that outputs 1 if and
m. This is a square of 1’s that can wrap around
only if X contains a 1-square of size m
(cid:1)
the edges of the lattice; note that only the variables along the sides must be set to 1, not
those in the interior. An example input, with a 1-square of size 3

3, is shown below.

√n (where

n1/3

X
|

X
|

∈ N

| ≥

− ⌈

×

×

(cid:0)

|

×

0 0 0 0 0
0 0 0 0 0
1 0 0 1 1
1 0 0 1 0
1 0 0 1 1

n1/3
(g) = Θ
m. Also, bsY (g) = Θ

Clearly bs0n
, since there can be at most n/m2 disjoint 1-squares of size
n1/3
m
for any Y that is 0 except for a single 1-square. On the
other hand, if we choose uniformly at random among all such Y ’s, then at any lattice site
(g) = Ω
i, PrY [yi = 1] = Θ

(cid:1)
(cid:0)
(cid:1)
. Hence RC0n

n2/3

n−

2/3

×

(cid:0)

.

8.5.2 Symmetric Partial Functions

(cid:0)

(cid:1)

(cid:0)

(cid:1)

If f is partial, then QC (f ) can be much smaller than Q2 (f ). This is strikingly illustrated
by the collision problem:
let Col (Y ) = 0 if Y = y1 . . . yn is a one-to-one sequence and
Col (Y ) = 1 if Y is a two-to-one sequence, promised that one of these is the case. Then
RC (Col) = QC (Col) = O (1), since every one-to-one input diﬀers from every two-to-one
input on at least n/2 of the yi’s. On the other hand, Chapter 6 showed that Q2 (Col) =
Ω

n1/5

.

(cid:1)

(cid:0)

n1/3

From the example of the collision problem, it is tempting to conjecture that (say)
Q2 (f ) = O
whenever QC (f ) = O (1)—that is, ‘if every 0-input is far from every
1-input, then the quantum query complexity is sublinear.’ Here I disprove this conjecture,
, a function
even for the special case of symmetric functions such as Col. (Given a ﬁnite set
H
n is called symmetric if x1 . . . xn ∈ S
f :
implies xσ(1) . . . xσ(n) ∈ S
where
0, 1
}
and f (x1 . . . xn) = f
for every permutation σ.)
The proof uses the following lemma, which can be found in Nisan and Wigderson

S ⊆ H
xσ(1) . . . xσ(n)

S → {

(cid:1)

(cid:0)

[185] for example.

(cid:0)

(cid:1)

Lemma 40 (Nisan-Wigderson) For any γ > 1, there exists a family of sets

such that m = Ω

2n/γ

,

Ai|
|

A1, . . . , Am ⊆ {
= n for all i, and

1, . . . ,

Ai ∩
|

γn

⌈
⌉}
Aj| ≤

n/γ for all i

= j.

A lemma due to Ambainis [26] is also useful. Let f :

(cid:0)

(cid:1)

n be a partial Boolean function, and let p :
0, 1
0, 1
{
}
}
polynomial. We say that p approximates f if (i) p (X)
), and (ii)
(not merely those in

g (X)

{

n

∈

p (X)
|

−

| ≤

S

1/3 for every X

.

∈ S

0, 1
S ⊆
}
R be a real-valued multilinear
n

→
[0, 1] for every input X

S → {

where

0, 1
}

∈ {

6
78

Lemma 41 (Ambainis) At most 2O(∆(n,d)dn2) distinct Boolean functions (partial or total)
can be approximated by polynomials of degree d, where ∆ (n, d) =

.

d
i=0

n
i

The result is an easy consequence of Lemmas 40 and 41.

P

(cid:0)

(cid:1)

Theorem 42 There exists a symmetric partial f for which QC (f ) = O (1) and Q2 (f ) =
Ω (n/ log n).

}

where

S ⊆ {

S → {

0, 1
}

1, . . . , 3n

1, . . . , 3n

Proof. Let f :

be as in Lemma 40. We put x1, . . . , xn in

= Aj for some j. Clearly QC (f ) = O (1), since if i

.
if and only if
Let A1, . . . , Am ⊆ {
(cid:0)
(cid:1)
= j then every permutation
x1, . . . , xn}
{
of Ai diﬀers from every permutation of Aj on at least n/3 indices. The number of symmetric
as above is 2m = 2Ω(2n/3). We can convert any such f to a Boolean function
f with
g on O (n log n) variables. But Beals et al. [45] showed that, if Q2 (g) = T , then g is
approximated by a polynomial of degree at most 2T . So by Lemma 41, if Q2 (g)
T for
every g then

≤

S

S

}

n, and let m = Ω

2n/3

∆ (n log n, 2T )

2T

·

·

(n log n)2 = Ω

and we solve to obtain T = Ω (n/ log n).

2n/3
(cid:16)

(cid:17)

8.6 Open Problems

(cid:17)

g

(cid:16)p

RC (f )

, where

deg (f ) = Ω

Is
deg (f ) is the minimum degree of a polynomial approx-
imating f ? In other words, can one lower-bound QC (f ) using the polynomial method of
g
Beals et al. [45], rather than the adversary method of Ambainis [27]?
RC (f )2
(cid:16)
R2 (f )2
(cid:16)

If so we obtain the new relations R0 (f ) =

Q2 (f )4
(cid:16)

Also, is R0 (f ) = O

and R0 (f ) = O

O

(cid:17)

(cid:17)

(cid:17)

?

.

6
79

Chapter 9

The Need to Uncompute

Like a classical algorithm, a quantum algorithm can solve problems recursively by
calling itself as a subroutine. When this is done, though, the algorithm typically needs
to call itself twice for each subproblem to be solved. The second call’s purpose is to
uncompute ‘garbage’ left over by the ﬁrst call, and thereby enable interference between
diﬀerent branches of the computation. Of course, a factor of 2 increase in running time
hardly seems like a big deal, when set against the speedups promised by quantum computing.
The problem is that these factors of 2 multiply, with each level of recursion producing
an additional factor. Thus, one might wonder whether the uncomputing step is really
necessary, or whether a cleverly designed algorithm might avoid it. This chapter gives the
ﬁrst nontrivial example in which recursive uncomputation is provably necessary.

The example concerns a long-neglected problem called Recursive Fourier Sampling
(henceforth RFS), which was introduced by Bernstein and Vazirani [55] in 1993 to prove the
ﬁrst oracle separation between BPP and BQP. Many surveys on quantum computing pass
directly from the Deutsch-Jozsa algorithm [95] to the dramatic results of Simon [220] and
Shor [219], without even mentioning RFS. There are two likely reasons for this neglect.
First, the RFS problem seems artiﬁcial.
It was introduced for the sole purpose of proving
an oracle result, and is unlike all other problems for which a quantum speedup is known.
(I will deﬁne RFS in Section 9.1; but for now, it involves a tree of depth log n, where each
vertex is labeled with a function to be evaluated via a Fourier transform.) Second, the
speedup for RFS is only quasipolynomial (n versus nlog n), rather than exponential as for
the period-ﬁnding and hidden subgroup problems.

Nevertheless, I believe that RFS merits renewed attention—for it serves as an im-
portant link between quantum computing and the ideas of classical complexity theory. One
reason is that, although other problems in BQP—such as the factoring, discrete logarithm,
and ‘shifted Legendre symbol’ problems [232]—are thought to be classically intractable,
these problems are quite low-level by complexity-theoretic standards. They, or their as-
coNP.1 By contrast, Bernstein and Vazirani [55]
sociated decision problems, are in NP
showed that, as an oracle problem, RFS lies outside NP and even MA (the latter result is
unpublished, though not diﬃcult). Subsequently Watrous [239] gave an oracle A, based

∩

1For the shifted Legendre symbol problem, this is true assuming a number-theoretic conjecture of Boneh

and Lipton [61].

80

on an unrelated problem, for which BQPA
MAA.2 Also, Green and Pruim [135] gave an
oracle B for which BQPB
. However, Watrous’ problem was shown by Babai [38]
to be in AM, while Green and Pruim’s problem is in BPP. Thus, neither problem can be
used to place BQP outside higher levels of the polynomial hierarchy.

PNPB

6⊂

6⊂

On the other hand, Umesh Vazirani and others have conjectured that RFS is not in
PHA.
PH, from which it would follow that there exists an oracle A relative to which BQPA
Proving this is, in my view, one of the central open problems in quantum complexity theory.
Its solution seems likely to require novel techniques for constant-depth circuit lower bounds.3
In this chapter I examine the RFS problem from a diﬀerent angle. Could Bernstein
and Vazirani’s quantum algorithm for RFS be improved even further, to give an exponential
speedup over the classical algorithm? And could we use RFS, not merely to place BQP
outside of PH relative to an oracle, but to place it outside of PH with (say) a logarithmic
number of alternations?

6⊂

My answer to both questions is a strong ‘no.’
on RFS, and show that all of them fall into one of two classes:

I study a large class of variations

(1) a trivial class, for which there exists a classical algorithm making only one query, or

(2) a nontrivial class, for which any quantum algorithm needs 2Ω(h) queries, where h is the
height of the tree to be evaluated. (By comparison, the Bernstein-Vazirani algorithm
uses 2h queries, because of its need to uncompute garbage recursively at each level of
the tree.)

Since nh queries always suﬃce classically, this dichotomy theorem implies that the speedup
aﬀorded by quantum computers is at most quasipolynomial. It also implies that (nontrivial)
RFS is solvable in quantum polynomial time only when h = O (log n).

Intuitively, given a Boolean function g :

The plan is as follows. In Section 9.1 I deﬁne the RFS problem, and give Bernstein
and Vazirani’s quantum algorithm for solving it. In Section 9.2, I use the adversary method
of Ambainis [27] to prove a lower bound on the quantum query complexity of any RFS
variant. This bound, however, requires a parameter that I call the “nonparity coeﬃcient”
to be large.
, the nonparity
coeﬃcient measures how far g is from being the parity of some subset of its input bits—not
under the uniform distribution over inputs (the standard assumption in Fourier analysis),
but under an adversarial distribution. The crux of the argument is that either the nonparity
coeﬃcient is zero (meaning the RFS variant in question is trivial), or else it is bounded below
by a positive constant. This statement is proved in Section 9.2, and seems like it might be
of independent interest. Section 9.3 concludes with some open problems.

0, 1
}
{

0, 1
}

→ {

n

2Actually, to place BQP outside MA relative to an oracle, it suﬃces to consider the complement of Simon’s

problem (“Does f (x) = f (x

s) only when s = 0?”).

⊕

3For the RFS function can be represented by a low-degree real polynomial—this follows from the existence
of a polynomial-time quantum algorithm for RFS, together with the result of Beals et al. [45] relating
quantum algorithms to low-degree polynomials. As a result, the circuit lower bound technique of Razborov
[198] and Smolensky [223], which is based on the nonexistence of low-degree polynomials, seems unlikely to
work. Even the random restriction method of Furst et al. [120] can be related to low-degree polynomials,
as shown by Linial et al. [166].

9.1 Preliminaries

81

, and are promised that there exists a secret string s

In ordinary Fourier sampling, we are given oracle access to a Boolean function A :
0, 1
→
{
}
n such that A (x) =
0, 1
{
}
s
x (mod 2) for all x. The problem is to ﬁnd s—or rather, since we need a problem with
is some known
Boolean output, the problem is to return g (s), where g :
Boolean function. We can think of g (s) as the “hard-core bit” of s, and can assume that
g itself is eﬃciently computable, or else that we are given access to an oracle for g.

0, 1
}
{

0, 1
}

0, 1
}

→ {

∈ {

n

·

n

To obtain a height-2 recursive Fourier sampling tree, we simply compose this
problem. That is, we are no longer given direct access to A (x), but instead are promised
n is the secret string for another Fourier sampling
that A (x) = g (sx), where sx ∈ {
0, 1
}
y (mod 2).
problem. A query then takes the form (x, y), and produces as output Ax (y) = sx·
x (mod 2) for all x,
As before, we are promised that there exists an s such that A (x) = s
meaning that the sx strings must be chosen consistent with this promise. Again we must
return g (s).

·

Continuing, we can deﬁne height-h recursive Fourier sampling, or RFSh, recur-
sively as follows. We are given oracle access to a function A (x1, . . . , xh) for all x1, . . . , xh ∈
0, 1
}
{

n, and are promised that

(1) for each ﬁxed x∗1, A (x∗1, x2, . . . , xh) is an instance of RFSh
; and

answer bit b (x∗1)

0, 1
}

∈ {

1 on x2, . . . , xh, having

−

(2) there exists a secret string s

0, 1
}

∈ {

n such that b (x∗1) = s

·

x∗1 (mod 2) for each x∗1.

Again the answer bit to be returned is g (s). Note that g is assumed to be
the same everywhere in the tree—though using the techniques in this chapter, it would
be straightforward to generalize to the case of diﬀerent g’s. As an example that will be
used later, we could take g (s) = gmod 3 (s), where gmod 3 (s) = 0 if
0 (mod 3) and
denotes the Hamming weight of s. We do not want to take
gmod 3 (s) = 1 otherwise, and
g to be the parity of s, for if we did then g (s) could be evaluated using a single query. To
see this, observe that if x is the all-1’s string, then s

x (mod 2) is the parity of s.

| ≡

s
|

s
|

|

By an ‘input,’ I will mean a complete assignment for the RFS oracle (that is,
I will sometimes refer also to an ‘RFS tree,’ where each
A (x1, . . . , xh) for all x1, . . . , xh).
vertex at distance ℓ from the root has a label x1, . . . , xℓ.
If ℓ = h then the vertex is a leaf;
otherwise it has 2n children, each with a label x1, . . . , xℓ, xℓ+1 for some xℓ+1. The subtrees
of the tree just correspond to the sub-instances of RFS.

·

Bernstein and Vazirani [55] showed that RFSlog n, or RFS with height log n (all
I
0 be an oracle that, for each n, encodes an
.

logarithms are base 2), is solvable on a quantum computer in time polynomial in n.
include a proof for completeness. Let A = (An)n
instance of RFSlog n whose answer is Ψn. Then let LA be the unary language
Lemma 43 LA ∈

BQPA for any choice of A.

0n : Ψn = 1
}

EQPA

⊆

{

≥

Proof. RFS1 can be solved exactly in four queries, with no garbage bits left over.

The algorithm is as follows: ﬁrst prepare the state

n/2

2−

x

A (x)

,

i

i |

n |
}

0,1
Xx
∈{

using one query to A. Then apply a phase ﬂip conditioned on A (x) = 1, and uncompute
A (x) using a second query, obtaining

82

n/2

2−

1)A(x)

(
−

n

.

x
i

|

0,1
Xx
∈{

}

It can be checked that the
Then apply a Hadamard gate to each bit of the
using
resulting state is simply
two more queries to A, to obtain
. To solve RF Slog n (n), we simply apply the
g (s)
i
above algorithm recursively at each level of the tree. The total number of queries used is
4log n = n2.

. One can then compute
i

register.
s

and uncompute

g (s)
i

x
i
|

s
|

i |

s

i

|

|

|

One can further reduce the number of queries to 2log n = n by using the “one-call

kickback trick,” described by Cleve et al. [87]. Here one prepares the state

n/2

2−

x

i ⊗

n |
}

0,1
Xx
∈{

|

1
i − |
√2

0
i

and then exclusive-OR’s A (x) into the second register. This induces the desired phase
1)A(x) without the need to uncompute A (x). However, one still needs to uncompute
(
s
|
−
after computing

i

A remark on notation: to avoid confusion with subscripts, I denote the ith bit of

.
g (s)
i

|

string x by x [i].

9.2 Quantum Lower Bound

In this section I prove a lower bound on the quantum query complexity of RFS. Crucially,
the bound should hold for any nontrivial one-bit function of the secret strings, not just a
speciﬁc function such as gmod 3 (s) deﬁned in Section 9.1. Let RFSg
h be height-h recursive
Fourier sampling in which the problem at each vertex is to return g (s). The following
notion turns out to be essential.

(partial or total), the nonpar-
0, 1
Deﬁnition 44 Given a Boolean function g :
}
{
ity coeﬃcient µ (g) is the largest µ∗ for which there exist distributions D0 over the 0-inputs
of g, and D1 over the 1-inputs, such that for all z
s0, and all 1-inputs
s1, we have

n, all 0-inputs

0, 1
}

0, 1
}

→ {

∈ {

n

b

Pr
D0,s1

∈

s0

D1

∈

z

[s0 ·

s1 ·

≡

z (mod 2)

z

s1 ·

s0 ·

≡

∨

b
z (mod 2)]
≥

µ∗.

Loosely speaking, the nonparity coeﬃcient is high if there exist distributions over
0-inputs and 1-inputs that make g far from being a parity function of a subset of input bits.
The following proposition develops some intuition about µ (g).

b

b

Proposition 45

(i) µ (g)

≤

3/4 for all nonconstant g.

83

(ii) µ (g) = 0 if and only if g can be written as the parity (or the NOT of the parity) of a

subset B of input bits.

Proof.

(i) Given any s0 6

=

s1 and s1 6

=

s0, a uniform random z will satisfy

z

z (mod 2)
s1 ·
b

s0 ·
[s0 ·
Pr
b
z
s0 then this probability will be 1/2; otherwise it will be 1/4.) So
b

(If s0 ⊕
certainly there is a ﬁxed choice of z that works for random s0 and s1.

s1 = s1 ⊕

z (mod 2)]

s1 ·

6≡

≥

6≡

∧

b

z

.

1
4

b

b

(ii) For the ‘if’ direction, take z [i] = 1 if and only if i

s1 arbitrarily.
This ensures that µ∗ = 0. For the ‘only if’ direction, if µ (g) = 0, we can choose D0
to have support on all 0-inputs, and D1 to have support on all 1-inputs. Then there
b
z is constant
z is constant as we range over 0-inputs, and s1 ·
must be a z such that s0 ·
B if and only if z [i] = 1.
as we range over 1-inputs. Take i

B, and choose

s0 and

∈

b

∈

If µ (g) = 0, then RFSg
47 will show that for all g (partial or total),

h is easily solvable using a single classical query. Theorem

Q2

RFSg
h

= Ω

(cid:0)

(cid:1)

h/2

1
µ (g)

(cid:19)

,

!

1

 (cid:18)

−

where Q2 is bounded-error quantum query complexity as deﬁned in Section 5.1.
In other
words, any RFS problem with µ bounded away from 0 requires a number of queries expo-
nential in the tree height h.

However, there is an essential further part of the argument, which restricts the
values of µ (g) itself. Suppose there existed a family
of ‘pseudoparity’ functions: that
is, µ (gn) > 0 for all n, yet µ (gn) = O(1/ log n). Then the best bound obtainable from
might still be solvable
Theorem 47 would be Ω
in quantum polynomial time. On the other hand, it would be unclear a priori how to solve
RFSg
classically with a logarithmic number of alternations. Theorem 49 will rule out
this scenario by showing that pseudoparity functions do not exist: if µ (g) < 0.146 then g
is a parity function, and hence µ (g) = 0.

, suggesting that RFSg
(cid:17)

(1 + 1/ log n)h/2
(cid:16)

gn}
{

log2 n

log2 n

The theorem of Ambainis that we need is his “most general” lower bound from [27],
which he introduced to show that the quantum query complexity of inverting a permutation
is Ω (√n), and which we used already in Chapter 7. Let us restate the theorem in the present
context.

Theorem 46 (Ambainis) Let X
f . Let R (x, y)

⊆

f −

1 (0) and Y

0 be a symmetric real-valued relation function, and for x

⊆

f −

1 (1) be sets of inputs to function
Y ,

X, y

∈

∈

≥

and index i, let

θ (x, i) =

θ (y, i) =

Y : x[i]

y∗∈

y∗∈
P
X : x∗[i]

x∗∈

=y∗[i] R (x, y∗)
Y R (x, y∗)

=y[i] R (x∗, y)

Y R (x∗, y)

y∗∈

P

P

84

,

,

where the denominators are all nonzero. Then Q2 (f ) = O (1/υ) where

P

υ =

x

X, y

∈

∈

max

Y, i : R(x,y)>0, x[i]

=y[i]

θ (x, i) θ (y, i).

We are now ready to prove a lower bound for RFS.

p

Theorem 47 For all g (partial or total), Q2

RFSg
h

= Ω

(1

µ (g))−

h/2

.

−

(cid:1)

∈

(cid:17)
(cid:16)
Proof. Let X be the set of all 0-inputs to RFSg
(cid:0)
h, and let Y be the set of all 1-
inputs. We will weight the inputs using the distributions D0, D1 from the deﬁnition of the
nonparity coeﬃcient µ (g). For all x
X, let p (x) be the product, over all vertices v in the
RFS tree for x, of the probability of the secret string s at v, if s is drawn from Dg(s) (where
we condition on v’s output bit, g (s)). Next, say that x
Y diﬀer minimally if,
for all vertices v of the RFS tree, the subtrees rooted at v are identical in x and in y whenever
If x and y diﬀer minimally, then we
the answer bit g (s) at v is the same in x and in y.
will set R (x, y) = p (x) p (y); otherwise we will set R (x, y) = 0. Clearly R (x, y) = R (y, x)
µ (g))h for all x, y
for all x
−
that diﬀer minimally and all i such that x [i]
Y is chosen with
= y [i]. For suppose y∗ ∈
X is chosen with probability proportional to
probability proportional to R (x, y∗), and x∗ ∈
R (x∗, y). Then θ (x, i) θ (y, i) equals the probability that we would notice the switch from
x to y∗ by monitoring i, times the probability that we would notice the switch from y to
x∗.

Y . Furthermore, we claim that θ (x, i) θ (y, i)

X and y

X, y

(1

≤

∈

∈

∈

∈

−

∈ {

0, 1
}

1, . . . , h
}

. Also, let zj ∈ {

Let vj be the jth vertex along the path in the RFS tree from the root to the leaf
n be the label of the edge between
vertex i, for all j
1 and vj, and let sx,j and sy,j be the secret strings at vj in x and y respectively. Then
vj
= g (sy,j) for all j—for otherwise the
since x and y diﬀer minimally, we must have g (sx,j)
subtrees rooted at vj would be identical, which contradicts the assumption x [i]
= y [i].
So we can think of the process of choosing y∗ as ﬁrst choosing a random s′x,1 from D1
so that 1 = g
= g (sx,1) = 0, then choosing a random s′x,2 from D1
g(sx,2) so that
= g (sx,2), and so on. Choosing x∗ is analogous, except that whenever we used D0 in
g
choosing y∗ we use D1, and vice versa. Since the 2h secret strings sx,1, . . . , sx,h, sy,1, . . . , sy,h
to be updated are independent of one another, it follows that

s′x,1

s′x,2

(cid:1)

(cid:0)

(cid:0)

(cid:1)

−

Pr [y∗ [i]

= x [i]] Pr [x∗ [i]

= y [i]] =

h

Yj=1
h

Pr
D0
∈

s

[s

zj 6≡

sx,j ·

·

zj] Pr
D1
s
∈

[s

zj 6≡

sy,j ·

·

zj]

µ (g))

(1

−

µ (g))h

≤

Yj=1
= (1

−

6
6
6
6
6
6
6
6
6
6
85

by the deﬁnition of µ (g). Therefore

by Theorem 46.

Q2

RFSg
h

= Ω

(1

(cid:0)

(cid:1)

(cid:16)

µ (g))−

h/2

−

(cid:17)

Before continuing further, let me show that there is a natural, explicit choice of
g—the function gmod 3 (s) from Section 9.1—for which the nonparity coeﬃcient is almost
3/4. Thus, for g = gmod 3, the algorithm of Lemma 43 is essentially optimal.

Proposition 48 µ (gmod 3) = 3/4

O (1/n).

−

Proof. Let n

n/6
⌋
(so gmod 3 (s) = 0); likewise let D1 be the uniform distribution over s with
+ 2
(gmod 3 (s) = 1). We consider only the case of s drawn from D0; the D1 case is analogous.
We will show that for any z,

6. Let D0 be the uniform distribution over all s with

⌊
n/6
⌋

s
|
|
= 3

= 3

s
|

≥

⌊

|

Pr
D0
∈

[s

z

·

≡

0]

−

1
2

= O

1
n

(cid:19)

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Assume without loss of generality that 1

(all congruences are mod 2). The theorem then follows, since by the deﬁnition of the
nonparity coeﬃcient, given any z the choices of s0 ∈
D0 and s1 ∈
n/2 (if
z
| ≤
≤ |
= 1, then clearly
z
If
by its complement). We apply induction on
|
|
1
2 ±

D1 are independent.
z
|

> n/2, then replace z

n/6
⌋

0] = 3

/n =

Pr [s

1
n

.
|

≡

O

z

z

⌊

.

|

|

·

(cid:18)

(cid:19)

For
the other 1’s. Suppose the proposition holds for

2, let z = z1 ⊕

| ≥

z
|

z2, where z2 contains only the rightmost 1 of z and z1 contains all

Pr [s

z

·

≡

0] = Pr [s

Pr [s

z1 ≡
z1 ≡

·

·

0] Pr [s

1] Pr [s

·

·

z
|

| −

1. Then
z2 ≡
z2 ≡

s
0
|
s
1
|

·

·

z1 ≡
z1 ≡

0] +

1] ,

where

Pr [s

z1 ≡

·

0] =

α
for some
|
1’s in s outside of z1 is (n

|

= O (1/n). Furthermore, even conditioned on s

− |

) /2

z1|
Pr [s

±
z2 ≡

·

b

s
|

z1 ≡

·

b] =

1
2

+ βb

1
2

+ α, Pr [s

1
2 −
z1, the expected number of
O (1) and they are uniformly distributed. Therefore

z1 ≡

1] =

α

·

·

for some

β0|
|

,

β1|

|

= O (1/n). So

Pr [s

z

·

≡

0] =

=

+

1
2
1
2 ±

β1
2 −

αβ1

β0
2

O

+ αβ0 −
1
.
n

(cid:18)

(cid:19)

Finally it must be shown that pseudoparity functions do not exist. That is, if g
is too close to a parity function for the bound of Theorem 47 to apply, then g actually is a
parity function, from which it follows that RF Sg

h admits an eﬃcient classical algorithm.

Theorem 49 Suppose µ (g) < 0.146. Then g is a parity function (equivalently, µ (g) = 0).

86

Proof. By linear programming duality, there exists a joint distribution
n, 0-inputs
1 (1), such that for all s0 ∈
z
0, 1
}
∈ {
1 (1),
g−
and s1 ∈

1 (0), and 1-inputs

s1 ∈

s0 ∈

g−

g−

over
1 (0)

D
g−

Pr
s1)

(z,

s0,

∈D

b
[s0 ·

z

s1 ·

≡

z (mod 2)

∨

z

s0 ·

≡

z (mod 2)] < µ (g) .

b
s1 ·

b

b
z (mod 2), since otherwise we could violate the hypothesis by taking
n and

s1 ·
It follows that there exists a joint distribution
b

D′ over z

0, 1
}

∈ {

Furthermore
s0 =
b

z
s0 ·
b
b
s1.
s0 or s1 =
such that
0, 1
b
}
b

∈ {

b

6≡

for all s

∈

g−

1 (0), and

[s

Pr
(z,b)

∈D′

[s

Pr
(z,b)

∈D′

z

z

·

·

≡

6≡

b (mod 2)] > 1

b (mod 2)] > 1

µ (g)

µ (g)

−

−

g−

for all s
functions. More precisely, there exist probabilities pz, summing to 1, as well as bz ∈ {
such that for all s

1 (1). But this implies that g is a bounded-error threshold function of parity
0, 1
}

n,

∈

0, 1
}

∈ {

Ψ (s) =

pz ((s

z)

·

⊕

bz) is

(cid:26)

n

0,1
Xz
}
∈{

µ (g)

> 1
< µ (g)

−

if g (s) = 1
if g (s) = 0.

We will consider var (Ψ), the variance of the above quantity Ψ (s) if s is drawn uniformly
at random from
bz is a parity
0, 1
}
{
function and hence µ (g) = 0. So we can assume without loss of generality that pz < 1/2 for
bz2
all z. Then since s is uniform, for each z1 6
are pairwise independent

bz1 and (s
·
random variables, both with expectation 1/2. So

n. First, if pz ≥

1/2 for any z, then g (s) = (s

= z2 we know that (s

z2)

z1)

z)

⊕

⊕

⊕

·

·

0, 1
}

{

var (Ψ) =

1
4

zp2

z <

2

+

1
4  (cid:18)

1
2

(cid:19)

2

1
2

(cid:18)

(cid:19)

!

=

1
8

.

P
On the other hand, since Ψ (s) is always less than µ or greater than 1

µ,

−

Combining,

var (Ψ) >

1
2 −

µ

2

.

(cid:19)

(cid:18)

µ >

2

√2
−
4

> 0.146.

