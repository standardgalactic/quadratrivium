you
you
you
you
you
Okay, well, for people who are joining this live stream, we are exploring some creatures
in the wilds of the computational universe related to combinators. These kind of show
us analogies to various features of physics, but also there's a hundredth anniversary of
combinators coming up, and so I've been exploring them for that reason. And actually hold on
one second while I restart something here. Otherwise, I will not be able to get any
interesting messages people send. Okay, right. So, okay, so let's just recall. So one thing
we're trying to do is we're trying to understand when we have a combinator expression, there
is a whole issue of its evaluation front, which I want to talk about, and I want to
talk about the ways to parameterize the evaluation front. And then I want to talk about what you
can evaluate. In other words, you know, normally when people talk about combinators, they talk
about compiling lambda expressions into combinators. But I want to talk about if you just enumerate
possible combinator expressions, what lambda expressions do you get out? If that makes sense.
Roman, are you with me? Yes, yes, I'm listening. Well, right. Okay, so what I tried to do here,
so let's see if I can do this a little bit more cleanly. So I'm going to do this with SK,
but the standard combinators. So this is now, okay, so this is the SK rules. So those are
the standard combinator rules from nearly 100 years ago now. Okay, so now what I'm trying to do
here is what I think I'm doing, let me see if I've got it down here,
is that I'm saying which combinators, so okay, so what's this, enumerate combinators. So this is an
attempt, and I'm not sure this is the right way to do it. Hold on, let me try and get. Enumerate
combinators, locate expressions, okay, locate expressions up to, all right, so what I'm trying
to do is given a combinator expression, okay, so let me write down a random combinator expression.
I want to find out,
all right, the idea of this is given
a particular, okay, let's take a look at how this works. So let's say S of S of K
of, I don't know what I have, of S of K of K of whatever, I don't know, some random expression
there. So now I can, if I give that two variables, okay, so let's say I say of A of B,
and then I say slash slash dot S K rules,
what, oh, I didn't evaluate that, okay, that whole thing turns into A of B, right, so my question
is, as I enumerate different combinator expressions, what essentially lambda expressions do they
turn into? Does that make sense? Yes, I mean, obviously given enough of these, I mean, any
expression is ultimately expressible in terms of S and K. Well, I know that, but we're looking
at that explicitly so that we can look at it for other things than S and K and see how that works.
See what I'm saying? So you're starting with simple
No, look, look, look, yeah, here's what I'm doing. Here's an example of what I'm doing. So what I'm
saying is, given I'm saying, okay, so here, what I've got is for a given, let's say four here,
that's all the S K expressions with four things in them. Okay, so for each of these,
I can now ask, what are they really? Do you see what I'm saying? Yes, if I say
Right, so if I do this now, if I apply this A B thing here, slash slash dot slash at this,
right, I get this. Now some of these still have S's and K's in them. Yes. Right, so those I'm
treating as not, you know, a sort of syntax errors. Is that a reasonable thing to do?
If you're looking for functions of two variables, yes, then obviously,
Right, the S's and K's are left in there, that doesn't work. Right, so to get valid
lambda, because those aren't lambda, you know, to get valid lambda expressions,
we need cases where all S's and K's reduce out. Do you agree?
Yes.
So then what I'm asking is, okay, so question,
you know, for a given lambda term,
what is the simplest
SK expression equivalent to it? And then sub question, what is an example
of a, okay, okay, is there an example
of a lambda I term
that cannot be reproduced with S alone?
There better be, otherwise, S would be universal.
Yeah, which it isn't. I mean, if you don't have K, then you need the other two
composition operators, right, B and C.
Because what S does, S sticks its third argument into each of the first two arguments.
So if one of these doesn't need it, you either need K to eat it alive,
or then you need a separate, simpler composition operator.
Well, so you claim, but I'd like to know what's the first lambda I term that cannot be reproduced
with S alone? You see what I'm saying? So we should be able to work that out.
Let's try and do that. Let's try and figure that out, right?
So the issue is there will be some things where the S expressions don't terminate, right?
And do you think it's important? Is slash dot enough to use here, or should we use
leftmost out of most to get the highest probability of termination?
Well, for these small terms, I mean, well, obviously, I mean,
there's at some point you will have a sub expression of the form SII in there,
and then you will get non-termination. But before that,
I mean, you can start with simple slash slash dot and see if there are cases that don't terminate,
and we can always look at them in detail.
Right, but I mean, I could equally well just use leftmost out of most,
but it's somewhat slower. So let's use slash dot.
But again, I would really like to parameterize. I mean, a sub objective here is to parameterize
slash dot versus leftmost out of most. But let's start writing out that issue.
So let me just save this here.
So we want the form of replacement where you can specify the strategy, right?
That's correct.
Some maximum number of applications before it gives up.
Right, version of replacement, specifying strategy.
Okay, so now I could go back to SMP because I had a version of this in SMP.
And we looked at this once.
Right, right. And what it did, so, okay, but maybe we should look at it here.
And, I mean, clearly what's going to happen here in terms of multi-way systems is we're asking,
you know, what, I mean, maybe we should take an example of one of the non-terminating
you know, combinator cases. Where is that one?
There's an SK, there's an pure S1. Let's see, I'm sure I've got that someplace here.
I also want to actually show the confluence property of combinators in a nice way.
Where is the S1?
Hmm, oh gosh.
That's S. Okay, and this is the leftmost, outermost evaluation.
Oh, S outer evolve list.
Okay, so that's the one which does an outermost evolution.
Right, so what that's doing is it's saying
for each of these only allow, oh boy, this is almost exactly the SIMP parameter of SMP.
It's how many to allow, right? How many updates to allow?
How does it specify that it goes from the outside to the inside?
Because that's what slash dot does.
Okay, but we can certainly have a strategy here where we have a different number
that are allowed, right? We can say just less than max here.
See what I'm saying? I mean, in general, then this becomes
less than max.
Okay, but now here,
what the heck is this? Is this the first one that is?
Okay, so this is the first one that blows up.
That's the case.
So that's asking it. In this particular case, it's sorting by, so this is another way of doing
the strategy. It's got the, you know, it's got those positions. It's saying, where does the match
occur? Okay, where am I going to do the?
The short positions first, the outermost ones.
Right.
Right, but I mean, in general, we want a strategy.
In general, this is a story of kind of foliating the multi-way graph, because if we look at this
thing, I'm sure I have a picture of it here. Let me just evaluate this and see what it does.
I have a handy function that makes,
should make.
A combinator
evolution graph.
Oh, it's a nest graph of that. Okay.
Oh, okay, fine. So this is,
okay, so combinator
evolution graph
of the rules, which is in this case, just this thing.
And the initial condition, which is in this case, just this.
Oh, come on.
Okay, five steps. We just get that.
What's going on here?
Oh my gosh, that is so freaking crazy.
Um,
look at the vertex labels.
That was uninspiring. Oh, no, look at the crazy things.
Oh, okay, it's of course rather big.
Well, so the question is, where is the branch point?
So in other words, here, right?
Right. So what we want to do with this thing is we want to say,
okay, with G equals this, then we want to say,
in the vertex list of G,
we want to say, select first of vertex out-degree
of G comma hash is greater than one. If I'm correct.
Look at that crazy thing.
Look at that crazy thing. So that's the first combinator
that in this evaluation scheme has two successors, right?
Yes. Now did we lose anything by ignoring the inner evaluations or not?
Probably not because, I mean, the S terms don't destroy anything.
So you're not actually losing anything by...
Well, let's find out where the evaluations actually were.
I mean, so this is, my gosh, look at this creature.
Look at this creature. That's the first one that has an ambiguous...
So actually, another interesting question is,
who has an ambiguous successor, right?
In other words, if we simply enumerate
let's see, combinator step.
Yes, I think we can do this. Let's do this.
We say combinator step of, and let's do the S term only.
And then let's say,
of that on this, and then we want to do that against
a groupings of table, let's say S, 3,
how I construct R02, that will give us the all possible S terms.
Oh, that didn't... Yeah, right. So that has no success, right?
Nothing. So this is like, okay, so here most of these have just one successor, right?
Some have some, some don't evolve at all. Some are fixed points,
but most have just one successor, right?
So now if we say select,
let's say select in groupings
of this, and now I say length of combinator step blah is greater than one.
Okay, no, nobody there. Okay, so let's go to seven.
Oh, exciting, exciting.
Okay, so those combinators have multiple successes.
So let's find out what their evolution tree is,
and whether they have big branching or whether they actually just terminate.
Well, there they are. They have branching, but they terminate anyway.
Somebody's asking people what the different shades of these nodes mean.
I think those nodes try to label, they have, I think, S's in them.
So some of them are leaf nodes. The leaf nodes have little tiny S's written in them,
but hard to see. Okay, but so this is, these are cases where there is actually,
let me just, to be complete, let me just run this against all of the up to size seven.
Let me just run this n up to size seven, just to make sure that we got this right.
So there were no earlier branches. No, those are the earliest branching ones.
Okay, so let's try size eight, and let's see, are there any of the size eight ones have a,
okay, so at size eight, we have that.
Now is the 10 in there actually suppressing some of those that might actually branch again?
Probably, yeah, but the ones which have now sort of pooped out and gone to a single one,
those are done, but there are a few here that branch again.
But what happens if you reach that 10, what does, do you see whether they die or whether
they just stop after 10? Honestly, you don't see if they die out. I mean, we're not correctly doing
that. Okay, although I will, I mean, if we just count here, we can plainly see these are gone
by before 10. Right, so here we've got basically two that puff out, right, and don't terminate.
Now, is that striking difference due to the graph embedding or is there more to it?
Let's just go off embedding. It just has a complicated tree structure, and this thing is,
this thing is going to multiply up. This thing's going to get very big very quickly.
So that's the halting problem basically for S combinators. And this is showing that there's
a non-trivial halting problem. And we can compute the halting probability for S combinators from this.
Right, I mean, the halting probability is basically, there's just two of these things
out of the S eights out of the case of eight. This is basically telling us that
the halting probability, non halting probability
is two over 429.
So what this is telling us, okay, so what we should be thinking about here,
well, let's look at the case of nine, just for the hell of it, just to get a bit more
intuition, but I think it's going to look very much the same. At least that's my theory.
Okay,
okay, so these are the nine branches, 180 of them, okay.
I wonder what I can say tiny there. Let's see.
Let me just put this.
Oh, this is some stupid bug. I really need to fix this bug. This is a, okay, what is wrong with
this? This is nest graph. Oh, I didn't pass the options to nest graph. Okay, let me just fix that.
Okay, let's try this again.
I should have used, I should maybe not have used 13 there, or I should have used my parallel
machines. Let's not use 13. Let's use 10.
And I'm just going to do this one. If this doesn't work, this time I'll use the parallel
machines. I think it's time to use parallel machines. Oh no, there we go. Okay, so
we have quite a few that branch here.
Some that terminate. That's a jolly one.
That's kind of fun. It's knitting a, that looks like it's knitting an actual manifold. Wow,
I had no idea that combinators could knit manifolds. I want to see that one do its thing.
I mean, because mostly that's an interesting universe, maybe. Yeah, well, quite.
It's some, that's, that's one, the wild one. This one here on the right. Yeah, there's even
two lines further down. I see it.
What's interesting about these is they're not growing that rapidly. Some of them are just
exponentially growing and they're just going to puff out. Yeah, this one here I suspect is
just exponentially growing. Let's take a look. Actually, it's not so clear. Well, let's find
out what the dimension of these structures is because that isn't obvious. Oh look, that's an
interesting one again. That's that same deal that we had before of,
okay, these are, I agree that these are interesting because they're not growing that rapidly on the
edges. Let's look at the sky. Boy, you know, it is the story of my life. I look at pictures,
I generate collections of interesting potential creatures, and then I study them. This is like,
this is what I always do. It's like it always ends up being the same thing.
Okay, let's take a look at this.
Okay, so this is the 10 case. So let's look at this up from, let's look at the eight
case for that. Boy, this is weird. Oh, this bug is still not fixed. Could somebody report that,
please? The double border bug in 12-2. Yep, got that done.
Um, let's do this. Let's make a graph 3D out of the sky.
It's definitely making a kind of grid-like structure. Let's see if that survives. Further
investigation.
Interesting. Okay, let's keep going a bit.
Good 10. Let's go to 11.
Is that terminated? Has that terminated?
Question is, what's the busy beaver s-combinator that takes the longest possible time to terminate?
It's terminated. It's terminated.
I'm telling you, there's always any, any time you think you figured out how these creatures
behave, they will always do something you don't expect.
All right, let's try this one here.
All right, what's going on in this creature?
Well, what's interesting about that creature is it's not getting, it's not like it's getting
a lot bigger quickly.
So, let's just try doing this.
Well, who knows?
Let's see, if we look at this graph and we look at the neighborhoods in this graph.
How can we see here? We should be able to see what the growth rate of these neighborhoods is.
I think if we start with, so this is that, starting with this initial state.
Did I get this wrong?
The graph neighborhood volumes,
of a graph, maybe I have to put that on a list.
Oh my, okay, starting parallel kernels anyway.
You know, this was supposed to be fixed. This, this launch dialogue was supposed to look different.
I have a screenshot for it.
Could you send that in, please?
Yep, that's John, right?
Actually, Roman, you'd be the one who'd be involved in this.
Well, as soon as I have the specification of that presumed new
function for progress dialogue, then I can hook it up.
Well, there is that exists.
So that's a project management thing. I'm not sure. I'll ask Sushma.
We'll do. Thank you.
Okay, let's look at that sequence for a second.
What is that? Look at that puppy. It's not, it's not uniform.
I'm telling you, these creatures are, they just, every time you think you know what's going on,
they show you that you don't.
So this is the graph neighborhood. So this is the number of things you can get to.
Now, each step, each combinator step here is, I think, a single update.
I think that's right. Let me just check this here.
Yes, it is. It maps over all, all positions at which the combinator applies.
So this is, this is asking, okay, boy, our friend, Mr. Sean Frankel,
really did figure out something quite interesting back in 1920.
I mean, it's like, how could he have figured out what this thing would possibly do?
Absent, you know, 100 parallel cores, you know, I did find, I was looking at the works of Felix
Klein, and I found this book about arithmetic and algebra. I know him exactly from around 1900,
I think, 1910, maybe. And he's talking about arithmetic. And there, to illustrate arithmetic
concretely, he goes computational, which means he has a picture of a Brunswigger mechanical
calculator. And he explains how the gearing works on the cogs to do certain kinds of computations.
So even though Klein was a fancy pure mathematician, at least in those days,
there was no snobbery about the computational world. All right, let's try and get two more terms.
Yeah. Well, these questions of growth or non-termination even
considered at that time, I mean. I'm not sure. Well, the, any Sigmund's problem, the guy who finished
Schoenfinkel's paper, Bayman, he, he claimed that he had come up with the end, how do you
say that? And Scheiden's problem? Yeah, that that's was the 1929 paper was very nice. That's
the other one. Well, no, that's the other Schoenfinkel one. But that is about a different decision
problem. Yes. Monadic predicate calculus, I think. This one is a 1927 paper. No, no, the main
Schoenfinkel paper is 1924. But this paper by Bayman is 1927, in which he introduces this idea
of an in Scheiden's problem. And I think he, which he does because he's obsessed with with
Whitehead Russell Prokippia Mathematica. And he is absolutely thrilled with the idea of types as
a way to avoid, you know, paradox, you know, inconsistencies and terminies, I guess he calls them,
particularly associated with transfinite numbers. Let me just get this out. Hold on a second.
That is really weird. That's really weird. All right, let's see if we can get this to go further.
Do we have the do we have the nerve to try 20? Let's try 20 see what happens.
We have no idea what this is going to do. So this is going to terminate. I mean, it's obviously not,
it's not going to, I don't think it's going to terminate. I mean, but who knows? Maybe this thing
will get to, you know, some tetration sized number and then terminate. Now, if we were
used to theorem prove it, could we tell him a thing about this?
But isn't it's known, it's decidable whether it terminates or not.
So I'm not sure it is. I think people have claimed that, but the paper that I need to find,
I know the guy who started this whole question of as alone combinators and
at that time, I think you didn't know, but then apparently the question was decided,
but I don't know which way. So I can ask him. Well, I think that
would these people like Baron Dreckton Klopp, who we were meeting next week,
would they, they would know this presumably, but I don't even necessarily believe that paper,
because it was rather complicated and didn't have any computation.
No, I can ask, I can ask the expert on S combinators.
There has to be one of those in the world.
Okay, this isn't even out there to the board. Okay, let's go to, let's go to 17 or something.
What did I go to before? Did I go to 15 before?
Okay, so let's see if we can make it to 17 and see whether it wiggles down again.
Or whether it just keeps going up. If it keeps going up, it's going to be more and more difficult
to compute. Although actually I suspect that this computation could be done more efficiently
because I don't actually need to keep all the stuff for this evolution graph.
I just need to keep each successive step.
I just need to map over each successive step, although I don't think the graph
overhead is going to be very high.
Clearly need at some point, we need a built-in version of this parallel,
outermost or parallel, innermost evolution strategies.
Right, right. Well, so let's look at
yeah, I mean, that was what we were originally talking about here was strategies for evaluation
strategies. This is crazy. How could it be, it's only two more, even if it was three,
you know, that's 10 times. It's not too bad. Well, let's let it compute for a second while
we make a strategy for what we're trying to do. So we've got two different things. We've got
the understanding of the, I mean, I want to go back and try and figure out the sort of
compile, you know, what can S combinators compute? You see what I'm saying? In other words,
but that's a slightly different problem, right? You take an S term and apply it to some arguments
and see what happens. Exactly. And here you're kind of having S terms alone and are interested in
those that do not do not terminate. Well, I'm also interested. I mean, oh, there we go, we got it.
Wow, that's quite steeply. Well, let's see what it's doing.
Oh, this is crazy. This is crazy. How could it do that? After, after running for this number of
steps, then it suddenly starts taking off. So another question is how big the trees get
by the size, they may be completely huge. So that those numbers that we see, those are the
that's the number of given that we start with a single term here.
That's the and given that we do
n updates, that's the number of distinct objects that we reach after n updates that we have not
reached before. Okay. So each one of those is a different combinator tree, 354,000 combinator
trees at the end there. And any tree that we would have reached before, we do not count in that.
This is the graph. This is if you look at layers in the graph, this is going successive layers in
the graph. Okay, so the graph in question is this elegant puppy here.
So if we look at that graph,
let's look at a slightly smaller version of that graph. Oh, gosh, it's already total mass.
Well, who knows, let's look at the 10 version and let's say graph layout goes to
layer digraph embedding aspect ratio half.
Okay, so that's what this looks like.
So what's happening is this is really weird. So what happens is
I see, I think I know what's happening here. Okay, I think I know what's happening.
As the terms get big, that's essentially an extension of the combinator in space,
in some sense. And what's happening here is that there are multiple events that can happen
at different places in space. See here, there's only one place in the combinator that the
expressions can, that the updates can happen. So I reckon if I go to, let's look at nine here.
Ah, that's what I wanted to do. Stop.
Okay, if we look at nine, it's going to look like that. Okay, so then we have a little
growth there. So let's look at 11.
And now one thing we could do is to say how big is the,
you see here?
Okay, but there are still nodes that only terminate because you've run out of steps, right?
Exactly.
Okay, so this is not the true picture of how it would evolve further.
Well, right. And so maybe looking at the bottom, yes, where,
I mean, it doesn't really converge, but the other ones just have reached their
11 steps, right?
Right, well, let's, let's indicate
which nodes here are terminal nodes.
And we can do that by simply asking, we can say,
if we call this G for graph or CG for combinator graph,
okay, now we want to indicate which are the terminal nodes.
And to do that, we just have to say which ones are not going to evolve anymore.
Okay, so what we can do there is we can just say,
if we just say combinator step,
so let me just check this work. So combinator step of that, okay, that gives that gives rise to two
nodes. That's right. Okay, so then say length of combinator step of this comma hash,
that's going to tell us whether it evolves equals equals zero. So let's say highlight graph
of CG, and then let's say this thing, and then let's say hash goes to,
no, we just need to map this over the vertex list of CG. And that's going to show us
which ones. Okay, great.
Oh, let's see, how do I do this with a style that is, I think if I say style
red, point size,
is that going to work for a style? Well, let's try it, point one, for example.
Or maybe none of them have terminated.
That's perfectly possible that none of them have terminated.
Let's just check that this code is right on something which does terminate like this guy.
I think that terminates there. So why isn't this showing that? All right, let's do this.
Oh, oh, I'm being completely stupid here. Sorry, this is completely stupid. This should be
the select in vertex list of CG length of the combinator equals zero.
I was still not seeing anything, but I'm not sure that we should. So let's just try this.
Yeah, okay. So there's one terminated thing. So that is because there's confluence in
combinators, there is a single unique terminating state, which is kind of cool in this picture,
to be honest. It's kind of cool. You can see that. So if we take this thing here and we do
this. Okay. Wow. How could Sean Finkel have known?
That's pretty nice that that thing is confluent. Look, I mean, it just
it confluent and it terminates. It's like a, you know, it's a black hole type thing. Okay.
All right. So let's go back.
And okay, so I want to answer the question, what can you make out of S's? Okay.
How was I doing this? It was 12. Okay.
Okay. So let's do this purely for S terms. Okay. What can you make?
What functions can you make?
With S combinators alone?
By the way, I'm still obsessing slightly about this, about what happens over here,
because did you understand my comment about space? The point is that you can think about
a combinator expression as having, you know, it was a, as having a certain extent in space,
in a sense, in the sense that although it is not a linear, although it's a tree,
you're saying it has many sort of pieces of that tree. And so what's happening
when this thing goes crazy and starts generating many different cases, what's happening there,
it's like physical space in the universe where basically there are updates that can happen in
many different places in space. If space is tiny, if there are very few points of space,
there are limited set of updates that can happen. But as soon as you can have many updates that
could possibly be in parallel, you can have a situation like this. And what's kind of interesting
about this is that then to define what it means to have a state of space, you're essentially
foliating this graph by saying, because this graph defines a partially ordered set.
And you're saying each one of these nodes is an event in this graph. And
no, I'm sorry, each one is a state. And so the
right, each one is, go ahead. I mean, what's also about these S terms, I mean, there's lots of
smaller copies of itself, right? I mean, they have a certain factor in nature because
the S term duplicates arguments. And so you have smaller pieces of itself inside.
How would we see that? Well, I mean, you have all these these nodes with which if you follow
each of them, they kind of look the same, right? I mean, this doesn't happen here because here
it dies out. But if it doesn't die, if it grows, then it has these smaller copies of itself.
Well, let's try and visualize for a second. Okay, so we've got like the minimal one that
blows up, whereas the minimal one that blows up. Minimal one that blows up is thing number five
here, one, two, three, four, five. That's really low tech. Actually, I don't even need to use that
low tech method. I can just use this thing up here. That's the first one that blows up, right?
And so here, and like this thing here, right? So let's take this here. This is
actually which step is that? One, two, three, four. That's the 11th step, I believe.
But anyway, let's take that as an example.
Oh, that's what we tried to visualize here. That's what that is.
So if we say combinator tree of percent 21,
graph layout,
okay, there it is. So that's that term and all the things. So I mean, you can see all the all the
copies are essentially the same.
Well, okay, so if we wanted to visualize this nicely, we could do sort of a common sub-expression
play on this. So we could write this in terms of its common sub-expressions. Although,
obviously, there's a triviality to that in the end, because in the end, when we sort of common
sub-expression, the thing all the way back to the beginning, we'll just get the evolution of the
combinator. But my question is, how can we draw this? What is a way to visualize this tree that's
useful?
Let me see this decidability result, because I'm really not convinced.
I have a bad feeling that what they've proved is decidability of some property that isn't,
you know, that's not the whole story, so to speak.
If that makes sense.
I think this is different.
Yeah, I think that's a different thing. Okay, let's go back to this. All right, so we're going to go
back to the question of what can we get from S's? Okay, so that's asking
given an S-combinator expression,
so let's look at S's alone. So this is groupings of table of S, let's say five, construct arrow two.
Okay, so those are all the S terms, right?
So now what we want to say is, for example,
and now we're just going to do slash dot
this thing.
Okay, hold on, I mean, you could just start adding a single argument and see what happens.
I know, I know that's what I'm going to do.
And then to those that survive, you add a second argument and see what happens.
Oh, that's a good idea. Okay, let's try this then. Okay, so this is now
whoops slash dot that thing.
Okay, so many of these, they get left with S's, right? Yes. Okay, so let's look at what happens
if I do two arguments here. Okay, so do any of them not get left with S's?
Okay, so what we have to do now is a select
free Q hash comma S, right?
Okay, so now let's try the one argument case and let's try six, for example.
Is it obvious that this can never work?
Well, I mean, even if you have a single table S comma one, you have S alone, right?
And if you supply three arguments, then S goes away.
Okay, but that's three arguments. So let's try.
Okay, well, let's see what we can get with one argument. Let's see what that I think.
Oh, for goodness sake, what's happening here?
No, it could. I mean, there are those that explode, right?
Right, so let's do a time constrained of that.
I'm going to assume that time constrained, I mean, this is a hack because the time constrained
could lead to one that has a particular property, but is, okay, so this time constrained is a hack.
Oh, you know, I could use the new confirming close framework.
Think. Why does it say aborted aborted?
Oh, because they're not, wait a minute, because there is aborted free Q of S.
Wow, that's convenient. I would like to use confirming close. Let me see how I would do that.
I'm not sure, but anyway, this is okay. So we're okay here. Okay, let's just try.
All right, so let's try the ab case here.
Okay, same two guys failing.
Okay, that is definitely not a simple puppy. Look at that thing.
Wow, that's so freaking weird. Okay, well, let's try the three argument case
for smaller versions, for smaller cases here. Oh, okay, so let's look at this. Let's go
and let's make a table of those. So I think this is saying that the S, I mean, what you
already observed that the S combinator, it has three things in it can have nontrivial.
Mm-hmm. I mean, you could still have a term like SSS that just requires no, no,
that requires a single argument, but then there are S is left. Yes. So eventually you need
three arguments for every S that is left in there.
Right, but the fact that there's S is left doesn't necessarily kill computation in a versatility.
Um, okay, so let's look at this. So let's say length slash at this.
I see. All right, let's just try and do the eight and nine cases. Just see how many,
how many expressions we get.
Oh, it's crazy. You know, non halting or undecidability is such a pain.
Still, it's what keeps life interesting. If everything was always halted, if everything
was always decidable, we wouldn't have to do this experiment because we don't.
I'm wondering whether there's a better way to control the,
the non-termination because you no longer, no longer have only S's in there, but you still
have these ABC's in there. So as soon as all the S's are gone, then it terminates, right?
So as long as it doesn't terminate, you still have S's in there and you need to
continue to see what happens. So probably.
What's your point? Well, I'm trying to, to get a better handle of when to stop searching
rather than this time constraint. I mean, the other thing is that are you still using
outermost now? No, I'm not. I'm not using outermost. But it doesn't really matter because it doesn't
destroy anything. So I think it doesn't matter.
Right. I mean,
well, we should be running this on more machines, but
yes, I can on an individual one. Yes. Okay. That's right. Yes.
Let's, let's just do that. Hold on one second here.
So this thing here should be a parallel map monitored.
All right, let's use your beautiful parallel computation stuff.
He says, it's a bad sign that it's not giving me the monitoring.
God, this is pretty painful. So this is now for n equals eight, right? That's the first case.
Right. And this is the second one.
Right. And we'll be 100 times faster than what we did before.
So ETA is two minutes.
Assuming it's roughly uniform, which it probably is.
Now too, if we want to search for interesting functions that you can express with this alone,
then we would really hope to see some simple A and B, ABC terms.
Yes, I suppose. Otherwise, there isn't really any useful, anything useful so far.
Well, but, but I mean, the fact is, if you're trying to do universal computation,
there's nothing that says you can't strip S's out of your results.
I really think we need to see this paper that claims that S's are decidable because I don't
believe it. No, I can ask about it.
Yeah. I think what's decidable is something like the halting problem, perhaps.
In other words, for what S is, do you get non-termination?
Even that, I'm not sure.
But like,
no, I mean, K, what does K do here?
K really helps prune things down for the lambda terms.
Yes, I mean, the problem with S is that it always puts its third arguments into both
sub-trees and so it goes and goes and goes.
Right, hold on.
So, for example, a simpler function, just a minute, flatten.
Okay, so these are the expressions.
These are the expressions that can be obtained.
Using S's alone with three arguments.
It seems to have some nesting structure.
Yeah, but they're trees, so why wouldn't they?
I mean,
No, but I mean some self-similarity also.
I'm not convinced.
Look at it more carefully.
I'm not convinced.
This is,
I mean, you're claiming that they successfully refine,
and that they're the same object, but successfully refined.
I mean, I'm just thinking of the S terms we looked at before,
which have this kind of structure.
If you then put in A's and B's and C's in these places then.
But of course, I mean, it could be very subtle.
So, it's difficult to see by eye,
whether it's up to its identical to another one or not.
Right, so the question is,
let's see.
Oh, this is the guy.
This is the guy.
This is the S-combinator guy.
This is the paper.
I mean, it's interesting in the abstract,
if there's something about the surprisingly rich structures.
Okay, the first part of the show is that the term we're writing
admits no ground loops.
The heck is a ground loop?
Is that what makes my headset buzz?
Hello.
The procedure that decides whether an S term has a normal form.
Right, but that whether an S term has a normal form,
in other words, whether it terminates.
Yes.
Okay.
And the fact that it is known whether it terminates
is taken to imply that it is decidable.
Yes, and the way he does it, apparently,
is by identifying building laws for those that go forever.
So, they have these structures.
Zero and fifty-five, there is a procedure.
You know, I do not trust these things.
I don't trust it.
But it says, show me an algorithm.
Existence is not enough.
I am not.
Classify the set N of normal forms according to the existence of certain subterms.
This is exactly the case where you need an automated proof,
because it's just hopeless to do this.
But, you know, the chance that this is right seems low to me.
Okay, well, this is the paper that claims this, then, basically.
No, I can ask about it.
Oh, the computer program 21 is able to make the given grammar deterministic.
For all states, x, y, with transitions like this, there must be states.
The set of normalizing terms is rational.
So, that's claiming that it's an automaton language.
Okay, so that's reference 21.
Oh, look at this.
Let's see if that's still there.
Let's see if there's a floating finite automaton on.
Oh, great.
This doesn't have to type this in.
Let's see.
Let's see, got, you know, livenets, libsig.de slash tilde joe,
whoever that might be, slash rx.
Not found.
Well, it's 22 years since.
I know.
So, let's go look at the internet and the Webeck machine.
Let's see if we can find a lost, see where the booster captured.
Oh, look at this.
Look at this.
Look at this.
We might have it.
Okay, 2011, it seemed to capture.
Well, let's try 2000.
Let's just try 2008.
22 years is nothing.
I've got code that just runs after 30 years.
Written in that language, of course.
No other code.
Well, let's see.
This is the Webeck machine on the joe page.
So, let's take a look in 2005.
That's odd, isn't it?
Why would the Webeck machine have captured that page when it was giving a 404?
That's really strange, yeah.
That is sort of common, though, on the Webeck machine.
But why doesn't it reject it when it's a 404?
Some of them are marked with different colors when it's a dead page, but yeah.
Here we go.
Okay, written in Haskell.
I wonder how big the program is, whether we could just implement it.
Well, let's just implement it.
Yeah, and we could also use the Haskell, the external evaluate of Haskell.
Yeah, we'll just translate it into our language.
Well, let's see.
This is the documentation.
It's an unsupported format.
That's unimpressive.
So, that's some kind of, okay, hold on.
Let me just...
Or DBI.
Yeah, it's a DBI file, right?
Most portable.
Well, it's kind of okay, I mean.
It was very portable format in its day.
There was no GitHub at the timeline.
Yeah, well, there's...
All the checks below should return the empty set.
Somehow, I don't think this is going to run if I press submit here.
All right, well, that's not a particularly complicated thing.
Okay.
Okay, so this is the...
These are the grammars.
Okay, so that's a grammar definition.
Well, that's interesting.
It's quite clean piece of code, actually.
If that's really it, then we should be able to translate it with any trouble.
Let's just see whether that is it.
Let's look at the Haskell source code.
I guess I look at version 2.1,
which is the most modern available.
Oh, it doesn't archive that stuff.
Well, let's see.
Maybe it didn't archive a .gz file.
Oh, look, there's a PS version of this.
Let's look at that.
Okay, so this is the version 2.1.
I think it's the most modern version of this.
I think it's the most modern version of this.
Let's look at that.
It's a PS.gz.
Something's happening.
What the heck?
What the heck?
Why is it an unsupported format?
Do you have a pro-skipped interpreter?
It's not PDF.
No, I think I do.
I'm not absolutely certain.
I mean, maybe I'm sure Acrobat would do this.
I have to save this file somehow.
Okay, so I go here, and I say save link as.
And go somewhere over here and save this.
I don't know if this is not as organized as it should be.
Disappointing.
Okay, that says it's not recognized there,
but what I can do here,
what the heck?
A .gz file should be...
Let me just do this.
This should work.
Hopefully, the PostScript will not be like splat.
Oh, this doesn't look good.
Yeah, the reason it's failing to open
is because that's what's in that file.
What's in that file is about this capture.
Okay, fine.
So that wasn't captured.
Well, it does seem that we've got...
I mean, I don't really understand these online examples.
Is that what language is that in?
Well, that's the RX.
That must be RX.
So he wrote an interpreter for specifying grammars
and then implementing decision procedures about these grammars.
But so the question is, where's the grammar?
If this is the grammar, we can make our own decision procedure.
No, but I don't, I mean...
We don't know.
We'll have to find out.
We'll have to find out.
I mean, maybe he's still around somewhere, so I could ask him.
Yeah, we could just ask him.
We could send a mail.
Always, that's the easiest thing to do with humans,
you know, if they're...
They always have the possibility that the human is still around.
To represent themselves, so to speak.
Hold on.
Let me just pull this back up.
Actually, I'm just going to save this S-combinator paper.
Oh, actually, wait a minute.
He thanks Klopp and Statman, both of whom Matthew said
Matthew probably knows these people.
We should just ask Matthew.
Well, okay.
Maybe he still exists.
Okay.
Now, it's some...
Potentially, it still exists.
Good.
Okay, in any case, so he claims...
This claims a decision procedure
for termination.
What seems to have happened that if they grow,
they do so in a certain regular way,
which you can recognize,
and then you know that it will go forever.
Right.
I mean, if you see all these sub-trees replicating,
then you know that it cannot ever get smaller again.
The question is whether inside that infinite growth
there are all sorts of things that can happen.
In other words, deciding that it grows forever
is not necessarily...
I mean, like you can perfectly well have a cellular automaton
that grows forever,
but yet its internal structure can do arbitrary computation.
Is that what I'm saying?
So it can be just...
So, I mean, can it still exhibit complicated behavior
even if it goes regularly?
Absolutely.
Absolutely.
You want to see an example?
I'll show you an example.
Trivial to find an example.
Here, I'll show you an example.
Just for the hell of it,
I'll find you a really lively example.
Hold on.
Let's see.
Good one here.
Oh, my God.
Okay, let's take an example.
Actually, that's irrelevant there.
Let me just run this for...
Hmm.
Definitely not simple behavior,
but yet a very regular outer boundary.
You see?
So I wouldn't be surprised.
I mean, there's nothing to say
that the S-combinators don't do something as wild as that,
even though you can prove that the sort of outer boundary
doesn't terminate.
Have you seen what I mean?
Roman, who's still there?
Hello, sorry.
Yes, I pushed the wrong button.
So, yes.
So for the S terms,
that would mean that somewhere inside
there could still be a lot of stuff.
Or each of these sub-trees that goes forever
could by itself be somewhat interesting.
Right.
But so the question is,
can we make a visualization of what's happening
inside one of these acts?
Maybe we should go back to identifying common sub-expressions
because that would also certainly make the computation faster
because then you don't have to do the same.
Yes, we can hash them.
Right, we just hash them.
Okay, so let's do...
All right, so what we've concluded with functions with S alone
is it's a pretty sparse set of functions.
Okay, so let's go...
That's not yet a new notebook.
Common sub-expressions.
Okay, so what we want to say,
we want to consider something like
one of these ones that grows crazily.
Well, let's just take what do we want to do here?
We want to say...
Here, I've got an idea here.
Let's just say nest list.
Slash dot this.
Now, slash dot will allow multiple instances of...
Okay, so there's a big crazy messy thing.
Right.
So now the question is,
okay, so let's say a combinator tree of that.
Okay, so that we think has some common sub-expressions.
Okay, so now a question is how we find those common sub-expressions.
So we could use level to go into this expression.
Let's call this expert eight.
How do we find the common sub-expressions?
By the way, given that this is just open bracket, closed bracket,
can't we make a black and white squares version of this?
I think we can.
I think we were doing that last time.
Yeah, last time we did that, yes.
I wonder where that is.
Well, we can just do it again.
Two squares of expo.
And what we want to do here is two string of expo.
And then what we want to say is string replace.
I think this is what we did last time, actually.
String replace S goes to nothing.
Am I right?
You were just concentrating on the brackets, right?
Well, we only need the brackets.
Zero, closed bracket goes to one.
Then we just say characters of that.
And then we just say two expression of that.
Okay, so now we say two squares of percent a hundred.
Oh, what did I do wrong?
There were more than one.
More than what?
I see commas in there, so there's more than one.
Oh, oh, that's because it's a nest list.
Silly me.
Just next.
Okay, so this is showing.
So now if I say array plot, I think this is what we did last time.
We had a ragged array plot.
We can just say array plot of that.
That plus.
How do we map this?
That slash dot zero goes to point two.
One goes to one.
And then we say pad right.
Do you think that's the right thing to do?
Not highly informative.
Maybe we can try.
Is that our minimal?
Let's try our minimal grower.
Actually, let's try one that was one of the lively ones that eventually dies out.
So this one here, for example, just to see what that looks like.
Okay, so that's what it does.
When it blobs around for a while and then it dies out.
Right.
And so a common sub expression here is a leaf piece, which means something that is a sequence here.
So a common sub expression could be seen.
We could do it essentially with run length encoding of that bracket sequence or something.
Or dictionary encoding of the bracket sequence.
I can't rework on the original.
Yeah, we can.
I mean, just take all the sub trees and hash them.
Fine, let's do it.
Okay, so let's take this thing, expert eight.
This is a terminating case.
Yeah, okay.
Okay, so now what are you proposing to do?
You're proposing to look at the sub trees of this guy, which would be the sub expressions.
By the way, this isn't right.
This should have been combinator trees slash at expert eight.
Okay.
All right.
So now let's say here.
Okay, how do we get the sub expressions?
We can say a certain level number, right?
If we say this,
if we say level EE eight comma five, for example, let's try level four.
That's weird.
Level three.
Am I being stupid about this?
Is this obvious that this has to be?
Do I need to turn this into something which is list pairs?
No, I have a deeply nested expression and you want to find
sub expressions of a certain depth, right?
Yeah, that's what I'm but I think it may not be the right thing because I think
let's see the expression graph of this just looks like that because it's a head animal.
It's a head based animal, so to speak.
Okay, I see.
You know what?
This is another thing we should do.
We should make heads goes to true be an option to expression graph.
Let's see.
Okay, but we can break out of that nesting by just saying
I think I have a way to do that here.
That's quite grotesque, but anyway, okay, let's look at this.
This I think breaks out.
So what I think this does if I'm not completely confused
is that it does that.
Okay, we should have a function repository entry that does this.
What should it be called?
It should be called function application to list or something.
Application to list no function function to list of expert.
Okay.
When the other thing you could do is use the new application operation
instead on the right hand side instead of f comma x, you say f applied to x.
Wow.
Oh, and this thing here to list.
Yeah, I mean that that's the alternative.
I mean you on the left and the right hand side you could use.
Okay, so why don't we say function to application?
Yeah, so on the right hand side instead of f comma x you would say f escape
a p or application of f comma x.
Yes.
Let's be modern.
That's really nice looking.
Okay.
So now we can do our level stuff.
Beautiful.
Okay, but that's essentially what what the combination was doing before.
But now we can say now in this form figure out the levels.
Okay, now I'm not completely sold on this kind of moon operator.
Okay, some expressions now.
Okay, so then this is a a eight for example here.
So now we say level a a eight comma three for example.
This is so hard to understand.
This is really hard to understand.
Okay, that was level three.
So some of these are isomorphic.
No, they're not.
Or maybe you should look work from from from the bottom up.
Okay, well hold on let's let's do that.
Let's say so if we say level minus one we should get a bunch of f of s's.
Right, so if we say level minus two.
Oh yes, look at that.
If we say level minus three.
Oh yeah, this is going to be cool because then that's going to start differing, right?
Except all of those are identical, I think.
Okay, I am officially confused.
Okay, so let's go counts onto level minus n.
What's depth of eight?
11.
You know the number of depth of 11 expressions that I have ever dealt with in my life is very limited.
That's some.
Okay, I don't understand this.
It's unique at every level.
How could this be?
Well, let's keep going all the way up to the top.
Okay, well this says that they are all common sub expressions.
This says that the lowest level that everything is a sub expression,
which is kind of probably not surprising from the way it was built.
So where does it?
So what's happening here if we say here let's do this.
Okay.
That was very swank what I just did.
Right, so that shows that at these successive levels, so that subtree there appears here
and everything contains that subtree.
Am I making sense?
So, and I wonder if this is a general result that it always contains in the container
which one was I looking at that was I looking at this thing here?
Okay, let's just try.
But this was much more complicated, right?
Which one?
The X for eight.
No, X for eight is what we're looking at.
This is the end result of X for eight.
Okay, so if we do if we do the same thing,
let's just do this again just for fun for expert 10, but it's going to be the same thing.
So, how do I see which levels have more than one?
None of them have.
Oh, gosh, you know what?
This is one of these.
The animals are always smarter than you are.
That's crazy.
Look at this.
One of these levels has three cases.
Look, see there it is.
Yes, that's what I mean.
And if you go down, I mean, there's again three cases.
And so these are the different common sub expressions.
But if you go enough down, then there isn't much left to be different.
Right.
Right.
So that's the non-trivial level.
So if you look from the end, then you have one, one, and then you have two.
And then you have.
Well, yeah, right.
So let's just do, if we just say,
oh my gosh, this is so crazy.
I mean, these things, there is no theorem that's true,
which is why I don't believe that that guy managed to get it right.
Well, but I mean, if you look at it, I mean, there are only two different sub expressions.
No, there aren't.
There's one with three.
Yeah, but I mean,
yeah, yes, but I mean, there were two,
then one of them branches again, and the other ones there.
So there's really not a lot of different stuff going on.
Let's try it one more time.
Hmm, look at that.
Yes.
So all the, all those dangling branches are all the same, right?
I mean, yes, they are.
So, yes.
So it looks interesting, but I mean, it's kind of a fractal thing, but it doesn't have a lot of
I'm telling you, I've hunted these things for so many years.
This guy is not as simple as you think.
I mean, it has some dressing that looks simple,
but it has something going on that isn't quite as simple as you think.
I'm pretty sure.
Now,
if you add one more S there, so
no, no, I mean, the starting expression, right?
You're always starting with the same.
Yeah, I know.
If you just randomly add an S, it's going to do all kinds of crazy things.
But you're not.
Oh, I know.
I just wanted to know the expression you start with is always the same.
Right, right.
But I'm saying, if you add an S to the expression you start with,
I'm saying, I don't think that's particularly interesting.
Why would that be interesting?
I mean, you have to try some other expression.
I mean, what I think is interesting.
Okay, I see.
You found that one earlier by that as one of those that exploded.
Yes.
Okay.
No.
Yes.
Right.
So what we can go over here is we can look for other ones that explode, right?
And we can see that all the ones that explode have the same properties.
Right.
So here we've got two exploding ones.
Right.
So five, six, seven.
Well, actually, let me just get the five and seven cases here.
Okay, neither of those is what we were doing.
Okay.
So simplest, simplest expanding cases.
Okay.
So then here, what we want to do is the analogous thing.
Let's just try to put that one first.
Okay.
So that one has a difference in that case.
Okay.
So now let's do it for the other one.
So that's interesting.
That again has a pair there.
One's a two, three, and the other's a five, three, but they have very much the same structure.
Yeah, those have really the same structure.
And in fact, look at this.
That subtree is the same.
So what may be the case is that every exploding S expression has common subtrees.
Right.
But if you look at the subtrees of anything that will explode.
Yes.
So there aren't that many exploding cases, right?
It's all basically the same thing.
Yes.
But I mean, there may just be one exploding case.
Let's find out.
Okay.
So here, we want to know.
Where was that really crazy one?
This one here.
There was one that just wasn't on the screen because it was so huge.
Well, right.
We can try some of those.
But I mean, I mean, like, okay.
So let's let's how do we want to figure this out?
The exploding cases.
We want ones whose vertex out-degree, right?
Doesn't, I mean, one thing we could do is just look at the case where, well, we could just do a fixed point list.
Let's see.
We want to know whether the thing reaches a fixed point.
Yeah.
One thing we can do is just do another combinator step and see if it's fixed after some number of steps.
But I mean, just evolve them for a number of steps and see which ones are the largest one afterwards.
Well, yeah, but I mean, so this, for example, here,
we can do that.
That's line.
Line 41.
I mean, here was that really crazy one.
I know, I know, I know.
Select from line 41.
Those ones were vertex count.
Well, actually, let's just do vertex count.
Okay, which ones would you like to pick?
1956.
Well, 76 is even better.
No, I know, but how big do you think the cutoff should be?
See, I think we need to actually compute something to know whether the thing is going to
reach a fixed point.
What would the Ks do if we added them into these?
They were doing practically nothing.
They were just pruning these trees down, wasn't they?
Yes.
Now, of course, when you add K, then you get I.
And then you will soon stumble on SII, applied to SII, which is one of those
non-terminating ones.
But, well, actually, you have non-terminating ones here as well.
So, they would clearly, I mean, SNK together would give you cases where you cannot decide
whether it ever holds or not.
Because you can build a fixed point combinator and other strange animals or birds as they are.
Right.
What is the fixed point combinator?
So, the fixed point combinator takes a combinator and
it has a combinator.
What does it do?
It has a combinator expression, which says Y of F.
Yes.
I mean, the construction is that fixed point combinator Y.
I mean, it's not unique.
I mean, there are several ones, but Y of F can be proved to be equal to F of Y of F,
or some of those even reduce to F of Y of F.
So, you can use these reduction rules and after four or five steps you get F of Y of F.
And then after another four steps you get F of F of Y of F.
So, you build that infinite fixed point.
And I mean, the property of combinators is that every combinator has a fixed point.
But usually, these are non-terminating or infinite expressions, but every combinator has a fixed point.
And the consequence of this is that every equation involving combinators has a solution.
That's kind of a nice property of these combinators.
Right. Although the solution might be infinite.
Yes. I mean, you cannot, I mean, because it's usually in terms of the Y combinator.
So, I mean, it's like finding a fixed point.
If the argument is numerical, then usually you can find fixed point in finite terms.
But if the argument is symbolic, then the fixed point is F of F of F of X applied infinitely often.
That's always obviously a fixed point because infinity plus one is equal to infinity.
But if you represented in terms of transfinite numbers, it may be less trivial, right?
Just saying it's infinite fixed point, you may be able to label those infinite fixed points by transfinite numbers.
You have to be careful about cardinals and ordinals.
I think they're ordinals, right? Because they're ordinals because they are trees.
If I'm not mistaken, isn't the cantonormal form some kind of,
you can represent that in terms of trees, as I recall.
In other words, given one of these trees, if I elaborate this tree infinitely,
the claim would be that I'm going to get something which corresponds to,
yeah, which, let's see.
So the thing that we still don't know, and we should probably wrap up soon for today, but
let's see. So what we're finding here, so the supposition is maybe all exploding s expressions
have the same fuel, so to speak.
Or at least only a very small number of different ways of growing.
Let's determine if that's the case. We should be able to determine if that's the case.
So what should happen is, what we should see is that there's a subtree,
that is the growing subtree, that's the subtree from which all the growth happens.
Let's see, so what that is in our multi-way version of the thing, which is over here.
Let me think here.
So what we want to show, we want to find all those s expressions that do not terminate.
Richard on our live stream is pointing out that some of these sk-commonator
is equivalent to the lambda expression that generates hyperoperations.
Yes. I mean, we indeed have that, because I know what that is. It's in the NKS book,
the one that generates tetration.
That is this crazy puppy.
This is the combinator. This is the one that is
e of x and y is x of x of y. By the way, we should be able to represent that as a,
let's actually do that. Let me see.
I thought I had done this. Wait a minute. Second.
Okay, so the tetration combinator is e of x and y
goes to, according to NKS, x of x of y.
Okay, so we should be able to get that combinator in terms of s and k. So if I say two
sk-commonators of x of x of y comma x y, there I have it. So that object,
if I look at that object,
well, let's see. That object is being applied to, that's what this says is.
What do I say here?
The actual conditions.
Okay, so this says.
That's an interesting claim that this quantity remains fixed through the evolution.
Okay, so this thing says e of e of e.
Okay, so Richard is giving us a general
hyper-operation. That must be an Ackerman function, basically.
Let's take a look here.
Okay, lambda.
Why is there an r there?
What's that doing?
Well, that's the r from the first lambda.
Oh, so what is this in?
The first argument. So if you want to see what it is in combinator, you would have to turn this into.
Well, so if I write it as function of this, function of b,
r of function of e comma function of x comma function of f comma
r, e of f, what does the bxf mean? Is that left associative?
Your x, left associative, yes.
Is that the, is that right?
Applied to function of h, function of e,
bug, bug, bug, bug, bug, bug.
I'm on it.
No, no, no, no. It's not your kind of bug.
Almost sounds like an insult.
Yeah.
Oh, yeah, there must be missing colors back in somewhere.
Yes.
I don't think this is quite right.
Richard, we need a different version of this.
This isn't, this isn't right.
But there is clearly a way of representing an Ackerman function,
which is basically what we need.
Using, using these combinators.
Yes. Now, the problem is with all of these recursive ones to actually
represent it, you have to first formulate the condition.
I'll say if x is 0, then this else that, and then you have to solve that recursion.
That means applying the fixed point.
Yeah, but, but look, look, I have a vastly simpler version of it.
I just have this, this e of x blank, y blank goes to xl, x of y.
And all can only apply to the inner part of an expression.
My God.
So this is saying that the thing eats away using this function.
And it reaches a fixed point when the depth reaches zero.
My gosh.
And now here, of course, the question is, if you now express this in terms of s and k,
then you probably have to be careful about the evaluation order.
Now, if you express it in terms of e alone, then no, I agree.
But I think what's true is it does terminate.
And so the terminating this, this result here maybe may depend on the evaluation order.
But the fact that the final result
is
let's see, any expression can be represented.
Oh, actually, it's not tetration.
It's 2 to the 2 to the m says here.
Let's see.
Oh, no, always evolves.
But for initial conditions, yeah, okay.
Of size n, it can take roughly annotated powers of two to do so.
This is a nice example, by the way.
Oh, somebody has a version of.
Okay.
This is in terms of lambdas.
Yeah, right.
Yes, you can do the silver or toss these in terms of lambdas.
But now, so could it have been that the S combinator alone has been looking as a universal system for 100 years?
Never being noticed.
I think it's possible.
By universal, but if it has such a regular growth, then
I mean, universal would imply all kinds of it implies that you can encode that it's possible to program it to do anything.
And what we're seeing if it's decided whether it stops or not.
How can that be?
Well, it may be.
Let me think about that for a second.
Okay.
See, the real question is, I mean, we really surprised if this thing, you know, given how complicated it looks here, I'm going to be
really, really surprised if it's not universal.
But the question is,
and let me think about that.
Usually intermediate degrees, for example, are
undecidable, but not universal.
They have undecidable halting problems, but they're not universal.
That's the claim.
But those things are a fake.
Because as soon as you look, okay, let me think about this for a second.
So this, the claim would be
that if there are only a finite number of seeds,
yeah, I don't believe that.
I don't believe that claim that because I think it's the same as a Turing machine.
A Turing machine in the end, the whole point is that it could be taken on these,
these explosion seeds, right?
Once you hit one of those, yeah, it's just going to loop forever, right?
No, it will not loop, it will go forever.
It will grow forever.
Great, fine, whatever.
But the point is, yes, but there's no bound on how long it takes to reach a piece of fuel.
So the only way you would get this is if there is a bound on the busy beaver function,
that is, if you know it's not going to reach an explosion seed,
you can bound the number of steps it takes to reach an explosion seed.
Right, the statement that it is decidable,
well, okay, the claim would be, yeah, I mean, you see, you can look at it and you can immediately see
that it will grow forever or it will die out.
Well, I know, I know, I know.
So that's basically the claim that there's a finite procedure for determining whether it
will ever reach, assuming there are a finite number of explosion seeds,
there's a finite procedure for determining whether it will ever reach one of those.
That's the claim.
Right, and I do tend to agree that if there is such a finite procedure,
then it can't be universal.
Yeah, then it would be interesting to gradually add the other required combinators until
we know that there's several sets of four combinators that give you this full lambda
i calculus.
And so you could start out with just one or two of them and see.
But I mean, look, I think the one thing that is fairly clear is that a single,
I think you need at least a combination of at least three arguments.
I don't think you can do it with two because I looked at all the two cases.
So then the question is only whether this one, or I mean, honestly, I have to believe if this one
doesn't do it, the one of the other ones with three will do it.
Because this one, if this one doesn't do it, it's really, really close.
Okay.
I mean, that's kind of what we did last time, right?
Look at all possible ways of representing three different arguments.
By the way, Richard says he's converting it to Curry's BCKW system.
I'm pretty sure that's Schoenfinkel system.
I'm pretty sure that's exactly what Schoenfinkel did.
Let me just look it up.
Well, I mean, it's, I mean, BC is an alternative to S.
Yes, I understand.
But I think that's where, as I recall, let me look here.
That's the so-called currying operation.
By the way, I do need to get copies of those papers of Angla's,
like the one about Aristotle.
I don't have that yet.
Yeah.
If we have a publication list, which has those in it, it will be really super useful.
My God, that's the end of the paper.
I thought he had some, I thought he had this some additional combinators.
What's the Z combinator?
I mean, here's, I mean, S and C, that's S and K, right?
Yep.
And C, no, Z, okay, I, and Z is what we now call B.
That's the, that's the composition.
Okay.
And T is kind of twisting, changing the order of the arguments.
But I think he goes that way.
He starts with S and K and then shows that all the other ones can be expressed in terms of thing.
Okay.
Particular functions, I, C, T, Z, S, and U.
For every predicate F, there exists a predicate G such that the propositional function F of X
and G of X is not true for any object deck.
So he's trying to show that you can derive NAND from the combinators.
Well, that's the fixed part combinator.
Which one?
Well, you?
These expressions that on the, on the top page, the last one, the U.S. that you, you, you,
they usually look like this two terms, same term twice.
So that's something.
The incompatibility function.
That must be either negation or, or, or fixed point.
But basically showing that you can compile things down to.
And this is the stuff written by Beman that's half wrong.
Yeah.
That doesn't seem relevant.
A formula can be written without parentheses.
A simple sequence of these signs.
That's just not true, is it?
Maybe with the Z there it is.
I don't think that's true.
I think it has to have parentheses.
I don't think this is correct.
Yeah, there must be.
Well, so what John Finkel appears to be doing here.
Is, I mean, he's showing.
Right.
He's showing that you can reproduce propositional calculus and predicate calculus.
From these combinators.
Which I, I guess is equivalent to showing universality.
I mean, here's another question, which is, you know, are these combinators something that we could
ever imagine actually finding useful and functional programming?
Or are they just too irreducibly obscure?
And what people normally find useful is just, you know,
saying, you know, pipelining something or something like this.
Not one of these weird things that distributes arguments in these complicated ways.
Well, anyway, all right.
Well, this is interesting.
I think, yeah, well, if you can, if you can try and find out about the S thing that will be helpful.
And I think, you know, this concept that, you know, everything has a fixed point,
but it could be infinite.
It's sort of interesting.
I mean, I don't know whether you'd say the same thing about a Turing machine.
Well, I see a Turing machine.
The issue is, in this case, in this case, fixed points happen because there just isn't
any rule that applies.
So the statement, but it could be infinite, it has to be, is it as simple to describe infinity or not?
Yeah, well, I still am concerned about what, you know, the ordinary combinators,
we can basically compile any lambda expression to them.
So the question is, what other definition of universality can we use if we don't have the
k, so we only are living in the lambda i calculus?
Well, still, we know that in the lambda i calculus, we have universality.
So if we can take any expression, any lambda expression that is in the lambda i case,
by the way, do you know in the theory of when people make models, like I think Dana Scott has
made, tried to spend years making models of lambda calculus and or combinators, do you know how those
work? Well, I mean, I'm only familiar with one model that's Engelos graph algebra because that was
also the topic of my thesis, but that's the only one. How does that work? Well, it's a generalization
of function graphs. So, I mean, an ordinary function graph can think of it as a collection
of error terms, a goes to f of a, and this goes higher levels, so the a itself could be an error
term. And in the generalization of graph algebra, but the left hand side of the arrow is not a single
element, but a subset, a set of elements. And the application operation is just how I think, how
would be a repeated list of what's the function that gives all possible right hand sides for
all application? Hold on, tuples, maybe. I mean, so you're saying, replace list, I think. Oh,
replace list. Yes, right. So you have a big collection of error terms, and then so f applied
to G is you find all the left hand sides of G terms that are subsets of f, and then you collect
all its right hand sides. So this is just, this would be a graph of a function, the collection
of all such things. Hold on. So a function, so a standard function, like a, I don't know whether
we've got our new thingy working yet. Oh, no. Does this at least work? Oh, they should at least make
this work. Okay, in any case, if I say a function f of a, you're saying
that if I map that over like range of 10, I'll get that. Oops, what do I get? I got the wrong
precedence here. Okay, so that's an ordinary function. Well, I could, I could, I could write it as,
I mean, the way you represent the function by its graph. So you would have a list of
list of error terms where the left hand side and the right hand side of these arrows are elements of
that domain. Okay, so here, so this, so for example, I could say there, I could say something like,
you know, I don't know, mod a squared comma 11. Okay, is that what you mean by a function? You
mean a function represented in something like this? Yes, as a graph. No, I mean, graph in terms of
functions, not, not the, not the, the one you look at. But what are we talking about? We're talking
about this kind of graph, a graph that says this, this element maps. Yes. Is this what you can
function graph? Yes. Okay. And then if you represent the function by, by the, by its graph,
then applying a function to an argument means to take all the matching left hand sides of the
arrow terms and replace them by the right hand side. And if it's a proper function, there's just
one of them, right? Because it's okay. So if I take this function here, this, then what I could do is
in this graph, I could just say, if it's an ordinary function, then four maps into five here.
If it's a deterministic function, then four maps to five. And so the application operator, so if you
have two functions represented by their graphs, well, and so the operation of application f applied
to G would be defined as essentially taking. Okay. So the function application is just you,
you run one of these graphs. So it's a graph uniting operation of some kind,
where you're gluing these graphs together. You know, I mean, you, you have to be,
let's see. No, I mean, this picture is misleading because you are assuming that the domain and
code domain are the same. You know, you cannot go from one to, you usually cannot go from one to the
next. And, but anyway, if you apply f to G, then you search all, you apply, you apply the rules
essentially to the elements of f. So let's just see what happens in this case. So in this case,
we've got this and our graph in this case with vertex labels, is that rather trivial graph?
Yes. Okay, let's just do range of five just to be less boring. Okay, great. There we go. Okay,
now what? Now you've got another function going to G. No, yes, I mean, for example, you could have,
so if you call this function G, let's say you have another function f, that is a ground that
just consists of a set of numbers. So no, no errors at all. Then you could apply f to G.
Now sorry, G to f goes the other way around. But so you literally search for all the left
hand sides of your arrows that appear in the other set and replace them by the right hand side. So
it is just essentially a replace list operation. You find all matching terms.
Okay, this is something somewhat weird way of thinking about functions,
you just represent them by the graph. And the graph is a list of or set of arrow terms left,
left hand side goes to right hand side. And if you represent functions by graphs,
then the function application f applied to G becomes
replace list operation, but just finding all matching terms.
How similar is this going to be to a multiracist, I wonder?
Yeah, I mean, that's a good question because I mean,
of course, I mean, you can then generalize say doesn't have to be a function. So there could be
several left hand sides that are identical that go to the different right hand sides.
So once you generalize, then you can probably get more interesting stuff. But
what would the big step in going to a graph algebra is that the left hand sides of these
arrows are now subsets instead of just singletons. Well, but that sounds like it's a multi-way system
play. So this seems to be, I mean, I pretty soon we're going to find your thesis.
Yeah. Yes. Okay. So you see the K and S and everything.
Existence of non-trivial combinator.
Yes. So these are, I mean, these are the, so you see here some of these arrow terms. So
nested arrow terms, sigma, arrow, row, these are higher. So, and sigma and row are subsets and not
singletons. Okay. And then the graph algebra of all subsets is a combinatorial algebra. So you
have to show that these properties hold, which is fairly easy because that's how you constructed them.
What is a combinatorial algebra? That's a structure that satisfies these combinatorial laws.
What do you mean a structure which satisfies them? I mean, that is,
that what do you mean by a combinatorial law? I mean, you mean that the K and S axioms. Yes.
So I see. So you think of the K and S axiomatic system.
Okay. So in this, in this, K would be a set of arrow terms, an infinite set of arrow terms and S also.
Then you have to show that if you use that application operation, then it does exactly
what it's supposed to do with K and S. And you also have to show that it's non-trivial,
that S and K are actually distinct. So that's not, there's no contradiction in the, in the model.
But so basically the point is that
you're saying that the axiomatic form of SK, which, which I think we have some version of those axioms
somewhere. Well, there will be soon be in the axiomatic series.
Yeah, I thought they were going to be. Yes. I mean, it's, it's, it's, it's happening.
So what they are off hand?
Do you know what the axioms actually are? I mean,
Well, I mean, for now, you can just formulate, find the question of proof and just give the
S and K in terms, you say for all X and Y, K of X and Y equals X and so on.
Okay. So maybe there's an example here.
I'm not sure. I mean, Jose did produce the number of examples recently. So I'm not sure they are already
there.
Yeah, he should put this in here. Yeah, that, that will, that will happen. Because now we have
proper notation for these things. Right. In terms of the application operator. Yes. And we, and the
the combinators and so on this nice print forms and everything.
What are we doing with the combinators? We have, we have an actual, what, what, what do we have with
the combinators? Well, just a notation. I mean, you, you will have symbols, combinator S,
combinator K, combinator I and so on. And they will, they will format nicely, but they will not
have any rules attached because you cannot just simplify them unconditionally. Right. Otherwise,
you would always run into an infinite reduction. So they are just there, but
the equation of theories can use them. So you can move. You mean they have, what is the print form?
Bold, bold uppercase letters. Okay.
And what's, what's the full form name? Combinator K, combinator S.
To lie to me, prior recent prototype may already be there. No, probably not. I'm not sure.
It's in a branch, but so it's probably, I'm not sure whether it's in a company,
the prototype or not. And it just happened today. Oh, I see.
It is nice that after a hundred years, we're actually going to be able to slightly mainstream
these things. No. Okay. All right. But basically what you're saying is that
in terms of these,
yeah. So, I mean, I can perfectly well write when I've done that, when we've done that,
you can perfectly well write combinator of S of explained, we can write the rules in terms of
yes, but I would use the application symbol just for so
because the rule, the equation of theories and everything will be expressed in terms of the
application and not in terms of construct. Right. So you would say combinator S.
Yeah, I understand. Right. Right. I wonder what I'm going to do for the thing I'm writing up.
No, I'm actually assembling a couple of examples that will already work in 12.2. So I can send you
those. Okay. Okay. Okay. Okay. Well, you know, one of these days, I'm finally going to understand,
I could just ask Dana, I suppose about this, but oh my God, there's like hundreds of hours of Dana
talking about models of lambda calculus. Oh no.
Oh no. I don't know that I can, I can, um,
well,
is this a model of the typed lambda calculus, the untyped lambda calculus?
I need to understand this. Finally, after all these years, I have known Dana for 40 years,
and it's like, I should finally understand what these, what these models of lambda calculus actually are.
Do you think you understand this? Well, I once did, but that's a long time ago, so I would have to
familiarize myself, familiarize myself with them again.
Let's see.
Optimal reduction of lambdas.
What is un-lambda? Oh, it's a programming language based on, well, okay.
Well, in case you're trying to show something, there's nothing on this.
No, no, no, I'm just playing around here.
Um,
I wonder how this language deals with evaluation.
They don't seem to be addressing that here.
Yeah, yeah, yeah, yeah, yeah, but where is the
concept of evaluation and how the evaluation order works, because that's the, um,
I see.
Hmm, okay.
I think I need to finally understand these models of lambda, of lambda calculus and, um,
gosh.
Um,
hmm.
Well, anyway, okay, I think this is, um,
right.
All right.
Well, we should probably wrap this up here.
Um, yeah, Bob is saying we should get Dana to join our live stream and we absolutely should,
and I've been in touch with Dana in the last few weeks, so he exists.
And, uh, yes, maybe I'll ask him.
Yeah, they, um,
and, um,
okay, well, we should probably wrap it up here in a moment.
Great.
All right.
Well, thanks for, thanks for joining us, people on the live stream, and we will, um,
see you another time.
Sorry, this got into the, the weeds.
What can I say?
The, the, uh, I don't quite know whether these, what are these things that the, um,
um, who knows?
Okay.
Well, anyway, see you another time.
