Alright, hi folks, happy Friday everyone, and hello folks from around the world, welcome
to the 167th episode of the Qiskit Quantum Seminar.
As is tradition, why don't you let us know where you're tuning in from in the chat.
I highly recommend playing with the chat now so that if you have any questions you want
to follow along with, use the chat and I'll definitely use that chat to ask questions
of our speaker today.
To start, I'm Ed Chen, a research scientist here at IVL Quantum, hosting this seminar
for our Research Triangle Park, North Carolina.
While people are playing with the chat, I just have a gentle reminder that this takes
place every Friday at noon Eastern and will be hosted on this YouTube channel still,
and we have a great lineup of speakers in the next few months.
And thank you to our producers, Lacomenev, our video producer, Paul Cyril, and managing
producer, Olivia Lanez.
Coming at the chat now, I see Juan from Colombia, Videsh from India, wow, got Kenya, Monarch
from India, California, India, Brazil, awesome, all over the world.
It's a really amazing group of people we have dialing in, and I think we usually notch thousands
of views of Monterey, California, that's a beautiful place, of course.
And today we'll be hearing from Zoltan Zimboris, who will be speaking to us about problem-informed
graphical quantum generative learning.
Zoltan is the head of the quantum computing information research group at the Wigner Research
Center for Physics and also a senior researcher at Algorithmic.
He obtained his PhD at the Uttavish University in Budapest, worked later at the University
of Bilbo, UCL and Free University Berlin before returning to Hungary.
His fields of interest include the theoretical aspects of quantum computing, as well as the
practical implementation of quantum algorithms on actual quantum devices.
Being a member of the steering board of World Quantum Day and of Q-World, he has involved
in various outreach activities.
Together with his colleagues, he's developed new university courses on quantum computing
that actively uses the online available quantum computer prototypes.
Zoltan has asked that folks interrupt with questions in the chat to follow along, and
I'll read your questions out loud at the appropriate times.
Please, Zoltan, take it away.
Thanks a lot, Ed, and thanks for the introduction and the kind invitation.
And this work that you can see the title of in my first slide.
You can find also in Archive, we put it on Archive on in May.
And it's a work done together with two PhD students of mine, Ben Zabako and Daniel Nagy,
and also two fellow researchers from Ericsson Research, Peter Haga and Jofia Kallur.
So what is our topic?
It's generative modeling.
And of course, in the time of chat GPT, generative modeling is talked about everywhere.
What is it?
Well, basically, we have to learn a representation of some probability distribution in order to
generate realistic samples.
It could be pictures of dogs or cats or sentences of the English language to talk about the
chat GPT.
And actually, among the various machine learning applications of quantum computers, it seems
that generative machine learning could be one of the natural applications.
Why do I say this?
Well, if you think about these typical supremacy experiments that show already for early quantum
computer prototypes and advantage of quantum computers, they are based on actually producing
some probability distribution, sampling from the probability distribution, which would
be hard to sample from with classical methods.
So this already seems that quantum machines would be a good fit for producing hard probability
distributions.
And obviously, because of this, there has been many work dealing with generative quantum
machine learning.
And here, I just showed this at the upper right corner, this very nice table that you
can find in Wikipedia about different type of interactions between quantum computing
and machine learning.
I will be talking about the interaction when we have classical data, but we want to produce
or act with this classical data with quantum methods.
So basically, I want to use a quantum computer in this case to learn a classical probability
and produce samples from it.
And I have listed here a couple of important highlights within generative quantum machine
learning.
And the one that I want to talk about mostly is the so-called quantum circuit born machine.
And more precise, I want to, in some sense, redefine how we could do a quantum circuit
born machine if we have some additional input about our model.
So this is actually a very paradigmatic quantum generative model.
This quantum circuit born machine is very general.
And the name comes from the probability distributions are generated by measurements, and those probabilities
are given by the born rule.
So here you can see a schematic picture of what the quantum circuit born machine looks
like.
You start, obviously, with an initialization of qubits that are in zero, and then act with
unitaries that have some parameters, or like a parametric quantum circuit, and then get
samples from these samples.
If you have, let's say, only a few outcomes, maybe then you can look at the full probability
distribution that you get, and then compare it with the probability distribution that
you want to have, and use a distance, let's say, the total variation distance or any other
distance between these two probability distributions, and then use that as a cost function.
Obviously, if you have too many outcomes such that you cannot actually diagnose the full
probability distribution, you just get samples.
For example, if you have 50, 60 qubits, you will not be able to obtain the probability
of each of the possible qubit configurations.
Then what you can do is actually taking samples from your original probability distribution
that you will want to learn, and from your circuit that you measure, and then use maximum
mean discrepancy, which basically gives you a measure of these, it's a measure comparing
these two samples in a so-called kernel real-world space, and then you use that as the function
that you want to optimize over.
So most of the work have focused on these quantum-circuit-born machines, and they are
like general purpose, meaning that you have, let's say, four random variables, ABCD, and
you want to learn, this is your task, learn the joint probability distribution of these
random variables, and they could be obviously correlated, and often one does like this general
purpose ansatz, which means that you have two qubit interactions all along between any
of the layers connecting them, and then try to optimize the parameter according to the
cost function.
However, there is a big issue here, namely a trainability issue.
In the quantum computing community, this trainability issue often goes under the name
of bar and plateaus, meaning that while you are training your circuit, the derivatives
becomes very low, very close to zero, actually exponentially closer to zero with the number
of qubits, and this, of course, is a huge problem because the way you train if you have
a loss function is that you usually do so-called gradient descent, meaning that you want to
find the point where everything is optimal and has a zero gradient, but if in every direction
the gradient is almost zero, you will not find the global optimum easily.
This is called the bar and plateau problem.
In some sense, this is deeply related to a very well-known folklore in optimization and
machine learning theory.
It's the no-free-launch theorem.
The no-free-launch theorem states basically that you cannot have a general-purpose optimizer
or a general-purpose machine learning algorithm that would have good performance for all machine
learning models, all learning models, or all optimization models, meaning that they
have full average performance.
I would say that this is a very important fact and I would highlight it because it's
often not so much highlighted in the quantum computing community.
The bar and plateaus are, but the no-free-launch theorem is not that much.
It's very important, for example, if you work on quantum chemistry and variational quantum
eigensolvers, I would say if you use the usual unitary coupled cluster ansatz, which is a
generic ansatz meant for all quantum chemistry models, this will probably not work for all
molecules.
However, if you would have some problem-informed ansatz, let's say the ADAPT unitary coupled
cluster or other types of ones, there is more hope to do.
This is similar in quantum machine learning, so what we did was to actually make some insufficient
inductive bias.
Sorry that I skipped over a slide.
This slide was the no-one-model-to-rule-them-all-this-is-a-usual meme in the machine learning community
coming from Lord of the Rings showing this no-free-launch theorem.
We don't want an algorithm that would learn any probability distribution.
We want to have algorithms that learn probability distributions when we have some knowledge about
the probability distribution, so we want to have some inductive bias.
And then there is a question, how to incorporate this problem-specific knowledge?
Well, we don't have to go long because these questions were asked, of course, in classical
machine learning, and we will just use what they use, namely one of the simplest way of
getting some knowledge about our distribution is asking questions about which random variables
are independent from each other or how their dependency relations are.
And using this, we will make a problem-informed answer.
In other words, we use something which in the language of classical machine learning is
called probabilistic graphical models.
So here is just a slide, the probabilistic graphical models.
They usually assign, say something with a graph, assign some dependency or dependency
conditions between the different random variables.
I will come back to the mathematics of this in a slide.
There are two big classes of this, the so-called Markov network and the Bayesian network.
And there is also mixed models.
This is often used for various problems, natural language processing, sensor networks, where
there is an obvious graphical structure or computer vision, and we want to use now quantum
methods to build up a quantum circuit that has such an inductive bias in it.
Going back to math a little bit more, basically, here I show how we should imagine what the
Bayesian network is and what the Markov network is.
It's basically a property of the probability distribution.
So for Bayesian networks, you have a directed graph, and the probability distribution factorizes
in a way where you have these conditional probabilities that you can see here.
In a way that, for example, you can see that E only depends on B.
So if you have the probabilities distribution of A, B, C, D, E, it is such that actually
it only, this factor only contains a probability of E conditioned on B.
It doesn't depend on the other variables, only on B itself.
Of course, B itself does depend on A.
So you have these P, B, A, and P, A, which doesn't depend on anything, has its own probability
distribution, so you can write up these products.
So we have to look, if you have such a graph, we can write up such a product, and we have
to look for probabilities distribution that factorizes in this way.
Markov networks, on the other hand, has a little bit different structure.
We write the graph.
This is not a directed graph.
We look at the maximal clicks.
Here there are two maximal clicks, the ABC.
A click means a graph where everybody is connected with everyone.
So the triangle, you are connected all three points.
And CD is another click, it's a small click, and you look for probability distributions
that factorize according to the clicks.
So here the probability of ABCD equals actually two factors.
One factor is, it depends on your ABC and the other on CD.
You have to have a normalization here, right?
So here you have these factor tables.
So what we want to do is that, once again, I'm just mentioning an example here, I give
you the values of ABCD, it could be 0 or 1.
And for each of these values, I assign for A1, D1, C1, a value for phi 1, and for, let's
say, C0 and D1, a value for phi 2, CD.
So this one, and then I have to normalize.
This is how you get your probability distribution.
And actually we will use, we will randomly generate these factor tables, and then we'll
look whether we can learn it with a quantum machine learning approach.
Okay, so how do we do it?
Actually what we do is that we use a higher order ising Hamiltonian, if you are familiar
with QAOA, this is type of Hamiltonian.
But here you have these ZZ terms, but here we can have multiple Zs over the clicks.
So our first approach, which I show here, I hope you see my pointer, our first approach
was that we put I plus ZV, V is a vertex, and over all clicks, a product over these,
and over each of the clicks, and then some over the clicks.
Apart from this, you can actually obtain a new Hamiltonian, where you have higher order
Z terms.
It could be a single Z, Z1 times Z2, Z1 times Z3, I will give you an example, and each of
these Zs will have a coupling alpha, and that will define a unitary, which is a parametric
unitary, and this will be our parametric circuit.
And after this, we do this parametric ZZ-circuit unitary, we also apply a generic one-qubit
gate.
And the initialization of our state is not 0, 0, 0, 0, but actually 0, 0, 0 acted with
all the Hadamard, so it's a uniform superposition of all computational basis states.
So I will give you a particular example in the next slide, but let me just compare with
the usual quantum-circuitizing ball machine approach.
Well, it's very similar, it also uses a ZZ to qubit gates, and then at the end, one-qubit
rotations, and we also start from the after Hadamard's uniformly uniform superposition
of all computational basis states.
However, we include other interactions, not only ZZ type of interaction, but ZZ, ZZ,
and so on.
On the other hand, we don't include all ZZ interactions, so we don't have any qubit,
it's not interacting with any other qubit, exactly because we have these independency
relations.
So let me show more completely what we do.
Here is an example of this graph that I showed previously with four nodes, ABCD, these can
be regarded as random variables, binary random variables, so they take either 0 or 1 as a
value, and what we do is that we look at each click, let's say we look at the ABC click,
and write up the Hamiltonian which contains all the Z terms where you multiply some subset
of Zs belonging to these vertices.
So for example, ZA, ZB, ZC is one, ZA, ZB, ZA, ZC, ZB, ZC, and the single side Zs, so
you can see all of them here, and we assign a free parameter to this.
And for here we would have ZC, ZD, so this is the one, and then we exponentiate this,
this will be 3 qubit and 2 qubit gates in this case, however these 3 qubit gates, they
are an exponentiation of a term with triple Z, Z0, Z1, Z2, these, and it's very easy
to decompose this into 2 qubit gates, and this is the way how you do it as I show it.
So of course one way of doing it is that you can ask, can I represent any probability distribution
which is such a model?
Of course, yes, if I take the full graph, then I have one largest click, the full click,
and it's a complete graph, everybody is connected to everyone, and then I have all the terms
and of course I can then with the circuit represent any probability distribution, but
this doesn't sound very useful, and I will show it's not super useful, so our question
is when is our representation useful?
And I have looked into some type of probability distributions, and here you can see, we looked
into 9 qubit cases, so when we have 9 random variables that are binary, taking values 0
and 1, and the independence relations are showed by this graph in the first one, like
a rectangular lattice, or this rectangular lattice with these extra connections here,
and also in the right hand side, and even a little bit denser lattice, and what we see
here is that if we try to learn such a probability distribution that we generated randomly with
those factor tables that I showed, then in the first case, the quantum circuit ising
born machine actually performs not that badly, I mean not that badly compared to the quantum
circuit Markov random field model, you can see, I mean here we do why we have four lines,
this depends on the fact how we do what we choose as a cost function for the optimization,
we can choose the KL divergence, cool black library divergence, relative entropy or the
maximum mean discrepancy, these are like very typical things that you can choose from, and here
you can see that in the first case actually the usual quantum circuit born machine performs well,
the reason for this is that actually these correlations like they have gates that are
pervised, they can capture very well these pervised correlations, however when you start to have
clicks already that are like here you have the triangles form clicks, in the second example,
then also in the probability distribution you will have higher order correlations and it's
much higher to much harder for the usual quantum circuit born machine to ansatz to get those
correlations while our model can get it, and on the other hand we don't have that much parameters
because we don't take into all connections just the ones that are connected to the clicks,
and in this third case we have even more clicks and you can see how the usual quantum
circuit born machines performance decreases while we still find very well the probability
distribution which is given by this total variation distance, the distance between the true
probability and the ones that we learn, however when you can ask can we learn it well or how well
can we learn different probability distributions, do we have for example this issue with the
barren plateaus, okay so we looked at this question and we looked at into
different type of models, one is the complete graph where the maximum we have the maximal click
and we have all the z terms, so we have two to the n degrees of freedom and when we try to
when we try to learn the cost functions variance actually decay exponentially which
actually means that we have a barren plateau, on the other hand for more less dense graphs for
example we looked at these triangle chains which are very much related to the previous figures
with the second one but here we have chains and those are random graphs, there we see actually
on an algebraical decay of the variance which could mean that we don't suffer in these cases
from barren plateaus that would be a great example of problem-informed model don't suffer from
barren plateaus and I would say that although we don't suffer from barren plateaus and we only have
fully n parameters in the in the circuit itself, it's classically still hard to produce those
probability distributions, I will come back to it and we have, it seems that we have a good
trainability for this using our approach while the general quantum circuit born machine won't have
a good trainability and so this means that we have an efficient, we have a good efficient
Markov network representation of these models. Let me also talk a little bit about the concurrent
work or also recent work that took the same approach as us, however they didn't look at the
quantum Markov random field networks, they looked at Bayesian networks and they did something called
basis enhanced Bayesian quantum circuit where they took a Bayesian network, typically this was the K-gram
network and added some one qubit basis rotations at the end and looked at the probabilities distributions
that can be obtained from this and then have some certain property, they showed that this is more
expressive than the usual classical Bayesian networks, this work one I appeared in PRX in
2022, Gao et al it came from the Sirac group and what we were doing if in classical machine learning
there are these two big cases of Bayesian networks and Markov networks, a Bayesian network is what
treated quantumly in this work and we treat the Markov network it would be nice to compare these
two, it's not so easy because one is about a directed graph, the other is about an undirected
graph but actually there is a way that the Markov network can be put into a form of a Bayesian
network but I mean it's rather a computational intensive trans, it has to be triangulated for
example if we have this Markov network showing ABCD this graph we can do this triangulation and do
and direct this graph in such a way that we get a Bayesian network but then of course the Bayesian
quantum circuit will require significantly more quantum resources than the R network, the quantum
circuit Markov random field method but we tested it and what we show is actually that although we
had much much less parameters for basically what we got is that we perform equally well as the quantum
circuit, as a Bayesian quantum circuit of the Syrac group you can see here and what happens here
what you can see mostly is that the number of parameters you can see the depth for them
and for us and for us depth didn't increase if we look at polygons of size for 6, 8, 10, 12
up to 18 while for them it increased the number of parameters and the depth also increased and
you can still see that R method learns as well as their method but so this means that we have a
similar performance at a much lower cost I have to tell you of course that I mean here is of course
there is a little bit of a mismatch that we have to do this triangulation to get the Markovian network
but it's still when we can compare then our method performs as well with smaller depth
so now we have learned that our method trains nicely it doesn't contain that many parameters
in many random cases but can we get any advantage well I mean what do we mean by advantage that could
be many things when we talk about quantum machine learning there could be quantum learning advantage
when we learn it learn the model in for example accuracy we can learn it better we showed for
example that that if we learn better than the for certain models than the quantum the usual
quantum circuit ising born machine we maybe there could be an advantage compared to classical
model in learning speed and so on or sample complexity we need less samples well probably
the learning speed there won't be too much advantage there although it's a little bit unclear
but where we can say definitely is where we can have a advantage is in the inference if you have
learned them learn some probability distribution how fast can you get it and we can show if we
look at the circuits that we produce this Markov network circuits these are in this
Venn diagram these are denoted by this green set or this green circle this intersects with the
vision network as I told you but if you look at those circuits actually it intersects also with
with certain the QAOA which is a circuit for optimization and it's unclear whether QAOA has
good advantage for optimization but it's known that QAOA sampling QAOA circuits is hard so
classically to get the same distribution that the QAOA circuit produces for some values of the
circuit some parameters is hard this was shown by several works here is why one by Fary and Harrow
so this this means so our work is in this if we look at in this shaded region we can we know that
actually it there are some circuits where it's hard to sample from probably there are many of them
so this means that that if the target distribution is learnable up to a given error by both the
classical and quantum model it could happen that or it is probable that it happens that the that
sampling from the train quantum circuit is more efficient so the inference I mean
if you think about chat gpt I mean this is maybe not the good thing to to think about it's learning
chat gpt a learning task is very hard for chat gpt but also to to put out those sentences that we
want in these larger models is also a hard task for which you need many GPUs but it could be also
you could have also some other cases that perhaps the learning is not so hard but but putting out
the samples could be a hard task and in that case we feel that there is a potential for
quantum advantage I mean we cannot prove it but we know that we have circuits that that what we
know is that we have these quantum circuit mark of random field models where the sampling
from those would be hard classically so they are probably the output probability distributions
that classical methods would have hard time to output okay so let's let's talk about what what
we found that's promising you know in our method well we have these higher order correlations
compared to the compared to the born circuitizing machine so it outperforms this problem agnostic
model the usual quantum board machine and for many for many problems for the most promising
problem we have only a polynomial number of parameters and it seems that the variance of the
cost function doesn't decrease exponentially so from maybe we can avoid burden plateau so it
could mean a better trainability than for problem agnostic models which seem to have a bottom plateau
you most usually although it's hard to compare with Bayesian networks but if we if we want to compare
our method gives the same performance but with less cost like less deep circuits
and there are problems in our methods there are probably distributions that are hard to
classical example so there is a potential here for quantum advantage thanks a lot and here you
can find the QR code for our paper and here are my co-authors thanks all right thank you Zoltan
I didn't see any questions throughout the chat
if anybody has a question please be sure to drop it into the chat I had a very quick and
probably naive question but in the training over epochs that you showed yes here yes yes
I don't I'm not quite following uh so you instantiated random graphs with random probably
distributions uh what you mean with this slide right yeah yeah exactly so what we do here is
they are not random graphs but they are random random distributions exactly yeah so what we have
is that uh so in this case it's uh the probability distribution is very easy it's a product over
uh two point probabilities that are that meaning or or two point factors like factors over only
two variables like this yeah yeah and uh and you have to multiply apply them and
and and okay we looked at this one and and this factor from the factor table that I showed you
we randomly assigned some numbers to the factor so these were like randomly chosen probabilities
that we gave and we this one like small uh I mean it's it here you had like five qubits and then
he had you here in qubit and so on so there were some small qubit numbers and what happened was
that we could both compare the full probabilities because from samples because we had enough samples
these were so these we trained with the KL divergence but we also did the case that would scale the
maximum mean discrepancy which compares basically the samples and here what we see is that
we have this epoch so each um we we we learn and infer new values
sorry and um yeah and and and and we can see that it's roughly the same as as the as the
Bayesian uh quantum circuit by this group it it has the same properties but I mean the depth
our depth is much much smaller yeah
perhaps I should also mention that classically there is an issue with training
around uh mark of random fields so this mark of networks and the issue is that
it's very hard to locally update the those those factor tables that you you want to like like learn
because actually you cannot locally update it because then also or learn it locally also this
there was this hybridization factor here this one over said
yes this one over said it contains of course this is like this partition function all the sums of
all these uh five one five two products so that that's like a long-range coupling
in and in our question in our model since the quantum circuit or immediately generates
probability distribution this is automatically given sort of so so we we also believe because
of that it's hopefully it's not much harder to train than classical ones and we get that
mean advantage in producing the probability distribution see all right thanks for clarifying
that point um and yeah I like that you finish with these open directions of uh yeah yeah and uh
that should be very exciting for those students who are embarking on their quantum journeys
um in this field and where they want to tackle these next levels of problems yeah so I would say that
like is the last word maybe that often currently uh models based on or or quantum
circuit and quantum algorithms based on parametric quantum circuits not that popular
and there are all these results about uh bar and plateaus and so on however I would say that it's
there are some reasons why they are not popular but we have to be a little bit careful if you have
a problem informed parametric circuit that's good because you decrease the number of parameters
the circuit is built up a little bit according to your problem so you have some information
and maybe even you might have information about uh how or not to choose randomly those parameters
but in the in the proper way then all these uh arguments that go into the theorems that show
uh bar and plateaus will actually disappear uh bar and plateaus coming from noise is a different
question that might be a more problematic one that also appears in these cases but I would
say that there is still hope for that I mean it's like hope but the hope for parametric quantum
computation okay thank you so much for your time we really appreciate it and uh before we uh sign
off people uh next week uh we're off for uh holiday and then the week after we'll have
professor Yoonah Kim from uh I believe Cornell University talking to us about uh their latest
work so thank you professor Zimbora on telling us about your latest work and uh if you if the
folks in the audience have any questions feel free to reach out and uh I'm sure he'll be happy
to entertain your questions all right uh thank you and everybody have a great week thank you very much
