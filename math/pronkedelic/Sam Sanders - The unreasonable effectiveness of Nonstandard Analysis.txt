Okay, let's talk a new actual session, the next speaker is Sam Sanders from Munich, I
guess.
We'll tell us about the unreasonable effectiveness of non-standard mathematics, not mathematics.
As you can see from the title of my talk, I will be discussing the computational or
effective content of non-standard analysis.
Now that there be such content, so non-computational or effective content in non-standard analysis
is a non-trivial statement, at least if one would ask these luminaries, Alain Kohn and
Eric Bishop, their view was that non-standard analysis has no numerical, constructive or
computable content.
Bishop went as far as calling on standard analysis a debasement of meaning, meaning
for him was computational or constructive content.
Alain Kohn stated in, well, somewhere, I think it was the mathematical gizette, once
you have one infinitesimal, all the computational content is out of the window.
So what is one to do in the face of such adversaries, well, prove them wrong, obviously.
So in this talk, I will try to convince you that non-standard analysis has tons of numerical,
constructive and computable content, and, well, that would be metric tons, by the way.
So the technical aim is to show that theorems of pure non-standard analysis give rise, produce
effective theorems, non-involving non-standard analysis, and vice versa.
So what do I mean by these things?
Pure non-standard analysis is, well, non-standard analysis only involving the non-standard
definitions of continuity, compactness, differentiation, Riemann integration, and so on.
Throw the epsilon delta out of the window, and ye shall have computational content.
That's pure non-standard analysis.
We also see that we can go beyond the analysis, that's the third part of the talk.
What's an effective theorem?
What you think it is?
A theorem from constructive or computable mathematics?
Or if that's impossible, some explicit or effective equivalence from reverse math?
So I'll show you some machine where you can throw in a proof of a pure non-standard analysis
theorem, outcomes, if you're lucky, constructive or computable analysis, otherwise some classification
from reverse math.
Now what's the vice versa about?
So I'll show you non-standard analysis, shake, shake, shake, outfalls constructive math or
reverse math.
But if you did it correctly, if you do the proof mining as it's called, well, you can
go back.
Certain effective theorems, which are christened urbanization, imply the original non-standard
theorem from which they were extracted, so there's somehow equivalent in the meta-theory.
The motivation for all this is, if you pick up a textbook on non-standard analysis, at
some point the author will always say, yeah, I know non-standard analysis and it's all
supposed to be non-constructive and ultra-filters and infinite decimals, but it still feels
somehow constructive, effective.
The practice of non-standard analysis is somehow constructive, and this is best formulated by
Horst Oswald, Munich's very own, in his notion of local constructivity, about which we will
write something in the forthcoming Brouwer volume.
So non-standard analysis 101, just in case.
It all really only started with Abraham Robinson's non-standard analysis in 1965, and essentially
what it amounts to is a semantic approach for a given structure M, say the real numbers.
One uses a free ultra-filter to build a non-standard model.
If you're a constructivist, you can use a weaker filter and build this thing inside
martin-leuph type theory, but most of us would just use a free ultra-filter.
So here we have our structure M sitting inside of the non-standard model star M. If M has
the natural numbers, then star N, of course, has a set which has the same properties as
the natural numbers, but much bigger, and we say that the new elements, not in N, are
the non-standard numbers.
In general, there is what is called a star morphism, which you build using the free
ultra-filter, which takes any set in M, X, and blows it up to star X in star M, and unless
it's finite, it will be much bigger.
So X contains the standard objects, star X minus X contains the non-standard objects,
and all this is particularly useless.
Non-standard models were already known to school them, and you can't really do anything
with the non-standard models alone.
What makes non-standard analysis tick, and which is Robinson's genius contribution, is
how M and star M are related.
Non-standard analysis is jumping back and forth between M and star M. If you run into
trouble here, you just go to the non-standard world and back after a while.
It's how these things are connected, and that's what Robinson's contribution was.
The three important properties he proved, which connect a structure and its non-standard
model.
So the first one is the transfer principle.
They were talking about formulas as ZFC.
The formula holds in the original structure, if and only if, star phi holds in star M,
and star phi just means putting stars in front of all the constants, thus blowing them up.
The truth in the small structure is the same as the truth in the big structure for formulas
in the original language.
Now there's also what is called the standard part principle, which is sort of the reverse
of the star morphism.
Star morphism blows up something in the small structure to something big.
But ultimately, we're interested in going back to the standard world to M, because that's
where real math happens.
And that's what the standard part principle does.
It tells you for any object, there's a standard object, so an M, which has the same standard
objects.
So, for example, for any hyper-real, which is finite, you can take the standard part,
which will be an actual real.
So, the star morphism allows you to go into the non-standard world.
The standard part pushes you back.
It should not be a surprise that the standard part of star M is exactly this M.
And then there's idealization and saturation, but we're gonna say dot, dot, dot there for
now.
So, this is Robinson's semantic approach, it's very powerful, it has a lot of advantages.
There's also, by Nelson, a syntactic approach to non-standard null, which deserves to be
better known.
And it's called internal set theory.
So, rather than building a model and so on, it's based on, yeah, extensions of the language,
syntax.
So, Nelson says, well, let's just take the language of ZFC and add a new predicate.
X is standard, STX, unary predicate.
We introduce two new quantifiers.
There is a standard and for all standard, which means exactly that there is a standard
and for all standard objects.
The formula is called internal, if it does not contain the new predicate ST, and external
otherwise.
And it's only internal formulas can be used to form sets.
That's why it's called internal set theory.
Internal set theory is just, IST is just ZFC, which is an internal, ZFC is just our internal
formula, and three new axioms, ISNT, idealization, standard part and transfer.
So Robinson had these as theorems, and Nelson just adopts these as axioms.
Transfer just says, well, if something is true for the standard rule, it's true everywhere,
with some conditions, standard part is standard part, and idealization essentially tells you
how you can push the standard quantifiers to the front of the formula.
It's just a bookkeeping device.
That's very, it's just very useful.
Transfer is about as non-constructive as the original sin, the law of excluded middle.
Standard part can be salvaged, and idealization, as I said, is just a bookkeeping device.
So, ZFC and IST prove the same internal sentences, IST, of course, proves them a little faster
on average, but ultimately, it's just that, a conservative extension for the original
language of ZFC.
So this is IST, and we can prove similar conservation results for fragments of IST.
And I'll, we'll be looking at a fragment of IST based on Gödel's tea.
So this is the paper, Beno van den Berg, Evin Brisset and Pavel Safarik.
They looked at Gödel's system T, which is just primitive recursion, which you all know,
in all finite types.
So, PRA, Hilbert's finitism, whatever, just has primitive recursion for functions.
This has, system T has primitive recursion for all finite types.
Actually, the system is named EPA-omega, pioneering the taking of finite types with
the axiom of extensionality.
So, as I said, we can have idealization, it's a bookkeeping device.
So, we can have a version of standard part, but we have to lobotomize it a little.
This is called urbanized axiom of choice, so here you see your typical axiom of choice
kind of stuff, for all standard there is a standard, then there's a choice functional.
But the choice functional is rather stupid, it does not provide a witness to why, it provides
a finite sequence of witnesses.
Of course, if we want a conservative extension of piano arithmetic, we have to somehow make
standard part weaker, or the axiom of choice.
So this is how it's weakened, if it does not provide a witness but a finite sequence
of witnesses, as you can see here.
No transfer, as I said, is the original sin of non-constructive stuff, and the system
P, which is good or C plus non-standard stuff, is a conservative extension.
And we could do the same thing for HA-omega and intuitionistic logic, it's all the same.
So far, I've hopefully only told you stuff you know, there's Robinson's non-standard
analysis, there's Nelson's syntactic approach, and we can do something similar for fragments.
Now comes the new stuff, a new computational aspect of non-standard analysis.
There's two words you have to remember, term extraction, as follows.
So again, in this paper, it's somewhat hidden, but you can bring it out if you read the paper.
If system P proves for all standard, there is a standard.
From this proof, we can extract a term such that the system proves for all there exists
and that there exists is witnessed by T, where T is again this Erbrandt witness, a finite
sequence of witnesses.
And this should be compared to another work of art by Goedl, the Goedl-Jinson or Friedman
translation for pi-02 formulas.
So if Pianometric proves some pi-02 formula using classical logic, then from this proof,
one can extract the term such that HA omega proves for all n phi n Tn.
So even though classical math is classical, we can still get computational information
about pi-02 formulas.
So for all numbers, there is a number.
Now what is different here, which is essential, is that these are pi-02 formulas, which is
a very small class of the edifice of mathematics.
However, this kind of formula is actually a very large class.
First of all, one observes that the usual non-standard definitions, continuity, compactness,
Riemann integration, all can be brought into this normal form for all standard there is
a standard.
And secondly, these normal forms are closed under modus ponens.
So any theorem of pure non-standard analysis can be proof mined using this kind of term
extraction result.
So this is the fundamental difference.
Here we have pi-02, a very small class, but if we butt straight into non-standard analysis,
this kind of class of theorems we can study is very large, namely all of pure non-standard
analysis for which there can be term extraction.
So non-standard analysis is very non-standard in that sense.
It has a huge class of theorems, which may be proof mined, if you like, whereas here
there's more of a restriction.
Of course, non-standard analysis, she is cheating because she has a much richer language.
So what do you mean by pure non-standard analysis again?
So theorems of non-standard analysis formulated only with the non-standard definitions of continuity,
compactness, and so on.
You know from calc 1 to 5, the epsilon-delta definitions, there's also the corresponding
non-standard definitions, which have less quantifiers in them.
And those definitions have this sort of normal form, whereas the epsilon-delta do not, of
course.
I'll show you examples now.
So here, for example, we have non-standard uniform continuity on the unit interval, which
well, no, not many quantifiers, for every x and y, if x and y are infinitely close, then
the images are infinitely close.
That's non-standard uniform continuity.
And if for a function f we can prove this in p, from this proof we can extract a term
t from Gödel's t, so primitive recursive, such that ePa omega proves the usual epsilon
-delta definition.
With a twist, I invite the versa if ePa proves 2, then p also proves non-standard continuity.
So 2 is the notion of continuity, where t is called a modulus from constructive and
computable math.
So normally we have for all epsilon, there's a delta.
Our t is kind enough to compute this delta for us.
So from a proof of non-standard uniform continuity, we get a modulus of non-standard uniform
continuity.
And so from, if we stand in the meta theory, constructive continuity or non-standard continuity
is just the same thing.
So we're doing it right.
I pour les constructivistes la même chose.
You can also do this in this heiting arithmetic and all that, but you guys being set theorists
and all may not care so much for that.
I'm still going to mention it every time.
Now let's look at a real theorem, which you all know from Calc 1.
Continuity implies Riemann integration.
So here we have the non-standard version.
Non-standard continuity implies non-standard Riemann integration.
So we need some notation of course.
So these pi these, they are partitions of the unit interval.
So 0, x0, t0, t1, x1, and so on, until xn and 1.
So this is a partition of the unit interval.
They have infinitesimal mesh.
So this pi is the maximum for i below n of the distance between xi and xi minus 1, for
the distance between two partition points.
So it has to be infinitesimal, so the partition is infinitesimally fine or infinitely fine.
And s pi of f is just the Riemann sum, which is just i from 0 to n, f of ti xi minus xi
minus 1.
So this is all stuff you might remember in the back of your mind.
So if our partition is infinitely fine, the Riemann sums will be infinitely close, of
course to the real Riemann integral.
Now assuming we can prove this in p, well actually you can prove this in p, we can extract
the term s2, so type 2, so that for any function f and any modulus g, if g is a modulus of
continuity, sg is a modulus of Riemann integration.
And this is exactly what the constructivists and the computabilists do.
A modulus of continuity is transformed by this explicit term s into a modulus of Riemann
integration.
If you read Bishop's book, this is exactly what they do in constructive and computable
analysis.
So from a proof that something non-standard uniformly continuous is non-standard Riemann
integral, from this proof you can read off this term s.
Every step in the proof is a sub-term of s, and at the end you just pile them together.
So this can be really read off, and I have a postdoc who is computerizing that process.
And you can do the same thing in intuitionistic, wow.
Now what happens if we take something that's known to be non-constructive, such as reverse
math?
I think you at least have heard of reverse math, pioneered by Harvey Friedman, 75.
It would prove stuff like the following.
RCA0, which is, say, computable math, proves some nice equivalents.
So you have ACA0, which is sort of the Turing jump exists, is equivalent to the monotone
convergence theorem.
Every monotone sequence in the unit interval converges.
And it's called reverse math because this is the usual way of doing math, from axioms
to theorems, and you can also prove the reverse, reverse math.
So this is the first equivalence in Steve Simpson's book on reverse math, and you can
prove a non-standard version thereof.
So a fragment of the transfer principle, namely for pi 01 or sigma 01 formula, can be
proved to be equivalent to the fact that every standard monotone sequence non-standard
converges.
As I said, we're interested in pure non-standard analysis, so no epsilon delta.
And so a sequence xn non-standard converges to x, if and only if, for every n.
If n is non-standard, then xn is infinitely close to x.
The non-standard convergence, usually you take the limit for n going to infinity.
In non-standard analysis, we have infinity, namely the non-standard numbers, we just plug
it in, and it's supposed to be infinitely close.
So here we have some reverse math equivalence.
Nobody would care, because nobody likes non-standard analysis, usually.
But then you do the proof mining, and you get two terms, u and v, such that if chi is
the Turing jump functional, then u of chi computes the rate of convergence of any monotone
sequence.
But if psi computes the rate of convergence of any monotone sequence, then v of psi is
the Turing jump.
So this is the explicit equivalence, ACA0 is equivalent to the monotone convergence
theorem.
So normally you prove non-effective equivalences in reverse math, ACA0 just says that the Turing
jump exists, and the monotone convergence theorem says yeah, this or that converges.
On the non-standard thing, you can actually extract all these effective results, namely
expressing the Turing jump in terms of a realizer for MCT, M5C-versa.
Ah yeah, you can do reconstructability.
So now to convince you that it's not just about analysis, we'll look at an example
from group theory.
So one of the stronger systems in reverse math is pi11 comprehension.
And this is equivalent to some theorem of group theory, which says that every countable
abelian group is the sum of a reduced and a divisible group.
The non-standard version of this equivalence is as follows.
Instead of pi11 comprehension, we have pi11 transfer, and it's equivalent to every standard
countable abelian group, is the direct sum of a standard, divisible group, and a standard
reduced group.
From this, rather for my standard strong statement, we can extract the following things.
If chi is the Susselin functional, then u of chi computes this direct sum, and if psi
computes this direct sum, then v of psi is the Susselin functional, where the Susselin
functional is a realizer for the chi11 comprehension scheme, more or less.
And this is again one of these explicit equivalences.
So who would have thought, like if we only had done our reverse math in non-standard
analysis, the computer could just extract all the effective content.
But alas, we did not.
Compactness is a particularly pretty animal, by the way.
So a space is non-standard compact if and only if, for any point, there is a standard
point infinitesimally close by.
And for example, you could prove that the unit interval is non-standard compact if and only
if every non-standard continuous function is non-standard Riemann integrable.
So and this is not uniform continuity, this is usual continuity.
Then you can extract terms such that if omega is the fan functional, then u omega computes
the Riemann integral, and vice versa, if it computes the Riemann integral, then you
can compute the fan functional.
The fan functional is a realizer for the fan theorem, which is the classical counterposition
of Wkl.
And so yeah, explicit reverse math for all.
However, compactness has multiple, non-equivalent, normal forms.
More than a normal form has the form for all standard, there is a standard.
And one of these normal forms, let's call it peak and compactness, is the space can
be discreetly divided into infinitesimal pieces.
This is exactly what physicists and engineers do when they trot out the infinitesimal calculus.
You can't let a physicist near a compact space, or he or she will try to divide it into infinitesimal
pieces.
That's what they do.
So this is peak and compactness.
And for example, if we have a peak and compact set, then the image is also peak and compact
if f is non-standard uniform content.
If we run this through the proof mining apparatus, we get the following, which is very ugly because
it's a theorem of constructive math.
If psi witnesses the totally boundedness of x and g is a modulus of uniform continuity,
then u psi g witnesses that the image is totally bounded.
Now what's totally bounded is the preferred notion of compactness in constructive and
computable math.
So let us get this straight.
From the preferred notion of compactness, peak and compactness, what the physicists might
use, outfalls the totally boundedness, the notion of constructive and computable math.
You would almost start believing that there's some independent reality which we're discovering
because it's so beautiful.
Which I do.
Right.
So technical conclusion, I don't think we're going to get much further.
Chair permitting.
Yeah.
So non-standard analysis is unreasonably effective as follows.
Look at pure non-standard analysis, where pure means use the non-standard definitions.
For example, psi on delta, the non-standard definitions of everything, term extraction
works for the huge class of theorems of pure non-standard analysis.
Every theorem of pure non-standard analysis has this normal form and we have this term
extraction property, this, this urban term, witnesses.
So it would seem that quantum bishop were wrong, to say the least.
And can I have one more slide?
Yeah.
So, I, maybe two, urbanization.
So what we've seen so far is that take proof in pure non-standard analysis, run it through
the Vandenberg et al machine, you get constructive math or constructive, effective reverse math.
We can do better than that, actually.
So here we are again with non-standard continuity implies non-standard Riemann integration.
If you try but a little harder, you can extract two terms, I and O, and they do the following.
So what is that?
So intuitively speaking, if we want to compute the Riemann integral up to precision epsilon
prime, we need to know that f is continuous as witnessed by g up to this precision.
The normal mathematics says what continuity implies Riemann integration?
This thing, number five, says a little bit of continuity implies a little bit of Riemann
integration.
It's a point-wise version, and it's in fact a theorem of numerical analysis, which I
call the urbanization, and it's equivalent to the non-standard version in the meta theory.
So from the proof of this non-standard thing, you can extract these terms, and if we have
a proof of number five, we can manipulate that in a proof of number four, all very algorithmically.
So non-standard analysis not only has some constructive content, which we can mine, we
can prove that in the meta theory, it's equivalent to some theorem of numerical analysis or vaguely
reminiscent of that, and every theorem of pure non-standard analysis has such a meta-equivalent
urbanization, evolving lots and lots of up to precision, this up to precision, that.
So that is quite something.
So theorems of non-standard analysis, believed to be totally non-constructive, are in fact
meta-equivalent to say numerical analysis type of stuff.
I therefore claim the moniker, the unreasonable effectiveness, is not so unreasonable.
Then Cambridge is very own, the Gandhi-Highland functional, named after Robin Gandhi and Professor
Highland.
So it's defined in terms of itself, which is very bad mathematics, you would think, but
it still makes sense.
So what do I mean by that?
Gamma takes as input a type 2 object, so which y takes as input a sequence, so in the sequence
is s followed by 0 and another sequence, and it's defined in terms of itself, because
gamma is defined in terms of gamma.
So gamma at y and s is defined in terms of gamma at y and successors of s.
So we can apply this definition again, so, well, gamma of s n plus 1 is this, but again
there is gamma in olive green and so on and so forth.
So we can keep applying this definition, and it seems to be a non-terminating recursion
or if you want non-well-found self-reference, call the philosopher self-reference.
Now they don't care about this, by the way, the Liar sentence is perfectly fine, but this
is totally ignored, unfortunately.
Anyway, gamma is not primitive recursive in the sense of Goethe's T, and this G is primitive
recursive, and it approximates gamma, so we can do the, it does the usual Gandhi-Highland
functional thing, but the self-reference is gone after m steps.
This is called a stopping condition.
You can only apply the recursion m times before you go default to the first one.
And of course, for non-standard m, G equals gamma.
So gamma is not primitive recursive, but G is, and what, they're equal if m is non-standard.
Now, and you can do this if you add some non-standard continuity, because it's actually bar recursive.
From this non-standard, again nobody cares about this non-standard stuff, until you show
them that from this non-standard stuff you can extract a term computing gamma in terms
of the fan functional and some continuity stuff.
So from non-standard analysis, claimed by Kahn and Bishop to be very non-constructive,
we can get recursion theory or computability theory, which is totally neat.
And finally, and then I promise I'll shut up, the partial computable functions are a
central object of study in computability theory.
In Soros book, there's an entire motivation why we should go partial, and of course, good
old system t is total.
So is Martin-Lewith type theory and this non-standard system p, because well, non-standard analysis
and partial objects is dangerous.
However, non-standard analysis can simulate partiality.
So everything is total in non-standard analysis and its partial system, as is system p.
But we have the following definition.
If we have the eith computable function, it's called standard total.
If for every standard input, it holds after a standard number of steps and standard partial
otherwise.
There are total, but standard partial phi e with standard index e.
So in system t, or in p, or in Martin-Lewith type theory, everything is total, so we can
only look at total recursive functions.
But there are standard partial ones, so we can mimic partiality in a standard world.
Again, people would say, yeah, but what is it good for?
Actually they have.
And actually, the previous results on the Gandhi-Highland function, you can sharpen considerably
if you use this apparatus.
So you talk about associates rather than type 2 functionals, and these associates can be
standard partial.
They're all total, but standard partial.
And then the previous results get sharpened and it's all wonderful.
But I've taken it up more of my time than I should have.
So vagueness, we shall skip.
I ask, this will not replace any of the existing fields.
This will not replace constructive computable math proof mining.
You'll just unify them.
I'm not here to take your job.
I just want to make it more interesting.
Thank you very much.
Thank you.
Thank you.
And of course, good good will have to be quoted.
Luckily, we have a long break now, so.
You don't say it.
Any questions?
Yes?
I have a question regarding Nelson's point of view.
So he was not only constructive, but he was a finalist.
Yes.
He developed some kind of approach to radical finalist probability theory, why I think he
also used non-standard.
ISD, exactly.
And so Mike, my question, the first question is, how did he, well, did he realize that
there's some computational content there?
And the second question is, when it comes to Conn, I think he developed his own way
of doing non-standard analysis.
Could you say a little bit how this relates to what you're doing?
Yes.
So Nelson had this, yeah, Nelson had some strange views sometimes, and he developed
this radical elementary probability theory.
I don't think he was so interested in the, he would not be interested in this, because
to make this tick, you need the exponential function, essentially, because these ur-brands
witnesses, they are finite sequences.
To nicely code finite sequences, you need the exponential function.
I don't think Nelson would have cared about this, because it's not, yeah, the exponential
function was inconsistent according to Nelson.
I think he was aware of this somehow, because he does hint at this in his publications about
radical elementary probability theory.
So one of the, one of my plans is to, there's some books on this, to see what's the, yeah,
what content can be mined from Nelson's probability theory.
I don't think Nelson, he didn't, he never responded to my emails, of course he was dying.
I don't think he cared really, because it's not, it has to be sub-exponential, or it's
not true.
That's, that's where his view, that he was a very strong finiteist.
Kohn had indeed developed his own view, I mean his own version of non-standard analysis.
What's going on here?
Ah, yes, we can, there's actually a nice example.
So with the Gandhi-Highland Functional, this G is independent of the choice of non-standard
number.
G equals gamma for any non-standard n.
And that's sort of the non-standard notion of computability.
If we can define something non-standard, using non-standard numbers, but independent of the
choice of number, like this G, it corresponds to a standard object, I call it omega invariant.
Kohn had this thing with traces and what not, and ultimately one could somehow construct
or compute something in his formalism, if it was independent of the particular choice
of trace, if I recall correctly.
So there is some correspondence there, I take it, but Kohn's approach is equally constructive
or non-constructive as usual non-standard analysis.
Canovay et al and Katz have looked at this in very minute detail actually, so there's
a huge paper on the archive and published somewhere.
So I'm just going to defer to that.
