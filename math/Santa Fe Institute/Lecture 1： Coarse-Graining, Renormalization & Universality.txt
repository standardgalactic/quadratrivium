All right. Thank you all very much for coming again. This is now the fifth lecture. We'll
read that together right now. No. Yes, fifth lecture. I'm very grateful for the chance
to talk to you. And because all of these ideas are not, they're not written down, I think
in a completely different way. And so, when I try to put these things together, I do a
lot of work trying to understand how to better myself. So, in some sense, to give me an excuse
to understand what I'm doing is much better. So, I'm very, very grateful. The three lectures
today on emergence are the ones I think that are most close to my heart. Oh, there we go.
All right. The ones that are most close to my heart in understanding complex systems and
the kinds of systems that I've worked on in the past. There, I think emergence is a vast
topic. It's sometimes given a somewhat mysterious aura because it comes up at the end of something
else. But we're going to focus today on emergence itself as a scientific concept. And I think
the key message here is that even though people say, oh, consciousness is an emergence phenomenon,
people say, oh, social systems are emergent and it's left somewhat up in the air, that
there is, in fact, a whole body of theory that Larson comes out of the physicist's toolbox
that allows you to talk about the kinds of things you think you want to talk about when
you're in an emergence system. Can we turn down the volume on the, thank you very much,
JV. There, oh, we don't have a pointer. No, no, it's okay. Thank you. So, the three sort
of central concepts I think are universality. In the second lecture, we'll talk about how
to apply notions of universality and the associated concepts of renormalization and course
graining to inhomogenous information processing systems. So, this is somewhat a known body
of work. This is much less known. It's something that many of us in the institute are interested
in developing. In the final lecture, we'll talk about the sort of non-equilibrium aspects
of emergence. We'll talk about phase transitions and critical phenomena, and we'll talk in
particular about symmetry breaking. So, this is a sort of linear path through a constellation
of concepts. I'm going to take a particular path through it. I'd really be grateful if
having seen these three lectures, if you're not completely confused by the end, if you
have thoughts about how to structure this set of topics, please do email me. So, today
we'll talk about universality, which is a very mysterious and beautiful phenomena, and
we'll talk about the associated concepts of renormalization, which we get from the
physicist, and course graining, which is the concept that underlies renormalization. As
I mentioned, emergence is itself somewhat mysterious at first glance, but I think reasoning
about emergent phenomena correctly is a central feature of modern science. It's not only a
set of beautiful mathematics and not only a set of empirical results that have a generality
that can be quite stunning, but it's also an interesting account of how to do science.
It's in some sense making a certain philosophy of science explicit, and so I'll start there
with what we want when we want explanations in a scientific theory. So, here's one account
of what it means to do science, right? So, you might be a psychologist, and you have a
fine-grained theory of people's motivations, why people do what they do. You may, for example,
have an a priori theory about certain kinds of motivations, and you make some measurements
on people, and you calibrate the parameters of your psychological theory. This in some
sense is all about statistical estimation, which is something we covered in the previous
lecture. So, you have some fine-grained theory with some free parameters that you're measuring
in the laboratory, let us hope. Let us hope you have good enough data to measure the laboratory.
You could try and extract it from some sort of non-intervention experiment, if you like.
Given that fine-grained theory, you then do a simulation. You have a large computer, and
you build an agent-based model. I know Sandra will think about lattice QCD as an example,
where you have some complicated set of interactions that you can't solve exactly.
You may have an exact psychological theory with a set of equations relating different
kinds of desires, but you don't know what happens when you aggregate that set of people.
So, you take somebody who's psychology you know or can calibrate, take someone else
who's psychology you know or can calibrate, how do they interact? And in general, how do
they become proud of those people interact? And so, to get from the fine-grained theory
to a full prediction about the aggregate phenomenon, you do a simulation. So, that's
certainly one thing that you can do. I'm going to tell you a different way to do science.
You have your fine-grained theory. You do something that we'll call coarse-graining.
So, you aggregate the individual variables in such a way that you produce a description
of the full system that's imprecise in some crucial way. So, all of us have a set of psychological
states that are unique, presumably, and beautiful and complicated, and you can spend years in
therapy. But when I come to describe the global psychological state of this room, I'm going
to aggregate over all your neuroses and joys in such a way that I produce a coarse-grained
description of the state of the room, okay? And in some sense, being good at science is
knowing the right coarse-grained description. Of course, I don't have to do that, okay?
If I have a big enough computer and a good enough base psychological theory, I can just
simulate all of you individually in all of your interactions, okay? But we're not going
to do that. We're going to coarse-grain you, okay? And the idea is that that theory that
I come up with by the coarse-graining operation allows you to explain the phenomena that you
see. Not just predict it, okay? Not just predict why, for example, this room might turn ugly,
might riot. Not just predict the circumstances under which that happens, but explain why
it happens. And so that's the goal I think we have here. Let me give you an example from
a wonderful book by Bob Baderman, who's a philosopher of science at the University of
Pittsburgh, who used to be a condensed matter physicist. So there's something called the
Euler critical point where I have a strut here. So I place my hand on top of this page,
but imagine this is holding up a seal and saying, and as I load this seal and increase
it, I reach a point where it buckles. In this case, it buckles to the left. How do we explain
what you just saw? How do we explain that particular phenomenon? Well, one thing you
can do is you can buy a very expensive computer and simulate every single molecule in this
folder, okay? All the air molecules, all the vibrations in my hand, right? All, indeed,
my psychological state described me how hard I'm going to push as I try to make this point,
okay? And not only will you predict it buckles, but you may even, if you're lucky and good
enough with statistics, predict exactly the direction it will buckle in, the violence
of which it buckles, okay? You may find that huge simulation not only tedious to do, but
also somewhat unsatisfying as a scientist. And I think what you may instead try to do
is realize that this process here of loading a symmetric system, loading a system where
it could buckle to the left or to the right, okay, to such a point that all of a sudden
it goes into a new state. Some coarse-grained description of that system might be preferable
because your intuition is that if this buckles, it's somewhat similar to, say, the column
in this room buckling, right? If the ceiling is overloaded, let's hope that won't be the
case over this summer. So not only, for example, might you be able to explain why, well, yes,
paper buckles, but so does, you know, iron. Iron also buckles under loads, under vertical
loads. But you may even be able to exchange your theory to analogous cases. So this is
here the exact same phenomenon where the load, so these are railroad tracks that you can
see they buckle. The load that's caused that bending is not actually a physical force,
but the expansion of the railroad tracks under you. Okay? So that's the goal that you have
in describing what you might call the emergent phenomenon of buckling. This is an account
that in some sense in a far less quantitative way has been a goal, I think, of the social
sciences, although I'm not sure what postmodernism did for this goal. But the idea is that there
are universal social phenomena that are somewhat independent, okay, of the details of the system.
Right? So is this like this? Right? Now, the accounts that I guess I'm treading on dangerous
ground here because I don't know the philosophy of the social sciences, one account might be
to say the reason that these two phenomena are similar, right, tethered square and the
French Revolution, the reason that these phenomena are the same is because the underlying
mechanisms are identical. That's one way to tell the story, right? It says, you know,
people are fundamentally the same. They have certain desires and needs, and when they're
not met, the liberal subject revolves. Okay? That's one account, but it's not necessarily
the only account. And one thing we'll see in these lectures is that in fact the mechanisms
can be very different, okay? Egyptians and, you know, 18th century Frenchmen can actually
be very different kinds of people. They can have very different ways to interact, and yet
you can still talk about the potential for universal phenomena in social and biological
systems. So just to be clear, I'm not saying that these are examples of universal phenomena
that will give you clear and reliable examples of universal phenomena that maybe aren't in
the sexes of these, but I hope you'll bear with me. So here are three crucial concepts
for the lectures. Course-graining. How do we simplify the system by aggregating the parts
together? How do we reduce, right, the 10 to the 26 states of all the atoms in this piece
of paper? How do you reduce that down in such a way that we can look at an iron bar and
look at the reduced description of that system and see that they're the same? Okay, so that's
the course-graining question. The next concept is a concept of effective theory, which is
something that gets talked about quite a bit. If you're a physicist you know what an effective
field theory is, but the idea is after you've course-grained the system, what is the new
description of the system? What is the new set of equations that govern it? Okay? And
finally there's a concept of the basin of attraction, okay? The basin of attraction is what
mechanistic stories course-grained to the same effective theory, okay? So we'll give
explicit examples, but I'd like to begin with perhaps, it's funny to think of this as a
emergent phenomenon or a universal phenomenon, but it actually is. Do people know the
central limit theorem? Are they familiar with it? Right? It's why everything's a
Gaussian, right? It's why you can talk about the variance in the system and feel like you've
described it all, okay? So here is a formal statement. We won't prove the central limit
theorem. There's many ways to prove it. They won't take us into mathematics that will be
useful in this case, but if you're interested I urge you to Google it because proofs are
legion. But the idea is that the distribution of a sum of independent variables as long as
that underlying distribution as a finite variance tends towards the Gaussian. So let's use the
concepts that we had on the previous slide. Effective theory, okay? The effective theory
here is the Gaussian distribution. That's going to be what describes our system when
we've course-grained. What is the course-graining operation? How are we aggregating everything
together? Okay? We're just taking averages or sums, okay? So we're not even considering
any spatial structure that might be found within the system. We're just saying every
observation is independent. I have a big list of them. I'm just going to add the whole list
together and take the meeting. Okay? And finally, what class of objects was the basin of
attraction, okay? Anything were the elements of the system. What you're measuring, okay,
is independent. So let me draw this up on the board here, okay? Here's a theory of the
distribution that you're drawing from. So this is some complicated thing here, right?
The probability of having any particular value is some discontinuous function, okay? If I
aggregate that function enough, I flow usually pretty fast towards the limiting distribution,
the Gaussian distribution, okay? Even if I have a distribution that has support only on
a finite number of locations, such as coin tossing. If I average the answers of a coin
toss, right, let's say heads is zero and tails is one. If I average this enough, I also get
to the Gaussian, right? So all of these theories of how you might draw from a distribution
look the same when you aggregate sufficiently. This looks different from this under course-graining
and they flow to the same place, okay? There is a boundary line. Some theories do not
course-grained to the Gaussian, okay? In particular, theories that do not have finite
variance, which include many power law distributions, okay? So if you course-grained them, they
go off somewhere else. And there's some sort of dividing line between these two kinds of theories,
right? Let me draw a circle around all these theories in terms of their underlying mechanism.
What I'll say, I'll say the underlying mechanism is the same. There's no interaction between
the particles, okay? What that means is that there are other theories that may course-grained
to different distributions, in particular, theories that have mechanisms that produce
persistent correlations, okay? So the central limit theorem is an example of not only the
construction of an effective theory, not only an example of course-graining, but also a
story about the basin of attraction theory, okay? Let me tell you a little story. This
is a popular story that's told every time the market crashes about the central limit
theorem. Say for some reason that we'll become a parent, you want to predict the value of
a portfolio of goods over time. So you know the value, let's say, at time t, and you want
to know how that value evolves from time t to t plus 1 to t plus 2. In general, you
earn interest, you have some rate of return that's multiplicity. So the portfolio at time
t plus 1 is some rate, alpha, let's call it, at time t plus 1 multiplied by your portfolio
at the earlier time. And two time steps away is the product of alpha at those two different
times and so on and so forth, okay? Let us say we have an underlying mechanism for the
behavior of financial markets. Let us say that that alpha is drawn from some distribution,
I don't care which one, it could be some really bizarre one, okay? As long as it has a finite
variance, okay? Then the log of the changes in your portfolio are driven by the sum of
random variables, okay? By the central limit theorem, all of the details of your financial
theory course grain away, okay? And only the variance matters, okay? So that's the argument.
It's a very simplified argument where, here's a space of mechanisms. Here are the variance
infinite mechanisms, here are the finite variance mechanisms, and here are the correlated systems.
As long as your theory of the financial markets falls anywhere in here, okay? If you course
grain enough, you will eventually get to the Gaussian distribution, okay? So that effective
theory of the markets is the basis of Black-Scholes, how people tried to price or determine the
value of an option on a portfolio. You have to estimate the fluctuations and how they might
affect the value, and then in order to estimate the value of a derivative that's predicated
on that portfolio. So as you know, Black-Scholes is a bad reputation, right? So people will tell
you it was responsible for one of the first big bailouts we have, which was the failure
of long-term capital management, but it sort of comes up every time this is an article
this year from the observer, right? The mathematical equation that caused the banks to crash, okay?
So what went wrong with our effective theory of financial markets? Sorry? They assumed
independence. That's a good answer. Here are two other answers, but we'll get to your answers
later because I think that's probably the best answer for this lecture. So okay, Mandelbrot,
do you know because he's on your dorm wall in college, the adventurer that Mandelbrot
said in one of the popularizers of fractal distributions, Mandelbrot would say, oh,
aha, that's fine. You're placing your underlying mechanism, puts you in this part of the space.
It puts you in the landscape of theories where the variance is not finite, where there are
fat tails to the distribution. And so if you coarse-grained here, you don't get to the
Gaussian distribution. You go out into some terrifying part of space that may work for
a while, but eventually it's going to kill you, okay? So I don't know what it means to
have a physical distribution whose variance is infinite. Every time I compute something
in a spreadsheet, I always get a finite answer as long as I don't divide by zero. So when
Mandelbrot, another way to put it, is that every power wall probably has an exponential
cutoff at some point, right? You can't have the market lose more value than exists on
the planet Earth, right? So really, what he said in the variance problem is not finite,
really the convergence to the Gaussian is too slow. That's what he really means because
the space of possible theories of finance all have finite variance, but they might
coarse-grained here too slow, okay? So that's Mandelbrot versus Black Scholes. Here is the
sage of Omaha Warren Buffett in his report in 2008 to the star of the shareholders of
his unbelievably successful company, Berkshire Hathaway. So his claim is not that the underlying
financial system has infinite variance, which I think is reasonable, but rather that our
knowledge of the variance is limited, okay? And so he gives the example here of trying
to price the value of a portfolio over 100 years, which essentially you're being asked
to predict the spread of values of the stock market in the 100 years time. Well, how do
you estimate, how do you estimate that variance, right? And so he compares these
examples, he says, you want to know what the value of your farm is going to be, and every
day some crazy guy comes up to an office you a different amount for, which is his description
of the variance of the stock market on a short time scale. So I'll put this in the
reading list. It's actually a very nice, very practical description of how effective
theories sort of scale or go wrong when you come to talk about social systems, okay?
So we know the Gaussian tends to work, right? We all use it, and in general we find it's a
useful way to describe things that are independent. But let me give you an example of a system
where the variables are not drawn independently, where there's some kind of collective fact.
And what I'll show you is that when you coarse-grain that system, you'll go into a different
distribution from the Gaussian, not because the variance is infinity, not because the
underlying mechanism has some unbelievably fat tail, right? And not necessarily because
the system is non-stationary, which is Warren Buffett's claim. Warren Buffett is saying,
he's not saying that it's necessarily that draws from the stock market are not independent,
just that the variance on different timescales changes, okay? And in fact, people who try
to flexibly know this is called the volatility smile, right? So you actually kind of put
a little hand crank correction. So even people who actually have to make money know this.
So let me give you an example of a system that I find particularly interesting, probably
because it's changing the world, and that is the open source software movement. So I'll
show you data from ten years of software projects on one of the most popular websites called
SourceForge, but you probably use SourceForge to download various kinds of software packages.
And what SourceForge is, is an archive of people to not only put the sort of binary files,
but also the source code as well, right? Which is one of the sort of commitments to the
open source movement, okay? So what I plotted here, and this should be familiar from John
Hart's talk, what I plotted here is the number of software projects that you find, up to
about 20,000, okay, that use different languages and I just ranked them, I ranked them by four.
So you can see the most popular software language in SourceForge over the last ten years was
Java, okay? Sadly Ruby, which I love very much is somewhere down here, okay? This is also
probably a good way to figure out which language to learn by Python. Python is up here, okay?
And you can see, right, you go all the way down Fortran, sadly it's not doing so well these
days, Haskell has elegance, Turburink, sadly a programming language named after the inventor
of programming, has only two projects in the entire system, in the entire database. So
let's look at that distribution and let's say, well, what if this was the following underlying
mechanism, right? I wake up in the morning, I look at all the languages, I say which ones
are the best and I draw one at random from some distribution, okay? By the central limit
there we aggregate 20,000 such decisions, you should get a Gaussian distribution. So
let me plot what the Gaussian distribution looks like overlaid on top of the actual
distribution, okay? So those are the one and two different contours and the dashed line
is the median, okay? What the system would be if it were actually governed by independent
identically distributed draws from the possible languages. What you actually find is a very
different distribution, you find a lot of normal distribution. And in fact, we had a nice
conversation with John Hart about this because this is of course very similar to the discussion
of species abundance rank or SAR. And in fact, if you actually do the model selection on
this, if we talk about the statistics lectures, you actually do the model selection, you find
actually the Fisher-Lock series is actually preferred, I wrote the AIC and BIC, you remember
that. And Y, even though in the sort of mesoscopic range here, the intermediate profit rank is
things like Ruby and Fortran, even though the Fisher-Lock series does poorly over predicts
their abundances, okay? It actually does well enough at the very highest rank that it beats
out the log normal. Y doesn't be not the log normal. Well, I think the answer is a Warren
answer, which is when you look at those really popular languages, you see these long term
trends in their use. So the distribution of seed language programs is not stationary
over time, okay? But if you go back and just look at the lower rank language, so everything
in this case that turns out below rank seven, that's the log normal distribution. So let's
just review where we have, we have the central limit here and just aggregation by adding.
You believe that the mechanism is independent of what the last person did, okay? And then
we have the open source language dynamics, which leads to a different limiting distribution,
okay? And the question is, how do we aggregate? Okay? Another way to put it is, do we have
a theory? Okay? There's some underlying mechanism, okay? That's discussing why a single person
chooses a language, how that language choice is influenced by the choice of people around
him, how that choice is influenced by groups of people all the way up to the largest possible
scales of ten years of open source software. And the answer is we don't actually have a
theory. All we know is that the limit of that theory, when you go, I mean, the visuals and
scale them all the way up to groups, is this log normal distribution. So let me put up
now a space, just think of it before, a space of possible mechanisms, okay? And whatever
I call this the third generation open source movement, for reasons that will become apparent
in a second, whatever mechanism underlies language choice in the open source movement on
sufficiently large scales, course brains for log normal distribution, okay? So here's
another base of interaction that we talked about already, the boring old Gaussians, right?
The finite variance distributions that aggregate independently, okay? Another thing that falls
under the log normal distributions is the set of mechanisms that underlie how people choose
to learn or are forced to learn or are born into speaking a human language, okay? So presumably
the reason that you choose a programming language is very different from the reason you choose
to speak a language you speak, or the multiple languages you speak, okay? So two very different
kinds of mechanisms. They happen at different ages, right? You learn your first programming
language probably when you're in your teens or your 20s. I don't know if I hope so. We
don't learn it younger. But somehow, those two mechanisms, okay, very different, right?
As different potentially as Egyptians in Tech Square and Frenchmen in Paris, right?
Course brain to the same universal class, okay? So I just put up the one of many papers
describing the log normal distribution here. So I haven't done that analysis myself.
We know another class that John Hart talked about, which is the Fisher-Log series distribution
for species area relationships, the species abundance distribution, okay? Somehow, the
underlying mechanisms that lead to the growth of pine trees on the side of a mountain, right,
cockroaches in your basement, right? All of those different kinds of ecological dynamics,
all of them course brain on a sufficiently large scale to the Fisher-Log series, and
particularly to the set of max-end distributions that John talked about, okay? So we're already
getting a sense of basis of attraction, okay, and universality classes, okay? So these
mechanisms here all belong, you might say, to the same class. Let me put another one in.
This is a picture of a mechanism that's postulated to underlie the development of
insurgency in different modern concepts, okay? So you can see here, right, this insurgent
groups of different sizes, they all decide whether to make tax, there's some kind of
broadcast of news so they update their strategies. Some complicated set of mechanisms there,
okay? And what these guys did, and by these guys, I mean, we're working at all in a recent
paper in science, what these guys did is they showed that that mechanism, right, this general
class mechanism here, okay, course brains to some limiting set of distributions, okay?
Now to be clear, they didn't actually go and construct an effective theory, okay? They
actually took that kind of detour through simulation, so they constructed a lower level
method, okay? And instead of course-graining it, as we'll try and do today, instead of
course-graining the mechanism explicitly to see what emergent variables arise, they
actually just simulated it and hammered the statistics in order to get the fits, okay?
But the idea is that this space of mechanisms here, okay, course-graining, in fact, two
potential cases. They showed that, for example, the insurgency in Afghanistan, the
insurgency in Iraq, and the insurgency in Peru all course-grained to this green
distribution here, I don't know if they gave it a name, but it's theirs, okay? But
other kinds of insurgencies, presumably because they lie in a different part of
parameter space, same mechanism, different parameters, they actually course-grained
the normal distribution, okay? So in this case, if you look at the distribution of
attacks in the Spanish of the war period and the mortality of those attacks, you find
the Spanish of the war is actually in the same university class as human language
choice. Let me take another step. Source-forge is now getting a bit old. In fact, if you
actually look, the number of downloads people make in source-forge, it's not doing
as well as on that exponential Moore's Law kind of current annual. And why is that?
Well, the idea is that open sources change, and in fact, it's becoming easier to
modify projects. The idea of having a monolithic, gigantic open source project is
becoming less popular. It's becoming more popular for open source projects that are
smaller in scale, okay? That they're easier to fork, right? It's easier to take somebody
else's open source code and say, you know what, I can do this differently. I have
different goals for me to fork it, right? And different kinds of websites to immerse
in particular GitHub now is one of the canonical places where you go through this
kind of program, okay? If you're in this sort of extreme programming, rapid
iteration world. So from source-forge to GitHub, you know, GitHub is more modern
because look at these cartoons, right? It says be social, right? You know, source-forge
never told you to be social. The question is, are the underlying parameters of the
open source movement such that when you coarse-grain what you might call the fourth
generation of open source, the GitHub open source, will that go to a different
limiting distribution? Instead of going to the log normal, will it perhaps go to
this decentralized insurgency model? So those are the kinds of questions the
universality allows you to ask, right? It allows you to talk about mechanisms, the
limits of mechanisms, and how different distributions can be obtained either because
you change the mechanism or because you change the underlying parameters, okay?
Somehow there's some sociality parameter in source-forge that's too low, okay? So
we'll make that more explicit in a second. So here's what we've gotten so far. We'd
like to talk about aggregate predictions. We'd like to talk about why something
is so complicated with the statistics of modeling, let's say, okay? One thing we
can do is begin with a fine-grained theory, okay, and simulate. So for example, in
order to generate the distributions that fit so well for the Iraq insert and the
Afghanistan insert, that's exactly what they did. They fit small-scale measurements
to the fine-grained theory, simulated an outcome whose wonderful limiting distribution.
What we're going to do instead is figure out a way to determine how this theory
changes as you aggregate on increasing larger and larger scales. And I guess to
follow that point, we've been so far rather sketched about what it means to aggregate.
I've only given you one explicit example of aggregation and that's simple addition,
where you take all your data and you simply take the average. You have no other
way to say, well, I'm going to average these things over here to get one number and
these things over to get another number, right? I've given you no explicit example of
that and I will in a second. What we've been able to do though is we've been able to
see how systems themselves can course grain just by measuring aggregate parameters
and seeing that they have the same limiting distribution. So we don't have an
explanation for why open source goes to the log normal. We can just observe that it
does and that gives us hints perhaps about the underlying mechanisms.
Yes?
How do you know, I mean, you can advocate on the, there should be some level of
use of, if there is a thing that is too much important, is there or?
Um.
You have parameters of interest. You have things that you want to understand.
And if you want to understand, for example, let's go back to the open source software
example.
The question is, what do you want to explain in this system here? Do you want to
explain why Java, C++ and C have the ordering that they do? Right? You might,
you might, right? It might be just a question inside. I think the answer is quite
interesting. The reason is, is that when people invented C++ it's a terrible language
for object learning, for programming. So you can tell those kinds of narratives,
right? But you may also want to explain, why is there this overall structure to the
system? And you're going to get a better and better answer, as you course brain,
because a lot of the underlying details of the mechanism will disappear, the details
that you don't care about, right? The details of why sort of CASL has a position it does
because so many rules that it's a cool language to use, for example. Those kinds of
distributional things will wash out. So, but it's an excellent question. Yes, sir?
So I'm trying to connect your initial example with the strut and how that applies to a
large range of phenomena, realties, etc. And this idea of looking at how a system
falls into the space of limiting distributions. So, you know, I look at this paper and
I go, oh, it works this way, and then I look at some of the walls here. And this is
a sense in which I'm meant to look at a course brain and see an analogy between
these two where the rules seem applicable. But now, if I were to not do that process
instead look at the walls through some level of description of the composition of
the heat properties and then map it onto some limiting space, how is that going to
then get me to this state where I feel like, oh, this is a strut where I can apply the
same rules. How do we go from seeing, oh, these end up in the same limiting distribution
to then seeing some kind of common set of mechanisms that make that aggregation
meaningful? So that's a very good question. Let me give you a preview of the answer,
but it's something, so the strut example I think the most advanced example, and by
the end of lecture three we'll have all the pieces in place. But let me give you a
quick overview. You have a set of mechanisms, all of these mechanisms course brain to
a limiting distribution, which you might call symmetry left. So you have a whole set
of mechanisms, and they all course brain to the state that you say has broken a left
right symmetry. So for example, in that case, from your point of view, it course brain
to the symmetry left case. All of these systems here course brain to a final distribution
that's symmetry breaking right. So these are not particularly interesting limiting
distributions, right? But what happens when you're very close to this critical point?
What happens when the system is reasonably symmetric? It's essentially a straight line,
and a course brain either to the left brain or the right brain to move. That process
there is universal, and that's where you find a set of descriptions that apply just as
well as a piece of paper, where you know what the symmetry right and symmetry left states
are to the railroad tracks, which again have that same symmetry break. So that's how
to connect, but so far it's been a picture without any notion of aggregation. We haven't
told people how to aggregate yet. But that's going to be the story that we get at the end.
So you're saying that by choosing to course brain, you're getting something about the bigger
picture, but we are using that from the grandest theory sector. It's somewhere I think
a moment closer to some transitions, closer to some change. So that's the price we're getting
to some sort of course brain.
So this picture here, which is a symmetry breaking transition, is different from the
central limit theorem. In the central limit theorem, every distribution progresses towards
the Gaussian, right? There's no sort of critical point, and you can look at the critical point
of distribution that if they decrease slightly faster than the finite variance to go to the
Gaussian, and if they decrease too slowly, they go off to some wild space. So does that
answer your question?
The price might be paying is that the variables that you're using to describe the system. So
for example, let's go back to the Gaussian case. So I'm going to draw here a space of
distribution. What I'm going to do is I'm going to tell you the variance and the skew. So this
is the variance, and this is the skew. So this skew, this remind folks, is the three-point
correlation function connected. So here's a distribution that you might begin with. It
has a certain skew and a certain variance. In fact, of course, it has a whole tower of
skews. As you coarse-grain, this flows to this line here, right? And this distribution
here flows to a zero skew. The path it takes might be complicated. Now if you're interested
in the skew, coarse-grain will destroy all that information for you. So if you think
somehow, for example, say you're an auction-pricing person, yes?
It's impossible to choose your method of coarse-grain in such a way to preserve what you're interested in. It's a very
difficult question.
It's impossible to choose your coarse-grain mechanism so that you preserve the variables you're
interested in. It may be the case that the variables you're interested in describing cannot be
described by a university living class. So for example, let's say that somebody invented,
please don't do this, a financial instrument that traded on the value of the three-point
function of the stock market. So somehow you get a point where you make money if the skew
of the stock market that we is plus one and you lose a difference minus one. You're shaking
your head.
Okay.
Okay.
So the central limit there now is no use to. Because it tells you that all it tells you
is that if you aggregate enough, that skew goes to zero. So you need to have a really
particular knowledge of the stock market now, a very particular knowledge of the underlying
mechanism in order to predict the value of the skew. Now if you're lucky, you might have
some great theory, okay, but it actually does that. It says, oh, this is some kind of interesting
phase transition. But in general, it's not necessarily going to work out that way. And
so that, for example, means that you may not be able to build a black-should slight equation,
and affect a theory with that variable.
Okay.
All right.
So how do we pour a strain in a way other than simply adding things together and dividing
by the total number? And this is the story that physicists examined for many, many years
of different angles.
But I'll tell you, so let's tell you the story that Leo Kadenow came up with. Leo Kadenow
produced a story about a course-raining that is very intuitive. He didn't win the Nobel
Prize because Ken Wilson came up with a story that was calculationally more powerful, but
much harder to understand. You're often taught the Ken Wilson story, and much later you learn
the Leo Kadenow story, and you wonder why it passes so hard. So the way I first introduced
it, I was talking at a meeting last year, talking about the mortgage crisis. And in talking
about the mortgage crisis, a friend of mine said, well, you know, I was applying for mortgage,
and I was very lucky because if anyone in my zip code had defaulted on their mortgage,
I would not be able to get one, or I'd get such a serious interest raise that I just
couldn't do it. So I was very lucky. Okay. So the bank there, or the system of banks that
have mortgages, have an implicit course-raining theory. Okay. So what they say is if your
neighborhood has one bad mortgage, the whole neighborhood is bad. Okay. So imagine all these
doctor houses here, right? And the houses can be in one of two states. They can either be
successfully paying their mortgage, or they can be in default for some reason. Okay. If
we have 64 houses, okay, there are 10 to the 19 possible states in the housing market.
And a person is going to grow exponentially. What the bank tries to do, instead of having
a detailed theory about, you know, your moral property, is that they tend to course-grave
your neighborhood. And instead of looking at your individual probability of defaulting
or paying your mortgage, they course-grave, let's say, the zip code level. Okay. And so
instead of, let's say, having 64 possible states, they group you into a little neighborhood,
let's say, of four houses. Okay. And so what that means is that what used to be a pixel
that required four variables to describe now requires only one. Okay. Now the exact way
you summarize these four variables, okay, in terms of this new one, so sigma to sigma
half, the exact way that you do that course-raining operation, okay, that's magic. Right. So
that's one part of this, how do you course-grave what's the exact technique? So for example,
the bank says, I'll take the min, right, so let's say default is minus one. So if anyone's
defaulting here, my average variable is minus one. Okay. The other part of that said, pick
one, you know, the destination, right, just pick one at random, okay, and make the sigma
half, okay, be the value of one of these things here chosen at random. The most natural one
is the average. Let's take the average value of all these four variables, okay, an average
to the sigma half one. So notice that we've grown out information, okay, so if you, for
example, were building an Asian-based model and you had an underlying mechanism that worked
for houses, okay, now we're talking about neighborhoods, you have fewer variables, okay, you still
have quite a lot, you have 16, but there's far fewer, and the price you paid the course
is that now you don't have exact knowledge about the details of the state, okay. So this
is what happens to physics when you use this cultural normalization group flow. So this
course-braining operation here, where you go from 64 states to 16, by aggregating in
terms of spatial proximity, is called re-normalization. Let's say you have, in fact, you know that
let's say you have a maximum theory, not because maximum theory is the only way to do
science, it's because it's an easy one to describe. So you have a set of constraints in
your maximum theory, okay, and let's say, for example, all you constraint is the overall
energy, okay. So this here, or the sum over i's and j's, this is what's going to go into
your max and exponent. It's going to allow you to predict, okay, the distribution of
the housing market. So I'll be really explicit, this is a crucial point. You're trying to
predict, okay, the probability of a certain configuration of the housing market. You know
let's say that it's described by a maximum theory, okay, a maximum distribution you get.
Let's, of course, brain that theory, and what might happen is the following, okay. Now
you're working with these reduced variables, sigma hat, instead of sigma's, okay, and
everything is the same. Everything is the same except for the value of the j parameter,
okay, that's changed by an overall multiplicative factor, okay. If this happens, you're
a happy person, and you're calling your theory to be normalizable, okay. So let's be really
explicit. What does this mean? What does this process mean when your course brain, from
a 64-node system to some theory, and in this case you just cash it out as a max n theory,
okay. Now you can go from talking about houses to talking about neighborhoods, okay. Let's
say you have a psychological theory of mortgage default that produces a set of parameters,
okay. So an individual in the house might look at his neighbor and say, well, my neighbor's
not paying, and his fine's also not paying, okay. If that theory is renormalizable, when
you scale up the households, the same story goes through. So now instead of having people
making decisions, you have sort of neighborhoods looking at each other and making decisions.
You have this new effective part of the household department, okay, that has a set of
interaction logic, okay. You have an emergence of the scriptural system that doesn't reply
upon the things that you previously identified as the underlying fundamental micro-physical
decisions, okay. So if a social theory is renormalizable, the way to put it is that a psychiatrist is
a very good sociologist, okay. Because all the psychiatrists have to do to describe a group
is say, well, you know, groups are like people because the theories are renormalizable. I just
have to change some of the parameters a little bit, right. I just have to say, okay, well,
you know, groups are like people that are probably a little bit more neurotic, right. And they
probably don't, I don't know, have these kinds of emotions as much, but the same mechanisms
go through. And in fact, there may be a person who's like a group, a person who has the unusual
parameters of what the group does, okay. Now let's aggregate his neurons in some way,
okay. So he has a description of 100 neurons. Let's aggregate them into a smaller number
of effective neurons, okay. Now let's try and make predictions about the behavior of those
effective neurons. If neuroscience is renormalizable, okay, in order to make that prediction, I can
have the same functional form of the theory just with different parameters. In particular,
I don't have to add higher order correlations into the theory by hand, right. And so that
is what happens here. What could happen, and in general probably will happen, is that in
order to make predictions about the coarse-grained theory, not only do you have to change the
parameters of the theory, but you also have to add new interactions.
So within the context of prediction, if it's renormalizable, I can keep the model the same
and all I have to do is to change the parameters differently to get the same level of prediction
for example. Exactly, yeah, okay. But if the theory is not renormalizable, I have new
emerged interactions, okay. So in this case here, I've gone from what you might call
fine-grained theory to more coarse-grained theory, okay. And the question is, what happens
when you make that transition? If you're a physicist, this is backwards from how it's
usually done. You usually go from the coarse-grained theory and zoom in here, but the story is the same.
So here's, yes. This is a little bit too much for you to tell, but you're not getting
on the expectation of the prediction, not other moments. Is there any reason to do that?
So, let's consider the max-ent theories, because it's a nice, it's a convenient example, right.
Given this theory, I can predict all the moments, right, because I have to calculate the distribution
over all possible states in the system. So I can compute on the variance, but I can
compute the kurtosis and, you know, anything I'd like, I can do the x-t. So this is a
complete description of the system, right. And, you know, in this case, the theory could be
similar to just pairwise interaction theories. And the question is, in order to get a complete
description of the reducing system, right, can I work with the same parameters? Yes.
So when you look at this, and I see sort of a normalizable, it just, it comes up to me a
little bit like sort of stereotypically, but you're essentially stereotyping those groups
in certain ways, based on the average or based on whatever. And I guess when I think of social
processes, I always think that human agency really matters. And I have a really hard time
thinking that, like, human agency is sort of an emergency process. I don't know, it's just,
I just wanted to, and I guess when you're thinking about, for instance, like the
Holocaust, where the Holocaust happened, that they're, you know, had not so personally
power. And you're thinking about, you know, the human agency's down to this part. So how do you, you
know, start to think about a model? This is a really great question. And we're on opposite
sides. Let me tell you why the side I'm on seems to make sense to me, right. We're
totally immersed in an emergency phenomenon. I was trying to think of retuting our
students to do. I remember when I was in college, I had to read a book of poems. And it suddenly
struck me that this was totally absurd, right. Why are they so obsessed by the same
words as animals? Right. Where do all these rules come from? Why do we have rules about what makes a good
poem? Why am I stressed about getting a grade in class? Right. Where do those burdens
phenomenon come from? Now, I would say they come from the coarse grain of human agency.
I'm, in that sense, a reductionist. Right. All of the rules of poetry, all of my anxieties
about doing well in the college class. Right. They all come from the coarse grain of
lots of people making decisions. Now, that process may be totally mysterious. Right.
But I still believe that process goes through. There is this sort of one way out of that
description that maybe you're getting out of the Holocaust story, the great man in
theory. Right. Is that what if there's some kind of perturbation? Okay. One of the values
of one of these statements here is sufficiently large and kind of blows everything away. And
in fact, the theory can kind of account for that if it's a particular person's note. Right.
So the coarse grain theory of an emergency social phenomenon will not allow you to pinpoint
who's hit her. Right. Unfortunately. Right. In the same way, in this case, you can't pinpoint
either which way is going to fold or the exact hypothetical structure is going to cause the
break in one way or another. So those are sort of two answers. One is that I think that
any good scientific theory of social systems has to, in some sense, describe how we scale
up. Right. The other thing is the influence of sort of these rare events. And I mean, it's
a crucial thing. Right. As people talk about, well, maybe the sophomore is fundamentally
unpredictable because of these rare events. Now, the story of universality allows you to
explain a little bit about why that might be the case if the underlying mechanism has a
certain set of properties. And yes, it is unpredictable. Right. Or if it's not eroded, we just
can't forget it because it's shifting a certain way. So I don't know if that's a consolation
answer to your question. It's a very deep question. Send me your question.
Yeah, I'm just, I'm going to, you drew a finite system, very finite.
I'm going to give you an answer to that. Let's say you begin, let's say your course
rating in the central limit theorem, okay, so you have some distribution and it has some
value in the skew. So if you measure a single value, that distribution has a strong skew.
As you average more and more and more, you approach this universal point, the Gaussian.
Right. The question is, is maybe I mean 10 to the 32 measurements to get to a point where
this skew is zero or very close to zero. And the approach to this might be slow. Okay.
And in the same way, it might be the case that even though the mechanism, let's say,
of the housing market is a member of a certain universality class, I just don't have a set
of neighborhoods that's large enough to reach that point. And this all gets to be a problem
because you don't have as many orders of magnitude to scale over. You know these things, but it's
a fun question, right? One question, sir.
Yeah, this is a very important point and one of the things that you can spend a lot of time
thinking about is what happens. So here you're doing, right? If you're trying to get a universal
description of theory, you only have two averages and nothing's going to work. You need a specific
description about the point. But as you get closer to this point, you get something called
finite by a scale. Okay. And so you can ask how quickly and in what regime am I approaching
that universal point of the system? And in some situations, not only is the final and safe
of the course rating process universal, but so is the way in which you approach that final point.
So you can also get some traction in there, right? So for example, you might imagine looking
not only at the distribution of programming projects, this is a good idea, not only the
distribution of soccer projects over the course of ten years, but you might look at smaller
scale studies. So you might look, for example, at the distribution of languages used in, let's
say, scientific programming. Or you might try to course rank by time and look at the
distribution of soccer libraries over a month and ask how am I approaching, let us say,
okay, the log normal is the correct description, how am I approaching, okay, that log normal
distribution. David. So if you're a reductionist, is that what you're saying, that all theories
to you are being normalized? No. No. There's a paper we'll put on the reading that's by
Phil Anderson, right? There's two kinds of reductionism, right? One is that sort of the trivial
reductionism is not going to portray very well. But the other one, because of my soccer
version, simply says that yes, like every version of the phenomena we see is the outcome
of processes on a finer-grained scale. And I can't imagine another kind of emergency
that's lost for a try, right? So some people might say, well, clearly there's no consciousness
in the firing of neurons, and yet we know my brain has aggregations of neurons, so there
must be some new magical thing that sort of flies in at some level of course, raining
and drops off some consciousness. I don't find that. But that would be an introduction
to this. If renormalization fails, all it means is that the details matter all the way up,
right? That the particular value of the skew matters, okay? That would be the case if
renormalization fails. Yes, sir?
So what happens, and it also relates to the reductionism. So you course grain, and then
you study the dynamics of the course of the variable, whatever you call the course grain.
But then what happens in, but for sure you can also assist them in all cycles,
you can also assist them in that. These things have their own dynamics, and this feedback
on the single way thing happens, right? So if you look at the distribution that comes out,
that's a course grain property, that's a mesoscale if you want. But how does this feedback
on the choice...
Think about the black... Here's an example. Think about the Black-Scholes equation, right?
So let's just say Black-Scholes works. People make money on them. Not maybe they make money
because they pay people off, but say they make money on them for the different reasons.
Hey, the Black-Scholes equation, right? The bargain that it makes says their interest
rates or growth rates are drawn from some distribution, and their reasonably independent
course grain doesn't make distribution, right? Okay. Now, in general, the idea that sort of
underlying career of Black-Scholes is that you're the only guy who's using Black-Scholes,
right? Okay? You're the only guy who's using it, and so you have some sort of really
sort of enormous system of experience that you have no effect over, and you're making
decisions based on it. Now, if everyone's using Black-Scholes, okay, it's a different
work, right? Now, potentially, all of the assumptions, for example, the independence
of draws from the interest rates, that might collapse, okay? So if everyone uses Black-Scholes,
it might fail, right? But that's simply because the mechanism now is more complicated. So
I think you can account for those kinds of stories, right? The stories where, yes, I'm
observing the distribution and trying to catch in on features of the distribution
with those everyone else, right? What that does, it makes the system more complicated.
It changes the mechanism, but it doesn't necessarily make it an unmanable scientific
description, although that may not be what you were suggesting. Okay, of course, yeah.
But that's what happens. Think about the language case. Why am I going to program in
Python, right? Well, presumably, I'm not just making a decision independently of
everyone else, right? I'm observing other features of the system, so my choice to
learn Python versus Ruby is governed by my observation of the average state of the
system. And in fact, if you look at how they even tell you what my answer is,
there's already this problem, right?
There is a sense that individuals of the time range know already what their higher
level or what their something about the force frame level.
Absolutely, yeah.
I'm not saying it doesn't happen. It doesn't ever happen.
It's not necessary. It's not necessary.
Or sometimes it doesn't happen.
Let me give you one further answer then, and then we can take this off.
I think it's a deep, considerate question that we can deal with.
But let's say I have a relationship between two course frame variables.
Of course we do, right? This theory here is talking about the relationship
not between people, but between neighborhoods, right?
So if you were to tell a story, you'd like neighborhoods making decisions, right?
So that's what the normalization allows you to do in a sense.
It allows you to even stop talking about the individual world together when you
talk about the large scale dynamics. And if you find a good course frame
description, it might have a very weird way of aggregating, and we'll see this in
the next lecture, aggregating over the fine-grained properties, right?
So in fact, the boundaries of the course frame object might cut individual
points.
Anyway, well, that's a deeper question, but we'll get to it.
May I have another question?
This whole issue isn't true that you would agree with me if I say the following.
Summarizing the realization of a group idea is successful most
than there is sometimes given variance about that.
If you think of a system which has a strong structural hierarchy, for example,
force, then it is in between, say, nuclei and then atoms.
These are structurally very different, which would mean that it's very hard to
force grain from the force that could be atoms, because it is a real structural
in between. And that, I think, was also remarked by Anderson a little bit.
You reach a structural, and then there is an intermediate structural field
where you have to force grain again or something. What is your feeling about it?
Well, let me just tell you an intelligent analysis story, right?
So let's say, before spraying, the theory gets more complicated.
This is exactly what happens in the important case.
So at the beginning of this talk, I said, oh, you can do a simulation
either agent-based models or a lattice GCD.
A lattice GCD is how people try to model force.
So let's say that you force grain, you've got all of these new interactions.
Neighborhoods were not only looking at each other, but they were behaving in group-like ways
that the underlying people didn't.
So that's kind of crazy, but another way to put it is this.
It's more complicated to do it in neighborhoods, but when you scale back down,
it gets pretty simple. So that's what happens in the lattice GCD.
It's not a static creator.
So in some sense, in normalization, if you start telling the story,
one direction is better than another, often.
Now, it can be the case that one parameter gets large,
and they get small in different ones, in different directions.
Let me just, since we're running out of time for this lecture,
this is a formal diagram.
Probably we can skip over, except let me just draw a little bit of the story here.
So in that top plane there, you have a theory that has two parameters.
These arrows here, you flow as you force grain in the system.
You flow to different values of those parameters.
So this is a large-scale picture of the kind of normalizations for Italian.
And in general, these arrows here, how you, for a strain in the physical world,
the values that we live in space-time and our theories about space-time
be coarse-grained by aggregating on increasingly larger or smaller spatial scales.
Let me skip over this. We can talk about this a little bit later.
Let me also skip the question of universality at a critical point.
Okay?
Well, let me talk about it. It's so wonderful.
So, here's what you have.
You have a particular underlying mechanism.
So first of all, you introduce the IC model, am I correct?
Did you introduce simulations of the IC model?
Yes, I think they're great.
So we'll do some more simulations of them on Tuesday.
So the IC model is a set of underlying mechanisms.
Right? As you ever say, let's say,
cloud-grained atoms from the Trontlos Hastings algorithm,
you can talk about space choosing whether or not to flip based on the behavior of their neighbors.
Okay?
And when you aggregate that system on increasingly larger and larger scales,
you can talk about the average magnetization of the system.
Okay?
You have an underlying parameter here, T.
Okay?
So think of T as, like, say the variance, right?
If some underlying feature of the system, you can turn that knob.
It's like the strength of the coupling between the arrows.
Okay?
And if you coarse-grained on increasing these large scales,
when you go from here, like in this case,
this is the overall sort of field,
but if you coarse-grained since it's decreasingly,
you get the following thing.
Below a certain critical value, okay,
of the coupling constant, T, okay,
when you coarse-grained the average magnetization of C-grained.
Okay?
But at a certain critical point, okay,
the system has to pick one state,
either the up state or the down state.
So this is exactly analogous here.
Okay?
So in this case, the temperature parameters,
how far are going to push you down?
At some point, it has to pick, okay,
an up phase or a down phase.
And this phase transition, okay,
the behavior of this magnetization parameter
is exactly the same as what happens in a totally different system.
Okay?
In this case, the mixture of liquid and gas at a critical point.
Okay?
The order of parameter is different.
What you're measuring is not the overall magnetization
but rather the density of the system.
And what happens is at a certain critical point,
the system either goes to an all gas
or an all liquid phase.
Okay?
And that happens at a certain critical value.
So we can describe the features of these two systems
that have completely different underlying dynamics.
Pretty different underlying methods.
In some sense, the icing model doesn't even consist.
Right?
It's just some simulations that I can use.
But it has exactly the same dynamics
as the transition between vapor and liquid.
I see a skeptical frown in the back.
Thank you.
Let me give you a preview of re-organization of biologics.
Please.
Okay.
Sorry.
Okay, go ahead.
Does the gravity go through further away
at a critical point also?
So let's consider the liquid gas phase.
Right?
If you move too far away, okay, in temperature,
then eventually you'll discover, for example,
the distance of solids.
Right?
And there's no such thing in the case of the icing model.
So if you move too far away in parameter space,
when you force grain and it's sucked off,
there's a new area.
Okay.
So.
Let me give you a preview.
Re-normalization by aggregation of spatial scale
doesn't really work that well in the system.
Right?
One thing that does tend to work
is re-normalization, okay, on the scale of time.
So let's say we have a sequence of observations,
alpha 1, 2, 3, 4, 5, 6.
Okay?
So these are measured at different times,
the second 1 and second 2.
Okay?
We can produce a description of the system average,
okay, on increasingly larger and larger time scales.
Okay?
And that will be a way in which you might choose
the force grain and biologics system.
You, of course, have to specify exactly the way
you do that averaging through destination rule.
Okay?
I put up a reference to Dan Roberts' work
because there's many different ways to do it.
The obvious one is that there are real numbers
that average them.
Okay?
And that's exactly what we did,
what we do when we look at time scales and biologics.
What we're asking here is,
what are the signatures as the average
on increasingly larger and larger scales?
Okay?
And what you see is, as it aggregates,
instead of looking at the finest range of time,
so as you aggregate on increasingly larger and larger scales,
you see not the phenomenon that emerged.
Okay?
And that's not the whole story.
The problem is that if you come to construct
these biologics explanations,
you rely not only on aggregating time,
which is a way, for example,
to look at the spectrum of fluctuations,
but you also aggregate in terms of what you might call
functional equivalence classes.
So in that paper,
when we come to make descriptions of the system,
when we come to make biological arguments,
we look not only on how the system behaves
when you coarse-grained it in time,
but also how the system behaves when you look at groups
of individuals or substance within the system
as a single unit.
So here, for example, we have a nice story,
but what happens if all you do is look
at the time scales of fluctuations,
how the system coarse-grains,
when you restrict your particular subclass of animals,
to be identified not by the spatial proximity,
and it will be the case in the physical system,
but by the functional rules they play.
Okay?
This is the large problem,
and it's the problem we'll talk about in the second lecture,
the problem of renormalization in homogenous systems.
So we do here, you take the nearest neighbors in the physics,
and why that works is sort of a profound fact
that physicists don't really think about.
What we do know is that biological systems look like this.
They're like networks that don't have clear ways
to find nearest neighbor averaging rules that make sense.
So let me just scroll through sort of a cartoon picture
and what happens.
So this is the classic text of quantum field theory,
and in quantum field theory,
you spend a lot of time doing these coarse-grained arguments,
because it's a way, in fact, to produce predictions
from a system where you don't know the underlying micro-visits.
Okay?
We have mechanisms in the social sciences as well.
We talk about interactions between individuals.
Okay?
And in some sense, they're equivalent to interactions
between particles.
The problem is, is that when we come to coarse-grained physical system,
we know what to do.
Okay?
When we come to coarse-grained biological system,
we know that this kind of thing doesn't work.
You want to understand why animals behave in the way they do
in some tent, you don't get an increasingly powerful telescope
to zoom in on exactly the hairs on the background.
Okay?
But a person is actually going to do moon physics.
The reason you build, you know, this is quite a discern,
is because you want to look at increasingly smaller and smaller scales
and confirm the effect of theories we know are true of larger scales
and potentially find new mechanisms.
Okay?
And the way we do that is we look at increasingly small scales.
Okay?
But that's not our explanation for the work of biology.
In general, actually, we often have access to the most fine-grained
descriptions of the system if they ever want.
Right?
So, for example, in the case of animals,
we know exactly what all the animals were doing.
Okay?
So, this does never have that information.
They have no way to know the exact position of a particle,
or the distribution or the position of a particle.
So, we have the fine-grained information.
The question is, how do we scale up?
How do we go from knowing the facts,
but the example is to knowing facts about the social system
of which there are particles.
Yes?
Do you think that we do have a smaller scale
that is neural information?
We do, yes.
Yes.
So, neural information is interesting because we have
complete or partial information.
We have exact knowledge of slight trends.
We just don't have exact knowledge of all the slight trends.
Okay.
But, in theory, some of you just want to scale up
with your direction.
Yes.
I see what you're saying.
Yes.
That is true.
In general, when we start with the animals,
we want to scale up to the larger system.
When we start with the neurons,
we want to scale up to the consciousness.
We tend not to want to go the other way.
Although, some kinds of people do go the other way.
So, we'll say, what are the neural correlates of attentionless?
Yeah.
Yeah.
Neural activation coordinates the immediate equation
of the individual's social interactions.
Yes.
And that means that we get a scale and we have a story, right?
You have defined-grained information on all the neural
and you're trying to scale it up to some large correlation
between animals and that.
Or, in terms of social behavior,
you're scaling down by looking at,
not just the interaction of the variables,
but also the interaction of your activation
between grains and between grains.
Yes.
One of the things that happens is,
as you coerce grain,
information about the defined-grained scales
tends to disappear.
So, in talking to you,
I generally have no idea about your neural activation.
In fact, I have a very cool idea about the underlying mechanism
of how you think it all,
which is why I have the problem that you might be a computerist, right?
So, oftentimes,
you're ignorant of the defined-grained properties.
What physicists try to do is,
even though the defined-grained properties tend to disappear,
you coerce grain.
You have a fairly accurate measurement to do the tech code.
So, that's because we have a lot there
that might be looking at your reaction times
and trying to figure out something about the internal dynamics
and testing and so you get your neural system, right?
And I need to do the accurate information for that.
And that can not happen in social sciences,
which is, I think, why are we trying to scale up
more than we do in physics,
where we're trying to zoom in by getting more and more
on the size of machines.
So, yes, John?
So, I just found myself thinking about this framework
in the context of asking questions about the world.
You know, on the one hand,
if renormalization works,
then it almost seems to be saying,
well, given this filter, this model,
I get pretty much the same type of answer
to this question across whatever scale, right?
Whereas it seems like in sitting in social phenomena,
we might say, well, I have this question
and presumably on certain scales
for a given model,
it's not going to give a meaningful answer at all.
The output is not going to be predictive.
It's not going to provide, you know,
conversions to certain statistics.
And presumably, and just trying to look at it
with an idea of emergence,
that maybe at some scale,
there's a critical scale beyond which
my model then begins to supply predictions
for descriptive statistics.
That's fair, yes.
Yeah.
I guess what's different in physics is
we have the intuitive experience
of the coarse-grained reality
that we can kind of test our theories against.
And then we have the kind of perplexing
quantum mechanic of the world.
Yes.
And it seems like we have these nice bounds
that we kind of know.
But a lot of them are going to be into
complex behavior or social systems.
There's kind of this vague point where,
you know, when do our questions start
becoming answerable?
Yeah.
No, I think this is a very fair point.
And I think my sense is that you're getting
into this question of,
you have your specific micro-theory.
At what point does it coarse-grained sufficiently
let us say that the details disappear
and you make an overall prediction?
Yeah.
Which is how fast do you get from here to there?
And are the phenomena you want to explain
all the way down here?
And it's a good question, right?
The speed at which you converge to that limiting
distribution can be very important.
There are some systems where it just happens
too slowly.
So our work in a system where the scaling
scales the logarithm of the logarithm
of the number of particles.
And that goes so slowly that, yes,
the limit of infinite particles it gets there.
But for any finite size system,
you're so far away from that sort of universal point
you can't make predictions.
And that can completely happen.
In which case, I do say, well,
we can't really talk about a universal phenomenon
out of this scheme described here.
All we can do is write simulations, right,
and look at their outputs and feel that we understand
it because we put it in the mechanism
and then imagine how it's inside the machine.
And some people might say, well, that's
a committee satisfying our own defense.
Other people might say, that's not how I feel
like I want to build explanations.
Isn't that kind of to say that maybe
your question is posed in such a way
that you can get a satisfactory answer,
but not a satisfactory time scale.
You haven't matched your question
to your time scale of interest.
It could be.
I'm trying to think of a really specific example
when I'm writing down the curve.
I mean, think about, for example,
you're trying to explain a social phenomenon
in fact-like institutions.
Well, if you're at the wrong scale
or too close in, then the reason this business
failed was this guy who was kind of a bad guy,
you know, like very detailed narrative explanations.
Okay?
So that's the wrong scale to expect a scientific
discretion to emerge, right?
All you can really do, maybe have a psychological
science that you can simulate the failure
of that business, but you can't come up
with an effective theory because all the details
don't matter.
If you can get to a scale where it does work,
I mean, let me give you one reason to think
that it might work for the same neuroscience.
We have an effective theory
for neuroscience already.
It's folk psychology.
I can predict what we're going to do
or try to predict what we're going to do
based on coarse-grained descriptions
like you're happy, you're tired, you're curious
and given that, I have a theory.
So there are these kind of convergent
coarse-grained descriptions
that work, or at least work reasonably well.
You can make the argument of work at all.
You can say folk psychology is actually not
pretty good at all.
We just pretend it if we predict it.
So that's one way.
But if you say that we actually can predict things
about what we're going to do,
then maybe there is a reason to believe
that we can scale to a normal level up.
Yes?
Well, I think certainly a double standard.
I think with social processes,
for instance, we go back to your original example
of revolutions or protests.
So if you look at my two years where in Egypt,
I think political scientists have been predicting
that there will be some sort of major protest
or revolutions, some sort of major...
Egypt will last 30 years.
And so I think really the question
for a lot of political scientists will be,
and I wonder if these first-graded models
can really give me that answer,
because I feel like it's not necessarily
the question of, you know,
will it happen, but exactly when will it happen?
And...
So let me give you the angels' position.
You're the devil and everything.
No theory is going to tell you the date of a revolution.
And it's not going to tell you the name
of the person who started it,
who showed up there on that day.
It's possible that a good theory
can tell you a distribution, right?
So, again, when I'm pushing on this,
and it collapses,
no one can tell you exactly when it's going to collapse.
But they can give you a distribution.
It's not going to take a year for this fold.
It's not going to happen in a microsecond.
So if you understand some of the fluctuations,
well, this is a bit of a non-inclusive question.
I'm forgetting the third question.
But you may not be able to predict the date,
but you might be able to predict and see the times together.
So...
I'm not sure if that's helpful.
It's also, of course, no one has to do theory.
So I'm sorry, I'm reading for the sort of
apirate plausibility of the theory of this thing.
So it's up to somebody to construct it.
And unfortunately, that may not be possible.
It may not be possible for a lot of people,
including, for example, the fact that the system
is not going to be normal last time.
So the only thing you would do is...
Yes, Graham?
Could you go back to the slide?
I don't know if you're sitting by this.
Can we repeat the question?
Yeah, sure.
So Graham asks,
why is spatial structure,
of course, raining based on spatial proximity
doesn't work.
Why is it of course raining
based on spatial proximity?
So just to be clear,
it's pretty clear to me that
if we go into the system with the animal,
the animal system, which you've heard about
on and off over the course of the summer,
when you go back to the system,
if we try to build a theory about
which animals are nearby each other,
we're not going to get a very big theory.
Because the effective theory,
the coarse-grained variables,
are aggregations of functional classes,
not spatial classes.
And others are not determined by
my spatial proximity to them.
In the time case,
in the time case,
it tends to work out
that events that are close
to each other,
or nearby each other,
often are associated with each other.
Now that's not a scientific argument.
What it just basically means is,
in general, it's reasonable to expect that it will work.
It's worth trying.
Now, it may be the case that
events that are close together
in time are not associated,
so trade them aside.
If we trace the exact moments as they go in,
that proximity is not important.
But in the system here, where everyone can see
that they're sort of synchronized
to the same kind of rhythm,
it's plausible to believe that
that temporal origination works.
It's only possible that you go in
and see if it actually can work out.
So, I mean, just to go to your own work,
I'm not going to give you the case,
but let's just say for you to say
that where the publishing house is located,
if you course grade on that,
it's probably not going to be that informative.
But if you course grade on the year,
it's the novel that's published,
maybe that's actually a better accurate class,
as opposed to the treated dresses
of the publishing house.
You wanted to follow up with this?
Yes, quickly.
So, I think,
so I start to realize
that these things are important
in the end of the schoolwork.
So, if you take your experiment,
the space over a spatial structure,
you cannot see that
because you're taking two people's space to come.
But two people's space
you can't compare to
how information is spread.
Yes, absolutely.
So, spatial structure is not
regular.
It depends on their use.
I guess that's very fair.
But notice there, what's happening is
that the reason that the spatial structure
becomes a good course grade variable
is because time already is.
Yes, well...
So, in a sense,
what you get,
you actually do is a sort of speed of life,
which is absolutely a speed of information,
actually.
This came up in somewhere
in Afghanistan, right?
What's the speed of information flow?
If it's mule, it's sort of five kilometers
of power or whatever,
travel is that.
But if it's a cell phone tower,
you can communicate instantaneously.
And so that's going to change
your spatial course grade levels.
And there, the idea is
that things that are close in time
are also close in space.
Well, that's fine.
For example, blocks.
Course grade and spatial, like a bird flock.
Presumably, you can get a very good description
where you course grade and spatial.
Exactly.
So, in the case where you're aggregating
on spatial scales,
the hierarchy is just sort of your resolution.
You kind of blur everything out.
And so, there's a whole bunch of structure here.
I blur it into one big picture.
And that hierarchy is approximately
continuous.
I don't feel like I'll go from one level to the next.
Although, in the case, for example,
the aggregated neighborhoods, in that case,
there is a discrete sort of house
and neighborhood.
Oftentimes, that knob in your turn
is continuous.
When you come to course grade
on functional plans,
then there really is a discrete hierarchy.
I look at individuals, and then I might
look, for example, at the course grade group
of all the young females,
or all the young males,
and try to build descriptions about the interaction
between those two groups.
And there, it really does feel
like I have one fine-grained level
and then just re-jump to the next one.
Sorry, but I meant that
you're looking at time
and then you're looking at functional
scores.
But you're saying that
the most important scale
that you're...
not scale, scale, scale...
I don't know which is
course grade in the time.
And then you have to look at
course grade on functional.
This is a very good point.
In fact, this doesn't really come up in physics
because we have one thing in course grade
that is spatial plus grade.
We course grade spatially.
So it's like, you have one direction
you can float in.
But when you have biological systems,
there's many directions that you can course grade on.
So you can course grade in time,
but not course grade on the names of the individuals.
Or, you don't course grade on time,
you can course grade in the functional
categories that way.
Or you can do both at the same time.
So there's different directions.
And that's actually a very nice point.
It's a very different system than you.
It's a very different situation than you have in physics.
Where there's only one thing you do
and that's an average on increasing
large number of equation scales.
As a continuation to that event,
is it possible that
the choice of this function
or the way in which you course grade
cannot make the same system
renormalizable in some sense
and in other ways
not renormalizable?
Yes, yes, yes.
There are better
course grades than course grading, right?
So here's a really bad way of course grading.
For example, it might be taking the extreme
value, right,
of your sequence to get the
sigma hat, right?
And you might worry there that course grading operation
is not even super sensitive to fluctuations
on the fine-grained scale, so it's not going to go away.
So there are bad ways to course grade.
In physics in general,
it doesn't really matter as long as
you're doing it.
In biology, I think the question in social sciences,
the question becomes much harder,
is when a course grade on a bright scale
so that you have theories that are simple
and what simple theories mean
is in this case, we normalize it
so they don't have enormous numbers
of adjustable parameters that you have to do.
So, just to go ahead,
we should finish with one last point, right?
So, we did a statistics lecture,
and I grouped people by sex and I grouped people by people, right?
So my course grade in the system
is either a field and field of field, right?
I didn't find anything, okay?
That does mean there's something
that I don't need to interact with the students.
It just means I didn't take a particularly
interesting way of course grading system.
Anyway, thank you very much.
I'm going off and I really appreciate the question.
