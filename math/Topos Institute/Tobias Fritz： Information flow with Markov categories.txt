Today, we have Tobias Fritz. Some of you have already heard about Markov category in the
previous events on categorical probability. So they seem to be a very nice categorical
framework for talking about probability in terms of category theory, in terms of arrows.
So Tobias, welcome and please the stage is yours.
Yeah, thank you. So yeah, it's great to be back at the MIT Category Seminary, even if
it's online now, or maybe especially that it's online now. So this is part of the ongoing
developments and categorical probability, which really is just a subject that seems
to be gaining momentum at the moment. And a lot of things are somehow turning out to
be possible, which maybe we're not expected previously. So my original title, as you may
have seen was probability theory with Markov categories. But while preparing the talk, I
realized that that's actually there's a different spin that one can put in it and maybe in some
sense a better spin. And so one of the things that I'm going to argue is that Markov categories
are actually a theory of information flow. And that also probability theory itself, when
you consider it somehow maximally generalized, is a theory of information flow. But that is a
more intuitive word for what this is about. Okay, but before getting to that, let me just kind
of do the necessary thing and kind of flash the list of references, at least the main ones,
there's certainly other ones. So Markov categories may not have been reinvented with some minor
variations a couple of times independently. And so the maybe most important first paper is the one
by Joe and Jacobs, disintegration, evasion, inversion via string diagrams. But turns out that
there was also in the 90s, sequence of papers by a Russian author, Perot Belyubtsov, who's done
more or less the same things. And then there's my long paper from last year, where Markov
categories were developed quite a bit further, and more, more in terms of theory, and proving
theorems about them and using them. And then there's a paper on on zero one loss. And there's
other stuff that is happening at the moment, also in particular Evan's talk next week, the Markov
categories, I suspect will also feature. Yeah, there's a couple of other things like Brendan
Fong's master's thesis, but but I'll get to that. All right, so before introducing Markov
categories, since this is applied category theory community, many of us really like reasoning
about processes and terms of the string diagrams, which is using string diagrams in general. And
so then we have these wires and boxes, wires are presenting things that kind of systems in a
system is an input can be an input to a process. And so a process takes the system as input
produces another system as output. And then we have string diagrams like this, if we compose
these processes into networks. Yeah, so this is the standard kind of thing that I'm sure most of you
will have seen some kind of recipe cooking. So I'm not as sophisticated as lemon meringue a pie, so
it's just an apple pie recipe. Yeah, so this is what string diagrams about and what they're useful
for. And the point that I want to make here is that these are in some sense, typically, or often real
processes, take that that can take place in a physical world. But now suppose that we want to
talk about things more like information and information flow and information processing. So
for example, let's say you want to device some kind of model of a medical trial. So then we have
medical conditions. And the medical condition as a patient of a patient would be our initial system.
And then we have a treatment outcome. And then there's a bunch of things that that treatment
outcome depends on. In particular, in medical trial, the outcome will depend on whether the
patient will actually comply with the prescriptions of taking the drugs or whatever. And the compliance,
just as the treatment outcome itself will depend on the medical condition in principle,
someone who's more sick than someone who's pretty healthy, it's more likely to actually comply,
I suppose. So here that means the medical condition has an influence both on the compliance and on
the treatment. And then the treatment, the influence on the treatment is both indirectly
through the other compliance, and obviously also directly. So it seems that to talk about this
sort of information flow, that the information about a medical condition flows into, or has this
influence on the compliance and on the treatment, we can just use plain these vanilla type string
diagrams because these don't have enough structure to express this dual role of the medical
condition here. So we need some kind of additional thing, which denotes the idea that we want to
copy this material, the information contained, the information about the medical condition.
And this is so this is where the black dot denotes. Oh, and I forgot to mention that my
string diagrams go from from bottom to top, kind of because of my, I guess mainly because of my
physics background and interest. Our time goes upwards. So copy means it well takes one thing
as input and produces two things as outputs, and those are to be thought of as copy.
So this means that a theory of information flow will need some additional pieces of structure
in just rather than just a monoidal category, symmetric monoidal category, namely something
that denotes copying information, sort of like a kind of co multiplication. And also we also
would like to be able to delete information. Then we have this just black dot at the bottom,
sort of a co unit. And so this leads to the following definition of Markov category.
Oh, sorry.
I did not.
For some reason, my computer is very slow at the moment.
Okay. Now it should be back, right? All right. Yes. Yeah, a Markov category
is hence a symmetric monoidal category equipped with these structures. And so copying and
deleting operations in every object, which, well, are those types and satisfying a bunch
of conditions that we would naturally expect copying and deleting to satisfy and namely that
these make every object into a commutative co monoid, the canonical way. So the co multiplication
or the copying is co associative, commutative, and co unit. So if I copy the information and
delete one of the two copies again, that's just the same as keeping information and doing nothing
to it. And while these pieces of structure have to interact with while of the monoidal structure
in a sense that I now haven't written down, it's the same as in the general theory of supplies
that Brandon and David have developed. And there's additional condition that if you process
information and discard it, then that's the same as just discarding it from the start.
And there are some indications that maybe we don't actually want this, or at least someone can do
still a lot of things without and sometimes even more things without. But still, it's a very useful
and natural condition to have around. So for now, for the purposes of this talk, I'm going to be
assuming it. All right, so the basic example is Finstock, the category of finite sets and
stochastic matrices or stochastic maps. So usually information flow is not necessarily,
or when you feed information into a process, while the outcome of the output of the process
is typically not uniquely determined, but there may be some randomness in it. And so we should expect
the definition of Markov category to be able to capture this kind of randomness that processing
may make a price to. And this is captured, for example, by Finstock or by many other categories,
place the categories of probability monads, this kind of thing. But for now, let me just
introduce Finstock. So the objects are finite sets. And morphisms are stochastic matrices,
which means I'd actually like to use conditional probability notation for these, because that's
exactly what this encodes, that it's a matrix indexed by the elements of the domain set X,
and the codomain set Y, and the elements in our f of little x, given little f, f of little y,
given little x, and this denotes the conditional, the probability of getting an output little y,
when the input is little x. And so then we should satisfy the axioms of probability that
they're non-negative values and sum to one when you sum over the outputs for every value of the input.
Yeah, these compose via what's called a Chapman-Komogorov formula. That's
it's the obvious thing that you would expect that the randomness involved in the two processes is
independent, and so the probabilities multiply, but the intermediate state is not observed.
So we just kind of sum over it. It doesn't matter which, what the value of Y of the intermediate
thing is for determining the relation between the overall input and overall output. Sorry,
this should be an X here on the right. Oh, and if there are any questions, feel free to interrupt,
so that I maybe get some feedback to see to what extent people are following or lost.
Okay, so in particular, morphism from the just the terminal set, the one element set one,
two X, when now the input is trivial, there is only one input value, so hence we get a probable
just a probability distribution on X. A general morphism, well it has many many kind of names,
as I said, it's a sort of process with randomness involved, often also called Markov kernel, that
that's what Markov categories are named after as categories of Markov kernels, or generalized
categories of Markov kernels, but also probabilistic mapping or communication channel information
theory, these are all really synonymous. Okay, so well, I haven't said yet how this is actually
a symmetric monoidal category, so the monoidal structure is just given again by doing things
independently, but now in parallel, where independently is supposed to be understood
in a in a sense of probability theory, stochastic independence, multiplying probabilities again,
probability of getting outputs X and Y given inputs A and B for the joint process, it's just
you do compute the individual probabilities and then multiply them by independence.
Yeah, so this now we have a symmetric monoidal category, but not yet a Markov category,
because for that we need to say what the copy and deleting maps are. So the copy maps,
well it's just the obvious thing of copying information, and in this case there is no
randomness involved, so all the probabilities are zero or one, and the probability of getting outputs
X1 and X2, when the input is X, well it should be one, when these two outputs are equal to the
input both and zero otherwise, so that's just the obvious notion of copying the value of X.
And the deletion maps, well there there is a unique morphism to the monoidal unit
one from every X, and so these are the deletion maps. The normalization axis here at the bottom,
together with interacting wealth and monoidal structure is actually equivalent to saying that
the monoidal unit is terminal, so we don't really need to care about making clear what the deletion
maps are, they're automatically unique. And then it's yeah pretty simple to check that all these
axioms actually hold. Any questions on this? All right, so at the rest of the talk I'll sketch,
although I don't really have time for a lot of, for all the technical details, how one can develop
some theorems of probability theory in terms of Markov categories, and in some cases while making
such developments as as we all know in category theory generalizing theorems from other areas of
mathematics may involve turning them into definitions, so in some cases we can prove theorems
from probability theory, in other cases we have to turn them into definitions as apparently or
at least then we haven't found a way of turning them into theorems yet, but at least we may have
ways of turning them into definitions. But on the other hand, so there is a vast landscape of Markov
categories, and so Finstock it has a bunch of variants from other probability monos as I've
mentioned, but there are actually a lot of other types of Markov categories out there and so far
these have barely been explored at all, and so one can instantiate a theory's theorems from the
previous item on these Markov categories, so that sort of become more general than just applying
in the context of probability theory, but so far it's, yeah, we really have little idea of what
they instantiate to, so we're just at the beginning with all of this, but in terms of these two
items, I feel like there's some kind of at least vague analogy with Topos theory, in a sense that
Topos theory also has these two kinds of features, it's a very powerful framework for developing
mathematics in the sense that every piece of mathematics that has a constructive, every theorem
that has a constructive proof holds in every Topos, and so this, so that means we can just
reason as if we were to reason reasoning about sets more or less, but still there are a lot of
Topos's around there, and they may not be so, this may not be so obvious, especially if you just
learn Topos theory from the first time if you just become familiar with it,
but again we can then instantiate all of these theorems, every constructive piece of
mathematics in every Topos, and that's a really powerful principle, and so I'm hoping that maybe
something like this will also turn out to be the case, which seems to be the case from Markov
categories. Also in Topos theory, there's a hierarchy of additional axioms that one can
impose on Topos's, like well in a Boolean Topos you can use the law fixed through the middle,
and then you can prove stronger theorems, and I think just the same is true for Markov categories,
so proving the theorems of probability theory typically will involve some additional axioms,
but depending on the theorem, those axioms will be different, so there's a hierarchy,
a whole sort of poset at least of additional axiom systems.
Okay, so overall another working hypothesis that I have is that the theory of Markov categories
is, as I've sketched at the beginning, roughly a general theory of information flow,
but on the other hand it's also a sort of about probability theory, and so the perspective that
I've arrived at is actually that probability theory, when you suitably generalize it,
in this categorical perspective it's actually synonymous with the theory of information flow,
and we can see this a little bit more concretely and maybe in what I'm going to show.
So, in particular, this is from what Brandon Fong did in his master's thesis,
what kind of firing based on networks and Markov categories, I mean Markov categories
didn't yet have that name at a time and didn't hardly even exist, so then he left it for it all,
so he didn't do this very explicitly,
but yeah, this should be, it's actually in some sense what I already showed here in the medical
trial, this is more or less a basin network, a way of a process where you have random things
taking place, and then processing between those, and then you get some output variables,
overall output variables, and there's a certain network structure, that's what basin networks are
about, so we can define these, but as far as I know the moments until now, and I think this is
something that is actually currently under development to some extent, is that we can do
causal inference in Markov categories, so if my hypothesis is correct, and I see no reason for
why this shouldn't be possible, it should be possible to generalize the theory of causal
inference to Markov categories, so causal inference is a theory which tries to make statements about
what actually is the network structure, when you only observe a bunch of, you only make a bunch
of observations, but you don't know what kind of network has given rise to those observations,
has generated those observations, and causal inference tries to, well, infer what the network
was, and given that the causal inference that I know, my impression is that this pretty much
directly generalizes to all Markov categories, with suitable additional actions like conditional,
there needs conditionals and things like that, so conditional probabilities, but those are,
we know how to make sense of those, so this is one example of a theoretical development that
should be possible, that hasn't really been conducted yet. Okay, theoretical development
that is already being used, I guess by some of you, is based in inversion,
so this is from the paper of Joe and Jakobs, and I've written this now in a bit of a forum,
which directly illustrates the connection to how basing inversion usually works in probability
theory, so basing inversion is that when you have a joint distribution, so two variables
x and y, then you can express the joint distribution in terms of just the probability of
the distribution of x, and then the conditional distribution of y given x,
and so in terms of Markov categories that corresponds to this diagram on the left,
and so you can see we just generate the value of x, we keep one copy around, and we feed the other
copy into the conditional distribution of y given x, and so then the output is the distribution
of y, and that exactly mirrors the string diagrammatic version of this algebraic expression,
and where the copying is also perfectly manifest, and that there are two occurrences of x.
Okay, and so now a basing inversion does is to turn this around, and instead regard x as sort of
depending on y, or yeah, to consider y as the variable is generated first, and then you feed
that feed one copy into the conditional distribution of x, and so basing inversion is the map sort of
from the left hand side to the conditional in some sense, so p of x given y, we can now
write this as the left hand side divided by p of y and the usual discrete probability,
but yeah, this can be generalized and say that whenever such amorphism exists in a Markov category,
then we call that the conditional, and well it's not unique, but it's not unique in the
script, not even in usual discrete probability theory, it wasn't unique to begin with because
of issues of zero over zero, so this can be generalized and this is basing inversion,
and one can show that there's a sense in which this map, so it somehow takes amorphism from x to y,
turns it into amorphism from y to x, also has good compositional properties, namely it's a dagger
functor, which is maybe one exactly what one might expect as a category if there is when you have
a map that reverses the direction of morphisms.
Okay, another thing we can talk about is, and so this is what I'm going to need before I'll state
one of the theorems that have proven the zero one law of u is savage, which is a classical
result of the measure of theoretic probability, and so this requires the notion of determinism,
so amorphism is deterministic if it commutes with copying, meaning that if this string diagram
equation holds, so the intuition here is that if you apply f to two independent, to two copies
of the input, and then you look at the two outputs that you get, well that should be the same thing as
just applying f to a single copy of the input and then copying the output,
and so one way to understand this is to say that well f is actually independent of itself,
but that sounds maybe a little obscure, but you can understand it very, or very intuitively by
noting that on the right hand side, these two outputs are always the same if you now think
about what this means in the case of Finstock, the two outputs on the right are always the same,
but on the left they're always the same only if f actually does nothing random,
well if f sometimes applies some randomness and so the output is not uniquely determined by the input,
then these two outputs are sometimes going to have non-zero probability for going to be different,
and so then the equation would not hold, so this equation says exactly that f should be
deterministic or in the case of Finstock just being a zero one-valued stochastic matrix.
All right, so in general, any market category, the deterministic morphisms form a Cartesian
monoidal subcategory, so basically by definition the monoidal structure turns out to be actually
the categorical product. Yeah, we can also talk about conditional independence or to
define this now what this means in any market category, so conditional independence is supposed
to mean that when you have the process which takes one input, produces two outputs, we'd like
to say that once we know the value of the input, well then the outputs may be still random, may
not necessarily be uniquely determined, but they're random in an independent way,
and so this is what this equation says, that there's no correlation or cross dependence
between the outputs other than what is determined by or what is induced from the input.
Okay, so before I can state our version of the unit savage zero one law, well I need to state the
usual formulation of it, and so this is closely related, but before that I'd actually like to
remind everyone or introduce the law, the strong law of large numbers, so this is now
a more just in conventional probability theory notation, you can perfectly think of these
x i's as just taking, let's say, taking on finally many values and being real valued,
so these are real, yeah, real valued random variables, and they're assumed to be independent
and identically distributed, so they're just sort of, for example, dice, right, you throw die
n times, you get outcome one to six, and then this would be the x i's, the individual outcomes,
so the left hand side is in the average value, and in the limit, as n goes to infinity,
well this may or may not converge, it depends on a particular realization of your dice
who throws, but this says that the probability that it does converge and that the limit is equal
to the expectation value, that is that probability is one that's the law of large numbers, this is
one of the things where we don't yet know how to state and improve this in terms of Markov categories,
maybe I'll say a little bit about it at the end, but there's a closely related classical result
which we have proven synthetically, meaning in terms of Markov categories, and that's the
UIT savage zero one law, and here again in the classical formulation we have these
independent and identically distributed random variables, and now instead of just talking about
this average and the conversions and the limit, we'll just look at any event, any sort of function of
the x i's, so or think of it as a predicate which depends on all the x i's, and which is invariant
under finite permutations of the x i's, so the conversions on top in this equation is independent
about when we just do finite remaining rearrangements of the x i's that doesn't affect the limit,
so the general statement assumes any such a, and then the claim is that the theorem says that
a probability of such a is actually zero or one, so it's deterministic, but a theorem doesn't tell
you which one of these two things happen, it's really kind of strange in that way,
and so yeah you can apply this in a situation of large large numbers, and then conclude that
the probability of it converging to the expectation value is zero or one, but it doesn't tell us which,
so but at least it's a statement going in that direction.
Okay so here's our synthetic formulation in terms of Markov categories, I don't want to
or don't have the time to really explain this in all detail, but just sort of maybe to give the
flavor of it, so this is from my paper with Igel, and this uses an additional axiom, the so-called
what would I have called causality axiom from Markov categories, and then is concerned with
actually a somewhat more general setting than the US average zero one law, so instead of this event
which would be a predicate, we now are going to have a deterministic morphism
taking values in any other object, you just need to take values in anything like truth values,
and instead of just having a probability distribution of infinitely many random variables,
we now have a morphism which is not necessarily just a distribution in the sense of having the
monoidal unit one as the domain, but having any domain a, so it's somehow more parametric in a,
yeah, and this involves another concept from the theory of Markov categories,
Kormogorov products and Kormogorov powers that we had also introduced, these are a version of
infinite monoidal products, so well when you have the object x, we can for any finite set,
we can take f many tensor products of x with itself, but for the theorem we actually need
infinitely many, and so the way that this is defined is in terms of a directed limit,
a co-filtered limit over all these finite tensor products, and while there's
projections between those, so we can ask if the limit exists, and have an additional
preservation condition that makes it into a Kormogorov power, yeah, and then as in a classical
theorem we have the invariance condition, well here we actually have two invariance conditions,
that's these are also contained in the classical one is saying that the x i should be independent
and identically distributed, so there's also the invariance condition there, and so it's exactly
analogous, yeah, except that this is in some sense more general, and then the claim, the result is that
the composite sp is deterministic, so going from this parameter space a to the truth values
object t, and then that instantiates in this particular case to the probability of being a zero one.
All right, so the proof, yeah, is by string diagrams, but still while other mathematicians
sometimes maybe scoff at string diagrams because it looks trivial, but the proof is actually not
trivial, or not obvious from just the statement, even if you try to fiddle around in string diagrams
a little.
All right, so this was part one, and that I wanted to indicate what sort of theoretical
developments one can perform with Markov categories, oh, and by the way, right,
to finish this up, let me maybe just indicate again how I've arrived at this perspective that
it's better to call this or maybe to at least to think of this as being about information flow
instead of being about probabilities and probabilities, distributions, and that also this
theorem is very much about a flow of information, and that the information here somehow goes from
a to this commogorov power, vfp, and then we apply s to that, and it ends up in t, and overall we
make a statement about a flow of information from a to t via this commogorov power,
and the usual sort of formulations, usual theorems of probability theory where everything
has, is assumed to have to join distribution and so on, doesn't, one can't see that kind of structure,
I think, as clearly.
Okay, and any questions on this?
So there have been a couple of questions in the chat,
so the first one was, is there a rigorous one line definition of causal inference?
Oh, I, I don't think so, in the sense that this is a sort of,
I'm sorry, you are muted, sorry, can you repeat?
Yeah, yeah, good. I don't think that there can be a one line, let's say at least not
mathematical definition of causal inference, because this is a, I mean, an area of study
as a scientific field, and so we can't really get four line definitions, or at least, yeah,
but the type of problem a causal inference tries to address is to try to infer a network structure
and to disentangle a particular cause and effect, so causes are things that are more like in the
input, further down in a network, effects that are things that are further up, to disentangle
these things and try to make inferences about the network structure from observations about the
network, in particular, observing outputs and correlations between outputs.
I see. Okay, some people are-
Just to start to answer, I'll address the question.
I don't know, Arthur?
Yes, yeah, I was just curious if you were thinking that Markov categories give you a
model for such a definition, I guess.
What do you mean by a model?
Or if you can use the structure of a Markov category to give such a definition.
Yes, that's actually, that's interesting. It would not capture everything that people
are trying to do, that they call a causal inference, but it would, let's say, definitely,
yes, so one can certainly, right, then give a definition of this main problem of causal
inference that I've just sketched, yes, I think so, but then there will still be other things
that people call a causal inference, for example, what do you, how do you deal with having only
finite statistics, how does that affect the problem and all that, but yeah,
but the main problem itself can then be captured, yes.
Good point. Thank you.
There was also a question by David Spivak. David, has the question been addressed in the chat in
the end? I think so, yeah, thanks. Okay, any more questions about this part of the talk?
It seems normal, let me just mention that there's a little bit of discussion already
in the chat about causal inference, and it seems very interesting, so I encourage these people to
continue that on solub, maybe at the end of the talk, because something nice may come out of it.
For the rest, please go ahead. Yeah, sounds great, so I can't even see the chat to the one
because of the screen sharing, I suppose, but I'd like to read it later, yeah.
What was I, oh, I forgot to mention, of course, I wanted to say at the references,
one of the other ongoing developments is generalizations, extensions of Markov
categories to quantum theory, quantum probability by author, in particular,
has a number of quite interesting works in that, I know.
Okay, so this was the part about the theory, and now I'd like to,
my other item was to try and give a flavor of the landscape of Markov categories and the scope,
and I still feel like, so new things are still coming up, and so I feel like I'm still
probably not able to sketch a complete picture, a reasonably complete picture of
how and where Markov categories can come up, but let me say what there is so far.
So first of all, that's some of the most basic and most important construction is that
classic categories are sometimes often Markov categories, and so the proposition here is that
if you have a category of finite products, C, and if you have a commutative monad, P on C,
which is affine, in the sense that a P of the terminal object is again isomorphic to the
terminal object, like for a monad of probability distributions, so there is just one probability
distribution on a one element set. Then the classic category of P is the Markov category,
and let's say the obvious way, so using the categorical product of C for the testimonial product,
and then, yeah, the copy maps are just the diagonals that we have from the categorical
product on C, but now they're in general just no longer diagonals of the categorical product,
because the monoidal structure typically no longer is the categorical, but typically the
classic category no longer has products even. So this is what happens when we get Finstock,
Finstock arises, sorry, no, it doesn't, because if we extend Finstock to all sets and finally
support a probability distribution, then it arises like this, and other monads, in particular
for the Zürich monad, we can do this, so this is the probability monad going back to Lavir
and Zürich on measurable spaces. It's a classic category, then it's again one of the
paradigmatic Markov categories for probability. We can also do things like the classic category,
take the classic category of non of the power set monad, and then while the classic category
is real, but you get a Markov category, there's the subtlety that we should just take the non
empty power set monad only, and then we get real with some normalized relations, so that has to do
with the normalization equation that I had shown in definition of Markov category, but it's almost
real. And also this proposition for constructing Markov category still holds when C is not
necessarily a Cartesian monoidal, but just merely a Markov category itself, so you can use it to
construct new Markov categories from given ones. Okay, another large class of examples are categories
of co-monoids, so when you have any symmetric monoidal category and you take co-monoids in it,
then you get a Markov category, and so its objects are the co-monoids in C, and its morphisms
are going to be the morphisms of C, but those that preserve the co-unit of that's in order to get
the again the normalization equation, but so in general a general morphism does not need to respect
the co-multiplication in any way, so this seems maybe a little silly to then even introduce
co-monoids to begin with, but the point is that the co-multiplications of the co-monoids
are what specifies or defines now the copy maps in this category of co-monoids.
So this is how that becomes the Markov category, and a good example or interesting,
somewhat interesting example from a probability perspective is if you do this actually with
vector spaces over a field K, and oh so yeah I should op it, the op here is pretty much the working with
predicate transformers as opposed to state transformers, or equivalently in physics terms
using the Heisenberg picture instead of the Schrodinger picture, so meaning that we now think
of these co-monoids in vector op which will correspond to just K-algebra meaning monoids
in vector as algebras of random variables meaning K-valued random variables, and then the corresponding
morphisms in the Markov category are going to be something like random variable transformers
or to be thought of as formal opposites of Markov kernels.
When we take K to be R, then this is actually fairly close to
yeah, traditional probability theory much below that this would allow negative probabilities.
Okay, but now there are more examples coming from yeah of constructing new Markov categories
out of given ones, and one of these is diagram categories, so this is again maybe also like in
a bit like in Topos theory in that the doctrine of Markov categories is stable under suitable
formation of suitable diagram categories, so when you have any category D to index our diagrams,
then we can see a Markov category, then we can consider diagrams of shape C
that take values in deterministic morphisms since C only. This is an important subtlety here,
that the diagrams need to consist of deterministic morphisms, and then we can consider natural
transformations between those with arbitrary components in C not necessarily deterministic.
So if we just apply this for example with the diagram shape D just being the integers
considered as a poset, so if arrow is going from lower numbers to higher numbers,
then we got a category of discrete time stochastic processes, so then this is, well in case
you know a little bit about stochastic processes already, then this is the Markov category version
of saying that we have a filtration of sigma algebras, and well this idea is really
pretty old and goes back to love year's 1962 paper on the category of stochastic relations,
but so this construction generalizes this formation of diagram categories to all Markov
categories. Another thing that may be interesting, another kind of instance of this, is to take a
group G and then just consider it as a single category, as a category BG with a single object
and just the G as the elements of G as the morphisms as usual, and then so then the diagrams
would be just objects to C equipped with a G action, and that way again we got, yeah Markov
categories are stable under this construction, and we can again get a notion of dynamical systems
with deterministic dynamics, but stochastic morphisms if we take the group again to be the
integers for example. Okay, a speculative one is, so this may well be a non-example, I don't know,
but at least I find it an intriguing direction to explore, except maybe Markov categories may
also be able to capture information theory, so in the following sense, so as I said Markov categories
are about information flow or sort of generalized probability theory, and so there's some well-known
analyses between probability theory and information theory, and information theory now really in a
sense of entropies, namely for example conditional entropy behaves a lot like conditional probability,
so if you take the formula for conditional probability up here, and you basically replace
probability by entropy, and instead of taking multiplication and division you turn this into
adding and subtracting, then the formula for conditional probabilities becomes the formula
for conditional entropy, the chain rule for entropy, so it's somehow since we can talk about
Markov categories, since Markov categories provides a general setting to talk about conditioning,
it's somehow tempting to think that well maybe we can also talk about conditional entropy like that,
and so I don't think it will exactly work like this probably, but I still suspect that there may
be a Markov category for information theory which may be able to explain these analyses
and sort of make them into more interesting analyses, but actually show how these are become
instances of the same concepts, and one idea might be to take objects to be finite sets,
and to do a sort of construction that's similar to Finn's dog, but just takes into account the
information theories about asymptotics, as in when you do things in a many copy limits,
many realizations of the same thing, and then look at asymptotic equivalences,
so the morphisms maybe should be something like compatible families,
instead of just being stochastic maps from x to y, maybe there should be families of maps from
a large power of the set x to a large power of the set y, and then the kind of asymptotic equivalence
that one usually encounters in information theory, but I don't know yet.
Okay, and then there's, this is actually not an example of a way of constructing Markov categories,
but it's an intriguing idea for further applications which emerged at the Categorical
Probability and Statistics workshop last month, and so basically Peter Arndt pointed this out,
that there is this theory of hyper structures in algebra, where one considers structures which
have, let's say, not a binary operation in the usual sense, which would assign a new element to
any pair of given elements, but a hyper operation by which people mean that it's a multi-valued
operation, you get a set of elements for any two given elements, or in another version of
hyper structures you get a probability distribution over elements for any two given elements.
So I think the Markov Categories would also be a general setup to talk about these kinds
of things, and to try and develop categorical algebra for hyper structures in a way that is
analogous to how one can develop categorical algebra just for ordinary models of algebraic
theories. And so, for example, we can define what a group is in a Markov Category,
so usually, well, a group is in a Cartesian monoidal category, a group is a monoid g,
we can talk about monos in any symmetric monoidal category, so I won't unfold this part any
further, but just the, we need to be able to talk about inverses and to say that every element
multiplied with this inverse results in the unit, and this in a Cartesian monoidal category,
we can express this condition like this as a string diagram. You take any given element coming in
from the bottom, you take two copies, you invert, you apply the inverse inversion map to one copy,
and then you multiply them, you multiply the resulting two elements, and then you got an
output, and that's supposed to be just the original element, so hence this equation.
So this is, well, this is well known, I mean this is the Hopf-Valchivar equation basically,
which defines Hopf-Valchivar, but my point here is someone can interpret this now in any Markov
Category, because there we have to come up with the copy maps available, even if the
category is not Cartesian. And so, more generally, I think one can consider models of any
algebraic theory in the sense of in the sense of a Lovario theory in any Markov category.
And so, yeah, in this way, trying to develop a categorical algebra for hyperstructures,
which would then encompass both the hyperstructures in the sense of multi-valued
operations, but also the hyperstructures in the sense of probability-valued distribution-valued
operations. Okay, I think this was pretty much the last thing that I had, so yeah,
let me try to summarize. So Markov Categories are an emerging formalism for providing a general
theory of, well, this was supposed to be, say, the same information flow and uninformation theory,
sorry. So I hope you have to have argued this for this, and that many qualitative results of
probability theories seem to generalize to Markov Categories and thereby become results
about information flow in some sense. And yeah, well, doing so usually requires some
additional axioms, and maybe I should re-emphasize it also in some cases, which may require turning
theorems into definitions. Okay, and so on the other hand, there's a big unexplored territory of
Markov Categories in which one can instantiate those results, and so far, and the work that's
been done so far, as far as I know, and maybe Evan's work is a very interesting counterpoint
to this. Most of the work seems to have just looked at, yeah, instantiated results in
categories that are classic categories of probability monas, or at least very similar to those.
And so yeah, these two features of having a powerful theory, but also many examples,
I think this is how similar to the situation with Topo's theory.
All right, there are maybe two more further interactions that I'm personally quite curious
about, and that especially the first one has been on my mind for a while, but somehow I haven't been
able to make any progress in it. And there's a lot of different concepts, types of probability
measures that people study, and I think also for a good reason, and then different types of probability
monas and different types of spaces. But one can still wonder whether there may be a most
convenient one, but so by convenient, I do mean in the sort of convenient category since
Markov category for measure theoretic probability. And so here I have a list of desiderata, and I
don't actually know of any category, I haven't had time to really explain what all of these things
mean, but I don't know if any category would satisfy all of these, and would just exclude
trivial examples to any non Cartesian Markov category at all, regardless of other models
probability theory or something else, which has all of these satisfies all of these properties.
And finally, a point that that many people I think have pointed out, starting with Lavire
a long time ago is that we really should be also looking at enriched categories
in order to be able to capture things like the law of large numbers. And many of the statements
of probability theory, which are cognitive rather than qualitative. So as I said, I think
the qualitative results of probability theory seem to be quite amenable to this kind of
generalization. But the corner that I've once is certainly something to struggle with. And so it
seems somehow natural to try and see how far we can go by looking at enriched Markov categories,
and in particular enrichment in metric spaces, of course, to talk about approximations,
but there may also be reasons to look at other kinds of enrichment. Okay, yes, I think that's
that's it. Thanks. Thanks for listening. Thank you for a nice talk.
And we have some additional questions. So first one by David Kirchevi, who's asking
about that construction where you form a Markov category from a community of monad.
So if you have a community monad, the classic category is a Markov category.
He's asking, would this work for co monads too? And then he's adding the example of the infinite jet
monad. Yeah, actually, I was looking at the jet monad, I suppose. Yeah, the jet co monad a few
days ago, and I haven't been able to understand what its co multiplication does. But but I've had a
feeling like like something similar may be going on there. But I don't know, in particular, because
I don't understand the jet co monad. Do you know if a good reference where the co multiplication is
explained in an intuitive way of the I mean, I understand obviously that what the co unit of
the jet co monad does, but the co multiplication I haven't been able to maybe one thing that could
help is that there is a discrete analysis of the jet co monad, which is of course much simpler,
which is a string co monad. So instead of looking at higher derivatives, it looks at
points which are further in the past, like for time instance. And there, of course, there are a lot
of references in computer science literature. I haven't written on that myself. But about the
differentiable case, I don't know, does anybody in the audience know maybe David himself depends
what you mean by intuitive. So, um, yeah, I'll try to I'll try to listen to email if I can think
of a way to just any reference for the co multiplication is treated and a little more
detailed and what's on the lab, for example, that would be okay. I mean, so I'll have to
check to see the references. I know how much it's it's spelled out. So I'll let you know.
Yeah, thanks. Somebody's writing in the chat jets. And I think it's just limit jets in differential
linear logic by James Welbridge. It's in the chat. If it can help the co multiplication of the
string co monads, looks at the history of the history, basically, popping elements from the list.
I don't know if that's analog to the differentiable case, I suppose, but I'm not sure.
Then we have another question by Arthur Arthur then said that he got it. But actually,
I wonder the same question to and I didn't get it. So let me ask it.
Namely, if we if we form this diagram category index by Z that has stochastic processes,
then we want commutative diagrams, I suppose, so we want deterministic maps.
Yes, right. But how does that the diagrams but not as morphisms between diagrams?
Yeah. But how does that then model stochastic processes, right? Because they're deterministic
in the horizontal direction. So the same. Yes. Yes. This is actually exactly parallel to how
stochastic processes are usually set up in that you have a basically also a diagram of
measurable spaces and measurable functions. And the way to think about that, so also indexed by time,
the way to think about that is that the measurable space at time t just represents
all the things that have happened until that time.
So it's sort of all that. So in practice, it will often be the sort of product,
the product space. And then as as the maps you have, I may be confusing future and
past. Yeah, maybe it's really backwards. It's like a backward filtration.
Yeah, I'm not sure. But yeah, forward or backward. Yeah. But right. So then usually this would be
an infinite this, sorry, no, yeah, you would have these these products
as the objects in a diagram. And then the product projections that that just project out one
instant of time, the values of the process of one instant of time and forget that.
And then you can think like, and the point of them doing that is because that way you can say,
as far as the morphisms of the diagram is concerned, you can talk about having a morphism
so stochastic processes where what happens at times t is allowed to depend on all the things
that have happened until then. But it's the natural rather naturalians acid, this basically
must be independent of the future. So it's allowed to depend on the past but not on the future.
And that's also why one wants to have these this actually this this these big product spaces and
then a deterministic projections going in between.
I see in that sense. Thanks. Thanks. I think that answers my question and probably also Arthur.
So there is a couple of other questions. So if you can go back to this
group objects in the Markov category.
I think if I can correctly summarize the question, both questions of both
Davis, if we can call Comfort, please correct me if I don't, they're asking what the white dot is.
And I suppose the white that is just that you're picking some monoid object in
this case. Yes, exactly. Yes. Where's the black dot is fixed.
Is there a good example of this that you have in mind in a Markov category you like?
Yeah, to be honest, for the group case at the moment, I don't know.
But oh, yes, of course. Sorry. Yeah. So two things that one thing that actually forgot to say here
just a caveat is that this definition of if you take this as a definition of hyper group,
then this is actually not as far as I understand the moment not equivalent to what people usually
call hyper group for the way they're which has inverses in a weaker sense.
But I think one so maybe I should be a little careful because I've just been
writing this up yesterday and really carefully thought about all of these things yet.
I think one kind of example, interesting example of this kind of
hyper group and as if you do this and again, and
no, sorry, that doesn't actually work.
Yeah, no, then I don't know the amount.
Under sort of weak assumptions, don't you have that isomorphisms are deterministic?
Yes, exactly. Yeah, right.
Yeah, do you have any good example where this is actually not the case?
Where isomorphism? Sorry. Well, so maybe these examples are not good because they're
maybe already too degenerate. But here in this category of co mononets isomorphisms are typically
not deterministic because well deterministic morphism in particular would have to be one
that respects the co multiplications intertwines the co multiplications. But of course, you can
have isomorphic objects and see or different co mononets non isomorphic co mononets
structures in the same object. Does that help? Yes.
We have a question from Cole like do you also need the bi algebra rule? I suppose we're still
talking about internal group objects in the Markov category.
Yes, that's a good point. Yes, right, I think so. So this is hidden in
how one can consider models of any Lovio theory in a Markov category, I think.
So yes, that's true. I was a bit too sloppy here. Yeah, thanks.
And then we have a question by Aigil in the Zulev channel. So Aigil Rishal,
I don't know if that should be discussed on the Zulev channel or here, but let me mention it.
Can you elaborate on your information theoretic example? Maybe if Aigil can unmute himself,
he can ask. Yeah, sure. I mean, I'm just sort of if you can go to that slide, maybe with the yeah.
Yeah, so you say that you should have like compatible families of stochastic compatible in
what sense? I suspect just in the sense of the obvious naturality condition.
Oh, so it's about like dependence. Yeah, okay, I see.
No. Well, maybe we can go into the, maybe we can write down the details.
What I mean is just that if you know that every fn and fn plus one should be compatible
in the sense that if you project down from x to the n plus one to y to n.
Oh yeah, okay, okay. So that fn plus one sort of would extend fn.
And the asymptotic equivalence is some sort of like frequency
Yes. Converges to yeah, okay. Yes.
All right. Well, in the sense of the
At least the first try that I would make is to
define it as saying something like for every, for all inputs,
the just a total variation distance on the outputs converges to zero as n goes to n.
All right.
But maybe we need something a little more sophisticated along the lines of also then
looking at locks of probabilities. Sure. All right.
Okay. Yeah, that answers my question.
Okay. Arthur has another question. Please go ahead.
Um, so actually about this slide, um, I don't see the connection between the bottom paragraph
and the top two at all. Can you just give some insight what you're referring to how
such a category if one exists may describe what's going on regarding conditional entropy and probability?
Yes. To be honest, I myself also don't. So this is what it's kind of wild speculation at the moment.
I also don't see much of a definite connection so far.
And in particular, I think that what I'm hoping is that the isomorphism classes of objects,
then will basically be, no, sorry, I'm going to say the, if you look at internal probability spaces
by which I mean objects equipped with the morphism from the monoidal unit,
then I think that the isomorphism classes, the these should be in bijection of just a non-negative
reals corresponding to just so a probability finite probability space is asymptotically
uniquely determined by its entropy. And then the conditional entropy equation would suggest
that maybe this also holds for the morphisms. And then also morphisms are some hope just
classified by by real numbers. But I suspect that that's probably not the case.
I see. Thank you. At least if one uses the definition at the bottom verse from what would
expect at the top, maybe would expect this to be the case that also the morphisms are classified
by non-negative reals. And so maybe there's a bit of attention there. Is this what you also noticed?
No, I mean, I was just asking. No, I was thinking about something else, but thank you. Okay.
Very good. Any more questions for Tobias?
Um, this is probably in the right smack in the middle of your paper, but is it
true that if I have a category that supplies comonoids and I fix any object, sorry, any monoid
and slicing over that monoid gives me a markup category?
If you start with what? If I start with a category that supplies comonoids,
and I take an object with a monoid structure and slice over it,
then it becomes terminal and that should be fine, right?
I haven't, I don't know. You haven't thought about it. Okay. Okay. Thanks.
Okay. Any more questions?
Can you, can you then copy the stuff from the chat in particular? Yeah, because of the chat comonoids?
I can say something about the chat comonoids, actually, if you, but it,
I mean, it's not as maybe as tangible as you want, but if you, if you view it
type of serratically, it's literally just push forward to the drum space and then pull back.
And it's just a co-unit map is here. This is induced by that, that junction that you have
between the slice over your object and slice over the drum space of it. Yeah.
Okay. Right. That's what I've seen, but at least until now I'm not somehow not having really
gotten up an intuition for that. But so then maybe I can ask, does it make sense to think of,
especially in a sort of SDG topo theoretic spirit of the, the jet space, the infinite jet space
as a sort of exponentiate, yeah, as, as a, as an exponentiation by some kind of infinitesimal
object. Yeah. And then the combinator rises also from, from that. I mean, it's when you,
when all of your formal neighborhoods are the same, this is true. So, yeah. Yeah. Okay. Thanks.
Very nice. Any more questions or comments?
Seems not. So, well, first of all, let's thank the viewers again. Very nice talk.
And now, if you have any more offline questions, feel free to use the Zolib channel. Let me post
the link in the chat.
