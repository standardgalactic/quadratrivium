I'd like to thank Bradley from Trinity to tell us about modeling language with tensor networks.
Thank you for the invitation to come. I really appreciate it.
So, modeling language with tensor networks. I thought it would be helpful before I start writing ideas on the board
to give a little bit of background or context for what this talk is about and where I'm coming from.
So, before I write anything down, I just want to spend a few minutes just putting together some ideas
that I want to then spend many, many minutes on the board.
So, I'd like to share some mathematics that I think is very interesting, that I like a lot,
and it's motivated by a very concrete problem in machine learning.
And the idea is to understand meaning of words or expressions or longer pieces of text,
understand meaning of these things in a way that a computer can understand.
So, I want to understand meaning of phrases in a language.
And then the first question is, how is this a mathematical endeavor? How is this mathematics?
One way I like to think of it is an idea from linguistics, which is this distributional hypothesis.
So, this is the idea that two words or phrases that appear in similar contexts in language will then have similar meanings.
So, I like to think of this as a yonata lima for linguistics.
I sort of know a word by the company it keeps. This is a famous quote by John Furth.
So, to me, this is like the yonata lima for linguistics.
So, already language is certainly mathematical from that perspective.
There's another way to think of structure mathematically in language,
and that is this idea that language has both compositional and statistical structures.
So, it's compositional in that words go together to form longer expressions like red fire truck, red fire truck.
So, you know, compositionality. But then there's also statistics behind that.
For example, in English language, we sort of know that red fire truck appears in language more frequently than blue fire truck or, you know, polka dotted fire truck.
And so, I'd like to propose then that, given that that's true, that somehow red is contributing to the meaning of fire truck.
So, you sort of see red in this context, this sort of network of relationships that fire truck shares with other words in the language more frequently.
So, there's statistics there.
And so, the question then would be, what is this mathematical structure? What is this compositionality plus statistics?
So, one route you can take to explore this is a very category theoretical approach.
And I'm thinking now of this disco cap model, maybe some of you have heard of it, some of you haven't.
I can briefly describe it.
So, the idea is that, well, language, the meaning of language should be, or the meaning of a sentence.
Let's just take a sentence, for example.
The meaning of a sentence, well, is determined by the meaning of these individual words that comprise the sentence,
together with this grammatical rules for composing them.
So, you know that, you know, noun followed by a verb, that's a valid thing in English.
So, then, what you can do is try to model this grammar together with semantics.
Maybe grammar forms a category like a pregroup, for example.
Your semantics, these are really like vector embeddings for words, if you want to then feed this into a computer.
You can try to model this structure as a functor from one to the other.
A functor from grammar into a nice semantics category like vector spaces.
So, this has a name, it's called the disco cap model distributional categorical compositional approach to language,
modeling for language.
But that's not what I'm going to talk about today.
So, I like to think of that as kind of an inside out approach.
But I like to describe something a little bit different today, which is more of an outside in approach.
And the key starting idea for this is to let statistics serve as a proxy for grammar.
So, actually, that's an important idea, so I will write that down.
So, what I want to describe today is a model for language in which I let statistics serve as a proxy for grammar.
In other words, what do I mean here?
I mean, I would like to infer, I'd like to infer what are valid expressions in my language,
just given some examples of valid expressions in that language.
I want to know that some words go together to form good sentences and others don't,
just by me saying the statistics of that language, just by seeing it.
So, in other words, if I write this down, I want to sort of refine my goal a little bit.
I want to infer a probability distribution on some data set of the text.
So, I want to know what goes with what in my language.
You know, with high probability, I'll see a red fire truck.
But with very little probability or lower, I'll see blue fire truck.
So, this is what I want to know what goes together with what based on statistics.
Okay, so then how do you do this?
How does one do this?
So, what I'd like to do is sort of give away the punchline.
I'm going to give away the punchline and sort of say how, I'm going to describe for you a model in which I can infer this probability distribution.
Give away the punchline, but then the rest of the talk, I'll sort of unwind it very slowly.
So, the idea for this model starts by viewing language as a quantum mini-body problem.
In other words, what that means is you have these atomic pieces of your language.
These are characters or words, for example.
And you think of these atomic pieces as individual quantum particles, each of which can occupy many, many states.
Then language, then, is this interacting system of a bunch of these quantum particles.
And, well, how do you model this system then?
What is a model for this language?
What you can do is then take samples of it, examples of expressions in your language,
which you can think of as observations of this quantum system.
Then when you have these observations, you can then try to infer the ground state of it using some machine learning algorithm or physics-inspired ideas.
Yes.
Which answer?
Algebra is an associative algebra.
Language is not associative.
Language is not associative.
Well...
In fact, associative.
How language associates is the essence of the grammar of the language.
Well, this is...
I haven't mentioned any algebras yet, though, although that would be interesting.
Yeah, that's a great point.
Yeah, okay.
There is maybe a little representation theory, so maybe it'll come up later in the talk.
Let me not get ahead.
Okay, so...
But I just kind of threw a bunch of words at you.
So what does that mean?
So that's now what I'd like to describe.
That's what I'd like to describe.
I'd like to unwind everything I just said.
And it turns out...
So everything I just said is a little bit complicated and a little bit heavy-handed,
and so you might wonder, why in the world would you do this?
And what does it mean?
But it turns out the mathematics is actually quite simple.
It's quite...it's a little bit elementary, actually.
I mean, surprisingly and wonderfully, it's very easy to understand.
And then also it's a very good thing to do.
So there are advantages to doing that.
So there are advantages to taking something that's very classical, like frequency counts, on text,
and then passing over into this world of what's really quantum probability theory.
So that's now what I'd like to describe.
So here's my...I've just finished my introduction,
and now I'd like to describe how do you start with something very classical, like text and frequency counts,
or just a probability distribution on a finite set.
Then what is this passage over into this world of quantum probability?
And then what are the advantages of doing that?
So this is just mathematics that sort of stands on its own outside of language.
But then I want to take those ideas, those mathematical ideas,
and then circle back to my quest for understanding something about meaning and language,
and what words go with what, and how can I model that?
So let me now just describe for you a very nice passage from classical probability theory to quantum probability theory.
And then at the end, we'll tie all of that back into modeling language, using what I'm calling a tensor network.
I'm not going to say what that is just yet, but whenever you see tensor network,
you're more than welcome to think string diagram and it'll be fine.
So let me describe for you now a passage from classical probability theory to quantum probability theory.
So this is now section two.
And just let me just start off with some notation on the board.
So in what follows, let's let pi be a probability distribution on a finite set.
So s is a finite set.
And some of all of these numbers are going to be one.
So finite distribution on s.
So the first thing I'd like to do is to describe the quantum analog of this.
What's the so-called quantum analog or the quantum version of a probability distribution?
Well, this passage from classical to quantum probabilities is essentially a passage from sets into vector spaces or into linear algebra.
So well, to every finite set, there's the free vector space on that set.
In fact, let's do complex vector spaces.
So there's the passes, so just note or recall, there's a natural map from s into the free vector space on s.
So every element in my set corresponds to an independent basis vector here.
For notation, if little s is an element in capital S, what's the vector representation for that?
I've grown to like this physicist's cat notation.
I can't see it in head nodding, so that's good.
So what do I mean by this?
Well, if I choose an ordering on s, then say this is the ith element in my set,
then the corresponding vector for that, I just really mean the ith standard basis vector.
So this is 0 with a 1 in the ith place and 0 in some re-bells.
So it's a familiar object, but this is just notation that I like.
Okay, so then this is now a nice orthonormal set, and it's a Hilbert space with the obvious ith part.
Okay, so I'm starting my passage from sets into vector spaces.
I just take the free vector space on that set.
But then what is this quantum version of a probability distribution?
Well, it's going to be a particular linear operator on this space.
It's an operator which satisfies some properties, and it's called a density operator.
So I'll write it down, but a density operator is a map on this space,
which is self-adjoint, positive semi-definite, and has trace 1.
So I can write that here.
So, and I like to think that this is the quantum version of a probability distribution.
So I'll say a density operator, rho, on Cs.
It's a map that is self-adjoint, positive semi-definite, and has trace 1.
Okay, so I'm talking about string diagrams or tensor networks.
So if I were to draw this, I just mean a node with two edges.
So the edges are my vector space, and the node is the operator itself.
And that is trace 1, I mean, is sum over a common index, which is connecting the wires.
Okay, so why is this the quantum analog of a probability distribution?
Well, these three properties may cause you to think of non-negative, real non-negative eigenvalues that add to 1.
So that's like a good sanity check.
But there's also a very concrete way to see this.
Mainly, any density operator on the space Cs defines a classical probability distribution on the underlying set S.
So this is important.
So, note, every density defines a probability distribution on S by, what is it?
Let me actually, well, I'm going to use this notation.
The probability distribution induced by rho, all right, pi sub rho.
So what's the probability of a little element S in my set?
What you do is you take the vector associated to that element, apply rho, get a new vector,
and then take the inner product with the thing we started with.
So if, you know, if I look back at my ordering, what's the probability of my i-th element?
Well, it's really just the i-th diagonal of my density.
So this is what this is.
Okay, great.
So rho, you can think of as, I'll say, the quantum analog of a probability distribution.
So then you might wonder, what happens if I already have a distribution on the set S?
Can I define a density operator so that the probability distribution defined by it coincides with the one I started with?
I mean, that's a good question, and the answer is yes.
It's actually there.
I mean, in fact, I can show you two ways that you can do this.
So suppose I'm going to leave this up.
So suppose you already have a distribution on pi.
I mean, sorry, a distribution pi on S.
Here are, I'm going to show you two ways to define a density so that the distribution induced by it is the one that you started with.
In fact, you can already think of the first way, or maybe it's the second way.
You can think of a way.
So I have a finite probability distribution, and I want a linear operator that is self-adjoint, positive, semi-definite, and has trace one.
Yeah.
Just have the probabilities on the diagonal?
Definitely.
Just the most boring thing you could do is to create a diagonal operator from it.
So option number one is a diagonal operator.
In other words, I have this finite list.
I just stick only to diagonal, like you said.
So row, maybe I'll say row, diagonal.
This looks like, well, it's the probability of S1, probability S2, the derivative of probability, you know, maybe we have n elements.
Okay, n zeros elsewhere.
Then it's easy to check that those things are satisfied.
But also, what's the important thing?
Well, the important thing is that, is this equality satisfied?
Yeah, it's just the diagonal.
Okay.
So that's the easy way.
I don't really like that way because we haven't done anything.
We just, like, took a list and then made it tilt it a little bit.
So that's not doing anything.
So there's another way.
Well, that was fun.
So maybe you can think of the other way.
Yeah.
Oh, should I?
Like, maybe you could have an outer product with the square roots of the...
Yeah.
Okay.
Absolutely.
Yeah.
Well, wow.
How did you get...
How did you think of that?
Well, I'm not sure.
But this looks sort of like a maximal ring thing, you know?
So maybe you're like, oh, let me think of the extreme cases, maximal ring version.
Maybe there's a minimal ring version, like, projection onto a single thing.
But then you mentioned something about square roots.
I don't know.
Why square roots?
That seems like a very clever choice that may or may not have taken some thought.
Or maybe it just felt like the right thing.
Or I mean, like, because you want...
Because you want to maintain that...
You want the diagonal to be the same.
Yeah, yeah.
I want the diagonal to be the same.
So you take the outer product, you're going to be multiplying things twice.
Yeah.
So, all right.
Option number one is not as exciting.
Because the second thing, it's exactly as you described, I can...
Let me write this as projection on a unit vector.
So in other words, I'll write...
Let me say row is projection.
I'm going to call my unit vector...
I'm going to use it over again several times.
So let me...
I like psi.
So I'm going to call it psi.
So I'm using this notation, my ket notation.
This is just a linear combination of elements in S.
What combination is it?
So psi is a very particular vector.
It's a very specific one.
What is it?
Well, I'm going to sum over all elements in my set.
But I'm going to weight them by...
Not the probability, but the square root of the probability of that element.
And why is it...
Why is this square root here?
As opposed to not...
It's precisely because I want this to be satisfied.
Okay.
That's why.
All right.
So this is like a rank one thing.
You can think of it as sort of two extremes.
This is boring a little bit.
And this I think is much better.
It turns out you can get a lot of mileage this way.
And so for the rest of the talk, unless I remember to say otherwise,
when I say a density operator that's induced by some distribution,
I'm really thinking of this one.
This rank one, one.
Okay.
So why then?
Why is it that I like number two better?
Well, to see why, we should go back for a few minutes
to thinking about classical probability theory again.
So let's momentarily go back to classical probabilities.
So why is two better, better, you know,
according to me, not in like a universal sense,
but just why?
So what we want to do is this is number two more interesting.
So let's think now back to classical probability.
I started with a probability distribution on a finite set.
I told you the quantum version.
So I'm actually thinking I'm sort of creating for you a dictionary
from the world of classical probability theory to quantum probability.
So let me now add another entry in that dictionary.
So consider for a moment a joint distribution.
So suppose actually that, you know, our set S maybe is really a Cartesian product
of finite sets X and Y.
So if I have a joint distribution, then of course I can get a distribution
on either one of the factors by marginalizing.
So given a joint, I can get a marginal distribution, you know,
just to have it on the board where the probability of a little element,
I sort of sum over integrate out this complementary set.
Okay, of course we know this.
Why am I putting on the board?
Because this has a nice quantum analog.
So, you know, I'm sort of adding again to my dictionary,
how do I go from classical to quantum?
Well, we already know how to do this joint, you know,
we know how to translate from classical to quantum for a joint distribution.
I get a density operator.
How do I do it?
Well, I really like this one.
Now, just imagine each S is a tuple X comma Y.
So I just have the probability of my pair X comma Y.
And, well, what's the vector corresponding to that pair?
Well, it's just their tensor product.
So I sum all of these pairs weighted by the scores of their probability,
project onto that vector, that vector spans a line, you project onto it.
And so that's a perfectly fine density operator.
I do the diagonal one, but I like this one better.
So, you know, what's the quantum analog of this?
Again, it's rho, which is projection onto psi.
And just to be explicit, let me write out.
Okay, so this is a vector in the tensor product.
So actually, I know I'm running into my outline there,
but I do like remembering my pictures.
So, of course, when I write a vector in a tensor product,
I really mean something like a node with two edges.
So this is like CX and CY.
And then, so this is what psi looks like.
So then projection onto psi, I can think of that as,
well, it's really the outer product of psi with itself.
So it kind of has this nice picture.
Okay, so I have the quantum analog of a joint distribution.
It's just this density operator projection onto the sum of my elements
weighted by these probabilities.
So then what would be the quantum analog of marginalizing?
Well, this sum has a nice linear algebraic analog.
It's called the partial trace.
Okay, sorry that I'm sort of crowded here.
But the mark, the act of summing over has a nice analog
in the linear algebra of the partial trace.
And the thing that you get as a result,
or the analog of the marginal distribution,
is going to be, again, a density operator,
but it's a density operator on a smaller space.
So it'll be called a reduced density operator.
Okay, so let me actually back up and explain each of these things
a little bit slower.
So this is actually where things get really interesting.
So what really is the partial trace then?
Actually, I see...
So there's sort of two ways to think about the partial trace.
There's just the linear algebra definition,
and then there's the intuition.
So let me give you the intuition first.
So you imagine you have this large system
of a whole bunch of interacting components,
and suppose you know the state of this system.
Suppose you have a density operator.
Why am I referring to a density operator as a state?
I forgot to mention this.
So another word, another name for density operators
is quantum state.
So if you look at physics literature, quantum information theory literature,
density operators in quantum states,
these are interchangeable things.
So suppose I have this big system
of a whole bunch of things that are interacting with each other,
and suppose I know the state of this system.
But also suppose I'd like to sort of hone in
or zoom in on just one smaller piece of it,
or maybe, you know, a subset of these components.
I'd like to know what is the state of that smaller subsystem.
So the partial trace helps you do this.
But the key is it's not understanding this smaller subsystem
in isolation, but it's learning the state of that thing
given the fact that it has all of these interactions
with this larger environment.
So the partial trace is this operation that allows you
to go from large to small,
but given that that small thing is interacting
with this larger system in which it sits.
So that's sort of the intuition.
But what actually is it in math?
What's the linear algebra behind it?
So let me tell you now.
So consider for a moment, you know,
I'm talking about this larger system.
So this is really like a tensor product of Hilbert spaces.
So consider for a moment just this tensor product
that we have on the board already.
Now here's just an observation.
There are no natural linear maps down to each factor.
So you do have maps down,
but these sort of require twice.
You have to pick a basis, then pick an element,
then do something like project.
But there's nothing really natural that goes down.
But it turns out there are after you pass to
endomorphisms of these spaces.
So what I bring on the board is not true.
So let me fix it.
If I then pass to endomorphisms of these spaces,
then I do have natural maps down.
And these maps are called, these are exactly what the,
this is what the partial trace is.
So these are partial trace,
sorry for my handwriting, partial trace maps.
So what are these maps?
Suppose if I have an element in this space,
an operator on CX tensor and operator on CY,
how do I just get an operator on the first factor alone?
Well, sort of in the name.
Yeah.
Take the trace.
Sure.
You just, well, take the trace of G and then multiply it.
Voila.
And then the same thing here.
If I have tensor product FG, how do I get a map on Y alone?
Well, I already have a map on Y alone.
So then I'll just multiply by the trace of F.
Okay.
So these are partial trace maps.
What's nice now, so what if the operator that I start with
is one of these densities?
In particular, what if it's this one density
which is projection onto this vector side here?
Well, well, the answer is nice
and the answer is really the reason why I'm thinking
of partial trace as the quantum analog to marginalizing
and why the resulting things you get down here
are like the quantum analogs of these marginal distributions.
So let me just write what I just said.
If I start with, I guess to be consistent,
my operator is green.
So if I start with a density that operates on the tensor product,
I can apply the partial trace.
Actually, I can give this a name.
Let me call this tracing out Y and this is tracing out X.
So I can apply this partial trace map.
So if I trace out Y and I apply that to row,
I get, well, let me say row sub X.
So this is an operator on CX alone.
Or I can trace out X from this operator row.
I get another operator.
I'll call it row sub Y.
This operates on CY alone.
And these are what I'm referring to as reduced density.
So it turns out that the partial trace map preserves
the properties which I erase, self-adjoint, positive,
semi-definiteness, and trace.
And so what I get down here truly are density operators.
And I claim that these operators are the quantum analog
of marginal probability distributions.
So it's not, maybe it's not obvious why,
but it is, it's quite clear when you look at the matrix representation
of these operators with respect to the basis given by your sets.
So actually, let's do that.
You asked for a half an hour warning, but I forgot.
So you have 22 minutes.
Okay, thanks.
Okay, so what is, so let me just focus on this row sub X,
this density operator on the space CX.
What is this?
So as a matrix, you discover that when your density,
the one on top, the green one on top,
is orthogonal projection onto that vector,
what you discover is that on the diagonal of this operator,
you have exactly these marginal probabilities,
the classical ones.
So I've just shown you another way to compute marginal probability.
You can map over into the world of linear algebra,
project onto an ice vector, and then take this partial trace.
So on the diagonal, maybe X has m elements,
I get exactly the marginals.
And if that was it, you should be a little bit dissatisfied
because wow, why go through all of that work
just to compute something I already knew.
So here's what's so great is that in addition to this diagonal,
there are also nonzero off diagonal entries.
These nonzero off diagonal entries tell you something about
how elements in your set X relate to each other,
given sort of shared relationships in the set Y
that you integrated out.
So you actually have information about how things in X relate to each other
based, you know, dependent on or given their interactions
into this other system that you sort of turned a blind eye to.
And this is more than just intuition,
there's actually very concrete and sort of combinatorial way to understand this.
So let me tell you what is this ij entry.
So what is the ij entry of my reduced density operator?
For simplicity.
I'll give you a minute.
What is the original density operator that you're working with?
Yeah, the original one is projection.
Yeah, yeah, yeah.
It's ketsai with brosai.
Exactly, exactly.
Yep.
So when that is your original density,
the ij entry on the off diagonal has a nice combinatorial interpretation.
For simplicity, let's just for the moment imagine that
our original distribution pi is an empirical one,
so I'm really just counting things.
Then the ij entry is basically,
let me just say this in words, then I'll write it.
It's basically the number of share continuations in y
that xi and xj have.
So what do I mean by continuation?
If I think of a pair x comma y,
let me just, you know, we talked about language earlier,
so let me think of this as an expression like red fire truck.
So I'll think of, when I say continuation or suffix,
I'm thinking of oh, fire truck is a continuation of the word red.
So this ij off diagonal, I'll say it counts.
It's proportional to the number of shared continuations
in y between xi and xj.
In other words, I have this scenario,
think of xi as something like red fire truck
and what else are fire trucks?
Like fast or something.
Big.
Big.
But what else is big and red?
Clifford.
Clifford, yeah, exactly, exactly.
So y could be fire truck.
So that contributes to this entry.
Clifford is also, oh, I wrote fast, sorry.
Clifford is also big and red.
Clifford, so then that contributes to this off diagonal entry.
What else is big and red?
Like what?
A bonfire.
Okay, bonfire.
Okay, and so forth.
So for everything that's like fast blah, red blah, fast blah, red.
Yeah, born, okay, great, great, great, thank you, thanks.
Yeah, okay, so for each such word in y that is fast and big,
I mean red and big,
then that's contributing to this red big entry in my reduced density matrix.
Okay, so that's nice, why is that nice?
Because this is exactly, I mean this is information you do not have access to
when you compute marginal probability in the usual way.
So when you compute marginal probability,
in the classical way you sort of can only see this diagonal,
but there's all of this other information
and it exists on the off diagonal of this reduced density.
So then there's sort of two questions that come to mind now.
One is, is there a more principled way of sort of harnessing this information?
And then two, if there is, what can I do with it?
So let me answer those questions.
One, yes, there is a much more principled way of harnessing this
and I think you can see it already,
it's in the spectral information of this matrix.
If the density I started with was this boring diagonal one,
I don't know, I erase it I guess,
and if I start with a diagonal density and then I compute the partial trace,
I'm going to get diagonal operators again, so you haven't done anything.
But the fact that we started with this projection,
now, oh, let me slow down.
If this is a diagonal operator, I haven't done anything,
and we're over, it's eigenvectors correspond exactly to the elements in X,
so I really haven't done anything.
But now that I've projected onto this unit vector psi,
and I've computed the partial trace,
I look at these off diagonal entries, they're non-zero,
so the eigenvectors of this operator
are then going to be combinations of elements in X.
I kind of like to think of those as picking up concepts
that exist in the set X,
concepts determined by the statistics given to me by pi.
So if you look at the special decomposition of this,
this is telling me interesting statistical information
about this joint distribution,
which I don't really have access to when I marginalize in the usual way.
So that's interesting.
That is just the, you know, this is number two on my list.
That's just, you know, some very interesting mathematics.
So now what can I do with that?
So what can I do with that?
Now I'd like to circle back to this quest for modeling language.
So I did a race, and that's good.
So what was my goal?
My goal was to infer a probability distribution on text.
In other words, if you show me some examples of valid expressions
in your language, I can sort of get the statistics from that,
and then I want to infer then what is that distribution
so that I can then sample from it and then generate new data.
This is sort of the goal.
Okay, so how is this connected to that?
So in the minutes that remain...
Alright, then I'm going to describe this.
So this is now part three.
So to sort of, you know,
what's my starting point for this kind of language type model?
Well, I like to view language, you know,
expressions or like sequences of things from some alphabet.
So let me now start to think of language as a set of sequences.
And then I'm going to be interested in a probability distribution on that set.
So let's set it up the following way.
So I'll say suppose now for this sort of application part,
let S be a set of sequences.
Okay, I'm thinking of sentences or like sequences of words
or a word is a sequence of characters.
And for simplicity, let's think of this set containing sequences,
all of the same, just for simplicity for the moment.
So sequences of length N,
I'll say length capital N from the finite alphabet A.
So, I mean, an element in S is something like A1, A2.
So, you know, if A, for example, is the set 0, 1,
I'm looking at bit strings of length N, for instance.
Or you can take it to be, you know, the 26 letters of English alphabet or whatever you like.
Okay, then, then what?
The idea now is that on the set S is some probability distribution.
Now, I may not have access to it.
Maybe I don't know what it is, like the probability distribution on language, the English language.
But I'd like to learn, I'd like to get really close to it.
So, you can just imagine that pi is a distribution on S.
Maybe we don't know it, but we want to infer it.
In other words, I want to know what things go together in my language and what don't.
So how do you do this?
Well, you know, maybe I don't know pi because it's very, you know, maybe my language is huge and massive
and not manageable and there's no hope.
So what I should do is look at a small subset, just a subset of sequences.
Just take a collection of things and then that will be, that'll determine an empirical distribution
and now I have something more manageable.
So let's, so let T be a subset of my sequences.
So then, so if I take a subset that then determines an empirical distribution.
So let me write it this way.
So I have an empirical probability distribution.
Let me use pi hat, pi hat is on T.
So pi hat is like an approximation of pi.
So it's not quite pi, but I kind of want to get better.
So I have pi hat and I want to do something, I want to tweak it somehow
so that I can then get this sort of true distribution from which I can then sample and generate new examples.
So then what is that? What is this process? What is this treating process?
So this is exactly now where I'm going to take this interesting mathematics
and apply it to this concrete problem and then see what happens.
So I want to sort of model this pi hat and then apply some algorithm to it, which gets me closer to pi.
And the first starting place for that model is to exactly define this vector psi.
So, so, so what's a model for pi hat?
So I'm going to again define a vector psi, the same one.
I'm just going to take the sum of all sequences and, but the sum of all sequences only in my set T.
It's like my training set.
And then weight those sequences by the square root of these probabilities, these empirical probabilities.
Now, here's what's sort of the thing to notice.
Remember that S is a sequence of length N.
So what space is this psi in?
This psi is in CS, but this decomposes as the tensor product, one factor for each copy of my alphabet.
So, you know, if A is bit strings and I have length, you know, N of them, length in bit strings,
this is like C2, tensor C2, tensor a whole bunch of times, so it's a huge space.
Okay.
And, you know, just to be clear, when I say S, this is the tensor product of all of these individual vectors.
Are any of the words in the language or are they the positions of words in a sentence of fixed length?
They are actual words.
So this could be...
Because it looks like the A's that you're doing if you're a sentence is a fixed length N.
Yes.
And then the words can be anything that you stick in there.
Yes.
Yeah, yeah.
So this, you know, a sentence of length, I don't know, whatever.
Big red dog named Clifford lives in that space.
So that's five, I think.
Okay, nine five.
Okay.
So, just to, you know, I have tensor network stream diagrams on the board, so just to be clear,
what's the picture for this?
This is like a blob within many indices coming out of it.
Okay.
And why do I draw that picture well?
Because what I now want to do, I want somehow to sort of massage or tweak a little bit this big vector,
basically the sum of all of my samples with these nice weights.
I want to tweak this vector.
I want to do something to it so that I get a new vector in the same space
so that the probability distribution given by these weights is very, very close to the one that I started with,
which is pi.
So in other words, in pictures now, I'm going to sort of give a picture description of what happens next.
I'm going to start with this vector psi, and the idea is you want to do something,
which I'll explain in a second.
You sort of want to take the math ideas we've been talking about, do something, tweak this vector,
and get another vector, which I'm going to draw like this.
Okay.
And it turns out that this vector, which I'll describe in a second,
defines a probability distribution which is very close to the one you wanted to start with.
So the question is what's going on in this squiggly line?
So there's two ways to explain it.
One is it has a name, it's an algorithm from the physics literature, and maybe that won't mean much.
So then I'll just tell you, but then let me go back and say how it relates to what we've been talking about.
So there's an algorithm that is called the density matrix renormalization group procedure.
It has a long name.
It's an algorithm that's used in physics to find the ground state of the Hamiltonian of some quantum system.
So the Hamiltonian is an operator that tells you something about the energy levels of your system.
But instead of applying it to a ground state of such an operator, we're just applying it to the sum of our data.
All right.
So that has a name.
This is an algorithm that was introduced in the early 90s by Stephen White.
And the paper that introduced it has something like 5,000 to 6,000 citations.
So it's used by real physicists to do real physics quite frequently.
So it's an important idea.
But it turns out what this algorithm is really doing, it's computing these partial traces.
Oh yeah, I erased it.
It's looking at these partial trace operators.
And it's recognizing that these off-diagonal entries convey meaning about sort of local information together with information that's not in your system, that's sort of in the environment.
It then harnesses that information and then it fills this low-rank tensor approximation of the vector that you started with.
It turns out that this low-rank tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics.
So it's taking-
Is it low-rank or low-orn?
I mean low-rank.
Actually, let me explain what I mean with that.
And I can do that in the minutes remaining by describing kind of what is going on here.
Okay, great.
So what's the idea?
The idea is to do exactly what we did.
You start with this vector side.
Then you project onto it, as we said.
Then suppose you sort of work from- you go down the line.
So you sort of say, what's going on here are my sentences?
What's going on here?
What's going on here?
So what you end up doing is you trace out, for example, all but the first- let's just leave the first two open.
So if I apply this partial trace operation, I'm sort of summing away all of the possible suffixes after sites one and two.
So this is- well, it's just an operator from a tensor product to a tensor product.
So let me just save some chalk and just write it like this.
So what I mean here is my row x, my reduced density that I had the matrix representation here.
But earlier we saw this has interesting spectral information.
So if I do a spectral decomposition, I get, well, a unitary, a diagonal, and another unitary, or the same unitary transpose.
But because we started with this projection operator, the eigenvectors are containing this information that you didn't have access to classically.
And as these eigenvectors, they're being contributed- they're taking contributions from these off-diagonal entries,
to tell you something about how things here relate to each other based on shared continuations in your language.
So what you can do, you can just sort of take off this tensor and then stick it as one of these.
So, you know, maybe this might be this one, for instance.
So, you know, these two legs are right here, and then this third leg is this one over here.
So when I say low rank, I mean, I mean, maybe I'm working in a very, very large space, and so I kind of want to, you know, not deal with all of that.
Maybe there's a lot of noise and I might get rid of that noise.
So what you can do is just take maybe the first few largest eigenvectors.
So I'm sort of compressing, and so now I've got this low rank operator and I stick it here, yeah.
I'm wondering about the picture right there, where you're tracing out a lot of stuff.
Yes.
Would that correspond to finding shared continuations, where the continuations themselves are like all the words except for two?
Yeah, totally.
Okay, but if so, like for practically sized datasets, you would almost never have that, right?
Never.
Never have, like, or actually, yeah, yeah, you would almost never have two sentences exactly done, except for two words, or is that not the case?
Ah, that's right.
Okay, so what I'm describing for you, ah, so let me say two things.
Yes?
Yes.
So, actually, this algorithm that I'm describing, this is an experiment.
So I'm describing work with John Torella, who's here from CUNY, and Miles Student Meyer, who is from the Flatiron Institute.
So we trained this on bit strings of a certain length.
So it turns out you do get interesting information there, but if you sort of want to bump up to something that's, like, more real-worldish,
then you can do things that are, like, other architecture choices, and you can try to tweak this model to handle things like that.
So I'm describing for you sort of like a nice theoretical, but if you want to handle, you know, you want to make it a little more robust,
then there are definitely things you can do.
But these are, like, different architecture choices for the model.
Yeah.
Um, so, ah, okay.
So I described for you how to get this first tensor, but you sort of, just like an iterative thing, you sort of keep on going down the line and doing this.
But the punchline is, is that at the end, this model, which by the way, it has a name that's called a matrix product state, product state.
Um, this is a particular type of tensor network or, you know, we look at it and maybe say stream diagram.
Um, and in the end, you learn very well the distribution you wanted to start with.
Um, and so the model sort of knows what words go together before meaningful expressions just based on the statistics that you started with.
And that was the goal.
Yeah.
What was I going to reference?
Yes, yes, thank you for asking.
Yeah.
Yes.
So this is a paper, um, modeling sequences with quantum states.
Um, this is with my, myself, ah, and my old student Meyer, and John Taylor.
Yes.
Hi.
Thank you so much for this amazing talk.
I wanted to pick up on one of your last couple of words, meaningful expression.
Yes.
I wanted to comment at all about, you know, how this is stimulated by and also can illuminate notions of semantics and pragmatics in language, because, you know, culture and, you know, extractions of emails reflect a certain collection of what's meaningful.
What is meaningful varies from state to state time.
I mean, New Jersey, you know, Massachusetts.
So if you wanted to get a grasp on meaning, it would seem like this is an approach.
I mean, the whole thing about meaning is what's so powerful about this.
Could you comment more about this?
Yes.
Yes.
So, um, one looks at this and says, ah, there must be a good definition of meaning underlying all of this.
Yes.
And I think a good candidate for meaning is closely related to this distributional idea I mentioned earlier, which I like it a lot because you see it all the time in mathematics, you know, thanks to Category Theory and the Oneida Lima.
You want to understand an object, you know, it's enough to look at its network of relationships that it shares.
Yes.
And then you might think, oh, okay, so then the meaning of a word like fire truck should be maybe if we're taking that direction, something like the totality of all expressions in which it sits.
But then not quite because you need something about the statistics as well.
So then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it sits, sort of how it fits into your language, but also the statistics that go along with that.
The probabilities of that.
That's sort of what's going on here.
I'm very open to other ideas as well.
Yeah.
Yeah.
If one has a good definition of meaning, you should be able to do a lot of things with it.
And so I'd be interested in a very good definition.
A meaning of meaning.
Yeah.
I would like to know that answer.
Yeah.
I was wondering what you were doing with T here, how you actually used that it was T instead of S.
Yeah.
So if I empirically just mean a sample set that could have mapped to S just as well as to T, right?
So I mean take a subset and then that, like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number for the cardinality.
But if a subset could only appear once, I guess.
Oh, okay.
So maybe.
So it's just some map.
T is mapping to S.
Yeah.
Okay, great.
Yeah.
Okay, thanks.
I'm wondering how this compares to hidden Markov models.
Ah.
Because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states in some sense.
Yeah.
So.
Because those are very popular to model language.
Yes, yes, yes.
Yes.
So I think there's a way to see that something like this is a little bit like the quantum version of a hidden Markov model maybe.
Okay.
But I think maybe, so the thing I like to think about, well, so one could make the case that language is not Markovian.
Right.
And that there are these sort of long range correlations that you'd like your model to know about.
And so tensor networks, since they come from the world of physics and quantum physics are, well, certain ones are very good at capturing these long range correlations.
And so that's something, you know, you'd like that to be built into your model.
And so what I've described for you now is a matrix product state, which sort of can be thought of as being a good approximation to a network which can, you know, recognize that language doesn't have as much of a coding.
All right.
Thank you.
Yeah.
An idea of meaning sequence to sequence.
Yes.
In translation of language.
Can you comment on that?
Or you might have two sequences and they're translating.
I don't have well-developed thoughts on that to say.
Yeah.
Nothing concrete.
We should stop here and then break and then people can go with full stick around.
You'll stick around for like a minute.
Yeah, for a little bit.
Okay.
