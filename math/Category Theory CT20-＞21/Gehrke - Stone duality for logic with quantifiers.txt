Now? Yes. Yes, so I shouldn't thank the organizers too much because I now start crying. I'm
so happy to actually be in a room with my sister. And I really am going to start crying.
And to be able to write on a blackboard. So excuse me, but I will not use slides.
So it will be messy and I will not be able to say so much, but I am going to enjoy so
much writing on a blackboard. Okay, so what I want to talk about is what
that call algebraic slash categorical math. Well, maybe I should say and duality in logic.
And so this is what I want to talk about methods. And the point is that if you look in propositional
logic, and this has been very powerful and very useful and has been very much used. So
at the basis what it provides for you is semantics. And this is of course very good, but also
it's really useful as a technical tool. Okay, for proving lots of theorems. Now, in first
on the logic, if you try to do it just really algebraic with cylindrical to those are something
it's okay, it kind of works. But if you look at it with category theory, then it's beautiful,
right, I'm sure you will agree with that. So, so that's why I'm saying slash. Okay, just
so we belong in the same place because I'm probably more of an algebraic, not a category
theories, but this way we have a common ground. And this gives very beautiful semantics. But
I would put a question mark to this so far. Maybe I'm wrong. But, you know, I'm talking
about things like proving interpolation, proving admissibility of deduction rules, being able
to compare different logics in, in, in a family of logics, like modal logics with different
axioms or intermediate logics between intuitionistic and classical logic, right, these are
studied very much at the propositional level and their algebraic semantics and duality
are very, very, very useful in the form, for example, of cryptosemantics and so on. But
as far as I know, this is not working here. And in fact, in, in first order logic, there
are lots of things that are not known. Maxim over proved that linear hiding algebras have
interpolation. Okay, but the corresponding predicate logic, the answer, the question is
still open. Okay, so if there was some good categorical semantics, which could actually do
the same kind of job, then we could know that. So this is, this is what I'm searching for.
And my idea is, I think what happens, if you say, ah, now I want to do this in a more general
setting, then you make a very beautiful general theory, but you don't necessarily understand
what is working about it. If you want to bottom up from some little thing that you know what
you want, then you're more likely to find something. Okay, so my idea is to approach this
rather in a bottom up kind of way, looking at very special situations of finding or quantifiers,
where we've seen the duality theory is actually really useful. Okay. And I want to talk in particular
about two such cases. And they're both in theoretical computer science. So one is domain
theory, and the other one is automatic theory. And I can't help myself, but in the last talk,
we had two categories, the opposite to each other, and that were both monadic over set.
And this is very strange, I completely agree. And I think Peter said, this is, this is just an
incredible coincidence. And that may very well be so. But in fact, in all these cases that I'm
talking about, I'm not going to prove this, but I think very fundamentally, they have something
like that happening. Okay, so whenever that happens, it may be special, but it's very magical. Okay,
something very nice happens. So it's maybe interesting after all. But anyway, so let me
start by talking about, about domain theory. So
domain theory, this is part of what is known as, as the notational semantics. And maybe I should
even say I want this not domain theory, but a very special problem, which is both solutions of
domain equations. And this is these are things of the form I want. Some object, which is a fixed point
for a functor. And what kind of functor? Well, a functor that's a composition of a bunch of basic
constructors, which correspond to things in programming languages. Okay. So, so the most
basic one is I want x to be equal to its function space. Okay. This you may know about it was solved
by Dana Scott in the 60s, long time after the lambda calculus, which the lambda is a binder,
right. And the point is you have an object lambda calculus, where every element is an input to every
function. And every element is the function. Okay. And so you have exactly this. And then for the
longest time, they could not find a mathematical object like that, that made sense. So this is what
Dana Scott did. And how did he do it? Well, as an inverse limit of a sequence of finite
process. So what's that? That's a pro finite set. And that's, that's a stone space. Okay. I mean,
sorry, not stone space in the Boolean sense, but in the distributed level sense or spectral space.
Call it that. So, so this, I would say was started by Scott, but there are lots of other names,
strategy, Smith. That can, I want to get signed up to get to Abramsky.
Okay. Because through all this, it was kind of clear that the duality we played some role.
But what Abramsky did is he actually said, look, this is entirely happening because of duality.
Okay. And so I want to explain this to you. So he's essentially saying, so that so I want to
explain this work of Abramsky's, which paper in 91 was called domain theory, logical form,
the logical form, meaning the logic is the algebra, which is dual, which is a distributed
lattice. Okay. So his idea is, well, let's say suppose X is a spectral space.
Well, then what, then we can just describe
dual of this constructor on spectral spaces. Okay. And then most likely, if it's a nice
algebraic thing, it'll be finite Terry. And then what in finite Terry algebras, we have lots of
fixed points, right? I mean, we have at least fixed point just starting from the empty set,
right? So if we take the free algebra on the empty set for the dual thing, so in other words,
this means we take the code limit of f n of the empty set, where f is the dual described,
the dual of this, let's call it f. And this is a fixed point.
But of course, the dual of a fixed point is a fixed point, right? So then we found our thing.
And this is the point that a topological category having something that is
a fixed point, this is much more complicated, right? But in an algebraic one, it's very easy.
This is exactly what duality works for. I mean, this is what it's right useful.
Okay, so and in fact, sometimes this one's trivial. So then that's kind of a problem,
like in this case. So then you might want to just start with more generally.
If you have some objects, some lattice, which fits inside f of it,
right, then you can iterate and you still get a fixed point.
Am I saying some nonsense? Can stop me, but anyway.
Right. So how does this work? So let me just show you how it works in this case.
So we have a first approximation,
which kind of rather comes from something we know from modal logic, actually,
which is that if you look at, well, that the set of all continuous functions from x
to up a viatoris of y. Okay, so, you know, my course talk was perfect because it introduced
everything I want. Okay, so, so I don't have to tell you any of this, right? He actually
told us about lower viatoris, but that kind of gives the meat preserving part of
eggly metal, right? And this gives the joint. So no, the other way around. Well, whatever,
it's the other half. So, so this is what I'm doing. So the point is that if you take in
general two spectral spaces, and you look at all the continuous function from one to the other,
this is not a spectral space. Okay, but if you look at all the continuous functions from x
into upper viatoris of y, then it is a spectral space.
Well, x and y are spectral.
And why is that? Well, because it's the dual of freely adding an implication type
diamond or box. Well, so what do I mean by that? Well, if I have two lattices L and M,
I look at the three distributed lattice over all
symbols. And so all pairs, one from L, one from M, I'm going to denote these pairs as A
in class B. I just mean it's like a symbol, right? And I generated that distributed lattice freely
on that. And then I model out by that I want that this operation ends up having the property that
the join of a finite set of things
in size B, this is equal to the need.
And A
is the need.
Okay, does this make sense to everybody? So this object that I get here will not
have an implication operation on it, right? It corresponds to like getting one freely
by the theory and induction, right? You start with the elements of your lattice,
then you take all single implications, then you should generate a lattice,
then do this operation again and keep doing this countably many times. And then the co-limit will
be your free implication. I mean, this is not implication in the sense of being adjoined,
but only implication type in the sense of having the correct join need behavior of implication.
Okay, so this one step of adding an implication type box, if that makes sense to you,
this is dual to this construction on the space. This is an easy duality exercise.
Okay, but it is kind of fun in that notice that the base is the natural, I didn't say here,
but I mean in the compact open topology. So what does that mean? I take all sets of functions
that map a certain compact set from here, entirely inside some open there, right? Then what are the
opens by a torus? They are unions of like some kind of box Bs, right? So like saying that
your open falls inside B hat. So actually the basic opens there of this form,
all functions so that for each X in A hat, there exists a B so that F of A hat,
F of X is contained in B hat, something like that. And this finite union is necessary.
And in fact, these sets will not become packed in general.
So these are not compact in this function space. So part of proving this here, this duality is to
prove that you can do away with this and you still have a basis. Okay, so these much simpler sets
form a basis. So that's a crucial lemma.
Is the compact open from the first space
is mapped inside box B from the second one form a compact open basis or the compact open topology.
Sorry. In the one case is the set that's compact open and the other it's compact open topology,
right? So that's very confusing, but there's not much I can do about that.
So now the interesting thing is that
a second step.
Like I said,
X, Y does sit inside this guy as a subspace,
but in general not as a closed subspace.
And therefore this is and this is in fact not compact, like I said, and so it's not a spectral
space. But so the second step is to realize, well, so the in fact the
duality theorem is the following. I can look at this F, let me just shorten it F arrow.
If you look at a quotient of that, because what a subspace here, this would correspond
to a quotient of the lattice, right? So I can ask which quotient of the lattice
get me entirely inside the function space, okay? So I can say such a quotient as dual
inside X, Y, if and only if data satisfies
what I have called, I made this up myself, so it's probably a bad name, but
data preserves
coins at primes, at prime filters.
So this says that every time you take a point of X of the dual, so this is like a prime filter.
Of L, and any time you take a point which is in that prime filter, and every time you
take a finite subset of M of the other lattice,
then you can go down inside the filter far enough that then there exists a point so that
A implies
join G mod theta
is equal to the join of A implies B, such that B is in M, G mod theta, or A prime.
Okay, so like at the limit, at the prime filter, actually your implication is weird,
it preserves joins in the second coordinate, okay? This is a equivalent to falling inside the
inside the function space, but in general, you know, there's no best such quotients,
no biggest one, but as a corollary, if L, if the first lattice has enough join irreducibles
or join primes,
then, then you can say this in a generator's and relation way, because you can just mark out
pi, okay, theta will be generated by
P implies join G equal,
join of P implies B for B and G, you know, so these things where P are the join primes.
And then you get exactly the function space, then F arrow mod this theta is the function space.
And by the way, having enough join primes, this is saying exactly that the dual space is what is
called an algebraic domain, which is what Scott invented with the Scott topology, okay? So you
fall bang on that when you just ask this question. You don't have to know about Scott's work, you
don't have to be a genius like Dana Scott, you just do a little duality theory and it comes
out in your face whether you like it or not, okay? So, so I would say here duality theory is very
useful, okay? I probably have talked already infinitely too long, right? But how much time
okay, good. Because this is what I really wanted to talk about. Okay, so this is what I always do
to myself. First, I talk about the thing I don't want to talk about so much, then I have to rush
like a maniac to also talk about what I want to talk about. So the other case is algebraic
automatic theory. And I don't know how much I have to say for this to make sense.
So the basic object you study, I want to call regular languages over an alphabet, okay? So
let's just say a is just a finite set called an alphabet and regular languages
over a.
This is a Boolean subalgebra of the power set of a star.
And what is kind of magical there also is that there are many different ways of saying what
those are. Okay, so first it was said by Kleeney. These are exactly the subsets given by regular
expressions. Okay, so if you don't know what it is, too bad. And
there are also the subsets which are recognized
by finite state automata.
And if you don't know what that is, it's also too bad.
There are also the subsets, the sets of models
of sentences
in the case
Okay, and so this is very beautiful that all this agrees. But since I'm talking about logic,
I'm just going to say a tiny bit about this one. Okay.
So the idea is that when you have a word
in terms of Scandinavian, let's look at the alphabet. And you actually view this word
as a structure for logic. So this what is what is the structure? It's one, two, three, four. It's just
this. And we look at it as having an order, namely the natural order, the usual order of
natural numbers. And it has a unary predicate for each letter. And it's just the places where
that letter occurs. Okay, so it's one and four. And for B, it's a two and three.
Okay. And now, now if you understand everything about that, right?
And now the point is that, okay, so what is the logic? So it just has this symbol
and the predicate symbol, the unary predicate for each letter. And then you're allowed to do
monadic second order logic. Okay, so you're allowed to quantify over
sets, sets of positions.
And you're allowed to quantify over positions. Okay.
So for example, you can say, there exists a position in which there's an A.
And this is the models of that. So this we call the language given by this
sentence. This would be what, as a regular expression is given by this regular expression.
Somewhere there's an A. Okay, but you can also say something more complicated using the monadic
part, which you can say, for example, that you have the even, just even length words.
So what would that be? That means you'd like to take two words, I mean, two letters next to each
other. Right, that's kind of a short description of even length words. And this you can actually
say with a formula. So maybe I don't spend the time on it. So if you don't like my talk,
you can think about that. But you have to use the monadic part to do it.
Okay, so those are regular languages and so what?
So computer scientists, they study this class of languages because they correspond to automata.
And in fact, also they want to understand how complicated the automaton has to be.
And there can be many different automatas giving the same language that is computing the same thing.
And you just care about how complicated is the thing you want to compute.
Right, so exactly you want to understand how complicated is the regular language.
And so you want to look at subclasses that are simpler. Okay, and that means you want to understand
subboolian algebras of this boolean algebra. Okay, and very often those are given by fragments
of this logic. So what's the most natural fragment to look at if you're looking at monadic second
order? First order, right? So you might ask, ah, what are the ones given by first order sentences?
And indeed, this is also from the regular setting, very, very natural. It's the star-free languages
they're called, where you don't need to use the cleanly star, but instead you're allowed to use
compliments. You take boolean algebra closed under concatenation of two things. Okay, that's
called star-free languages. And that corresponds to the first order
fragment. And now you might, especially if you're computer scientist, then you might want to ask,
well, can I check whether something belongs to that or not? If I give you an MSO formula,
can you tell me whether it's equivalent to a first order formula? If I give you
a regular expression, can you tell me whether I can rewrite it without the star?
So there, for example, if you are further bored with my talk, you can show that
the even languages is not star-free.
But for example, A, B star, that is, you can repeat A, B however many times you want.
That's a bunch of even-length words. Okay, but every other one's a B.
This is star-free.
So it's not so easy. And in fact, the only way that is known, but star-free, is this decidable?
Is this subclass decidable?
This is the kind of question they ask. And in this case, the answer is yes.
And it's very non-trivial. And the only way to do it is using duality. Isn't that wonderful?
So this was done by Chitzenberg in 1965. And of course, he didn't know he was using duality theory,
but he was. So star-free-ness is decidable.
And the way he proved that, okay, and so since then, lots of questions of this kind
are being answered and are still open. And every time, the proofs are very similar to this one.
So this kind of blueprint for how it works. And it is a kind of duality story if you look at it
from my point of view anyway. So I want to tell you how it works in some general lines.
So the very first thing is a very old thing also by Robin and Scott.
And I think my hill and road were also involved more or less at the same time. I mean,
it's a very natural thing. So this is in the 60s that L is regular if and only if.
What I call BL, which is the
Boolean algebra generated when, maybe I should say that first. So the point is,
as you definitely know, one way to look at a monoid is as actions of the monoid, right? And in particular,
any monoid by acts on itself, okay, by actions. So I mean, it acts on the right and it acts on the
left. And those two actions are compatible. Right. And because of that, by duality,
a star acts on the power set, of course, by inverse image, right? Just by duality.
So like, normally you write U inverse L, V inverse, this is the language of all
words, so that U, W, V belongs to the language, right? That's the dual action.
So this action is central to all this business. Okay. And in fact, we want to close under this
and invariant under the action. So we close under this division, action, and then generate a Boolean
algebra. And if this thing is finite, then the language is regular and if it's not finite,
it's not regular. Okay. So it's regular if and only if this is finite. So see, you could add
another thing there and it's maybe your favorite, right? And of course, one can just do the dual
thing. And this says that the dual of this is finite. And the dual of this is actually,
this is what is called a syntactic monoid
of the language. In other words, you just look at
the least monoid quotient of a star, which saturates L. Okay, you want L to be entirely inside one of
the equivalence classes. The least monoid quotient that does that, that's the dual of that
Boolean algebra. And that's what tells you whether it's regular or not. So who actually
L is regular if and only if it's saturated by a finite quotient of a star. Okay.
So now, a consequence of this, a fairly immediate consequence once you look at it like this is that
so this is kind of where I came into the picture with Grigoryev
and Frantz in 2008. We use this to study all this stuff with duality theory and kind of a basic
so the one basic observation is that this syntactic monoid of a single language, that's
a monoid. I mean, that's a dual space. But more generally than the dual of the regular languages
is obviously the pro free profile monoid. Right, because you just take inverse limit
of all the finite quotients and so that's the dual of Grig A star is what they call A star hat,
the free profile finite monoid. Okay.
How am I doing for time now? Okay, so I will skip a part now because now I wanted to tell you
you in more detail how Schützenberger's proof goes. Okay. And the point is essentially
that you look at the dual, you look at these monoids. Okay. So in fact, star free what he showed
is that corresponds to a periodic pro finite monoid. So a periodic means they don't contain
any non trivial poops. Okay, only the so if you iterate an element, you always just get
a single point, you don't get a Z mod n ever. And so in fact, the dual of the of the Boolean
algebra of star free language is the free pro finite a periodic monoid. And this is this is what
he showed essentially. And how did he show it? He showed it by induction on the complexity of
the formula or doing concatenation product is closely related. Okay, so you start with
with atomic formulas. And then you show that maybe I should at least say this.
Where did that put the eraser?
If I have a language that's given by a formula in a free variable now a first order formula
and say it is recognized by that is it's, it's saturated by
some quotient into a profile at one of the x.
Now, why am I saying this thing? It's because one trick they make is to
to if you have a word
with free variables. Okay, then you have an interpretation say x is located a position two.
This you encode in an alphabet by putting zeros in the places where it's not one where it is.
Okay, so this is a rich alphabet, and it includes the interpretations. Okay,
so this way I can actually talk about models with pointed models, but just as being models in a richer
in a richer alphabet. Okay, so this is already a very interesting thing because the context
of a hyper doctrine here I interchange with predicates that are richer. Okay, you take
more predicates and you can encode these things. Okay, so let's say this is encoded there. Then
once you add a layer of the quantifier, then this is recognized by
I
by a morphism that you can build from that one. So it kind of does the
the step of the Nuterian induction and quantifier depth. Okay, and what is this object? This is
actually a semi-direct product.
No, a viatoris of x with, I don't know which order I want to put them in, but it's a semi-direct
product of viatoris of x with x, where the multiplication is as follows. You have a closed set
and a point and another one and y. Then how do you multiply? Well, you have a funny multiplication
here which shows that you do v times y union x times w and here you do x y. Okay,
so you have a semi-direct product that exactly gives you the next step
in adding quantifier depth. Okay, and then you use that to prove that that gives you
one direction and then you have to also prove a kind of completeness that if you have an
aperiodic monoid you can decompose it along something like this. So that's the difficult part
but so I don't tell you anymore about that proof. But I think, I hope now, can you see
anything in common to these two stories? Well, well, both use duality, but both use very much
this kind of Nuterian induction of adding one layer of an operation, right?
And what I would say, if you look at f, all formulas of logic,
then you can decompose it in two different ways. You could say, okay, let's look at atomic formulas.
Okay, and then f1, all the ones with quantifier depth less than equal to one and so on. Okay,
and then in the end, you get the full thing. Or if you're looking at it as a hyper doctrine,
so this is kind of the Nuterian induction way of looking at it. Or you can say, oh,
I don't care how deep my quantifiers are, I care how many free variables I have.
Right, so I look at all the formulas having no free variables. I look at all the formulas having
at most one free variable. And so on.
And then you get the Leverian way of looking at it, right, or the hyper doctrine.
And so what I'm seeing in these cases is that what's actually powerful for proving things
is this, which is kind of orthogonal to this. I mean, I can't make this precise because I'm not
category theories. But to me, this feels like a co inductive point of view, right? Because what are
we interested in? In the end, we want to understand the sentences. While the sentences sit inside
every layer of this thing, they're already present everywhere in order to be able to talk about the
junction. But they are only fully understood once you understand all these more complicated things,
right, which contain it already. So it's kind of like the intersection of all the things we're
looking at was here is the union of all the things we're looking at. Okay, so we build it little by
little. Okay, so
let me just end with we started talking about this. And so he wanted very much to understand this
automata theory from the point of view of hyper doctrine. And in fact, the very talented student
of mine, Jeremy Marquez, he just started working this past year. And he's already proved a million
things and much beyond what I'm going to say here. And he has a paper on comics this year that's going
to be in November. Based on these internet conversations with André Jolyal and I, okay,
and he's looked at this a periodic monoid case, the star free, that is the first order fragment
of bitch logic from the point of view of hyper doctrines. Okay, and let me at least say the
punchline relative to that because I might be running out of time. So this is this is like an
infinitesimal piece of this work. Okay, so sorry. But is that in fact, the hyper doctrine which
corresponds to corresponding to echo of the key logic
is given by
a fairly trivial function which just
takes for
for each, well, so you should look at it as going from
finite total orders
to Boolean spaces
and for finite total order n,
we just send it to the monoid to the n. So this is here m is a pro finite monoid.
Then the presentation of the monoid is just this, right? In category theory, you look at it like
that, right? Zero corresponds to, no, sorry, the function that sends zero to one.
This will correspond to the identity element
in the n, the unique
projection from two to one. This is exactly the multiplication
and so on, right? So this is just the presentation of the monoid. Well, that presentation of the
monoid, this is actually what Joyel calls the sterling kernel of the pulleadic space to work
to the hyper doctrine, okay? So you take hyper doctrine point wise, you can dualize it,
you get what he calls the pulleadic space. Now, you don't really need the whole pulleadic space.
In other words, you don't need finite sets here. You can just look at interpreting each
variables distinctly and if you know you have a partial order, you can even do it with a
total order on your theory, then you can do it like so. Maybe about a few minutes,
I will say a little more about that, but let me just for now say that this is a very trivial
function, right? And essentially, this is the rule of the hyper
doctrine corresponding to this logic, okay? So essentially, to get the n types
of this logic, you just take the zero types, which is this free
aperiodic, this free, yeah, so in the case of bulky logic, this m is the free
aprofinite aperiodic monoid.
And if you present it, this is all there is. So it's kind of not telling you anything,
you know, the fact that you have all these things,
they all just act trivially like these powers of the monoid, okay? And really the only one
that is something going on is this one, which is the monoid, okay? And like this one is,
you know, when you have n variables, then it's just essentially given by mn, m to the n,
to only add in some kernel sense, but anyway,
whereas to understand how to move between layers here, that is this very
interesting, or now I erased it,
semi-direct product, right? So yes, I'm out of time. So sorry about that. But really to you,
what I wanted to say mainly is that I wish you would try to make some nice mathematics out of
this, okay, or some nice category here, a theory out of this decoupage of the formulas,
rather than justice. I mean, this is important, but this is very useful technically. Okay, thank you.
Thank you very much for a nice talk. Before I ask the questions, could I just ask people not
to rush away at the end of the question session, because we're going to have a group photograph
before lunch. Okay, now questions. Thank you very much for a very nice talk. I wanted to say that
this has been partially done, if I understand correctly, by Mark Kamsma, and the title of the
paper is Types, Space, Functors, and Interpretation of Positive Logic. And what he does, if I understand
what you say, is precisely instead of studying an hyper doctrine, which is a kind of pre-sheep into
falsets, is that this pre-sheeps into spaces, which precisely, if I understand, correspond to
space types for everything. But you know, this is not really, I mean, this is, yes, so I think
already in 70s, Joayal came up with this idea of polyallic space. Okay, and that's exactly
point wise dual of hyper doctrines, and understanding the hyper doctrine like that. Okay, and in fact,
in this stuff that I'm saying here in a very haphazard way, I'm already using that. And this is not
the issue. Okay, the point is that when you do that to this case, you essentially just get this very
trivial factor, which doesn't help you to understand M, right? Well, I mean, if you have the multiplication
of M, you understand M, okay, fine, but you don't get anywhere, right? So this is, here it's not the
dualizing of the hyper doctrine that I'm asking for. I'm asking for coming up the formula algebra
in a different way than what a hyper doctrine would do. This is, here duality theory doesn't
have anything to do with that, in a way, right? I'm not sure whether what comes,
Ma does is to point wise to allies. That's why I would address you to, at least check it out.
Okay, yes, I definitely will. Yes. But just from what you said, I couldn't hear, but yes.
Are there any more questions? Anyone online wanting to hand up?
Thank you for the talk. The answer to this question could be just go and read the paper,
but I was wondering if you'd put any thought into, like you said, with the monoid,
you only need to understand the multiplication in order to get all of the data. I thought
whether you or your student had put any thought into how you might reconstruct this,
as you say, trivial functor just from like a truncation of it to the multiplication.
Say it the last part again, because I didn't understand.
So you have this functor which corresponds to the hyper doctrine.
Yes. But the whole monoid structure is only contained in the multiplication map.
So I have any idea how you might reconstruct the whole functor just from this smaller?
Yes, yes, yes. So that's why I'm really sorry I didn't give more.
I should have spent less time on the other stuff. But on the other hand, really,
Jeremy should get to talk about this sometime and not me, because it just works.
So maybe it's good I didn't get to it, but yes, this is what he very much does.
Okay. And this actually gives some interesting model theoretic information.
So I'm kind of downplaying the importance of this. Okay. Because yes, indeed, the fact that this
is this simple that it's obviously somehow zero types easily gives the end types.
That's very special. Right. And that actually shows that your space of models have
a concept of concatenation on them, which is very powerful and very strange.
Okay. And this explains this. So this is very much part of that work. Okay. So you're completely
right. And this is in there. And yes, I have to tell you to just bring the paper because I didn't
have time. But this is very interesting. And I'm not saying that there's nothing in this.
Okay. In fact, Jeremy does a whole bunch of super exciting things from it. But I'm just saying
that relative to, say, proving something like this side ability or whatever. Okay.
This in itself doesn't get you anywhere. Whereas if you have an Ethereum decomposition,
then this is really useful. Okay. So this is just why I'm just clear. Hey, I'm not at all
wanting to say this is not useful. I'm just wanting to say that I'm observing that in these two cases
where duality theory actually was useful. It's rather using that point of view.
And if you guys know about that, then I have hope that somebody will do something about it in the
future. Thank you. Any questions? Hello. Thank you for the talk. I just wanted to ask, like,
very maybe it's off topic. I don't know. But you started with this example with automata from
from there you developed. So my question is, what happens if you change automata for something else,
like you go up to the Chomsky hierarchy or use patronage or whatever that does everything explode
or you consume or I don't know. So everything does not explode. So there's some things that can be
done. And in fact, much of what I have been spending the last five years on is trying to go to
not to the Chomsky hierarchy, but rather to what is called Boolean circuit
languages, because this is very important in complexity theory, right. And there,
they don't have finite monoids. But you have what Daniela and Luca Reggio and I call the Boolean
spaces with an internal monoid, which play pretty much the same role and which comes from the same
story. And which may allow you to do a similar thing and get the side ability of some of all these
low level complexity languages like AC0 and then C1 and so on. So that I mean, that's what I thought
if anybody else outside of my or our little world with mathematics is going to listen to us, I have
to solve a complexity problem or something like that, right. So this is what we concentrated on.
But I think there's, you know, Chomsky hierarchy, well, I mean, Chomsky hierarchy specifically
are not even lattice, right, because they're only killed on the union of intersection or something
like that. But you can also deal with that. But anyway, there are many, many cases that would be
very interesting to study and just very few people who are both interested in that and know enough
mathematics to be able to do it. So, so there's a whole ton of stuff to do. Okay, thank you.
By the way, let me make a comment. I'm really so excited to see so many young people.
You guys are so young. Well, maybe I just got very old.
If there are no other questions, anyone online?
Okay, let's just thank my...
