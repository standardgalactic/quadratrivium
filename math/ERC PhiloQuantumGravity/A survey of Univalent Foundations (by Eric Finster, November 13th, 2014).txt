ლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლ�
Okay, so we're back on the schedule.
So it's a pleasure to introduce Eric Finster,
who has a background in algebraic topology
and is now interested in the application of homotopy theory in logic,
and is going to introduce us to homotopy type theory,
and the links between logic and homotopy theory.
Excellent.
Thanks very much to Matthew and Gabrielle for inviting me.
It's a pleasure to be here.
So my talk...
Yeah, it might be a little bit...
I'm not sure how it's going to fall in this audience.
I guess we have a sort of big mix of people, which is great.
So some people I think what I'm going to say is going to be very trivial,
and for some people maybe sort of very different.
And so I think there's a little bit of a challenge
in introducing some of the ideas
that are motivating what's going on in type theory right now
to a sort of traditional mathematical audience.
One of the reasons being that there's a lot of...
there's a lot of connections with logic and programming
and these kinds of constructive ideas that sort of aren't...
well, they don't always form a basis of a mathematician's education.
So for example, for me this was a very...
learning about type theory was a very new experience
because it's not the kind of thing that we learn
as a mathematics graduate student or something like that.
So we're going to start...
I'm going to try to go really back to the basics
of where these ideas come from,
where the idea of type theory comes from
and how to see how it gets started.
So if I'm being too elementary or you have questions,
you want me to go faster, just yell at me.
Okay, so what is type theory?
So you might say it's a formal system.
I'll say it this way, for constructive mathematics.
And I'm going to sort of expand on what I mean
by constructive mathematics a lot in this talk.
But one way to say it is,
well, in set theory we have these classical axioms.
We imagine that all propositions are either true or false
and we imagine maybe you like the axiom of choice,
maybe not this...
the idea that all propositions are either true or false
is called the law of excluded middle.
And so because of these axioms,
which seem very natural when you start,
set theory allows us to prove the existence of objects
very often without ever giving a construction of them.
So this is a very...
and this is something that has been criticized about set theory
well for a long time since Brouwer and Hayding
and a lot of people who did intuitionist logic.
They said, well, a proof is not really a proof
unless you tell me how to do something.
If you're going to give me a proof that something exists,
you should tell me how I build that thing.
You shouldn't just give an abstract proof.
And so some mathematicians have taken that idea very, very seriously
and they want a world of mathematics to work in
where that's all you can do,
where some of these classical axioms
where you can produce something out of sort of thin air
are sort of banished.
So that's one way.
Now I should say a lot of constructivists want me to point out,
which is true.
When you let go of some of those principles,
it doesn't mean you can't add them back.
So it's not that constructed mathematics is a weakening of mathematics.
It's that it's a sort of larger system
in which if you want to use classical reasoning, you can,
but you have to realize that when you use extra principles,
you give up something.
So anyway, so I'm going to say more about this idea of constructivism.
But in a nutshell, one way to think of what type there is,
it's a system of mathematics where everything is constructive from the start.
That's one way to think of what it is.
Another way to think of what it is,
and this is why it has a lot of connections with computer science,
is you can also think of it as a programming language,
which is something that mathematicians are maybe not so accustomed to doing,
but why can you think of it as a programming language?
Well, the reason is this constructive thing.
If you give a proof that is completely constructive,
you can think of that as a set of instructions for building something.
And when we program on a computer, that's what we're doing.
We're providing the computer with a set of instructions for doing something.
So once you sort of see this connection, it's very nice.
You can read mathematics in two ways.
You can sort of read backwards.
You can say that the systems that people use on their computer to program something,
you can say that they are giving proofs of mathematical statements.
It's just that their languages, say C++ or Java or HTML or some JavaScript,
something that people use, they really are giving proofs.
They're just, their languages are not very elegant.
So they don't see their proofs as proofs, but they really are.
So that's another way to think of it.
We can see type theory, and this is going to be the point of the talk,
as an abstract formalization of homotopy theory.
So not only is it a language of sort of constructive mathematics,
of programming, and it has these nice connections to logic,
it also has this very simple and elegant geometric interpretation.
So this is what I want to sort of explain today.
So in order to sort of, right, in order to make this connection clear,
this connection between programs and proofs and things like this,
I need to go back and talk about a sort of long tradition in logic.
Well, okay, so let me first make some contrasts with set theory.
So what makes type theory sort of different than set?
And I think for a long time people, so in set theory,
we have this notion x as an element of A, and in type theory,
we have this notion x has type A, and for many, many years,
there was this sort of like a little bit of a,
well, are these not the same thing?
I mean, are they the same or not?
And in what sense are they not the same?
One of the nice things about Voivotsky's work is we have a very much,
very clear notion of why these are not the same
and what the two different things mean.
So in set theory, we have, I'm not sure I really want to,
a global notion of membership and equality.
And this is going to play a sort of,
well, this gives set theory a certain character
at the type 31 half.
So in type theory, the notion of membership is sort of local.
It is only x, this statement here just says x is a variable of type A,
and it makes no sense that x is an element aside from A.
x is not an element of some universe somewhere,
whereas when I have x being an element of a set,
x is itself an element of a set.
So the way we're going to read this over here is x is a proof of A,
and A is a statement.
Whereas on the set theory side, we write regard A as an object
and x as a particular fixed element of that object.
So on the left-hand side, it's really this sort of traditional set theory picture
of A with a bunch of dots.
On the right-hand side, A will be more like a statement
and x will be a proof of that statement.
In fact, this is one way to view the elements of type theory.
So local typing notion, membership replaced by is a proof of,
and I'm going to sort of make that more precise as well.
So another thing to say about set theory is,
set theory is a theory in first order logic.
That is to say, before defining what set theory is,
we have to first get a notion of logic off the ground.
So we build logic, and we talk about propositions p and q,
and p and q, and p implies q.
We develop the sort of system of logical propositions first.
And then we say, inside of first order logic,
with this in the background, set theory is a particular theory.
Well, it has a certain set, which we think of as sort of the set of all sets,
and this has its membership relation.
And so, no.
So what I was saying is, at a sort of structural level,
set theory consists of sort of two things,
one of which we often kind of ignore,
which is that the logic in the background consists,
the logic is sort of the theory of propositions
when something is true or false,
and how we connect these with connectives.
And then we have this theory of sets themselves,
which have sort of elements,
which may not be, I mean, p and q sort of always
in classical logic range over just true or false.
And sets, so just sort of, if you like,
sets with two elements, they're either sort of, well,
the set of sets with two elements.
But sets can have sort of like arbitrary many elements.
So you have this sort of two level dichotomy in set theory,
where these two sort of don't really play together.
One is that they're in the other.
Type theory, on the other hand,
what we're going to do is we're going to try to extend logic
to encompass sets.
So instead of taking,
stopping a propositional logic,
things are true or false,
we have and and or and implies, and if and only if,
and then build a theory of sets inside of there,
instead we're going to start with logic itself
and try to see sets appear naturally coming out of logic.
So what would that mean?
So now I get to search to marker, I guess.
Okay.
And the motivation for this idea comes from what goes,
I don't know if I,
from an idea that goes probably back to, again,
hating in Broward and a lot of the intuitionists,
that when you ask what is a proposition,
you can, the idea is a proposition,
instead of just being either true or false,
is the, and I'm going to try to expand on this,
is the collection, I specifically didn't say set there,
of its proofs.
And so let's try to expand on this.
So for example, so let's just suppose,
so what is, let me say A or B,
and I'm going to use logical notation for a little while.
So if a proposition, and we're going to read this as or,
so A or B, just a logical proposition,
if A is the set of its proofs, and B is the set,
sorry, I shouldn't say set, the collection of its proofs,
and B is the collection of its proofs, what's A or B?
It's a proof of A or a proof of B.
And I mean this in, so to give a proof in this collection,
if I give a proof of A or a proof of B,
it counts as a proof of one of these things.
Okay, so what's A and B?
It's a proof, well obviously this is going to get,
of A and, so let me put it this way.
It's a pair of a proof of A and a proof of B.
And then similarly, the last thing,
this is a sort of intuition,
does this one work better?
So what is A implies B?
Well, it should be, it's a function,
which, given a proof of A,
produces a proof of B.
Okay, so if we start to read,
so we can start to read these,
these are going to be our connectives,
they're going to replace our logical reasoning
in informal language, we have A or B,
A and B, A implies B, these sort of things,
except for we're going to give them
a very specific interpretation,
that is there, and we're going to think of them
as quote unquote true,
if we can find a proof of them.
Okay, so true, a proposition in this view
is true if you can give a proof of it.
So what this leaves out,
so sort of unclear,
why, well,
we're sort of mystery,
that we're going to sort of try to explain,
that I'm going to come back to over and over again.
Now propositions might have more than one proof.
Okay, so,
and that's going to be important for what we do,
the idea that,
in classical set theory,
these two things are very separate from each other
because a proposition is either true or false,
there's no way, there's no extra stuff,
but in this view where we take logical connectives,
as consisting of their proofs,
the connectives actually start to have structure themselves,
they may not just be true or false,
they may have more than one thing in there.
Okay, and just to give you an idea of how we can,
so the idea of type theory is to set up a proof system,
a system of proofs, of keeping track of proofs
to realize this idea.
Okay, we're going to try to set up a logical system
in which we can literally keep track of the proofs of A and B
and A implies things like that.
So just to give you an example of how this might connect
to ordinary set theory,
let me define for you the natural numbers,
proof theoretically.
So what I'm going to do is,
I'm going to just give a set of inference rules,
and then we're going to see how we can regard the natural numbers
as the set of proofs of natural numbers.
So I'm going to give a very simple thing,
first of all I'm going to say that this inference line
is just from the previous thing I can deduce the next thing.
So the first thing is going to say,
from nothing I can deduce that zero is a natural number,
from no axioms I can deduce this, it's just true.
And the second thing is going to say,
if I have a natural number, say n,
then I can have a new natural number
called the successor of n.
So this sets up a very sort of simple,
almost idiotic proof theory here,
but now I can think of the elements of the natural numbers,
as the set of proofs.
So for example the natural number 1
looks like the following proof
that there is a natural number called 1.
So this proof, this derivation,
literally is the element living in the natural numbers.
Okay, I'll do another one, let's do two.
That's slightly bigger.
So you can see successor of zero.
So I'm just using this simple elementary proof theory
to produce numbers.
This is obviously a very sort of trivial example.
But here is the proof that 2 is a natural number.
And I regard this proof right here as the element 2.
Okay, so that's how we can recover sort of ordinary sets.
The inhabitants of type theory are proofs.
So we need a language,
a formal language to keep track
of the proofs.
Okay, so this is a...
Yeah.
So we need a language
to keep track of the proofs.
Okay, so this is...
Do you know exactly what you have defined?
For example, have you infinitely large natural numbers?
So this is incomplete.
It's obviously not enough to characterize the natural numbers.
And I'll get back to what we need to add to characterize them.
Right, I'm going to come back to that.
But we need more.
Just that proof theory is too naive,
but there's going to be more.
So the next step is that this is something
we don't really bother with in set theory.
We use just the theory of set sort of informally.
So we substitute our own natural language
for the proof theory of first order logic
when we do set theory.
We don't keep track of the proofs.
But in type theory, because this is a more rigorous,
more rigid system,
we need a language to keep track of the proofs we're doing.
If we want to say that the elements of our system
really are proofs,
we need something to keep track of those proofs.
And so here is what comes to the rescue here
is something called the lambda calculus.
So you can think of the lambda calculus
from my perspective, from the perspective
that I'm sort of presenting today.
The lambda calculus is the language
in which we will keep track of our proofs.
And of course there are many other ways
you can view the lambda calculus.
It's also, I mean,
this is where the connection with programming languages comes in.
You can view the lambda calculus as an elementary programming language
where people use it on their computers.
But for us, I want to view it as
the language in which we will denote our proofs.
And the reason this works
is the reason this is useful,
why?
And I'm going to say because
I'm going to talk about Curry and Howard.
And I'm going to try to give you...
So Curry and Howard,
notice that when lambda calculus terms,
when they tried to figure out
the types of these terms in a programming sense,
that they essentially ended up with logic itself.
And so I'm going to sort of try and just...
And I'm going to give you an example of this
in a very basic logical system.
So I'm going to assume that I have AND and implies.
And let's just do a sort of proof
in ordinary tree...
This would be called natural deduction logic.
So I'm just going to prove a little statement,
then I'm going to show you why lambda...
And I think I'm just going to do one example
to illustrate the isomorphism that Curry and Howard found.
And then we'll see why lambda calculus is this useful thing.
So a typical proof might look like this.
So I have AND and B.
From AND and B, I can deduce AND and B.
Now let's suppose another hypothesis.
Let's suppose that AND implies C.
So if I have these two things,
then what can I deduce?
I can deduce, well, C, because I deduced AND and B.
I have AND here.
And I have AND implies C.
Modus pulent, exactly.
So I can deduce C.
And what typically...
So I'm sort of...
You could stop here.
This is perfectly good,
but I'm going to make a couple more steps.
What you can do to introduce implication
is you draw a line here
and you discharge the hypothesis.
So I'll do something like this,
and then you say B implies C.
So you have this open leaf here,
and now you've closed off this leaf.
B was an assumption,
and now you've taken that assumption,
you've used it,
so you're saying that B implies C.
This internalizes that.
So let me then say that, well,
let me discharge the assumption of A.
So now A implies B implies C.
And let me do the last one.
We'll discharge this.
So we have what a type...
We would call it closed term.
We have AND implies C.
It implies A implies B implies C.
And you can read this.
You can probably just read this
as an isomorphism as well.
If you know the theory of Cartesian closed categories,
this is just a deduction
in the Cartesian closed categories
But now what Kareen Howard noticed
was that I could label...
I could follow this proof,
annotating it with terms,
and I would get the lambda calculus.
So I'm going to do this proof again,
and this time I'm going to do it a second time,
and this time I'm going to carry along variables
corresponding to a variable proof of A
and a variable proof of B.
So if I have a proof of A
and a proof of B,
then the pair x, y
is a proof of A and B.
And if I have f,
which is a proof of A and B implies C,
then f of x, y, f applied to...
This is one of the fundamental operations
of a lambda calculus.
I can apply something.
This is a proof of C.
And now, when I go up to the next step,
when I abstract,
in other words, when I say for any y,
I was able to prove f of x, y,
I do something like this,
I'm going to discharge that.
I'm going to get...
This is where this lambda thing...
What I'm saying is I'm abstracting.
I'm saying for any y,
I could do this f of x, y,
and this is a proof of B and C.
And what you read here is this is bound to the variable y,
it's now become a function which takes one input.
And whenever I have something,
when I have a legitimate proof of B,
I can plug it in for y,
and then plug it in here,
and I will get a proof that I said.
So it's building exactly...
It's a language which is recording the idea
that a proof of A implies B is a function
which, given a proof of A,
produces a proof of B.
Here's that function.
I think you want an x.
A lambda x.
No, I think y was the...
Oh, sorry.
I gave y a title.
Then shouldn't it be a map from A to C?
Because the free variable is in x.
Sorry, the free variable is x is in A.
But x is still free in this proof.
Yeah, so it should be in A.
No, no, no, no.
Think of this as a...
Think of this as a global element.
So it's an element of...
Ah, ah.
Okay.
Yeah.
Think of that as something...
Sorry.
Yeah.
Yeah.
It's right this way.
So you take that.
Anyway.
So the next step, again,
I think, well,
we're probably going to get bored now.
So I get lambda x,
lambda y,
f of x, y.
This is a proof of A
implies B implies C.
And finally, when I just change this,
I get lambda f,
lambda x,
lambda y,
lambda x, y.
It's a proof of what A implies.
What?
A and B
implies C
implies A
implies B
implies C.
Okay?
So...
And if I read this,
this closed term here,
it's supposed to be an implication.
You give me f,
the function from the pair
of A and B to C.
And then I spit out a new function,
which is what's left after I substitute f,
which takes an input x of A,
et cetera, et cetera.
So this is what Curry-Howard noticed,
is that if you just take
the ordinary proof theory,
you're approving a proposition
and use the lambda calculus
to record the proofs you have,
you can see this term here that we produced.
It's sort of just an abbreviation
of this whole tree right here.
We just kept track of it.
Okay?
So this is why people use
lambda calculus as this sort of
the language of type theory,
because it's a language for recording proofs.
And moreover, it's a constructive language.
This is why you can do type theory
as a programming language.
You can also view this as a program.
It tells me what to do.
If you hand me a function f like this,
and an A and a B,
it says, this program says,
take your f, take your input y,
and stick x and y into f, whatever f was.
So not only is it recording the proofs,
it's also recording the mechanism
with which you execute the proofs.
Does that make sense?
Right.
So we can make this idea precise
by recording the proofs
with lambda calculus.
So what do you exactly call the
t-re-colon?
I think it's this, well,
if I were to give a formal statement,
I would say the following.
Take a set of names.
Maybe someone else in the audience
can answer this better than me.
I would say it's the observation that
if you take a set of names,
base types people would say,
just formal variables,
and take the free Cartesian closed category
on this set of names,
that the arrows of that category
are labeled by lambda terms.
That's what I would call the
Curry-Howard correspondence.
Does that make sense?
In the free thing, the terms are labeled.
Is that a reasonable statement?
That's, yeah.
I think it's nice to say that
That's probably a modern way
of looking at it.
I think the idea is that
more generally,
it's more like some kind of
general principle.
Generally in every proof,
in whatever system,
there is some computation of the
algorithm.
Some normalization.
That's why it's a general principle.
In the very specific case,
the nice thing about this case
is it's somehow like an isomorphism.
It's something you can say
mathematically, but I think, of course,
it's taken as a more general principle
than that.
I hope this is enough to illustrate
the idea of what's happening.
I think it's pretty striking when you see it
written down this way.
This programming language is, in a sense,
a way to write proofs.
I think that's nice.
Okay.
Where am I going next?
Okay.
Good.
Good.
Okay.
Let's move on and say it.
So we're not done yet.
So this language is pretty weak.
Because as we have so far,
the version I showed you so far,
we might have a or b,
a and b, and a implies b.
But for example,
we would have no way to do something
which we do in first order logic,
something like quantification.
We would like to say something like
for all x,
or there exists x.
So how can we get quantification
back?
Well, if you think about what's supposed to happen,
so let's just take a look
at something like this.
We would expect,
if I want to make this into a language,
I want this to be a new proposition
that I create out of old propositions.
We expect that this is some proposition
where I have a given proposition
p, which now depends
on x.
And similarly, if I have exists
such that p of x
we want to introduce
propositions like this
for quantification,
but we suddenly see that
in the Cartesian closed category case
all I have is a and b
and a and b
and situations like this,
but a and b, they're just
atomic things.
They don't depend on anything.
So in order to enrich our language,
we need types
which depend
on other proofs
in our system.
If x here is supposed to be like an element,
we need a type which depends
on other proofs in our system.
Another way to think of it is
we're missing elements that are sort of very
it's sort of the analog
in set theory.
You won't
you won't get very
far, I think, if you really
think about it.
In set theory, if you don't have these notions,
if you don't have the notion of taking the
disjoint union of a family of sets
or the disjoint union of, or the product
of a family of sets.
So these are sort of primitive notions that you need
to even get very far in set theory and so forth
are missing.
So a dependent type system
is one where the propositions themselves
may have
variables in them.
So we're going to sort of turn the system
in on itself. The variables will be
ranging over proofs, but the propositions will
depend on those proofs
as well. That's why these things are called
dependent types.
So before
I do this
so
I'm coming from homotopy theory
algebraic typology and stuff
so let me think of
what is p of x
in this situation?
Or how should I think of
the disjoint union of a family of sets
or the product of a family of sets?
What I want to think
I want to get this idea
in my picture, in my head
that what p really is
maybe I should have said
well this should range x is some
element of a
we should really, we're doing type theory
we should record the types
that what p is
is that it's a new type
ranging over x so in other words
again I'm going to say the type of types
or the set of sets
we're not going to worry about size
I claim that the way I should think of p
is that p is a map
to
the collection of all types
so let me just call
it something like this
and so what we really want
and then there are two things
I can build
once I have a family of types
a family of sets
depending on
a given base set
on the one hand I can build something
which I might call
well which corresponds to this
which I might call the total
space or the sum
x in a
p of x
so if I just think of this as
a family of sets
then I can build a sort of
I can take the disjoint union of all those sets
and view it as a set over a
okay so this is
this sort of duality here
this I would call sort of a classifying map
and this map here I might call
a vibration for example
so there's this and then
once I have and in fact any map
if you think in traditional set theory
suppose I have a map b over a
so we're just going to work
in traditional set theory for a second
then I can produce a classifying map
that goes from a to the set of sets
oops I did it again
by sending a
if this is f to f inverse of a
so this is a very classical picture
in category theory and set theory
and something like this
that when I have a map into something
many times in many situations
I can produce the same data
by mapping out of the base
into something else
so one way to think
of what a dependent type is
it's a type that depends on
something a in other words it's a classifying map
for some vibration over a
so it's really nice to keep this picture
of this dual picture in your head
and then we're going to interpret
this sort of thing geometrically
so here I'll put the sum
that's we want to condition
we want to construct her
if you like we want to be able to construct
unions of families
we want to be able to construct
what I'm going to say pi here
it's going to correspond to
well if you're familiar with sheet theory
category theory the sections of this thing
so
an element of this pi type here
will be something that to each a
assigns an element in the fiber over a
so this is a very
this is a very for me
coming from homotopy theory
this is a very geometric picture
if you read everything on the board as sets
then this makes sort of complete sense
the same thing that happened for sets
I can have a family of sets
and I can have an element in the product
of a family of sets is just one point
in each
in each of the sets
for each element of A
that's the same thing as a section of this map
okay so this
and
so dependent type theory
is what we're going to get
and allow families of types
and consider sections
and total spaces of them
so
what else did I want to say about
ah okay so why is this
why are these
good notions of
I claim these are going to replace our quantifiers
I'm going to axiomatize them for you in a minute
but this idea of
the family of a set
and the product of a bunch of sets
is going to play the role of quantifiers
why?
suppose here's a picture of my base set
A
okay and I have
a proposition or a family
p of x
for x in A
let me just start out by drawing
like this
the term style notation
for every x in A I have some family
so here's
so I think I'm my family
what would it mean
to be an element
if I want to read this as
oh I'm sorry
yeah okay sorry
sorry
I wanted to mention you completely
oops sorry okay
so remember we're reading
truth
as the sort of
existence of a proof
something like this
so proof
we're reading truth as these things are
slightly more general than sets
we're not exactly sure but we're reading truth
as the existence of something so let's try to read exists
in this vibrational language
what would it mean for
the total space of this thing to be
non-empty
what would a proof in this if I view this
disjoint union thing as a new proposition
what would a proof of it be
well I would need
I want this thing to be non-empty
in particular I need to find something in A
and I need to find something
in the fiber of A
so this is exactly sort of non-empty
when the sum
the disjoint union of these things has an element
does that make
does my picture make sense
now on the other hand
suppose that I want to say
that if I view
VP as a bunch of propositions
but I don't know or true
for a moment let's just suppose that each
of these is either empty or
or a point
then that's like saying that I have a
my vibration is a monic arrow into my base type
now what happens if I ask for a section
of this thing
you can see if there exists a section
it needs to assign to every
point a proof of the thing in the fiber
so I'm going to need
so you see the sections are exactly recording
if there is one
then
if every sort of
one of these propositions has some element
there'll be a section
and the number of sections
will correspond to if each one has
different ones there might be more sections
so there might be more proofs
so here you have just a sort of
geometric interpretation
of the quantifiers
where instead of for all exists
having their traditional classical meaning
they can either be true or false
they have been generalized in this type
theoretic meaning to denote
particular collections of proofs
that I might create
the collection of
so
does this
sort of make sense this geometric
picture of the quantifiers
it's very nice that you can
sort of picture quantification
in this language as something very natural
from set theory or from homotopy theory
just a little question
you're using this classifying
some notion
of local triviality comes up
a notion of local triviality
because I don't understand
how the classifying thing makes sense
there's not in this language
which
yeah
so you can say
the specific fiber that varies
you should believe
if you have this map being constant
let's put it this way
all vibrations in this sense
will be locally constant
they'll be homotopy invariant
so when we reinterpret
our types here is not as the collection
of their proofs but as spaces
then all whenever I have a p
it will vary
based on lines in this space
will give equivalences
between the points of p so in other words
p always has to be locally constant
so that's the notion of local triviality
I guess that appears here
but yeah
they're not quite bundles yet everything's sort of
just up to homotopy
so what I claim is that
if you're a corner to dependent type theory
which I was
one way to read what it is
it's an axiomatization of this particular situation
of the distinction between
classifying maps
and the things they classify
so we're gonna
that's how I'm gonna read so now I'm gonna start to write some rules
what I want to do is
is
read these rules
through the lens of this idea
through the idea that they are
rules for working with classifying maps
so this is the picture we have in mind
and as I was pointing out in the audience
this naive proof theory for n
is too weak doesn't say really anything about n
I need to say a bunch more
so
Martin Lough
sort of
a father of the dependent type theory said
we're going to give freeze type
so unlike in set theory there's not just
things aren't just homogeneous every time I want to introduce
I want to tell you something about a new type
I'm gonna need to go through a process
of telling you
I'm describing it enough
so that it makes sense
in this proof theoretic system
and there's gonna be a little bit of work of doing that
we give
four types
of rules
and these will actually
well after you play with this
so the notation starts to look bad
but after you play with this idea it's very nice
and he sort of noticed a
symmetry between these rules
and it's sort of a methodology
I guess you would say about how to
introduce new types and I think
he was sort of his insight that
that these giving these four
kinds of rules would characterize
the sort of type theory of types
would characterize your types up to
a reasonable notion of equivalence
so one
formation
which is this
in what context
is the type
defined
so for example
I'll carry on the example
of the natural numbers
the natural numbers
and is a type
with no hypothesis
in any context you like
the natural numbers is a type
it's always there
looking ahead a little bit
let me give you an example
of a type
which has some
required context
which is going to be the type of proofs
that X equals Y
so the identity type on X and Y
so we'll talk a lot more about this in a bit
but in the context where
well I should be more careful
in the context where A is a type
and X and Y
are elements of A
then the identity
type on A between X and Y
is a new type
so you see
this is how we, like I said before
we want to allow
our
our oppositions, our new types to depend on
variables
and in order to do that we need to say
so for example here is
this is not a dependent type
and does not depend on anything
that's why it's valid in any context
the natural numbers are always a type
but the identity type
in order for it to make sense
I need to have two variables around
so it only makes sense in a context
where there's enough information
so that's what the formation rule says
it says when can I have this type
to
which is exactly the difference
I can have a type always
there's no restrictions
exactly
in set theory I can always have a set
I can always have the set of sets
I can always have the subsets of n
I can always have everything
so in type theory we make the existence
of a type sort of depend on
particular proof theoretic structure
there must be enough around for it to make sense
so introduction
how do I produce an element
of my type
so in the case of n
this is
these are
there could be more than one of these rules
these are my two introduction rules
it says how I produce elements
of my type
so not only do I need to say
when is a type a type
I need to say how do I make things in that type
I need to give proof theoretic rules
the analog for the identity here
is going to be the following
suppose a is a type
and x is an element of a
I'm going to say the only way
to produce an element of this type
the identity type on a and a
the proof of
that x and y are equal
is to give
is to have a particular
variable x and to give
be one element in id from x to x
it's going to say x is equal to itself
so refl here stands for reflexivity
so the way I get
the two things are equal
is I have them be equal
and then I get a canonical element
I get a representation of the fact that I've
proved x to be equal to itself
so I need to tell you
once I tell you when a type is
well defined I need to tell you how to create elements
of that type
oh that was really bad
sorry
okay three
elimination
how do I use elements of that type
so
in other words
or if you like how do I map out of a type
or in this picture
it's
an eliminator is going to
define the following
set of data
very sorry
suppose
suppose I have my type n that I'm trying to define
could you read again the sentence
it says how do I use elements of a type
yeah I shouldn't say
maybe I shouldn't say elements
we have
we're going to define the natural numbers here
suppose I have a family
of types over the natural numbers
and I think of
let me just write it p tilde
I think of the sort of space that
classifies it's the union of all these propositions
over n
then as I said
for all here it's going to be
for all it's going to be interpreted as
a section of this map
the elimination rule is going to say the minimum amount
of data to produce a section
and I'm going to show you how this
this works
let me write out the version for
the natural numbers and we'll see how this works
so here's for example
the elimination rule for the natural numbers
suppose
okay so maybe I have an introduction as well
so gamma typically
improve theory stands for
some list of
some list of assumptions
suppose that I have some list of assumptions
and the last assumption is that n
is a natural number
and suppose from that I can derive
some that I have a type
depending on n
this is the
this is the
this is the dependent type theory notation
for this data
where I should say
I should say the extra gamma
is the fact that this
n might depend on some initial
parameter space but
in practice
right so gamma think of it as
sort of the space of parameters
in particular of which I have a natural number
so this really is
when you look at these formulas
and they look intimidating
for me this is
that's what this says
suppose that for every natural number
p of n is a type
and suppose
that in whatever context
gamma is
that I have a p
that I can prove p of 0
okay this is the
notation for
there exists some proof
in some type
and since p of n depends on n in particular
I have a type by substituting in 0
now suppose when n is a natural
number
that I have
if p of m holds
so that I have
let me just say it like this
oh I'm not so sure
this is very confusing
let me say it like this
that I have a map
from p of n to p of success
from f
then the conclusion
is that
is
that
is that
I don't know why she should change to n
I guess I should be consistent here
sorry
okay
gosh I think I went too fast
so let me
let me go back
for just a second so when I say
gamma derives sort of b of x
type
this is the notation for
as I was saying
this is the notation for some
some classifying map to type
if I have gamma x as a
derives some family
of elements in this family
of elements
little b is in capital B
this is the notation
for
let me call this little b
for a section
this one says that for every element
x in a I have some type
this one says for every element of x
I have a b of x
which lives in the type b of x
so that's exactly
that's exactly the notation for
b is a family
depending on a and for each
x in here b of x takes it to
the fiber living over a
okay so
so these are sort of the two positions
that type 3
this is why I say type 3 is going to argue about
these kinds of rules so it's going to be a
calculus of classifying maps
so here I've given an example
so if n is if I have a family
over n that's exactly of this
kind right here
and I have an element in the fiber over 0
and for every n
I have a map from the fiber over n
to the fiber over psn
then I have a section
of the entire vibration
okay does this
so the intuitive picture here
so it's the following
so I have n here
and I have my p to type
and let's
let's picture p
of n
sort of vertically like this
then this says
that I have my p
I have a fixed element p
living in p of 0
and for every n
I have a map between these fibers
and the point is
then I can produce
a section of this vibration
by
okay so I didn't say the last rule
yeah but it's clear what the section of this vibration is
for any n in here
I start at pn and I just iterate these maps
until I get to a point
so you see this rule can be described
as saying it's the minimal amount of data
to produce a section
of a bundle over n
okay
okay
if that makes sense
sorry
and this
characterizes n
this is what was incomplete
for the question so this is
you can read this as
induction or recursion
the induction of
description of the natural element
and so finally rule number 4
computation
when you
when you produce these
oh I shouldn't have called it p
let me call this
sort of p0
when you define this section
you want to say that the result
here is compatible
with the data that you had
so n is sort of not
finished yet I need to say
something about this situation
here I get a term
which is going to be some lambda calculus term
so the whole language p here
is standing for some term of lambda calculus
I need to say that when I do p of 0
I should get p0
right
and when I do p of the
successor of m
I should get
let me call this map
phi
phi sub n
it should be
phi sub m of p
of m
so
so I need to say
I need to say something about how
the resulting section
here is related
to the input data that I gave
if you think of the
hypothesis of elimination rule
as sort of a minimal amount of data
to produce a section and the
output as the section itself
then the computation will say
how that section was related to the input data
ok so this is the sort of
it looks like a lot of work
I admit that
I mean it's a little bit striking the first time we see it
but for every type for the natural numbers
we need to give these four family rules
when is it tight
how do I make its elements
how do I produce sections over that type
how do I satisfy propositions in that type
that's another way to say it
and what relationship should those propositions satisfy
ok
how much time do I have left
10 minutes ok so this is
so I won't maybe I won't
let's
ok let's
let's leave this
this sort of complicated system behind
I was going to write some other ones but it's ok
ok
ok so I gave an example
really quickly but so what are we still missing
this is a pretty nice theory
we have
we can create types
we can move things around we have quantification
we can say for all exists
by the way oh well
and
and implies in the traditional
sense can be derived from
from these two notions of
exists and for all
so
the observation is that
is that all the other connectives that we had
are special cases of these two that I just described
this total space idea
and section of the total space idea
for example if I want to get
a implies b
then this is exactly just
the product over
x in a
b where b doesn't
depend on the element
a
ok
so I define implication
to be a special case of this product
what this says from a geometric point of view is that
the sections of a trivial bundle are just maps
from the
from the base to the fiber
so that's
actually
if I have a here and I have b
constant on the fiber
it's b everywhere
then a section of this thing
is just a map from a to b
and similarly
so and
a and b
is the same thing as duality
because
the total space of the trivial bundle
on a with fiber b
is just a times b
so this is our interpretation of an
ok so we have that
some of the constructors become special cases
of this geometric idea
but the more important thing is equality
so I showed you how to introduce it
but so the point is
if you think about this system as it goes
we don't have any way
to say when two things are equal
and this is an important
concept in mathematics
so what are we going to do?
everything in our
system, every proposition
every statement we can make is a type
so this is what the system reasons about
so there needs to be
a type for equality
but we're going to
right
so if you think of mathematics as about proving equalities
we need a place
and the objects of this theory are proofs
we need a place to put those proofs
so exactly
so what you do is you say that
whenever you have two elements of a type
if a is a type
I will give you all the rules
but if a is a type and you have two elements of a
then there's a new type
this would be the formational
of the type of proofs
that x equals y
and you think of an element of this type
as a proof
that these two terms x and y are equal
and I showed you that the only proof
that you give
the only canonical proof that x and y are equal
exists when x is y
okay
but
okay so now
this is where
and this system sort of as presented
I probably have not done the best
that's for a long time
for verifying computer programming languages
but there's something we left open
that I've been sort of not
that I've been ignoring
there's something strange
a little bit about the idea
that all
of our propositions can have
multiple proofs
in particular
we've sort of allowed, we've just generalized
logic to say that
our logical propositions
by the way this is often written as like
x is
equal in a to y
so this is a nice
short
shortcut and I'm going to use this in a second
we've allowed our types
our propositions, the language we're speaking
to have more than one proof
and we haven't really dealt with that
fact yet, okay
so Univalence comes in to
the Univalence is going to be
about how to deal with the fact
that
there might be more than one proof
of something
and how to incorporate this fact into this language
okay
so for example
if x and y are equal
if I have the identity type on it
I now have the following derived rule
so suppose I have x and y
so this thing is a pipe
very good
and suppose that I have p
id a
x y
and suppose that I have q
from id a
x y
in other words suppose I have two proofs
that x and y are equal
according to this introduction rule
I mean I've just satisfied the hypothesis
with a certain substitution
there is now a new type
the type of proofs
in the type of proofs that x
and y are equal
so once I have
this rule
this rule follows
so in other words
there is a type
just by the formation
of proofs
the two proofs of x and y being equal are equal
and the question is
what to make of that
it's just a natural sort of outcome
of the fact that we have
that our proofs can have more than one element
we're suddenly saying that the proofs that x and y can be equal
might have more than one element
there might be more than one way
that x and y are equal
and the solution to that
is to start thinking
of x and y
as points in the space
and start thinking that the proof
that a proof p that x and y are equal
is just a line
in that space from x to y
and so if I have another one
q
then
the type of proofs
that the proofs of x and y are equal
I can think of geometrically
as sort of a two-dimensional piece of space
interpolating between p and q
and similarly this thing
again it's a proposition
everything in our system is a proposition
it might have more than one element
there might be two proofs
that the proofs x and y are equal
and on the other hand if I have two of these
call them alpha and beta
I might have a new type
I might, I do, it's there in the system
it's been defined
and so I might have a proof
that these two proofs are equal
so suddenly the system, once I just have one identity type
and I'm allowing the fact that
all my proposition is going to have more than one element
I've just admitted
that whenever I consider
a fixed type
its identity types sort of go off to infinity
there might be
there might be identity types
of all levels
because propositions in this theory are not unique
in any sense
the proofs of propositions there might be more than one
now
yeah
now things that you can do
suppose
so this is where we're starting
you can see where
suppose, well I don't need to make squibbles
there's lines
suppose that I have a couple elements
as you imagine this taking place
in a type A
so I'm just
switching order of sort of groupoid notation
to make some contact with
metjou's top
suppose I have P as a proof
in this is A
so P is like a path between X and Y
it's an element of this identity type
in practice it really is just
a lambda calculus term
in the sense that I showed you earlier
in this proof
that if X is equal to Y
and Y is equal to Z
that
that there is
that X should be equal to Z
by composition of paths
so Q
this is a theorem
so you can prove this
a type theorist would read this as equality is transitive
a homotopy theorist
reads this as the composition of two paths
but this is
I didn't write down the
elimination rule
how to create a section over the
but it follows from that rule
the rule is a little complicated
I don't think we want to really get into it
so in other words
and also it is
provable for example that if P
if X and Y are equal
the Y and X are equal
okay
now on the other hand
so these operations are provable from the rules
that I
apply P inverse
and by the way there is always an element
of X being equal to itself
this was the introduction rule
this is the only way to produce elements of the identity
there is this thing here
we are talking about reflx
this was the single way
to produce an element of an identity type
now if I am thinking
very carefully here
well I have two things
I have reflx
both with P inverse
which are both endomorphisms of X
they are both ways in which X is equal to itself
they are both proofs
that X is equal to itself
are they equal
and the answer is yes
but if you write down where are they equal
both of them
so
P compose with P inverse
lives in this type
and reflx lives in this type
so the fact that these two are equal
lives in the identity type
of P compose with P inverse
comma reflx
in other words there is no other way
to say
that P compose with P inverse
that the symmetry just gives me the identity
other than to say
that there is an alpha
that was a mortal sin
that there is a proof alpha
in this identity type
now this rule is also provable
but what does it say in our geometric interpretation
I have two things
it says that there is a two-cell between these two things
and the interesting thing
what is so fantastic about type theory
at least from my perspective is
there is no other way to do this
the only way to say two things are equal
is to give a cell in an appropriate identity type
and this may
continue to higher and higher dimensions
there is just no other notion of equality
so in a sense if you like
we are
all types
inherit
via this identity
identity construction
inherit the structure of group oids
we have to start viewing them
as group oids
because of this fact that we are allowing
propositions
which are the only objects in our theory
this
this sort of this theory of
it runs away with itself
we get
non canonical elements at all levels
so
so a couple of people noticed this
and having sort of category theory in mind
proved some formal theorems about this
so for example
people like
Warren
Vojlowski
Lumsdane
let me just say at all
so lots of people have worked on
formalizing the notion
that
the structure of type theory
the structure of this logical thing
whips all types
with higher categorical structure
so types
are
models
let me say it this way
types, model
infinity group oids
in various contexts
so as con complexes
internally
internally means that
out of type theory I can construct a category
just like out of
lambda calculus I can construct a cartesian closed category
out of the syntax of type theory
I can produce a category
and internally I have an action
of a particular globular operad
which is a sort of classical definition
of an omega group oid
so you can show that the identity types inside that category
furnish
all types with this higher dimensional structure
has interpretations
in model categories
so once you have this idea
of interpreting types geometrically
there's a number of different ways you might make a formal statement
about the idea
that types are
higher dimensional group oids
and these are some people who did it
and some of the ways you might do it
I'm not in the sense of freedom
yes
you can immediately sort of see the idea
I showed that
I was trying to explain that
when I have a family of types
varying over type I can make
this type
the sum of p
and it's easy to show in type theory
there's a canonical projection
derived from the logic
so the class of maps
of this form form the vibrations
in a model category structure
it's exactly like you said
the classifying map of a vibration
you have a vibration itself
and when you have the projection you call that map a vibration
so any map sort of isomorphic to one of this form
is a vibration you show that those
endow enough structure to
produce a model category
how much time do I have left
none? oh I can't
can I just say one word about univalence
okay
before you move on just make one comment
on these infinity group oids
since I was remembering about the geometric
the smooth the lead group oids
so types model geometric
this text
yeah
I mean you can interpret this
yeah it doesn't have to be
right
this is a
broad interpretation of group void
and there can be group oids in many different contexts
and you can think of type theory as a language
for talking about all of them
so if you want smooth group oids
something like this this also makes sense
you can use type theory to talk about them
of course if you have smooth group oids
that you can't see in type theory
you'll have to add axioms to capture
that smoothness but right
it's a base language
let me say one final word
sorry I know I'm out of time
about the universe
so I've been intuitively
I've been using this I've been breaking
the rules and considering
the type of all types
thinking of a proposition as a map out of
a type into the type of all types
where a family of sets says a map from that set
of all sets
so in order to make that rigorous
what you need to do is introduce a new type
called the universe
and have some sort of size restrictions
but basically
in type theory what you do is you make this formal
by now adding
a universe u
or whatever
actually for now I'm just gonna
I'm just gonna stick with type there
let's pretend
for a second the type was a type
then I would have the following
for any types a and b
I would have the identity type
in type
between a and b
and the question is
well
when should two things be considered
to be equal to each other
now in set theory this is not
an interesting question
there's a definition of when two sets are equal to each other
if they contain the same number of elements
but in type theory
it's a question
because again as I've sort of tried to emphasize
the whole picture of type
there is a
using logic to
encompass the theory of sets
propositions might have more than one element
in particular two types might be
the same in more than one way
and in particular
for example with sets
there can be lots of ways
this is where we start to think about
well when two sets are isomorphic
actually in practice
in mathematics when two sets are isomorphic
they have the same properties
if one has a monoid structure so is the other one
if one has
a group structure so is the other one
if one is a league group so is the other one
as soon as they're isomorphic I can transfer
but of course this fact
depends on the choice
of an identification between them
so if I say two sets
if I give a particular
bijection between two sets
suddenly they're totally
indistinguishable
any property one has the other one has
but that thing depends
but now in type theory we have a repository
to put
the fact that A and B might be isomorphic
in different ways
that two sets could be the same
in a number of different ways
so it makes sense
it wouldn't make sense in set theory
but it does make sense in type theory
so let's say that two types
are going to be
equal
when they're
let me just say bijective
so you can define internally
what it means for two types to be
to have a bijection one map one way
the other map the other way
identity elements going between them
so you can define an internal notion of bijection
the univalence axiom
says
actually we don't use this
but it's a good approximation
when we're thinking about sets
says that we will specify
that the identity type is the type
of bijections in other words
you give me any bijection between A and B
and using that bi-injection I can consider
A and B to be equal throughout the entire system
now what does this mean in particular
so this forces
the system to contain higher
potential information
because take the finite set with two elements
which computer scientists would call Boolean
the type of things with two elements
it's just a set
it can be defined
like
with two introductions there's true
and there's false in any context
so nothing interesting has happened
but you can prove that there are two
different bijections between
Boolean and itself
in any map
and there's not
there are two maps here
according to univalence those two maps
produce two
distinct ways
which the type of Booleans is equal
to itself
in other words they produce two
non-equal paths
in the universe
so as soon as you have univalence around
this you may not
have been able to create any higher dimensional structure
but as soon as you have univalence around
as soon as you add the idea that
in the universe
two types are equal exactly when they're in bijection
with each other
you force higher dimensional structure
because it's easy to prove in the theory
that there are non-trivial bijections
and those non-trivial bijections
produce non-trivial higher dimensional structure
so it's actually a theorem
that the universe itself is not
an n type for any n
arbitrarily high dimensional
non-equal proofs
of identity
using this sort of action
and last word
I'm done after this
what you can think of this
this idea
is an extremely strong
invariance principle of type theory
what it says is that for any two sets
and any choice
of bijection between them
the theory is completely invariant about
what you said about them
the system will treat those two sets
as indistinguishable
in all contexts
ok I think I'll stop there
applause
applause
no?
any short questions?
yes
is this part of the world in the long course?
yes
no other questions?
no
no questions?
no other questions?
yes
and
what about this duality
in the view of a proposition
that could happen to
types
and the type theory
precisely action
that's the way I think of it
I mean when I
I came to type theory from
from a Hobbit W theory
and
a higher category theory
so for me the following
there is a universal
vibration of spaces
a point in the space of spaces
is a space
and the fiber over that point
is that space
and this thing
classifies in infinity
it classifies vibrations
in the sense there's something like this
so this picture
of the universal vibration
and pull backs was something that was already in my head
and when people showed me type theory
I said in my head
it's this
it's an axiomatization
of the universal vibration
of spaces
and if you read it that way
it works
that's what it is
and in my mind
at least
I think dependent type theory
at least for me was easy to learn
because I had this picture in my head
and this is what's happening
when you have a P here
actually I should write it like this
this is gamma
this is a dependent type in context gamma
this is its category of elements
or whatever
that's what contact extension is
this is a very
in topos theoretic language
it's very natural
it's an axiomatization of the universal vibration of spaces
that's what I'd say
I think Martin Luff thought of it as an axiomatization
sort of of the universal vibration of sets
but with univalence this doesn't make sense anymore
you need to add something
but if you just work in the world of spaces
that finishes the picture somehow
in a nice way
does that make sense
I have one question
yeah
what is the feeling of what the univalence
axiom does
what are the other options
that we are eliminating
by fixing this axiom
what kind of identifications
I could have between types
which are not the rejections
what are the other options
that I'm just putting aside
by fixing the axiom
before that
before the axiom is added
and this is sort of the minimal option
is that two types
are equivalent
just when they're the same
under
reduction
so
the minimal thing is you can just say
a type is equal to itself
and nothing else
does that make sense
the only way to ever have
so the thing I'm hesitating
what's subtle about that is in type theory
a
it might depend on some variable
so
when types may be
not syntactically equal
but equal sort of by definition
so suppose I have a function f of x
is x plus 2
then I have like a of 4
where 4 is this proof that I gave you
the proof successor
and I have a of f of 2
so these two types
are not identically equal
but under the rules of type theory
you would say they're sort of definitionally equal
because f of 2 will reduce to 4
so there's a tiny
little ambiguity which has to do with the fact
that we can compute with definitions
but other than that
the minimal equality you can say
is two types
are equal to each other if and only if they
compute to the same definition
under a certain set of computational
so
it's adding a lot more ways for things to be equal
it's saying in addition to that
if I have any type b
and I'm able to produce an isomorphism
my g became a q
between a and f of something else
this counts now
as a way in which a and b are equal
so you have the sort of minimal choice
and you have the univalent choice
which is we identify
equality with isomorphism
or equivalence is better to say
because things are home with a few times
there probably are other choices
I think one of the motivations
for the univalent choice is
well it's sort of philosophically satisfying
in one sense
and also why
it's nice to think
well it sort of encapsulates
the sort of structuralist view
I think this is a word of Michael Mackay
of mathematics where
or like the category theoretic view
of sets
in the category theoretic view of sets
I can forget about sets
I just have the category of sets
I can never look inside at their elements
if two things, if two sets happen to be isomorphic
in the category of sets everything I can say
in the language of the category of sets
can't tell them apart
it just can't
whereas in set theory itself
I'm able to break that
that invariance principle
I'm able to look inside a set
and that element is this
and it's not that
so I think univalence sort of
in captures that categorical
invariance principle of sets
it sort of
it axiomatizes the system of sets
where as long as you tell me a way
to translate between one and the other
it can sort of bijectively
then I can't tell these two sets apart
that's as good
as completely identifying them
and saying the same because anything I can say about one
or the other
but of course that depends on a choice
which is why we need to remember all the choices
and in type theory we have a place to put them
in the identity type
so
another way I think this notion comes up
in logic and some people say
yeah
maybe since we are discussing
there is something I find a bit
kind of
too
univocal
so
so in type theory
often we consider
different notion of equalities
and there is some kind of
balance between
equality and structure
of property so the
two things are identified when
indeed the path
that goes from one to the other
we preserve the structure we are interested in
so that's the common
so my question is
all you say is perfectly right
but at the same time we could consider situations
where we have different notions of identity
yeah I was coming to that next time
I was just trying to explain the univalent choice
and saying that there can be other choices
with univalence I think
because it relies
and you would say there is
you could imagine
kind of different notions
univalence seems to give some kind of strong
condition but probably
you can do things like
you can do things like have relative
notions where if you have a map
from A to B or some set of maps
you consider types to be equal
well
you have the standard notion of localization
so if I have a map
from A to B this induces a map
x from
yeah this induces a map
the other way
and I can
like say fix a space x
and ask to invert all the maps for which this map
is an equivalence you have lots of different options
so you can repeat sort of the
story that Matiu was saying about
considering things up to some
specified notion of equivalence
you can repeat that internally into type theory
you can make these kinds of choices
of a different kind of equality
and you can consider types
up to that equality
and I think what this does is it generates
internal models of type theory
so you have a different model of type theory
in type theory where you say
now a type is something up to a particular notion
of equivalence and then the rules
of type theory get reinterpreted
in that system up to some level of equivalence
the fundamental rules will still work
the equalities will add
new axioms that you can't see
in the basic system that you can work with
and try to understand
so yes so there are other
notions that you might choose
I think one of the advantages of the
available notion for the sort of general framework
of type theory as a thing
is that people think it's constructive
in the sense that you'll actually be able to do computations
with it and you
might choose other notions
that are not careful but don't have that property
which might be okay for the system
but for type theory in general it might be
less desirable if you lose that
that I don't have to catch one
thank you
to me the main difference we said
is that
equities float to the under the stick
yes right exactly anything can be equal
you can't ask whether a cat is a prime number
right exactly
it makes sense
which is ridiculous right so
that's another I think
nice feature
of type theory is being very
careful about the grammar about what things
about what questions make sense to pose
because sometimes it's not worthwhile
to have a bunch of questions that are
that are silly and
I think the consequences you pay a little
price which is the theory is more complicated to set up
no I know the difference is that
I mean if you mentioned in the beginning
that in ordinary mathematics and boy it was
keeping it makes that different
there's a mark that you have two levels
first of logical level
but here like
in top of theory
there is a confusion between the first two
right exactly
in top of theory also the logical
you can have boolean by
logical values which are part of the
part of the language
of the speaking
I think it's one of the great benefits
yeah yeah I agree
so mathematics and metamathematics
one
yeah exactly
and also
it's interesting that usually
I mean in the usual presentation
of
of logic
of logic
I mean people don't take into account
the notion of definition
right
I find some people speak
loosely of an abuse of language
or an abbreviation
also because
if you read the classical logic
something from the antiquity
from the antiquity
definition is really part
of the logical constant
and in most of the exposition
of predicate logic
that's exactly right
I mean I think and this thing
I was showing about some sort of
if you like primitive notion of being
equal to each other Martin Love really
had this idea or brought it back maybe
in some ways that
when you that there was a different
equality there's something else to say
that f of x is defined
by x plus two
and that now when I stick into the fact
that I get four
is not an equality of
the kind we're discussing on the board
this is because that's what I said f was
there's something more primitive about
and this equality
this notion of like I make a definition
and then I substitute in a
variable for this definition
this is what in type theory is viewed as
computation it's the part of mathematics
where there's nothing to do
the definition was given in a set of variables
and I stick in the results of these variables
well if I do this often enough
what I see is the steps of a bunch of computations
and that's why we use
lambda calculus for this proof term
because that's what it is it's just recording the definitions
but
lambda is exactly what
is to say
if to say what it is
based on some variable
but it's usually in most of the position
it's not taken part of the logic
which is the weakness
yeah yeah yeah
yeah
excellent
have a question
will we come and speak again
anyway
thank you
