So, welcome back everybody after the summer break.
This is the first session of this semester and we're very happy to have my scrimer today who is a graduate student at Princeton and who is doing a lot of very nice things in machine learning and especially how they can be applied to astrophysics and cosmology.
So today, Matt is going to tell us about how to use, how to represent probably physical forces with machine learning and try to get some intuitions about what is going on and is probably will imply using some graph neural network, but I don't want to teach too much.
Take it away, Matt.
Thanks. So yeah, so today I want to talk about symbolic progression and how you can take a deep neural network which which might otherwise be kind of like an uninterpretable black box and then and convert that into an analytic equation that's very interpretable.
So, so yeah, so I'm a PhD student at Princeton Astrophysics, advised by David Spurgle Shirley Ho. This is a collaboration of us and then some people at DeepMind and also Kyle who, by the way, I'm unrelated to him it's very weird that we do pretty much the exact same niche but yeah we're unrelated.
So, so I just wanted to start off with a story. So I think when I was maybe 19 I read this interview of this.
This physicist Lee Smolin and in the interviews talking about kind of the the whole research picture of modern physics. So he says this, let's focus on quantum theory and relativity and their relation if we succeed it will take generations to sort out the ramifications.
We are still engaged in finishing the revolution I started. This is not surprisingly a long process. So, when I when I read this it.
I found this very frustrating. And the reason is because I don't, I really don't like the idea that
there could be these huge mysteries of physics still remaining when I'm on my death that I really don't like the idea that
it's kind of the research process is kind of like a multi generational thing just to solve these these current puzzles so I'm interested in
using artificial intelligence and machine learning to to accelerate the research process. This is this is how I got interested in machine learning is just how do we do astrophysics more efficiently so that a single researcher can accomplish a lot more
in their their own research so this is kind of how I got into it so I'm interested in machine learning for astrophysics.
And we can we can look at the whole research process so astrophysics research is kind of experiment design you get data you reduce it you convert that to knowledge find patterns in the data you build models you integrate the models.
Maybe you derive them from first principles and you make new predictions and it goes through this cycle so.
I think this talk kind of it focuses on one of these points so it's the the data to knowledge aspect. So a lot of machine learning is is for astrophysics is kind of the data reduction and this is the data to knowledge aspect so I'm going to talk about that so
to just give you some background on other stuff I do I try to do half methods kind of pure ML methods that could be useful for astrophysics and half on the applied side so I've done kind of energy conserving neural networks Lagrangian neural networks I'm not going to talk about that in this talk.
But I am going to talk about symbolic regression graph neural networks is kind of my main focus.
And then on the applied side I look at normalizing flows how we can use them to model color magnitude diagrams and then more recently how we can predict instabilities in simulations with machine learning.
So that's kind of the research I focus on.
If you remember nothing else from this talk.
These are my main two points. So I think that symbolic regression. I'll tell you what that is symbolic regression should be a first class machine learning algorithm in astrophysics I think it's, I don't think I've read a single paper in astrophysics that uses symbolic regression and I think that should change.
I think symbolic regression should be one of the primary algorithms in our field so the second thing I want to talk about is the method that I'm going to teach you today is how you can extend symbolic regression to high dimensional data sets.
So I'm going to talk about that.
So let's let's start first with symbolic regression.
Why it's useful how it works so symbolic regression is a type of machine learning algorithm. You want to predict why given X and
Now you do that by searching the space of analytic equations. So if you look at the diagram on the right there's this equation Z equals one s plus three p minus six. That's an equation and
When you do symbolic regression you usually represent this as a binary tree of operators so you can see this equation is equivalent to this tree.
And you want to search over all trees, well not all trees, but you start from the simple trees and you get more complex until you find an equation that matches your data set. So this is typically done with genetic algorithms.
I won't get into the weeds too much about that but basically there's a couple basic operations you do you can you can mutate operators you change like multiplication to division. Maybe you you randomize
Parts of the tree and these are these are genetic algorithms and it basically lets you search the space of equations. So astrophysics is actually
We do this sometimes by hand, but there are really there's these efficient algorithms for it and I think they should be used more often because so much of our field is just simple symbolic expressions.
And symbolic regression is is an efficient automated way of finding those equations. So, so why symbolic equations.
So if you just, I think I just googled cheat sheet for physics 101. You can see that so many of these equations are just simple analytic expressions and they they match the data very accurately.
So many of the rules and physics and astrophysics are accurately described by simple symbolic equations. So why aren't we using this space as the machine learning model.
And so there's this famous paper by Eugene Wigner.
He talks about how it's kind of remarkable that symbolic expressions are so good at modeling the world. So he says the miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve.
We should be grateful for it and hope that it will remain valid in future research and that will extend for better or for worse to our pleasure, even though perhaps also to our bafflement to wide branches of learning.
So it is kind of weird that simple analytic expressions work so well but we should we should really exploit it as often as we can.
So another thing I want to talk about is is when does machine learning become knowledge. So why are Maxwell's equations kind of considered a fact of science, but if you train a deep learning model on some magnetic simulation.
Why is that why is the deep learning model not knowledge. Well so for one the deep learning model, you wouldn't expect it to generalize to out of distribution data, but the symbolic form Maxwell's equations obviously do.
So that's one aspect. The other aspect is maybe maybe there is something really special about the simple symbolic equations that make them so effective at describing physics.
So we want to introduce this term called a Wignerian prior. It's kind of like Occam's razor, but emphasizing these symbolic expressions. So it's named after Eugene Wigner basically says in a in a machine learning problem.
You should favor simple mathematical expressions as your as your model it's kind of like it's a prior over machine learning models.
And this this works for like mechanics physics chemistry because we know those equations are effective there.
But we don't really know about other fields.
It is kind of like a mystery why simple analytic expressions don't work for like social science, human behavior.
But we know it does work for astrophysics which is important.
That's why I think symbolic regression should be used much more often in astrophysics and I'll talk about kind of how we can measure that later on, you can actually show that symbolic expressions generalize better in astrophysics than a neural network.
So I'll talk about that later. So the second thing I want to talk about is a lot of astrophysics data sets are high dimensional and symbolic regression is not good at high dimensional data sets.
So I want to show you how you can extend symbolic regression to high dimensional data sets using our method.
So symbolic regression gives you compact expressions with explicit interpretations I can see this equation and immediately understand the scaling of it, and it generalizes well, but it's bad at high dimensional problems.
And that's because it's, it's a combinatorial scaling, it's very bad scaling, there's so many different equations, whereas deep learning is very efficient at learning in high dimensional spaces.
So this is why it's become so common in computer vision, it's very efficient at these problems.
But it's uninterpretable. And it, it only gives you linearly extrapolation outside of the training set.
So that's, that's a weakness. So we are interested in combining both of these strengths with we want to get symbolic regression but high dimensional data sets.
So this is our method. And this can be applied to different architectures, we're going to focus on one for simplicity, but you can really apply this to many different architectures.
So here's how it works.
You design a neural network with separable internal functions.
Okay, to solve some problem.
And every internal function should operate on a low dimensional space. So you have a big neural network.
You have several multi layer perceptrons inside of it, accomplishing some small tasks.
Every MLP should take as few inputs as possible and output as few outputs as possible.
And you take that neural network, you train it, you train your neural network normally, then after you apply symbolic regression to approximate each internal function.
After that you have, you have a list of symbolic expressions that approximate each function internally, and you compose those together to build a analytic equation that's equivalent to that neural network.
So the deep neural networks job is still to learn the neural network still is predicting why given X, but now you kind of restructure it in a way where it's broken up into smaller internal functions that operate on low dimensional spaces.
And once you do that, it's really easy for symbolic regression, because you just approximate each piece of it separately, and then you just compose them together, and you get an equivalent model.
So we're going to focus on graph neural networks, which are a type of neural network with internal functions that operate on low dimensional spaces.
So as an example, we're going to focus on n body simulations. So we have like simple particles moving around interacting. We're going to model that with a graph neural network.
And then we're going to approximate that neural network with a symbolic expression that accomplishes the same thing. And as I'll show you, we actually recover the same equation that was used to generate that data set.
So this is a graph neural network.
So you have this, this input data set, it's a time series of maybe three particles moving around. So at every snapshot you have XY, VX, VY, and maybe the masses of the particles.
So you have these particles, and you look at all the pairs of particles.
So the, the equivalence to Newtonian mechanics is you look at all the interacting particles. So in a graph network, this is called pairs of nodes. You look at all the pairs of nodes.
You pass these, each pair of nodes through one MLP. This is one of the internal functions in that neural network.
It's called the edge model.
You compute what's called a message, and this message is kind of like equivalent to a force between particles, but in the graph network, it's a high dimensional vector.
You then pool all these messages in a graph network.
And you get this, the summed message vector between all, all sending nodes. So this is kind of like a net force, but it's a high dimensional vector.
Then you take this, you take this summed message, which is kind of like a sum force.
You concatenate it with the current node state, and you pass it through the second internal function, which is called the node model, and that's kind of equivalent to acceleration.
You take the net force, you divide by mass, but here it's high dimensional vectors.
So then you get the updated node state. So this is like you're computing the acceleration of every particle in an in body data set.
So in our model, so this is, this is a common architecture, it's a graph neural network, but in our architecture, we encourage sparsity in the messages.
So while we're training the network, we tell it, okay, use as few vector components in the messages as possible.
So we want to encourage these messages, which start off maybe 100 dimensions.
We encourage the graph network to kind of use as few dimensions as possible.
And then after this is done training, we rip out the edge model, we rip out the node model, and we approximate each with symbolic regression.
So these are low dimensional problems. So it's easy for symbolic regression, and then you can compose them after to get an equivalent symbolic model.
So we, we done some experiments.
So we have like all these different and body systems one of our squared one of our spring laws, damp springs charge particles this one's continuous or discontinuous so it's actually the force law here is actually an if statement.
The distance is greater than this, they pull and if it's within some distance they don't interact at all. So this is like more complex force law.
We also do Hamiltonian so we try to recover symbolic forms for energy. That's in the paper I'm not going to talk about here.
And then in the end we try to recover an unknown law in a, in a dark matter simulation.
So, these are our potentials. So we have this, all these data sets the time series of, of different numbers of particles, and you can see that this is like the discontinuous potential.
So, here's how we train it. So we have this data set.
One quick question. Yeah, are all the models for particles as, as diagramed or no so so one data set is for one is eight and one is 12.
And we try to repeat it for all the different data sets, but the graph network only sees. So we only train it on four nodes, or we only train it on eight nodes.
So we train together, but like a graph network could do that, but we don't, we think it's like a stronger result, if we only show it a fixed number of nodes.
Let me ask you another question. Yeah.
So, so do you, do you get consistent results if you train with four with eight or more. Yeah.
Okay, so then why do you just focus on four particles.
I only show four here, just for simplicity, but in the paper we, we do different training examples on four eight and 12.
Okay.
Thank you.
In terms of the training of the model actually, how does the computational requirements scale. Is it because this seems like an end by end problem right.
Yeah, so it's so a graph network is n squared.
Yeah, okay.
And so what you can do is if you have a, so the dark matter example I'll talk about later is there's 200,000 nodes.
So that's terrible scaling. So what you do is you only look at nearby neighbors with the graph network. Yeah.
So we have this four body data set.
Here I think it's one of our squared for slow.
You train a graph network to predict the next time step of the simulation.
And while this is training, you try to make the messages as sparse as possible. So here the, the amount of grayscale just represents how much information is in each vector component.
So you try to make it in as few as possible. So you optimize the L one norm of the vector. Well, so the L zero norm, the L one norm is kind of like an approximation.
So that's done. We have this procedure of since we know the force law.
We validate this by fitting the known force law to the messages.
So, so we have this, this proof in this paper last year where we said, if you encourage these messages to be sparse, they will be equal to the force law, plus a rotation.
You can validate it and you can see that the messages are equal to a linear rotation of the true forces.
And if you don't know this force, then that's where you go to symbolic regression, and you extract this symbolic formula that's equivalent to the force law.
So this is a, this is a video demo of how this works.
So I'll just walk you through it.
So you start off with simulations of particles connected by springs.
So this is just an equilibrium position of one.
And you learn to predict these dynamics with this graph neural network.
And you can see that here. So I made a mistake here, it should be the colored particles try to match the gray particles here on the right side.
And you can see that over the course of training to get closer and closer to the gray particles.
So this is this is not new. So people have been doing this before with graph nets, but here what we do is we regularize the messages with L one.
You can see that over the course of training to get more and more sparse, and eventually it's just two components.
This is a 2d problem. So that makes sense. It's like a 2d force.
Now we see if those messages are equal to the forces.
And we, we fit it. So you can see that over the course of training, the relation gets more and more linear.
And now you can see it's a linear combination of the forces as expected.
So we found that the messages are equal to the forces.
When you apply this sparsifying regularization.
So what about unknown laws, what if you don't know that force, that's where you go to symbolic regression.
And you can recover this, which you can see is equivalent to the true force laws plus a rotation.
And the rotation is just because the second neural network can undo the rotation.
So neural networks.
They're hard to police. So it basically the first one learns the rotation this way the second one learns the rotation the other way.
But that's just something you have to deal with.
So we recover the true force laws for many different simulations. So this is, this is a spring law.
This is one of our squared.
We successfully recover it. And we even do it for if statements.
So we recover the true force law here, which is completely discontinuous.
Which is cool. So those, those are the main two points and now I'll talk about the dark matter example so symbolic regression I think should be a first class machine learning algorithm and astrophysics I think people should use it more often.
And I just talked about a way you can extend it to high dimensional datasets.
So now I'm going to talk about the dark.
Can I ask you a question.
Have you had the case that the neural network were actually too smart and managed to encode the information in something really non trivial.
So that's a great question.
Yeah.
So.
So, if you don't apply the one regularization.
This vector doesn't have to be anything.
It can basically this vector can basically be concatenation of nodes. And then the second model does the force in this huge encoded space.
We actually have a, like a simple linear output proof that if, if that message vector is as sparse as possible.
It will be equal to a rotation of the force law.
And that's, yeah, I mean, so you can always have a case where like, it encodes it into, maybe it encodes two bits into.
Two bits into one bit.
Maybe it looks at like the floating point 32 structure, and it kind of encodes two bits in one, but we, we kind of assume that that shouldn't happen generally.
So if you have two bits and there's, there's two bits of force law, it should learn a rotation.
And do you control like the complexity of your neural networks.
You know that to the encoding and encoding.
Oh, yeah, we just have a fixed size. We didn't look too much at that. I think, okay, I think they're both two or three hidden layer with 300 hidden nodes.
We use a fixed training time.
I don't know, maybe eventually, but I mean, we do date augmentation. So I think it'd be really hard for it to actually learn and encoding there because there's this infinitely large training set.
So yeah, so the next thing we want to do. Okay, so we've shown how, when you know the law in the data set, you can recover it.
Right, so we've shown force laws and we also did this for Hamiltonians, you can recover the, the analytic Hamiltonian.
But what we're really interested in is how do we recover analytic equations that are unknown for some data set.
So here what we did is we took the Kyoto dark matter simulation, and we asked, can we predict the dark matter over density in a given halo using the surrounding neighbors.
So we're using the neighbors as an estimator of the over density in the center halo.
So we do this with a graph network. And then after that, we ask, can we extract an analytic equation that approximates that graph neural network.
So what we've done is we kind of have an analytic equation that predicts over density for a dark matter hill.
And the advantage of that is you get, it's an analytic model so you can immediately see what it's actually doing.
And it can lead back into your own understanding. So this is the problem setting. So the, the Kyoto data set is a, we basically get a massive list of halos.
So it's a, you start off with a grid. It does the halo finder, and we treat every halo as a node in the graph, kind of like every halo is a particle.
And you look at the surrounding neighbors, I think we did 50 megaparsecs per H, all the neighbors in that it's typically like 70 neighbors for each halo.
And you try to use all the neighbor properties to predict the over density of the center halo.
So we train this with a graph network and we apply the exact same model as before with the force loss.
And so we encourage this low dimensionality representation, and we recover this equation. So it says that the dark matter over density is equal to constant plus one over constant plus constant times the center mass multiplied by this sum over the neighbors.
So the sum over mass plus a constant and then you have this distance fall off.
So this is like the equation we discovered. And, yeah, so this this describes the over density of the center node, and this is a sum over the neighbors. So this is kind of like the force law in the first example.
So this is like your message function.
So here it's just a scalar. So when we sparsify that space, it decided it only needed one bit. It doesn't need to know.
It doesn't need multiple vector components, it only needs to sum a scalar over the neighbors, which is cool.
Excuse me, how many neighbors do you take into account like does it vary from. Yeah, so it varies. We fixed 50 megaparsecs per H as the cutoff.
It could vary from zero neighbors to I think one example was like 1000 neighbors, but the average was about 70 neighbors per node.
Miles, this over density and what scale is the over density to 20 megaparsecs moving.
So then if you change the smoothing length is the form of the equation that is learned.
That's a great question. So we're following up on that right now.
So yeah, we're trying to do like a more in depth cosmology paper on this and try to look at if we can actually have like a smoothing scale in this equation, which would be cool.
Yeah.
Yeah, so the solution we found. So we wrote down this this heuristic equation that basically looks at the density within each ball, and it multiplies it by the center mass.
So this is kind of like the, the simplest analytic equation you could write down.
And then our equation gets a better loss. So this discovered equation. And then we also do an example where we don't give it the surrounding masses, we just give it the surrounding velocities.
So we give it the velocity of every surrounding halo.
And we see if it can predict the over density and that actually does. And it beats this handwritten equation, which is pretty cool. And you can see that in the example where we're just look at the velocity.
It's actually using relative speed as a proxy for mass. So you can see that in this equation it's mass but when we say you can't use mass anymore.
It's actually starting to use relative loss relative speed.
As a proxy for mass, which is cool.
So these, these formulas looks, look pretty similar bit worse loss if you don't give it the massive surrounding halos.
So, another thing we wanted to see is, okay, so at the beginning of the talk I talked about symbolic expressions should generalize better than neural network.
We actually wanted to test that. So we want to test this Wignerian prior.
So what we did was, we took the same data set and we cut off all the halos with greater than an over density of, I think it was one.
So we cut off all those examples from the train data set. We train the graph network the exact same way.
We approximate the graph network with a symbolic expression. And now we see which generalizes better to the unseen data. Is it the graph network or the symbolic expression.
So the graph network obtains an average of 0.06 on the training set, and then 0.14 on the out of distribution data, but the symbolic expression gets 0.08 on the training data set which is worse, but it generalizes much better to the out of distribution data.
So it gets 0.09. So this is, I mean, it's kind of mysterious why this works and that goes back to Eugene Wigner's paper that symbolic expressions kind of have this remarkable ability to describe the natural world.
And yeah, so this was kind of, yeah, it's weird that this works so well.
And so one thing I should emphasize is the force law. So the force law examples, those were the true law. So we put in the force law to generate the data set.
But this analytic expression, that's not the true law for dark matter. I mean, so we start off with a random universe evolved dark matter over time.
This is not, it's not like the true law for that system. This is a estimator for the over density.
Yet, this equation describes the data better, which is interesting. Yeah.
I might be misunderstanding this, but the M term that he has in the last equation best with mass C1 plus.
So that MI is the mass of the halo at which you're calculating the density.
Yeah, exactly.
So this is saying that the order density is inversely proportional to the mass of the halo. So I have the mass of the halo is less.
I don't know if they're causally related, but that's the correlation we found which is unintuitive. Like we wrote down this handwritten formula.
Yeah, that's why I would have expected as well that the higher the mass of the halo, the higher the order density should be.
Yeah, it's really interesting that that's the relation we found, but it beats the handwritten model. Like if you multiply by center density, center mass, 0.12 if you divide it 0.09.
So it's unintuitive and yeah, I think it'd be like a big advantage of these models is we can have this discussion about why did the symbolic model learn that if if this was a deep model, you couldn't actually look at the scaling right.
So we can actually have this discussion. Why is it inversely proportional to mass. So, well, back to the, yeah, question. Sorry, it's a detail but see three could be negative and push the denominator to us to a small value and.
Yeah, I don't remember.
Yeah, we can conclude here.
Yeah, in the paper all the constants are given in the appendix. I don't think they're in this.
Yeah, they're in the paper though.
So yeah, so this, this was really, this is really weird, we actually, I think we just ran this at the end of our, like before we submitted the paper.
But it's just so weird that like this symbolic expression is equivalent to the graph network. Well, it's a bit worse loss, but it's, it's an approximation of the graph network, but it generalizes better to the out of distribution data so it's really like this, this
Wignerian prior actually improves generalization.
So yeah, so just to you.
Yeah, have you looked at like non mechanics problems.
No. So, I mean, so I feel my intuition tells me that a Wignerian prior will work will work well in fields where we know analytic expressions already describe relations well.
So it's like physics mechanics chemistry, but I don't know if a Wignerian prior will also work well and like social science.
No, I guess I'm thinking about like, I do supernova research so I'm thinking about like light curve modeling or something like that, where, you know, right now we're using these like incredibly complex models to model all of the physics.
But there are some, you know, historically analytic expression so I think that that is something that could potentially lend itself but there's not like this concept of velocity mass and force in there.
Um, so for time series.
I mean, yeah, I guess you have to give it features that you understand to start with. So maybe you could give it like a for a decomposition or something.
Or, I mean, yeah.
You could like encode it and then work with the encoded vectors in the symbolic case.
I don't know, or just give it give it features you think are intuitive and would be important like, like time to max brightness or something.
Like maybe you start off with features that you understand. And then you you train a model like this.
Or you, or you give it like a wavelet decomposition or something.
But I think, yeah.
So we haven't actually tried this for convolutional neural nets, but I think that's an interest really interesting next step. And I think,
yeah, I mean, like a graph network is at some level kind of equivalent to a com net. If you treat neighboring pixels as neighbors on a graph.
So maybe you could apply similar principle. I'm not quite sure, but I think that would be really interesting thing to try.
Yeah, so I'll just restate this method here.
So you write down a neural network to train on your data set, which has internal functions that operate on kind of low dimensional representations.
So you then sparsify the input and output of those internal functions. So we start off with 100 dimensional messages, but we increase the sparsity and it goes down to like two dimensions.
In fact, you learn symbolic expression for each internal function with symbolic regression package. So we use Eureka, you could also do like a brute force search a polynomial search.
There's other packages. I'm starting to get into DCGP, which is open source.
So you learn a symbolic expression for each internal function, and then you compose those and you're left with an equivalent symbolic model for your data set.
So one question you may ask is, is why don't we do straight symbolic regression? Why do we need this neural network first. So let's say you have a time series model.
You have time series data, AI, BI, etc. Say that your true model is, it's a sum, so you start off by summing exponent of AI squared plus cost BI.
So this is like your summary at each time step. You sum this into the Y variable, and then your output is the square of the sum.
So it's kind of like the variance of this expression. So if you were to do that with, with pure symbolic regression, and you limit yourself to say 10 tokens for this equation and 10 tokens for this equation, max.
That means there's 10 to the nine possible equations for Z and 10 to the nine possible equations for YI.
If you do the vanilla approach symbolic regression, you have to search for Z and YI at the same time.
That means there's the square number of possibilities. So that's 10 to the 18 equations.
But with our approach, you fit a neural network here and a neural network here, and your networks are really good at high dimensional problems.
So you search, you train this neural network, it's really fast.
And now you have one MLP for YI and one MLP for Z.
You break them up. Now you fit 10 to the nine, only 10 to the nine equations for YI and only 10 to the nine equations for Z.
And you do those separately. So that's only two times 10 to the nine.
So you've gone from 10 to the 18 equations to only two times 10 to the nine equations.
So this is why you want to do this two phase approach when you work on high dimensional problems.
And I mean, I didn't even talk about when you have like multi dimensional vectors here.
Like so if you're summing force laws, 2D vectors, 3D vectors, or if you don't even know that you're trying to find this latent space, then it's much, much more equations.
And then if you have like even more latent layers, there's many, many possibilities.
If you do this approach of using a neural network to factorize the problem into smaller sub problems, it's tractable for symbolic regression.
So this is why you want to use this approach.
Yeah, so, so that's my talk, you can read more in the paper, and we have a code here.
So all the code for a paper.
And we have a Jupiter notebook tutorial for the graph networks.
Yeah, thanks.
Thank you.
The actual clothes.
Okay, this was really interesting.
I'm going to open the floor for questions.
I have a ton but I won't need chances for other people.
One thing I'm wondering is the subject of like the best equation to fit the profile of a dark matter hail the density profile has or has is or at least was a contentious subject.
So have you thought at all about how you could maybe get a best analytical equation for a dark matter profile based on those same simulations.
Yeah, that would be really cool.
We haven't looked at it but um, yeah, I mean that would be awesome.
Um, yeah.
Yeah, I think this would work for that definitely. I mean, you take the same data set and try to fit a profile with symbolic regression.
Yeah, I mean, there's, there's so many things you can do in astrophysics with symbolic regression. And I literally have not read.
I don't think I've read a single paper in astrophysics that uses symbolic regression, but I feel like it's, it can impact the entire field and yeah, there's so many low hanging fruit.
I mean, I don't have time to do them but I think everybody should should kind of look at that symbolic regression. Yeah.
Other questions.
I have a question I have my hand raised but I don't know if.
Well anyway, I was wondering about this Wignerian assumption about finding the simplest possible model so you know as you noted like with the gravitational force law between particles it becomes an n squared problem.
When you have this one over r squared force law, but there are more complex approximate, but yet much faster force law that you could infer from a particle distribution that would allow, you know, for a tractable solution to the gravitational force of a multi component
system, like, yes, a multiple expansion or something. So, so we're actually doing exactly this.
Right now, we're trying to look at kind of like effective force laws and trying to do this more efficiently with graph networks.
So, yeah, so we're doing it for turbulence and and dark matter sims. And we're trying to look at, can we take the same approach and learn like a fully symbolic model that I guess like,
gives you an approximate answer, but kind of like summarizes the data, rather than computing every single interaction. So like summarizes the data first, and then computes the forces from the summaries, we're trying to look at that right now.
But I think that's like, that's something that would be hugely useful from this technique.
Yeah.
Alex, you have your hand raised.
Yes. I wanted to know if, when, when do you feel it's sparse enough when you impose your sparsity constraint.
So, we actually did no hyper parameter tuning here.
We just picked some L one loss and it worked for every single problem. So like every single force law problem in the dark matter problem we didn't have to change the, the coefficient for the L one norm.
And I mean, maybe that's.
Maybe, maybe that's just how it goes and you don't really need to tune it. And you just apply any L one regularization and just once it fits the data well then it just starts regularizing.
Yeah, I feel like if you have a small L one norm regularization.
After it's done fitting the data set and kind of reducing the main loss, then it will just reduce as much as possible over time.
And you like we never really said, okay, find this many dimensions, we just punished high L one norms and it found that dimension. So it's kind of like an unsupervised method.
Yeah, so we didn't do any hyper parameter tuning. I don't know if that's general or not. I mean, I had it for many different problems in the same L one norm regularizing worked.
Okay, a related question would be, did you find somehow that the dimensions that you get after imposing that sparsity constraint is related to the dimension of the equation that you're, you're getting or can you use that as a prior.
Exactly. So, yeah, great, great questions. Okay, so the first question is, yeah, so the dimension you get for the force law is always the same as the force law you put in.
And so we actually have a proof of that in the appendix of why you should expect that. And so like if it finds the true dimension, then it will find the force law.
Yeah, it's not like completely rigorous, but we have this is this like kind of proof using linear algebra in the appendix.
Okay, so your second question is, can we use that as a prior. That's a great question. We studied this so we have, we have an L one model, where we start with 100 dimensions, and we just regularize the, the number of non zero components.
So that's sort of a bottleneck model. So that's where we start off with messages that are three dimensions, or two dimensions for 2d forces.
And those do find the correct force law.
But it's interesting, those actually have worse loss than the L one model. And so the reason is because of something called the lottery ticket hypothesis.
The ticket hypothesis says that if you start off with very high dimensional vectors.
There's some pathway through all the nodes, that's the correct one.
And it's kind of like that pathway through your neural network is like the lottery ticket. So you start off with big dimensions, then you're more likely to have the true function already in your model, and then you just train it and it like emphasizes that
So if you start off with like 100 dimensions.
Um, then it finds the components that are like most similar to the force law.
So you start off with more dimensions, you're more likely to have some components that are already equal to the force law.
But if you start off with a bottleneck, and you, like, you're randomly initializing weights here, you start off with a bottleneck.
It's actually likely that you don't start with the force law, and you actually have to train the force law. But if you start off with 100 dimensions, you're more likely to already have that force law somewhere in your neural network.
And it's just like regularizing it.
So this is called the lottery ticket hypothesis.
Makes sense.
Thanks.
I'm also wondering if you think it would be possible to recover probability distributions and analytical form for some probability distribution with this.
Yeah, I think so definitely.
This is kind of like, yeah, so I think this is a, I mean, we only demo it for graph networks.
And Hamiltonian graph networks, but we have a upcoming time series model where we look at this for time series.
And it works there too. So this is kind of like a general approach to turn any neural network into an analytic equation that you can then compare with your theory and get better generalization interpretability etc.
So if you have a neural network that's predicting probability distributions, I think you can apply the exact same approach and kind of get an analytic form of that neural network.
Yeah, and if you're, yeah, so if you're new to symbolic regression, Eureka is like a really user friendly starting point they have a 30 day trial, it's proprietary and
Yeah, but they have a 30 day trial, but I think DCGP is like a better open source version.
You want to try those out.
And we have, yeah, in our GitHub, we have like a tutorial on all of this. So we actually walk you through the training of a graph network and then
we point out, okay, send this to Eureka and fit it.
Yeah, I have a question about convergence.
Yep.
So, when you keep iterating on the whole thing here.
So if I look at the slide you have on the screen at the moment.
So you will get, you know, updates on the values of the coefficients, C1, C2, etc. But it's possible that as you keep iterating the equation might change, right?
You might suddenly like jump into a new equation. So my question here is how do you know you have converged? How do you know the system is giving you the best possible equation you can find?
That's a good question.
So, I mean, so generally with neural networks, they do converge and you can, like you follow the loss curve and once the loss curve bottoms out, then you assume it's converged.
We haven't actually tested if once it's at that, like once it's bottomed at the loss, if the equation changes, we haven't looked at that. That's an interesting question.
I don't know if this equation would change or not.
Yeah, I really don't know. I guess it depends like how flat the minima is for your problem.
And I'm sure that's problem dependent.
That's a good related question. So when, when you optimize something complex, and it's a very complex optimization space, usually the answer that you get depends on your starting point.
And there is some stochasticity unless this is very well constrained by the data. So here, for instance, you're showing one fitting function.
But is it the case that in principle, there is a whole distribution of fitting functions that kind of agree.
In symbolic regression, you recover many different functional forms that fit your data.
And this is called the Pareto front. And it basically, it's like a list of every possible equation that approximates some relation in order of complexity.
So you assume that if you increase complexity, you get a better loss, and you have to pick, you have to minimize like both the loss and the complexity.
And so in the paper, we give an equation that tells you if you have this list of equations, losses and complexities, it tells you how to pick the equation.
You basically look for a drop in the Pareto curve, or the Pareto front.
And once it drops off, so you increase complexity a little bit, and the loss drops. That's when you know you've found like the true equation.
But there's always going to be like higher orders that better fit your finite data set.
But yeah, so yeah, if you use that that equation to look for like the drop in the Pareto front, you should find we usually find the correct force.
Oh yeah, so I just wanted to mention the last question I got.
I just want to say, okay, so we find rotations here, right, the force, the, the message function can find like an arbitrary rotation of the force law.
This depends on your initialization. So that's different for each initialization. And I guess that's one example of where, yeah, I'm sure this, I'm sure if you're at the minimum, maybe this rotation would change over time.
So I think in the video, we can see that.
Oh, it's okay, so it's actually fixed so you can see.
So you can see here like it's training this message vector. And eventually so here it's like it's a pretty accurate depiction of the force laws.
And so these, these arrows here are how it rotates the force vectors, you can see that over the course of training, the rotation slightly different, but eventually it gets fixed.
So it's like a constant rotation. Yeah.
And my last question is, is any of those packages using gradient descent in any form for symbolic regression.
Okay, so could you could you directly fit some formula as part of a larger model to the.
Yeah, so, okay, so I think Eureka.
Eureka uses gradient descent on the parameters on the constants in your equation. So does DCGP. There's also, there's this package called equation learner, which it basically writes your equation as a giant neural network.
And every activation is like a different operator.
And then you sparsify the constants. So you're like learning the equation through gradient descent.
So again, that's a, that's another kind of low dimensional symbolic regression technique. And you could use that instead of Eureka here.
Um, but I don't think you could kind of like replace this thing. So that's still like a low dimensional symbolic regression technique, but that does use gradient descent.
Because, you know, we live in the age of GPT three and all of this kind of language models. So I could imagine that you could have such a model that can represent symbolic expressions.
And you can embed it inside a.
Yeah, so there's something called grammar VAE.
And they actually learn so a grammar is like an algebra.
So they actually learn grammars for equation space.
But, um, so I'm skeptical that these techniques will work.
Because this space is so nonlinear.
Like if we look at where is it.
Yeah, so if we look at like these different equations, if you change this multiplication to a division, that's completely different equation, right.
Like you swap multiplication division, one of our acts and one times X are just completely different. So I think it's really hard to search this space continuously.
It's kind of the same in a sentence if you remove not or something like that, you know, I don't know, I'm not an expert.
That's true.
I don't know.
Yeah, maybe eventually we'll just have some neural networks that can do all of this.
But I feel like right now, even modern algorithms, they're still using genetic algorithm for this. So like auto ML zero.
It's this approach where you, you learn a neural network from scratch. So it's kind of like neural architecture search, but it's like from scratch, like you learn every single vector operation.
So that package does genetic algorithms.
And genetic algorithms, it's kind of like brute force. I mean, I feel like at some level you have to do brute force on these things because it's just such a nonlinear space.
But yeah, maybe eventually we'll solve this. That would be, that would be nice.
Okay.
Thanks so much. I don't see any other hands raised and we are at the hour. So I suggest we end here for today.
Thanks again, Miles. This was super interesting.
Thanks a lot.
Thanks for having me.
Thanks, Miles.
That was great.
