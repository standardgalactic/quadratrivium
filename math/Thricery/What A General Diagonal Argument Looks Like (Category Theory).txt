One of my mathematician friends recently told me about a famous thing Poincar√© said
Mathematics is the art of giving the same name to different things
My favorite instance of appreciating this was when I learned what a diagonal argument was and how it shows up in so many different theorems in maths
You may have heard of all some or none of these famous results
But did you know that they all secretly use the same trick in their proof and also that that trick is very
Concretely describable by the same picture in each case
Today, I want to show you what that picture looks like and how it teaches us some broader lessons in maths
I'm going to be giving a high-level overview of what diagonalization is how it crops up
Even when there are no apparent shape to draw diagonals in and how to start spotting it elsewhere
I'll be assuming you're familiar with sets and functions, but not much else
Some of you may have even seen the first result I prove but I suspect out many of you have been shown the general pattern
I know I certainly wasn't taught this at university
Some sets like the real numbers are described as uncountable
That means if one tries to list every real number one at a time
There will always be some real number that is not on this list no matter how cleverly one tries to write this list down
Let's see an example where we prove this rather interesting fact
Imagine an infinite binary sequence or in other words an infinite sequence of digits where each digit has to be zero or one
Such a sequence can be described by specifying its digit at each of its positions
So we can think of an infinite binary sequence as a function from the set of natural numbers to the set containing zero and one
For a sequence viewed this way plugging in n into the function outputs the nth digit
There are many many sequences of this kind for instance the sequence that is zero at every position or the sequence that goes
Zero one zero one and so on or even the sequence whose nth digit is one whenever n is a prime number and zero
Whenever n isn't now consider the set of all such infinite binary sequences and call it B
It is easy to see that this set B must have infinitely many elements as it contains a distinct sequence for every natural number n
For example the sequence that is one at the nth position and zero everywhere else
But B in some sense is much bigger in size than just the set of natural numbers
In fact, we shall prove that B is uncountable to do that
Let's first assume that B is actually countable. This means we can find some exhaustive list of infinite binary sequences
Where every possible infinite binary sequence that is every member of B appears somewhere on this list
We shall now contradict this assumption by finding a special example of an infinite binary sequence that cannot be on this list
Let's now construct that example
Its first digit is going to be the opposite of the first digit of the first sequence
And by opposite, i mean that if the first digit of the first sequence is zero
As in this case the first digit of our special sequence will be one and vice versa
Now the second digit of the sequence is going to be the opposite of the second digit of the second sequence
So in this case, we shall take the opposite of one and so the second digit in our special sequence will be zero
Similarly, the third digit of the sequence is going to be the opposite of the
third digit of the third sequence and so on. In general, the nth digit of the
sequence is going to be the opposite of the nth digit of the nth sequence in our
list. Since B is infinite, our list better be infinite too, if we have any
chance of listing out every member of B. So our special sequence will have
infinitely many binary digits and hence is a valid infinite binary sequence. Now
here's the cool observation. Suppose this new sequence appeared on the list. Say
it was the 17th sequence, then its 17th digit by our construction must be the
opposite of the 17th digit of the 17th sequence, which in this case is the
special sequence itself. This means that the 17th digit of our special sequence
is the opposite of itself, which is clearly nonsense. So our special sequence
could not have been the 17th sequence on our list. More generally, for any
natural number n, our special sequence differs from the nth sequence at the
nth digit and so it could not have been the nth sequence on our list, which
means this special infinite binary sequence, a member of B, is in fact no
way on our list contradicting our assumption that every member of B appears
somewhere on the list. So our assumption must have been false and we can deduce
that one cannot list out the members of B one at a time. In other words, the set
of all possible infinite binary sequences is uncountable. From our picture
here, we can see how our proof involves the diagonal and can be called a
diagonal argument. Each digit in our special sequence was simply the opposite
of the corresponding digit of this diagonal sequence of ones and zeros. Hmm,
I wonder if we can represent this in a slightly different way using functions.
At this stage, if you're wondering why, why do this? Why express everything in
terms of functions? That's a very good thought to have. Remember, our goal here
is to understand what the essence of a diagonal argument is. What we're looking
for is a general pattern that we will spot in several other proofs in
different areas of maths. To illustrate what such a pattern might be, let's use
an analogy. Suppose Alice, an English speaker is trying to learn Hindi. She
wants to say the sentence, I will drink tea. She looks up the word for I, which
in Hindi is meh, the word for tea, which is chai, and the word for will drink,
which is piyunga. She then goes up to Ram, someone who knows Hindi, and
translates her sentence, I will drink water, using the Hindi words she's just
learned, meh piyunga chai. Ram then shakes his head and says, hmm, that
doesn't sound right. You should say meh chai piyunga. This seems rather strange
to Alice, as the word order feels rather unintuitive. Ram explains that in
Hindi sentences, words representing a doing action, which we call verbs, come
after the words representing what an action is acting upon, which we call
objects. In English, however, the verb usually comes before the object. For
instance, in I ate rice. These concepts of verbs, objects and the order between
them are useful because they help in learning several other languages, not
just Hindi or English. So a linguist, someone who studies languages as a
whole, may use these concepts to describe patterns in any new language they
encounter. This idea of giving names to patterns and seeing them in several
different places is the main drive behind this video. In languages, patterns
for language rules are helpfully described using the notion of verbs,
objects and the order between them. It turns out that in maths, the useful way
of describing patterns is in terms of points and arrows between those points. In
our particular case, these points are going to be sets and the arrow between
the points are going to be functions. Another example is just the real number
line, which tells you about the order of numbers. There, points are just numbers
and arrows are just comparison signs that point to the smaller number. Maths is
littered with examples of this structure. And as you study other areas of maths,
you see this cropping up all the time. So bear with me. But once we describe
this diagonal pattern using points and arrows, that is, in terms of sets and
functions between those sets, we'll start recognizing it elsewhere. So let's give
this a shot. Recall that our proof starts with some given list of infinite binary
sequences. We can think of that list as a function that takes two inputs, a sequence
number, for example, two, and a position number, say three. Then, plugging in two
and three into this function looks at the second sequence and tells us what the
third digit of that sequence is. More generally, plugging in numbers i and j
into l looks at the sequence at number i on our list and tells us what the digit
at position j in that sequence is. Hence, l is a function whose input is a pair of
natural numbers and its output is just zero or one. Overall, this l is completely
representing our given list by capturing this entire grid of zeros and ones. Now
remember that sequences are merely functions from just the natural numbers
to zero or one. So, given this representation of our list, l, we can think of the first
sequence as always forcing the number one, or hard coding the number one, into the
first input of l. We write this as follows. Think of this as a specialized version of
l, where l's first input, the one corresponding to sequence number, is forced
to be one. Now, what is left is merely a function that takes in one input, much like
a binary sequence. In fact, this is equal to the first sequence on our list. Think
of the hyphen as a placeholder for an input and whatever goes in here goes in
here. So, to get the fourth digit of our first sequence, I can just plug in the
number four. More generally, the nth sequence can be thought of as a similarly
specialized l, where the first input of l is forced to be n and the remaining
input acts as this placeholder. This process of taking a function with multiple
inputs and specializing it into a function with few inputs is often called partial
application. This is because we can think of this process of specializing a function
as applying it on some, but not all of its inputs, leaving in room to still take in
more inputs and hence overall become a function on fewer inputs. Now, the nth
digit of this diagonal is simply the nth digit of the nth sequence. In other words,
l with inputs n and n. Our special sequence is the opposite of the diagonal. So, let's
create a function f, short for flipping, that sends a digit to its opposite. That is,
it sends 0 to 1 and 1 to 0. This is getting rather complicated. So, let's draw a diagram.
We have our list l from pairs of natural numbers to 0 or 1. Then we have f, which flips
digits. We want to construct a special sequence and sequences are just functions from natural
numbers to 0 or 1. So, we can create one last function from here to here, which completes
this path. Namely, this function takes the number n to the pair n comma n. This function,
while simple in its definition, is capturing the heart of what it means to be on the diagonal.
It sends 1 to 1 comma 1, the first position on this diagonal. It sends 2 to 2 comma 2,
the second position on this diagonal, and so on. Call this function d for diagonal.
Now, our special infinite binary sequence s, a function from the natural numbers to 0 or 1,
can be thought of as going along this path of functions in the diagram. In other words,
we are creating s by composing these functions, d, l, and f, which we can write as follows.
To see this in action, let's see what happens when we plug in 1. First, 1 goes to the pair
1, 1, the first position along the diagonal. Then, 1 and 1 are plugged into l, which gives
us the first digit of the first sequence on our list, in this case, 0. Finally, f flips
that 0 into a 1, and overall, the first digit in our special sequence is 1. More generally,
plugging in a number n into s gives us the opposite of the digit located at the nth
position in sequence n, which is exactly what we wanted. Hooray, we got there. Let us now
complete this version of the proof that uses functions everywhere. We are given a list of
infinite binary sequences, this function l, and we need to show that there must be some
sequence that does not appear in it. Remember that a sequence appears at number n on this list
if that sequence, thought of as a function on the natural numbers, is the same as the specialized
function l, partially applied in the number n. So, our assumption of suppose every binary
sequence appears on this list, translates to the following. For any sequence, that is,
for any function f from the natural numbers to 0 and 1, there is some number n for which l,
partially applied to n, is the same as the function f. Now, we have constructed one such
special function s, using the diagram and function composition. If our assumption holds,
it must be on the list. So, for the function s, there must be some number, let us call it k,
such that s, as a function, must equal l, partially applied on k. Now, let us see what
happens at the number k. Plugging in k into s, by our construction, is the same as plugging in k
into this formula, which gives us this, and once we expand out d, the diagonal function, we get this.
However, by our assumption here, plugging in k into s, must be the same as plugging in k into
this placeholder, where we get this. Now, we see two different expressions that should equal the
same thing, as applied to k. They look awfully similar, but this one is just that one, plugged
into the flipping function f. Since f sends 0 to 1 and 1 to 0, whatever the value of this
expression may be, this thing on the left, can never equal that thing on the right.
Stated another way, if this is 0, that must be 1, or if this is 1, that must be 0. We would
conclude that 0 equals 1, which is clearly nonsense. So, our assumption must have been
false, and this completes our proof. The set of infinite binary sequences cannot be listed out
one at a time. While we're here, let's just make two more observations. First, we can think of these
sequences, functions from natural numbers to 0 or 1, as subsets of the natural numbers. Given a
function, the corresponding subset is the set of all inputs for which the function's output is 1.
Similarly, given a subset of the natural numbers, the corresponding function is a function whose
output is 1 whenever its input is present in that subset. I'll leave it as an exercise for you to
show that this correspondence is indeed fair, and that, for instance, two different functions
don't have the same corresponding subset. With this lens of viewing things, the set of all possible
sequences corresponds to the set of all possible subsets of the natural numbers, which we often
call the power set. Now, we can interpret our list, which originally specified a sequence for each
natural number, as a function from the natural numbers to the power set of the natural numbers.
Then, our result says no function on the natural numbers can output every possible subset of the
natural numbers. Using function terminology, we showed no function of this form can ever be
subjective. The second observation is that our proof works even if we replace the set of natural
numbers with any other set. I'm glossing over some details here, but see if you can use the diagram
from earlier, replacing the natural numbers with some arbitrary set A, and convince yourself that
there is no function from A to the power set of A, which hits every single subset of A. This is
sometimes stated as some infinities are bigger than other infinities. No matter how big A is,
I cannot use it to exhaustively name every member of the power set of A. I like to think of it as
there is always a bigger fish. This theorem, called Kantor's theorem, was historically the first
example of a diagonal argument, and now we can find a proof template of what a diagonal argument
in other theorems might look like. The basic pattern in all proofs which use a diagonal argument is
something like this. First, we need to encode the stuff we're talking about into a diagram of the
following form. There's an underlying collection representing things we might be interested in,
and an arrow that resembles the diagonal function. Next, we have a powerful arrow from pairs of things
to something simple, the collection of some values. Next, we have an arrow that does something to
flip those values around, and using all of this, we construct a special arrow from things to values
by going along this path. Then, we use our powerful arrow and the encoding to represent our special
arrow in terms of a specific thing which we shall call k. Finally, we look to see what happens at k,
often to conclude that we didn't have as much power as we thought we did. This might seem super
abstract right now, so let's see another example of a diagonal argument in a different world,
the world of computers, to start seeing the similarities. On computers, we have data, things
like images, audio, pieces of text and so on, and programs, things that manipulate that data.
For example, we may have a program that takes an audio file and a series of pictures and outputs two
things, a video file which combines the pictures with the audio and a subtitles file which transcribes
the audio into text. Inside a computer, however, the little electronic pieces can't really tell what
an image is and what an audio file is. All they see are a bunch of zeros and ones which are stored on
tiny electronic bits, pun intended, as mathematicians who want to keep things simple.
We can imagine life at that level and think of any piece of data as a long sequence of zeros and ones.
Then, we can read the sequence in binary and obtain a natural number corresponding to that
piece of data which we shall call its underlying representation. So, every piece of computer data,
every file of any type has an underlying representation, some natural number.
Similarly, every natural number corresponds to a piece of data, if we see things in reverse.
Given a number, we can write it in binary and then set up the little electronic components
in the computer with that configuration of zeros and ones. We then have some data on our computer
whose underlying representation is precisely the number we started off with. Through this lens,
we can view programs, things that input and output data, as functions between those underlying
representations or functions between tuples of numbers. Now, programs aren't exactly the same
thing as functions for two reasons. First, there are programs which on some input get caught in an
infinite loop and never return an output or they just crash and return no output. If you have any
experience with programming or just use computers to do a variety of things, you've probably run
into these scenarios. This is quite different from mathematical functions because mathematical
functions have to give an output for every input. So, let's come up with a notion of partial functions,
functions that may not be defined on every input. Then, we can think of programs as
partial functions between the tuples of underlying numeric representations. If a partial function
returns an output for some input, we say it finishes on that input because the corresponding
program did its job to completion. And if a partial function is not defined on some input,
say we're trying to represent an infinite loop or a program crash, we say it does not finish on
that input. Note that the word partial may be misleading because to some, it may imply that
the function must not finish on some input. But we're not saying that. All we're saying is that
partial functions don't necessarily have to finish on every input. In fact, some partial functions
might finish on all possible inputs. And we call those total. They complete their job on the total
set of inputs we can give them. The second reason why programs are not just the same as partial
functions is slightly more complicated. Not every possible partial function might correspond to a
program. And this is because programs always have to do something determined by an algorithm
or some set of instructions that they follow. Defining this rigorously is a philosophical
rabbit hole that I'm not going to explore here. But for our purposes, let's just say that every
program must be written down using some programming language, which breaks down the program into a
sequence of explicit operations. For instance, the programming language might be equipped with
operations to add two numbers or to copy some text from one place to another, or to check if two
numbers are equal and do something based on that check. Since we're trying to model the world of
computers, we can assert that there should be programs for some simple operations because
one can very conceivably write some code for them. And we have examples of them in our real life.
For instance, there are programs that take some data and duplicate it. If you recall what the
diagonal function was, what we're saying here is that it's possible to represent the diagonal
function as a program. Similarly, there are some trivial programs that return no output,
no matter what its inputs are. Now, programs do resemble mathematical functions in one crucial
way. It's that they can compose. If we have a program that takes an image and crops it,
and another program that takes an image and makes it black and white, we can easily conceive of a
program that does both operations sequentially. Here's another cool observation we can make about
programs on our computers. Every program is specified by some set of instructions or code,
which itself is a file. Hence, at an electronic level, this is merely a sequence of zeros and
ones. So we can view a program just like data and assign to it an underlying numeric representation,
just a natural number. This may seem like a trivial observation in the world we live in,
where one has to download and install programs that occupy memory on a computer. But mathematicians
had this idea long before computers as we know it were constructed. Think back to the first
computers where programs were just huge mechanical constructions, input was just some buttons,
and output was just some flashing lights. In fact, thinking of programs as data is such a
powerful idea it has given a special name called the Goodell numbering, named after one of the most
influential mathematicians of all time, Kurt Goodell. Not every number needs to correspond
to a useful program. In fact, many numbers may just lead to garbage pieces of software that crash
immediately. But that's fine, our partial functions are allowed to do that. What's important, however,
is that every program is represented by some natural number, and the set of all natural numbers
corresponds to the set of all possible programs. While we're here, just a quick tangent. If you
recall from earlier, we showed one cannot list out every possible binary function on the naturals
with just the natural numbers. The analogous result here is that you cannot represent every
possible binary function with just a set of programs. And if you can't do that, you certainly
can't represent every possible partial binary function, or indeed, every possible partial
function from naturals to naturals, because those are even bigger sets. The reason this is true,
that the set of programs can't capture every possible partial function, comes from the fact
that our programs are countable. And in fact, we can list them all out using our underlying
numeric representation. Alright, tangent over. Now, given this powerful idea of representing
both programs and data using just numeric representations, we can conceive of even cooler
programs. Imagine a program with two inputs. The first input is some other program P, which we
can do because every program is secretly also some data. And the second input is some example data,
D. This program should output the word yay, if the program P finishes when we input D into it,
and should output whoops, if the program P does not finish when we input D into it.
If such a program existed and always gave the correct answer, its applications are amazing.
Let's call this program a for amazing. And look at a useful thing it can do. Say we had a maths
program M that takes as input a maths problem and outputs a solution to the maths problem.
However, sometimes this program gets stuck in an infinite loop, say in cases where no solution
exists. By using our amazing program, we can figure out if this is going to happen in advance
and avoid it. We give a our maths program and the maths problem and see what a says. If it says yay,
we should be fine. But if it says whoops, there's no point running our maths program and waiting to
see if it will ever finish, because a promises it never will. One might think that describing such
an amazing program is easy. And there should be a way of implementing it. Think of the following,
given an input program P and a piece of example data D, we can run P on D. If P finishes on D,
output yay. If P does not finish, say whoops. However, the problem arises because we have no
way of knowing if P does not finish on D. We need to wait for as long as P is running,
which may be forever. So our amazing program may never finish on some inputs defeating its whole
purpose. Now, one might think that we could cleverly inspect the contents of P and see what
is going to do on D rather than actually having to run it. But using a diagonal argument, we can
show that no matter how clever we try to be, we can never actually do this. Let's use our proof
template from before. The things we're going to talk about are programs and data, which we're going
to think of as our underlying numeric representations. Let's denote this as natural numbers with the
subscript rep to remind us that these numbers are representing things. The arrows between numbers
are simply partial functions on the numbers, which can be implemented as programs. The values in our
pattern corresponding to outputs of partial functions are also just pieces of data and so can
be thought of as just their underlying numeric representations. What should go here is clearly
the diagonal partial function, which remember, corresponded to a program that just duplicates
a piece of data. Note that the diagonal partial function finishes on each of its inputs and so
is total. Our amazing program A is clearly playing the role of this powerful thing here.
And so it occupies this arrow for numbers P and D. If the program corresponding to P
finishes on the input data corresponding to D, A will output a numeric representation for
A. Otherwise, A will output a numeric representation for poops. Our main assumption is that this
program A is total and it correctly finishes on all of its inputs. At present, we don't exactly
know what to put on this arrow. So let's leave it unspecified as some generic program F and we'll
choose what goes there in a bit when we are trying to finish our proof. Now, since the composition
of programs is a program, we can write a special program S that represents the program which does
these things in order. In other words, plugging in a number N into S is the same as plugging N and
N into A and plugging A's output into F. So we have constructed our special program using this
diagram just as before. Our next step is to encode the special arrow in terms of the things that
we're talking about which is possible thanks to the idea that programs are also data. Since S is
a program, it too has an underlying numeric representation, say the number K. Now, the final
thing our template tells us to do is to see what happens when we plug in K. One of two things could
happen. S could finish on K or S might not finish on K. Let's investigate each of these cases.
Remember, the program corresponding to the number K is in fact the program S. So if S
finishes on K, this means that the program corresponding to K finishes on K and so plugging
in K and K into A should output Y and following that through plugging Y into F is overall what
plugging K into S does and we can conclude that F must finish on Y because overall we're assuming
that S finishes on K. In the second case, if S does not finish on K, that means that the program
corresponding to the number K does not finish on K and so plugging in K and K into A should output
whoops. Since plugging K into S by our construction is going to result in plugging in whoops into F,
we can conclude that F must not finish on whoops because S does not finish on K. One of our two
cases must hold so either F finishes on Y or F does not finish on whoops. Now recall that we
have specified absolutely nothing about F except that it has to be a program. So our conclusions
about the behavior of F should hold for any old program but we can easily write a program that
does not have that behavior. Simply specify that the program enters an infinite loop if its input
is Y and output something specific like aha if the input is whoops. This program is now going to
do the effect of flipping which is what we were looking for to complete our proof. More explicitly
imagine our proof but with this flipping program F instead of a generic one by seeing what happens
when we plug in K into S we'll conclude that our flipping function needs to have some particular
behavior which it most certainly will not have. This is the contradiction that we were looking for
to conclude that our assumption was false. Such a powerful total program like A could not have
existed. So we have proved that there cannot be some amazing program that accurately and without
fail will tell you whether an arbitrary program will finish on an arbitrary input. Aha another
cool result proved using a diagonal argument. This is called Turing's halting problem and
is a fundamental result in computer science. Let's compare our two proofs so far to see the
similarities. They both have a similar structure and use the same pattern as before. A diagonal
arrow, an arrow assumed to be powerful and an arrow that does some flipping. In both cases we
constructed a new arrow using a composition of these arrows and represented that new arrow by
some number K. We found a contradiction by seeing what happens when we plug in K into our special
arrow. Remember that in the second case we figured out what program works in place of the
flipping arrow just by going through the pattern and deducing some properties that that program
must have. We then created a contradiction just by choosing F carefully so as to not have those
properties. In fact we could have done that in our first example too. If you go back and run through
our proof of Cantor's theorem without actually specifying what the function F does you'll conclude
that F must send 0 to 0 or 1 to 1 and this will be true for any function F from the set 0 and 1
to itself. So to get a contradiction forcing that F sends each digit to its opposite that is it sends
0 to 1 and 1 to 0 perfectly does the job. In fact this is often how the pattern is described in a
more general sense. If you imagine the different areas of maths as different languages there's
an area of maths called category theory that plays the role of linguistics studying languages
themselves. The linguists as in the category theorists describe many patterns in maths using
the structure of points and arrows between points which together form what they call a category.
Our first proof lies in the category where the points are sets and the arrows are functions
between sets and if you want to be precise about it our second example lives in a category
where the points are what are known as assemblies and the arrows are what are known as computable
functions. This notion of a powerful arrow in the first case a list that promises to be exhaustive
or in the second case an amazingly clever program also equipped with the encoding of programs as
data is captured by the notion of what is called a point subjective arrow. Defining that term is
tricky but what's important is that it's a property that shows up often enough in different
places of maths that category theorists have given it a name. Then there's a theorem in category
theory called Lauvier's theorem that says in a particular class of categories like the category
of sets or the category of assemblies having such a point subjective arrow results in this arrow F
always having a particular property. The flipping functions that we created in both of our examples
turn out not to have that property which leads to our contradictions that help us finish our
proofs. When you see a diagonal argument in an area of maths in some sense it's using
Lauvier's theorem as part of its proof. Now I just want to talk about how strong the idea of the
diagonal function is even outside a mathematical setting. Think of the following loosely defined
non-mathematical setup. Our things are going to be sentences and our powerful arrow G is going to
do the following. Given two sentences G outputs yes if the second sentence correctly describes the
first and no if not. For example say the first sentence was whales are big and the second sentence
was that sentence has three words. Then the second sentence is correctly describing the first and our
arrow will output yes. If however the first sentence was whales are very big then our arrow
will output no because the second sentence does not describe the first sentence. Now consider our
flipping arrow to be one that swaps yes and no that is it sends yes to no and no to yes. Now we
can construct a special arrow from sentences to yes and no using composition as before namely if we
input a sentence into our special arrow it will output yes whenever the sentence does not describe
itself. Many such sentences exist the most famous being this sentence is false called the
lyre paradox. If the sentence is true then its contents say it must be false but if it's false
then reading it as an assertion says something that is right and so the sentence must be true.
If we think about what's happening here the arrow G relates some sentences to others and what the
diagonal arrow is doing here is enabling a sentence to talk about itself that is it's enabling
self-reference. This idea of self-reference is extremely powerful and as a mathematician
friend of mine puts it allows things to eat themselves. Self-reference is also the basis
of an amazing book called Goodell Escher and Bach which is filled with diagonal arguments
and I would highly recommend it to anyone watching. The sentence this sentence is false
is also the main trick used in some of the most philosophically interesting theorems in maths
Goodell's incompleteness theorems which in some sense talk about what maths can and cannot prove
and they too use a diagonal argument. I want to cover Goodell's incompleteness theorems properly
and we'll make a separate video about that that really shows what's going on using the pattern
we discovered today so stick around for that video coming up. This has been a long video and I just
want to summarize what happened. We started with the proof that some sets are uncountable a pretty
cool result in of itself. By expressing that argument in terms of points and arrows we found a
pattern the pattern of a diagonal argument. We then reused that pattern in computer science
to prove another pretty cool result. Then we saw how the diagonal function allows for self-reference
which is the thing that shows up in several areas of maths. To conclude I just want to leave you
with one last treat an argument called Russell's Paradox which I shall also express as a diagonal
argument. Have a play around with it to see what's going on and if you can see the power of self-reference.
I encourage you to start looking for patterns and thinking about what patterns
crop up in different areas of maths. I know that starting to see things in this way give me
lots of joy and a deeper understanding and I hope you get the same out of it too. Thank you.
