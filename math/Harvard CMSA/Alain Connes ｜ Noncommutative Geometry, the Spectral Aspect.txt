Okay, so let me start by saying that I am really grateful for this occasion to talk about
non-commutative geometry, and I mean, I will concentrate on the spectral aspect of the
subject.
So somehow, you know, I will start by explaining the origin, I mean the spectra, how, you know,
it led Eisenberg to matrix mechanics, and emergence of time, as I will explain, which
is related to the ideas of von Neumann.
Now the next point will be, if you want the spectral paradigm, the new paradigm that comes
from dealing with non-commutative spaces, and which is spectra.
And this will be, if you want, analyzed and explained in two, at two levels, the first
the microscopic level, where it will give, if you want, the fine structure of spacetime
and the Euclidean level, and at the astronomical level, where it will sort of reveal, if you
want, the music of shapes.
And I will end by exhibiting, you know, a mysterious shape, which is related to recent
work with Gattaconsani.
Okay, so let me start by, you know, the old times, and I mean, this picture represents,
you know, like, what happened when, for instance, Newton was decomposing the straight line
of straight ray of light, if you want, coming from the sun, by letting it go through a prism.
And I mean, one obtains, you know, the rainbow.
Now what is really interesting about this rainbow is that, you know, when you look at
it very carefully, you find out that, I mean, there are some missing lines, I mean, there
are some dark lines.
I mean, at first one was discovered for sodium, but I mean, the real discovery was made by
Fraunhofer at the beginning of the 19th century.
And I mean, he exhibited about 500 of these dark lines, which are understood now as absorption
lines, in the sense that what happens is that when the light goes through some chemicals,
like in the neighborhood of the sun, then as a presence of these chemicals, and as a consequence,
which is that the sort of signature of the chemical appears in negative, if you want,
through these dark lines.
So somehow, I mean, a few years later in around 1860, what was discovered by Bunsen and Kirchhoff
was that, in fact, one could obtain the same lines, but now as bright lines over the dark
background, by eating a body, for instance, like sodium, and letting the light from this
heated body go through prism.
But then what happens is, if you want the negative of the previous, and I mean, they
were able, and people after them were able to identify many, many of the lines which
had been identified as absorption lines by Fraunhofer, they were able to identify most
of them as coming from chemicals.
So this means that each chemical has a kind of barcode that is its own signature.
And what they found also is that there were a few of these lines which actually would
not be pertaining to any chemical bodies that was known on Earth.
And so they invented a new chemical body, which they call helium, in the honor of the
sun, of course.
And what is amazing is that, at the beginning of the 20th century, there was an eruption
of the volcano, the Vezoglio, and then people did a spectral analysis of the lava coming
out from the volcano.
And amazingly, they found that the corresponding emission spectrum was exactly corresponding
to the missing lines before, and so it was helium.
And of course, now helium is used on Earth.
So I mean, this is just clearly featuring the fact that chemicals have their own barcode.
Now these barcodes were studied by physicists, and what happens is they have quite a remarkable
compatibility property, which is that some of these lines, when you express them in terms
of frequency, you have to be very careful that you should express them in terms of frequency,
not wavelengths, some of them actually add up.
And in order to understand how they add up, I mean, it's Ritz-Rittberg who invented, I
mean, who found, if you want, what is called the Ritz-Rittberg principle.
And the idea is that these lines would be indexed not by one index, but by two indices,
like, I mean, they could be Greek letters, they could be whatever you want.
And the point is that the Ritz-Rittberg principle tells you that the line with indices alpha
beta will combine with the lines with indices beta gamma.
So I mean, you see, the second index of the first line has to be the first index of the
second line, and then they combine and they give you the line corresponding to alpha gamma.
Now, this Ritz-Rittberg combination principle had one amazing consequence in the end of Eisenberg.
And what Eisenberg found out is that out of this principle, you get what he didn't know
about matrices, but you get matrix mechanics.
So why?
Because when you take the product of two matrices and you look at the index IK in the product
of AB of two matrices, well, this, the product is obtained by summing over all the intermediate
IJ and JK, which is exactly the Ritz-Rittberg principle.
Now Eisenberg, you know, was when he was in El Golan, which I will show very shortly.
I mean, he was preoccupied by the fact that, you know, he wanted to show that with his
formalism, he had the conservation of energy.
So I mean, he was doing calculations.
He was doing calculations when he was alone in El Golan, where he had been sent because
he had the A fever.
He had been sent by his university because, okay, there was no cure except to send people
in a place where there was no source of pollen.
So he was there and he had all the time he wanted to work.
And at some point during the night, I think it was like four o'clock in the morning, he
had proven that, you know, the energy is conserved because, of course, if you take H to be A, then
there is a commutation between these two terms and you will get that H is preserved by the
time evolution.
But then, you know, instead of going to bed, I mean, what he did was he went and he escalated.
He climbed over one of the peaks, which were along the coast.
And he waited for the sunrise on the top of his peak.
And he explained, you know, that, I mean, well, he was seeing, of course, in his mind,
you know, in his discovery, an incredible landscape.
Now what he had discovered had one peculiar consequence.
And that consequence was that because matrices don't commute, you know, when you work with
observable quantities for a microscopic system, well, you have to pay attention to the order
of terms in a product.
In fact, the order of terms in a product plays a crucial role.
And in fact, you know, if you come back to the evolution equation of Eisenberg, you find,
of course, that, you know, if everything would commute, this evolution would be the identity.
In fact, as we shall see much later, you know, the commutative world is static, whereas the
quantum world is dynamical.
This is the first instance.
Now, so in particular, you know, what it means is that the commutativity of Cartesian coordinates
does not hold in the algebra of coordinates on the phase space.
And this is one fundamental instance, you know, of the appearance of such a non-commutative space.
Now, as a corollary of, you know, this, you might think that this is very strange and that,
you know, dealing with this care with the order is something which we are not used to,
but this is wrong.
We are perfectly used to that in the language.
I mean, when we use words, we need, of course, to pay attention to the order of the letters
and the order of the words.
And I mean, otherwise you get anagrams.
So what I have shown here, you know, is a French anagram, which is quite amazing.
But somehow one can clearly see that when you go to the commutative, you lose meaning.
For instance, you know, I have written here that, for instance, Santa and Santa are the
same in the commutative world.
There are two As, one S, one N, one T. Listen is the same thing as silent and so on.
So in fact, what you find out is that this quantum wave, this way of being forced to
pay attention to the order of the letters is a way to keep meaning.
So in ordinary algebraic geometry, one forgets completely about these nuances.
Now one corollary, of course, of the non-commutativity of the Heisenberg uncertainty principle is
quantum variability.
And I mean, to understand this quantum variability, one needs to, for instance, give an example
that several Swiss engineers have manufactured a small device, which you can use in a mobile
phone, and which will generate random numbers.
But I mean, the way they will generate these random numbers is simply, you know, by letting
a photon go through a small slit and land somewhere on the photo cell, you know.
And I mean, which one of these cells it will land on is something which is totally unpredictable
by the uncertainty principle of Heisenberg.
And so what this gives for you is a way to generate random numbers.
And this way of generating random numbers cannot be attacked.
I mean, for security reasons, you know, it is a way which contrary to what you would
obtain if you would generate random numbers from a computer, I mean, it's totally different
because not only by experiment, but by the theory, you know that they are not reproducible.
So in fact, you know, there is this fundamental variability, which is in quantum mechanics.
And when you think about it, you will find out that, you know, quantum mechanics is in
fact a much better formulation of variability than ordinary classical mathematics.
For instance, if you ask a mathematician what is real variable, well, very often you will
get as an answer the fact that it's just a map F from some set X to the real line.
Now it turns out that this formalism is in fact rather poor because you cannot have coexistence
of discrete and continuous variables in this classical formalism.
So Heisenberg is very simple.
So Heisenberg is that, you know, if you have a continuous variable with a given X, then
this given X has to be uncountable.
And then, you know, any variable that is meant to be discrete will in fact take some value,
an infinite number of times, and in fact, more than infinite, more than countably infinite
number of times, so they don't coexist.
And amazingly, they coexist in the quantum formalism.
So if you want the continuous and the discrete coexist in the quantum formalism, because
in this formalism, a real variable becomes a self-adjoint operator in the Hilbert space.
And in the same Hilbert space, you can have a self-adjoint operator, which is, for instance,
a multiplication by X in the Hilbert space, which is L2 functions on 01.
But this Hilbert space of L2 functions on 01 is isomorphic to the Hilbert space, which
is the Hilbert space of little L2 sequences on the integers, in which you also have another
variable, if you want, which is a multiplication by n, which is self-adjoint, and which is
obviously discrete.
So if you want, because there is only one Hilbert space, namely infinite dimensional
with countable basis, well, what you find out is that there is coexistence of the discrete
variables with the continuous variables with the only proviso that, of course, they cannot
commute.
So, I mean, there is this nuance, and this nuance will play a fundamental role later,
as we shall see.
OK, so we have this dictionary, which is coming from the quantum.
And I mean, of course, the values of a real variable, it's just a spectrum of the self-adjoint
operator.
But physicists have been very, very early on capable of, if you want, of applying this
also to complex variables.
And in fact, they applied it to a very peculiar situation where you would like to have a complex
variable z, which is such that absolute square of z is a integer, I mean, this is related
to the Planck discovery in 1900 and to what Einstein wrote in 1906, which is that the energy
of an oscillator should only take integral multiples of h nu.
So I mean, in the end, so this oscillator was first understood in a paper of Born, Eisenberg
and Jordan, I think, in 1925.
And then Dirac was able to use this very same ansatz, you know, which is that you replace
a variable z, which was supposed to be a complex variable, replaced by an operator a.
And the only condition on this operator a is that its commutator with its adjoint is
equal to one.
That suffices to ensure that the spectrum will be formed of positive integers.
It's a little exercise.
And in the hands of Dirac, I mean, this allowed them to actually prove, you know, what Einstein
had guessed when he had guessed the constants a and b of emission and absorption of radiation
by an atom.
So I mean, this is, if you want, a very successful and very amazing formalism, which replaces
a classical formalism.
And I mean, there is something, in fact, which is quite striking if you want at the level
of the variability, which is that normally when we are pressed by people, you know, to
explain what is really the essence of variability, what is the cause of the variability in the
external world.
Well, the usual answer that comes, you know, I remember giving this answer when I was in
high school, the natural answer that comes to mind is just, you know, the passing of
time.
I mean, this is the only sort of reasonable answer we are able to give.
But now, you know, because of this intrinsic and sort of fundamental variability, which
there is in the quantum, comes a very natural question.
And this question is, you know, that, you know, of course, we have not been able in
the formalism of quantum mechanics to reduce this variability because of the reduction
of the wave packet, you know, which is something which is outside the time evolution.
So if you want this intrinsic variability in the quantum world, sort of poses a very
natural question, and this natural question is, would it be more primitive than the passing
of time?
Namely, how could time emerge from this quantum variability?
And what I want to explain briefly is that, you know, the study of subsystems, which was
initiated by Murray and von Neumann, you know, in the 1930s, 1940s, leads, in fact, to a
potential answer to this question.
So what did they do, I mean, what did they do?
Okay, I mean, this is just a picture, just to make sure not to forget that, you know,
von Neumann is also very, very well known for inventing the computers.
But what did they do?
They studied, they started by studying factorizations.
And in that respect, they were motivated by quantum mechanics.
So if you want, they wanted to understand that if you happen to have a inner space H,
which is a tensor product, which splits as a tensor product, then you can consider in
this inner space the operators, which are of the form T1, tensor 1, where T1 is acting
in H1, and 1 is the identity in H2, and, okay, somehow, you know, you want to understand
algebraically what are the algebras, which appear in this way.
So they motivate their work by quantum mechanics, by, of course, saying that you want to consider
observable quantities, which occur in a subsystem.
And well, then, of course, you are dealing with rings of operators, with algebra operators,
and you have a commutation between what happens in one system and the complementary system
and so on.
And, I mean, so they studied these factorizations, and the term factor comes from designing,
if you want, the algebras that you will have, that would imitate the situation of a tensor
product.
But amazingly, what Muren von Neumann found is that, you know, besides the factorization
which occurs from factoring the underlying inner space, it turns out that there are factorizations
which do not come from there.
And so the factorization that comes from factoring the inner space are called of type 1.
They are the simplest, by far, but they found two other types.
They found what are called type 2, and type 2, what does it mean, in which way, if you
want, the type 2 factorizations are different, are distinct from the type 1 factorization.
Well, they are very distinct because when you consider a type 1 factorization, after
all, the algebra will just be the algebra of operators in a given inner space.
So if you want, what would correspond to the subspaces are classified by the antigens,
by the dimension of the subspace, could be infinity, of course.
Now, in the case of the type 2, what happens is that what corresponds to the subspaces
are no longer classified by an integer, but they are classified depending on type 2 1
or type 2 infinity, either by the interval 0, 1 or 0 infinity.
And I mean, this is the first appearance of continuous dimensions, which I remember reading
a paper from Neumann when I was in Ecole Normale, and this really bear with me, I mean, the fact
that you have this continuous dimension which appears.
And then, okay, what do you have?
You have the type 3, and the type 3 is all that remains.
Okay.
All right, so, I mean, in fact, what came as a very important tool was the fact that,
you know, the link between the Boltzmann state, which is given when you consider all operators
in the inner space by the trace of x multiplied by the exponential of minus beta h, where
h is the Hamiltonian, and beta is the inverse temperature.
So this is related to the Eisenberg time evolution, which I showed you before, namely, sigma t
of x equals exponential i th x exponential minus i th.
They are related together by something which is, which can be formulated purely algebraically
in terms of the state itself and the time evolution.
And this is the Kubo-Martin-Schringer condition, okay, which is a condition which can be formulated
in terms of volumetric functions.
And I mean, a very important step was done by Tomita and Takizaki around 1970, when they
proved that, you know, this association of between a state and a one-parameter group
of automorphism actually holds for any fundamental algebra.
So if you take a fundamental algebra and take any faithful normal state on them, then there
exists a unique one-parameter group of automorphism that actually fulfills this KMS condition
of the association for beta y y y y y y y.
Okay, so, I mean, I started my thesis, and in my thesis, what I proved in 71-72, in fact,
I proved it in April 72, that in fact, you know, this one-parameter group of automorphism
is unique when you look at it in the quotients of the group of automorphisms, of M, divided
by inautomorphisms.
You see, when an algebra is not commutative, it admits trivial automorphisms, namely automorphisms
which are obtained just by conjugating an element by another, by a unitary element in
the algebra.
So by X goes to UX, U star.
And okay, because, you know, these automorphisms are completely trivial in a certain way, they
form a normal subgroup of the group of automorphisms, and the interesting automorphisms are forming
a quotient group, which is the group out M. So what I proved in my thesis, which was
under Jacques Lixier, I proved that in fact, you know, there is a unique, independent of
the choice of the state, automorphism from the real line to the group out M of automorphism
classes of M. And okay, I mean, you know, this is an amazing fact in the sense that what
it tells you, it tells you that just this algebra, just from its non-commutativity,
acquires an evolution.
And okay, I mean, you know, this of course, here did it gave me the classification of factors.
So I could define new invariants, and I could also reduce type three to type two and automorphisms.
In fact, I left one case open, which was later done by Takesaki, but I had defined two fundamental
invariants, so the module S of M, which is a close up group of our plus star and which
allowed to classify if you want the factors of type three to type three lambda, where
lambda belongs to zero one.
And I mean, the reduction from type three to type two, I did in the case where lambda
was different from one, the three zero case was particularly interesting.
And I also defined a group of periods, which is a subgroup of the real line.
But this time it's not a closed subgroup, so it can be quite wide.
And I mean, it's a remarkable subgroup in the sense that when it means that there are
certain times from the subgroup of the line, which are periods of the factor, namely in
which the factor doesn't move.
Okay.
So now, if you want, once I had done this work, I arrived in EHS in Buur, and I mean,
I found out that, you know, of course, I was a specialist of a specific topic, but the
people were at preoccupations, which were rather far from mine.
And I mean, I had the luck to meet Dennis Sullivan and to discuss with him a lot.
And after these discussions, I found, if you want, that there was a completely canonical
way to associate fundamental algebra, in fact, in most cases, a factor to foliations.
So foliations are very, very familiar objects in differential geometry, essentially what
they are, are decomposition as a product, but given locally only.
And what is interesting is not their local properties, which are trivial, but their global
properties.
And what was amazing is that this association, which I had found from foliation to factors,
was allowing me to exhibit the most exotic factors in the simplest case of foliation.
For instance, if you take the chronicle foliation of the torus, it gives you type 2 infinity
hyperfinite, but if you take, for instance, the annus of foliation of the sphere bundle
of a Riemann surface, this gave you the unique type 3-1 hyperfinite factor, which is extremely
exotic.
So on the other hand, you know, what happens is that this association from foliation to
factors and from fundamental algebra was only taking into account the major theory of the
foliation, but foliations are much richer in a way because they belong to differential
geometry.
So they have differential structures, they have topology, and so on and so forth.
And so, I mean, this led to develop geometry for spaces whose coordination was not commute
because when you deal with the algebra of foliation, of course, the factors you get
are not commutative.
I mean, this non-commutativity comes from the fact that, you know, you are allowed to slide
along the leaves and so on.
So this led me to a spectral version of geometry, which I want to present, and which is intimately
related to the formalism of quantum mechanics.
And as a warm-up, one has to understand what is sort of, you know, miraculous in this formalism
of quantum mechanics and why it can be so pertinent and so useful for doing geometry.
So one first thing which is remarkable is that if you read Newton, you'll find that provided
you read what he wrote in the quantum mechanical formalism, it will lead you to the, it will
immediately give you the right answer for what are infinitesimals.
So first of all, Newton was not interested in numbers, he was interested in variables.
So what he says is that in a certain problem, a variable is the quantity that takes an infinite
number of values, which are quite determined by this problem, and arranged in a definite
order.
And then he talks about infinitesimals.
And for him, infinitesimal is a variable.
So a variable is called infinitesimal.
If among its particular values, one can be found that this value itself and all following
are smaller in absolute values than an arbitrary given number.
Now, what is amazing is that when you apply this notion of infinitesimal, which is essentially
there, essentially defined in the words of Newton, then you find that they correspond
on the nose to a notion which is well known in operator theory and which is compact operators.
Because compact operators, well, they are variables as we saw because, you know, variables
were corresponding to operators.
But moreover, they have exactly the properties that Newton was saying, namely that if you
take their characteristic values, so the characteristic values, you know, they are like the eigenvalues
for the absolute value of the operator, and these characteristic values have the properties
that for any epsilon, there are only finitely many of them, which are larger than epsilon.
So it corresponds exactly to what Newton had in mind.
Now, in this formalism, you also have a rule immediately for what is an infinitesimal of
order alpha.
So for that, you look at the rate of decay of the characteristic values.
And for instance, you know, infinitesimal of order one is one such that the characteristic
value decay like one over N. This is fundamental for later because such things, I mean, they
are not traceable because, you know, the series one over N is divergent.
But when you look at their trace, it has a logarithmic divergence, and it is a coefficient
of this logarithmic divergence, which gives you something local.
Okay, so there is also the differential of a variable.
So the differential of a variable, okay, normally, you know, you try to differentiate
the function and so on.
But yeah, it's just defined for a bounded operator.
It's just defined as a commutator with the operator F, which satisfies two conditions,
the condition that it is self-adjoint and that the square is one.
So there is no content in the operator F itself.
What is really important is if you want the relation between the operator F and the operator
T. Because F square is one, you can easily show that the square of the differential in
a graded sense is zero.
And then you have the notion of differential k-forms, which are obtained just by taking
operators which are sums of products of k-1-forms, if you want, where they are defined in the
obvious way.
So many, many properties come out naturally.
But the most important was that this quantized calculus led me in 1980, 1981, you know, to
the cyclic homology.
And cyclic homology is really, you know, playing a fundamental role in non-commutative geometry
as a Durham homology in this non-commutative framework.
And I mean, I gave a talk in 1981 in Oberhofer, whose title was spectral sequence and homology
of currents for non-commutative algebra, where if you want all the basic properties, all
the fundamental properties of cyclic homology, follow, in fact, if one takes seriously this
quantized calculus.
So because in the quantized calculus, what you get is what is called a cycle, because
you can use the trace to integrate the differential forms.
And you get that this cycle is closed and so on, and moreover, you know, this cycle,
if you can integrate forms of a dimension k, you can also integrate forms of a dimension
k plus 2.
There is a distinction between even and odd cases.
And this yield to the operator S of periodicity and to the SBI going with a sequence and to
the corresponding spectral sequence.
So of course, you know, this is just one instance of the use of the quantized calculus.
There are many, many other instances, you know, where somehow, for instance, you can
do differential geometry on the grouping of a free group using the quantized calculus,
which is defined by the action of the group on a tree and so on and so forth.
So but these are tools, these are tools.
And now with these tools, we want really to come to the geometry itself in the metric
sense.
So the geometry itself, in the metric sense, we have to do a bit, a little discussion going
backwards to, if you want, the Riemannian paradigm and to, if you want, the way the
metric system evolved.
So the Riemannian paradigm is based, of course, on the Taylor expansion remain, I mean, at
this fantastic inaugural talk, you know, which, to which we shall come back later, in which
he had the insight, if you want, to define the metric locally by looking at the Taylor
expansion of the line element in local coordinates.
And in fact, I mean, he was looking at the square of the line element.
So there is a square root involved, and the distance between two points, as you know,
very well, is computed by minimizing, you know, the length of a path, like here, for
instance, between Seattle and London and so on.
And so, I mean, you know, it's a very concrete definition of length.
And somehow, you know, it fits very well with how the metric system was developed.
And I mean, typically, what happened, I mean, what I am showing you is the fact that in
the French Revolution, they wanted to have a unification, if you want, of the unit of
lengths.
So they, they define it as one over 40 billion times the circumference of the Earth.
And of course, they were measuring just an angle, they knew from the stars.
So I mean, this angle, this distance was between Dunkirk and Barcelona.
And two physicists, astronomers, you know, went along, and they made a concrete measurement
and out of this concrete, so they were the long one mission.
And out of this concrete measurement was fabricated a platinum bar, which was deposited near
Paris, you know, in Pavilion de Seve, and which was supposed to be the unit of lengths.
Now, what happened was very interesting, because what happened is that around the years 1925
or something like that, physicists discovered that the unit of lengths, which was deposited
near Paris and so on and so forth, didn't have a constant length.
And how did they do that?
Well, they compared it with wavelengths of crypto.
I mean, there is a certain orange wavelengths of crypto, which they used to measure this
platinum bar, and they found that the length was changing.
So of course, I mean, this was not a good definition.
And they shifted, I mean, the physicists shifted them in the conference on the metric system.
They shifted first to define the unit of lengths by means of this orange radiation of crypto.
OK, a certain multiple of these wavelengths.
But then they found out that there was a better way of doing it, which was with cesium.
And in fact, with this definition of cesium, you can buy, you know, in the store, some
apparatus, which will allow you to make immediate measurements of lengths with the precision,
which is perhaps 10 decimal places.
OK, so I mean, it's a big step forward.
And I mean, what happens is that there is a hyperfine transition, which is between two
levels.
And I mean, it corresponds roughly to a wavelength of about 3.26 centimeters.
And then you redefine the whole thing.
So in fact, what you do is you define the speed of light, you fix the speed of light
at this number.
Now, I heard that, you know, botany was furious when you heard that because you wanted it to
be 300 million.
OK, but the reason why you cannot do that is that there are already measurements which
are very precise, which are done so you have to accord with them.
So I mean, there is a strange value which is taken.
And then OK, then you define the second, of course, as a certain number of periods of
this radiation coming from the hyperfine transition.
And as a color of all that, you know, the meter, this unit of length is therefore defined
as being this proportion, you know, this rational number, this proportion times, you
know, the wavelengths of the hyperfine transition.
Now, what is extremely amazing is that this transition, which the physicists have done
long ago, you know, between the bar, the platinum bar and the spectral definition is exactly
parallel to the transition between the Riemannian paradigm and the spectral paradigm, which I
will explain.
And I mean, what is also amazing is that Riemann was incredibly careful in his inaugural
lecture to say that there was, I mean, he didn't really believe that his notion of metric,
if you want, would continue to make sense in the very, very small.
And the reason that he was advocating was that when you work in the, because he was
using solid bodies and he was using light rays in his theory.
So what he, what he was writing was that when you work in the very small, solid bodies
no longer make sense, and neither do light rays.
And so what he wrote, for instance, is that it is therefore necessary that the reality
on which space is based form a discrete variety, or that the foundation of the metric relations
be sought outside it in the binding forces which act on it.
And as we shall see, I mean, this, this will be exactly what happens in the spectral framework.
So, I mean, the possibility to do that, to transfer to the spectral framework with ideas
is in fact, you know, coming from work of Hamilton, Clifford and Dirac.
And essentially what it is, is a way if you want to extract a square root in the formula
of Riemann, where there is a square, if you want, of the line element, what would in fact
like to have, not the square, but to have the line element itself.
Now, it is possible to extract the square root at the level of the quantum formalism,
the level of operators, and it is possible thanks to Hamilton, Clifford and Dirac.
In fact, Hamilton was the first one to write really the Dirac operator because he had the
quaternions and he wrote, you know, i d by dx plus j d by dy plus k d by dz, which is
an example of a Dirac operator.
So, and the key to all of this stuff is that when you have two operators, x and y, which
anticommute, then in fact, you can write x square plus y square as a single square,
namely as a square of x plus y.
So, through the work of Dirac and also of Attia Singer, who defined, if you want, the
Dirac operator for arbitrary spin manifolds, then emerge the Dirac operator d.
Now, so in the in the spectral theory, the line element, which is like the square root,
if you want, of the Riemann d square, is an operator.
It is an infinitesimal when the right is compact and so on.
And what is it?
It is simply the inverse of the Dirac operator.
Okay.
So, of course, I mean, there are minor things one has to be careful about, you know, what
about the zero and so on.
But I mean, this line element is what is called the Fermant propagator.
And you have to think of it as a physicist write it when they write Feynman diagrams.
I mean, it's a very, very tiny little line, which is joining two points, which are very
close by.
And then out of this, or rather out of its inverse, which is the operator d, you can
compute the distance between two points.
And I mean, this distance between two points is no longer computed by taking the euphemum
of an arc joining the two points, but it's computed if you want by looking at the maximum
wave shift between the value at A and the value at B when you subject the waves to the
fact that their frequency is bounded.
So it's what is called mathematically speaking counter of the usual formula.
So you have this dictionary now that the line element, you know, is ds, which is this propagator
of fermions.
The distance is computable.
It's computed not by an infimum of an arcs, but by a supremum and by the way, notice
that then it applies to many more spaces because there are many, many spaces in which you cannot
join two points by an arc.
Think about, you know, a space which is disconnected and so on and so forth.
Whereas, you know, the formula by the formula on the right, the quantum formula does make
perfectly good sense.
And the volume, for instance, is defined as the integral of the power of the line element
that will be of order one.
And as I said before, when something is of order one, when an infinitesimal is of order
one, then it means that its trace is logarithmically divergent.
So what you do is you take the coefficient of the log divergency and this will give you
the volume.
Okay.
So, I mean, somehow, you know, also what one has to understand is that this formula is
in which geometry is defined in the quantum form is defined from the Fermin propagator
and so on, immediately allows you to understand how to incorporate the quantum corrections.
Why?
Because we know very well that the Fermin propagator, when we look on the field theory,
doesn't stay as it was before.
It acquires quantum corrections.
So, I mean, there are minute modifications of the geometry, which, I mean, are given,
say, by some kind of formal power series in each part, but which can be incorporated
in the spectral formalism.
So the spectral formalism is encoded in what is called a spectral triple.
So such a triple, if you want, contains three data, the data of an involutive algebra, which
gives you the space, essentially the coordinates of the space.
This algebra is acting in the inner space, the inner space is fixed, if you want, and
moreover, what you have is you have the inverse line element, which is the self-adjoint operator,
which is acting in the inner space H. So, in most cases, by the way, I mean, what you
will find out is that the representation of both A and D is, when you take them together,
is irreducible.
So this is a spectral paradigm, and, you know, what I want to explain is to illustrate the
power of this paradigm by a number of cases.
So the first thing which happens is that now, because you can talk about geometry for when
the algebra is no longer commutative, because, OK, I mean, you know, now you don't have the
GPU, which depends on X and so on and so forth.
So just because of that, you can see, you can look at the most simple example, the simplest
example, which is not commutative, is to replace the algebra functions on the manifold M by N
by matrices of this algebra.
So if you do that, and you just look at the algebra for a while, then what you find out,
you know, is, as I said before, when an algebra is not commutative, it has no trivial, I mean,
it has, it has this non-trivial exact sequence where you have the trivial automorphisms, which
are the inner ones, and which form a normal subgroup of automorphisms, and then you go
to the quotient, which is the outer automorphisms.
Now, when you apply this sequence, which is general, when you apply it to the algebra of
N by matrices of the manifold, what you obtain is an exact sequence where the inner automorphism
becomes a map from the manifold M to the group G, which is in this case a group S-U-N, you
know, if you take N by matrices, and then this goes to the group of automorphisms, and it goes
to different morphisms.
So what you find is that automatically by going to this very simple non-commutative extension,
you have enhanced the group of different morphisms to a group which physicists know very well,
because this is a group of invariance of the action functional, if they couple minimally,
gravity with a young wheel theory, with group S-U-N.
So, I mean, in our work with Alisham Sedin, what we found, we found also what was the action, what
is the action functional.
So what we found is that if we take the above case, very, very simple case of taking N by
matrices over a manifold, and if we look at the action that would replace the Einstein
action, which is a spectral action.
So this spectral action, I mean, it can hardly be more invariant.
It is an action which only depends on the spectrum of the line element.
What you do is you write, you know, the asymptotic expansion, and you get the Einstein action.
I mean, you get a cosmological term, which I will come back much later.
So you get this Einstein gravity, but you get this Einstein gravity minimally coupled with
the young wheel theory when you do the calculation.
And the young wheels gauge potential as they appear as the inner part of the metric.
So in exactly the same way as what I just said, that the group of gauge transformations of
second kind for the gauge group S-U-N appears as a group of inner different morphisms.
So you have this blending together, which just comes from, you know, having replaced
the algebra functions by matrices over it.
OK, so this is, you know, very enticing.
And I mean, with Alisham Sedeen, I mean, we have done a lot of work then, you know, with
Martin Marcoli, with Walter Van Seulekom, and also with Slava Mukano.
We have done a great amount of work in order to go much further than just this simple instance
of Einstein young wheels.
So in this work, I mean, there is an essential role which will be played by the real structure.
So what happened is that if you want, there is a reconstruction theorem that allows you to
reconstruct the manifold from the spectral data.
And in order if you want to restrict to spin manifolds, not to have to deal with spin
see or things like that, one needs to to incorporate a little decoration in the spectral
data, which is that of a real structure.
So it's an anti-linear unitary operator.
And we shall see what it is in the physics language and in the mass language.
But essentially, you also have to add another decoration in the case of even dimension, which
is a chirality operator.
So we have these two and they fulfilled some commutation rules.
And these commutation rules, in fact, they tell you that you are dealing, in fact, with eight, eight
fold theories.
I mean, there are eight possible theories, which in the ordinary manifold case, depend on the
dimension modulo eight.
And I mean, if you want the underlying conceptual theory is what is called K O
homology.
And the reason why this K O homology plays a fundamental role is that if you try to understand
that the conceptual level, what is a manifold in the ordinary situation of ordinary differential
geometry, what is a manifold, you will find out this is work which goes back to the 1970s, in
particular by Dennis Sullivan, you will find out that what you need to do first, you assume
that, of course, a manifold that's poincare duality in ordinary homology, but this is not
sufficient at all.
It only suffices to put, if you want, the space in question into Euclian space so that it has a
normal micro bundle, but this micro bundle is by no means a vector bundle.
And the difficulty in order to transform into a manifold is to elevate the structure of this
micro bundle into the structure of a vector bundle.
Now, to put things, you know, to encapsulate things very briefly, in the simply connected case,
what you find out is that the obstruction to do that is that you should have also poincare
duality in the deeper theory, which is K O homology.
Now, thanks to the work of Attila and Singer on the index theorem, they found out that the
representative of cycles in K O homology is in fact exactly given by the data that you need to
build a Dirac operator.
So, and that you have eight possible theories in this K O homology, and they are corresponding to
the various possibilities that I was exhibiting here.
So, in fact, you know, this J, this real structure has three roles.
So in physics, where people will recognize it as what is called a charge conjugation operator,
while working in Euclidean, you know, imaginary time.
Now, mathematically, it is turned out to be very deeply related to Tomita's operator.
Why? Because in the non-commutative case, what you want is that you want, you know,
the some restoration of commutativity in some way.
And how do you do that?
You do that by some kind of a trick, you know, you flip, you are able to flip the algebra to its
commutant by using this operator J.
So, I mean, and Tomita's theory, I mean, allows you to do that in general.
So, I mean, we prove the theorem that if you take, say, a factor in the space, which has
cyclic and separative vector, which is always a case in type three, then you can always find
such operator J that flips it to its commutant.
And finally, I mean, the deepest meaning, if you want, of this J, as I say, is to say that you
have punctuality in k-homology.
And I mean, this gives you, if you want, the fact that because of the J, not only have k-homology
cycle for the algebra A, but also for the algebra tensed on by its opposite.
And in particular, you have an intersection form and so on and so forth.
Now, it turns out that this has played a key role in the development of, you want, the
understanding of the standard model in the sense that, you know, usually when you work
with spin manifolds and so on, there is a link between the metric dimension and this
scale dimension modulate.
I mean, what happens is that there is a notion of metric dimension for a spectral geometry
and this metric dimension just comes from the growth of the eigenvalues of the spectrum
of the Dirac operator.
But there is also a k-o dimension, as I just mentioned, and it turns out that normally the
k-o dimension is equal to the metric dimension modulate.
But when you look, you know, at spaces of dimension zero, you find out that this is not
necessarily true.
I mean, you can fabricate spaces of dimension zero, but which are of arbitrary dimension
modulate.
Now, this could look as a curiosity, but in fact, it's not at all.
And it has played an absolutely key role in 2005 in our joint work with Shamseddin and
Markoley.
And what we have discovered, you know, with Shamseddin, we had been sort of abandoning
our understanding of the standard model in 98.
We had done it in 96 and we had been abandoning in 98 because of the discovery of
neutrino mixing.
And it seemed to be impossible to accommodate neutrino mixing with what we had.
But what we discovered in 2005, three of us, is that in fact, I mean, if you try the
values dimensions, k-o dimensions for the finite space, you know, that you put in as
a fine structure for spacetime, then amazingly, you find that if you take dimension six, k-o
dimension six for this finite space, which has, of course, metric dimension zero, then,
you know, not only the neutrino mixing comes out absolutely naturally, but also the
CISO mechanism.
And I must say, you know, I was amazed because I didn't know CISO mechanism.
And then I did a calculation of what we had.
And I rediscovered the CISO mechanism.
But unlike in physics, it's not put by hand.
You find it as a consequence of the calculation.
So what we did then later with Ali, we classified the various finite space of various
k-o dimension.
And of course, we were interested in the k-o dimension six.
And among them, we found one which we found extremely interesting, whereas the algebra
that was underlying the finite space, so it's a finite dimensional algebra, was two
by two matrices over quaternions plus four by four matrices over complex numbers.
And, I mean, in what we are doing, the breaking to the standard model gauge group was done
by what we call the order one condition.
But then by joint work with Vanser-Lecom, we analyzed the full model without reduction
to standard model.
And we found a beautiful Patissala model, which is, in fact, much more interesting and
symmetric than the standard model itself, particularly because of asymptotic freedom.
Okay, so in fact, at this point, we had found and we would, you know, be extremely
interested in having some kind of other way of finding the same algebra.
But this algebra is strange in the sense that the real dimension is different from the
two sides, you know, there are 32 and 16.
So, I mean, it looked like a very, very difficult thing to obtain this in a natural
manner. But this is what we did.
And this is what we did in our work with Shamseddin Mukano.
And, I mean, the idea, the new idea that came up there is that now we are not only going,
you know, to encode, if you want, all the momentum, assemble all the momentum together
as Dirac did, you know, using the Dirac operator, which was blending together all the
components of the momentum into a single entity.
But we investigate what would happen if we would do the same thing with the coordinates.
So, and what we obtained is an higher analog of the Eisenberg commutation relations.
And, I mean, what we first investigated was, as I will explain, you know, we wanted to blend
together the coordinates into a single operator.
So, we started, of course, with the finance slash.
Okay. And so, we wanted, if you want to assemble them into a single entity.
And what we found very quickly is that the right condition to, once they were assembled
because of the different matrices and the relation they were fulfilling was to have a
self-adjoint y or skewered joint, depending on the value of kappa, which is plus or minus
one, which would satisfy y square equals kappa, which is plus or minus one, and y star equals
kappa y. So, either it's self-adjoint or skewered joint.
And, I mean, of course, you know, this is based on gamma matrices.
And at first, it's very reminiscent of the sphere because then the components ya, they
have to satisfy the sum of their squares is equal to one.
So, at first, we wrote an higher type of Eisenberg equation, which is like, you know,
computation relation. In fact, if you want to understand what is behind this, I have to
come back to much simpler example, which is the geometry of the circle of length to pi.
It's an easy exercise to show that if you look at the geometry of the circle of length
to pi, it's uniquely specified by an equation, an operator's symmetric equation, which is
you start the commutator u is equal to one. u is a unitary operator, d is a self-adjoint
operator, and there is a unique essentially, having up to a parameter that plays no role
in the metric, irreducible representation of these relations into inverse space operators.
And when you compute, you find that the spectrum of u has to be the circle, and the operator
d defines the metric, and you find that the corresponding circle has lengths to pi.
And of course, I mean, you find, you know, this geometry in the right way. Similarly,
okay, we had started with, in fact, many years ago with Gianni Landi to do the geometry of
the two sphere in a similar manner by combining two biometruses with projection.
So, I mean, all this, in fact, you know, with Shamseddin Mukanov, we first started with a
finance slash, we wrote down this Heier-Hasenberg equation, which resembles, you know, the equation
for the circle, except that now the commutator or decommutator y is raised to the power,
which is the dimension of the space. So we wrote down this equation, and we investigated
this equation. And one of the first things that we found is that this equation exactly
like, you know, in the case of the circle, it was giving you the length to pi, while
it quantizes the volume. So the volume, which is given, if you want, by the growth of the
eigenvalues, or if you want, the logarithmic divergence of the trace of the right power is
quantized. So this is the first thing. But then, you know, we're a little bit disappointed,
because what we found is that when we have a solution to this equation, then automatically,
the solution will break, the manifold will break, as disjoint some spheres of unit volume. And I
mean, if you work in physical units, you will find that this unit volume is like the Planck
volume. So in fact, you know, at this point, we were quite disappointed because we said, OK,
well, look, I mean, spacetime, Euclidean or not, I mean, doesn't look like that. I mean, it's not,
you know, a union of spheres, a very tiny little sphere. But I mean, we had made, if you want,
we had forgotten, we had forgotten the essential piece of structure, which is the J, which is the
charge conjugation, the real structure J. And when you incorporate the real structure J, what you
find is that it automatically forces you to refine the Eisenberg equation. And I mean, because of
this issue, you know, of dimension six and so on and so forth, what do you find? You find that
you are forced to refine the equation by involving the J. And the J now is involving by, you know,
passing the projection, which is coming from Y to the committant. So the equation becomes this
equation. Now, what really came out of the blue is that all of this, what I said now was inspired,
you know, by trying to present the geometry in the simplest possible way. I mean, you know, by
having this kind of pairing between the Dirac and what you obtain by assembling the coordinates
into a single operator. And now we looked, you know, at exactly what are the needed
preferred algebras in order to obtain this equation, to obtain a solution of this equation.
And what we looked at the table of preferred algebras and what we found is that in the case of
dimension four, okay, so when you take five gamma matrices, then you find that in order to write
this, you have two preferred algebras which appear irreducibly. And I mean, the first one gives you,
in fact, M2H plus M2H, but because you want to take an irreducible piece, you have M2H.
And the second is M4C, and they appear altogether. They appear, if you want, as the sum of these
two pieces, C plus and C minus. So in fact, okay, out of the purely geometry problem,
we found exactly the algebra that was sort of put by hand, you know, in our previous work as a kind
of bottom-up story. So this was, you know, quite amazing. But then, of course, we had to go further
and we had to prove that we could obtain all possible spin manifolds from this construction
and no longer, you know, disjoint union of spheres. So what happens is that instead of having a single
map from the manifold M to the sphere, and you know, because the sphere is simply connected,
as I already mentioned, then you couldn't escape M had to be itself a collection of spheres.
But now you have two maps, y plus and y minus to the sphere. And the only condition is that when
you pull back the volume form of the sphere by plus and plus minus, and by minus, it's not that
individually they don't vanish, no, it's that their sum never vanishes. So their sum has to define
a differential form that never vanishes, not individually, of course, which is not possible,
unless you have a sphere. So very quickly, we obtained two results. We obtained the fact that
the volume was quantized. I will come back briefly to that. But we obtain much more precise fact
that if you take a compact oriented spin, remain a manifold of dimension four,
then the solution of this equation exists. If and only if the volume is quantized to belong to a
certain invariant, and this invariant is simply if you want the sum of the degrees of these maps,
five plus and five minus, which fulfills the condition that when you pull back the volume,
you get something which doesn't vanish. Now, after a lot of work, a lot of geometric work,
you know, which was using the existence of ramified covers of the sphere, and also
using the full power of the immersion theory, which goes back to Smale, Meenor and Poenaroo.
In fact, you know, a serum of Poenaroo that you have an open oriented manifold of dimension
n, then you can immerse it in Rn. So, I mean, then we were able to prove that in the case of
dimension four, any spin, for any spin manifold, this invariant will contain orientators m bigger
than four. I mean, the case of dimension two and three is much, much easier by, you know,
transverse general transversality arguments. But the case n equals four is more, more difficult.
And, okay, and so, I mean, what happens if you want is that now you can obtain any spin manifold
and of armitrae volume. So, if you take a smooth connected oriented compact spin,
four manifold, then this invariant contains orientators bigger than five. And what does it mean?
It means that, you know, you can sort of obtain this manifold from these two little spheres
of, of, of a blank size. But of course, the manifold itself will sort of develop and it will
develop to arbitrary size. And I mean, this is why we entitled the paper of, that we wrote with
Alisham said in the book, and of quantum geometry, you know, because, I mean, it's really what is
going on. I mean, there is little quantas which mesh together and so on to form this huge manifold.
Now, what happens also is that now, because the volume is quantized,
you know, when you write down the spectral action as we had written with Ali, as I said,
you know, in the spectral action, what you have is that there is, if you want, there is a cosmological
term, which is huge and which is quite bothering. But now, because the volume is quantized,
this cosmological term, which is the leading term of the spectral action,
plays no role when you write down the variational equation. And so, I mean,
when you write down the variational equation, you really reproduce the Einstein action
coupled with matter. And I mean, the geometry is reconstructed as a joint spectrum of this,
and it's a four dimensional, some manifold of this eight dimensional product of two very little
spheres. And, I mean, there are rather general facts which are key, if you want, in order to do
this reconstruction. There is a fact that the joint spectrum will be of dimension four. I mean,
this relies on deep results of Dan Boykoulescu. And also, it relies on the fact that, you know,
the index theorem will tell you that the volume will remain quantized. And I mean,
this follows from the fact that the fact that the volume remains quantized follows from the fact
that you have the Eisenberg higher condition, which gives you that this quantity is equal to gamma.
So, the gamma square is one, so they cancel out. But this is also an index. So, in fact, you know,
the reason why it's an index relies on the index theorem that we proved with Henri Moscovici
back in 1996. But in fact, one can use a less general result, if you want, because it turns out
that the chain character, the components of the chain character of the y automatically vanish,
the lower components. So, in fact, one doesn't need, if you want, in cyclic homology, the full
understanding of the index, one just needs the understanding of the achieved class of the index.
So, I mean, this is, of course, very instrumental in proving this result. Okay, so I hope I have
convinced you, you know, I mean, I want just to add one thing, which is that some physicists would
dismiss this, because at some point, we had made a home prediction, which was about the
mass. But there is a very interesting story, which is that with Alisham Sedin, we wrote
a survey paper in 2010, in which we were explaining the theory. And in that paper, we had a scalar field,
which, in fact, we ignored when we did the randomization group calculations. This color field
was appearing, you know, as a coefficient for the neutrino and so on. And I mean, what happened is
that, okay, so this paper is published in 2010. Now, what happened is that in 2012, Ali wrote
to me an email, and he told me, you know, it's amazing, because there are three independent
groups of physicists who have shown that if you add a scalar field to the standard model,
then you can recover the stability of the X scattering parameters, the positivity of the X
scattering parameter unification, which is exactly what was, if you want, contradicting our prediction,
the fact that it was no longer positive in the usual model. So, I mean, okay, I couldn't believe
my eyes. I didn't believe Ali and so on. So I checked, and all the signs were correct, and our
scalar field was exactly the right one, so that it would correct this prediction and make it
compatible with the actual value of the X mass. So, I mean, you know, the model is not at all
quite out by this. So now, let me come to large distances. I mean, Riemann was very, very carefully
in his talk, in his inaugural talk, to distinguish between large distances and small distances. So,
I hope I've made the point about small distances. And now, when we look at large distances,
I want to explain that the spectral point of view is equally relevant. For that, I will ask a
simple question, which is, where are we? By this, I mean, you know, how can we try to specify
the earth? So, you know, if we send, for instance, a probe in outer space, how can we specify where
this probe is coming from? Of course, you know, you can show the solar system with the third planet
and so on and so forth. You can show what we look like. But there is something which is much more,
much closer to the answer, I want to explain. And which is this picture here, where you have,
you know, these straight lines, which all go from the same point, and on which each of them,
you have a frequency, which is indicated. Now, this hinges on two mathematical problems. The
first one is, can one specify a shape, a geometric shape, by a list of invariants? So, you know,
if you try to specify, for instance, the universe or whatever the space, by giving a chart coordinate
system, and so on, this is ridiculous. Because, for instance, if you give a coordinate system,
you have to specify what is the origin. So, the question is completely circular. So, what you
have to do is, you first have to specify the shape by a geometric invariance, by a list of
invariants, and then, can one specify in an environmental point in a geometric space? Now,
so these are two mathematical problems. And I mean, the answer relies on two papers. I mean,
there is a paper of Milner, in 1964, showed that, you know, when you take a space and when you
take eigenvalues, when you take the spectrum, if you want, when you take the eigenvalues of the Laplace
operator or of the Dirac operator, that doesn't matter for this. Then, I mean, it turns out that
this is not a complete invariant of the geometry. He exhibited two spaces in dimension 16 that are
the same eigenvalues. And the reason is just, you know, modular forms and theta functions.
And then, there is another paper, which is by Marc Katz in 1966, which is kind of one here,
the shape of a drum. So, you know, if you take a drum, I mean, it will vibrate, it will have many,
many forms of vibration, which will depend on the, how many, you know, variations you have when
you go around and how many vibrations you have when you sort of go from the center to the external.
Now, if you want a sequence of eigenvalues, they grow. I mean, they're computable,
has zeros of Bessel functions, they form a kind of spectrum. And when you look at them for higher
and higher frequencies, I mean, they form a parabola. And this parabola indicates that you are
handling a form of dimension two. This is, you know, result of Hermann-Weil, which will be quite
instrumental later. Now, the answer, which I want to explain is what is missing when you only have
the spectrum, when you only have what I would call the scale, if you want, because you could do music
with shape, but you would have a scale which would be forced upon you, which would be this
very specific frequencies, which are given there. Now, it turns out that the missing information
that you need, that you are missing in order to reconstruct the space, the geometry with all its
properties, is in fact given by the relative position of two abelian algebras, operators in
Hilbert space. So there is, of course, the Dirac operator. The spectrum is uniquely, if you want,
embeddable in Hilbert space. But there is another. And this hinges on a theorem of von Neumann,
because von Neumann proved that, I mean, the result of von Neumann proves that if you take two
manifolds of the same dimension, it turns out that the von Neumann algebra of multiplication by
function, by measurable functions of the manifolds, they are isomorphic interaction, and not only
isomorphic as algebra, but they are isomorphic in the way they act in Hilbert space. So if you want,
the pair which is given by this algebra and the Hilbert space is unique. The pair which is given
by the Hilbert space and the Dirac operator is given by the spectrum. The only thing you are
missing is how, what is their relative position? And I mean, this relative position led me to
define an invariant, which is rather subtle to define, but which I can illustrate very simply on
an example, which I call the CKM invariant. And the reason why I call this CKM is because of
Kavibo, Kobayashi, and Masukawa, where if you want, using a similar invariant, when they
define their, you know, the thing that was actually breaking the C, Cp, you know, in the
star model. So in fact, the invariants are given by the spectrum of the Dirac operator,
but by something which is like, if you want, giving the possible chords on this spectrum.
And to illustrate this, I will show you in a very simple, of course, very naive example,
what it is. And for that, I will work in dimension two, because thanks to the work of Gordon Webber
and Wolpert, for instance, one as beautiful example of isospectral shapes in dimension two. So these
two shapes, for instance, they have exactly the same spectrum. And of course, you know,
they are not the same because here you have this protruding little square, which doesn't appear in
this other thing. So there is another example, which I would use, which is due to Shatman.
And the two shapes which I will use are not connected. So the first shape is this union
of this isosceles triangle and this little square. And the second shape is a union of
this isosceles triangle with this little rectangle. Now it turns out that when you compute
you find out that these two shapes, they have exactly the same spectrum. Okay,
each of them is disconnected, but they have the same spectrum. The fact that they are disconnected
will help me to show you what is going on. So when you compute the spectrum, you find that
when you write the squares of the spectral lines, you know, they have three types. So there are
three types of nodes, if you want in a scale. There are nodes which have fractional part one
force, making five force here, okay, or 17 force here. There are nodes which have one half as
fractional part, and there are nodes which are full integers. So in fact, I mean, you know, this
form, this form a kind of a scale like this, where you are like in a piano, you know, where you have
the black and white, but here you have like the blue, the red, and the yellow. Okay, so you have
three types of nodes. Now, as I said, they have the same spectrum, spectrum looks like this.
And so they have these three types of these three classes of nodes. And now the two shapes are not
the same. Why? Because the possible chords are not the same. What you find out, you have to think
a bit, is that the chord blue, red is not possible for shape two, the one which contains the rectangle,
but that this chord blue, red is possible for shape number one. Okay, you have to define what you
mean by a chord and so on, you know. But what it means in general, is that, you know, the idea of
of a point also emerges from this type of thinking, the environment is quite difficult, quite delicate
to define. But the and what emerges also is the idea of a point. So the idea is that a point in
the geometric space should be sort of as a correlation. In fact, it's given there as a
specific Hermitian matrix, but what it encodes is a scalar product at the point of the Eigen
functions of the Dirac operator. But what it encodes, if you want, is a correlation between
various frequencies. And this is very convincing. Since, you know, our face in the existence of outer
space is based on the strong correlation, which exists between different frequencies.
And for instance, you know, when we look at the Milky Way, we can look at it in visible light,
but we can also look at it, you know, in other frequencies like X-ray or infrared and so on.
And it's crucial that all these various pictures that we get, if you want, in different frequencies
are actually correlated to each other. So, okay, so, you know, this is what the additional
invariant is telling. Now, to make a little break, you know, when I was playing with these
various shapes and with scales and so on, I was wondering, you know, is there one
that would allow us to do music as we like it? I mean, you know, like on the notes of a piano
and so on and so forth. And of course, you have to know the minimal amount about the music, which is
that the ear is sensitive not to adding one, you know, like you would get in an arithmetic
progression, not at all. The ear is sensitive to ratios of frequencies. If you multiply a frequency
by two, you know, it's like you play on a piano, you play an A, and now if you play the same A1
octave up, you are in fact just doubling the frequency. And the ear is very sensitive to that.
Now, it's also sensitive to multiplication by three. And one fact, mathematical fact, which is
extremely used in music, is the fact that when you look at two to the power 19, it's almost three
to the power of 12. Of course, they can't be equal because one is even the other is odd.
But what it means is that if you take log three over log two, it's very close to 19 over 12.
In fact, 19 over 12 appears in the continued fraction expansion. So in fact, the 12th root of
two is very close to the 19th root of three. And it turns out that the correct musical shape
is the one that you can see on a guitar. You see, when you look at the guitar, you will find out
that the frets on the guitar, which are like here, they do not form at all an arithmetic
progression. They are not equally spaced. No. If you think a bit, you have to think you shouldn't.
And then you compare it, you make some measurements and so on. You'll find that these frets,
they are exactly the powers of this number, two, which is two to the power of one over 12.
And that the spectrum we are looking at, if you want this musical shape, what it should be
is exactly what happens with the frets. Namely, it should be the powers of this number.
Now, you can look at the shapes we know we are used to and try to find this spectrum.
You get nowhere. I mean, if you take the sphere, for instance, okay, the two sphere,
well, you get, you know, of course, the high frequency, it looks like a parabola and so on,
but you get nowhere. Why? You get nowhere because this musical shape, when you look at,
you know, the spectrum, I mean, it goes exponentially fast, of course, because it's a geometric series.
So when you look at its dimension, I mean, you know, involving the previous ideas that you have
to use a vile, a non-vile cell, you find that it has dimension zero. Dimension zero, you know,
it means that it's sort of hopeless to try to find it among the shapes that we know. But amazingly,
it does exist. It does exist in the non-commutative world. And I mean, it is the quantum sphere,
which is a deformation of the sphere, and whose beauty is due to the fact that not only, I mean,
it has, you know, the spectrum which we'd like to have, but also it has the symmetries we'd like
to have. They really like a sphere, which has a full group of symmetries, which is acting
transitively. The quantum sphere has a quantum group of symmetries, which is acting transitively
in the suitable sense. Okay. So let me now, you know, come to a very important topic, which would be
the, you know, like ending of my talk. And I mean, there is a book which I have written with my wife
and with Jacques Dixmier, which is a kind of prelude to this last topic. So this last topic
is a mysterious shape. There is a fairly mysterious shape, which I am showing you as far as its
spectrum is concerned, or, you know, and this is how it appears when you first look at it.
And what is it? I mean, you know, people will know a little bit of number theory. They will have
recognized the zeros of the Riemann zeta function. Now, this is a very mysterious shape. And I mean,
you have to admit that it looks a bit like a spectrum of some kind of Dirac operator. I mean,
this remark was made to me by Atia. And I mean, Zellberg, you know, tried to find, to construct
a surface where the spectrum would be quite related to this. He constructed a surface which,
because of a cusp, was related to primes. But when you compute with this, you find out that
there is a minus sign which appears. And when you compare to the explicit formulas of Riemann zeta.
So this sort of didn't quite work. I mean, this is the type of cusp, you know, that Zellberg was
getting and which was coming rather close. Now, the reason why I mentioned that is that a very
recent work which was done in the last month, you know, with Katia Kansani, what we have found,
we have found non-commutative geometry. But this non-commutative geometry is, of course,
given by a spectral triple. But the algebra is commutative. So the algebra is just the algebra
of ordinary, even functions on the interval from minus l over two l over two. I mean, you better
think of it multiplicatively. This is a bit better. So you think of it as functions, you know, on the
lambda, from lambda inverse to lambda by using the exponential. Okay, the Hilbert space is equally
simple. It's the Hilbert space of all l2 functions on this interval with respect to dx or with respect
to the multiplicative armature of this star lambda when you think multiplicatively. And what about the
Dirac operator? The Dirac operator is a very tiny perturbation of the ordinary Dirac.
And the ordinary Dirac is given by d by dx or lambda d by d lambda when you work on the exponential.
And this tiny perturbation is by a vial factor. You know, it turns out that you can write in general
the Dirac operator from a given metric to the Dirac operator from the new metric obtained
by just introducing a vial factor by the formula rho d rho. So this is the formula I'm using here.
And the vial factor is couldn't be simpler. It's of the form one minus p where p is a finite rank
projection, which is associated to even functions with support is between minus lambda and lambda.
Now I am in the multiplicative framework and with support of the Fourier transform is also
in minus lambda lambda. So one has to be very careful. I mean, this is impossible, but it's
possible up to epsilon and this leads to prolet functions. Okay, so what we did was we defined
this spectrum triple. It depends on the length of the interval and we were able to compute the
spectrum, the corresponding spectrum of Dirac operator only in very simple cases because
the formulas for the prolet functions are quite complicated. So we did compute it for
small values of L and to our amazement, you know, I mean, one of the two things, yes,
as this thing is a spectrum of, I mean, is the zeros of data. And this is what we found
a spectrum in the first example. And in the second example, we had an even better coincidence
between the two. So we are now, if you want exploring this coincidence, trying to understand
in which sense, in the limits of two spectra coincide. And that, if you want, is just the tip
of an iceberg in the huge programs that we are pursuing with Katerkonson on the Riemann data
function. And which of course, I mean, has many connections to non-competitive geometry. But
finally, if you want, it had a connection with a spectral point of view. I mean, it had many,
many connections with other sides of non-competitive geometry. In fact, you know, to the singular space,
it's like the ideal class space, and also to top of cell and content, and so on and so forth.
And so if you want to, this is a spectrum of work, which is so similar, you know,
spectrum of our data operator, so similar to the zeros of data. And I mean, I want to end my talk
by mentioning two recent, very active developments. I mean, there is one development, which,
you know, I like very much, which is that when we develop the spectral action with
Shamsedin, I mean, we had the spectrum action is depending on the function. It depends very
little on this function, because you just use the asymptotic expansion. But we had no way to choose
a function. Now, it turns out that with Ali and Walter, we showed that in fact, the spectral action
is equal to the entropy of the second quantized fermions, but for a very, very specific test
function, which is related to the Riemann data function. And the other development, which I
didn't have time to mention, is the interplay of the curvature, which is typically curvature,
Riemann curvature, you know, the extension of the Riemann curvature to the non-competitive case.
There is a fantastic interplay between this curvature and the modular theory. The modular
operators that I mentioned, you know, with respect to time, evolution, and so on. And
so this theory is amazing in the sense that one has to compute asymptotic expansions.
And as far as I'm concerned, you know, I start working on that with
Polatretkov, Polakoen, at the end of the 1980s. And this was revived more recently
to prove the Gauss-Bonnet theorem in the non-competitive setup. This Gauss-Bonnet theorem was
proved in a particular case, but then was proved, you know, by Masoud Kalkali and his collaborators
in the general case. And I mean, this work, as far as I'm concerned, really, I mean,
acquired incredible substance in my collaboration with Henri Moscovici. And what we found,
in particular, is that the formulas were fulfilling certain finite difference equations
that were allowing, you know, to compute the functions of several variables which were occurring
in the interplay of curvature with modular theory. And, I mean, since these things can be put on
the computer and can be checked, I mean, we were absolutely amazed that, you know, the theory was
predicting some very complicated relations which were actually fulfilled. And, I mean, an higher case
of this was done in my collaboration with Fazad Fattizadeh, I mean, when we completed the A4 term
in the asymptotic expansion. So I will end with a diagram which shows, you know, the relations
between non-competitive geometry and other branches of mathematics. So, I mean, a non-competitive
geometry fits itself tremendously on its relation with physics, high-energy physics, as I explained,
its relation with number theory, you know, with the space underlying the
adult classes and so on, with operator algebra, of course, you know, from the very start,
with K theory index theory and fantastic KK theory of Kasparov, which is one of the key tools,
with, of course, algebraic topology, you know, geometry group theory and so on and so forth.
And also, I mean, with differential geometry, because, and there, I mean, you know, in all these
cases, there is a feedback, I mean, for instance, in differential geometry, what Scandalis and his
collaborators have shown is how much, you know, it is relevant to not only study manifold, but study
smooth corporates. And to study smooth corporates, you need non-competitive geometry. So, okay,
so I think I will end here, and I will thank you for your patience. Thanks a lot.
