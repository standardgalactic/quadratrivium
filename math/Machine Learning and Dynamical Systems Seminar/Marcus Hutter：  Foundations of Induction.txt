Okay, so welcome everyone. So it's my pleasure to welcome Marcus Tunter. We're talking about foundations of induction. Thank you.
Okay, thank you very much for inviting me and for the opportunity to talk here. And you said I have unlimited time.
I don't know what that means.
It means that you can talk as much as you want. I mean, it's not like you don't put an upper bound in your mind about the length of the same.
So, so I will talk about the foundations of induction. So it's a quite philosophical talk.
I hope you enjoyed it. Nothing really, you know, very new.
But I saw I asked you, what is the background to the diversity of the audience? I mean, the Turing Institute, are they all computer scientists or is it broader?
So this is, so this is like special interest group on machine learning and dynamical systems. So I think most of people are mathematicians interested in the mathematics of machine learning.
Okay, okay.
So, for one second, sorry.
Not everyone.
Sorry.
Not everyone comes from that background. I think there are others. I, for example, have a control and systems background, engineering backgrounds, and I would imagine that probably some others.
Okay, okay. But anyway, I will keep the mathematics to its minimal.
So, not much. So that should be, should be fine. So, okay, so first I will motivate and why it's important to have, you know, a single theory, which, which does induction in all circumstances.
And then I will critique some popular approaches.
Then I will introduce this year of universal induction.
And after each part, I think, you know, I can stop and and I mean, you can also ask questions in between.
And depending how much time you want to spend here with me, I can either stop after universal induction or then talk a little bit about the application to artificial intelligence, some approximations and cool applications.
So maybe I should cover the conclusions.
Okay, so first, what do I mean with induction and prediction. So here's some examples of the first three are induction examples.
So for instance, hypothesis testing or hypothesis identification is a problem of induction. So for instance, that's treatment x cure cancer.
So we have some observations and we ask the hypothesis x cure cancer, and we want to confirm or refute this hypothesis or do observations of white swans confirm that all ravens are black.
I mean, it sounds like a ridiculous hypothesis, but it's also one which we want to induce. So that's an induction example.
The model selection is also induction sample. For instance, I mean, we have two theories one, the orbits are circular, they're one one orbits ellipses, which one is correct, given some limited data, say, you know, hundreds of years ago.
How many ways let's do I need to describe my picture well, or which genes predict cancer. So these are induction problems that go from data to models or to hypothesis.
Parameter estimation is also induction problem. So what is the bias of my coin, right? I have coin flips and I want to further the bias real value parameter between zero and one.
Or what is the eccentricity of my orbit of Earth or so with all induction problems.
Prediction problems. For instance, sequence prediction have weather data, you know, days by day and I want to predict the weather tomorrow or the stock market tomorrow.
It's also a prediction problem.
IQ tests, many are addiction problems. For instance, one, four, nine, 16. What comes next? So that's a prediction problem.
Classification is also prediction problem. It's actually can be reduced to a sequence prediction problem. You just have the sequence of a feature and then the class label corresponding feature class label feature class label feature.
What comes next? So which class label. So it's actually a special form of sequence prediction.
And the question I want to discuss here is, is there a general and a formal and a complete and a consistent theory for this induction and prediction problem?
And, well, you know, induction is a very important problem and I'm working here on AI.
And it's a significant part, but it's not everything. So decision making, actions, planning, optimization or game theory, multi-agent theory. So that goes beyond pure induction.
And I will only talk about it if I have time. Just a check because all the videos is gone. Can you still hear me?
Yes.
Okay, good. Yeah.
Okay, so.
Okay, so why do we need a unified theory? Okay, so
first, I mean, if we come up for every new problem or every new class of problem, you know, with a new solution, that is pretty cumbersome.
And also if I have a plurality of theories, you know, it's prone to disagreement and contradiction. So if I come up, you know, I come up with my new theory for predicting this kind of problem and you come up and then we have, you know, different answers.
So what should we choose, right? And I mean, this is roughly how, you know, okay, I'm exaggerating, of course, a little bit frequent statistics work. I mean, this has, you have a rich set of tools.
And for each type of problems, they came up with sometimes ingenious ways of hypothesis testing and so on. But this is not, there's no overarching framework.
I mean, of course, it's not totally random, but compared to, say, patient statistics, which I come to, it is much more disorganized.
So also in general, look at any field, once you actually monetize it mathematically, then, you know, you just boost the field, right? I mean, so look, for example, it's logic or deduction,
which has been rigorously formalized to great extent. I mean, that's how we do mathematics, right? So without this formalization, you couldn't prove theories.
And I believe the same holds true, but they should hold true and actually does hold true for induction, okay? Also it provides a convincing story and conceptual tools for outsiders, right?
You know, if somebody asks three different statisticians and they come up with three different answers, that's not necessarily very convincing.
It's also necessary. I mean, induction is a significant part of science. I mean, probably half of science, maybe experimental design is the other half of science.
So if you can formalize induction, then we can formalize science itself and machine learning, of course, you know, also does this.
And also once we have this general universal theory, then we can relate it to this narrow or heuristic or more practical approaches of induction and that can deepen our understanding of the existing techniques.
And it's necessary for solving deep philosophical problems. I will probably only very briefly talk about the Black Raven paradox, which is a quite famous problem where most inductive systems have had problem with.
And often also universal theory or general theory is beautiful.
And finally, there's no convincing argument that this may not be attainable. I mean, that's all in a huge argument against induction and so on.
But you will see that induction is in a much better state than many scientists believe. Okay.
So here's a major comment about mathematics where those words, I mean, my talk is mostly words, although I prefer mathematics.
And this is a quote by true style. There's nothing that can be said by a mathematical symbols and relations which cannot also be set by words.
Okay.
The reconverse, however, is false, much that can and has been set by words cannot be put into equations.
And the reason is because it's nonsense.
So you cannot convert nonsense into mathematics. And so that's a good filter.
I mean, people talk a lot. And, you know, sometimes if you sit down and you have difficulties forming and formalizing mathematics, I mean, turns out because it was just, you know, just a sequence of words, which were not very meaningful.
Okay, we can weaken the statements a little bit and can say, maybe, sorry, that is true stuff, except in 1966. So maybe we can weaken the same statement and say it's because it's non science.
So I strongly believe that everything needs to be formalized in mathematics.
Before that, you can never be sure whether it's nonsense or not.
Okay, so here, that's maybe one of the most important slides or the most important slide. So here I try to, by the way, can you see my cursor?
Yes.
Yeah.
Okay, great.
So on the right hand side, I list deduction on the left hand side induction, and then various same properties.
And I tried to have an approximate correspondence. I mean, it's not perfect. I mean, induction deduction are very different, but I tried to make a correspondence between induction and deduction.
So for instance, the type of inference in deduction is what I call specialization of derivation. So we started in very general axioms, and then we derive various specific theories.
I mean, the theorems can also be general, but the theorems are always sort of more narrow than the original axioms.
Induction goes the other way around, right? So we have very specific sets of data, and we try to infer general rules.
Okay.
So what's the framework? The framework in deduction is logical axioms.
And in induction is the probability axioms, say the Kolmogorov axioms of probability, for instance.
And what are the assumptions? Well, indeed, so okay, so the logical axioms are universal, right? You know, like A or not A is equal to true.
I mean, this statement is a logical axiom, whatever domain you're dealing with, whether you're dealing with natural numbers or with sets or so on.
But then we have non-logical axioms. So for instance, we want to say the piano arithmetic, you have axioms for the natural numbers.
This corresponds to the, well, in the Bayesian framework to the particular prior you choose for your problem class, okay?
So what's the inference rule? Okay, there are many different logical inference systems.
But one of the sort of purest one is the Hilbert style where you put everything in axioms and you have actually only one inference rule and there is modus ponens.
I think Hilbert style has just three axioms which are propositional and then three, four for practical logic.
So it's very simplistic, it's very impractical, but very pure. So you only need modus ponens as an inference rule.
And in induction, you only actually base rule.
So I mean, if you believe in Bayesian statistics, the only inference rule there is is Bayesian rule.
And what are the results of deduction? What are the results of theorems?
And in induction is the posterior. You start with a prior belief and then you get a posterior belief.
So is there some universal scheme or something like this?
Well, in mathematics is Samuel O'Franco's set theory and all of mathematics has been formalized in set xd for instance.
I mean, you know, there's not the only starting point, but there's a starting point.
By an induction, it's the so-called Solomon of probability, which everyone should know.
I'll come back in a second and I will introduce it here in the talk.
This corresponds to sort of set us in deduction in importance.
And how do we do inference? Well, you know, in theory, you could have a universal theory improver which takes the axioms, you know,
it generates all kinds of proofs, it generates all kinds of theorems.
I mean, of course, nobody does that, but you know, you can generate all theorems in this way in principle,
in deduction that will be using the so-called Solomon of probability and doing universal induction,
which can be used for any kind of prediction problem.
So what are the limitations? Well, we know the logic is incomplete.
I mean, you know, I don't have time to go into all the details, but you know, it's incomplete.
The girly incompleteness, while and we will see that Solomon of probability is incomputable.
Yeah. Okay. So that's a problem.
Well, how do we get around this problem?
But in practice, I mean, we use our ingenuity to find proofs and we usually don't, you know, prove on a formal level,
but you use same informal proofs.
And for induction, you also use all kinds of approximations.
And I will also very briefly tell a little bit sort of what practical systems like NDL can be,
the minimal description-like principle can be regarded as an approximation of Solomon of induction.
And what's the mode of operation?
Well, in, you know, in deductive setting, you prove something and the inductive setting, you compute.
I mean, you know, this correspondence is not perfect, but I tried my best here.
Okay. My, the real point here is the takeaway point is that, yes, you can criticize induction and limitations and problems,
but whenever you criticize the left-hand side or question the left-hand side, you could equally well question the right-hand side.
Yeah.
So for instance, you know, what are the probability axioms?
You know, okay, why do we use Kolmogorov axioms of probability?
Why not something else like imprecise probabilities or fuzzy logic or whatever out there is?
But you can also say why these logical axioms, right?
Why is A or not A true?
Well, I mean, there is ternary logic, right?
Where, you know, have, you know, A or not, you know, where they have a first option.
And, you know, some logicians study this, but all people who use logic use the standard logic axioms and proof theorems.
There's never discussion in mathematics.
Oh, in this theorem, you use modus ponens or you use teratum nodata or something like this.
Yeah, I mean, this is sort of, this is fixed and given for everyone who uses logic.
But of course, logicians, you know, think about the foundations and the same in induction.
You can reason about the foundation and question it.
But for any user of induction, that should be absolutely unquestionable that the conmogorov axioms of probability correspond in being established as well as logical axioms.
Maybe let's pick something more controversial.
Say, what should we do?
Maybe base rule or something like this.
Okay, you know, they're frequently statistic to believe, you know, you know, that basis may be just an alternative or it's not right way to do or where the suppliers come from.
But then you can ask, why do we believe in modus ponens, right?
Why does A and A imply speed?
Imply speed, right?
I mean, it sounds ridiculous to question this.
But for me, it sounds equally ridiculous to question base rule.
Okay, so what I say is if you believe induction is not on a solid foundation, at least after I introduced Solomonov, then you should equally believe that beduction is not on solid foundation.
And that's fine, right?
I mean, either you question both or you accept both.
That's sort of my point.
By the way, please interrupt me anytime if you have any questions or comments about my.
I'll take advantage of that and I will, maybe if you allow me to ask a question here.
Will you elaborate a bit later about the incomputability of induction?
I mean, as much as I'm familiar with the completeness on the deduction side, for me saying that induction isn't computable is like a major learning for me from this talk.
I'm wondering if this is something you're going to dive into a bit later, or if you can comment on it now.
I can go a little bit slower over the part where I introduced some existing.
So for instance, as I mentioned already, minimal description length principle is a used regularized machine learning, regularized maximum likelihood approach.
And I mean, if you have a smaller model, okay, maybe more general.
If you reduce your model class, rather than taking the universal model class, then it becomes computable.
For instance, if you don't start with the piano axioms for the natural numbers of a set of C, but you start for a finite groups, right?
But then the theory is complete.
So you don't have an incompleteness on the right hand side.
If you just take smaller model classes, I mean, the simplest would be sort of a newly tetra class with a directly prior or so.
And this is, of course, very efficiently computable.
So this is the correspondence between the incompleteness and completeness on the right hand side and the incomputability and the computability on the left hand side.
And when I come to some examples, I will point out how you can overcome the incomputability.
It's related to compressibility and you can use it sort of from a golf complexity. You can use practical compressors.
Depending on time, I will go a little bit more detail, but you know, that is not like the incomputability in logic.
And this doesn't sort of invalidated all mathematics and maybe still continue in the same is true on the left hand side with the incomputability.
So there are ways around it.
Thank you.
Thanks.
So the incomputability refers to the universal induction.
If you really want to sort of, if you have this high goal on the perfect inductor for all kinds of problems, then and only then you run into the incomputability problem, like on the right hand side, only if you want to formalize all of mathematics.
Well, unfortunately, already the natural numbers to formalize falls into incompleteness, right?
But there are other like, like finite groups and where you don't have any incompleteness.
Okay.
So, yeah, my claim is that the foundations of induction are as solid as those for deduction.
Okay, so let me now spend some time criticizing alternative attempts to deal with the induction problem.
Okay, so my, my old time.
So I mean, you know, this is old work from the I think 1940.
So proper was good.
You know, at his time, probably, and he was very good at popularizing philosophy and, you know, most scientists don't really care about it.
So it was.
And so in this sense was good.
And his appeal was that, you know, the ideas are simple.
They're clearly expressed, you know, and here they're very noble and heroic vision of science.
So that's all positive.
I think I might him a sort of pop star among mine, among scientists, but you know, unfortunately, his peers ideas are, I would say, seriously flawed.
I mean, the falsification principle and what he calls corroboration, or at least misguided or limited and I will explain what I mean.
And furthermore, there has been before him at his time and after his time, much better philosophy and philosophers out there.
They didn't do a good job of advertising and he brought he brought, you know, very thick volumes about this problem.
And so I think it's time to move on for the scientists.
Right.
I mean, the philosophers dealing with induction and process and they moved on.
I mean, you know, proper still important, but, you know, they are aware of the limitations and other approaches, but it seems that scientists, you know, if they care about philosophy,
they all they all aside pop up falsification and that's, you know, the Holy Grail.
But it's not.
Yeah.
Okay.
So here's some references are some books.
We can recommend.
Okay.
So what's the problem with pop up so with falsification.
So, so first, let's call it, talk about the demarcation problem.
So the demarcation problem is what's the difference between a scientific and a non scientific theory.
And proper solution is falsification isn't so a hypothesis is scientific if and only if it can be refuted by some possible observation.
Okay.
In this sense, falsification is a matter of deductive logic.
So that is my data is my hypothesis inconsistent with the data inconsistent in the sense of, well, that that is just false.
Okay.
And well, problem one is that we all know now, many things are uncertain.
We have stochastic models and stochastic models can never be hardly computed in a hard way.
So, I mean, they can become unlikely.
So in the literal sense that the likelihood becomes small.
And, you know, maybe you install some threshold, but in the literal sense of being refuted by the data that doesn't work for stochastic models, which is what we care about in machine learning statistics.
And the problem to is that falsification is alone cannot prefer, say, a well tested theory, you know, over a brand new untested one.
So maybe, you know, I have a fear of how to build bridges, and then I have a brand new crazy one.
And, you know, maybe the theory is such, such particular that, you know, applies to all the existing bridges and makes vastly different predictions for future bridges.
And so both theories have not been falsified.
And Hoppe cannot make a distinction, at least that the principle of falsification isn't cannot make a distinction between sort of crazy theories and reasonable theories.
But of course, he was not stupid.
So he knew that we need more.
So he knew that we need a simplicity bias.
And he didn't like sort of apparently Occam's razor so much.
So, so he said we should prefer reasonable theories over obscure theories, right, makes sense.
And the proper before simple theories over complex theories or the simple is sort of the reasonable and obscure is the unreasonable ones.
Because I he believes that the simple theories are either to easier to falsify.
So what he does it equates simplicity with falsifiability.
So he's not advocating a proper what I would call a proper simplicity bias.
And the problem is that, you know, complex theories can also sometimes be easily falsified.
You know, and his argument was if you have a complex theory with lots of free parameters, right, which then can always be adapted to the data, then it is hard to falsify.
But let's assume I have, you know, the sequence one, two, three, four.
I fit a fourth order polynomial through it, such that in such a way that, you know, for input five, the output is 17 or whatever.
And compare that to the linear theory, right, and I fixed the parameters of this polynomial.
So I have a complicated six fourth order polynomial.
And that just has, you know, wise equal to x and I have x equal one, two, three, four.
Both of these models are equally consistent with the data.
One is just complex and the one is simple.
But as long as I don't see the fifth data item, I cannot rule out one of these theories, according to his.
Principles where he said complex theories are easier to falsify.
In this case, this complex theory, you know, equally hard to falsify or equally easy.
So we need a better principle of simplicity.
So now he says, yeah, sort of he weaseled around and said, yeah, you know, we need, he said, no, no, confirmation is not possible, but we need corroboration.
Okay, so let's see what that is.
Okay, so first he says, we can never be completely certain about sexual issues.
I completely agree, you know, you know, nothing is certain.
Even one plus one is equal to who knows, right?
Okay, that's maybe not a good example because it's a lot of mathematics, but you know, I observed three balls, right?
Is it really three?
Is it four because of an optical use that we can never be certain kind of a certain certain about.
Okay, that's fine.
Then proper skepticism.
So scientific confirmation is a myth.
Right.
And, you know, they are strongly disagree.
Then he says, well, there is no confirmation.
We can not even increase our confidence in the truth of a theory when it passes observational test.
So he doesn't even, I mean, I agree, you can never 100% confirm a theory and 100% sure that it's correct.
But he said we can also not even increase our certainty in a theory that is also not possible.
And, well, then it says no reason to bury induction is a myth anyway, but science doesn't need it.
Okay, so what do we do is what he says we need corroboration.
So a theory that has survived many attempts to falsify is corroborated.
And it is rational to choose the more corroborated theory.
Okay, I agree to that.
But either corroboration is just a new name for confirmation or what he's talking about is just meaningless.
So that's the only conclusion I can draw from this.
Okay, so what is missing is a theory of softly confirming statistical models based on data.
Okay.
I mean, from from proper theory, I mean, as a machine as machine is because of know what how to do that in in particular places, right?
I mean, you know, you have likelihood functions and model selection, all these kinds of things.
It's not that we don't know how to do this, but I'm talking about the very foundations.
Is there a fundamental theory which gets it always right in a certain sense.
Another very popular theorem in some communities is the no free lunch theorem.
You know, in some it's like a, you know, a Holy Grail or so.
And you always have to write a paper because of no free lunch theorem.
We cannot solve this problem in general.
And therefore we develop our very specific and very particular solution solution to this very specific problem.
And okay, so, so this no free lunch theorem is actually more no free lunch myth.
Okay.
And the reason is as follows.
So consider algorithms for finding say the maximum of a function, right?
You know, most problems are somehow optimization problems.
And then we compare their performance averaged over different functions.
So it's an unknown function, which we want to optimize for the classical machine.
Okay.
So, but now consider all possible functions.
And okay, you know, to make it simple, let's assume the domain is finite and the range is finite, you know, then there are finally many functions and you can really uniformly average over these functions.
Okay.
Just to keep it simple.
Okay.
And what the no free lunch theorem in excess is that if you uniformly sample a function from from this set of functions.
Then sorry, maybe I should pop that up.
Then any reasonable optimization algorithms performed equally well or equally bad.
Yeah.
So there's no I wouldn't prompt better or worse on the class of all functions.
Okay.
So the theorem is correct.
But what does it actually say this theory.
So, since we sample uniformly, that leads with very high probability to a total white noise random function.
Okay.
I mean, that's sort of how quite noise random functions are defined, right?
It's the main point, you know, uniformly and independently.
Okay.
So what the theorem says is the no free lunch theorem that there is no way of cleverly optimizing white noise functions.
Okay.
That's fine.
But nobody cares about optimizing the white noise functions.
So this is pretty useless the theorem.
Okay.
So it has no practical implications.
Oh, that's just that.
Okay.
So what is the solution to it?
Well, the solution is instead of taking a uniform prior, you take a universal prior and that will, you know, introduce what that means.
And this universal prior still has no specific domain knowledge.
I mean, that's the point.
Of course, if I have specific domain knowledge, we know the functions convex.
Then we know we can optimize it, you know, with gradient descent and so on.
And we can perform better with some others.
But the no free lunch theorem is detailed of any knowledge about our problem.
And my claim here is that this universal sampling is as close as having no prior knowledge as is needed for any practical purpose.
Okay.
I will explain that later.
Again, you know, please interrupt me or ask questions.
I wonder if I might just before you move on from popper just make a comment.
My reading of popper is very much that he was talking about a particular context and that context was planned experimentation in science and planned experiment.
If you look through the book, you'll see that those phrases coming up again and again.
Now within that context, by the way, I'm no fan of popper.
I'm just feel that you've been a bit hard on him.
But within that context, falsification makes sense in a practical sense, because if you have got a model based on data, then until it is falsified.
It is something you can use.
I think that's really what popper was saying.
If you take out some of the philosophical jargon that goes around it, and that has been useful, because otherwise if we couldn't falsify it.
And if we can't falsify it, we try very hard.
Then at that point, the model we've got can be useful.
And I think that's really what popper was saying.
So, okay, first, I have no problem with falsification.
The only thing is that I say is we need more than falsification.
And his collaboration is either at this guy's confirmation or it is just sort of words.
Okay, I have no problem with the falsification per se, but it's way too limited.
But I do believe that Popper had grand claims of how to do induction beyond just biology.
At least the thick volumes I've tried to read, they sound very general.
But of course, if I have a very limited domain of study, and my theories are very deterministic,
and I never dream up ridiculous theories, I confine myself automatically to sensible theories, then falsification is all we need.
I agree.
Yes, thank you. I mean, just one point if I may.
It seems to me that this context that he was working in, of planned experimentation.
A lot of what he says make sense within that context.
But once you move out of that context where you can't perform planned experiments, then I believe that's where Popper completely fails.
You know, I agree to you partly, but I think, you know, in the plant experiments, I could come up with crazy theories, right, which are super duper complicated.
And, you know, which explain all those things and I fixed my parameters and then, you know, the series also easily falsified as my examples with the polynomial.
But, you know, I agree that in biology for the plants, probably nobody is that crazy.
But okay, I mean, essentially, we agree, but outside it like a machine and there's this huge model of massive data, right, you know, then this breaks down.
And, okay, you know, I look at these books again and if it states clearly, you know, all what I say is applies to plant biology or whatever, right.
And I'm not making any bigger claims, you know, then, you know, I maybe should be less hard on him.
I think you may have missed, I said planned experiments, not plant, plant.
Planned experiments. Ah, okay, now, okay, that makes more sense now.
Planned experiments.
In other words, you don't take the data that you've measured, say on the climate, an interest of mine, or COVID, for example.
You can't perform planned experiments.
If at all on such things. And therefore you're confronted with data from which you have to make an inference. Now I don't think Papa was talking about that at all.
Sorry, he was not talking about.
He was not talking about observations that were made on systems that for on which you could not perform planned experiments, like observing the climate.
Okay.
Okay, that's interesting. I'll, I'll think about it, but even if I plan an experiment.
I still have to have a hypothesis or I plan to check whether treatment x cures cancer.
So, I mean, I planned experiment and my hypothesis is that x cure cancer. Right.
And, but I can have this experiment and can come up with crazy theories. For instance,
x only cures cancer if the person is one meter 80 in height plus minus one millimeter or something. There's another hypothesis.
I'm not up also with within plant experiments. I can come up with crazy hypothesis, which may be easy or hard to falsify. And if there is no simplicity buyers, we have a problem.
Yes. So I agree with you on that too. Yeah.
Okay.
Yeah, I don't think about a little bit more better than the plant experiment limiting to plant experiments makes any difference in my, in my, in my, in my view of.
Okay.
So, so we had to know for lunch.
And now frequentism, let's bash frequentism a little bit.
So, so, I mean, the basic idea of frequentism is that you have independent observations and observations.
And, you know, you ask for certain events, you know, how often, you know, in any experiment I saw head or a number one on my coin.
And I, and I count them and divide by n.
I take limit n to infinity. And I say this is my probability of this event.
Okay.
First problem is that this is sort of a bit circular this definition.
I mean, if I flip a fair coin, it does not always the limit goes to one half. It only goes to one half is probability one. Right. So remember, right.
So the limit exists. I mean, you know, it converges to the correct solution. I mean, even exists, right. It could even sort of have 0214081.
So maybe the limit doesn't even exist. Right. But of course, the limit exists with probability one.
And, you know, okay, I don't have to say more. But now what we have done is we have asked what is probability.
Well, we have explained probability, but use the term probability one. So but what does probability one mean. Right. So in this sense, it's a bit circular.
I know you can break the circularity. That is Kornos principle.
But I guess most have not heard about it. And it's actually called Kornos forgotten principle.
But I don't want to go into this in more detail, roughly what Kornos says is if something happens with probability one in your mathematical theory, then it happens for sure.
In reality, and let's assume we know. And so let's assume we agree on what does it mean something happens for sure. So assume the deductive side, right.
You know, true and false is clarified. Right. Then we have reduced the meaning of probability one to something where we know what it means.
So that's the way around the circularity.
Next, more important, it's essentially limited to IID.
And, of course, you know, frequentist push the boundaries, right. I mean, if you have a Markov process, right. I mean, then conditional IID and just stationary approach.
Of course, you can push the push the limits. But essentially, there's this element of that something has to repeat multiple times, and then the relative frequency leads to probabilities.
Third, of course, is the reference class problem. So yes, I can count these frequencies among similar events, but what I mean that no event is ever exactly the same.
If I flip a coin tomorrow, right, I mean, it's a different day, the coin has been used a little bit, or more, you know, plausible set patients, right.
So, you know, we've built some statistics over patients.
And so, but if you consider all, I mean, if you just put all in a big bag, right, you know, then we may make weak inferences.
But if we separate them according to symptoms and rate and age and ancestry, then then maybe, you know, we only have one patient left for each question.
And then, you know, we don't, we can't compute common frequencies anymore.
But of course, you know, in machine learning, we try to solve this problem by feature selection, generalization, and so on.
It's not impossible to solve, but I, you know, this is sort of the general theme that if you push the limits of the idea of frequentism, you run into problems, which you don't.
And that is my claim with base and Solomonov.
Next, statistical learning theory. So, one step up.
Pretty good. Right. But again, most of the statistical learning theories about ID data, probably even more than classical statistics.
You know, think about this concept of empirical risk minimization, pack bounds, we see dimension rather much complexity, a concept of cross validation, you know, train and test that split.
I don't have slides here, but I'll talk about why this is a bad thing to do.
It's mostly developed to ID data, and it's, you can push the limits, but I mean, not arbitrarily beyond.
Okay. And what they practice, you know, well, many applications are ID, that is true.
But if you think about, you know, agents acting in a world, reinforcement learning, and so on, or your self driving cars, you know, this data is not the data is not ID.
Yeah, we have to deal with this.
Yeah, real life is one long non stationary non ergodic trajectory, and we would like to have a theory right, which does induction and prediction properly.
I'm also in this more intricate settings.
So can I just ask the question there.
Yeah, sure. Just a question I was going to ask at the end, but just, you know, you're sort of similar topic.
Why do you think that prediction is actually possible at all in general, given that, you know, if I make a prediction today about what might happen in experiment, and then tomorrow I make another prediction like in the context of a repeatable experiment.
The history is different yet the time. The things are different today than they were tomorrow the length of time since, you know, I was born or since something happened or the president was voted it.
You know, things are changing and yet we still have repeatable experiments with set up prediction. So, you know, it's kind of like,
why do you think the prediction is actually possible given that the world is not the same today that it was yesterday.
Well, I will definitely come to that and show you how Solomon solves the problem but the short answer is outcomes razor and a bias towards the world is governed by simple rules.
And the simple rules doesn't mean that it needs to be ID or sort of the path is the same as the future, but that the future has a relation a computable relation to the past.
And of course, it is not perfect and you can make mistakes with this with this assumption, but the point is that let me get back to that when it's time.
The short answer is all comes right by solution.
Okay.
Can't be fighting terms of Kolmogorov complexity.
Okay.
To get it from philosophy to something you know which is in the realm of computability and machine learning.
Okay, so here's some other approaches, which I just want to briefly list.
There is the subjective patients who say, well, we have a subjective prior.
So we have a certain belief about something before we see the data then we see the data and then the updates and our prior to the posterior.
The other problem is there's no formal procedure how to get this prior.
So it's an incomplete theory.
Well, they are luckily objective patients.
I think they are right in spirit.
But I mean, if you don't know, there's no problem.
But you know what they usually use it is compact model classes.
And then they have certain criteria.
For instance, they say we want minimax optimality or we want a reparameterization invariant prior and that leads to so called Jeffery's prior or Bernardo's prior, which is uniquely falls out of the mathematics.
It has very good mathematical and statistical properties.
And that solves the prior problem.
The problem is essentially roughly speaking limited to compact classes, which means small class, right?
Open ended.
Okay.
So then there's the minimal description next principle, which is one version.
I think a very solid version of regularized maximum likelihood or the ML principle, which is similar, which goes in the spirit of universal induction.
I always regarded as an approximate practical version of universal induction.
Yeah, then sometimes you hear, okay, let's be pluralistic, right?
We have multiple theories, right?
You know, maybe a frequentist produces this and maybe, you know, we'd have a likelihood principle and we get this and then we have Pearson or so he says this and then the patient, which says this, you know, just be open minded.
But the problem is, you know, if you if you if you get multiple answers to a customer, that is not very satisfactory because not all answers can be right, right?
You know, either all of these theories need to provide confident or credible intervals or something and they should sort of have a good overlap in their confidence intervals.
Then it's probably fine.
Or they have confidence intervals which don't overlap.
Then one theory must be wrong and the other must be right.
At least, you know, sometimes if this situation is not and that is a very unfair situation or think about, you know, in physics, right?
Okay, you know, we have quantum mechanics and then field theory.
So, you know, we could say, well, you know, we have field theory for light as a wave and we have quantum mechanics provides as a particle.
And, you know, we get different answers and, you know, that says that sometimes they contradict each other and, you know, that's life.
But, well, luckily there's quantum field theory now and we get the, you know, proper answer.
So I think pluralism is also, you know, not the solution to this.
Deductive logic is not strong enough to allow induction.
That's sort of the falsifiability principle is sort of an inductive approach to induction, which is too weak.
There have been many systems developed like non-monotonic reasoning, inductive logic, default reasoning, and they all don't take uncertainty properly into account.
That is way too limited.
Our Carnab's confirmation theory is, I mean, given that it's, I don't know, it's from the early 20th century, was a good step forward.
Roughly speaking, it is Laplace rule.
I mean, yeah, I'm oversimplifying.
The base rule for the Lully-Cetter with the uniform prior or with the Dirichlet prior, so very roughly speaking.
The problem is it cannot confirm universal hypothesis.
It can predict the next element law simply.
But if you ask, for instance, like, you know, you see one black raven, a little black raven, a third black raven, a thousand black ravens.
And then you ask, are all ravens black?
And you ask this theory, then it says, absolutely for sure, not all ravens are black.
Not, you know, if it gives, you know, some, which you've actually answered this, 100% they are non-black ravens.
And that's, of course, a nonsensical answer.
Okay.
You could regard the Lomotov induction as a very sophisticated improvement of Carnab's theory of solving this issue and many other issues.
Okay.
Then, you know, machine learning of the data, you hear about the data paradigm, and we don't have to care about our algorithms and regularization.
I mean, you know, more data solves everything.
But I don't think so.
I mean, you know, the extreme case would be, you know, having a big lookup table, but I don't think you will get to AGI artificial intelligence ever with a sort of this lookup table approach.
We will never have enough data because we always need to generalize.
And the better our model and our prior and our regularization, the better we be able to generalize.
And it doesn't matter how much data we have.
There's also something, it's a bold style learning.
It's some old thing where you have deterministic hypothesis.
You sort them in some way.
It doesn't matter.
And then you look, you cross out all the hypothesis which are inconsistent with the data and you choose all the hypothesis which has the smallest index in your set.
And if one of the hypothesis in your set is correct, then eventually you get to the correct hypothesis.
That's very neat.
But again, it ignores uncertainty.
It doesn't work with statistical models.
And, you know, it doesn't tell you any how to sort this hypothesis.
It's an asymptotic and result only.
It's neat, but also very limited.
Okay, so to summarize the first part.
Yeah, I heavily criticized, you know, a lot of other approaches.
Hopefully nobody is too attached to some of these.
So I think they cannot serve as a general foundation of deduction.
On the other hand, of course, you know, they have been very successful in limited domains.
And these limited domains can be, of course, also, you know, quite, quite interesting and wide domains, but they are not universal.
So they have limitations.
Okay.
So now what, you know, it's always easy to criticize.
And it's a little bit pointless.
The crucial question is, is there something better out there?
Well, and indeed there is.
So that concludes my critique.
Any further comments or questions about this part?
No.
Okay, so then I started with the positive part of the talk, universal induction.
So what are the foundations of universal induction?
I'm already nearly at one hour now.
But I have unlimited time, so it's not really a problem.
To continue, there's no, there's no limit and people have to go.
They can go and then they watch the recording.
No, I will keep it.
I think it is officially two hours and I tried to keep it to maybe one and a half and then I allow people to ask questions.
Anyway, I don't have to go through all the slides.
That's fine.
Just the induction part and that should be fine in half an hour.
Okay, so foundations of induction.
So again, I have to be very short.
I have written a long paper on the philosophical treatise of universal induction, which gives a really nice gentle philosophical induction, but also presents all the technical results without any proof.
So it's simple as I was able to.
It was actually my second attempt to write a paper accessible to philosophers.
I mean, I guess it's also a big failure, but maybe it's, you know, simple enough for someone with a math background to appreciate.
About the philosophical part, I mean, the half of the paper is really just blah, blah.
Okay.
So foundations of universal induction.
So what comes later I mentioned already, I think I don't have to tell you what it is.
It decores principle of multiple explanations, sort of contradicting opus razor.
It says, if you have multiple theories consistent with the data, then you should keep all of these theories around and not just the simplest one.
So what do we do?
We have now two contradictory principles.
Next, I come to the contradicts in a second.
We need base rules.
It tells you if I have a prior belief, I have new data, how I should update my belief and this rule is super general.
No limitation.
But base doesn't tell you how to start, you know, what, which prior to start.
So that's a problem.
But Conmogorov, those that for us.
So Occam's razor says, you know, talks about simplicity.
But we have to formalize simplicity and Conmogorov complexly formalizes simplicity or more precisely complexity.
I will slide on this and come back in a second, which is based on Turing machines and universal Turing machine.
And since I'm talking to the Alan Turing Institute, I will not tell you what a Turing machine is.
Okay.
So and what Solomon of did is he combined these sort of two philosophical principle, the statistical principle, and then sort of the computational principles into one formal theory of induction of prediction.
Okay, and let's go through these different elements in a little bit more detail.
Before I do that, I would just guess that many of you haven't even heard now after I advertise Solomon of theory for 20 years and he developed it in 60s.
Right.
Which is, which is quite sad, I would say, because while it's incomputable, you know, it's like, you know, no scientist uses set as C in the daily life, but everyone should have heard about it.
And it's the same for induction.
So at least you have heard about it now.
And it's not only me who thinks that it is very important.
So the AI father, Marvin Minsky, he has a great endorsement of this.
And quite late in his life of 2010, there was a World Science Festival, the limits of understanding.
So there's the YouTube links and at the very end.
So, so first Marvin Minsky never worked on this universe induction.
So long enough, he did quite other things.
So it's not that he sort of, you know, his life work.
Right.
But what he said is, it seems to me that the most important discovery since girdle.
And that's quite a remarkable sort of bar.
Was it discovered by 13 Solomanov and Kolmogorov of the concept called algorithmic probability, which is the same as Solomov probability.
And this is a beautiful theory.
Everybody should learn all about that and spend the rest of their lives working on it.
Okay.
So that's a big ask.
And yeah, he never worked on this approach, but still he endorses is very late in his life and sort of I.
I heated his statement already.
So it started in 1998 to sort of rediscover Kolmogorov complex of Solomov induction.
And since then, sort of for 24 years, I'm working on it.
Not my whole life, but sort of half of my life I dedicated to this theory.
Okay.
So let's now go to the concept.
So Occam's razor.
I mean, you all know Occam's razor.
You.
I don't know.
I present this standard, you know, grew emerald paradox and also only half of the story.
So you have two hypothesis.
All emeralds are green.
And then you have another hypothesis.
All emeralds are green till the year 2050.
And they are after their blue.
And by the way, all emeralds, which have been found so far in the green.
Okay.
No.
Okay.
So we have two hypothesis.
They're both equally consistent with the data.
And they're both equally easy or hard to falsify.
So if we now go back to Popper, what do we do?
Well, Popper doesn't have an answer.
He says we should have a bias towards more easily falsifiable theories, but they're both
equally falsifiable and they're both equally consistent with the data.
So we need something else.
Okay.
And the else is Occam's razor.
So we should have a bias towards simpler theories, which I think is the most important principle
in machine learning and even science overall, and sort of maybe apart from logical deduction
and maybe experiment design, then sort of maybe Occam's razor.
These three things experiment to design logical deduction and Occam's razor.
I mean, this is the definition of science in my opinion.
No science without Occam's razor.
The problem is, well, how to quantify simplicity?
That's a nice principle.
And we could look at the sentence while all emeralds are green, look simpler than the
second sentence.
So maybe we should have a bias towards hypothesis one, which is roughly right.
And I know there is the...
Oh, it popped over to the next slide.
I mean, I know the group problem goes much deeper, but I will not talk about this.
This is a whole separate story.
So, I mean, what does simplicity...
Beauty or elegance or description length?
And, you know, we will go for description length.
Next, a Turing machine.
Okay, so I really have to skip this.
Okay, the Turing Institute.
I mean, a Turing machine is just an abstract, super-stip-down, simple form of computer,
which is still universal and can do everything.
So now information here can move over complexity.
So it's aimed at quantifying complexity or simplicity of objects and hence sort of quantifying
what comes razor.
It's the shortest description.
And so the claim is that the shortest description is the best explanation, so that's the
quantitative version of what comes razor.
And so let's take some Turing machine T.
We have some object X.
It could be a string X, or it could be, you know, a hypothesis, which is then somehow coded.
And we look for the shortest program, which describes this X.
So this T could be, you know, something a specific Turing machine which allows to input
English language sentences or something like this.
But what we are aiming at is at the universal description language.
So we are aiming at using some universal Turing machine.
And what you can show is that the description length of an object X with respect to a universal Turing
machine is always smaller than any other coding scheme.
Apart from some additive constants, which does not depend on X.
So that's the argument for using a universal Turing machine because it leads to the shortest
code possible.
Roughly speaking, what we want is, I mean, if you think about X's data, later X will be a hypothesis.
So we have a lot of data and we want to understand data.
So we want to extract or find regularities.
What does that mean to find regularities?
That means structure, but what does structure mean, right?
And if you think this through, that means you have simple descriptions.
You have 0, 1, 0, 1, 0, 1.
OK, what is that?
OK, 0, 1 repeats, right?
You have the disease of tie.
You have a small algorithm, right?
So in order not to fool yourself, you need to describe this data in a formal language.
And if this description is shorter than your original data, then you have found some structure.
If your description blows up your data, then probably you haven't done much.
OK, so this now quantifies comagore complexity.
I think it can also be very quickly based probability.
You have the likelihood.
You have a prior probability.
Then you observe some data.
And then with base rule, you can compute the posterior probability.
So base is great to update your belief, but it doesn't tell you how to start with your belief.
OK?
Well, if anybody wants me to be slow on this, I can do that.
Otherwise, I would just keep it.
So now let's combine this.
OK, so base misses the prior.
OK, so api-cura says keep all fields around.
And that would sound like, you know, choose a uniform.
Let's assume we have finally many hypothesis.
Choose a uniform distribution of hypothesis, which would say that, you know,
all MRLs are green till 2050 is equally plausible to the simpler here,
which, you know, makes no sense.
OK, so if you refine this with Occam's razor and we say,
well, we have a bias towards simpler hypothesis.
So we say the probability, the apriori belief probability in a hypothesis
should be 2 to the minus the comagore complexity of this hypothesis.
And there's a reason why it's 2 to, I mean,
it just, it needs to be a monotonic decreasing function.
And the reason why 2 to the minus k of h is the,
well, essentially the only choice of the best choice.
OK?
And if you're interested in universally solving the reduction problem,
then you would choose the comagore complexity with this universal Turing machine.
If you're interested in, you know, having a practical system,
then you would choose, you know, some more narrow Turing machines.
For instance, say, you know, there will be a different talk.
We have neural networks, right?
And we want to penalize maybe complex neural networks.
We can count the number of weights, for instance,
and from the MDL principle, we know that we have to code each weight
to a certain number of bits.
But this is just some very specific coding, right?
Which is very good in many cases.
And then you would, you know, use a Turing machine,
which is a compressor, actually, of this hypothesis.
And I have some application where you use standard practical universal compressors,
rather than this.
So you can have the universal Turing machine, which is incompatible.
You can have very specific ones for your practical application,
and you can have something in between.
And I will, if I have time, if I have not, I'll say now
that Li Bitani and students and others have developed
this universal similarity measure or normalized compression distance
where they use this idea.
And what they did is just replace the universal Turing machine
by standard data compressors.
So state-of-the-art data compressors, which are out there
as an approximation to this universal compression.
Sorry, can I have a question?
Yeah, sure.
So two different hypotheses can have the same length and description, you know?
Yes.
But then your, the way you define the weights doesn't necessarily give you anything, right?
So if they have the length, same, ah, so the two hypotheses having the same description,
it's not a problem.
They have the same, then you should a priori believe in them equally.
And then you see some data, and if the data favors one hypothesis over the other,
then base rule will shift the posterior belief towards one or the other hypothesis.
If the data doesn't tell you anything about the hypothesis,
then also a posteriori, you should be indifferent between the two.
So you get a posterior probability, which is just, you know, half-half or so.
The prior doesn't give you a probability or anything that converges at all, maybe.
Sorry, can you repeat the statement or question, please?
Like the P of h i, you can have many h i's with the same length, maybe infinitely many of them.
And if you sum all of them, that's not convergent.
Ah, no.
Yeah, yeah.
Okay.
That's a technical thing.
You cannot have, I mean, you know, there are only two to the power of n strings of length m.
Okay.
Right.
And that's the reason why I use two to the minus.
Assume you have two to the power of n hypothesis and you code them all in n bits.
That's all.
I mean, there cannot be more different ones.
And then I have two to the minus n and the sum over all the two to the n hypothesis.
And that gives me exactly one.
Well, now we have, of course, hypothesis of different lengths.
And so then you need a so-called prefix free code and craft inequality,
so that the sum will be one or bounded by one.
Okay.
Okay.
And another question is that it's not related to the group problem.
Like this length is also dependent on the conceptual scheme that you pick, right?
Like if grew is our concept, instead of green, then the group is actually the better, the shorter description.
Yes.
So going to this, this will go into a deep philosophical discussion.
And it's related.
So let's put it at the end.
This will be, this is a very difficult problem.
And I have not written a paper about it.
I have ideas, but I don't want to write papers with the problem.
I only want to write a paper if I have, you know, the mathematics absolutely right, which I still don't have.
I do think it does not pose a problem.
We, okay.
Okay.
Very briefly.
So you build a camera, right?
And it observes a color RGB camera and it observes, and this camera observes green.
I don't know any person who has built a grew camera.
Okay.
Of course, in principle, you could do that.
Yeah.
But people don't.
So if suddenly, or at some point, everyone builds grew cameras, you know, then.
Having a bias towards grew is actually probably the right thing to do.
Or if physics is like this, that in the year 2050, sort of the color switches.
Yeah.
So, so that's the short answer, but this is really, this is sort of a very deep and difficult problem.
Right.
The reason I asked is because you're calling it the universal something and that sounds like a very strong thing to say.
So I was just curious how universal this is because it seems to be dependent on the conceptual scheme.
So you're not talking about, it seems that I can have different choices of universal Turing machine, but more specifically about the group problem.
Do I see that right?
That's correct.
Yes.
Yeah, correct.
Okay.
So with a group problem, my, as I said, my short answer is.
You do not observe grew, right?
What we observe, I mean, now in the, in the computer age, you know, everything is big.
Right.
So you have a camera, it observes something.
This is your observation stream.
Right.
And that's your data.
If you now come and say, well, well, well, I fiddle with the data and say in 2050, I have a small program in 2050, right?
I will suddenly change this stream and make it sort of, you know, blue and green reversed, right?
I mean, then you have fiddled around and have increased the complexity.
Okay.
Maybe you don't have to fiddle around.
Maybe you do that in the camera itself.
Yeah.
But then you have to have fiddled with the observation device.
Right.
So my claim is that an observation device, which observes grew is more complex than an observation device, which observes green.
But, you know, even this you can counter.
I know that.
So let's leave it at that.
And, you know, I'm happy to have an offline conversation, you know, after the talk or, you know, some other day, because, you know, there are some things which always derail the discussion.
Right.
And this is one of them.
Yeah.
But yeah, it's a good point.
It's a very difficult problem.
So even more difficult than the, than the, than the black raven problem.
I just ask, will you, the shortest description length is a very general concept.
Will you later be talking about what that might mean, for example, if I'm looking at science, then most science is based on models of differential linear or nonlinear differential equations.
I'll be talking later about what you, how you mix between the shortest description length and say a differential equation, will it be the order of the differential equation, the number of parameters in it.
Will you be talking about that later.
No, not really.
I also have unlimited time.
I don't, I still think unlimited time is not enough to talk about it, but a short answer is so what you have to do is, so you have your data.
And, and you want to make sort of, I always say predictions, right.
Okay.
So what you do is and say you have a differential equation and you believe that your data is governed by this differential equation.
So what you have to do is now you have to write a program which discretizes time.
Then you discretize this differential equation and maybe it's not perfect, right.
Then you have imperfect prediction.
Then you have to sort of code the noise and so on.
But, you know, let's forget about this.
Let's assume it's perfect.
And then you look at the description length of your differential equation, which includes the discretization, which includes, of course, the background mathematics.
So it needs to include everything in principle.
So in principle, you really have to code it up, you know, on.
And if you want to be really honest on a blank Turing machine and not, you know, or in in Python where you have, you know, all kinds of libraries that do all kinds of stuff for you.
So you have to do that on a Turing machine and not to cheat yourself.
I mean, in practice, you can get away with a lot of things, right.
If you have a lot of data, because this is still just all of one complexity.
But that's the way to deal with, say, continuous theories to discretize them because your data is discretized.
That seems very abstract to me.
I mean, in the world I live in, I would be looking at a differential equation that expressed the dominant modes of behavior in the data that I was looking at.
Now, I can obtain a simple model.
I've written papers myself where I've used the term Occam's razor in the title.
So I'm all about the simplicity of a differential equation description linear or nonlinear, whatever you have.
Now, it seems to me that what you're talking about is very abstract and I can't see the bridge between what you're talking about what you described just could be very dangerous.
I don't I can see that failing.
I think you're probably referring to you have an approximate model of your data and this approximate model is of course not exact.
And at the moment, sort of at least, you know, the last two minutes, I told you this model should describe exactly your data, but this does not have to be the case.
So if you have models which only approximately describe your data, then what you do is you roughly speaking, you model the errors in your prediction.
And if you have say in the simple case, if you have a stochastic differential equation and you make probabilistic predictions, you know, then you just take the log loss, which then with arithmetic coding can be can be coded.
So you specify how inaccurate your theory is.
And then, of course, theory say as you have two theories, which are equally complex, but one theory is less accurate than the other than the less accurate theory gets penalized and not in the prior.
But in in the posterior then because that because well if you talk about specificity in the likelihood, or if you don't want stochastic models in error correction.
Yes, that's the normal approach that is used in estimating differential equation models from data.
Yes, it was described it. What I can't see is how the shortest description length bridges to that.
Are you going to be talking about it in other words.
Not also not really sorry.
So, so I mean, do you know the minimal description like principle.
I know that yes but I can't see how that helps me.
Okay, so, so I don't have slides here now so but so in the end what we have to do is we have to approximate comogorov complexity for it to practically work.
Okay, if I look at MDL what it does it is minus log probability minus log likelihood plus complexity of my model.
What this is, this is a two part code describing exactly my data right it describes the model in the in the complexity part.
And then it describes my data wire arithmetic coding of my minus log likelihood.
So this forms a two part code.
This is an upper bound on comogorov complexity was called go go complexity is the shortest code.
So this is also a code right it's just, you know, not the best code but you know, then you minimize of course over your model class so you get you know the best within the model class.
And that is how you bridge the gap between.
And this year so actually if you plug in this prior right and into base rule, and then you take the logarithm, you get exactly the MDL principle out, because you have the likelihood times the prior.
So if you take the log you get the log likelihood plus log prior and the log prior is just no log of two to the power which comes without getting to get chaos age.
We could perhaps discuss this some other time.
Yeah, happy.
Happy to discuss it.
Yeah, a separate meeting.
And one little comment there.
So I've actually spent quite a few years of my academic life trying to actually apply algorithmic probability in real context so specifically in like biology and different areas so I've got about four or five papers why like like you mentioned earlier using approximations of
actually so if I was interested in like how this kind of equation can actually relate in a real setting even in differential equations and biological molecular shapes and things.
I'll be happy to share some of that work.
Okay, that's fantastic so if you can share your papers also with me, you know, I would be curious to look into this.
Yeah, sure.
Okay.
So, okay, so now how does Solomon of so so I mentioned one way to I mean we can plug this into now the base rule and then take.
When we don't have to take the logarithm but you know if you take the logarithm because roughly speaking in the LL it's not strictly true but roughly speaking.
But I'll do that, some are differently so there are three different representations of so long of distribution and one is via, you know, multiplying with the likelihood and taking the base average and so on, and computing the evidence I don't do that here.
There's a very neat and simple, you know, presentation which I can, you know, it's, which I can put on one slide.
So Solomon of combined at all of this and he defined this universal probability distribution M of X, so X is a string, and you take a universal Turing machine, and you put random noise on the input tape and output tape and you put random noise on the input tape.
And sorry one.
And the Turing machine does something and put something on the output tape. And for some inputs, you know, if you produce X, of course, right, and you ask what is the probability, you know, the chance that this random input is such that X is on the output, and you define it as M of X.
So that seems like a rather weird and useless quantity. But what you can show is that, well, next, first, the next thing is, you now define a conditional probability distribution, you know, in the standard way, you say the probability of why even X where why the continuation of the sequence X.
And defining the standard way, you know, just how conditional probabilities defined as M of X, Y divided by M of X. Okay.
And for a sequence, so I assume we have given a sequence X one up to T minus one.
You know, say, you know, whether data sun rain, sun, sun rain or stock market data, we ask what is the probability that of the next, at the next time that X can, and you plugged it into M and you get a prediction.
And what you can show, and, you know, again, these are words, but I will have the theorems and actually have some slides, but again, I will probably skip this slide.
And I was asked to give a philosophical talk, which I did, and I can give another talk, you know, maybe on this workshop, which suggested, so then we have more time.
And so you can show in a certain sense that this is the best possible way of prediction if you do not have any a priori knowledge about your domain.
If you have a priori knowledge about your domain, there are various ways of integrating this domain knowledge into the theory.
And if there's a question or timing, I can very quickly go to this.
Yeah, here's some examples, which I already mentioned. So if you have IQ, you have one, two, three, four, five, what comes next, you plug it in.
And maybe it says eight or something like this because the sequence is too short, and it doesn't have any prior knowledge about, you know, a bias towards, you know, number sequences.
But, you know, once you have one, two, three, four, five, up to 10, maybe it will predict 11 or digital pi or so.
Which is pretty amazing, given this definition, you pipe random noise through a universal Turing machine.
So you uniform random noise, which there's no free lunch theory and then says, you know, it needs to, you know, use less stuff.
But if you pipe it through a universal Turing machine, you get an extremely interesting distribution.
Obviously, there's no a priori knowledge in there.
But you can predict in any domain and you want to make predictions in a certain sense, which I will not quantify here better than any other prediction system.
Okay, so here is, you know, some of these technical bounds.
Yeah, so you can, you know, you can prove regret bounds and you can show that any other predictor has larger regret bounds, you know, than the solomon of predictor.
You can prove asymptotic results, you can prove instantaneous results.
Then, in deterministic, so if you have a domestic problem, it works, you know, if you have an ID problem, it works.
If you want to predict the digital pi, it works.
There's some funny things which, you know, some really odd things which happen there, but if you think about it, it's reasonable.
Then, there's some arguments, you know, maybe these bounds are too large in certain situations.
And I have proved that this improved bounds to solve this problem.
And, you know, I will just skip over this.
Okay, this is it's often the, the, the black raven problem which I mentioned before.
So, while kind of confirmation theory or base Laplace model or any model which takes a continuous identity as a prior will predict that the probability that all ravens are black is identically zero, even if I only observe black ravens, which makes no sense.
Right.
So, while the universal prior converges to one.
Sorry, the salon of distribution converges to one.
It has very favorable statistical properties, it's reparameterization invariant regrouping invariant.
If you, if you know about the philosophy literature, you know that the problem of old evidence how to integrate it, the problem of new theories.
So, the problem of our evidence is in invasion statistics, right, you should first choose your prior.
Then you have your data, and then you update, but in practice, usually you have the data already before you start, you know, using your model class and the prior.
And the question is how can you do that and without cheating yourself or cheating others.
And there's no problem in this universal scheme, because this pie is just fixed once and for all is true to the mind to come to go of complexity.
That's right.
So, whatever data you have.
The problem with new theories is, you know, sometimes you use basic modeling and then you see, oh, my model was bad, you know, and I have to update the model and increase my model class, which is sort of strictly outside of the patient framework.
So, how do you deal with it?
And of course, there are ways of dealing with it.
But again, in this universal theory, you never have to do that because it includes all computable hypothesis and argument.
Well, I haven't actually made this argument.
I should have a sentence about it that we never ever have to consider non computable hypothesis.
What I mean is a hypothesis.
It can be stochastic, of course, but if it cannot be implemented on a computer, you know, then it's possibly meaningless.
Most likely meaningless.
And I mentioned already so you can also show that and it's better.
It's not just a great predictor in a variety of circumstances, but you can also show that it's better than all other predictors.
Again, you have to qualify that in a certain sense.
Also, this theory is rooted in computable distributions and computer environments.
You can even show in non computable environments.
And then, you know, again, there's a standard critique.
What if I have prior knowledge, there's two ways of intercooperating prior knowledge either I actually fiddle around with the prior, but they're fiddle around with this in one very principle way.
So instead of taking the common goal of complexity of two to the minus the complexity of the hypothesis.
I conditioned this on my prior knowledge and I can code up, you know, I have, you know, I have to write down my prior knowledge.
I can write it down in English.
It doesn't really matter.
So I write it down in any form.
This gives a binary string and then take the string and then I use the conditional common goal of complexity rather than the plain common goal of complexity.
But you don't even have to do that.
There's another way.
It just say, okay, my prior knowledge is also actually data and that just prefix my real data with my sort of prior knowledge data.
And I mean classical machine that I just would have a great difficulty because suddenly, you know, the style of data.
First, you have an English description of your medical knowledge.
And now you have data from some, you know, medical equipment or so.
So you have a big break, right?
You know, maybe transfer learning at some point gets to deal with this.
So that doesn't work.
So Lomonov doesn't care about this as long as there is a commonality or regularity across your prior knowledge and your real data.
So Lomonov would find this commonality or regularity and compress and then will exploit this prior knowledge.
And, you know, if you have not thought about common goal of complexity and what it does in this university, it's hard to see how all this works.
Yeah, but amazingly, completion takes care of virtually everything.
Yeah, then the standard critique is, you know, there are multiple university machines, which one do we choose?
And we choose one which is simple, but sort of it sounds like also a regression problem, but it's simple.
So in case of doubt, just choose Turing's original 1936 Turing machine.
And there are theories which says if you go from one Turing machine to another, as long as they're not sort of cheating in the sense of, I mean, I can have a universal Turing machine
which codes all of Wikipedia sort of in there, which obviously is a very big Turing machine.
So as long as these Turing machines which are constructed are reasonably small and you can quantify that.
So if the Turing machine, the universal Turing machine has 10,000 bits and have another universal Turing machine with 20,000 bits,
then the difference in performance in say the number of errors this system makes is at most of the order of 10 to 20,000.
It sounds like a lot faster in the modern area where we have, you know, give you a bit of data and there's no problem anymore.
So there is a certain problem if you want to predict small frequencies.
So for instance, if you want to use Solomonov to predict 1, 1, 1, 1, 1, 1, what comes next?
I mean, most people would say, I mean, unless, you know, you want to be fancy or say, okay, probably 1, right?
And Solomonov with a reasonable, decent universal Turing machine may not get it right.
I mean, maybe that is a little bit too simple, but, you know, maybe 1, 0, 1, 0, something like this.
Okay. But there are ways to get around it.
The point, the reason is why we, after 5, scene 5 once, believe that, oh, seeing another one is plausible
is because in our life we have seen so many constant sequences.
I mean, it doesn't need to be constant sequence of smooth constant surfaces, you know, things, you know,
constant sunrise every day, constant things, right?
And you could say this is prior knowledge, and if you put this prior knowledge in, then also Solomonov can predict short sequences.
But, you know, I'm mostly interested in large data and not in this small data regime.
And finally, of course, the criticism is that it is computable.
One second, and then you can ask the question, and I think there was some.
The incomputability, of course, that's a problem, but you approximate that.
And as I repeated, I mentioned MDL is a practical approximation of Kolmogorov.
And, you know, there's a classical MDL where you just, you know, each real number has one half log n code length,
but on a more abstract level, MDL is just a two-part coding, which if you have noisy data is the way to go and not just one part coding.
And so the Kolmogorov complexity can serve as a gold standard, and then you do approximations, right?
And, you know, from a more compression point of view, you can use standard compressors, which has been done.
And I have a slide on this if you want to, and I can show you how it is used in the context of similarity in Kamaludin Dingel.
Sorry, it's probably pronounced wrongly, sort of wrongly on this.
Okay, there was a comment or question.
It was just that you haven't mentioned at all whether this representation that you're going to use for prediction makes sense.
That is, can it be interpreted in a way that makes scientific sense, for example, you haven't mentioned that at all.
It's just a black box you're talking about.
You mean the Kolmogorov complexity? Yes, it's a black box.
But that is the world is not just a black box, and you won't get far in science if you only have a black box.
No, no, okay. So, yeah, of course, if you black, well, we get pretty far with neural networks and transformers, and it's a black box and it works amazing.
And we get, I mean, maybe it's not science, it's engineering.
So maybe they get very far in engineering, not very get far in science with this approach, and I agree to this.
But you wouldn't use, and you cannot use Kolmogorov complexity process in computable.
So what you have to do is you have to develop computable approximations.
And there you can either say, okay, you know, I continue to develop general purpose compressors, but I mean, they're not really general purpose.
They are tailored towards, say, text, and you have words, frequencies, or they are tailored to images.
And then you start to incorporate and bias it towards your domain.
But the point is, it's always the goal is always to compress as maximum as possible.
But from a practical point of view, you're designing these compressors, and by doing this, you also then have insight and you understand the problem deeper.
Okay, thank you.
Okay, but, you know, in AI, we're getting more and more two words, black boxes and not really understanding what is going on, which is very disappointing from a scientific point of view.
And, you know, you know, on my list is, you know, theoretically understand how in internally transformers, you know, store and understand, you know, to produce code like alpha code.
I mean, this is this is pretty amazing. I'm still baffled how that works right and I really would like to understand this.
Okay, so I think I talked a lot.
So that's the main part of the talk.
So we can have now some discussion about these things.
The other slides is about how to use this universe induction in the context of AI, where you have to make decisions and actions, and also some of the approximations, like this universal similarity metric but it is also very brief only and then final slides,
or maybe I should do that some some discussion about what I said.
And so maybe I take some more questions now. And unless there is strong.
Well, I mean, you can choose right sort of, I mean, I can go through.
Through through through this universal similarity metric and approximations and some, you know, applications you say some diagram here, or I just, you know, give you a detailed summary and discussion and conclusions, which is a couple of slides actually, what I've talked about so
either more content or more summary or free form discussion.
So it's up to you now.
It's up to you. I think it's, it's your call.
Yeah, so anybody wants to ask something or as a specific request that you want to know about universal AI.
I would be interested in continuing the discussion of a question of Peter young because this is the special group on machine learning and dynamical systems.
So it would be interesting to see how you would extend this or apply this in a practical way to learning dynamical systems for example, or for and more precisely like either some differential equation or some discrete time dynamics.
If you have a lot of data and then what would you do with you just like choose the order or choose like linear versus not linear stochastic versus deterministic autonomous versus non autonomous.
I mean, where would you, where would you place this kind of arguments like as you know, I know from my 20 or how many years ago, 25 years ago, physics study.
I know what you do with dynamic. I mean, I know what a dynamical system isn't that it's and but there are many different dynamics of society, you know, they are, they are stochastic ones.
They are deterministic ones and blah, blah, blah. So how you model this, I probably wouldn't, you know, dictate this to you.
You are very free to do whatever you want. The only thing is how you evaluate your models. Okay.
And, you know, do the model selection. And here, I mean, let's, let's, you know, start with the, with the, with the pure version.
And then, you know, discuss, say, maybe you have your specific, you know, loss function might have something or so.
So in the pure version, and it's easiest to explain in the stochastic assume you have a stochastic differential equation which I mean, I hope I get it right.
What do you do with dynamical systems? I mean, it's just a differential equation and you have your data and you make a prediction, right?
And this prediction, if it's noisy, it's of course never perfect, right? I mean, then you see the next observation, you make the next prediction.
Okay. So what you do is you compute the probability. I mean, you first predict.
Sorry, you have your data up to t minus one. Yeah, you train.
Okay, you have your model sort of whatever it does. And it predicts a probability distribution over the next observation. Let's assume the next observation is xt.
So you plug that in. You have the probability of xt then. Okay, even the whole history.
And ideally, of course, the model predicts one for the observed observation and zero for all others. That would be perfect prediction.
In the stochastic case, of course, that's usually the case. So what you do is you take minus log this probability and that is your loss.
You essentially use log loss, okay? And that's one important thing rather than, but maybe that's a side, I mean, you know, that's a separate issue.
So you do this online prediction, you add up this log losses, and that gives you the sort of the, how good your model is.
I mean, a perfect model always perfectly predicts and has probability one on the correct sequence and probably zero on all others and the log of one is zero, right?
So the perfect model is zero. Stochastic models, of course, never perfect, right? So you get some code length.
So now, if you compare different models, and one model is significantly more complex than another model, then you have to penalize the more complex model.
And strictly speaking, you should code up your model on the computer, which we do always anyway, and then measure the code length.
But of course, in practice, you can have shortcuts, right?
If it's, you know, the classical statistical, you have your one model, which has seven parameters, real value, and the data has 15.
The MDL principle says each parameter is penalized by one half log m for the NSU sequence length, you know, that's a good statistic.
I'll make a comment on what you just said. I think perhaps you, there are other areas where work, which you might consider to be parallel to what you've been talking about is going on and perhaps you're not aware of these other areas.
I think, for example, in the area where I started out, which was the control and systems area.
It's not going into multidisciplinary work, but in that area, there have been a number of developments which one can see as being parallel to what you're talking about.
So for example, one would find the simplest model in various ways.
One finds that, for example, with a dynamic system, that there is a dominant, there is some dominant modal behavior.
And the, what you're trying to do is to find this dominant modal behavior because it is in fact the simplest description of this system.
And then if you're going to try this out in prediction, as you're talking about, there are various ways of doing this. How do you choose the model in that regard.
There are things called information criteria, which you use, of which the most famous, but not necessarily the best is the Akaiki information criteria.
I think that the limitation, I enjoyed your talk very much, but I think its limitation was you have not been exposed to what you might call a parallel universe that's going on elsewhere, which has links with what you're talking about.
And that's why I asked you earlier about the bridge. And I think that it's in those other areas that perhaps you will find.
Things in the real world, not in the black box world, but are similar to what you're talking about.
Yes, that is most likely true.
It would be good if, you know, somebody, it could be me, but it could come with somebody else, right, take some concrete, you know, approach, and as you mentioned, and see how it fits into this framework.
I would guess that in many cases, your model selection is probably not that critical that you have to be so concerned about maybe getting the regularizer exactly right or using the code length.
So the holistic which has been developed in the field, right, because, you know, maybe your models are so good and you have enough data that the model complexity is really absolutely not an issue.
And then, you know, I'm not claiming that everybody should use Solominov and only saying that if you have a conceptual problem in your field, where different people disagree on what is the best model or, you know,
then going this step to the more fundamental theory can help decide this.
One example where I thought about, you know, writing a paper is, you know, string theory where the standard model, sorry, I cannot give you examples in dynamical systems, but they string theory where the standard model of particle physics, right.
And the claim is that the string theory is the always best or string theory doesn't make a prediction, right, which is not really true, right.
I mean, the problem is, I mean, maybe now I'm talking to you and you don't know these details, but, you know, you have to compactify this many faults and there are, you know, billions of ways, and this has to be put in the theory.
But once you do it, it has exactly the same predictive power as the standard model.
Okay, so now you put something extra into string theory, but well, in the standard model, on the other hand, you have 23 masses and coupling constants, there are also a lot of parameters.
So which theory is better, right.
And I mean, this, this fight goes on for, since string theory exists, right.
And it could be solved by looking, you know, at which theory is actually more complex or more simple.
In this case, it probably doesn't make a big difference for practical purposes because they are so close.
But who knows, I mean, that's one, one, one example where, where this could help.
I mean, there's a very, yeah, physics example.
And more in machine learning.
An example we have, I mean, now we have the problem, you know, the double descent phenomenon and these neural networks are bigger than the data.
And, you know, they're overfitting, but it seems to be sometimes be nine and sometimes it's not be nine.
So what do we do?
And so what we started here project is to evaluate to the different models.
And, you know, even GPT-3, right, I mean, after they trained the network, they realized, oh, there was tested contamination.
Okay, now I'm talking about tested.
So that is another paradigm I haven't talked about.
I mean, in practice, people use training and test data.
And, but if you can have easily tested contamination and, you know, another talk, I think it's not even well-defined whether tested is contaminated or not with training with data.
There's an ill-defined problem in full generality.
Anyway, so, so this theory about comparing theories based on their total code length, that means sort of the log likelihood plus their complexity,
helps in selecting better models.
Because the neural networks, they have the problem that they're all bigger, right?
So, so what you have to do is some trick in order to make this model small.
You know, it's going to take me probably, you know, five minutes to explain it.
But yes, yeah, I mean, in the ideal world, I would have the time to go to these other fields.
But I don't think, I mean, I'm not sure what your, what, what your claim is.
Are you saying that this alternative approaches sort of, you know, you know, it's like, you know, not everybody needs to prove all their theorems instead of C, right?
But if there's any sort of doubt, go down to set that C.
Similar, Franco said to you in the same here, sort of, if your induction system works and you're happy, and you have lots of different problems in science, then forget about Solominov.
But if you have problems with your regularizing, you know, maybe use NDL, you have problems, go one step deeper, one deeper, deeper, deeper.
Ultimately, Solominov will tell you one way or another.
I mean, all that I'm saying is that the concept which I call simplicity out of complexity is what you're talking about.
And there's been a lot of work in other areas, which I think could draw parallels. I'm trying to be positive. I feel that talking together with you, for example, we could link what you're doing with these other approaches,
which are very much more engineering, scientific, they are real systems, not black boxes.
And I believe a bridge could be built between what you're talking about and that primarily because the models that you come up with doing this are the simplest models, and you're talking about the simplest models.
So, when I'm working on, say, hydrology or climate models, I'm looking for the simplest model that can explain the data, and then using that simplest model.
It seems to me there is a parallel with what you're talking about, and it would be nice to see the bridge between these two.
Yeah, that would be great if you can break up, create a bridge. And I mean, you maybe have a, you know, I don't know what notion of simplicity you have in your head, but yeah, you know, maybe we can try, you know, to see how close they are,
how far they are part of whether they are sort of an interpolation between sort of the very abstract thing and the more concrete thing. Yeah, that sounds, yeah, we should maybe talk more about this.
I think I will be giving a talk in May and perhaps you might come to that and we could perhaps I'll be introducing the sort of things I do and you may then be able to see parallels as I've seen some parallels that might be a constructive way of progressing.
Okay, that sounds great. If you can either send, if the date and time is fixed, if you can send it to me now or remind me closer than to, I mean, I will not forget as long as I get one email with the link.
That would be great.
I think.
So it's the same same in our series. I mean, the, and I'll send you the details later on.
It sounds good. Yeah, I will definitely attend.
Thank you.
Any more questions, comments.
Can I ask, can I ask one.
Yeah, so I've come across that actually I even used that Marvin Minsky quotes in one of my presentations for that long ago, which is very sort of inspiring and obviously, you know, I have a big interest in algorithmic probability as well.
So you're working in the real, let's say the real world of machine learning in deep mind, and you also have a strong background, obviously very strong background in this kind of algorithmic information theory.
Where do you see in the future, apart from model selection, can you see in the future like an interesting direction where you can start to merge these kind of things, you know, in like longer terms or projects or directions.
And you probably know that I also run this compression contest, the human knowledge compression contest since 2006.
So, and the motivation there was one gigabyte of Wikipedia, ideally corresponds to roughly the non visual knowledge which a human sort of can store in their brain, you know, in a lifetime.
So, if we can compress that maximally then we would have sort of understood, in a sense, expected all regularities out of it.
And then we should be able to make, you know, good predictions, but you know, the GPT three or go for makes these days.
I mean, they use transformers for, I mean, they don't talk in the term in the language of compression, but I mean, they use log loss for prediction and, you know, perplexity is essentially exponentiated compression.
I mean, it's just a little bit.
I mean, I mean that differences.
So, so one way beyond model selection is to it's a way of actually, as I mentioned before.
It's a Peter sort of said it sort of, yeah, this color of complexity is all a black box. Yeah.
But in practice, right, we need to develop these compressors right. I mean, in a sense developing compressors is the same as developing statistical models.
It is just taking the logarithm at the end.
It's, but, but the compression view can not always, but you know, sometimes can can lead to different insights. I mean, I'm repeating myself the universal similarity metric and was really great, although that is more like a black box.
So for model selection, I mean, you said beyond model selection, but I want to say that there's a neural network. It's difficult because they are so big. And if you just take, I mean, you know, I blow up a neural network by a factor of 10, you know, weights and it can work somewhat better.
But if I would penalize my look like it was with this huge model, I would never select it. So we have a problem here. Right. And this is a solution to this is that you say, Well, I mean, you know, yes, we are designing this neural networks and they are huge.
But before we train them, they're actually super simple, right? I've specified the layers and okay, I have, you know, a transformer architecture.
And then I have random weights are random is always tricky because is it information or not, but I just choose them to do randomly. There's a very small program.
Generating pseudo random numbers, generate network. So it's a very small program, which generates my untrained neural network.
And now what I can do is I can now incrementally learn my data and predict sort of in an online session and evaluate my neural network in an online session.
And the claim is, if you do this in this way, yeah, this leads to this avoid tested combination. It leads to improved transfer learning to different domains.
In principle, what we should do is we should also sort of take the code length of this initial program, but you know, this is small and you know, this doesn't really matter much.
You can just ignore it. Yeah. But so this idea of taking code length and everything and honestly into account.
And led to the idea of, of evaluating these neural networks in a in a potential way, in order to solve the problems I mentioned.
I'm not so sure whether this is, you probably want more exciting.
I'm just interested to see what I mean, in general, I'm interested in, I think, you know, a it is a kind of an underappreciated or under applied area and I'm trying to kind of explore that.
I'm just trying to see, you know, you know, which kind of ideas obviously, yeah, compression or developing compressors, you know, is, is, is part of that because a big hindrance like you mentioned is call me about complexity itself how can we approximate it so
definitely if you have better compressors then that's going that direction and I'm just generally interested so just throw that out though.
Thanks. Thanks for that.
I think I'm hoping and a couple of people here at the mind.
Hope I'm not spilling any sort of secret thing, you know, I'm working on on this online evaluation which is, you know, more than just heavily inspired of evaluating things by code length.
And this trick, you know, gets us around still being able to use this huge neural network so and I, well, I'm, you know, I'm hoping that for many years now but you know, I think, ultimately, I think this will take on.
Thanks.
Any other comments, questions?
I guess I've, I mean, I mean, what does unlimited mean?
As I said, feel free. I mean, if you want to talk more or give a conclusion, it's your call.
People who have to leave, they can leave and then they can watch the recording on you. But it's nice to have you here talking and then giving your point of view about things. Yeah, thank you.
Yeah, I think two hours is maybe, maybe the patience of people and I can always get, you know, the other part, the AI part and the approximation so I can always give another more technical talk.
Yeah, maybe for the workshop we are talking about. Yeah.
Yeah, yeah.
Okay.
Well, let me see, maybe, okay, let me go through these slides at the very end, whether there's something which I really, really have to tell you.
Conclusion.
Yeah, I more or less presented my sense. So I believe that induction and science and machine learning outcomes razor, the bottom party in compression and intelligence, have an extremely large overlap and even if you go to things like what is understanding.
Yeah.
And, you know, I think understanding is also compression essentially.
So that's my claim.
Yeah.
Okay, now, okay, I was funny sort of quote here so they were never the last line and never trust in experiment. Let me trust the theory if it's not supported by experiments.
I know that but you should also not trust an experiment. Oops, I also cut off experiment if it's not supported by a theory. So I think what I want to say sort of goes hand in hand, you know, just, just, you know, running big experiments without, you know, actually I'm agreeing that, you know, without understanding is also dangerous.
I, yeah, here was like when it's okay to ignore UI.
I know Peter wasn't proposing to ignore it, but you know, bridging the gap. But, you know, as I said, you know, if your approach works, you know, forget about, you know, universal induction or if you're probably simple enough.
Or you don't really, you know, care in the sense of need this simple sound solution.
Yeah. Outlook. This is something interesting in the outlook.
Yeah, you should use compression slices at a format measure.
You know, like people lose perfect complexity in language.
And, you know, the funny thing is, you know, I have this question here with somebody asked me look I have here a statistical model which predicts a bit.
You know, my data may be just the digits of pie. So I want to throw in, you know, a deterministic program which predicts the digits of pie.
And, you know, he was absolutely sort of not aware of how to compare, right?
A model which predicts the digits, say, each with probability 110 and a model which predicts the digits, right, rather just, you know, deterministically according to pie.
So how do you compare this, right? If I predict the first three digits of pie, you know, by one over 10, the likelihood is one over a thousand and then it takes the log, right, which whatever it gives.
And how do I compare this to a deterministic model? Yes, of course, it gets the right prediction.
But, you know, then I can always, you know, every data has a deterministic model.
I just say print quotation mark deterministic sequence.
And, you know, this, this general idea of code length.
I mean, you don't have to go to Solomon of again, in this case, you know, two part and the L is sufficient.
You just take minus of likelihood plus code length of your model.
And if you look at this example, if you take an IID model, then your likelihood will just be, you know, scale linearly with the data size because it's just, you know, predict garbage.
But your model is super simple.
If your model is predicted digits of pie, well, this is a little bit more complex model.
But then the log likelihood is just zero because you perfectly predict the digits and the log one is zero, right?
And so that means after certain number of digits.
So if you have a small number of digits, the stochastic model wins.
And if you have more digits, then the deterministic model wins.
And it, but exactly.
And again, with carriers, yeah, it tells you where the transition where you should trust, you know, that there's really a pattern in your data.
Right.
So, I mean, again, it was a super simple bottleneck, you know, would be nice to, to, to tell your story in a more complex setting like your string theory versus standard model particle physics or the models.
Maybe Peter Young is sort of investigating.
Okay.
So I never give them a talk with unlimited time.
So I give myself my limit now.
Just for you guys.
Has been two hours.
Thank you for listening.
And see you in the workshop maybe or whatever.
Yeah, okay. Thank you again for the very nice talk.
Yeah, I'll send you an email about trying to organize that workshop and then I need to coordinate with the people at the Turing Institute because I thought you last time they are still not doing any on site events but yeah, I didn't quite.
Yeah, but it would be nice to have this kind of workshop. Thank you.
And again, you know, now I gave the talk virtual and the Turing Institute is around the corner. So, I mean, you can hold off course as many workshops as you want, but I would like to have a physical workshop and if you take six months, then I just attend a workshop in six months.
Okay, yeah, I met like a few like an in person workshop, not not online but really on site at the Turing Institute. Yeah.
I met on site. Okay, great.
I met on site. Yeah, I think it would be nice to just start like meeting people with I think nice thing about in person is that people would meet and then they talk about coffee etc.
It's much better than online. Yeah.
Yeah.
Okay, any other comments questions from the audience.
I will ask I'm happy to stay here as long as you want. Actually, my battery runs out in 20 minutes, maybe 15 minutes.
Yeah, but everyone is is tired now anyway. Yeah, yeah. Okay.
Okay, thank you.
Thank you. Thank you.
