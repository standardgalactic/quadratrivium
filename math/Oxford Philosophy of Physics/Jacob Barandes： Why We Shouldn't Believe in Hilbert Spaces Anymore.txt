now. Welcome, everyone. It's week six, Trinity Term 2021 of the Oxford Flossier Physics Seminar.
This week, I'm very pleased to welcome Jacob Barandas from Harvard. Jacob is a lecturer in
physics, in fact, but researcher in the Flossier Physics these days. He's also a faculty affiliate
with the Harvard Black Hole Initiative and founder and organizer of the Harvard
Foundations of Physics workshop series. His work encompasses foundations of quantum, classical
limits, field theory, general relativity, thermodynamics, and formal methods in mathematical
physics. And his more recent work has been with, for example, David Kagan on a minimal modal
interpretation of quantum mechanics on his most recent paper just out this year in Foundations of
Physics. He's on a classical analog of the classification of particles as irreps of the
Poincar√© group, specifically for massless particles. But today, he's going to be telling
us why we shouldn't believe in Hilbert spaces anymore and give the case for platonic quantum
theory. So take it away, Jacob. Thank you very much for being here. Thanks so much, Adam. Thanks to
everyone who's organizing this seminar series. It's been a real joy to participate, especially in
these times. And thanks to everybody for coming. All right. So this is a bit of an ambitious talk.
I'm going to go somewhat quickly. So let's get started. Right. So the title of this talk,
Why We Shouldn't Believe in Hilbert Spaces Anymore, it's a bit of more provocative
statement than it is maybe warranted, but I'll get to the subtleties later. And the case for
a different interpretational approach to quantum theory. I have here a reproduction
of a part of a letter from von Neumann to Garrett Birkhoff from 1935. And I'll make
reference to this letter a little bit later. All right. Can people see my mouse pointer?
Great. Okay. All right. So the outline of this talk will be some introduction, motivation. We'll talk
about the classical case and then the quantum counterpart. And then I'll talk about the measurement
process, both classically and quantum mechanically, and then we'll move on to the conclusion. So
let's start with the introduction. All right. So this, I think people know, right? Quantum theory
very apparently successful, but still no consensus over its proper physical interpretation.
As part of this talk, I will argue that the historical foregrounding of the Hilbert space
picture has been a mistake when it comes to trying to interpret the theory. This is not to say
that Hilbert space pictures are not unbelievably mathematically useful and that we shouldn't
keep using them. We absolutely should. But in particular, I'll argue that Hilbert space pictures
are in some ways analogous to gauge pictures for classical field theories or or phase space
pictures for classical mechanics. They're mathematically useful, but interpretationally obscure.
And in particular, they're not where we should start when looking for an ontology.
Imagine if Newton had been handed the phase space formulation as the starting point for classical
physics, all these canonical transformations that we've been very difficult to figure out where to
get a grapple on the ontology. This is the von Neumann quotation. I would like to make a
confession, which may seem immoral. I do not believe absolutely Hilbert space anymore. Now von Neumann,
what he was thinking about was generalizing classical logic to quantum logic and trying
to think about quantum mechanics from the standpoint of a new logical framework. This
is not the direction I'm going to take, but I still take some inspiration from this quotation.
Since about 2017, I've become increasingly interested in the Gelfand Neymar Segal construction,
the GNS construction, both for foundational and also pedagogical reasons. I first got interested
in this in trying to teach quantum mechanics to students with very little background in the subject.
And if I have some time, I'll talk a little bit about why I found that approach helpful.
Roughly speaking, with the GNS construction, and many of you already know about this, but some
don't, what it implies is that when you have enough starting ingredients, the Hilbert space
picture emerges more or less automatically. If you consider how bizarre and difficult to interpret
wave functions, self-adjusting your operators and so forth are, particularly in that light,
the GNS construction suggests that maybe we shouldn't read too much into those ingredients.
Hilbert space picture is not wholly written. It's a paradigm. Perhaps it's akin to the
shadows on the walls of Plato's cave. And in that analogy, the GNS construction is like the fire
that produces those shadows. This is actually where the name of this interpretation, chronic
interpretation, comes from. Relative mathematics here is cister algebras. Many of you know about
this, and some of you don't. That's okay. I'll review what we need to know. These are widely used
in algebraic quantum field theory, also by some philosophers of physics and mathematicians who
work in functional analysis. I will say they're almost unknown in most of the physics community.
I can say that relatively confidently. This is a bit of a complicated slide, but I wanted to fit
it on one slide, because this is a summary of the interpretation. This is the whole thing on one slide.
The basic idea is that we're going to start, and I'll go into more detail about how all this works,
but just to summarize, we start from the get-go with a statement about what the ontic stuff is
of our model. These are ontic properties of some kind. If we're thinking about mechanical particles,
these could be positions of momenta for fields, intensities, and rates of change.
Then we encode these ontic properties, potentially with some core screening for convenience,
into a corresponding set of random variables. Already at this level, we've introduced a mathematical
abstraction for mathematical convenience. Random variables have individual statistical
distributions, but we'll see that in the quantum case, there are also subtle
interrelations between these ontic random variables. Then what we'll do is encode
all of that epistemic statistical NOMIC data into a set of things called state maps that will
capture all that information. With these ingredients, we'll be able to build a CSTR algebra.
We don't explain what that is. You could think of it as a convenient software layer,
and then once we have a CSTR algebra, the GNS construction generates a Hilbert space picture
as a higher level software layer, with an additional level of abstraction.
I almost think of this almost in analogy with you start with Tony mechanics,
you introduce Lagrangian mechanics, then you introduce Hamiltonian mechanics,
then you introduce the Hamilton Jacobi formulation. You keep stacking abstractions on top of each
other. What are the key takeaways? One is, I'll argue the botanic quantum theory is a
conservative interpretation with one world, with a non-perspectival notion of ontology.
This is not to say everything is non-perspectival, but I'll talk about exactly what is and what isn't.
From this standpoint, neither CSTR algebras nor Hilbert spaces are regarded as fundamental,
or even unique. We allow that some familiar features of classical probability can break
down. Gotta bite the bullet somewhere, and I'm gonna bite it there. In particular, joint
probability distributions will only be assumed to exist in special cases, and I'll go and detail
how those work. In particular, when we talk about the measurement process. In particular,
when there's no direct entanglement between relevant ontic properties, which is a novel
statement about when joint probabilities should or should not exist. From this standpoint,
decoherence is a physical process, whereas collapse of wave functions or what have you
is a subjective, perspectival thing. We also bite the bullet on non-locality, as I'll explain.
But one of the key motivations of this approach is to do something along the lines of what
James asked for in a contribution to a set of essays in 1989. And he said,
our present quantum mechanical formalism is not purely epistemological, it's a peculiar mixture
describing nature, realities of nature, incomplete cubic information all scrambled up by Heisenberg
and Bourne doing an omelette, knowing this he had an unscramble. The people who work in cubism
particularly like this quotation. The platonic interpretation is a different way to try to
unscramble this quantum omelette, in particular by separating off the ontic sector of quantum
theory from basically all the other stuff, the epistemic statistical nomic stuff. The ontic
sector will be those ontic properties and the epistemic statistical nomic sector will be basically
everything else. Okay, so that's the introduction. Now motivation. So what are we doing in classical
physics? Well, you know, generally, we take a physical system, we regard it as existing in
some sense, the 3d space consisting of material bodies moving along depth of trajectories or
field intensities, varying in strength of localized points, this neglect subtleties that arise for
gauge theories, but more on that later. And this is a particularly nice picture for attaching
ontologies to in fact, usually we do is we start with the ontology and then we layer mathematical
description on top. In quantum theory, we start from traditionally textbook quantum theory,
the Dirac von Neumann axioms, these paint a far murkier picture of reality. At a kinematical
level, the description of what the ingredients are, but all contingent facts are supposed to
be encoded in a mathematical state vector and a Hilbert space over the complex numbers equipped
with a product. For dynamics, there's supposed to be a Hamiltonian operator for which state
vectors evolve according to the famous Schrodinger equation. And there's trouble right away for open
systems. You all know this, but in an undergraduate and directory textbook, usually this isn't
treated right away. But for open systems, as most of you know, in the systems that can exchange
information environments, you get entanglement between the system that you're looking at and the
environment or apparatus or whatever it is that's studying the system. This leads to a failure
factorization between that open system and its environment and open systems generically don't
have state vectors. Instead, we have to use a density operator and generically density operators
in contact with an environment evolve according to a much more complicated master equation,
like the Lindblot equation is a simplified version of this. If you assume Markovianity and you assume
homogeneous time, but more generally, you can get a much more complicated master equation,
and these generically lead to ever more entanglement with the environment.
The connection to empirical results makes the story even murkier and more complicated.
In the simplest case, each observable feature A of a quantum system corresponds to a self-rejoint
linear operator. The possible measurement results correspond to its eigenvalues. Here,
I'm labeling eigenvalues with some label lowercase a that distinguishes them.
For a system described by a state vector, as you know, we let ket psi a be an eigenvector of a.
We can introduce a projection operator. It's this ket bra construction. And then we can compute
the associated probability of getting that measurement results as an inner product or by
sandwiching the corresponding eigenprojector in the state vector. And observables more generally
have expectation values that are expressible in this famous way. For a system described by a
density operator, we generalize a little bit. We work with traces. We multiply density operator
by projector and trace to get the probability. Or to get expectation values, we multiply the
operator in question by density operator trace. If a ket vector has a unit probability for a
measurement of A to yield a specific result, definitely going to get the results, a particular
result for the observable A with probability one, then the ket's supposed to be an eigenvector
of that operator with that eigenvalue. This is the famous eigenvector eigenvalue link.
Mainly following the measurement, the state vector density operator collapses to that definite
eigenvector, assuming an idealized measurement. This ensures robust results under repeated
measurements. We measure again quickly before we let any time or lose happen. We're guaranteed to
get the same result again. An important example, if Q represents a classic light configuration,
like a particle position or a bosonic field intensity, the system has some state vector.
We associate a wave function to be these inner products of the state vector and the eigenstates
corresponding to that observable. Equivalently, the coefficients in an expansion of the state
vector in the coordinate basis. Then the absolute value squared of the wave function gives you
probabilities or probability density. These wave functions naturally live in configuration space.
For these systems, that configuration space looks a lot like what you'd think of as the
classical configuration space. For n particles and 3D, it would be a three-end dimensional
configuration space. So what's the measurement problem? Well, there's a million ways to phrase
this. One way I like to think about the measurement problem is, on the one hand, we have standard
time evolution, Schrodinger equation for a closed system, or some suitable master equation for an
open system. But neither of these singles out a definite measurement result. Whereas when we
have collapse after a measurement, we get a definite result. One problem is, the Dirac
von Neumann axioms don't rigorously define what is an observer, what is a measurement,
just measurements or things that observers do. And that's a problem. There's this ambiguity built
right there. Now, decoherence can't save us here. Decoherence is the tendency of environmental
interactions to make the dense-diameter system evolve rapidly into a form that resembles a
classical probability distribution for specific variables. The problem is, this doesn't single
out definite measurement outcomes. And it doesn't work. In fact, decoherence increases the level
of entanglement with the environment. But, you know, from time to time, you see people try to
argue that you can somehow make measurements happen through some sufficiently sophisticated
decoherence process. The problem is that collapse when it happens looks at the level of Hilbert
spaces like some non-local event. Whereas decoherence, if it's working under, you know, local
Hamiltonian or, you know, a local, you know, quantum channel or something like that,
it can only produce local effects. And this is thanks to the no signaling or communication
theorem. So there's like a fundamental problem here. All right. So an interpretation of quantum
theory should resolve ideally this measurement problem among other things. It should ideally
also have some kind of ontology, which I take to be part of the objective structure of the
interpretation. This is what exists supposedly in reality. It should have some epistemology,
which may combine objective and subjective elements, chancey and uncertainty type elements.
This is what we can know, what we do know, what probabilities we can assign to things that we
know or don't know. And again, there could be objective limitations. It might not just be a
matter of our subjective ignorance. And it should be a nomology, which can entail both objective
and subjective features as I'll explain in a moment. These are the laws, the laws that summarize or
govern depending on one's attitude toward laws, the behavior which could potentially be statistical
with the ingredients in theory. I have an analogy here that will be important just to raise, you
know, attention to a couple of subtle details here. So let's talk about nomology for a second.
Consider the sequence of numbers, one, four, nine, 16, 25, 36, dot, dot. We'll call this our ontology.
If you want, I've got a bunch of rocks sitting in front of me and I've got one rock and four rocks
and nine rocks. Or I have rocks that change in number as a function of time, I don't know.
It seems like there's objectively a pattern here, especially if when you check, the next
numbers happen to be 49, 64, 81, and 100, setting aside the problem of induction. And this is not
going to do one of those trick questions where it turns out this is some sequence that after the
15th number becomes something totally different. I'm not playing any games here. You look at this,
you go, well, I'm going to posit that there's a law here, a nomology that summarizes or generates
this pattern. It's the sequence of square integers. But then someone else comes along and says, no,
no, no, that's not the law. The law is that we're adding consecutive odd integers. Well, who's right?
It doesn't make sense to ask who's right. That's not a meaningful question. These two laws are
mathematically equivalent. The pattern is some unique thing, but laws that describe it are not
unique. There could be a many to one relationship between nomologies and patterns, and that's okay.
We'll see this arise again a little bit later. So some prominent interpretations and approaches.
And I apologize right now, because many people here may have attachments to some of these.
And this is a super superficial summary, and I apologize. And if I'm mischaracterizing it because
of this, I apologize in advance. To the extent that there's anything like a Copenhagen interpretation,
there's some notion of microscopic versus macroscopic systems. And when classical macroscopic
systems interact with quantum systems, you get collapse. And there is a vague notion of a division
between the two kinds, the Heisenberg cut. But where is this? What is the meaning of an observer?
And so forth. In De Bruyne-Bohm or Bohm Mechanics, and I'm collapsing the two together, I know that
there's subtleties between them, there's different flavors of them. But here we supplement the standard
formalism of quantum theory with ontic hidden variables, particle positions, bosonic field
intensities. There's a problem with fermionic fields, which I'll come to a little bit later.
But here the ontic random variables are traditionally read off of the configuration
space of the system, or from the domain of the wave function. Now, there are other approaches to
this, but this is the more traditional approach. We then regard wave functions as ontic pilot waves.
Or not as ontic at all, but as nomology, sort of like Hamiltonian that govern how
the ontic hidden variables behave. There are major problems, as we all know, with relativity
at varying levels. There's also trouble with systems that don't seem to have configuration
spaces in any familiar sense, like fermionic fields, and I'll come back to those later.
Then there's the Everettian approach. We shrink or expand, I guess, the ontology to a universe,
to the universal wave function. Although there are many different flavors of this,
there's space-time local realism where you're attaching ontologies maybe in some local sense.
Again, this is a very stripped-down version of the story. The nomology is shrunk down to the
Schrodinger equation in some flavors, but not in all. And then the idea is that the familiar world
or worlds or our experience emerges in some branched way globally or in local patches of
space-time through some emergent process. Traditionally, there's been a lot of contention
about the emergence of probability from this approach. I would argue that if you're going to
derive probability deductively, well, then it's got to be inherently premises. If you're going to
derive it inductively, well, inductive reasoning already includes an notion of probability,
but this is a much larger discussion I can have here. So I'm not going to say much more
than the Everettian approach. I'm going to leave it here. There are other approaches,
spontaneous collapse. There are quasi-instrumentalist approaches like cubism. I'm not going to say
much more about this except to say that in quasi-instrumentalist approaches, you run to this
question of, well, if you've got probabilities, what are these probabilities of? In the moment you
say what they're of, you're saying something and suspiciously ontological, and that's an area of
tension. I would argue that all these interpretations and approaches appear to take the Hilbert
space pictures fundamental, but Hilbert space pictures are fickle. I'm going to set up an
analogy here. So bear with me. So recall classic electromagnetism. We've got the Maxwell equations,
there they are. We've got our physical fields, the E and B fields, but it's often mathematically
very convenient to introduce gauge potentials, the phi and A fields. We can express the E and B
fields in terms of them. This gauge picture is metaphysically murky. Phi and A are radically
non-unique due to the theory's invariance and a gauge transformation. I can replace phi and A
with new gauge potentials under this transformation where lambda is an arbitrary function of space
time. So what do we do? We regard the physical and maybe, if you want, ontological sector of the
theory of classical magnetism to be its gauge invariant content, to be the stuff that's gauge
invariant. This includes E and B, it includes fluxes. If you include a little bit of quantum,
it includes, you know, art of bone type phases, which are gauge invariant.
But I'm going to take this a couple of more steps. So some of you may know that you can encode
the Maxwell equations into a Lorentz covariant formalism. We stick the E and B fields into this
anti-symmetric rank to four-dimensional tensor, the Faraday tensor, which can be expressed in
terms of a four-vector gauge potential, a gauge connection, as it's called, which has connections
no pun attended to some interesting constructions and fiber bundles and different geometry that
we want to talk about. But all of the three-dimensional variables are encoded into this
into this Lorentz covariant formalism with the current density. And then we can express the
Maxwell equations in this way. Gauge transformations now take this form with the connection, the gauge
connection, transforming by the addition of a four-dimensional spacetime derivative of an
arbitrary scalar function. This is actually a special case of a more general construction.
This is the most technical slide, I apologize for its technicality. But if you're given a collection
of mutually interacting massless vector fields, not just the electromagnetic field, but cousins of
it that can all mutually interact with each other, they all carry charge under their own
interactions, there's a mathematically consistent way you have to put it together, and that's as
a non-Abelian or Yang-Mills field theory. There is some Lee group associated with this field theory,
some symmetry, the theory needs to be consistent. The symmetry has matrix generators. We stick the
matrix generators in with the field and define matrix-valued fields. So these look like a mu,
they look like the gauge potentials we had before, but they actually take matrix values,
they're non-Abelian and non-commutative, that's where the non-Abelian comes from.
We introduce a generalization of the derivative called the gauge covariant derivative
given by this formula. We introduce a generalization of the Faraday tensor, which can be expressed
in this way in terms of the gauge covariant derivative. And then with the theory, the gauge
invariance we had in the case of ENM is generalized to a transformation where
certain objects like the Faraday tensor transform by conjugation, by a local unitary
transformation generated from early algebra, and gauge connection transforms in a more
complicated way rather than just VAV dagger. We get VAV dagger minus this inhomogeneous piece,
and these are homologous, if you go back a slide, to the two pieces you see here
in the Abelian case, electromagnetism, A mu doesn't get a VV dagger, or if it does,
it's like a one by one phase factor that cancels. And then the second term is this derivative term.
This sort of more complicated construction you see here is a generalization of that derivative term,
and this can again be expressed directly in terms of the gauge covariant derivative.
These transformations leave the physical ontological sector theory variant.
They're very useful, especially if you want to quantize this, write down a pathological
representation due to standard model physics. This is almost unavoidable.
So Harvey Bren had a really beautiful, beautiful paper in 1999 that had a big impact on me,
and he begins the paper by talking about the appearance of a very similar kind of gauge
invariance in quantum mechanics. You can take the Schrodinger equation, the standard Schrodinger
equation, and rewrite it in this way in terms of a gauge covariant derivative with a Hamiltonian
role of a gauge connection. And then the Hilbert space picture has a non-Avelian gauge invariance
completely analogous to the case of a Yang-Mills theory. In fact, the formulas are identical.
This transformation leaves the Schrodinger equation unchanged in form,
and it preserves probabilities expectation values. This is the gauge invariant content of
quantum theory, and I would argue that this means that's where we should be looking for our ontology,
not the level of the Hilbert space, which from this point of view is more akin to gauge variables.
Right, so this is just summarizing what I just said, but matters are even worse.
You can take a complex Hilbert space and replace it with a real Hilbert space by introducing a
single so-called universal qubit. These are just the details, but you introduce some
orthonormal vectors for the qubit. Any operator can be written as its real plus its imaginary part.
You can take the imaginary unit and write it as a two by two matrix once you've joined this
universal qubit, and you can rewrite this as A tensor product with this I state and the complex
conjugate of A tensor product with its other state, and it turns out this leaves all the
predictions of quantum mechanics completely unchanged as long as you slightly renormalize
the board rule. The only downside is that the formalism ends up looking a little more
non-local than it did before, and that's the substance of this paper by Renoux et al.
They have a paper on the archive from this year where they argue that real quantum mechanics
looks a little more non-local than it would otherwise, but fine. I mean, coolant gauges
also look superficially non-local in terms of gauge variables, but it doesn't bother us.
There are other gauge choices we can make. If you've got a quantum channel and you're
unhappy that it's not a unitary operator, you can join an additional quantum system with its own
Hilbert space k and make it look unitary. There are other gauge choices. When I teach QFT, I like
to start with a single particle Hilbert space, enlarge the Hilbert space, drop the requirement
that the inner product is positive definite, and this turns out to give a very elegant
Lorentz covariant way to describe particles, massive and massless particles of arbitrary spin,
and then you get back to the physical states by projecting down, or if need be, quotienting
this larger Hilbert space. Hilbert spaces are very manipulable. They're very, I would argue,
flimsy and non-robust. So at this point, I'm reminded of a famous painting, Rene Magritte,
the treachery of images, the famous pipe, how people reproach me for it, and yet could you
stuff my pipe? No, it's just a representation. Is it not? So if I'd written on my picture,
this is a pipe I'd have been lying. With apologies to Magritte, this is not a state.
All right, so what am I going to do the rest of this talk? I'm going to argue we can derive
quantum theory from a sharp ontology consisting of physical systems with basic properties,
but that exhibit a non-sherveil probability structure, and this leads to this new interpretive
framework. So there'll be an ontology, states characterized by properties, there'll be an
epistemology, interdependent probability distributions, that capture the degree to which
those underlying ontic values are manifest the external world, and you can take a chancey,
frequent, disperpensity, best systems, whatever approach you want to probability. I'm not going
to solve the probability in this talk, I'm going to bite off one problem at a time.
And then there's a subtle but ultimately codifiable collection of rules for those ingredients,
that's the nomology. Let me just say a word about this notion of manifestness. So Schrodinger said,
I think rather beautifully in his paper, this was the Schrodinger cat paper, there's a difference
between a shaky or out of focus photograph and a snapshot of clouds and a fog banks, right?
The picture can be sharp of something blurry or the picture can be blurry. The quantum
interpretation, the underlying ontology will be sharp. It just won't, it'll be blurry in terms of
its availability, its manifestness to the outside world. This means that epistemology from this
point of view is not a wholly subjective thing. It has an objective aspect, there's only so much
that nature will allow us to know about certain underlying properties, and those properties
can exhibit a non-trivial statistical behavior. In a sense, we're shifting in the definition
of the weirdness from the ontological sector over to the epistemic systemic sector as I'll explain.
All right, so what obstructs full manifestness of the systems ontology? Well, there are a lot of
things, but the key, no idea, as I said before, is that not a properties generically will not
share a meaningfully correct overarching joint probability distribution outside of certain
special cases that have to hold at least to a good approximation. The corresponding observables
have to have commuting operators, and there has to be negligible direct entanglement between the
second property is new. After a measurement process, we'll see that these conditions will be
satisfied to an incredibly good approximation, and so it is in that case that joint probability
distributions between measurement results and pointer variables will emerge. Not the ontic
properties themselves, which are always there, but their statistical correlation will emerge,
and that will account for definite measurement outcomes. So from this point of view, again,
the Hilbert space picture is a convenient tool, software-like, not assumed to be a fundamental
aspect of reality. Okay, let's talk about the classical case, and this will be a motivation
for what we're going to do in the quantum case. So what is the ontology of the classical system?
Well, you know, you could have different opinions on this, but a flat footed attitude for ontology is
classical systems have got properties that define the system's ontology, could be particle
locations and momenta, whatever you want. Each of the system's mutually exclusive
states of ontology, ontic states, which are labeled as x, x prime, x double prime,
these are different ontic states, specifies once you have one such state, it determines
the values of all those properties, and there could be some possible core screening here,
as can be, maybe even into discrete possibilities like coin flipping, heads or tails.
So examples, mechanical systems, particles, systems of fields, so forth. All right, practically
speaking, we don't always know the values of those properties with certainty. So, you know,
our epistemic state, our epistemology is captured by an overall probability distribution. We associate
probabilities with those ontic possibilities, those ontic states, and this means the ontic
states form a sample space for some probability distribution that obey the standard comagora
vaccines that are listed here, positivity sum to one. At this level, it's convenient often to
associate each ontic property with a symbol that stands for the list of its possible real values
that are determined by the underlying ontic state x or x prime, x double prime, and these have
probabilities given by the epistemic state, in that case we call A a random variable. We can
define algebraic combinations of random variables, A plus B, A times B. In general, a function of
random variables is defined for a given ontic state x to be whatever the corresponding formula is
when you plug in the values of the individual random variables. In this case, our collection of
random variables is said to form an algebra. It's a set of things you can add together,
you can multiply them together, you can multiply them by real numbers, and so forth, you form what's
called an algebra. To each ontic state x naught, we can associate a particular random variable,
p sub x naught. This is a projector. It's just a true or false question. The system's ontic state
is x naught true or false, with one for true and zero for false. So, its values, when you plug in a
particular ontic state, it's just a chronic or delta function. It's no potent. If you square it,
you get one because one squared and zero squared are both one, and we call this an elementary
projector. Alimentary projectors obey mutual exclusivity. If you sum them over all the ontic
states, you get the identity. That's i, the identity random variable. It's always true. It's always one.
And for all the random variables, A, we have an eigenvalue equation when they act on these,
and we can also expand any random variable as a linear combination of these elementary projectors
with the eigenvalues of the observable appearing as the coefficients. Given subsets of the sample
space, omega, subsets E and F, we can introduce generalized projectors to be a sum of the projectors
corresponding to all the points in the two subsets E and F. These automatically have the
property that you multiply them. You get the projector ontic intersection. Notice,
set theoretic intersections are commutative, and this is compatible with the idea that these
projectors should be commutative, which they are. We can choose complex value random variables for
convenience. It's very nice to have complex variables available. And then we can introduce
an involution, a star operation that just replaces Z with A plus IB, with A minus IB. We call that
star operation. Or in terms of ordinary complex conjugation, once we plug in an ontic state,
well, then this is just an ordinary number, and then star just gives you the complex conjugation.
We can choose the norm to be the supremum among all of those absolute values for a given random
variable, and we can use this to introduce notions of limits and continuity and topology.
If we extend our algebra to make it closed by including all limit points of Cauchy sequences
and other technical assumptions that are very convenient, we now have what's called a C star
algebra. C stands for closed in the sense of topology, including all of these limit points,
and the star is the star. Given an epistemic state that assigns probabilities that it states,
that's now equivalent to introducing a so-called state map defined on the C star algebra itself.
Omega is a state map. When you act on an element of the C star algebra, it just fits out the
expectation value. If you act on the element of projectors, it gives you the individual
probabilities back. Let S be the set of all those state maps. We can get joint probabilities from
state maps. You just plug in the product of your two projectors, and it will give you the
joint probability for both of them. You can introduce covariance, standard deviation. All of
these can be expressed directly in terms of state maps. You can even re-express the C star norm itself
in terms of the state maps. The C star norm can be expressed as the supremum among all the states,
all the state maps, the square root of omega z star z. This gives a nice convenient way to
express everything in terms of the state map. When you have a C star algebra and you have a
state map satisfying these basic desiderata, which are satisfied by our classic state maps,
and you require that the norm given in this way is a C star norm obeying the usual requirements,
then the GNS construction swoops in and says, you've got a Hilbert space. The state map
corresponds to a state vector. Classical random variables correspond to, in this case,
commuting operators for the classical case, and expectation values can be expressed by
sandwiching the operators in the state vectors. Look familiar? That looks awfully familiar.
So key lessons. Hilbert spaces need not be regarded as fundamental aspects of reality, at least in
this classical case, although they may be convenient for calculations. Some of you may know that there
is actually a, oh, people do this. It's called the Kutman von Neumann formulation. It was just a
workshop a couple of weeks ago that I participated in that was just devoted to the Kutman von Neumann
formulation. And it's useful for studying dynamical systems, for chaos, for the classical limit,
but even for purely classical purposes, it's useful. It makes it easy to derive lots of
fun formulas. This is a super cool way to derive the famous formula that the correlation coefficient
of two random variables is between minus one and one. It follows directly from this formalism.
You can get it directly from the Cauchy-Schwarz inequality. It's a very cute way to derive this.
But the Kutman von Neumann formulation contains a lot of weird ingredients,
strange non-ontic ingredients. You can introduce non-commuting operators. You can do all kinds
of bizarre stuff, but we just don't read a lot into that because we know. We're going to start with
the Hilbert space picture. The Hilbert space picture is a clearly derived higher level software layer,
and so we're naturally suspicious about attaching to which means. Now let's talk about quantum theory.
So to begin with, we're going to do a quantum theory. We're going to generalize the classical
case. We're going to drop the assumption that ontic states form a valid sample space.
This doesn't mean there's no ontology, there's no ontic states. It just means to be a sample
space, you have to assume there's a well-defined joint probability distribution, and that's what
we're not going to assume is true anymore. And this is a matter of epistemology. I'm not saying
the ontic states aren't there. I'm just saying that nature is limiting what we can know about them,
and in particular doesn't let us assign them a joint probability distribution in general.
Now the system still has ontic random variables representing possible underlying values of
nonic property that you could still do if you want. It could be some course grading here.
Let's suppose we pick some ontic property. We say the system has some ontic property,
A, and I represent it with a random variable. Well, it'll have some list of possible values.
Those will make up its own individual sample space belonging just to A with its own associated
set of probabilities. I can define projectors in exactly the same way. This is a carbon copy
of an earlier slide, but this is just for that one random variable A. Given a second ontic
random variable, their individual probability distributions may now be incommensurable,
which is connected to the failure of the overall set of ontic states to necessarily be
a sample space. There may not exist a physically accurate joint probability distribution
that reliably describes the current underlying values of both A and B,
and that yields the individual probability distributions by the correct marginalization
formulas. That's strong constraints, and I'm just saying maybe it's not always true.
Could be subtle interdependencies between the two probability distributions,
so that physical effects or trying to uncover one of them or pin one down affects the other.
There's already hints here, the uncertainty principle entanglement and incompatible observables,
but this is just at the level of looking at random variables. If there's no joint probability
distribution for A and B, and this is crucial, then A plus B and A times B and other such
algebraic combinations are not random variables. They're not because there's no probability
distribution for them, because I can't assign them probabilities anymore. This doesn't say that
there aren't underlying values for A and B, just that they're not going to be captured in a well
defined joint probability distribution. Again, on a matter of ontology, it's just a
matter of what can be known, what nature is letting us know about these things. I can still
define a formal algebra. I can still talk about A plus B and A times B and just regard them as
formal things, and I can introduce formal state maps that take this formal algebra
and map them to the real or complex numbers with the condition that if I plug in an ontic random
variable, A, then I'm getting the expectation value. But the complex numbers associated to A
plus B and A times B by the state map, they don't have a clear obvious interpretation,
they're encoding something. It's not clear what they are. We'll see that they encode
detailed interdependencies. Now, our choice of set S of state maps is now really crucial,
because for a given choice, it may be that certain combinations like AB
always give the same result when you plug them into an arbitrary state map with anything else,
as maybe BA plus C. And if that's true, if our set of state maps has this feature,
that for all things you plug in, AB always gives the same result as BA plus C,
well, then we can say that our algebra effectively has a commutation relation,
AB equals BA plus C or AB minus BA equals C. So in this way, a particular choice of set
of state maps, which again encode epistemic information laws, they dictate a particular
algebraic structure on the formal algebra. Once we quotient out by these relations,
we get commutation relations, whatever algebraic relations we expect, but these are
coming from our choice of state maps. Suppose we pick a set of state maps, and they satisfy
the same disorder rather than the classical case, normalization, linearity, positivity,
and we require that this way of defining a norm is a C star norm. In particular,
these automatically imply that under complex conjugation, we get a reversal inside of state
maps and more generally that the star operation is product reversal. If we include limit points of
Cauchy sequences under this norm, we get a noncommutative C star algebra, and that means
we get a Hilbert space from the GS construction. With this dictionary between on the left hand side,
state maps and the C star algebra and on the right hand side, the given Hilbert space picture that
has been generated. And this slide just shows that in the special case in which the projector
we're asking about is rank one, well then this formula just fits out the usual vulnerable.
You can also show that if your given state map omega happens to assign a trivial probability
distribution to a particular ontic random variable, well then a simple calculation shows that
the corresponding state vector obeys the eigenvector eigenvalue link to come out automatically.
Automorphisms of the C star algebra that preserve all probabilities, these are information
conserving transformations on the C star algebra, correspond, there's a theorem, correspond to
unitary transformations on the Hilbert space. So if our C star algebra of quantum random variables
evolves by a smooth sequence of automorphisms with time, that's equivalent to unitary evolution
of the Hilbert space. If Stone's theorem applies, you have a system of finitely many degrees of
freedom obeying Heisenberg commutation relations or somewhat more rigorously vile commutation
relations. Well, with some other assumptions, you get the Schrodinger equation for some
Hamiltonian H. And moreover, that representation is unique up to unitary equivalence. But I'm going
to take a step farther because there's a lot of dispute over how general that idea is as a generalized
beyond non-relativistic systems of finite many degrees of freedom. And here, regarding Hilbert
spaces, even C star algebra is not fundamental, but just as mathematical contrivances helps us.
Because we're not putting our ontology in there, it's okay if our Hilbert spaces and C star
algebra is not fundamental. So in the language of, are you an algebraic imperialist? This is
our George's formulation. Do you take algebras to be fundamental or you're a Hilbert space
conservative? Neither. Neither of them are where the ontology lives. They're both software layers.
And this gives us some additional freedom. In particular, Fel's theorem has a profound
impact here. So Fel's theorem says that if omega is any state map for a C star algebra and you've
any finite collection of elements that C star algebra, then if you're given any faithful Hilbert
space representation, coming from some, you know, any faithful Hilbert space representation,
there will be, you know, with corresponding operators corresponding to the ones that you're
talking about, then for any epsilon, any positive epsilon, that Hilbert space representation will
have a density operator that will give measurement results predictions that are
accurate to within epsilon for all i. Now, keep in mind n could be a thousandth busy beaver number
raised the power of a Google Flex. It doesn't have to be a computable number. It could be
something insanely big. Epsilon could then be one over n. So there's just no empirical way
that you are ever going to distinguish between two faithful Hilbert space representations, even
if they don't have the fundamental property of traditional unitary twist equivalence.
And even if one of those representations has parochial observables, certain limiting
observables that aren't any other, you can make n as big as you want. You're never going to be able
to tell the difference between them. Now, if you take Hilbert spaces as fundamental, then you have
this problem of which one is fundamental. But from our standpoint, Hilbert spaces are just
fictions. They're just mathematical conveniences. You pick whichever one you want. You could think
of the different choices of Hilbert space representations. As long as they're faithful,
it's just different gauge choices. You pick whichever one is sufficient for the purpose of
hand. We can see right away that there's going to be problems for joint probabilities.
We know that if you stick in a single projector for corresponding to one outcome, corresponding to
one on a random variable, you get the corresponding probability. But in general, projectors don't
commute anymore. If you recall back, we talked about set theoretic intersections having to commute
and this corresponded to classical projectors having to commute. Well, these projectors in
general don't commute. And that means that if you attempt to plug in a product of two projectors
for two incompatible observables into a state map, in general, you got a complex number. So this
can't possibly be a probability. You can't even fix it by taking absolute values that will screw
up partialization rules. You can't try to redefine it in some clever way because you won't be guaranteed
to get non-negative probabilities. Quantum mechanics is screaming at us that even forgetting
anything else is at this talk. These are things you can plug into state maps. These are questions
that simply do not have probabilities associated with them. This is screaming at us that probabilities
just don't always exist. This just is the more general statement of this for more general outcomes.
So the door is just open. The door has been kicked open to the notion that joint probabilities can
simply fail to exist. And I would argue objectively as a fact of the matter. In lexotic interpretation,
this is extended, not just incompatible observables, but anytime you have a
bipartite system, a composite system, where the overall state map is not separable. And again,
this will feature prominently when we talk about the measurement process. Separability, however,
emerges naturally under decoherence, environment-induced decoherence, for example, during the
measurement process. This renders state maps for joint systems over the relevant observables
separable. And that's when joint probabilities approximately emerge, and we get to a very good
approximation, correlations between the underlying ontic properties. This emergence, however, is
epistemic, not ontic. It's not that the ontic properties didn't have values beforehand. It's
just they didn't line up in a statistically correlated way until after this step. Let's talk
about the classical measurement process as a set up quantum process. So what we have is a subject
system, which I'm going to model super simply as just having one observable A with underlying values
labeled by little a, a measuring device D, this is our pointer variable, and an environment E.
Initially, the measuring device has some known value. The pointer variable D is labeled by D
naught. It has some particular specific value. The environment also has, we'll assume, some known
value or pre-measurement value. Neither of these systems know the underlying value of the thing
to be measured A of the subject system. So they would assign a non-trivial state map to it with
some probability distribution over the possibilities. P of A is the probability distribution, and omega
little a corresponds to being a trivial distribution that assigns unit probability to just that one
outcome of the day. There's some positive entropy associated with this. This is just the standard
Shannon entropy that characterizes how much information currently is missing, that the
apparatus and the environment lack. The overall state map is factorizable. It's uncorrelated.
There's no correlation between environment or pointer variable or subject system.
Well, then the measuring device goes in, measures the value of A. Well, now the overall state map
has classical correlation. The environment is still trivial, unit probability of being E naught,
but the combined system AD, A subject system D pointer variable, has a non-trivial state map.
And that non-trivial state map is correlated. This is a standardly correlated state map. The
probability coefficient is the same under an ideal measurement. This is classical. So even though the
overall state map is not factorizable, it's a sum of factorizable terms. Classically, that's always true.
There is a growth in correlational entropy. We could calculate the new entropy of the
pointer variable and also the combined pointer variable and subject system, and they've both
grown to equal the original entropy of the systems measured. And we interpret this as information
has been communicated from the subject system into the apparatus. The environment then quickly
joins the classical correlation. Now, we have a sum over triply factorizable terms,
and the environment grows to have the same correlation. The measuring device initially
has an insider view. I mean, initially, after it does its measurement, I'm sorry, before the
environment jumps in, the measurement device has insider view. After the measurement device has
correlated with the subject systems underlying variable, it now knows that specific underlying
value. If there were a passive observer, so we're setting up here a Wigner's Friend type setup,
Wigner's Friend is the measuring device. Wigner is this passive observer who has not
talked to anybody, has not interviewed, has not looked into what's been going on.
The outsider view does not know the underlying value of A. The insider view, Wigner's Friend,
would now use a trivial state map because Wigner's Friend knows the outcome, knows that it was little
A. The outsider view wouldn't. The outsider view is still going to use the old state map because
the outsider view has not learned any new information yet. So we see right away that the state maps
here are perspectival classically. And in particular, we also see that the insider view sees collapse,
but the collapse is not a physical process. It's just an objective process of updating,
updating based on the new information, and it's, again, perspectival, but not ontologically
perspectival. The subject system always had that value before it was looked at.
All the systems always had their values, although those values have changed, maybe.
So the pointer variable has changed. But now Wigner's Friend, the measuring device,
where they give it that way, would naturally use a different state map from Wigner.
That said, these two state maps, despite being different, they do agree the level of an ensemble
average. If the outside observer, Wigner, considers all the possible outcomes, evolves them all with
time, and averages them using the overall uncollapsed state map, that should be compatible with the
particular evolution seen by Wigner's Friend, who got a particular outcome. There should be
a compatibility level of ensemble averages. All right, well, now let's do the quantum case.
Here I have a quotation from Asher Perez, quantum phenomena do not occur in a Hilbert
space taker in a laboratory, and that's where we'll see this happen. Okay, so similar setup,
subject system A, measuring device, pointer variable D, environment E.
Again, initially, the measuring device does not know the value of whatever wants to measure
with the subject system. It's got some initial value labeled by E0, environment labeled by E0.
In this case, to make it a quantum story, I'm going to assume a state map whose Hilbert space
representative state vector would be a quantum superposition over possibilities with square
probabilities. The overall state map is initially factorizable uncorrelated. The measuring device
now goes in and measures the underlying value of A. Now the overall state map has direct entanglement
between A and D. This is new. This did not happen in the last case. This is known by some as a
pre-measurement, which is sort of funny because the measurement has already begun to happen,
but it's called pre-measurement anyway. We have a factorizable overall state map for environment
and then the subject plus the pointer variable system, but the subject plus pointer variable
system is not factorizable. In particular, that state map is still pure. It's given by a single
state vector and it's of this form. Now note, the entropy at this point of the subject system
and the pointer variable is actually zero. That was not true in the classical case. The classical
case, after the step, the entropy grew. Information was communicated from the subject system to the
apparatus. Here, the entropy is still zero, which already makes me very suspicious that we
should think that information has actually yet been transmitted and correlations have yet been
introduced. In the platonic interpretation, it says, don't assume. Yes, there's entanglement.
Yes, the entanglement predicts that ultimately there should be some correlation, but at this point,
no. At this point, the pointer variable and the subject system's ontic variable,
they do not necessarily have correlated underlying ontic values, even though there's entanglement.
But now the environment jumps. It gets entangled everything and now when we trace out the environment
to look at the decoherence going on between the subject system and the pointer variable,
now the state map for that bipartite system, subject system and apparatus, now it has a
separable form, just like in the classical case. If you compute the correlation entropy,
now we'll agree with the classical case. The platonic interpretation says this is the moment
when correlation has been established. Information has been transmitted. Things have lined up.
The students have compared notes and gotten their answers lined up after the decoherence step,
which is a physical step that actually can make ontic variables move around.
And this is the famous step at which the nonlocality jumps in and I'm biting the bullet.
Biting the bullet and nonlocality. Nonlocality happens at this step, but this is where it jumps
in. And I'm personally not persuaded that any other interpretation does away with nonlocality,
but that's obviously up for debate. Now, many of the more serious no-go theorems,
Potion-Specker, PBR and so forth, they don't attach. They don't attach because they assume
crucially that you have entanglement and you have joint probabilities at the same time,
but we don't assume that. That's not assumed. And so the platonic interpretation neatly evades
all of those no-go theorems. GHC, it avoids all of them. However, this rules out like
the theory is inconsistent. It does have a lot of nonlocality and the GHC, no-go theorem,
the implication that there's nonlocality going on, that part still attaches. But at
least we don't get a fundamental ontological paradox. We do, however, still get some nonlocality.
Measuring device, again, has the insider view. It knows the specific underlying value. Passive
observer, Wigner on the outside, doesn't. Same as the classical case. This is identical to the
classical case. Insider view uses the trivial map. It knows the outcome. Outsider view uses
a decoherent state map because the environment has come in. Although the outsider doesn't know
the specific results, the outsider is going to use a classical looking decoherent overall
state map to make future predictions. And thanks to decoherence, these two state maps
will be compatible at the level of a suitable ensemble average, as in the classical case.
So quantum state maps collapse, but decoherence is separate from this. The decoherence process
was physical. It moved onto things around and established correlations. But the step of replacing
the old state map with a new one after the appearance process by the inset observer,
that's a subjective change. The observer knows the final results and is going to use that and
use the rules of quantum mechanics to predict future evolution of the system from that point.
I would argue this is actually implicit. When you take a branch by branch point of view,
we have a reading approach after the measurements, that each newly branched post-measured observer
is going to use a subjectively updated state map in a similar way. And also it's explicitly done
in the Bohmian approach, where we explicitly update state wave functions based on measure
of outcomes. But we know that the person who gets the result is going to see a different
wave function from someone on the outside. Okay. So according to the Platik interpretation,
decoherence, again, is a physical process. It actually realigns onto properties, but
collapses subjective epistemic conditionalization that lets the insider make better informed
predictions using whatever dynamical rule is then most appropriate for further evolution.
Trojanger equation, master equation, semi-classical methods. And again, I would argue that a similar
subjective update of state maps for post-measured observers is a feature of other interpretations.
This isn't actually new, I would argue. According to the Platik interpretation,
Trojanger's cat is always analogically dead or alive. We pick our ontic variables to be
particles, locations with a particular configuration. And that means there's
an ontic fact of matter at every moment about whether Trojanger's cat is dead or alive. Opening
the box allows the environment to decoher everything. It allows us to see what the answer
is, but it doesn't make Trojanger's cat jump from being a superposition of alive and dead to
one or the other. So here's the conclusion. I'm just about out of time. This is a quotation from ASAP
that I particularly in Fonda, beware lest you lose the substance by grasping at the shadow.
The shadow here, I would argue, is Hilbert space pictures or even C-ster algebras. The substance
is the underlying ontology. So each quantum system, according to this approach, has an ontology,
has an ontic state consisting of ontic properties. And this is what probabilities are probabilities of.
There's a thing for probabilities to be probabilities of. Could be positions invented from
mechanical systems of particles, local intensities, rates of change for field theory.
There's also epistemology and epistemic state consisting of the degree to which
those underlying values are manifest in the external world, coded in individual
probability distributions and their relationships. These ontic and epistemic
ingredients follow complicated but ultimately summarizable nomology, set of rules and laws.
We're going to code all these ontic properties, their individual probability distributions,
as ontic random variables, and we can codify all the individual probability distributions,
their detailed interdependencies, their rules, all of this into a set of state maps, chosen
empirically for the given quantum system, for empirical adequacy, which then determines a
c-ster algebraic structure. Maybe not a unique one, but it doesn't have to be unique. On the set
of ontic random variables, that original choice has to be such that it generates an empirically
adequate c-ster algebra. And then we can talk about when it is that joint probability distributions
exist. They exist in these special circumstances. And in particular, this novel ingredient that
they don't yet exist when there's entanglement, not until the state map is separable.
So, given a c-ster algebraic set of state maps for the system,
where the set of state maps actually determines the structure, the algebraic structure of the
resulting c-ster algebra, the genus construction automatically spits out a Hilbert space picture,
which we then see as a useful software layer. There's no need to reify it or attach a direct
meaning to wave functions, just like we shouldn't read too much into classical, canonically
transformed phase space variables, Hamilton's principle functions, symmetric manifolds,
non-commuting Kuhl and von Neumann operators, and so forth. And then there's the obligatory
quotation from Korzybski, a map is not the territory it represents, but if correct has
a similar structure to the territory, which accounts for this, it's not confusing the map with
the territory, which was, I think, also part of the motivation, the inspiration for Ren√©
de Brix, you know, famous pipe. Great. So, are there hidden variables here? Well,
ontic properties are the starting ingredients. I'm not supplementing an existing Hilbert space
picture that I'm taking seriously and regarding as physical and ontological with additional
ingredients. I'm doing that I do classically. We have some ontic properties. Newton says,
oh, there's some particles floating around that's my ontology. I'm going to layer mathematics on top
of it. I'm going to layer on calculus and vector functions and differential equations and use these
big predictions. But then you don't go back and say, well, now I guess the original particle
just hidden variables. I mean, no, the hidden variables are all that's really there and the
patterns of their behavior. So going back to that sequence of numbers, there's ontic variables,
ontic properties, and they exhibit patterns of behavior, statistical patterns of behavior.
And that's what's there. And everything else is just convenient mathematical layering that lets us
make predictions and formulate and work with these things in a convenient way. So this point of
view, it's the Hilbert space pictures, unobservable wave functions linear operators and so forth.
And those are the hidden variables. There's no Heisenberg cut. There's no special parameters
like spontaneous collapse approaches. There's actually no modification to textbook quantum
dynamics. This actually validates a lot of this stuff we talked about. We talked about, oh, yeah,
there's an article, it's fluctuating, and we don't know what its position of mental or whatever.
That way we talk is actually validated to some degree by this approach. There's no
privileged status from the agents at all. What does it mean for a system to do the measurement
device? It just has to be able to correlate with a point of variable with the thing being measured.
It's got to be big enough that you get good, robust decoherence. There's nothing else special
that measurement devices have to have. There's no need to reify ever ready in branches.
There's no need for ad hoc guiding equations. After measurement, you just use quantum mechanics
to predict what systems they do. If it's a big classical type system, well then semi-classical
method is going to give you very good predictions about the average behavior of the underlying
ontic variables. But in particular, and this is crucial, there's no need to derive an ontology
from the configuration space, from the domain that wave functions live on. This is important because
if you just take a flat-footed attitude toward fermionic field theories, fermionic field theories,
to the extent they have wave functions, they're wave functions defined on a grassman anti-commuting
set of variables. That's really tricky for talking about the Bohmian type approaches,
trying to derive an ontology from them. Here, not a problem at all. Fermionic field operators
anti-commute fine, but that anti-commuting is just a statement about the epistemic
behavior. Fundamentally, fermionic fields have real or complex values like bosonic fields do.
The fact that there's no eigenstates for them, mutual eigenstates for all the field operators,
there's no wave functions, is completely immaterial. In this point of view, we can assign
fermionic fields local numerical values, just like bosonic fields. After all, their expectation
values are not grassman numbers, their expectation values are really complex numbers, just like
they're for bosonic fields. And so this just eliminates a huge source of problem for certain
approaches. And that's it. Thank you.
