When learning calculus, you are probably accustomed to the idea of higher-order derivatives.
The first derivative indicates the slope of a graph, the second derivative indicates concavity,
and so on.
Calculating the nth derivative of a function is taking the derivative of the function n
times.
And this made sense, but what does it mean to take a fractional derivative?
Today, we're going to be exploring another branch of calculus, fractional calculus.
This expression can have multiple meanings.
First, it can be thought of as repeated differentiation.
So if we take the nth derivative of a function, it means taking the derivative n times.
However, this only makes sense for positive integers.
If we are to extend this to other numbers, we must think of this expression as a transformation,
something that takes in a function as an input and gives a function as an output and not
repeated differentiation.
Now we look at d to the nf by dx to the n as an operator that transforms f into its nth
derivative.
Let's establish a bit of notation.
When I put i next to a function, I mean the indefinite integral of the function, from
0 to x.
As we talked about earlier, we can think of this as a transformation, something that
takes in a function and outputs a function.
Similar to differentiation, we put an exponent like thing to denote the nth integral of f.
So for example, the 3rd integral would look something like this.
As we increase the number, the expression gets more and more complicated.
And complex analysis legend Augustine Louis-Coshy found a way to look at repeated integrals
like this in a much simpler way.
He proved the following formula.
i to the n f of x is equal to 1 over n minus 1 factorial times the integral from a to x
of x minus t to the n minus 1 f of t dt.
Let's see if we can prove this for n is equal to 2.
Plugging in the formula, we get the 2nd integral of f is equal to the integral from 0 to x
of x minus t times f of t dt.
For simplicity, I'm going to assign g of x to the 2nd integral of f.
So our goal now is to show that g of x is equal to i squared f of x.
Using properties of integrals, we can split this into two integrals.
Now let's differentiate both sides.
The first term requires the use of the product rule, and we can also use the first fundamental
theorem of calculus.
Using all this, it's equal to the first integral of f.
Since g of 0 is equal to 0, g of x is equal to g of x minus g of 0, which is simply the
integral from 0 to x of g prime of x by the second fundamental theorem of calculus.
This is equal to the second integral of f of x, so we have proved g of x is equal to
the i squared f of x, and so the formula works for n is equal to 2.
A proof for all positive integers n can be accomplished by using the binomial theorem.
But that's a challenge I'm going to leave for you to do.
For now, let's assume this formula works for all positive integers n.
Now the real question is, how do we define this function for any positive number?
The answer lies within the gamma function.
The gamma function, which looks like this, is equal to n minus 1 factorial for all positive
n.
The gamma function is an extremely important function that appears all over math and deserves
a whole video.
If you want the video on the gamma function, let me know in the comments.
It gives us a way to extend the familiar domain of factorials from positive integers to positive
real and even complex numbers.
Basically, the goal of the gamma function was to define a smooth curve that would go
through factorial points.
Here, I've plotted three points, which all satisfy f of x is equal to n minus 1 factorial.
So for x minus 1, the y-coordinate is 1 minus 1 factorial, which is 1.
Similarly, for x is equal to 2, the y-coordinate is 2 minus 1 factorial, which is equal to
1.
Similarly, for x is equal to 3, the y-coordinate is 3 minus 1 factorial, which is equal to
2.
The gamma function is defined as follows.
It's the integral from 0 to infinity of e to the negative t times t to the power of
z minus 1.
Here's a challenge for you.
Try to see why this equals n minus 1 for positive integers.
Hint, it has to do with integration by parts.
Since the main thing restricting the domain of our formula for repeated integration is
the factorial, we can replace this with the gamma function.
Recall, gamma of x is equal to x minus 1 factorial, so it still works for positive integers as
it should.
But now we can plug in any positive number for n and get a value for this integral.
Fractional integrals sound absurd, but this is a valid operator.
This particular operator is called the Riemann-Luyville integral, or RL integral for short.
Although there are many other ways of going about fractional integration, the RL integral
is probably the easiest to understand.
We can also plug in complex numbers into n for re of n greater than 0.
There are a few important relations to understand with RL integration.
The first is that i to the a times i to the b of f of x is equal to i to the a plus b
of f of x.
And the second one is that the derivative of i to the a plus 1 is simply i to the a.
Notice how we are still preserving the first fundamental theorem of calculus by the second
formula.
One might assume that fractional differentiation can be accomplished by assuming that d to the
n f of x by dx to the n is equal to i to the negative n over f.
But this doesn't really work because the gamma function is not defined for numbers with real
components less than 0.
We're going to have to be a bit more creative.
Let's remember that for all positive integers n, we understand that the nth derivative of
the nth integral is the function itself.
So as we define fractional derivatives for any positive number, we must follow this property
too.
One other thing that we'd like is to write this in terms of functions we understand.
We understand positive integer derivatives and positive number integrals.
So fractional differentiation operator that follows this property written in terms of
operators we understand can look like this.
d to the n f is equal to d to the seal of n divided by dx to the seal of n of i to the
seal of n minus n of f.
That was a mouthful.
The operator seal is the ceiling function which basically rounds up any decimal.
For example, seal of 4.1 is 5, seal of 4.6 is 5 and seal of 4 is 4.
Now using the Cauchy formula for repeated integration, we can write a formula for the
fractional derivative in terms of regular positive integer differentiation and fractional
integration.
This is the left Riemann-Louis field, fractional derivative.
Looking at this monstrosity, it's pretty clear why there hasn't been any significant
applications of fractional differentiation in the past 300 years.
Computing these without a computer is pretty tedious and difficult.
When n is equal to 0.5, we call this a semi-derivative.
Combining fractional differentiation and fractional integration, we can define an operator called
the differ integral by the following piecewise definition.
Let's look at a nice animation that demos our differ integral operator.
Let's start with the function f of x is equal to x.
The first derivative is f of x is equal to 1.
And the first integral is f of x is equal to 1 half x squared.
Now, let's see how the function varies, applying our differ integral operator from n is equal
to negative 1 to 1.
One thing to note is that familiar properties like the chain rule and the product rule don't
really work as well anymore, while this is to be expected and they can exist in much
more complicated forms.
There is one property that makes fractional derivatives useful.
Normally, when we take first or second derivatives, the output of the derivative only depends
on the input we give it.
This is called locality.
If we go back to the definition of our fractional derivative, we have this little constant a
at the bottom of the integral.
Thus the fractional derivative has non-locality, which means it doesn't only depend on an input.
This is useful in analyzing functions that only don't depend on time.
For example, some phenomenon in the real world have something called the memory effect, which
means that the current state not only depends on time, but also on previous states.
Traditional differential equations have a hard time modeling phenomenon like this, but fractional
derivatives can make the task easier.
Let's look at a few examples of fractional derivatives.
The fractional derivative for x to the n for n is greater than 0, which is our power rule
is given by d to the a x to the n is gamma of n plus 1 divided by gamma of n plus 1
minus a times x to the n minus a.
A special case of this is the semi-derivative of f of x is equal to 1, d to the half of
1 is 1 over the square root of pi times x.
And for f of x is equal to sin x, we have d to the a sin of x is sin of x plus a pi
over 2.
For the exponential function, the fractional derivative is given by d to the a e to the
k x is k to the a times e to the x k.
Let's look at one application of fractional derivatives, the tautochrone problem.
The tautochrone problem looks for a curve that does not depend on the initial height
for the time it takes to reach the bottom of the curve.
What that means is, let's say we let an object go from any point on the curve, and the only
pulling force is gravity, it will always take a certain time to reach the bottom of the
curve.
You can see that by the animation here.
Niels Abel, the one known to solve this problem, came up with the following equation to calculate
the time taken to reach the end of the curve, where y of 0 is the initial height.
Here s is the curve parameterized.
Abel solved this problem using convolutions and Laplace transforms, but using fractional
calculus, we can see a much quicker solution.
If we divide both sides by gamma of half, which is equal to the square root of pi, and
move the constant to the other side, we get this.
Now if you notice something, is that this is in fact a semi integral, so our equation
just ends up being the square root of 2g over pi t of 0 is equal to i to the one half
of ds by dy.
Now to calculate ds by dy, we simply take the semi derivative of both sides, and we
get the following equation for ds by dy.
And in fact this differential equation describes a curve which is called a cycloid.
And the cycloid is a curve you get when you roll a circle and focus on a point.
This curve is also the solution to the Brachiestochrone problem, which tries to solve for a curve
that takes the fastest path from one point to the other, only pulled by gravity.
This topic is really interesting and I ve left a few videos in the description if you
are curious.
The idea of fractional calculus, although it doesn t have that many applications, does
demo an extremely important lesson in math, and that s to try to break the rules and see
what happens.
This is what led to the discovery of so many things in math, like complex numbers, when
people tried to take the square root of negative numbers.
And although it s hard to see fractional derivatives geometrically, it s quite a fascinating topic.
Thank you for watching.
.
