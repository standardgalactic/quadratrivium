All right. Thanks very much. So I'm very grateful to the organizers for organizing tutorial
day. Sorry, I just realized there's a slide bar on. I'm trying to turn it off. Okay. So
thanks to the organizers for having tutorial day. I think it's a really, really great idea.
I know it's probably a little bit of a whiplash for folks in the audience because we're all
sort of assuming slightly different background and trying to speak to slightly different
audiences. But the hope is that everyone will be able to get something out of some talks
and some people will get something extra special out of a certain talk that's aimed just exactly
at whoever they happen to be. So let me explain the point of this talk. So if you get a pretty
into category theory, so this is not just, you know, knowing the definition of category
and functor, but you're like, you know, want to absorb the categorical worldview for whatever
reason. At some point, you're going to start hearing about the yenaid alama. And the reason
is the yenaid alama is just really the fundamental theorem of category theory. There is a game
that I know some category theorists play, which is one person supposed to state a theorem.
And the other person is supposed to figure out how to prove the theorem using the yenaid
alama. And it's sort of surprising how often that works. It's just, it's everywhere, you
know, then of course it's hidden in other areas of math as well, but secretly have some
category theory behind the scenes. And it's just a really, it's like a magic tool. So
the problem with the yenaid alama, though, is it's very, very, very hard to understand
the first time you see it. It's one of those things that you have to get used to it. And
then once you get used to it, you can sort of understand it. And then years after you
thought you've understood it, then you like all of a sudden really understand it. So it's
a, you know, it's sort of a long, slow process. And it's not something that you can expect
to learn in a single day. So, you know, the first time you see the yenaid alama, it'll
be a bit of a mystery, but then you'll see it again, and you'll see it again. And then
eventually you'll get it if, if again, you're so inclined to try and understand the yenaid
alama. So that, that's who this talk is for as somebody who wants to understand the yenaid
alama. If you're brand, brand new in category theory and don't yet want to understand the
yenaid alama, then I apologize, but this might be kind of like an eating your vegetables.
So, so what's the plan for the talk? So the yenaid alama I've warned you is this very,
very abstract thing and part of what it's so abstract about it as it involves sort of
natural transformations, which are very weird concept. And then there's actually some
extra naturality involved in the statement, which is something already about natural
transformations. And like, maybe there's set theoretic issues that we're concerned about
and so on and so forth, but none of that is going to appear in this talk. Instead, what
I've done is I'm specializing to kind of the most concrete setting that I can think of
where the yenaid alama has something interesting to say. And that's the category of
matrices. So in this talk, there will be a single category that we're going to talk about
matrices. And we're going to see what the yenaid alama has to say about matrices. And that's
the entire entirety of the talk. So there are 17 slides. I'm going to try and go at a pretty
leisurely pace. You shouldn't feel shy about interrupting me. We've got plenty of time for
that plenty of time for side discussions. I hope we have a discussion at the end. So,
so if I say something and you don't get it, but you want to just, you know,
ask me a question and we'll see what we can do. Okay, I guess the last thing I want to note is
there's a, this, this is dedicated to category theorists who unfortunately died in September
2017, but who I, you know, had the few years of acquaintance before this is to Fred Linton,
and I'm going to say a little bit more about his role in the story at the very end.
Right. So let's go ahead and begin. So I promised this talk is going to feature a single category.
It's called the category of matrices. And so the first thing I have to tell you about is what is
this category of matrices? This is the only category we need to know in the talk. So what is
a category? So again, the idea is it's something that has some objects and has some arrows between
objects. And then there's a composition law that allows you to compose arrows, assuming there are
sources and targets match up, and then there's some axioms. Okay. So what is the category of
matrices? So when you declare a category, you can choose anything at all that you want for its
objects. And in this case, I want the objects to be natural numbers. So zero, one, two, three, you
know, they're countably many of them. Those are the objects of the category, essentially just because
I said so. So if you're this sort of person who's inclined to be confused about zero, pretend zero
is not a natural number. I guess I listed it because I secretly wanted to be there. But
if zero is going to confuse you, just pretend it doesn't exist. Okay. So we have a category. I've
told you what its objects are. They're natural numbers. Mostly I'm going to use variables for
these natural numbers. So like j, k, m, n, those will all stand for natural numbers, the objects
of this category. And they're all in orange to just help us remember that they're the objects. So
these natural numbers are playing the role of objects in this category, and they will be in
orange. Okay. So then a category also needs to have arrows. And I'm going to tell you what an
arrow from n to m is. I've drawn the arrow in a little bit of a funny way. I'll say something
about that in just a second. But an arrow from n to m, those being two objects in my category of
matrices, is an m by n matrix. So if you like, this is one of these locally small categories,
if that's a term you're familiar with, the set of arrows from n to m in this category
is the set of m by n matrices. So going from n to m, it's an m by n matrix. And that's just
the way I want it to be for whatever reason. You could have picked the opposite convention,
but I want this one. So we're going to go with that. I didn't specify what the coefficients
for these matrices are because it completely doesn't matter, but you should pick your favorite
field. So the real numbers, if you like, these are m by n matrices of real numbers. And if you
don't know what I meant by that sentence, just pretend I didn't say it. Okay, so I'm trying to
define a category, category of matrices. The objects are these natural numbers. The arrows are
these m by n matrices if I have an arrow from n to m. So I need to explain now how to compose
arrows. So if I have a k by m matrix and an m by n matrix, so that's an arrow from n to m
and also an arrow from m to k. Does anyone want to guess for me what the composite
arrow from n to k will be?
A way to guess would be to drop a guess in the chat, if you will.
Right, so what I'm looking for here is, so the composite of an arrow from n to m
and an arrow from m to k will be an arrow from n to k. So a k by n matrix, absolutely,
but it needs to be a specific one. So if my arrow from n to m is called a, it's the matrix a,
and my arrow from m to k is a matrix b, then I need a specific k by n matrix to be their composite
and it's defined by an operation that some of you have identified, matrix multiplication, absolutely.
So right, so if I have my arrow from n to m and my arrow from m to k, in this category that just
means again an m by n matrix and a k by n matrix, I've drawn them in sort of a schematic way over
here and if you see your m rows and n columns and your k rows and m columns, you can multiply them
and you'll get a k by n matrix and that's the composite. Okay, just because I'm looking at the
chat, you do not need zero as an object, so if it makes you unhappy, throw it out, but I'm happy
to consider a matrix for one of the dimensions of zero, it's just an empty matrix, there's no
data involved, but okay. So the other, another thing you need in a category is something called
an identity arrow, so this should be a specified arrow from n to n for any object n. So in this
context in the category of matrices that should be a specified n by n matrix, what is it?
All right, so I'm looking for a special n by n matrix, something that I can define for any n
and I mean answering this question is easier if you remember the rest of the category axioms,
so this matrix needs to serve as an identity for composition, so if I composed it with an arbitrary
n by n matrix, I need to get that n by n matrix back and absolutely it's something that's called
the identity matrix, this is what it looks like with ones along the diagonals and zeros everywhere
else, absolutely, and what's special about this matrix again is if I multiply it by an n by n matrix
a, I get a again and that's the axiom of identities, so we also need an associativity axiom, matrix
multiplication is an associative operation, you might remember that matrix multiplication is not
a commutative operation, I mean often it doesn't even make sense to try and multiply matrices
the other way around and this is exactly the behavior we expect in a category, you don't
expect a commutative composition law in general, so okay, so that's the category of matrices,
this is the only category we'll need for this talk, I mean and maybe secretly sets, but okay,
so first of all good, so just as a quick recap, the main components of this category of matrices
is the objects are natural numbers like k, m, and n and an arrow from n to m is an n by n matrix,
oh I forgot to explain why I'm drawing my arrows pointing left, so this is, this is something I
learned from one of my PhD students, Salo Klingman, you know usually category theorists agonize over
whether to write composition in sort of composition order or in diagrammatic order, if you're familiar
with those two terms and drawing your arrows pointing left allows you to have both, so you know
there's this thing called composition order that means that I really want to write the
composite of a with b as b times a as opposed to a times b, but so I, for secret reasons of my own,
I really want to write this composite as b times a and it's just sort of less confusing if the
arrows go to the left because then everything's kind of on the correct side, it's going to make this
m by n match up visually with an arrow from n to m pointing to the left, but anyway it's just a
convention, so okay, but no this is not intended to be the opposite category, this is not, I mean
you could consider an opposite category of this, but even though I've drawn the arrow going from
right to left, n is the domain of a and as the source of a and m is the codomain, so the direction
of the arrow still indicates what the source and what the target is, I'm just drawing it to the
left, so when I compose I can kind of write the name of the composite in the same order as it
looks visually on the page, that's the only reason. A diagrammatic order is what it's sometimes called,
which essentially means it should look, well maybe I'll leave that for after the thing.
Okay, so right we have this category of matrices, objects are natural numbers, an arrow from n to
m is an m by n matrix, okay, and the next, so there are two key players in this talk and one of them
is, well I guess an example of a functor, so I should remind you what that is, so
a functor, the data of a functor is given by a set hn for each natural number n,
so the reason it's indexed by the natural numbers is those are the objects of this
category of matrices, and then I also need a function between these sets associated to each
matrix, in other words to each arrow in the category of matrices, so that's the data of a
functor, and that data is required to satisfy two axioms, these functoriality axioms,
and the first of these says that if I'm looking at the function that's associated to the identity
matrix, it's the identity function, so here I haven't indicated in any way what the elements
of the set h of n are, but the identity function is the function you can define for any set that
sends each element to that element itself, and then the second functoriality axiom is about
composition, so it says that if I have a pair of composable arrows, so a pair of an m by n matrix
and a k by m matrix, then I could, I have the associated function from hn to hm, and from hm
to hk, and if I compose those functions, what I get is the function that's associated to the
composite matrix, so the function, the composite function from hn to hk is the function associated
to the composite matrix b times a, so these are the sort of functoriality axioms, it would be
reasonable notation to refer to this as ha rather than a, I've just declined to do so because it's
sort of less cluttered, so it's an abuse of notation that I actually think makes things
clearer, but you may, you may not disagree, yeah. Okay, great, so what's next? So the,
there's only one sort of functor that we're going to consider today, so if you want to forget about
the general notion of functor from matrix to set, this is the only example that we'll meet today,
and I'll just introduce you to it independently, so we're going to call it the k column functor,
and k here is again one of these objects in the category of matrices, in other words, it's just
a natural number, and I mean, a way to think about, yes. Can I ask a question? Yep.
So just a notational point, so when you write the function a acting on
h of n, shouldn't, is it better to write h of a because it's a function
that lives in the category set, right? So it's, right, it's quite common to write h of a here,
but there's this procedure called abusive notation where sometimes you sort of write
something simpler than what it would be the more proper thing to write. So, yeah, so many people
would write h of a for that and mean exactly what I mean here, but my view is because of the context,
because the name a is now stacked over an arrow from h of n to h of m. Okay, all right. It's gotta
be something slightly different, but it's essentially just associated to a, and I'm just trying to keep
the notation as simple as possible. Oh, okay, all right, thanks. Yeah. Right, okay, so the, and I guess
part of the reason for the simplified notation is there's really only one example that I'm going
to be particularly interested in, and this is an example of what I'm going to call a k column
functor. And so this is the only sort of functor we care about. So if you're unfamiliar with the
general notion of functor, you're entitled to forget it and just remember these sort of k column
functors. And colloquially, what the k column functor is, is it's the set of all matrices
with k columns. So that's maybe even a more useful way to think about it. So the k column
functor is the set of all matrices with k columns. So it's a bit more than that. I mean,
really, the k column functor organizes that data in a better way. So the first thing to say is it's
really a graded set, meaning for each natural number n, there's a set h k n, that's what I've
written here. And it's the set of matrices with k columns and n rows. So if I, if I just thought
about the set of all matrices with k columns, it makes more sense to break that up into a set
where you organize k columns in one row, k columns in two rows, k columns in three rows,
k columns in four rows, and that's what these h k n's refer to. So k is telling us we're in the
k column functor and n is telling you how many rows we're considering. So really, the k column
functor records the data of all of those sets. That's his first bullet point in a functor. A
functor is a set for each natural number. But then there's a second bit of data as well,
which says that for every m by n matrix, so every arrow from n to m in this category,
I need a function from the set, in this case, of n by k matrices to the sets of m by k matrices.
So I'm wondering,
yeah, if somebody wants to, before I was going to ask a question, but if you want to ask a question,
go, go for it. Sorry, I changed people's settings so they couldn't unmute themselves,
but I'll change back. So now you can unmute yourself. But yeah, I think generally, well, yeah,
good for. Okay, well, this doesn't strictly satisfy Paolo's criterion for urgency.
So I won't ask such questions again. But so just a notational question.
If I want to write down, let's say, so h of n is an object, right? And a is a morphozoom.
So is there a notation for saying that, you know, h of n belongs to
something like OBJ set? Is there a notation like that? Because, you know, set of the category
it has morphisms and. Sure, that seems like a reasonable thing. And you would just kind of
write it like that. HK of n is an element of the collection of objects of the category of sets.
And just don't worry about set theoretic issues there. So it's not an official convention.
Lots of, I mean, I don't know that how many, yeah, lots of people use that notation. It's very common.
Okay, so what I'm trying to introduce you to is the K column functor. So what we've
understood so far is that essentially, it's just the set of all matrices with K columns.
But it's better to think of that data as organized as a graded set. And what that means is that,
you know, really, we're thinking about it as this family of sets, HK n, where that's the
set of n by K matrices or matrices with K columns and n rows. And so that's not all of the data
of a functor. So part of the data of a functor is I need a function from the set HK n to the set
HK m associated to a particular m by n matrix A. So suppose we've picked an m by n matrix A.
Can somebody think of a function that would convert an n by K matrix C maybe to a
m by K matrix? So I want a procedure using an n by n matrix A to convert an n by K matrix C
into an m by K matrix. Yeah, and absolutely. The idea is we can multiply by the matrix A on
the left. So if I have C, my K by n matrix, hit it with A, multiply on the left by A. And the
result here, since this is an m by n times an n by K will be an m by K matrix, exactly what I wanted.
Okay, so that's all the K column functor is. So if you think about the set of all matrices with K
columns and what its natural structure is, it's exactly the structure of one of these functors.
I mean, that's sort of recording the rows as well as the columns and observing that you can
change the number of rows in a matrix while keeping K columns by multiplying on the left
by some other matrix. Okay, so that's going to be one of the key players today, this notion of a
K column functor that I'll now summarize. So again, it's given by this family of sets, the sets of n
by K matrices. And then we have these functions. So for any m by n matrix, I can convert a n by K
matrix into an m by K matrix just by multiplying. Okay, and then the second key player today, and
this is I guess really the main player of the talk is something that we might think of as a naturally
defined column operation on column functor. And for the category, sort of the category
theoretic fluent among us, what I mean is just a natural transformation. So a natural transformation
from the K column functor to the J column functor, where K and J are two natural numbers. In other
words, two objects, they might be the same or they might be different. So a natural transformation,
that's a thing in category theory. And we're going to think of it as a naturally defined
column operation between column functors. Okay, so what is it? So firstly, I mean, informally,
what it is, is an operation that's going to convert matrices with K columns into matrices
with J columns. Because remember this HK kind of stands for the collection of matrices with K
columns. And HJ stands for the collection of matrices with J columns. But it's something a
bit more precise than that. So firstly, the data of a natural transformation is given by a family
of functions for each N. So the K column functor really is like the family of N by K matrices.
The J column functor also has a family of now N by J matrices. And so part of the data of a
natural transformation, one of these column operations is it's going to be a function that
converts an N by K matrix into an N by J matrix for each N. So we can convert one by K to one by
J, two by K to two by J, etc. And then there's a condition. C here is just the name of a particular
matrix. That's an example of a N by K matrix. There's not a functor associated with each matrix.
There's a functor associated with each dimension. So the K column functors are associated with each
dimension. So yeah. Okay, so right, so natural transformation, again, the data. So this is
from the K column functor to the J column functor. The data is it's a family of functions that converts
N by K matrices to N by J matrices. But then there's a condition and it's called the naturality condition.
And what that condition says is we have all of these functions involved. So this alpha remembers
this natural transformation. It's this natural column operation. It converts an N by K matrix to an
N by J matrix. But also whenever we pick an N by N matrix, we get a function just involving the data
of the K column functor or just involving the data of the J column functor. These are the functions
that multiply by the matrix A. And so what this diagram says is that if I started with an element
maybe C of the set in the upper right, so if I started with an N by K matrix C, I could apply
the column operation, whatever it happens to be. And when I apply the column operation, I'm going
to get now an N by J matrix. And then I could multiply by my matrix A to get an N by J matrix.
So that's starting with an element at the top right and going down and then left in the diagram.
Or I could go across and then down. And what that means is I start with my matrix C, I multiply it
times A. So now the matrix A times C. And then I apply my column operation to convert this N by
K matrix into an N by J matrix. And what naturality says is those two procedures should be the same.
Okay, so this is a weird definition as I indicated at the start part of what's complicated about the
Uneta lemma is it concerns natural transformations and that's a little bit of a weird
definition. But let's see what it looks like in some examples, sort of how this
naturality condition goes on. Okay. Oh, so informally again, and the way we'll speak about it,
I'm not going to say natural transformation very much. I'm going to say that alpha is a naturally
defined operation on column functors. So it's some procedure that takes a matrix with K columns
and gives you a matrix with J columns. But it's, I mean, you can imagine a lot of procedures,
but only some of them will be natural. And we're going to start to see in examples which ones are
natural and which one are not. So I'm not allowed to, I mean, this procedure needs to respect the
number of rows. So that's kind of part of saying this natural transformation has these functions
alphas of N. So it's a procedure that takes K column to J columns, but it has to keep the number
of rows the same. So you might think of other procedures that would change the number of rows.
Those aren't allowed. Those won't be considered today. Okay, so let's see some examples of this,
because I know it's a little bit abstract. So an example of one of these naturally defined
column operations is some sort of projection operation. But let's just sort of remember what
the key points of the definition again, the idea of a natural transformation is it's a family of
functions. So from the set of N by K matrices to the set of N by J matrices and then satisfy some
sort of naturality condition that we'll see what it looks like in an example. So one instance of
a naturally defined column operation, this will be one that goes from K columns to K minus one
column is we just delete the last column. So one example of these column operations is we're just
going to delete the last column. Okay, so I mean that's certainly an operation. You have to check a
naturality condition though. Some column operations will be natural and some won't. And so let's see
what that check amounts to in this case. So what the check says is firstly let's imagine A is an
N by N matrix. So I'm holding on to some N by N matrix in my head. And here I've drawn on the upper
left this C. So this is the matrix C, the N by K matrix that I'm starting with. But I've written it
out. This is kind of common matrix notation where I'm representing a matrix by its column vectors.
So since C has K different columns, it has K column vectors, C1, C2, etc. And maybe a way to think
about the matrix and sort of fit it all into the slide is, you know, C1 is standing for the whole
first column of the matrix. So it's an N dimensional vector. C2 stands for the second column, C3 would
stand for the third, and then CK is the last one. So in particular this operation alpha N, let's go
back to the naturality. I've arranged things in the same order as it would be on this slide. So C is
an element of this set, HKN. So that's an N by K matrix. And what I'm going to do first is I'm going
to see what happens when I apply the column operation. Let's go down to here and then multiply by A.
So when we start with C and we apply this column operation, the operation we're considering right
now is delete the last column. So that's deleting the K column. Now I have this matrix like C, but
you see the CK, the last column is gone. And then I could multiply by A. That was what these horizontal
functions that are part of the data of a column functor do. And the result then is, well, it's A
times this matrix, A times this truncated matrix with the last column deleted. Or I could go the
other way around. I could start with my matrix C and multiply by A. And sort of the way matrix
multiplication works is what I get then is a matrix whose columns are, well, I guess I put
the subscript on the outside. I could have equally put it on the inside. So AC1, the first column of
this multiplied matrix is also A multiplied by the vector C1. The subscript would be fine either way.
And then when I map down and I delete the last column, the naturally check is that I need to
compare this matrix here with this matrix. And depending on how much matrix algebra you remember,
you may or may not know these happen to be the same. So if I, the columns of this matrix,
the first column will be A times C1. And that's exactly what the first column is over here.
The second column is A times C2, et cetera, et cetera. So the summary is that this is a naturally
defined column operation. Okay. So that's one example. Let's see another one. So another sort
of column operation is an inclusion operation. So this is going to be an operation from K
columns to K plus one columns. And how it works is I'm going to append a column of zeros at the end.
So let's check this naturality condition again. So what the operation is this time is it takes my
matrix C and I'm going to get a, C was an N by K matrix. I'm going to get an N by K plus one matrix
by sticking a column of zeros at the right. Now if I multiply times A, what I get is the
matrix A times that, whatever that happens to be. Or I could go around the other way. I could first
multiply times A and I get the matrix whose columns are AC1 through ACK. And then I can stick on a
column of zeros that's applying the matrix, the column operation after I've done the multiplication.
And again, you can check these results are the same essentially because A times the zero vector
is the zero vector. So these two, these two expressions agree. So this is another example
of a naturally defined column operation. Okay. So, well, maybe I should stop here.
Give you all a chance to catch your breath and see if anybody else wants to ask questions.
Oh, while I'm waiting I might.
Yeah, look, you know, can you give a specific definition for the naturality condition in general?
So this, what is on this slide is the specific definition for the naturality thing.
So it's a bit strange. But what it's saying is for every matrix A,
what these arrows represent in this little square at the bottom are functions. So this A is a function,
this is the function which is multiply on the left by A. This alpha M is the function that is
apply the column operation to convert an M by K matrix to M by J matrix. And similarly around
the other side, this alpha N is apply the column operation to convert an N by K matrix and N by
J matrix. And this A is multiplied by the matrix A. And what the naturally condition says is that
the composite of those functions agrees. So if I started with an element of this set, so if I
started with an N by K matrix C, and I multiply by A and then applied the column operation,
it gives the same result as applying the column operation first and then multiplying by A.
So naturality is saying that you could apply the operation before multiplying or after multiplying
and the result is the same. It's a kind of high level way to say it.
This is exactly an instance of the categorical concept of natural transformation. So if you're
familiar with that, this is really just a specialization of a very general definition,
but that's all that it amounts to in this case, sort of nothing more, nothing less.
And maybe just a bit of history. I mean, part of the whole birth of category theory was
Eilenberg and McLean trying to give a pin down a precise definition for this sort of colloquial
idea of naturality. There's a sense in certain areas of mathematics that some constructions
are natural and some are not. And their insight was that there could be a precise definition
of naturality, and this is what it amounts to in this particular case.
Yes, I think it compares the action of two functors. That's a nice way to think about it.
I'm kind of a kind of random question here, but could you, is there any particularly sacred
reason why we've gone with the K column functor rather than like a K row functor? Could you just
switch everything around? It just takes the thing and basically we need to move around the
horizontal arrows, I think. Or you'd be switching around the operations, but if it seemed to be,
you know, six of these, half a dozen of the other, right? Right. So there's a dual universe
somewhere where somebody instead defined row functors and gave the entire talk about row
functors rather. All right. So it would be, okay. So absolutely, that's possible. I've just made
this choice. Cheers. Okay. So what I'm hoping we might be motivated to ask or to entertain anyway
is the following challenge or the following question. What are all of these natural transformations?
So what are all of the natural transformations?
So or in other words, can we classify all of these naturally defined column operations that
transform matrices with K columns into matrices with J columns? So I've given you two examples
of column operations. There are a whole lot more than the two that I gave you.
There are also a lot of things that aren't natural for one reason or another. Maybe they
fiddle with the number of rows or maybe they're not natural for some more subtle reason. We'll
see an example like that later. So can we understand all possible column operations? So all like
operations that transform matrices with K columns to matrices with J columns, more precisely,
they should transform N by K matrices into N by J matrices plus do so naturally. Can we classify
all of them? And this is exactly the question that the innate alum is going to answer. Okay.
Could that be another category? I'm sorry. Could there be a category of the natural
transforms? Yes. The natural transformations form the arrows in the category of functors. So in this
case, the objects would be things like the column functors, the K column functors. Those are the
objects now. And the natural transformations are then the arrows. So in particular, natural
transformations can be composed. We'll talk about that in fact later on. So yeah, absolutely.
Okay. Great. Right. So we're trying to do, yeah, some of you have some very astute ideas for how
we might possibly classify all naturally defined column operations. But I'm not going to ask this
as a question. I'm just going to tell you the answer because the point is to understand what
the innate alum is telling you. So let's just go ahead and see what it is. So here's the innate
alum in the category of matrices. So the innate alum is going to entirely answer this question
and give us a very explicit answer. So the challenge was to classify all naturally defined
column operations that transform matrices with K columns into matrices with J columns. Let's do it.
So firstly, what the innate alum says is that every naturally defined column operation from
K columns to J columns is determined by a single K by J matrix. So this is a, this is a bijection.
It says that there's a one to one correspondence between these naturally defined column operations
from K columns to J columns and K by J matrices. Okay. That's all well and good. I mean, these are
two uncountably infinite sets. But the point is you can figure out exactly what that matrix is. So
here is the K by J matrix. So it's, it's written in kind of obscure notation, but this is the
notation that you will see if you read anything about the innate alum ever. So I decided to go
ahead and use it sort of full ugly notation because part of the point is to remember what
this formula is. So, so it's going to be a K by J matrix. In other words, it's an arrow in the
category of matrices from J to K. And how do you get that K by J matrix? Well, what I do is I take
my column operation alpha. I look at one component of it. So remember, a column operation is a family
of functions, HKN to HJN for every N. But I'm going to look at the component at K, where the K is the
same K that is the domain index. And what's special is this is the set now of K by K matrices. And it
has a special element from the point of view of the category, namely the identity matrix. So this was
an arrow from K to K that was particularly important to the definition of the category of
matrices, this K by K identity matrix. And the claim is that this matrix that somehow classifies the
entire column operation is the one you get by thinking of this column operation as a function
and applying that function to the identity matrix. So in other words, you start by the identity
matrix. It's a K by K matrix. If I apply my column operation, I'll get a K by J matrix. And that's
this thing I've written here. Alpha K I J means apply the column operation to the identity K by K
matrix. Okay. And finally, so the claim was that the column operation is entirely determined by the
single K by J matrix, which you get by applying that column operation to the identity matrix.
And how is it determined? So somebody had this intuition already, which is great. So a column
operation is then so the general column operation that converts N by K and N by J is then just defined
by multiplying on the right. So that's the other side than we were multiplying before. If your
rights and lefts are a little confused, this is the other side and that's important. We're going to
multiply on the right and by this special matrix. So the claim is that all naturally defined column
operations are really just given by multiplying on the right by some matrix. And what matrix is it?
It's the matrix that you get by applying the column operation to the identity K by K matrix.
Okay. So that's, yeah, hang on for the Gaussian stuff. So this is the Uneta lemma. Let's see.
So here's the summary statement again. So the Uneta lemma says that every naturally defined
column operation, so every column operation that satisfies with natural reality condition,
is given by right multiplication by a single K by J matrix. And it's the one that you get
by applying the column operation to the identity. So that was a summary of the three points on this
side. The column operation is determined by just one matrix. You get that matrix by applying the
column operation to the identity matrix. This will be my notation for that. It means apply the
column operation to the identity matrix. And then the full column operation is just right
multiplication. Okay. So let's see what this looks like in some examples. So here's an example of
a naturally defined column operation that we haven't discussed yet. It's called, it's a permutation
operation. So this is going to be an operation from K columns to K columns. And let's consider
the operation that just swaps the first two columns. So, I mean, you could imagine a different
permutation. Those work exactly the same way, but let's just, to be concrete. So I'm claiming that
there's a naturally defined column operation, which is swap the first two columns. So if your
columns are C1, C2, et cetera, if you see K, that matrix you get swaps C1 and C2, and then leaves
the rest of the columns the same. So by the Yeneda lemma, this column operation is classified by
a matrix. What matrix is it? You start with the identity matrix on K, and then you swap the first
two columns. This is what it looks like when you swap the first two columns of the identity matrix,
the ones and zeros in this upper left square get swapped. Okay. And then if I want to implement
the operation of swapping the first two columns on a generic matrix C, what do I do? I multiply
by this special matrix on the right. That's the summary statement of the Yeneda lemmas. If I want
to implement the generic column operation, which is swap the first two columns, what I do is I
first do that just to the identity matrix. And then for any other matrix, I can just multiply on
the right by this matrix. That's an implementation of the column operation. And it's guaranteed to be
natural. I guess I don't have a slide about this, so maybe I'll pause and say it and somebody can
ask me to write it out for you at the end. I mean, the reason that it's guaranteed to be natural,
essentially, is the naturally condition is about multiplying on the left, where the column operation
is implemented by multiplying on the right. And what naturality says is that these operations
need to commute, but multiplying on the left and multiplying on the right do commute, essentially
by the associativity of multiplication. You can first multiply on the right and then multiply on
the left, or first multiply on the left and then multiply on the right, and it doesn't matter. It's
the same result by associativity. Okay, let's see another example. So another instance of a
naturally defined column operation is one that multiplies a column by a scalar. Here to be concrete,
let's say we're going to just multiply the first column by a scalar. So by the innate
lemma, this column operation that multiple, so scalar just means like a real number if we're
talking about real matrices. So if you, so this is a column operation that will take the first
column of your matrix, multiply every number in that column by some constant, which are calling
lambda, and then you return that result in the first column. So again, the innate lemma says
that this operation should be implemented by, or classified by a single matrix. And how do you get
that matrix? You take the identity and you multiply the first column of the matrix by lambda. So
that's the result here. So we took the identity matrix, we looked at the first column, which was
1-0-0-0-0. When you multiply by lambda, you get lambda-0-0-0-0-0. And so this is our special matrix.
And now the claim is that to implement the column operation, what do you have to do? You
take your matrix, your generic matrix, and just multiply on the right by this matrix. The result
of that, you can check, is that it leaves columns 2 through k alone, but replaces the first column
by lambda times whatever it had been previously. Okay, so this is another example of these naturally
defined column operations. And I'll tell you about one more. So this is some sort of column
addition operation. So this is the operation that adds the first column to the second column and
then puts the result of that in the second column. So this is an operation that takes a matrix of k
columns. It's going to leave the first, third, fourth, etc. through k columns exactly as they
were before, but replace the second column by the sum of the second and the first. And
the claim again is this is a naturally, this is classified by a single matrix. You get that
matrix by starting with the identity, so ones at the diagonals. And then what you do is you
add the first column to the second. So if I add the vector 1, 0, 0, 0, 0 to the vector 0, 1,
0, 0, 0, what I get is 1, 1, 0, 0, which you'll see is what's displayed in the second.
And so then if I want to implement this column operation for any other
matrix, all I have to do is multiply on the right by this. It'll have the effect
of leaving columns 1, 3, 4, 5, etc. alone, but replacing column 2 by the sum of column 1 plus
column 2. Okay, let me pause for some questions.
So how about the general linear matrices of size k by k?
Right, so each k by k matrix determines a column. So this is going to answer somebody else's question
in the chat, so let me start back here. So a column operation. So to specify a column operation,
first you need to say how many columns, first you specify the dimension of the start
and also specify the number of columns of the start.
So k, say, and also specify the number of columns at the end.
So like part of the typing of a column operation is you know how many columns
you're supposed to start with and how many you're supposed to end with. So for instance,
so for the permutation operations, go from k to k, and there are only interesting ones
when k is greater than 1. If k is equal to 1, there's no way to permute one thing,
so there's no interesting permutation operation, but each of these column operations is typed
by saying how many columns you start with and how many you end with. Okay, so sorry that, so
if you pick any element of the general of glnk, or sorry gln r, glk r, you said k by k. So if I pick
a k by k matrix, it will define by the yanata lemma. Let's go back. So by the yanata lemma,
in fact, I don't think I said this as clearly as I could have done.
So this first statement is really, it's a bijection. So every single, every nationally defined,
so that's a great question, thanks for bringing this attention, every naturally defined column
operation is determined by a single k by j matrix, and conversely, any k by j matrix determines
a naturally defined column operation. So this procedure, right multiplication by some matrix,
is always going to give you a naturally defined column operation. So that's an example of a
naturally defined column operation, and moreover, this is the only example in some sense, every,
well in a precise sense, given by the yanata lemma. So every, every matrix gives you a naturally
defined column operation, and column operations exactly are given by matrices.
Other questions? I'm sorry, I don't know how to like make sure I'm not cutting someone off,
so I'm just gonna quickly help us out. So I'm not super comfortable or familiar working with
matrix things. So I'm trying to think of this more generally, so I can just kind of avoid that
problem. So generally speaking, what we're saying is there's a bijection between a mathematical thing
that we might be interested in, and an operation that might help define it, and that operation,
in this situation we're talking about a column operation, but it could be something different
in a different context, would be given by that natural transformation composed with the identity
on whatever you started with. So we had... Yeah, applied to the identity, yeah, that's right.
Identity, and then... Yeah, sorry, sorry, yeah, so they're all together compositionally,
and so I guess the question I had, and this is probably because I don't understand the matrices
as well as I'd like to, is that the bijection here between that single k by j matrix, or that single
mathematical thing we're going to talk about in a different context, is somehow very closely related
to that natural transformation on the left-hand side. In some sense, they're not just randomly
associated, but there must be something very close in their connection. I'm not going to ask you to
come up with some random other example on the fly, because I don't understand matrices, but I think
I'm roughly getting the shape of it. It feels like at least. Right, so you're correct that all of this
is much, much, much more general than is indicated here. But for expository purposes, I'm only going
to tell you about this in this very one special case, and apologies if you don't like matrices, so...
Not to worry. Not your fault, definitely something I can brush up on.
Yeah, cool. Other questions?
So, just for some context.
Who? Shall I go? Yeah, go for it. You're new, so go for it, yeah.
Thank you very much. I'm loving this so far. I have a question about
how we apply unit... Basically, I'm used to seeing unit dilemma as given some functor defined on a
locally, locally small category, the natural transformations from the representable functor,
given some object to this given functor. That set of natural transformations is isomorphic to
that functor applied to this object that we just... You know what I'm talking about, but I can't see
that in this statement of the unit dilemma. We're talking about the set of natural transformations
from one column up to another. So, how is it representable? Sure, so firstly, some people
use this notation h sub something for representable functors. So, it was for people like you that I
chose to write column functors this way. So, h sub k is the functor represented by the object k.
It's the covariant functor represented by the object k. And I think McLean uses this notation,
so that's why it's here secretly. So, I'm going to help you answer your own question. So,
here we're considering natural transformations from the represented functor to a functor that
happens to be represented. So, this is a special case in the version you said, but that's okay.
So, this is the functor f. So, the statement you just told me is that it should stand in
bijection to taking this functor and evaluating it at the representing object. So, in other words,
it should be in bijection to the set h sub j of k. And now you just have to remember what that set
was. h sub j of k is the set of, well, k by j matrices. So, there it is. Cool. Good. Someone,
one person is happy. Great. So, yeah. Question? Yep, go for it. Does Unidus
Emma always classify natural transformations or it just happened to be in this case?
Great question. So, it classifies natural transformations whose domain is a
representable functor. I'm not going to define that for you, but the domain has to be something
special. And then it actually applies in even greater generality here. The co-domain can be an
arbitrary, you know, functor that goes between the same two categories. So, but yes, the Unidus
Emma applies in absolutely any category. It's always, always there. Thank you. How special was it
that the consequence of the Unidus Emma was a k by j matrix and that happened to be that the k by
j matrix was an element in this category or an object in this category? So, the k by j matrix
is an arrow in the category, not an object. And that's true in general. So, if you have an arbitrary
category and you're wondering what, and you have, so we have an arbitrary category C,
I'm going to pick two objects of that category. I'm going to call them k and j just so we can
look at this slide, but they can represent whatever. So, we have an arbitrary category,
we have a pair of objects of that category k and j. So, it will always be the case that natural
transformations from the covariant functor represented by k to the covariant functor
represented by j stand in bijection with the set of morphisms in the category from j to k.
So, that's what's true in general. And in this matrix context, the morphisms are just k by j
matrices. Okay, thank you. Yeah. I have a question. Okay. So, I read, I think maybe in one of the
blog posts by Ty Bradley, that the United Lemma is basically the notion that if you want to understand
something, some of some system, you have to understand how it is associated period or viewed
from the perspective of all the other systems. Yes. Right. I mean, that's, I think what she said.
Yeah. So, from that point of view, how is that manifest in this example?
Great. So, that's a very useful perspective on the United Lemma. And it is not at all the
perspective that is in play here. So, there's a reason for that, essentially, which is that the
category of matrices is skeletal, and that sort of none of the objects are, each object is distinct.
You know, no objects are isomorphic if they aren't literally equal. So, that perspective is really
not so useful in the category of matrices for that reason. Why don't I push the rest of this
question to the end? I can give you an illustration of the United Lemma in a different category that
would highlight that point of view. So, I 100% agree with Titanay, but that's only one role
played by the United Lemma, and the role played by the United Lemma here is a different one that
I'm emphasizing. Oh, thanks. Can I ask a question real quick? Yes. Thank you. This might be a bad
question, but I was just wondering if any of this carried through if you could somehow look at the
category of matrices over vector spaces rather than over natural numbers? Yes, absolutely. For the
same reason, let's ask me that again at the end. That's an excellent question. I would love to discuss
it, but let me hold up a second. Sounds good. Yeah, great. Okay. But ask that again at the end.
I'm putting you off, but I want you to ask your question again. Okay. Anna, do you want to ask
something too? Oh, so you were going to go on with the talk first? Well, if you ask your question,
and then I may or may not answer it. Sure, I was going to ask whether you could put this a bit in
the context of representation theory, because it seems very much like how we learned linear algebra
in our first undergrad year, like how to represent linear transformations, and then also how exactly
it's a generalization of Cayley's theorem with permutation groups and so on. Yes, save it for
the end. I mean, you're right. There are very interesting things to say there, but let me not
say that right now. Okay, great. So let me, for those of you who wanted to break before the next
talk, let me just real quick finish up. So we had seen some examples of nationally defined column
operations, these permutation operations that swap columns, these multiplication operations that
replace a column by a scale or multiple of it, and these addition operations that add columns,
and those are familiar operations. And I just wanted to sort of connect to something that's
in the matrix literature. So there's a consequence of the innate alumma. So the innate alumma is
itself has some naturality in its statement, which I didn't mention. But the consequence is this,
that you can ask whether a nationally defined column operation is invertible, meaning if you
start with K columns, you start with a matrix with K columns, you do some operation and you get
another matrix with K columns, can you somehow invert that operation and get back to the matrix
that you started? That's what I mean by invertible. And it turns out the answer is yes, the operation
is invertible if and only if the matrix that represents it is invertible in the sense of a
for matrix multiplication. So it would be a K by K matrix that has another K by K matrix inverse,
so that if you multiply them together in either order, you get the identity. So this is a consequence
of the innate alumma. In this case tells us that my matrix operation, my column operation is invertible
if and only if it's representing matrix is invertible. And so in particular, your column
operations can never be invertible, I guess in the two-sided sense, unless K and the dimension
that you start with and the dimension that you end with are the same, because there are no
invertible one by two matrices, for instance. Okay, so the elementary column operations that I
just mentioned, swapping columns, multiplying a column by a scalar, adding a scalar multiple of
one column to another column, these are all invertible, since the corresponding elementary
matrices are invertible. So you could check the three matrices that I displayed here are all
invertible matrices. If you remember how to compute inverses, you could sort of work out
what their inverses are, and so therefore those operations are all invertible. That's one thing
that the innate alumma tells you. And another thing that the innate alumma tells you is how you
could compose these column operations. So we mentioned that natural transformations are the
morphisms in some category, and as such they can always be composed. But we've seen that these
natural transformations, these column operations, are classified by matrices, these representing
matrices. And you can ask, what is the matrix that classifies the composite of two column
operations, first to alpha, then to beta? Well, that classifying matrix is just given by right
multiplication, sorry, it's just given by the product of the representing matrices. So the
composite column operation is then right multiplication by that matrix, which is the product.
So here this alpha, k, i, k is the matrix that classifies the first column operation, this beta,
j, i, j is the matrix that classifies the second. If I multiply those matrices, what I'm going to get
is the matrix that classifies the composite column operation. That's what this slide is.
And yes, absolutely everything I say is general. So these are all innate effects. I can say them
again at the end of the talk in general category theoretic language, if you like. But the point is,
in this way, you might know that the elementary column operations, so the three
operations we just discussed, permuting rows, scalar multiplying, or adding a scalar multiple
of one row to, or sorry, one column to another column operations, these generate all invertible
column operations. So all of the, there was this question before about, right, yeah, so it's a fact,
I guess, this is something you learn, maybe if you're learning that Gaussian elimination
procedure, it's a fact that all of these column operations can be built from these elementary
ones. And if I want to classify them by a single matrix, I can get that matrix by just multiplying
these matrices in general. Okay, so the last thing I wanted to leave you with was a non-example. So we
can, you know, not all things that you might imagine are a column operation are one of these
naturally defined column operations. So an example is that we could, we talked previously about a
column operation that is appending a column of zeros, that is a natural column operation. But
if you append a column of ones, it is not. So how do we see that? Well, by the innate
lemma, if it were a natural column operation, it would be classified by the matrix that you get
by starting with the identity. So the identity matrix is this bit on the left, and then I'm
appending a column of ones. But then if I right multiply by this matrix, it's going to give you
some column operation because right multiplication always gives a naturally defined column operation,
but it's different than the one that we intended. If we right multiply by this, what it gives is
it's the k by k plus one matrix that puts the matrix on the left and then adds all the columns
and puts that in the right. And that's different from just putting ones on the right, which was
our intention. So this is an example of a column operation that violates the naturality condition
that we discussed previously. Okay, so the very last thing I wanted to explain was the dedication.
So I first learned about this example that you can use the category of matrices and these elementary
matrices to give an explanation of the innate lemma. And again, I think it's a very illustrative
explanation of the innate lemma. I learned this in late 2014. I was preparing to teach an undergraduate
category theory course. This is, I wrote lecture notes that became this category theory textbook,
which you can find on the web category theory in context. And I spent a lot of time thinking of
all the examples that illustrate categorical concepts that I was familiar with. But, you know,
I knew that if I asked the experts in the field, they would come up with even better examples
than I was familiar with. And so I sent an email to the categories mailing lists asking for help
coming up with the best illustrative examples. And this is only part of that email. I sent a
pretty long email. But I received this really, really lovely response from Fred Linton that
firstly encouraged me to get in touch with David Spivak. So that was a, that was a nice suggestion.
And then told me this entire story. So he's, he's really the one who should get credit for
everything that you've seen here today. And I'm very appreciative and thank you also for your time and
attention.
