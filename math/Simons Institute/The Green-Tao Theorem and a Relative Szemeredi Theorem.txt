So I want to thank the organizers for
organizing this wonderful conference and inviting me to speak.
So this is based on joint work with David Cullman and Jacob Fox.
So I've given this talk a few times before,
but it's the first time with both Ben Green and Terry Tell sitting in the audience.
So.
Okay. So about 10 years ago,
they proved there's spectacular theorem that
the prime numbers contain arbitrarily long arithmetic progressions.
So this is one of my favorite theorems in mathematics.
I learned about it when it was first announced.
I was still a high school student back then and at the time,
I felt this was such a beautiful statement,
but of course the mathematics involved was far beyond my understanding at
the time as a high school student.
Now that I know a bit more math,
I want to use this talk to explain to you some of
the ideas involved in the proof of the Green-Tell theorem,
and also discuss our work which simplifies a key component in the proof.
So I first need to tell you about another result which is
Samaritanism theorem proven in the 70s,
that every subset of the positive integers with
positive density contains arbitrarily long arithmetic progressions.
So this is a very deep result.
There are now several modern approaches,
but none of them are easy and it is used as
an important key input as a black box input to the proof of the Green-Tell theorem.
Now, the Green-Tell theorem does not follow
trivially as a consequence of Samaritanism
because we know that the prime numbers by
the prime number theorem have density decaying as 1 over log n.
So the density of the prime numbers is 0.
So as an aside,
so it's believed by people of this community that
in their famous conjecture of Erdos,
the density of the prime numbers alone
should guarantee the existence of long arithmetic progressions,
but this conjecture is still open even for three terms.
That is not the approach taken by Green-Tell.
So instead, they use what's known as
a transference principle strategy and it goes roughly as follows.
We start with a set of prime numbers,
denoted P and they embed P in a slightly larger set, Q.
For the purpose of this talk,
you can think of them as almost primes or numbers with few prime devices.
That's a bit of a lie,
but it's a good image to keep in mind for the purpose of this talk.
And P sits inside Q as a set of positive relative density.
The proof in the Green-Tell paper has two parts.
In the first step, they prove what's known as a relative similarity theorem.
Informally speaking, this says that if you start with a set S,
so S is Q here, and if S satisfies certain pseudo randomness conditions,
then it has the property that every subset of S with positive density
should contain long arithmetic progressions.
In the second step, they construct such a superset of the primes and
verify that the superset satisfies these pseudo randomness conditions.
And for the second part, they credit some of the number theoretic inputs
to Goldstein and Udyram who are working on the problem of prime gaps at the time.
So for this talk, we'll just look at this relative similarity theorem.
So in this informal statement, what does pseudo randomness condition mean?
So Green-Tell required two pseudo randomness conditions,
which they called linear forms condition and correlation condition.
I'll describe in much more detail what the linear forms condition is.
But the correlation condition is a slightly more technical condition
that feels somewhat less natural for the purpose of this problem.
And it almost feels like it shouldn't belong here.
Now, this is an interesting problem in its own right.
It's interesting result in its own right.
And a natural question that one could ask is,
does it hold under weaker and more natural hypotheses?
So this relative similarity theorem,
what pseudo randomness condition do you really need to have the property
that every dense set contains long arithmetic progressions?
Our main result is that, in fact, we can use a weaker version
of Green-Tell's linear forms condition and
completely get rid of the correlation condition.
So this is both a simplification and also a strengthening of
the relative similarity theorem which plays a main role in the proof of the Green-Tell theorem.
The recap, so we saw a similarity theorem where you start with a set of positive integers.
And the conclusion is that dense sets contain long arithmetic progressions.
And relative similarity theorem, the host set,
we replace the integers by some sparse subset of the integers.
So this is the sparse setting.
To put our work in some context,
you can ask, what sort of host set would you consider?
Well, natural candidate is, well, let's take a random set as a host set.
So numbers from one to n, I pick each number to be in my set with some probability.
This probability decays with n, so that it's a sparse set.
You can ask, well, does that have the property that relatively dense subsets contain long APs?
So this problem was investigated initially by Koheakawa, Wichak, and Rodo,
and they solved this problem in the case of three-term arithmetic progressions.
And they identified the correct threshold with the correct exponent for
which this phenomenon occurs.
And this problem was solved in general independently by Colin Gowers and
also Schacht, where they solved this problem completely for all k-term APs.
For us, we'll focus on the pseudo-random case.
So we'll prescribe some pseudo-randomness conditions to the host set.
And as I mentioned, so in the Green-Tau paper,
they required two different conditions called linear forms in correlation.
And we'll show that a weaker form of what they call linear forms is
sufficient to guarantee the relative semi-ready theorem.
Yep?
If you pick a random set of the density, does it have the real form condition?
It does, but with the worst exponent, yeah.
Okay, so the first half of the talk,
I will describe to you what is this linear forms condition.
But to do this, I want to first recall a proof of Roth's theorem.
So Roth's theorem is the three-term case of Szemeredi's theorem.
It says that, okay, so this is what it says, this is a three-term case.
It was proved by Roth's using Fourier analytic methods, but
I want to show you a proof found in the 70s that goes through graph theoretic methods.
And the reason for showing this proof is that there's a graph theoretic
construction that will be very useful for
motivating the three linear forms condition.
So, okay, so the Roth's theorem is about subsets of one to n, but
it will be easier and equivalent to work instead in a cyclic group, Z mod n.
So these two objects, the systems are very similar.
One difference is that a three AP, so three-term arithmetic progression,
might wrap around in a Z mod n.
Whereas I don't want that to happen in one to n.
And there's an easy way to get rid of this issue,
which is that you embed your one to n in a somewhat larger cyclic group,
so that these wraparounds don't occur.
So it will be equivalent to working in a setting of a cyclic group,
and we'll do that from now on.
Okay, so here's a proof of Roth's theorem.
So I construct for you, based on the set A, a tripartite graph GA.
It has three vertex sets, X, Y, and Z, all of them having n vertices
labeled by elements of Z mod n.
I put in an edge between X and Y, if and
only if this expression 2X plus Y is in my set A.
So that's just the rule for putting in the edges.
And you can think of this as a modified KD graph.
I put in an edge between X and Z if X minus Z is in my set A.
And between Y and Z if minus Y minus 2Z is in my set A.
And so these are the rules for putting in the edges.
Now, note that a triangle in this graph corresponds to
these three linear forms, O, B, and the set A.
And the reason for choosing these particular linear forms is that
they form a three-term arithmetic progression, with common difference minus X,
minus Y, minus Z.
So this says that triangles in this graph correspond to
three-term arithmetic progressions in the set A.
But we started out with a set A, which is free of three APs.
So does that mean that the graph has no triangles?
Almost, not quite.
There's still some triangles.
Namely, the triangles corresponding to trivial three-term arithmetic
progressions, those with common difference zero.
So you still see some triangles, but very few.
And in particular, every edge of the graph is contained in exactly one triangle.
Namely, the one which completes the equation X plus Y plus Z equals to zero.
So this is a construction for this graph GA based on the set A, which is three AP free.
Okay, to recap, starting from A, we constructed this graph with three N vertices,
three N A edges from between X and Y, each vertex has A edges coming out of it.
And every edge contained in exactly one triangle.
So there's a classic result of Roussia and Szemeredi that says that in a graph
where every edge is contained in exactly one triangle,
then the graph must have a sub quadratic number of edges.
So this is, it's a tricky result.
So it follows from, it's a consequence of what's known as a triangle removal lemma.
Using this lemma, we find that the number of edges must be little o of n squared.
So the number of elements in A must be little o of n.
And this proves Roth's theorem.
Now, of course, it's a bit of a cheat because most of the meat of the proof is in this result.
But I'm not going to show you this result.
And also, as I mentioned, the point of showing you this proof is that I want to use this graph
construction on the previous slide.
Okay, so now I'm ready to describe what is this relative Roth's theorem.
So the sparse version of Roth's theorem that we have.
It says that if S satisfies, and I'll tell you exactly what are the pseudo
randomness conditions that we need, then it has this relative Roth's property
that a dense subset of S, well, so it has a product.
So I'm using the counter positive.
It says that if it's 3Ap3, then it has little o of S elements.
And so what is the pseudo randomness condition?
Well, let's revisit this graph.
But now construct the graph not on A, but based on S.
So somehow we know that this graph encodes the three APs.
But I, okay, so again, the vertex sets are Z mod n.
And I put in the edges based on whether these expressions are in my set S.
So this is a graph GS.
And the pseudo randomness condition for my set S is the following.
It says that this graph GS has asymptotically the expected number of
embeddings of this object, K222.
Meaning that if the density of S is P,
then the density of embeddings of this guy should be P raised to the 12th.
Up to 1 plus little or 1, an error.
Yep?
Don't you want S also to be relatively large or
it can be as any size you want?
S is sparse.
So the density of S goes down with n.
Yeah, so that's the whole point.
Because if S is already dense, then it falls from raw steel.
So it can be as sparse as well?
Not quite.
I mean, there are limitations to how far you can push with this condition.
But for, yeah, so.
So, but we need a little bit more.
Just because of technical reasons, we need a little bit more.
We need that not only it has the correct,
the expected number of embeddings of K222, but also the subgraphs of K222.
So this guy has the number of embeddings P raised to however many edges I've grown here.
And that's the pseudo randomness condition that I need for
guaranteeing a relative roster, and that's the version that we have.
Any questions?
So you've stated the theorem, and it seems as if,
do you mean what you said, or is there some extra condition that you have?
This, I'll state some more precise version later on.
But this is a version that's.
That's, we can understand that.
Yeah, yeah, okay, all right.
Let me try to make some arguments that this is a natural condition.
It's a natural pseudo randomness condition.
And to do this, let me draw some analogies with this classic result of
Chung, Graham, and Wilson, which says that in graphs of constant edge density.
So they stated several quasi-randomness conditions which all turn out to be equivalent.
So these are things like subgraph counts or eigenvalue conditions or
co-degree conditions, discrepancy conditions.
But in particular, those one condition that stands out, which is that having the
correct C4 count is sufficient to guarantee several other quasi-randomness conditions.
So this is a classic result of theirs.
However, these equivalences fail in the sparse case.
They break down in the sparse case if the density of edge density goes to zero with m.
One way to interpret our results informally is that if we want some sort of
extreme or Ramsey type results about some graph H, so in rostrum H is a triangle.
Then we can get these results provided that there is some host graph,
which is pseudo random with respect to counts of the two blow-up of H.
So two blow-up, meaning I start with my graph and then I double every vertex.
And if you look at Chung, Graham, Wilson, the four cycles are two blow-up of an edge.
So this is just drawing some analogies to try to convince you that this is a natural condition to consider.
So let me outline the proof of this relative rostrum.
So this is basically the version that we saw earlier.
If you have density at least delta, then the set A necessarily contains a three-term arithmetic progression.
There's a standard averaging argument credited to Bernoulli's saying that starting with this result,
we can obtain a counting version of rostrum where we're guaranteed not just one three AP,
but many three APs, a quadratic number of three APs.
And this is essentially a sampling on averaging type argument where you look at a progression of length and not,
and then you find a three AP and then you do it again and so on.
So you get many three APs.
So let me show you how to prove rostrum.
So this is known as a transference principle and it's featured in Greentau's work.
Starting with a set subset A of S.
We're going to find a dense model, A tilde.
So my set A originally is sparse, but there's some sense in which I can find a set A tilde,
which is both a dense set and also in some ways is a good approximation for A.
And this will be in terms of then something that I'll call cut norm, which I'll elaborate on later on.
And they also have similar densities.
And furthermore, a counting lemma will tell us that the number of three-term arithmetic progressions in my original set A,
appropriately normalized, will be close to the number of three-term arithmetic progressions in this dense model.
Now, by rostrum, which we'll use as a black box, the counting version of rostrum,
we know that there are lots of three-term arithmetic progressions in A tilde because it's a dense set.
Therefore, my original A must also have a lot of three-term arithmetic progressions.
That's the proof of relative rostrum.
Of course, I have to explain to you what are these two steps, these two ingredients.
But this is the general outline.
It'll be helpful to, and by the way, let me say a word about, so I've so far been mostly just talking about the three-term case, rostrum.
So for a similarity term, for a k-term, it turns out that if you want to prove a similarity term,
it's much easier to prove the three-term case than to prove the four-term or longer case.
As it turns out, for doing transference or transferring this result down to the sparse setting,
at least the way that we do it, there's no additional difficulty involved besides notational difficulties.
So I'm going to show you how to do this for the three-term case,
but if you expand out a notation and make some minor modifications,
then you can also use the same technique to prove relative similarity there.
But I'll just explain what happens through the three-term case.
So now I want to convert language somewhat.
So instead of talking about sets, I want to go from sets to functions.
So to have a weighted version of these results.
So we saw this before.
So this is a counting version of rostrum.
So if you have a dense subset of Z mod n, then it contains a quadratic number of three APs.
Using the counting version of rostrum, we can deduce a weighted version
where instead of considering a subset A of Z mod m,
we now consider a function from Z mod n to the zero-one interval.
If you're limited to zero and one, then these two are equivalent.
But now I'm relaxing f to take values between zero and one.
And you can deduce this from this.
The first result, up to a change of parameters.
So these two are equivalent up to a change of parameters.
So we'll use the weighted version of rostrum.
So this is the dense setting.
This is a sparse setting correspond to.
So I start with some sparse host set S.
And more generally, I'll use some weighted form,
which will come in the form of a normalized measure, nu.
And nu will be normalized so that it has expected value one.
So for example, if I start with some concrete set S,
then I want to take nu to be the indicator function appropriately normalized
so that it has expectation one.
So the subset of S then would correspond to a function f
that is majorized by nu in the sense that it's less than or equal to nu point-wise.
And this condition of A having relative density at least delta in S
then corresponds to the expectation of f being at least delta.
So this is just a table of translations, what I said earlier.
So we started out originally in the world of sets,
but now we're going to move to the weighted world where I now talk about functions.
And here's the dense setting, and here's the sparse setting.
And if you set nu, the majorizing measure, to be identically one,
then that's precisely the dense setting.
So then the relative rostrum, which I stated somewhat informally in the previous slide,
now I stated somewhat more formally,
and I'll explain what this three linear forms condition means in a second,
it says that if you start with a nu which satisfies this three linear forms condition,
then every function f majorized by nu and having expectation at least delta
has this lower bound on the weighted number of three APs.
And the C can be taken basically to be the same C up there,
because the result transfers over through this transference argument.
Everything, of course, provided that n is sufficiently large.
So let me say what this three linear forms condition is,
and this is basically this thing I mentioned earlier about K222s.
Now let me do this precisely.
So I look at this graph, so this is essentially the same graph
they were at construction I made earlier,
except that now I used to put the weights on the edges
based on the function majorizing measure nu that I started with.
And I want to say that the density of this K222 in this weighted graph
is somehow pseudo-random, meaning one plus little or one.
So writing it out, so if you look at this graph,
and I want to count the occurrences of this,
so I just write out the expressions,
it means precisely that this expression is one plus little or one.
And the same is true, so that's an additional assumption,
the same is true if any subset of these 12 factors were deleted.
So this is the precise statement of our result.
Any questions?
So the error term in that one plus little or one
would have to be an hour of exponentials in...
No, it can be anything that the case with n.
So hitting it here, I'm assuming that there's some...
So you take maybe a sequence of these things depending on n,
and I want to say that this is something which the case to 0 with n.
Sir, I mean, when you want to then derive the relative stability theorem,
your assumption of the strength of the three-year foam condition
will have to be tower exponentially strong in terms of density.
No, I don't... It depends on how you interpret it.
But at least the way I phrased it here,
there's no requirement on how fast is the case.
Right, but this should be a planetary version,
where it would be only relative rough or fixed delta.
Sure.
Or won't you just...
It doesn't need to be...
I would show this one because we can...
But so then you're asking how large is n
need to be sufficiently large?
How small is it all?
One has to be just...
Okay.
Yeah, sure, yeah.
Okay, so...
Probably a short tower.
Yeah.
No, but actually the only place where you have major losses in the original...
So however, whatever bound you get for the original Roth's term or Szemeredi's term,
that's the major source of loss.
And in the transference, we only occurred a polynomial amount of loss
because of the arguments used.
So are you using Roth's theorem as a black box?
We'll use Roth's term as a black box.
Through the Szemeredi rule.
Well, you said...
I'll explain this slide as well.
What I would present in this talk will use it as a black box.
Any other questions?
Okay.
So that was for the three-term case, the Roth's term.
For Roth's Szemeredi's term, it's a similar K-linear-forms condition.
And the K-linear-forms condition then says that,
well, instead of taking a tripartite graph,
now I take a hypergraph.
So for K equals to four, I'm looking at a four-partite,
three-uniform hypergraph.
So essentially, things correspond to a simplex.
And I put in the weights in a similar manner to how I put on the weights for graphs.
So these expressions are chosen so that they form a four-term AP.
And each term depends on only three variables.
So I construct some weighted hypergraph.
And I want the four-linear-forms condition to say that there's something about
the density of the two blow-ups of the simplex being correct.
So it's a natural generalization of what I told you earlier,
except now you have to work with hypergraphs.
The notationally, it gets a bit more complex,
but mathematically, there's not more difficulty.
Okay.
So to answer your questions earlier,
so there's a couple of papers that we wrote.
So the initial, where we first found this result that we wrote up in this paper
that we titled a Roth's Szemeredi's term,
where we proved this result through transferring the hypergraph removal lemma.
Okay.
So this is very expensive, but it is a more general result.
So transfer in the hypergraph,
so transfer means that we assume the hypergraph removal lemma as a black box,
and then we transfer it down to a sparse relative setting.
So this gives a relative sparse hypergraph removal lemma.
Subsequently, I found that you can actually transfer Szemeredi's term.
So a modification of the argument allows you to transfer Szemeredi's term
instead of then setting to the sparse setting.
So there, the quantitative bounds on Szemeredi's term
can be just brought down to the sparse setting.
So what you say in the case of three predictions that it's easier to think about
is that you can say that in a kind of graphs that come up in better reduction,
just by the analytical tool for Roth's theorem,
those graphs satisfy a random removal lemma with a really good bound,
but just with a single exponential.
So can you say your question again?
Yeah.
So it seems that the argument that you were sketching was to say,
well, let's look at the rusha sketch.
So that was actually, that was for motivation.
So the reason why I sketched the rusha Szemeredi
was to motivate the graphed construction.
Right.
So you just do the same argument,
but with that full tripartite graph,
and then because they're trying to remove a lemma,
it's true for the dense graph.
So that's this approach.
Right.
That's this approach.
You just need, okay, maybe we can talk off the line.
So in comparison, this approach gives a more general result about
hypergraph removal lemma in the sparse setting.
And this result, this approach is more direct.
Now, they're not independent.
So I like this paper a lot because it's really short.
But it does quote some, well, a key fact from,
a key step from this result.
So, yeah.
So it's a quoting lemma, which I'll explain.
And that's like the main new advance that we have.
And this quotes the counting lemma.
So somehow this doesn't actually contain all that much new
mathematical content.
But it's an outline of how to put things together to give you
a very short and direct way of getting the relative semi-ready
theorem.
Okay.
And.
You've approved that counselor without referring to graphs.
Sure.
Yeah.
This set contains six pages.
It's not self-contained at all.
So it's very much not self-contained.
But I'll explain.
So the approach I'm taking in this talk follows this outline.
Okay.
So I gave a transfer and slide early on with SESS.
Now I'm just giving the same slides with functions.
So I start with f majorized by nu.
So f is some sparse thing.
And sparse means that it's unbounded because of the normalization.
And I, and there's some dense model theorem, which guarantees
that you can approximate f in a certain cut norm, which I'll
explain in a second, so that this dense model, f tilde,
is a dense object.
So it's bounded.
They have the same expectation.
And a counting lemma then would guarantee that these two
expressions, which are the weighted three AP counts,
they are similar.
So, and by the weighted version of Roth's theorem,
we know that this is lower bounded.
And therefore, we have relative Roth's theorem.
Okay.
So now I need to explain these two steps.
What is dense model theorem and what is counting lemma?
So let's start with dense model theorem.
So in what sense, okay.
So, you know, I said dense models.
In what sense does this dense model, f tilde, approximate
the sparse object, f less than nu?
In the previous approach taken by Green and Tell,
they use something which is based on the Gauss uniformity norm.
And the key difference for us is that we don't use that.
We do not use the Gauss uniformity norm.
Instead, we'll use the cut norm, which is also known as discrepancy.
And yeah, so I'll explain with that in the next slide.
The comparison, so what is the pro and con?
With the cut norm, we get a cheaper dense model theorem.
So it's the same dense model theorem,
instead of applied to a different norm.
And because of that, we require fewer pseudo-randomness
hypotheses.
The trade-off, what we have to pay is that there is now
a trickier counting lemma.
And so that's really the new thing that we have
that allows the simplification.
Okay.
So the cut norm.
So if I start with two weighted bipartite graphs,
G and G total between X and Y.
So the cut norm, which is featured in Frieze-Cannon's
weak regularity lemma, says that I say that these two weighted
graphs are closing cut norm if for every subset A of X
and every subset B of Y, let's say if you sum the edge weights
between A and B, then the sum of edge weights for G
is close to sum of edge weights for G total.
And this is true for every A and B.
And so written out, it looks like this.
That's what it means for two weighted graphs to be
closing cut norm.
For two functions on Z mod n to be close in cut norm
means precisely that I look at that graph that we saw many
times that was constructed for the purpose of illustrating
that that graph constructed based on these functions
on Z mod n, they are close in cut norm.
So that's what I mean by two functions on Z mod n
to be close in cut norm.
So precisely this expression here, because you remember
2X plus Y is what arose in the graph through
the construction.
This 2 is not important.
I can erase both 2s because I can just make a change
of variables.
This is the graph that came up in our earlier construction.
And this notion is weaker than f and f total being close
in the Gaussian informity norm.
So then the dense model theorem says the following.
It says that if you start with a majorizing major nu, which
is close to 1 in cut norm.
So this is a statement about pseudo randomness.
And this is implied by the linear forms condition that we
had earlier.
Then the following is true that if f is majorized by nu,
then there's always a dense model, f total, which approximates
f in cut norm.
So this is a dense model theorem.
So it seems a little strange if you haven't seen it before.
Like how do you approximate a sparse object by a dense object?
Let me just mention the history on this result.
So this was used in the original Green-Tau paper.
And they used some sort of regularity type energy increment
argument.
And this result was clarified in a subsequent work by Tau
and Ziegler, where they showed that the primes contained
arbitrarily long polynomial progressions.
And afterwards, there was a simplification of this theorem,
so a simplified proof, given independently by Gauss
and also Rengel, Traverson, Toussaint and Madonna, where
the proof uses Gauss phrasing in terms of a separating
hyperplane theorem, whereas Rengel, Traverson, Toussaint
and Madonna used, actually, the Minimax theorem.
And also, in both cases, they had
to use the Weierstrass polynomial approximation
theorem.
But this is a very nice and short proof.
So this is a very nice and short proof of the dense model
theorem.
Is this the same as the Han-Bannach?
Yeah, that's the Han.
Actually, so I was looking at them,
and I thought this result, Gauss only
uses the finite dimensional case of Han-Bannach, which
I think that was actually attributed to Minkowski
even before Han-Bannach, anyways.
So that's a dense model theorem.
So that was this part of the outline of the
transference result.
Now, I want to talk about what is new for us.
That's the counting lemma.
So what is our counting lemma?
So first, let me review the classical counting lemma.
So a counting lemma is a result that says that if you have
two weighted graphs, G and G total, if they're close in
cut norm, for the moment, we'll only
consider the dense setting.
So G and G total are both bounded by one.
If G and G total are close in cut norm, then they
have similar triangle counts.
So this is the classical triangle counting lemma.
Let me show you a proof of it.
It's quite short.
Recall what it means for G and G total to be close in
cut norm.
It means that this expression is small.
So if I restrict this expression to indicator functions
A and B, I can relax these indicator functions to 0, 1,
interval 0, 1-valued functions just because this is a
bilinear object.
So the extremum always occurs at 0, 1-values.
So this is an equivalent.
So these two statements are equivalent to each other.
Instead of taking indicator functions, I now take
functions taking 0, 1-interval-valued.
So let me show you that these two expressions are close
to each other.
I start with the expression for G, which will be highlighted
in blue.
So this is the triangle counting G.
I claim that I can swap out a factor of G by G total.
And the reason why this step is true is that if I look at the
difference between the two sides and I fix a value of Z,
so just imagine I fix the value of Z, then the difference
between these two sides has precisely this form.
So for every fixed value of Z, the difference between the two
sides is at most epsilon in absolute value.
And if you allow Z to vary, then they're still bounded by at
most epsilon in difference.
That's just one step.
I swap out a single factor by G total.
Now I swap out the second factor.
The same argument applies.
And I swap out the third factor.
And I see that, well, I get what I want.
So the difference between these two sides is at most
3 epsilon.
OK.
So that's how you prove the dense version of the triangle
counting number.
It's fairly straightforward and fairly quick.
This proof breaks down almost immediately in the sparse setting.
Remember, the sparse setting refers to the case when G is
unbounded.
And when G is unbounded, the first thing I said about these
two being close and cut-norm, because you can look at that,
it's no longer true, because, well, I can only use A being
bounded and G is unbounded.
I can't use the cut-norm anymore.
So we need something else.
And we need some additional conditions in the sparse setting.
And the sparse triangle counting number that we give says the
following.
If you assume that one of the G's, so the sparse G, is
majorized by a new that satisfies the three linear forms
conditions, or this condition about pseudo-randomness with
respect to k222s, then G and G total and G total is still a
dense object.
Then G and G total being close and cut-norm implies that they
have similar triangle counts.
So this is a new sparse triangle counting number.
So the proof has a few ingredients.
There are many, many applications of the Cauchy-Schwarz inequality.
This is a standard technique in this area as popularized,
for example, by Gowers.
I won't say too much about that, but I won't bore you with too
much of the details of repeatedly applying Cauchy-Schwarz.
So that's one of the steps.
The new idea for us is something we call densification, which
I'll explain on the next slide.
And we also need the proof in the dense case, which I explained
on the previous slide.
So what is this new densification technique?
It's actually quite simple.
So you start out wanting to count triangles.
You apply Cauchy-Schwarz in some manner a few times.
Cauchy-Schwarz involves some squaring.
And squaring gets you double the vertices.
And by the way, so the loss, so why
we need a three linear forms condition mostly
comes from this step, repeated applications of Cauchy-Schwarz.
That's the most expensive step in terms
of pseudo-randomness conditions.
But after applying Cauchy-Schwarz, let's say,
a few times in a few different places,
one place you may end up is this object, wanting to count
this four cycle.
So how do you count the four cycle?
Here's what we do.
Let's construct this auxiliary graph, G prime.
G prime is just to set the cold degrees.
So I put down a complete graph between x and y,
a new graph, where I put on the edges the cold degrees.
The idea is that even though I start out
with something which may be sparsely supported,
but it's bounded by a pseudo-random graph,
the cold degrees are going to be much smoother.
They're not going to behave nearly as badly
as the original graph.
And you can view these cold degrees
almost as if you were a bounded graph.
So it behaves like a bounded object.
It behaves like a dense object.
Now, looking at this expression here for the 4AP count,
if I integrate out the Z primes,
then this is just a triangle count in this new auxiliary
graph, where I keep the original edge weights,
but I add in the new cold degree weights as the edge weights
for the new graph between x and y.
So now I'm back at evaluating some triangle counts.
At first, it seems like we went around in circles,
but I want to do triangle counting.
So doing a bunch of Cauchy shorts, I get the count four cycles,
and this reduces the counting triangles.
So it seems like we went around in a circle,
but we gained something really important,
which is that one of the edges now behaves like a dense edge,
a dense bipartite graph.
Now, do this two more times.
Do this two more times, and you convert the problem
from the sparse setting to the dense setting,
which we already know how to handle.
And that completes the proof, well,
the sketch of the proof of the triangle counting lemma.
And then putting everything together,
so I showed you what this dense model theorem is about.
I didn't show you the proof, but I showed you what it's about.
And then we have this counting lemma
putting everything together.
We get this relative Ross theorem.
So since the original green towel work,
there's been several simplifications.
So towel simplified some of the number theoretic estimates
that were involved in constructing the pseudo random host.
And the dense model theorem was,
the proof was also simplified, as I mentioned early on.
And now we're putting some new insight into the counting lemma.
So there's been several simplifications
since the original green towel paper.
I think it would be good to collect them into one place.
So we are currently finishing up writing an exposition
of the updated exposition of the green towel theorem.
And the idea that this will be a gentle exposition
giving a complete and self-contained proof
of the green towel theorem that could be used
for educational purposes.
So it's self-contained with one caveat.
So we're going to assume the similarity theorem
as a black box.
So we will not be able to contain some ready term
in a very short exposition.
It will contain all the number theoretic estimates.
It will also contain a number theoretic estimates.
That's right.
Assuming rudimentary knowledge.
OK.
So coming soon, hopefully.
So this is my last slide, again with a statement of result.
That's all I want to say.
Thank you.
So you have to check these linear form conditions.
Could you say that's scary if you haven't looked at it?
How scary is it?
So it depends on what you want to do
is to prove the green towel theorem.
So you have to first think about how do you construct
this pseudorandom superset or the pseudorandom measure.
OK.
So there's some number theoretic work that's involved.
It's not too bad.
So Terry has simplified some of the sense of the original
green towel paper.
It's not too bad.
It's not too bad.
Like, it's clever.
It's not too bad.
You don't need the parallel theorem anymore.
Yeah.
Oh, here you go.
Delicious.
Lots of fries.
So would you say it's significantly easier to verify
your version of the linear form condition
than it is to verify that version of the linear form condition?
I mean, there's less things to verify.
I don't know whether, in retrospect, things are easy
once you figure them out.
But there's fewer things to verify.
Is it possible that the linear form condition alone implies
that that's more the theorem with respect to the double
norm, or is it a non-conflict example?
That I'm less familiar with.
So, yeah.
I mean, that's more, I mean, you can look.
So Gellwitz has a paper explaining the paper
where he gave the shorter proof of the dense model of them.
He went into some of the aspects of, yeah, I'm not too sure.
So you motivated the appearance of the tube level
of the paper very nicely by saying,
I want to be able to come in and actually be able to come
into the model of the entry.
But it turns out that in a footnote, in your paper,
you actually want the tube level of the triangle
not exactly to be enough to deliver something slightly weaker.
Sure.
So the question is, so.
The tube level of the triangle would have been a very natural thing,
but it's actually not exactly what you would do.
Do you have an explanation for that?
No, actually, so for triangles, you do need a tube blow-up.
It's that for other graphs, if you want to do removal of them
or for other graphs, you can do away with something slightly.
For triangles, you still need a tube blow-up.
For another graph, you can use what we call a weak tube blow-up,
where you save a few of the other edges.
I mean, it's a legitimate question.
The amount of pseudo-randomness condition,
you need more than just knowing the triangle counts.
Just knowing the triangle counts won't get you anything.
You need more than that, and we know that tube blow-up is enough.
So the truth.
Do you think this is the right condition?
It feels like it, but I'm not sure.
I mean, the truth could be somewhere in between,
but it certainly feels like a natural condition.
Yeah.
So is there a chance that this transference principle
or even more generalized transference principle
would enable to boost up the bounds in the original semary
or semary theorem, where the idea would be
to start with some not-so-dense object
and to put it pseudo-randomly in a slightly more dense
and then in a slightly more dense and so forth?
Yeah, that would be very nice, wouldn't it?
I mean, I think it's an interesting direction to consider,
but we had some examples which were not,
I mean, they're not necessarily counter-examples,
but they were not, they raised some skepticism
towards that approach.
But it's, I mean, it's maybe, I think it would be,
I mean, even for the prime, you know,
constructing this majorizing measure is a non-trivial task.
And yeah, I don't know.
Yeah, so in fact, the hypergraph removal version is,
it works very well for any, for any group.
Yeah, so I forgot to mention that, you know,
in this outline, yeah, so in the original Green-Tau paper,
they did something which is similar to this,
and then subsequently, Terry Tau had a paper
that shows that the Gaussian primes contains
constellations, arbitrary constellations,
and that was more similar to this approach here.
In fact, you know, we took a lot of the inspiration
from those two works of course.
Thank you very much.
