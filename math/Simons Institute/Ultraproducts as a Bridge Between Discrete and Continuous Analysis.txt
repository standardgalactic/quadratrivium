Thank you. It's great to be here, back up here in Berkeley.
It's a nice place.
So, you know, I was workshopped as
neoclassical methods in discrete now,
so it's a little trouble.
I can't figure out exactly what that meant.
But I decided that the closest topic I could give was one of
my favorite sort of things to evangelize about,
maybe, is the connections between
discrete mathematics and continuous mathematics.
And we did that they're not as far apart as we sort of
traditionally make them to be.
You know, so most of us here are working
at discrete mathematics or maybe
my call of pioneering mathematics,
one-tiered mathematics.
Sleet objects, finite objects,
and more bound, explicit bounds.
And that's one side of analysis,
what's sometimes called hard analysis.
And then on the other side of analysis,
you have the opposite of where you work
with continuous objects or infinite objects,
infinite uncarnable spaces and things like that.
And you often only get about qualitative results.
So, you don't care about bounds,
you just care that something exists or something converges,
but you don't specify rates or anything.
And you've got sort of different people working in
one world and in the other.
But there's really a lot of connections between the two.
So, certainly there's a lot of analogies.
So, for example, in the continuous world,
you have remaining manifolds,
for example, so very continuous object.
And there are isoprometric inequalities
and Qigur inequalities and so forth.
And of course, in the discrete world,
we now have always nice angle log.
So, please, money and geometry type results
for various apps of graphs and so forth.
So, there's certainly lots of analogies
between discrete mathematics and continuous mathematics.
But the connection is stronger than just analogy.
There are really rigorous bridges that you pass
between discrete objects and continuous objects.
And in particular, allow you to use results
from continuous mathematics as a black box
to prove results in discrete mathematics.
So, there's lots of examples of this
that you probably already know of.
So, for example, in the discrete world,
you might care about finite graphs,
finite dense graphs, say.
And you want to prove various theorems
about finite dense graphs.
And nowadays, we have this great tool, graph limits,
which takes a sequence of finite dense graphs
and produces out of it a continuous object,
which is what we're going to now call the graph on, okay?
Which is basically a measurable function on the unit square.
And you can then use tools from continuous mathematics.
For example, you can hit graph on some measure theory.
Tools like the Lebesgue differentiation theorem, for instance.
And you can use that to start to prove things back
about finite dense graphs.
For example, a property testing or triangle removal lemma,
regularity lemma, these sort of things.
Another famous example, you might be interested
in dense subsets of the integers.
Okay, for example, you want to prove a Zemmery's theorem
that all subsets of the integers positive density
contain arbitrary long progressions.
And in the 70s, first one, we're going to introduce
the Gospons principle.
Which connected dense subsets of the integers
to measure preserving systems,
or positive measure subsets.
Of measure preserving systems.
So a measure preserving system is a probability space
with a shift on it, some sort of measure map
which preserves a measure.
And then once you have this set up,
you can hit this with the techniques of agotic theory.
And also spectral theory and a couple other
continuous tools, mathematics.
Like the agotic theorem, for instance.
And so he famously gave the second proof
of Zemmery's theorem using agotic theory methods.
See more examples.
You might be interested in, what are the examples?
Okay, so you might be interested in discrete metric spaces.
For example, Cayley graphs or something.
And we have this tool to convert,
if you have to take limits of discrete metric spaces
and produce continuous metric spaces out of them,
this is the tool of Gromov House of Limits.
And then you can use lots of tools
from continuous metric geometry to then,
like selfless spaces and whatever,
to analyze the underlying discrete spaces.
In particular, the continuous limits
may have various group symmetries
that only hold some approximate or asymptotic sense
in the discrete world.
Okay, you might be interested in
or normials of bounded degree.
For some discrete problem over some field.
Or maybe varieties, object varieties
of bounded complexity, almost the same thing.
Propriety, come on, come on, come on, come on.
That's weird, that's right, that's wrong, okay.
Well, bounded complexity.
So it's just a variety set up by a bounded number
of polynomials of bounded degree.
And you want to ask quantitative questions
about these things, like for example,
if I give you a bounded number of polynomials
just cutting out some set, some object variety,
it may have a number of components.
How many components do you get?
And can you bound that the number of components
in terms of the degrees of the polynomials
that you have and so forth.
And there's a way to take limits
where you just, and in the limit,
the qualitative analog is just polynomials
and varieties with no bounded degree.
Okay, so if I have time, I might talk about this sort
of limit here that the cheapest thing to do
is take an ultra product.
And the point is that you can use tools,
so over here, you can use the tools
of this classical algebraic geometry.
The algebraic geometry is, the way it's classically presented,
is qualitative.
It tells you, for example, a basic result
is that every algebraic set can be decomposed
into a finite number of irreducible object varieties.
But it doesn't give you a bound.
It just says there's a finite number.
But once you have that fact,
and if you just use the ultra products as a tool,
you can get as a very quick corollary
that automatically there is a bound,
the sort of compactness there.
This is, in fact, once you know it as a black box
that every algebraic set can be decomposed into finite,
many irreducible subvarieties,
you can conclude that, in fact,
any algebraic set of complexity, most m,
meaning it can be defined by, you know,
at most m polynomial degree, m or something,
can be decomposed into a finite number of varieties.
And the number of varieties is bound only
by constant depending on m.
And you can actually get that for free.
And so there's lots of qualitative results
by algebraic geometry,
automatically give you quantitative analogs,
basically because, in some sense,
algebraic geometry is closed under ultra products.
Okay, and then another example,
which I think I want to have time to talk about,
both, okay, recently in additive combinatorics,
we studied a lot of approximate groups.
These are sort of sets A in some real group,
actual group, which I say finite,
but such that the product set is small.
A dot A is finite by constant times A.
Okay, technically, this isn't quite
the definition of approximate group,
this is a definition of set of small doubling.
We actually usually impose a stronger condition.
We want A to be symmetric in the identity,
and we want A dot A to be covered by K transits of A.
But essentially, this is the sort of object
that we care about.
So things like progressions,
geometric or arithmetic progressions in a group,
or finite groups and things like that.
And again, we have a way of taking limits,
much like graph limits, actually,
which turn approximate groups
into neighborhoods of the identity.
In a locally compact group.
So this connection was first worked out by Khrushchevsky
using model theoretic methods,
and then later, Emmanuel Breard, Ben Green and myself,
observed that you just need octogorax again.
To convert these discrete objects,
you have to take a sequence of discrete approximate groups,
and in the limit, create a continuous object,
which would be, so the picture to keep in mind
is that you might have a discrete box,
is a good example of an approximate group.
And then you might make it denser and denser and denser.
You have a sequence of approximate groups
which get denser and denser and denser.
In the limit, you should get a continuous square,
starting from discrete boxes that get denser and denser,
and there's a general way to do this.
And the point of doing this is that,
there's the powerful tools
from the subject of topological group theory,
that allow you to study these things.
In particular, there's the solution
to Hopper's fifth problem on how to classify
lead groups topologically.
It becomes very useful and gives you non-trivial results
on approximate groups.
In fact, it even classifies them completely.
So there's lots and lots of ways to take limits
of discrete objects to create continuous ones.
And you can see that there's also different ways to do it.
But it turns out that there is a universal way
to take these limits,
and the universal way is ultra-products.
I could take all of these,
I could have just an ultra-product of all of these.
There's a single gadget, an ultra-product,
which what it does is that it takes a sequence of spaces
which in practice would be discrete spaces.
And produces for you automatically,
a canonical continuous object,
which you should think of as the limit of your good object.
And so you don't need to learn,
in fact, you need to learn all these different ways
of taking limits.
You just need to take, learn this one way.
And then you can pass back and forth
and you can continue some discrete fairly easily.
So this is one I want to describe.
Before I do that,
I just want to say a little philosophy about limits.
There are actually three notions of limits in mathematics,
and most people only learn one or two of them.
And which ones you learn depends on whether you're an analyst
in algebra or a logician.
So just a little bit of philosophy.
So what is a limit?
So the, even analyst,
the notion limit that you're most exposed to
is at a topological or metric limit.
So here, what happens is that you have some big space X,
which is either a topological space or a metric space.
And then inside the space, you have maybe a sequence.
Sometimes a net, which is a fancy version of a sequence.
But yeah, you have some sequence in your set,
and you have a notion of the sequence converging
to a limit, which in the metric cases means
that the distances all go to zero,
or the topological case means that all the neighborhoods
of X trap your sequence.
And so this is the standard notion of limit
that we're used to.
And so like when you write down say,
go house of limits, you've got a metric,
you're on a graph limits,
you usually also go some sort of metric.
First of all, correspondence principle,
you often use the topological limit using,
say, big convergence or weak start converters of measures.
So this is a limit,
which these are the limits that analysts
are most familiar with.
One thing to note though is that to take these limits,
all the elements, all the objects you want to limit of
have to lie in the same space.
So you have to build a space of all graphs,
a space of all dense subsets of integers or something,
and create a space out of those.
Sometimes that's not so natural.
Sometimes all these objects really are living
in different spaces, like a graph and n vertices
where n keeps changing.
And you could just take a big union
and make them a big space, but that's not really natural.
Okay, but anyway, these are the limits
that analysts usually learn.
If you're an algebraist,
you'll usually instead use categorical limits,
direct and inverse limits category theory.
So now your objects might be set a set of spaces,
a sequence of spaces, and you might have various maps.
So you might have a sequence of spaces
which all map into each other, like this.
And these might be groups or vector spaces
or rings or whatever.
And very often, you can take a limit
and create a big space which everybody maps to.
Or if the arrows go the other way,
you can often build a space going the other way.
And you can also have more fancy diagrams
than just these chains.
Okay, so these are direct and inverse limits.
Typical example is that if you take the cyclic group
of a prime, this projects the cyclic group of p squared,
cyclic group p cubed.
And if you just take the inverse limit
and the category of rings, you get the p-addicts.
So these are categorical limits.
Algebraists love them.
But so you can take limits of all kinds of things.
But in order for it to be a useful limit,
you need lots and lots of maps,
morphisms between objects before a useful limit emerges.
So yeah, so the morphisms.
Topological or metric limits need convergence.
Okay, so of course, the famous issue,
you can't take a limit of just any sequence.
First, it's all in the same space.
And second, it has to converge
before you can extract a limit.
So, ultra limits.
Okay, so you can,
if you use ultra filters and ultra products to create limits,
what this does for you is that
you can take now elements in any space.
You can take elements Xn,
which each of which lives in different space, Xn.
Okay, and you can form a limit.
Which won't lie in any original spaces.
It will lie in the ultra product of these spaces.
Which is a limiting space of all these spaces.
But the advantage of ultra products is that
you don't need any morphisms with the categorical limits.
And you don't need any convergence.
You can take any sequence in any space and you create a limit.
And it sounds like that's sort of too good to be true.
It's too content free to actually do anything.
But what it does very neatly is that it converts discrete to
continuous and that allows you to just take or
use all these continuous objects out of the box.
Okay, so that's sort of the philosophy.
All right, so now I have to tell you what
ultra filter and ultra products are.
Okay, and these are not scary objects.
These are voting schemes, just on an infinite set.
They're very discrete things actually,
just haven't been on the natural numbers.
Okay, so I need to define the notion of a non-principle ultra
photo.
Alpha, okay.
And the space of a non-principle ultra photo is called beta n
slash n, but never mind that.
Okay, so a non-principle ultra photo, what it is,
formally, by definition, it's just a collection of subsets.
Of the natural numbers.
And these subsets will be called alpha large subsets.
So there's an uncountable number of subsets of integers,
eveningers, oddingers, whatever, prime numbers, so forth.
So some of them we're going to call alpha large and
some of them we're not going to call alpha large.
And you want various axioms.
And there are four axioms that you want.
One of them is that no finite set is alpha large.
So, okay, only big sets get to be large.
Another one is any intersection, let's say,
of two finite, or two alpha large sets is always alpha large.
Okay, so, all right.
Then the third axiom is that any set containing,
any subset, of n containing an alpha large set is still alpha large.
Okay, so if you have an already alpha large, if you make it bigger,
it's still alpha large.
These two axioms basically tell you that this set is a filter.
Okay, and then the final axiom tells you it's an ultra-filter,
and the first axiom tells you it's non-principle.
So the final axiom is that if you take any subset of the integers,
then either E was complement, alpha large.
So either the odd numbers or
the even numbers, for example, are alpha large, one or the two.
It can't be both because they're both in the intersection would be alpha large,
but no, but only the empty set is alpha large,
but no finite set is supposed to be alpha large.
Okay, so this is a non-principle ultra-filter.
It's a mechanism for defining alpha large sets, for large sets.
If you like, an ultra-filter is a counter-example to
Aero's theorem for infinite voting population.
So once you have an ultra-filter and if you have an infinite number of voters,
each one of which is given by a natural number, index by a natural number,
then you can set up a voting system that if you have a referendum,
if you want to vote on something,
you can either vote yes or no,
and by the last axiom in particular,
either the yes voters or no voters will be alpha large,
and then whichever answer is alpha large,
that wins the election.
Okay, so that's a voting system,
and all these axioms tell you that this voting system is sort of rational.
If someone switches their vote from no to yes,
then that increases the chance that the outcome is yes,
and so on and so forth.
No finite set is alpha large means there's no dictators.
So you can generalize that given any finite set of options.
You can vote for any finite set of options,
and exactly one of the options will be an alpha large set.
So it's basically a voting system with no deficiencies.
That all the paradoxes of voting don't occur,
and it's because you have an infinite number of voters.
So these things don't exist if you replace this by a finite set,
they only exist in infinite set.
Now, at this point,
you're supposed to give an example of an ultra-photo,
non-photo ultra-photo.
But yeah, this is a slight problem is that
these things can only be constructed using axiom of choice.
If you drop this no finite set,
then you can make a dictator.
If all the sets that contain 17,
that's an alpha large set.
That's an ultra-photo,
but it's not as a principal ultra-photo,
but those ones we don't use.
So if you have a choice,
these are very easy to construct.
It's an easy, honestly, my argument.
Basically, it's a greedy algorithm,
but you have to run it at an uncannable length of time.
So no finite set is alpha large,
so any co-finite set,
any set which is a component is alpha large,
so you've already got some sets you declared alpha large.
Then maybe you look at the even numbers and odd numbers.
So at present, neither of those have to be alpha large,
but you've got to pick one of them.
So you make a choice.
So I choose the even numbers to be alpha large.
That gives you a few more alpha large sets.
So any set that contains the even numbers is alpha large,
anything like you move a finite number of
self-albums or even numbers is still alpha large.
But then let's say the multiples of three.
One more three, two more three, zero more three,
one of them has to be alpha large, you make a choice.
You pick one, you throw that in,
and you just keep doing this an uncannable number of times.
You make it a huge number of choices,
but eventually you create an ultra-photo.
So to build on these things,
it needs to be actually a choice.
Now, that is a little bit,
well, if you're really a finetist,
that's sort of unsatisfactory.
But one nice thing about,
there's a nice sort of girdle that says that
any statement which is really finetary,
and by finetary I mean that which is
a phasible in piano arithmetic.
So just in the natural numbers using
just addition and multiplication and so
on like Fermi's Last Theorem or something.
Any statement which is
phasible in piano arithmetic,
so basically anything finetary which is
provable using the axiom of choice,
or more precisely in the ZFC at
septory axioms is provable without axiom of choice.
So you can use ultra-photos
using axiom choice to prove something,
but if what you're proving is finetary,
say the triangle removal or
the Zemurray's Theorem or whatever,
you can also prove it without choice.
So from a logical perspective,
the fact that you use a choice is not an issue.
Okay. So these are non-prism ultra-photos.
There's lots of them.
There's an uncannable number of
these ultra-photos because you have to make
the uncannable number of choices.
But what one does to take these limits is you just pick one.
Okay. So you just choose one non-prism ultra-photos.
And it just doesn't really matter which one.
I mean, they're not quite all isomorphic,
but the finer properties of
ultra-photos are not so
important for the purposes of taking limits.
They're important for other things,
but you just pick one alpha and you just fix it
for the rest of your mathematics.
And so basically, you just pick a voting protocol.
Okay. So now you have a way to take votes.
You can sample any,
you can ask the natural numbers
any question you like and you will get an answer.
Okay. So you just pick one.
And this protocol always has
a good consistency properties
and so all these axioms are supposed to tell you.
Okay. So now once you have the ultra-photos,
you can start taking limits.
So as I said, if you have any finite set,
and if you have any sequence in a finite set,
so you have a finite number of choices,
and every natural number picks one of these choices,
then there's a unique limit in the space,
the outcome of your election.
So you define the ultra-limit of
the sequence in a finite space,
a coloring if you wish,
to be the color or vote or candidate,
which requires an alpha-large set of voters.
Okay. And there's always exactly one.
Okay. So you have a notion,
any sequence taking a finite range has a limit.
Okay. There's a notion of convergence.
Okay. I mean, if it does converge,
then if this converges in the classical sense,
like if this X is eventually constant,
then this limit will agree with
the classical limit because of no finite set is alpha-large.
But okay. But it generalizes the limit.
It extends limits just like
the binoc limits if you know what they are,
in construction, the Han-Binoc view,
they're very similar actually.
Okay. So for any finite set,
you can take any sequence which is
finitely valued has a limit which is still in the space.
Okay. Now, what if X is infinite?
So suppose now you have a sequence
taking values in infinite space.
Well, actually, the way I'm going to think about it is that
actually suppose every element in
your sequence just takes values in a different space,
and X n takes values in
the space of graphs and of n vertices.
Okay. So you get maybe a sequence of
graphs and vertices where n gets bigger and bigger.
Each X n is finite but the union is infinite.
Now, in this case,
the limit usually when you have an infinite space here,
there will be no single X n
which comes up in alpha-large set of times.
So normally, you won't have a limit in this space.
But what you can do is that you can still define a formal limit.
So you just formally define.
To just be a mathematical object,
and formally, if you want the precise definition,
you define this to be the equivalence classes
of all other sequences,
which agree with this sequence where you see
that two sequences similar,
if X n is equal to Y n for an alpha-large set of n.
Okay. This is the formal definition,
but you shouldn't make too much attention to it.
It is analogous to how you
construct the real numbers in classical analysis.
So you start the rational,
so you want to build the reals.
On the standard ways to build the reals is as
the quasi-completion, metric completion of the rational.
So you can define the reals
as equivalence classes of
Cauchy sequences, rationals,
where two rational sequences are equivalent,
if the differences go to zero.
Now, this is a formal way to define the reals,
but no one actually uses the definition.
I mean, that's only for foundational purposes.
But the important thing is that the reals exist,
and that they complete the integer.
So never mind this.
The thing is that you just formally define a limit,
and two sequences have the same limit
if they agree eventually on an alpha-large set,
that if the voters agree that they're the same.
So this is just formally how you define a limit,
and if the X n's live in all these spaces,
this is the set of all such limits for the ultra product.
X n's, and it's called,
and well, there's various notations,
but I will call it product of alpha n as
an intensive limit, so it's a limiting object.
So in the finite case,
if you just take the actual limit,
ultra product, sorry, of a finite set,
you will just get back the same finite set.
That's the only thing you can get.
If this is if X is finite,
if X is infinite,
take the ultra product of a set or itself,
this is called the ultra power of X,
this is called the ultra power,
and it's strictly bigger than X.
So if you have a constant sequence,
then your limit is that constant,
but you can have non-constant sequences,
which will take values in outside of your space.
So just like if you take limits of
Cauchy sequence of rationales,
sometimes you might get a rational,
but usually you get a irrational,
usually you'll get a bigger space.
So just like the rationales will complete to the reals,
every space gets completed to a bigger space here.
So for example, the natural numbers,
now live inside the ultra power of the natural numbers.
So these guys are called the standard natural numbers.
These guys are called the non-standard natural numbers.
Yes. So among other things,
ultra products are used to build non-standard analysis,
but it's not really a scary term.
A non-standard object is just
basically an ultra limit or ultra product of standard things.
So for example, the limit of sequence N,
1, 2, 3, 4, 5, 6, that doesn't exist in the classical sense,
or you could say it's infinity or something.
But this is not a standard natural number,
but it is a non-standard natural number in this bigger space.
In the reals, also some of them live inside the standard reals.
These guys are the non-standard reals,
also called the hyper-reals,
and you can have things like the limit goes from 1 to N.
This is not zero because
zero and one of N disagree for every N.
So the voters will see that one of N and zero are not in the same space.
It's what's called an infinitesimal real.
It's not actually zero, but it's infinitesimally close to zero.
It's reciprocal of this natural number which is an unbounded natural number.
So you can build all these objects.
So just from that,
you can formally take limits of everything on the left and get something,
which is a sort of triviality.
But if you have a sequence, for example,
of graphs or finite graphs,
then just from this ultra-product construction,
I can build for you an infinite graph.
Okay. So V alpha is what it is,
it's just the ultra-product of the Ns,
and the E alpha is just the ultra-product of the Ns.
Okay. So in this limiting graph,
the vertices here are just
actual limits of sequences of vertices in the original graph,
and any edge is just a pair,
each one of which is an actual limit of the original graph.
So automatically, you can take ultra-limits of anything.
Problem of this variety, whatever.
But why would you do this?
So the first observation is that any operation that you have
on your original discrete objects carries over to the continuous world.
So suppose for example, you have a sequence of groups.
So you've got an application operation,
also an inversion operation,
this is the same application at finite groups.
So you can take a sequence of finite groups,
and then you can take the ultra-product.
So this is the set of all four more limits of group elements,
one for each one of your groups.
So you've got maybe one group here,
another group, another group,
and you take every single sequence,
and you get this limiting point here,
and there will be this really huge uncountable limit.
But the thing is, it's still a group,
you still got an operation on it.
Just like once you define say addition on the rationals,
and then you complete it,
you can define addition on the reals just
by because addition is continuous
with respect to Takashi sequences.
If you have two elements in this limit group,
so what is that?
So one of them would be the limit of
one sequence coming from your finite groups,
and then you've got another limit coming from another sequence.
You just define the group law on this limiting object,
what you do is you just go back to the finite world,
you perform the group application here,
and then you take ultra-limits again.
This is how you define multiplication,
and there's no issue of convergence
because everything converges.
So this just works.
One thing you have to check is that
things are equivalent,
like if you replace the sequence of
an equivalent sequence and this sequence,
this product is still equivalent,
but that just follows from the ultra-portal axioms.
Okay. So you can define any operation basically
that you already had in the finite world,
still will still be there in the limit,
and the great thing is that any axiom that you already
had in the finite world will automatically persist to the infinite world.
So the group law is associative in a finite world,
and it's very easy to see that therefore in the limit,
the group law is still associative.
So the ultra-particle of any sequence of groups is still a group.
Ultra-particle of fields is still a field,
ultra-particle of algebraic closed fields,
the algebraic closed field, and so on and so forth.
In fact, this is a great thing called Lawsh's there.
What it basically says is that if you have any predicate on
some finite number of variables and maybe also some constants,
and these will take values in some finite space.
So I guess you have to index these things.
Okay. So if you have any predicate at all,
like the associativity law or the inverse law or whatever,
using some constants and some variables
quantified over some language using the form that exists and everything
quantified in the space, if this predicate is true,
one alpha-large set of n,
then when you take ultra-limits of everything,
if you take the ultra-limit of these variables,
these variables, and these constants, and so forth.
Now you're working in the ultimate space,
then this thing will be true.
This is the same predicate will be true in the ultimate space.
Conversely, if this is true,
then this has to be true for an alpha-large set of n.
So never mind exactly what this says,
but basically it means that anything you already knew about
for your standard spaces are also true in the limit.
So for example, in the natural numbers,
for mass loss theorem is a theorem,
so automatically the one problem I see was also true in
the non-standard natural numbers.
So I can tell you that there's
no non-standard number bigger than 2,
instead of that x to the n plus y to the n for some
particularly positive x, y, z,
where these are all non-standard natural numbers,
so ultra-limits of
natural numbers and things like this, which are unbounded.
I know that the standard natural numbers contain
infinitely many pairs of prams 600 distance apart,
same thing is true for
non-standard natural numbers and so on and so forth.
Okay. So you don't really lose anything.
As long as it's describable in first-order logic.
Of course, for example,
anything which is not describable can be lost.
So all these finite in the limit,
you get something else.
It's not finite, but being finite is not a first-order predicate.
Okay. So you have these limits.
Okay. So maybe I could give you
an example of how you can use this machinery.
So I'll give you a very simple example first,
and then I'll give you a more advanced thing.
So actually, there was a talk yesterday,
talking about the polynomial regularity limit.
Maybe I'll start there.
So this is a very simple thing,
but it already illustrates some of
some of the ideas of how you actually use these limits.
So, okay.
Okay. So let's be now very concrete.
Let's take a field F,
which you can think was a finite field.
Okay. And yes, some dimension D.
Okay. Let's call the dimension capital N,
so that's the number.
Okay. And let's define polynomial is a degree D.
Okay. So these are just polynomials from Fn to
F of degree. So these two,
these are like all the quadratic polynomials
on this finite field vector space.
Okay. F doesn't have to be finite,
but you can think of it as finite.
Okay. So we have this notion of rank.
So if you have a polynomial in here,
you can define, as we saw yesterday,
the D minus 1 rank of P is
the fewest number M of
degree D minus 1 polynomials,
degree less than D minus 1 polynomials,
Q1 of QM, so that P is a function.
Okay. So if you can write your degree D polynomial
in terms of a finite number
of degree D minus 1 polynomials in your finite rank,
and the smallest number of
polynomials you need is the D minus 1 rank.
Okay. So we saw that before.
In a finite field, these ranks are always finite.
In an infinite field,
you can have a function of infinite rank,
but let's never mind that.
Okay. So here, let me give you a simple case.
Actually, just the first step of
the polynomial regularity lemma.
So, okay. So I haven't said a lemma yet.
Okay. So what is the lemmas?
It's the quantitative version.
Which version can you use?
There is it. Okay.
So suppose you have a bunch of polynomials, E,
built. So given any polynomials,
they may or may not have low rank or high rank,
but okay, so you have some polynomials,
also in their function,
after you function, think of like
an exponential tau, exponential function.
So some rapidly growing function.
Then, I can always find a good basis.
I can always find another set of polynomials.
These are the refuer of these.
So this number of b's, I said most a,
such that every p i or every i.
Oh yeah. This and also a number m,
and m is some bounded number bounded by f.
Okay. There's some number m,
but not only by this function f.
It's of a tau exponential in f basically,
such that for all i,
every p i can be written as
a linear combination of the q j's plus an error.
And this error has bounded rank.
The rank of these errors.
So I can find a good basis of polynomials,
such that the original polynomials can be written
as a linear combination of these coefficients,
plus an error of bounded rank,
and that these guys are independent in the following sense,
that there are any non-trivial combination.
So if you take
any non-trivial combination of these coefficients,
of these polynomials,
then these guys always have big rank.
And in fact, very big rank,
bigger than the function f applied to m.
So in practice, using m,
f is being like a tau exponential.
So these errors are rank most m,
but these guys have ranked bigger than 2 to the m or whatever,
something really big.
Okay. And so, this is
the actually the first step
of the polynomial regularity lemma.
In practice, you take the polynomials,
the d minus 1 degree polynomials that come up show up here,
and you have to recognize them further to d minus 2 polynomials and so forth.
But let me just ignore all the remaining steps of the process.
Okay. So this is a lemma.
It's not hard to prove, as was stated yesterday.
It's a very simple algorithm.
You take these p's polynomials,
and either that they're already independent in the sense that
every non-trivial combination already has really big rank.
Okay. If they are independent like that,
then you're already done.
If not, one of them is equal,
is going to be written as the combination of the others,
plus a bounded rank error.
So you remove that polynomial in terms of all the others.
And now you ask, are the guys that left,
are those independent and so forth?
But you have to keep track of all these quantitative things.
What was the rank of all the errors and so forth?
And it's a little bit fiddly.
It's not difficult, but it's,
to actually write it out in full detail, it's just a, or it's a page.
But you have to keep track of this m and this f,
and it's a little bit fiddly.
Okay. On the other hand,
if you don't care about bounds,
then it's really simple.
So here's the qualitative version of the same lemma.
A qualitative bit like that.
So you have a field which can now come in infinite.
Take a bunch of polynomials in that field.
Okay. And now there's no function.
Okay. So the qualitative version is that,
given any bunch of polynomials,
I can find another set of
polynomials with fewer subsets actually of the original set.
Such that every polynomial is a linear combination,
plus an error of not just finite rank.
Okay. So up to now finite rank rather than bounded rank.
There's a subtle difference here.
Okay. That every polynomial is
the linear combination of these polynomials,
and these polynomials are
independent module of lower rank errors.
So. For all non-trivial.
So, okay. So any combination of these now has
infinite rank rather than bigger than this f of f.
Okay. Now, this lever really is
just an immediate consequence of set of linear algebra.
See, so a basic theorem in linear algebra is that if you
have a bunch of vectors in a vector space,
then you can always find a sub-basis which are linear independent.
It's such that everybody here is
a linear combination of everybody here.
I mean, this thing doesn't even have a name,
so it's still trivial.
Every space, every set of vectors,
has a linear independent basis.
Okay. And this lemma is,
you just apply that lemma and to what?
What you do is that you take the polynomials of degree D,
and you caution out by the finite rank polynomials.
See, the great thing about working in the qualitative world,
rather than the quantitative world,
is that the sum of two finite rank polynomials is still a finite rank.
And so the set of all finite ranks is a vector space,
and you can caution by it.
And this is something you do in continuous mathematics.
You take caution.
You want to do this in the discrete world.
It would make your life easier if you could caution up
by all bounded rank objects,
but you can't quite because it's not quite close to an addition.
But sum of two things are rank m is a rank 2m.
And so you don't have a single space of bounded rank objects.
You just have sort of this whole nested sort of family of spaces.
But in the qualitative world, you just have one,
the finite polynomials.
And so you just take the original polynomials,
caution up by finite rank.
This is an effective space.
Linear algebra tells you there's a basis there,
pull it back, and it gives you this theorem.
Okay, so you don't have to work.
Okay. You have to use a very trivial linear algebra effect.
Now, okay.
So, now this one looks weaker than this one.
Of course, if you have a quantitative lemma,
then it's very easy to deduce the lemma.
But these are actually equivalent.
Okay. Once you have the qualitative regular lemma,
you just use an ultra product.
And you automatically get the finitary thing too.
So let me see if I can do that.
Okay. This was supposed to be one of several examples.
Okay, but I only have one.
Okay.
All right.
I was going to show you the seminal regularity lemma
in the ultra product method.
That was probably the most interesting,
because this is fairly simple,
but this was sort of a warm-up.
Okay. So, suppose you know the qualitative regularity lemma.
Why is the quantitative regularity lemma true?
Okay. So, you have to have a contradiction.
Okay. So, suppose quantitative lemma is false.
Okay. So, then you're going to negate all these quantifiers.
So, yeah, it's a bit messy, but what you find is,
when you carefully negate everything,
what you find is fine.
This means that there is no bound.
This statement is not true for any bound on M.
So, okay. So, there exists a function.
So, it's that no matter how large a bound you pick,
you can find a field, fn, and dimension, I guess, nn.
Okay. Okay.
You can find a field, you can find a dimension,
you can find a set of polynomials.
Okay. So, you can find a set of polynomials in this vector space,
such that you can't do this.
Okay. Such that for no.
Okay. Yeah. Okay.
This looks complicated,
but it's very straightforward that once you get a practice of this,
it's always the order back,
just follow your nose and it will always work out.
But okay. First time to do it has to be rather careful.
So, you can find a bunch of polynomials just that no matter
what M you pick up to the specified range,
there is no u1 up to qb.
A, such that these things are true.
Okay. I'll just call this one and two.
Okay. Okay. So, when you negate this statement,
you get a rather complicated assertion,
but okay, you get something.
So, basically, the general philosophy is that you
take a quantitative negated,
then once you have this counter example,
you take the ultra product of a counter example,
and that will give you a counter example to this limit here.
So, maybe I won't do it,
but okay. So, the sequence of fields,
you take an ultra product, you get another field.
Okay. You take an ultra product of these vector spaces,
you get a big vector space of this field.
You take the ultra product of these polynomials of degree D,
you get another polynomial of degree D
over this much bigger vector space.
Okay. I won't do it,
but you can just believe me that if you take
the ultra product of everything that I just wrote down here,
you'll get a counter example to this statement here,
and hence, because this is true,
this has to be true as well.
Okay. So, it's still the same proof.
Here, how do you prove this limit
that every finite vector has an independent basis?
You want the same algorithm that you do to put the quantity of things.
If the independent already you're done,
otherwise one of them is in some of the others and so forth.
But the thing is, this is already in a little literature.
You don't actually have to use this to explain the argument again.
You can just use it as a black box.
You have these convenient things,
you take quotients and so forth.
So, it's repackaging the same argument,
but in a clean conceptual way.
You don't have to worry about exactly what
lower-rank and big-rank mean.
This is all done automatically for you.
Okay. I won't have time to regularity them unfortunately.
I think I can do the correspondence principle.
Okay. Maybe I'll just end with that.
Okay. So, it's everybody's theorem.
So, what's, okay.
It's everybody's theorem of a phrase like this.
So, for every k and every delta that exists in n,
such that for all subsets,
or sufficient large n,
actually, sufficient large n.
Such that there's a subset that every subset of
integers of one to n,
of density at least delta n,
the K-tune athletic progression.
Okay. So, this is everybody's theorem.
So, first and foremost,
prove this theorem by
proving a measure theoretic analog.
So, for every k and every measure preserving system,
X, so a measure preserving system is
a probability space with a shift which preserves measure.
So, if you have any space X with a measure,
probability measure and a shift on it,
and any set of positive measure now.
So, there's no delta anymore, just positivity.
This is more qualitative statement.
Then you can find a positive number n such that
A and the shift of A and the shift of A a few more times.
Also, it's positive measure.
So, given any set A,
it's possible to shift it in and again,
so that the intersection is still non-empty.
That's still positive measure in particular, it's non-empty.
Okay. So, this is a thing that you can prove by
got a few methods,
but you don't need to know how it's proven.
If you're willing to accept it as a black box,
this thing again is actually equivalent to somebody's theorem.
If you have somebody's theorem,
it's easy to prove this theorem.
You look at a typical orbit of a point,
and the typical orbit will be like a dense set as I'm ready.
But conversely, you can use
this qualitative result to prove this quantitative result.
The way you do this is that,
okay, so if this is your supposed not,
so suppose Xamarin's theorem is false,
then there exists a K and a delta,
and there will be a sequence of numbers going to infinity,
and a sequence of a n inside these n n,
of all of positive density,
such that no a n can stick at K a p.
So, you've got a sequence of counter examples,
which are getting bigger and bigger,
but all have uniform density.
It's possible to take an auto-product of everything.
So, these n's will, you take an auto-product,
and you'll get a non-standard natural number.
So, you've got the natural numbers,
and then you've got these much bigger,
sort of double-position natural numbers if you wish,
okay, long natural numbers, okay.
And you take the ultra limit,
so you get this really big non-standard natural number,
which is bigger than all the standard ones,
and you get a subset of,
okay, so you have this big interval now,
and you've got this big subset here,
okay. Now, each of these finite intervals here
comes with a natural measure,
which is the normalized counting measure.
So, on these n n's,
there's a natural measure,
which is just the density measure.
So, that's a probability measure on this space.
There's a way to take auto-product of measures.
So, you have to be a bit careful because
measure theory is not a first-order theory,
and you have to use the categories,
you have to use actually a bit of measure theory to do this.
Okay, but it is possible to construct a measure,
a probability measure on this space,
which in some sense is the ultra limit of the measures here.
I won't explain how that works because I don't have time.
But it's called load measure.
I'm going to look it up.
Okay, so there's a nice probability measure on here,
which is in some sense the ultra product of all the measures here.
And so, all these sets of positive density,
if you take the limit,
you get a set and it will still have positive density.
But still we measure bigger than delta, so it turns out.
Now, why do you pass this limit?
The thing is, over here on the limit,
you have a shift x goes to x plus 1,
which isn't quite well,
and then you wrap around.
If you go over here, you wrap around a zero.
And the thing is, it only needs to be defined almost everywhere.
Okay, so you don't define that at the end points,
but the end points have measures zero.
So outside of the system measure zero,
this shift is well defined.
And this is a measure-preserving system.
You see, on any finite interval,
the shift is not a measure-preserving because every time you shift by one,
somebody falls off the edge.
Okay, and so you lose a little bit of measure.
But in the infinite world,
or once you go through this non-standard world,
you up to the system measure zero,
you are invariant.
And first and foremost,
theorem can tolerate bosses on a system measure zero.
And so you've now constructed a measure-preserving system.
And it's not hard to show that this sequence of count examples in the limit
gives you a sequence of count examples to first move through.
So what you see is that every time you move a quantitative statement about
finite tree things, you assume it fails, you have a sequence of count examples,
you take an outer product, and this will be a count example to some continuous
statement, and if you just prove that continuous statement,
then you prove your discrete thing.
And you can do this for all kinds of other things,
like the regular dilemma and so forth.
But that, I think, well, you're interested, I can tell you in person,
but I think I'm up for it.
Thank you.
We have a few minutes for questions.
Oh, first came up with all the talking.
I have no idea, actually, I'm too good at that.
After a fix.
No.
I mean, it's very old-top, I mean, Boba can use them all the time,
but they weren't up here, certainly.
But no, there's some.
Hi.
Actually, I'm one of the people who originally defined
counterproducts in the late 50s, and influenced Tarski.
He didn't understand Lerner's paper in the time
because he wrote it in terms of Boolean algebras,
but then later we defined counterproducts.
And I was the first person to prove the compactness in Boolean,
which you made use of.
Yeah, it's implicit, yeah, I didn't have time to talk.
So it's a little weird, after 55 years,
to hear a lecture about autocollet.
So I'm glad you're bringing the fact to people's consciousness
because there are a lot of uses of them.
And of course, the work of Jerry Keasler
and his properties were very, very important.
You referred to the word major there.
Well, it's a special case of Keasler Major's construction, yeah.
Basic discovery and stuff.
I hope people will find other applications.
Oh, yeah, yeah, there's a whole bunch of useful stuff, yeah.
So this was done in schools.
Okay.
Okay, that answers your question, right?
You can use that.
Yeah, you're referring to a theorem of Yiddel
that is a finite statement is proven in the DSC,
that it's proven in the...
Yes.
Does it have a constructive proof?
Is there an algorithm given that you can prove?
It's literally constructive.
Okay, so in the sense that the way it works
is that you work in a universe in which you only have ZF
and not CFC.
And then inside the universe,
you can prove what's called the Constructible Universe.
Roughly speaking, that's all the things
that you can define uniquely with predicates.
And even if your external universe doesn't obey choice,
then this new sub-universe that you construct
doesn't obey choice.
I think it's a constructive universe.
See the VOL, I can never go and move on this question.
Okay, but...
Well, it's even better than the first-order properties
in terms of the absolute...
Oh, well, yes.
Second-order properties here are preserved as well.
That's true, I don't know.
Yeah, I don't know how that one's proven, though.
That's a more difficult theorem.
Yeah, but yeah, so there's a sub-universe
which you actually have choice.
So, oh, you, please.
And it turns out that the natural numbers
are the same in both universes.
And so, whatever you can prove in a big university,
look at this one, university, let's face it.
So, when you deduce a finite tree,
there and from the infinite terms,
you don't get any effective bounds on the boundaries.
If you use the infinite thing as a black box,
then you get absolutely no bound whatsoever.
In practice, though, if you actually open the black box
and you look at every single step in the proof of the infinite,
then usually every single step in the infinite world
has some analog in the final.
The analog will be much messier.
There's a lot more epsilon than quantifiers.
And every time you take a limit,
you have to instead run an algorithm.
And there's a lot more looping.
And so, you can unpack.
And so, roughly speaking, if you completely,
you know, using a proof of somehow complexity n,
then you can get a bound which is somehow like,
you know, a tau of height n or something else.
There's some relation between the difficulty of the proof
of the infinite statement and the bounds that show up.
But, typical things are tau exponential, right?
The rule of thumb I have is that if you're trying
to find a result and the only bounds that you hope to get
anyway are tau exponential,
then you may as well go to the infinite world
because you're not losing much by leading up.
But if there's a chance of polynomial bounds and so forth,
then usually you don't want to use the infinite world
to prove those, you want to say a plan out?
So, the fact that you generally get a tau of exponentials,
is this kind of an empirical thing, or is there a general?
There are these general, these things called proof mining,
that the proof theory was set up to,
I don't know the precise statements that these,
but it's how many times you have to do things
like take limits, or take spans, or take completions.
So, you know, for some of you,
in the polynomial regularity example,
often what you do in the continuous world,
is that you have a finite number of generators,
or some small set of generators,
and you just look at what they said in a span.
You look at all the combinations
and you take a completion.
Every time you actually use one of those completions,
that will cost you some tau,
when you go back to the finite world.
I mean, it's something that you do
without almost a lot of thinking.
You just, you know, you take spans,
the completions, and the continuous world.
That is often what's expensive.
Do we have any notion of rate for the ultra limit?
Would it be meaningful to have such a notion?
Well, I mean, you can use various infinitesimals
to gauge rate.
So, like, if you have a sequence of numbers
that converts to zero classically, in a classical sense,
then if you take the ultra limit,
then the ultimate is not zero,
it's what's called infinitesimal.
But there's different ranges of infinitesimal.
So, for example, let epsilon be the limit of one over n.
Okay, so this is an example of infinitesimal.
This infinitesimal will be less than epsilon
if xn is eventually less than one over n.
If it's less than one over n for an alpha-dark set of n,
then this is less than, okay.
But, so for example, the sequence one over n squared
is less than epsilon.
But you can ask, are you less than epsilon squared?
And that measures a faster rate,
or are you less than e to the minus one of epsilon?
That's an even faster rate.
So, rates of convergence become replaced
by just comparison with various infinitesimals.
Yeah, so when you go to the non-center world,
you don't see limits anymore,
because all the limits have already been taken.
What actually matters is it takes all your limits
for you at once at the beginning.
And then, calculus and analysis becomes sort of algebra.
And saying that one sequence is eventually bigger than other,
just saying that one is less than a constant as the other.
I'd say we thank Terry again for being here.
Thank you.
Thank you.
Thank you.
