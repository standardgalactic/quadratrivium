Thank you, organizers.
The talks have been really very, very interesting.
And so the topic we discussed today
comes from the study of a very specific problem
we were interested in, which I think
hides some interesting mathematical aspects
and also I think is appropriate for this workshop.
And so what we'll be talking about
is some work that we have been doing in our group in Turing.
And I should mention Carlo Baldassi, Federica Gerace,
Alessandro Grosso, Carlo Luccibello, and Luca Salietti.
There are three PhD students, a postdoc and a colleague
of mine, Baldassi, who did quite a great part in this work.
And then there's some other ongoing project
with Christian Borgia and Jennifer Chays in Boston.
So essentially, the type of problem we want to discuss
is try to understand if what we have learned about random
constraint satisfaction problem can help us in understanding
issues in learning.
And in particular, we will see that treating learning problems
as constraint satisfaction problems
will open some new perspective and will
allow us to understand why and how
such a type of neural network work.
And this will lead to some new results
and hopefully to some new algorithm based
on a new measure for analyzing solution
in the case of random instances of learning problems.
So just in a very philosophical sense,
what we wanted to do at the beginning
was try to put together our understanding of constraints
as a satisfaction problem and of learning problems.
But then what happened that, in fact, by doing this,
we discovered some new geometrical structure
that somehow escaped from our analysis.
So this is a nice example in which, actually, the replica
method is not sufficient to understand what happens
in the learning problem.
Because what is relevant from an algorithmic point of view
is hidden in the tail of the Gibbs distribution.
And it has to be somehow revealed by a large deviation
analysis, which is not captured by the standard replica
symmetry breaking scheme, at least OK.
So just a little bit of review.
So spin glass theory has been used
to study learning problems since the 80s.
And there were some very important results in the 80s,
like the analysis of attractive neural networks,
memorizing random patterns, or the study
of the computational performance of fit forward
networks, and so how many random patterns you can actually
store with the neural networks, and with one
or multiple layers, and so on.
And, however, somehow, I should mention that in that period,
many of the things that we have been discussing recently
about random optimization problem
were already observed in the case of formal neural vectors.
For instance, the fact that the space of solutions
decomposed into many domains, some of them
have carried the dominant Gibbs weight,
and some other are subdominants, and so on.
All these things were already known
in the case of neural vectors storing random patterns.
So there are a lot of things that we did knew about.
But of course, the problem was that at that time,
we were just using essentially the replica method.
So no algorithms.
So somehow, the interest in this kind of study
faded away.
So somehow, there was a peak of interest in the 80s.
And then, somehow, the communities
stopped working on this type of topic.
But of course, in the last year, there's
been a lot of activity in the field
of artificial neural network.
This is due to deep learning, so to the fact
that GPUs became available, a lot of data, and so on, so forth.
A lot of biological data, which one can use to model systems.
And so now, we are back.
And we think this is the appropriate time
to try to treat, again, this kind of problem
and try to understand more from the algorithmic point
of view what's happening in this system.
And as we know, in the case of deep learning,
the performance are kind of interesting.
However, there's a lack of understanding
from a mathematical point of view.
And also, there are some clear computational bottlenecks
that are emerging.
For instance, already now, deep networks
are just using few bits of precision
because they need to run fast.
Otherwise, they cannot cope with the amount of data
they had to deal with.
So somehow, even at the hardware level,
there are some problems emerging,
even though the theoretical understanding is not even very
limited.
So some notation.
So we consider the problem of storing some patterns
that I call xi nu.
And in particular, we consider the case
in which these patterns are binary,
but this is not necessary.
And the parameters of the models are called w's.
These are the so-called weights of the neural network.
I guess most of you have seen what these kind of models are.
And I'll show it in a minute.
So essentially, the idea is that you give,
this is the architecture.
On the edges here, you have some weights.
And these nodes compute a nonlinear function
of the sum of the incoming signals that are weighted
by the weights.
So you see here, you have psi entering here.
You reweight the psi j with the wjk.
And so you have an input signal arriving here.
This neuron makes a nonlinear transformation.
In this case, it's a step function and produces an output.
And this output is projected to another neuron and so on.
And you can generalize this to very complicated architecture,
but the scheme is this one.
And the idea is that you want to find the set of w's
such that a given set of input is labeled in the right way.
You want to implement an input-output mapping,
which is correct on a given training set.
Now, if you think to this problem in terms of a factor graph,
this has this form.
So now your independent variable will
be the weights over the edges.
And so I represent them with circles.
And each pattern, so each input-output relation
corresponds to a constraint that the w have to satisfy.
So the factor graph you have to deal with
would be a fully connected graph with the number
of factor nodes, which is equal to the number of patterns
that you want to satisfy.
So this is just like a case set problem,
where instead of having k input links in the factor graph,
you have order n input links.
And the function, which is implemented by this factor node,
depends on the architecture of your network.
In the case in which you have just one output
for an architecture like this, essentially what you're trying
to do is to divide your inputs into sets.
But of course, this can be generalized to much more
complicated objectives.
So the problem I want to study is I
want to see which are the performance of objects
like this in the case in which the w's are constrained
to be very, very simple.
So low precision synapses, weights.
I call the weights synapses.
So essentially, I consider the case in which the weights are
bits, just plus or minus 1.
So I want to know what happens if I try
to learn with binary neural networks.
And there are some, so as I said,
we were interested in the problem.
So there are some biological reasons
why we're interested in this problem.
So this is a beautiful picture.
I don't know if the resolution allows us to see all the synapse.
You can see the vesicles, which allow charges
to go through the synapse.
Anyhow, there are some experiments
showing that synaptic potentiation
is a kind of a unitary event.
It's not a very precise graded process,
but it's a rather a step function like this,
in the sense that if you stimulate the synapse,
this undergoes a potentiation.
And then so it's a kind of a noisy switch.
That's more or less the idea.
And also from the modeling point of view,
I really like to believe that you have 10 to the 15 synapses
and you want to have a model that doesn't
rely too much on the precision.
And also, as I mentioned before, even for technological purposes,
you have to deal with the fact that you must have limited
precision connection in your hardware.
Otherwise, it is too time-consuming and too
heavy for the hardware to run.
And there's some recent work that shows that in general,
I mean, some experimental work showing that in any case,
and whether the synapses are binary or not, it's not clear.
However, synapses have a limited precision, a few bits.
This is done by measuring some, with the higher solution,
the synaptic contact.
But anyhow, so I don't want to enter.
Just to say that I do want to give some motivation,
because these are the true motivation for us
as a group of physicists to study this problem.
So the problem I'm going to study now is the following.
I'm going to take the simplest possible neural network, which
is the building block of deep network,
so the single perceptron.
So I just have one layer and one output.
And I have the weights here.
I will assume that the weights are binary.
And the output is given by a nonlinear function.
In this case, there's just the sign
of the sum of the wi times the input.
So this is the nonlinear transformation.
And what we want to do is to find the w's, which are binary,
such that a certain set of input-output association
are implemented by the machine.
And so essentially, this is, again,
the same pathograph as I've shown you before.
And here I consider the regime in which the patterns are random.
So they are just plus or minus 1 with equal probability.
And the interesting regime is the case
in which you try to store order n patterns with this machine.
So it's just like random case set where you have, yeah.
Do you assume anything about the input
are all some kind of distribution or anything?
Well, I take just a flat distribution.
Just plus or minus 1 is probably 1 half.
But this is not.
I mean, it has to be factorized.
But you could consider bias things.
OK, so this problem was starting in 1989
by Kraut and Nezade and by Gardner and Dalida even before.
So there's a lot of paper.
And so the phase diagram is very similar to case set.
So there is a critical, actually it's harder than case set.
So there is a critical alpha above which you cannot satisfy
the constraints, so you cannot store the patterns.
And it's very interesting because in this region here,
you have even what is called freezing.
So from a statistical physics point of view,
as you lower the temperature here,
the system freezes completely at finite temperature.
So it's like a random energy model.
So it's a particularly hard problem here.
And in this region here, well, you
have a certain entropy, which means
you have a certain amount of solution which goes to 0
as you approach the critical capacity.
And so this was the scenario.
And what it was known also that the solution were isolated.
So the dominant solution for this kind of random satisfaction
problem are kind of isolated clusters.
So each solution is ordered and far away from another solution.
They're point-like solutions.
So it's like a network-correcting code.
So completely different from random case set.
You don't have the unfrozen variable you can play with
or you don't have that.
That was the scenario.
And so what you would expect is that it's
very hard to find a solution.
And in fact, heuristic algorithm would believe to just not
to work for this kind of problem.
And so more or less the situation is just
to make a comparison with case set that in case set,
you have this kind of phase diagram
has the number of constraints over the number of value increase.
So you have a connected cluster of solution
which breaks into many components and so on and so forth
until they disappear.
And there's a region in which it's
hard for a Monte Carlo algorithm to find solution
because of the glassy states.
And so there's a 1 RSB region which is hard.
But in the case of the binary perceptron problem, which
is more or less the same, just fully connected,
what you have is just a kind of isolated solution.
That's all, apparently.
And of course, the problem is apparently hard for any alpha.
And so one could wonder whether learnings
is at all possible with these kind of problems.
So if you have a simple model of a new one
for which learning is kind of impossible,
it starts to be annoying.
And if you just run simulated annealing on this problem,
and this is just small simulation,
you clearly see that it gets stuck.
I mean, this is for a small size.
Yeah?
Just in terms of modeling, why do you
want to associate two arbitrary patterns, arbitrary plus
and minus one?
Do you think that that's something
that happens in new problems?
No, your problem can be easier.
You're right.
This is kind of a hard case, but you might have to.
So yeah.
So the question we wanted to address
is whether it's actually learning
impossible in these kind of systems,
and so what's going on in this constraint satisfaction
problem, which is strange.
So recently, Kabashim and Huang and other people
started to study the actual distance
between typical solutions.
So what they use is they use the so-called Franz-Parisi
potential method, in which essentially they
take sampling from dominant solution
and compute the distance between solution.
So you have to compute this quantity by the replica method.
And they were able, they just computed,
which is the minimum distance between dominant solution
as a function of alpha.
And what they found is a curve like this,
which shows that for any value of alpha,
so even for very few patterns, dominant solution
are far apart.
So which means that at the landscape,
you should think of, for this problem,
is the landscape in which you have solution, which
are far apart, order n, I mean distance,
and then you have a lot of local minima
above which trap the Monte Carlo algorithm
or any type of algorithm that wants to minimize this energy.
So this is a complicated but standard replica calculation
that can be done.
And so the question is, this is the replica prediction.
By the way, this replica prediction,
I mean, the original work, Bernhard Kraut and Marc Mesard
studied this problem up to two levels of replicas
into breaking.
And they were not able to find any other solution except the one
that I've been describing.
So unless there are some very complicated numerical issues,
the main message is that even if you use the one RSV scheme
and you look for, say, subdominant contribution,
as it would happen in the case of KSAT.
I mean, you have shown this picture, which
depends on lambda, which you compute the complexity
versus the entropy.
And then you find those which dominate the Gibbs measure.
I'm referring to the talks of yesterday by Alan and Nike.
Then, well, if you do the same thing here, you find nothing.
You just find one point.
Don't find the whole curve.
Just one point.
Yeah?
If you have a low-correction capabilities,
and you always have a distance over that.
Yeah.
So any memory device would have the same importance.
Yeah, but let me, because the conclusion I want to reach
is actually the opposite of that.
So let me.
So this was the, so essentially the question is,
is it landscape, a golf course, and is learning impossible?
Well, that's a numerical evidence.
With Alfredo Branstad and some few years ago,
we just applied the belief propagation
with reinforcement, so-called as a smooth decimation
procedure based on belief propagation.
And we were able to actually solve this problem up
very close to the critical capacity.
So you see, so you have a kind of a paradox.
On the one hand, you have this landscape,
which is made of isolated solution.
On the other hand, you have an algorithm,
which is very simple that works.
Not only that algorithm works, but even very, very simple
versions.
You take the, I don't want to spend much time on this,
but you take this BP equation, try to simplify them.
This is a work that we did in collaboration
with Nicola Brunel, who is now in Chicago,
who sent us a list of five conditions for our algorithm
to be biologically plausible.
And so we implemented them by modifying
and simplifying the equation.
Everything at the end, the algorithm boiled down
to a very simple update rule for the weights,
which is very, very similar to the so-called
perceptron algorithm, just adapted to binary weights.
And what we could see is that this extremely simple
algorithm indeed works again,
and doesn't show any finite size effect.
Carlo Baldassi considered the case in which
instead of storing a fixed set of random patterns,
you store a pattern that are generated by another network,
so you continue to feed your perceptron with new patterns.
And so you have a stochastic process
with independent inputs.
And so you can study the evolution equation
as a differential equation of this process,
and it was able to show that, in fact,
it's possible to do learning
with a very, very simple algorithm, okay?
So the main question is, you know, is this,
I mean, is this a finite size effect, or what's going on?
So that was the, so another thing that one can do
is just to take one, use one of your favorite algorithms,
and then use BP to compute what is called
the weight numerator function
from the solution that you have found.
So compute how many solutions you see
at a given distance, D, from a reference vector
that you have found with some algorithm.
And if you do this, what you find,
that in fact there's no cut.
So the solution that you find are actually not isolated.
And if you actually plant a solution
by these teacher-student mechanisms,
you do find that there is a cut.
So there's a kind of mystery here.
So the kind of solution we found
are absent from our phase diagram and steel,
and they appear to be not isolated.
And so what we did is that, okay,
so we took again this problem,
and we decided to look for large deviation properties
and to see what was going on.
So again, we take the binary perceptron,
and what we decided to do is to actually try to amplify
the contribution from regions
that contain a high number of solutions.
Since the evidence was that,
we set up an analytic tool, say,
so we define a new measure for counting solution,
which gives more weight to dense regions
compared to sparse regions.
So we define a new energy function,
which is not just the number of errors
that the device makes,
but is actually the log of the number
of satisfying assignments for the W's
within a ball of radius D from a reference configuration.
So we define this new, this function,
which is, which we call the local entropy.
So it's the number of solutions,
which is, it's an exponential number
that we assume it's exponential,
that you find within a certain hypersphere of radius D.
And so what we now want to do is to study
the property of this objective function.
So we want to see if by analyzing the problem
through this measure, which is completely different
from the Gibbs measure,
you're actually going to see something different.
And you clearly understand that if you use this kind of energy,
all the point-like solutions disappear from the scenario
because they would have energy,
this local entropy equal to zero.
Okay, yeah.
Can you just comment how different is it
from the temperature from sparse regions?
Well, the point is that here you're going to optimize this W
in order to maximize, so these are not typical, okay?
So you're not sampling from the typical solution
of the Gibbs measure,
but you're going to optimize at the end
with respect to this W.
Okay?
That's the main.
And so, well, there's a lot of calculations
to be done here, but using the replica method.
So this is, we use the replica method as a method
to compute the average energy for this problem,
but this has nothing to do with the replica method used
to analyze the problem from the beginning.
I mean, the replica method you would use
from with the standard energy.
So it's just a tool that we're using now
to study this measure.
And okay, I'll skip the details,
but we have to introduce a lot of other parameters
in order to study this problem.
So you have to, there's an overlap
between different high-density reference solution,
overlap between solution belonging to the same region,
well, and so on.
You're all possible overlaps between solution and centers
within clusters or between different clusters.
So this is what comes out from the theory directly.
And if you're interested,
I can tell you the details of all these calculations.
And this is what comes out.
So first of all, let's focus on these red lines.
And the red lines are telling us the following.
So here is the distance.
And this is the entropy at a given distance, okay?
Now, the red lines are actually the solution
which dominates the Gibbs measure.
And you see that for a given value of alpha,
let's say alpha equal 0.6,
as you reduce the distance,
this entropy hits zero,
which means that there is a gap
below which there are no solutions, okay?
This, if you take as a reference vector,
a typical solution.
So it's a solution that would dominate the Gibbs measure.
And for a different value of alpha,
you would have another curve.
But for any value of alpha, you have a gap here,
just like in error-creating code, okay?
However, if you now look at the,
if you look at the other measure,
based on this local entropy,
which of course we are going to,
so let's, sorry, I probably, let me just go back.
Yeah, so what we want to study
is actually this function here.
And if you take, where you raise the power,
you raise the number of solutions to a power y,
which is equivalent to put a temperature here
in front of the energy.
We want to take this artificial temperature
to have a large value in order to find cluster
that are very, very dense, okay?
And so this is what happens is that
if you use this measure,
you actually find that there exist solutions
that present no gap.
So there exist regions that are extremely dense.
And when I say extremely dense, I mean, look at this,
you see this, well, you cannot even see,
there's a black curve behind this blue curve here.
And this is just the binomial coefficient,
so it's the total number of configuration
within a certain wall of radius D.
And you see that for alpha, you could say 0.4,
it's short distance, essentially all solution,
all configuration around the reference solution
or our solution.
So somehow you have a lot of points,
an exponential number of points,
but somewhere you have a region
which is extremely, extremely dense.
And this is true for any value of alpha.
And simply, you know, for a certain distance,
the curve just overlaps with essentially the binomial.
Of course, there's a tiny difference,
but so these are extremely dense region.
And then up to a certain critical value of alpha,
where this is true up to a certain critical value of alpha,
where this cluster disappears.
And so the value of alpha where this cluster disappears
around 0.77.
And already, you know, 30 years ago,
there was some numerical evidence
that, you know, algorithm could be working up to that value.
So I found really, I mean, if you think,
that we started from this problem,
and then all of a sudden we,
by doing a different type of analysis,
we found that there are, you know,
kind of extremely dense regions
that, you know, were missed by the replic analysis
that are those that allow algorithm to work efficiently.
This is a numerical exhaustive simulation
that I find very nice.
So let's say that we are here.
So we are close to the critical point
where this dense cluster disappears.
So this is a numerical simulation, exhaustive simulation.
So this is a cluster alpha is equal 0.756.
And you have 95,000 solutions in this green.
And then you add one pattern, and this breaks,
and you add another pattern, another pattern,
and then it disappears.
So three.
So just to say that,
this disappears with the first order transition.
All of a sudden it's just.
And we also analyze the problem
of making prediction with this device.
So instead of providing,
just taking patterns at random,
we consider the case in which we take patterns
generated by another network
so that we can ask the question,
are we able to learn a rule with this kind of device?
And it's very interesting because for this kind
of simple model, you can actually compute
what is the optimal Bayesian prediction you can do,
which is this green line.
So here you have the number of patterns
that the so-called teacher network
has provided to the student network.
It's called the teacher student scenario.
And this is the generalization error.
So the chance that you make,
you give the same answer on a pattern
that you have never seen before.
And this is the Bayesian optimal that you can do.
This is the behavioral typical solution,
this point-like solutions.
And these curves here are those,
the errors provided by the solution
which belong to this very dense region.
So they have, the intensity regime
is an under sampling regime.
So it's here and you see that it,
they behave quite nicely.
So that's.
And okay, so this was a,
and also we applied this to some real problem.
And this is the very famous,
most studied database in, I think,
in computer science,
the hand digit recognition problem and so on.
And with a very simple architecture,
essentially just one layer of learning
and then you just have to have 10 outputs.
And with this kind of device, with binary weights,
you could have a prediction error of 1%,
which is more or less state of the art
for such a simple architecture.
And also what it's possible to show
is that if you look now to the solution
that the algorithm find and you do a random walk
starting from one of the solution,
you can find that indeed,
all these perceptron here have their weights
which belong to a dense region of their weight space.
Okay, so learning takes place
in this rare, very dense regions.
At least this was the numerical evidence.
Everything generalizes to multiple states
and we are actually now studying the continuum limit,
which we believe also has similar properties
and I can tell you why,
but it requires few minutes and I don't have probably time.
So everything is more or less remains unchanged
and also where one can discuss
of how many bits it's worth having to have good performance
and it turns out that few bits are enough
to obtain very high performance.
So the question is now,
well, what happens when,
so can we use all this idea to make new algorithms?
Well, we have a new measure,
we have a new objective function, okay?
And the first thing you want to do
is to create a Markov chain
that tries to maximize this new objective function.
Just forget about the take balance
of relative to the original problem.
So this is what you would do.
You would define an energy function
which is essentially the number of errors
that the network makes.
And there are some details here
because you also would like to use the information
of how bad is the prediction.
So how bad is the input field
so you can improve your simulated annealing,
but it's not relevant.
So this would be your energy function
and you can try to do simulated annealing
and you would see that this Markov chain would fail.
But now what we can try to do is that,
okay, instead of using that objective function,
we use this new objective function
where we take as an energy the local entropy.
And so the question is,
how do you compute the local entropy,
which is an exponential object?
You can use belief propagation
to compute this local entropy.
And so you can set up a Markov chain
in which you flip your reference vector, okay?
You evaluate your local entropy
and if this increases, you keep it,
otherwise you reject it with a certain probability
that depends on the artificial temperature
that you have used, okay?
And so if you do this, it's very nice
because this is what you obtain
with the standard simulated annealing.
This is a small, just 800 neurons
with very small value of alpha.
So you have to imagine that if you increase
with the size of with alpha,
this plateau just goes to infinity.
But you clearly see that this is the situation.
And if you now use the other Markov chain,
this is what you obtain.
You just go down straight to the solution.
So all these local minima that are confounding
and so on just disappear,
you just go straight to the solution.
So I think this is a very nice result
because the idea is you have a new measure
which allows you to look at the space of solution
in a different way and you can exploit this
for a delinquent purposes.
And now we are working to some generalization of this.
I want to mention few preliminary results.
And the idea is that this power Y,
which we have used to tune how much we want
the entropy to be high, say the local entropy to be high,
you can take it integer.
And this allows to create a lot of new algorithms
which are actually very simple that do not rely.
So this way you can solve the problem of having to use BP
to compute the local entropy.
So you take Y integer.
So the objective function you want to maximize,
if you take Y integer,
is just, you remember that you have N to the power Y.
So you take just N times, Y times the product.
So you have a sum.
So essentially what everything boils down to
is to have a couple system of Y real replicas,
which are constrained to be at a given overall distance
with respect to a vector
that doesn't have to satisfy any particular constraint.
So this would be your new objective function.
And you can even trace away your reference vector.
You can sum overall configuration.
And so you end up with this cost function,
with the objective function.
So essentially you can run your simulated
and in your Markov chain directly on this problem
with a certain choice of Y,
which can be three, four, five, so a finite number.
And it's quite interesting the fact
that this energy function depends
on the temperature of the original problem.
So somehow this is going to smooth the energy landscape.
And so there are many technicalities here involved.
Like, you know, first of all,
you can compute the update efficiently.
The Monte Carlo move have to be done in a smart way
in order to optimize the rejection rate
because you have to keep the distance relation.
So there are many, so it's quite involved,
but let's forget about all the details.
And so at the end what you have
is just a Monte Carlo process that maximize that function.
And to give you an example here,
we have the case of, we compare the case
in which we take just Y equal three,
so just three replicas.
And we compare the case of a standard simulated
annealing optimized in the best possible way
with respect to this replicated Markov chain.
And you see that the replicating Markov chain
just goes down to the solution,
doesn't suffer any slowing down.
So, and I think this is, I mean,
another interesting application is that, yeah.
It was still exposure.
No, no, no.
What's the point?
It's just, and actually it's pretty big for Neuronegant.
6400, it's a pretty big system.
And a nice thing is that,
given that you have a replicated system,
you can imagine to run BP on the replicated photograph.
Okay, again, now that we have accepted the idea
that our effective energy function
is this set of real replicas that have to be coupled.
So, a nice thing that comes out is that,
if you now write the BP equation on this replicated system
and you make the simplified assumption
that all the replicas give the same signal,
so you look at the subspace of the solution
of fixed point equation,
which corresponds to symmetry contribution
from all the replicated system,
what the message passing equation
boiled down to are the message passing equation
become essentially a variant of BP with reinforcement.
So, I think we should say the other way around,
that BP with reinforcement is a particular case
of BP run on this replicated graph.
Okay, and so one can choose the pattern.
So, here you see you have Y,
and if Y is equal to one, everything disappears
because we don't have replicas.
But, so this is the structure of the equation,
which is very similar to the structure
you would have with the reinforcement BP.
And so, if you choose properly the parameters,
essentially they behave the same.
So, finally we understand what BP with reinforcement at least,
we understand, is doing in the sense
it's actually looking for regions of a high local entropy.
And this, you know, and then another thing that you can do
is of course do just a gradient.
Again, we take, so we pseudo gradient,
we have integer variable, we make them continuous,
we take a derivative and then we go back to discrete,
so it's a very dirty, dirty way of doing gradient,
but anyhow, it's not so crucial.
And just the parameters which are important here,
this is eta is the learning rate,
so the coefficient of the gradient,
and this mt is the number of patterns
you use to compute the gradient.
So, if mt equals m equals, say m is total number of patterns,
then this is called batch gradient.
Otherwise, if this is the small subset of patterns,
this is called the mini batch.
This is just in the language of deep learning,
deep networks, okay?
But this is more or less what you would do naively.
But now, what you want to do is, in fact,
to do the same thing on the replicated Hamiltonian,
on the replicated energy function.
And again, you can trace away variables
and the reference vector.
And if you run this, quite some surprise,
at least for me, it's extremely efficient.
It's even better than BP with reinforcement.
So, this is, okay.
So, this would be the gradient for a certain,
so this is a small system with 1,600 weights.
This is a committee machine.
I forgot to say that all these experiments are done
not for a machine which is a bit better than the perceptron,
so it's a machine which is,
it's a neural network in which you have,
so K hidden units, any inputs,
and all the hidden units are fully connected, okay?
So, and then all these hidden units give a certain output.
So, this is the kind of architecture we are considering.
And here we have the W's here and here there are just one,
but this is already a pretty complicated object.
So, for this object here,
in which we take K equal five and N equal 1,600,
just to give you some numerics,
you see that the replicated gradient works very, very well
and the standard gradient stops working at,
small volume of alpha,
and even where it seems to work,
in fact, it takes a lot of time to converge,
so which means that if you go to larger N,
actually this curve is going to go to zero.
Okay, so, okay.
So, now we have been able to do the analytic calculation,
we are just in the process of writing up things,
and we have been able to do the analytical connection
of the local entropy for these multi-layer architectures,
and what comes out is that in fact,
also for these multi-layer architectures,
you do have these subspaces which are extremely dense.
And so, the conjecture is that the reason why,
do you mean multi-layer or two-layer?
Are you doing really?
No, well, two-layer, this architecture here.
Analytical calculation,
I don't think you can do wrong, and this was at least.
That's what I was using.
Yeah, no, no.
But this is very complicated already,
I mean, we took just a lot of days,
Carlo is fighting with numerics and so,
but this is nice, again, you see that you have this overlap
with the binomial coefficients,
so again, these are extremely dense regions.
And so, I think that we can somehow reach some conclusion
now that is somehow by using this measure,
you can turn our typical properties
into typical properties from the combination of perspective,
you have just to kind of forget about detail balance,
who cares if you are in algorithm,
I mean, you wouldn't care about the balance, right?
So, that's the idea.
This can be generalized also to finite temperature,
I think there are a lot of physical problems
which could be addressed by this kind of analysis,
in not necessarily question related to satisfying assignment,
but you know, other question related to physical properties.
And all this, and one thing which I find very interesting
is that we believe that, and we have some,
also we are studying this problem,
that this scenario also extends to the case
of continuous weights, because in the case
of continuous weights, what happens is that the space
of solution, which is a continuous space,
breaks into an exponential number of small domains,
and then this domain may aggregate according
to this kind of behavior, so that's the main conjecture.
And so, these are the conclusion that if you,
with the one RSV scheme, if you look at the event,
you might lose this kind of information,
either because it's hidden in the paramagnetic solution,
or for reasons that are not clear to me at the moment.
So, somehow you have to adapt the tools
and try to look at every event.
So, how general this is, I have no idea,
it could be that we have not yet studied
the finite connected problem, like KSAT,
or other problems like this,
but this is certainly something we want to do.
And it's clearly that, it's clear that this notion
of having high density region,
it's a way of defining accessible regions,
because you have a lot of solutions,
it's clear that if you have a region with a lot of solutions,
this should be accessible for stochastic process,
which, however, should not satisfy detailed balance,
because if they would, they would get trapped
in the Gibbs measure.
So, somehow they are accessible,
but not for detailed balance dynamics.
And also, we are very much interested
in computational neuroscience questions,
which are related to, you know,
to how to use this in neuromorphic devices
and also for learning problems and so on,
but these are very, so the message for this workshop
is, well, you know, there's a large radiation measure,
which is very powerful from the algorithm point of view.
There's a lot of work to do,
and maybe this generalizes to other cases,
and it could be interesting to think about.
Thanks.
Thank you.
Thank you.
Thank you.
Questions?
And where do you do this, in the beginning,
so you are at a higher energy?
And which one?
The replicated one or the standard one?
No, the standard one.
The one where you put the weight
proportional to the local entropy.
Yeah.
Okay?
So, you want to put this extra weight with the local entropy,
but since you are in a configuration,
which is non-zero energy,
are you looking around you for other configuration
of that same energy,
or you are counting solution around you,
but you are not a solution, so...
Oh, you don't care.
You don't need to be a solution if that is the question.
You don't have to put the constraints
that the reference vector is itself a solution.
You forget about that.
You just try to maximize the density of solution around you.
Then, you can compute an elliptic.
I didn't show this is a byproduct of the calculation.
The chance that you are a solution is actually very, very high.
So, at the end of the day,
you don't need to impose that constraint.
So, you just look at solution around you,
and that's it.
You don't care about the reference vector
being a solution.
And you approximate that entropy by dp?
Well, that's one way of doing it,
and another way of doing it
by doing the replicated version.
But, again, in the replicated,
the y replicas, none of them has zero energy,
because we are still in the initial part
of the single end meaning.
Yes.
So, it's contradictory that you have y,
say, three replicas, none of zero energy,
and you are able to count around you
how many configurations there are.
But this is...
I mean, it comes out from the calculation.
You take y integer, and you get this replicated.
I understand your point.
It's counterintuitive, but that's it.
How big is d in the calculation?
How big is d in the calculation?
d is over the n.
So, typically...
Okay, one thing that you do
is that you reduce d along your simulation
so that you get more and more confined.
At the beginning, you get it large,
and then you reduce d.
Actually, we use the conjugate parameter
to do overlap, so that's the main message.
Is there a dual approach?
I mean, it's sort of what people often do in practice,
where you just...
You have an under-constrained problem in sort of how
you can take the examples,
just perturb them a little bit.
So, each example, you perturb, you know,
having distance delta, you take many copies,
and then you solve the optimization problem.
Are you expecting to see the same kind of smooth things?
Maybe.
I don't...
I should say that, for instance,
yeah, we didn't think about...
For instance, there should be a straightforward
generalization to linear programming also,
but we didn't have time to do that.
And also, there is a paper by a group of Yanle Kuhn,
Yanlekar, who...
And what they are doing is they call it elastic gradient.
So, essentially, they do the same thing
in the continuous limit.
They just...
The argument is just say it's better to have
more samplers than just one.
That's the idea.
And they can show that for deep net,
it's actually working very well.
So, this is another argument, I think,
in favor of the fact that in the case of deep net,
essentially, one conjecture is that the landscape
could be such that there exists this local dense region
which have good generalization properties
and are accessible.
Yeah.
Your modification of the...
of the Gibbs measure created solutions
which are very good learning properties
but very poor storage properties
because you lost the basis of the production.
Because it's okay.
If they are accessible, there must be...
But storage...
Okay, I see.
But storage, I think that...
probably what you mean by storage,
you are thinking about an attractor network
that needs to be... to remain confined.
No, no, but this...
Here's the difference.
There's no dynamics here.
You give an input, it's enough that the output is correct.
So, it doesn't need to be attractive.
So, even if it is...
even if it doesn't have a gap,
it could still have good storage properties.
But it's not a good error correcting.
No, it's not a good error correcting. No.
So, can you take other systems which are error correctors
and do something similar to create...
Maybe...
I don't think that if you use parity check,
I guess this would not help very much
because parity check are...
my construction, you know, kind of isolated.
So, I think this is a...
I'm sure there are other systems for which all these properties hold,
but I wouldn't look for parity check
because I would rather look for, you know,
kind of an interpolation between parity and sum.
I don't know exactly, but I would...
I don't know.
But I wouldn't think that in the case of parity check,
this would lead to anything new
because there are really a point-like things.
So...
Let's thank Ricardo.
Thank you.
