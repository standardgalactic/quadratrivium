We should get started.
So, you know, let me just say that, you know,
we had a couple of colloquial last semester,
which were roughly on this topic of how do you architect,
you know, fault-tolerant qubits.
So we had a colloquium by Michel Devereux
from Yale on a Masonic air correction
and from Patrick Hayden on architecting a fault-tolerant qubit.
And so, you know, I view this colloquium today
by Jeff Thompson in the same spirit
of continuing this exploration.
And so I should say by way of introduction
that Jeff is, he got his PhD with Michel Lucan
and he actually did his post-doc there at MIT
and Harvard as well.
He works on experimental atomic physics
and he's currently a professor in electrical engineering
and computer engineering at Princeton.
But, you know, I think he's been thinking
in a very interesting innovative direction
about how you might, you know, do air correction more
robustly in atomic systems.
We are converting general errors to ratio errors.
So I'm really looking forward to this talk.
It should be quite exciting.
So Jeff, just turn it over to you.
Great, thank you very much.
Just my screen sharing going.
All right, so it's a real pleasure for me to be here
and I'm very grateful to Umesh for the invitation.
Let me, you know, say at the outset
that, you know, I'm an experimentalist
and I normally give a talk that focuses
on experimental work, but we have this kind
of more theoretical result recently
that I think is interesting
and is guiding the experiments we're doing now.
And Umesh really encouraged me to focus this talk on that.
So I'm a little bit out of my comfort talking,
you know, mostly about theory.
I'll discuss how exactly we're gonna implement this
in our experiments and a little bit of background there
towards the end of my talk.
But, you know, as a consequence,
I'll probably say some things that are,
I'll probably say a lot of things that are very obvious.
I'll probably say some things that are wrong
and I would love for, you know, which ideas
or which to be a topic of discussion at the end.
But an overview of what I'll tell you about is, you know,
basically start with the, you know, the claim
that engineering qubits, physical qubits
with favorable noise models can lead
to significant improvements in, you know,
either the fault tolerance threshold
or the overhead that you would need to do
while tolerant quantum computing.
And this is something people have been thinking about
for a while in a variety of contexts.
In this talk, what I'll tell you about is a kind of a way
of engineering a noise model for a neutral atom qubits,
which are, I would say an up and coming,
but, you know, very exciting qubit platform
that converts the dominant errors into erasure errors,
which I'll argue are very favorable
in terms of both threshold and overhead.
I think this is probably, I'll sort of belabor the definition
of erasure error and why it's beneficial
because I think probably these ideas can be extended
to other qubit systems.
We haven't exactly done that yet.
And so for most of my talk, I'll be talking
about this theory work in this paper here,
which I want to, you know, emphasize was a really fun
and unique collaboration with Shruti Puri
and her student, Yui Wu at Yale, you know,
experts in error correction and with another experimental
experiment on qubits who also sort of, you know,
was moonlighting as a theorist with me on this.
But it was really, you know, this is really a unique
collaboration to actually think, be able to think sort
of deeply about, you know, what happens
on the atomic physics side when errors occur,
but also what the consequences would be
on the error correction side.
So I would, I guess kind of describe in a nutshell,
what we're trying to do is cheat to win at error correction.
You know, there's many, many, you know, for a long time,
thought about the thresholds and things like that
for error correction in terms of the very idealized
and well, maybe not ideal, but abstract model of errors,
which is just depolarizing, so X, Y and Z errors,
you know, are equally likely and act on the qubits
in an independently random way.
But if you have a simpler noise model than that,
you can have much higher thresholds that are advantageous.
And I think probably the most discussed example of this
is biased noise where, for example, say Z errors
are much more likely than X or Y errors.
And this case has been pretty well studied
and kind of the, a good code that you can use
to take advantage of this is something called
the tailored surface code.
And it's been studied that for at least
in the kind of quantum memory context,
the threshold can actually be very high.
In fact, in the case of infinite bias,
the threshold would be 50% if you have this bias noise,
where it's only 18% or so, if you don't have any bias.
The Z errors are independent.
But there's actually two kinds of thresholds
that are relevant for when we talk about codes
and computing.
The first is what I'll call this quantum memory threshold,
where you imagine you have a code word with no errors
and then you sprinkle some errors in
if you get to do perfect, you know,
syndrome measurements and decoding operations.
So this, you know, it's with this kind of quantum memory
threshold, the surface code threshold for unbiased noise
and MWPM is usually stated as 11%.
Maybe ideally it's 18%, but it's in this context
that you get this 50% for bias noise.
But what is actually more relevant
for real world quantum computation,
but a little bit harder to compute
is what's called the circuit level threshold,
where instead of just considering errors sprinkled
into an ideal system that you have to then correct
that ideal operations, you actually consider
the circuit itself that you need
to go and measure these operations
and, you know, infer what errors happened
and then implement the corrections in a noisy mode.
And these circuit level thresholds
are probably always lower than these ideal memory thresholds.
And for example, for the surface code,
the standard surface code with unbiased noise,
this circuit level threshold is around one percent.
It's quite a bit lower.
But it's the circuit level threshold
that's really the thing that's relevant
for fault tolerant computing in a lab
because it's actually, you know, noisy operations
are actually the real world case that we have.
And actually extending bias noise to the circuit level
is a little bit tricky.
So there's a lot of physical qubits
that naturally have a bias towards one type of error
or another.
The rate of bit flips and phase flips
is often described as T1 and T2 for qubits.
And those are not necessarily the same
in most qubit systems.
But once you consider a circuit
where you put all these elements,
all these operations in,
those gate operations actually mix up the errors.
They convert Z errors into X errors and vice versa.
And so that undoes the bias at a circuit level.
So to maintain bias at the circuit level,
the one thing that helps you do it
is if you have something called a bias preserving gate,
which is essentially a native gate
that maintains the noise bias.
And one really well-known example of this,
and I would argue the only one
that's been really extensively studied
at the circuit level is these kind of cat qubits,
like the Kerr cat qubit, for example,
which my collaborator, Shruti Paria-Gail,
did a lot of pioneering work in.
And with these kinds of qubits,
it's, you know, there's a technique
that they proposed in this paper
for actually implementing a bias preserving CNOT gate
that maintains the bias of the noise
even through the CNOT gate.
And with that, they're able to get an increase
in the circuit level threshold
compared to the case that you don't have,
you don't have this bias noise
or you don't use bias preserving gates.
And this increase in threshold then means
that for a given physical error rate,
you obviously have much lower logical error rates.
But even though we know of a lot of qubits
that have this bias noise,
we actually don't know of many
where you can maintain that advantage at the circuit level.
So another way that you can cheat,
another type of noise model that's more favorable
than just decolarizing errors is erasure errors.
And by way of explaining what this is,
let me say that in general,
the reason you get an improvement
from these simplified noise models
is that you reduce when it comes time to decode
what errors have occurred,
you reduce the space of possibilities
that the decoder has to distinguish between.
And so if I had five qubits
and sort of generic to polarizing noise,
I have 15 errors I have to distinguish.
If I had bias noise
where I really only had z errors with any probability,
then I've reduced the space to just five errors.
But actually if I have the error,
if I somehow know independently
for my syndrome measurement
that it was qubit number two that had the error,
then I've reduced it now to three possibilities.
I have to fix the error on that qubit
to detect which one it was,
but I've reduced the space of possibilities.
And so when we know the location of an error
and can use it to facilitate decoding,
that's I think what's meant by an erasure error.
That's the essence of an erasure error.
Erasure errors are actually pretty common,
like classically, for example,
bad handwriting, you don't often read,
somebody tries to write an A,
you don't often read it as a J,
but instead you just say,
I can't read what that letter is.
And it's also actually erasure has been pretty well studied
in the quantum context,
in the context of photonic qubits,
because if I have a qubit encoded,
for example, in the polarization state or path of a photon,
then I might expect a click in this detector,
if it's in the V state,
a click in this detector, if it's H,
but if I don't get any clicks,
then that is a third measurement outcome
that's an erasure error tells you
there was an error on that qubit.
And so because of this interest in photons,
thresholds have been studied for erasure errors
and actually the surface code also as a quantum memory
gives you a 50% threshold
in the case that you have pure erasure errors,
it's studied 10 years ago.
So another way to understand
why these erasure errors are more beneficial
is that actually you just get a larger code distance
for the same code.
So in the depolarizing channel,
if I consider this repetition code,
distance three repetition code,
I can detect up to two errors,
but only correct one of them using majority vote.
But if I have this erasure error
where instead of zero flipping to one,
both zero and one become some other error state,
these actually error states don't even have to be the same,
then I can actually detect in a distance three code
up to three errors and I can correct up to two.
And so it's actually generic property
of any code, quantum or classical in fact,
that you can correct twice as many erasure errors
as depolarizing errors.
So this, if we could engineer physical qubits
that would have sort of a bias
towards erasure type errors instead of depolarizing errors,
we would expect them to be significantly easier to correct.
So the context in which I'm gonna discuss this
is neutral atom qubits.
And in atom qubits, and this also applies to ions,
to first order, there aren't errors, right?
These are basically non-interacting perfect particles
that sit floating in an ultra vacuum chamber
and the qubits might be encoded
and save a spin, two different spin levels,
sub-levels of the electronic ground state.
And to reasonable approximation,
these states can have essentially infinite coherence time,
but the challenge is that they don't,
in that state where they have this infinite coherence time,
they also don't interact with each other.
So there's no two qubit gates going on.
And this is a classic trade-off with qubits.
You have non-interacting qubits that live forever,
but then you can't do gates
or you have strongly interacting qubits
but then they decoherent.
So the thing that makes atoms and ions work really well,
then for quantum computing is that,
what you can do is use the fact
that there's more than just two levels in this atom
and you can transiently couple the qubit
to some other degree of freedom
that is strongly interacting
and use that coupling transiently
to implement a two qubit gate
and then turn it off when you're done.
So in the case of trapped ions,
this sort of degree of freedom
that you couple to transiently is the motion,
the collective motion of the ion chains,
the suroxylar gate.
And in neutral atoms, this degree of freedom
that you couple to transiently
is a highly excited state called a Rydberg state
that's very strongly interacting.
And I'll talk more about that on the next slide.
But the challenge is that this other degree of freedom
is typically, by virtue of being strongly interacting,
typically a lot more sensitive to environmental noise
and therefore much less coherent.
And so it gives rise to errors.
So the way errors come up, crop up mainly
in these atom and ion qubits
is actually during a two qubit gate
when you transiently couple to this much noisier degree of freedom.
And a consequence of this is that if you look at
no properties of atoms and ion qubits,
you'll see that the two qubit gate error rate
is typically a hundred to a thousand times worse
than single qubit errors.
So this is really the fundamental limitation.
That's not to say that this is the only limitation
and actually a lot of, when it comes down
to building these things,
there's a lot of other sources of error
like coherent errors, addressing,
and crosstalk errors, things like that.
But this, I think what I say here
is sort of true about the fundamental limitations.
So I want to, at this point, explain just a little bit to you
about the Rydberg states that give rise to interactions
that let us implement two qubit gates
because it's correcting errors from these Rydberg states
that's going to be the key idea.
So the energy levels of an atom,
kind of no matter what atom it is,
basically look like this.
They're far apart near the bottom
and then they get really close together
as you get to very high principal quantum number states again.
And so our qubits are encoded down here,
but these very highly excited states
are called Rydberg states.
And they have some universal properties that are important.
The first is that the size of the electron wave function
gets really big.
So a ground state might be this big,
but an N equals 50,
which is not even a very big principal quantum number
for us state is orders of magnitude bigger.
These can be almost micron-sized wave functions.
And because these wave functions are so big,
they're very polarizable, sort of fluffy.
And so that gives rise to a Van der Waals interaction
where if two of these atoms are close to each other,
they have an interaction and strength
grows as the 12th power of the principal quantum number.
So if you go from N equals five to 50,
you really switch on a very,
almost pathologically strong interaction.
And the lifetime of these states is not very long,
but gets longer as you go.
So all of these things are favorable for intermittent gates.
And to give you kind of a sense of scale,
if you have atoms that are a few,
few 10 microns apart in a pretty high principal quantum
number state, you can have interactions
that are of order hundreds of megahertz
and lifetimes that are hundreds of milliseconds.
So there's a favorable conditions
for implementing high fidelity gates.
So the way that we use this interaction
to implement a gate is by coupling our qubit state
into the Rydberg state.
So I have these two ground states where I might store a qubit,
to be the atom to be a zero state,
I can spin down or the one state spin up.
But then if I couple one of these states spin up
to the Rydberg state,
I can do something like do a two pi pulse
and pick up a patient, a minus one.
And that would let me do a single qubit gate.
But then if I have two atoms,
I have to consider their collective dynamics.
So if neither of them,
if they're both in the zero state, nothing happens.
If only one of them is in the one state,
then I can do a Rabi oscillation like this,
just as if I had a single atom.
But if they're both in the one state,
then I'm trying to excite them both simultaneously
to the Rydberg state.
But the challenge is that I can't do that actually,
because the doubly excited state
because of this very strong Rydberg interaction,
this Rydberg blockade is shifted way out of resonance
with my driving laser.
This shift is much larger than my driving laser.
So then I won't actually do a Rabi oscillation
all the way up here.
I'll just have this constrained dynamics
coupling from one, one to this symmetric W state.
And that'll happen with a different Rabi frequency.
So because the speed of my dynamics is different,
zero omega and two omega
in these different computational states,
I can pick the right kind of pulse
to implement a CZ gate,
essentially using these different dynamics.
And that's the basis of the Rydberg blockade gate.
That's how this interaction is used
to implement an effective two qubit gate
between the ground state states.
But the fundamental source of error,
the challenge that we have to deal with
is that this Rydberg state has a long
but not infinite lifetime,
maybe a couple of hundred microseconds.
And so during the time that it takes me to do this gate,
it has some probability of decay.
And that's gonna give an error.
And for kind of current parameters for these gates,
this is at the level of a one to a few percent error
per two qubit gate,
but an optimistic projection of where we could get
by making these lasers faster and so on
is maybe to about a 10th of a percent.
But this is sort of the fundamental limitation
to the ability.
Now, the thing that's nice about atomic qubits though
is that, you know, as a civilization,
we're very good at atomic physics.
We've been doing it for a long time.
And so we really understand the physics
of these things exactly
and also the physics of the air model exactly.
So by thinking about what energy levels we use,
we have some ability to shape and control
what happens when these ribbon states decay.
The conceptually simplest model to set up
would be one where when there's ribbon state decays,
it can come back down to either the zero state
or the one state.
And in that case, that would implement,
that would just give you a depolarizing error
every time you have this decay.
A clever thing that you could do
would actually be to choose your spin states here, zero and one,
such that maybe angular momentum selection rules
forbid the decay from R down to zero.
And so R is only allowed to decay to one.
And in that case,
you would actually have a better error model
than this depolarizing error because it would be biased.
And the sort of reduction of these ribbon decays
to biased errors is actually the basis of a very,
I think, you know, important for us,
important to set up how we thought about this problem paper
from Misha Lutkin's group on engineering error models
and ribbon kind of qubits.
So the, but kind of the key idea of our work then
is to try to take this engineering one step further.
And instead of using, and actually what we do,
instead of encoding qubits in the ground state,
we encode them in sub levels of a metastable state,
a state that doesn't have an infinite lifetime
like the ground state,
but still has a very, very long lifetime,
you know, to be seconds in practice.
But if we pick the right metastable state,
then we can have the property that when we excite
to the Rydberg state, if the Rydberg state decays,
those decays overwhelmingly actually do not come back
to this metastable state,
but they go down to the true ground state
because in general, that's where, you know,
Adams want to go when they decay.
And I guess that statement, you know,
that's what Adams want to do is not in line
with the level of sophistication that, you know,
I claim we have in understanding the atomic systems,
but it wants to go back to the real ground state.
And so if we then have a way to detect population
that has fallen down into this ground state
and to do it in a way that doesn't disturb the qubits
that are left up here,
then we've basically converted these Rydberg decays
into erasure errors.
And I want to point out that there's actually
a very analogous concept was proposed
in Trapped Ions by Wes Campbell.
This is a very nice paper.
So if we want to propose sort of a physical system
and would implement this,
there are two questions that are really critical.
The first question is, you know,
what fraction of the errors actually become erasures
like this?
What fraction can you detect?
This is equivalent to sort of asking
what the magnitude of the noise bias is
for bias poly noise errors.
And then the second question is, you know,
can you actually detect them efficiently
without disturbing the other qubits?
If when you try to measure whether there were any erasures,
you have some probability of inducing errors
on all these other qubits that didn't have errors,
which is hopefully most of them,
this might not actually be beneficial.
So the central result of our work is to kind of think
very carefully about the level structure
of specific atom, the terbium,
and to show with sort of detailed atomic as a calculations
that using this sort of triple P zero metastable state,
we can probably get to about a 98% fraction of errors
converted into erasures,
which is a very large bias towards erasures
and detect these erasures in a way
that has negligible back action onto the rest of the qubits,
the very efficient detection.
So in the kind of this, you know,
the rest of this theory part of my talk,
I'm gonna explain how I'm gonna take this result
as a given and explain to you what the benefits are.
And then in the last half of my talk,
I will actually talk more about the specific atomic physics
of a terbium and how it is that we're able to do this.
The picture that I have in my head actually
for these sort of using these metastable qubits
to detect errors is always somebody walking on a tightrope.
And this I guess starts from a notion
that maybe a quantum computation is a little bit,
like your qubits are a little bit like dancers,
they're doing a dance.
The dance is very intricate.
They have to do all the moves exactly right.
But the, you know, the challenge with error corrections
that you're actually not allowed to watch them do this dance.
You just have to look at where they are,
you know, at the beginning and where they are
at the end of the dance and infer
whether they did all the dance steps correctly.
So the notion of using this metastable state
is somehow instead to ask these dancers
to do their dance on a tightrope,
you know, way up 100 feet in the air.
And the benefit of doing it this way
is you don't actually have to watch the dancers
to know that they're doing the dance correctly.
All you have to do is look at the ground underneath the tightrope
and observe if any dancers are falling down.
And actually you don't even necessarily have to watch
in real time, you can just come periodically
and check to see whether any errors occur.
So by making the system sort of very unstable,
very fragile, you sort of make these errors easier to detect.
So in our work with UA and Shimon and Shruti,
we have this atomic physics model
and then the question is how do you actually incorporate this
into a circuit to see if you really get
a circuit level advantage from students.
And so we consider this specifically
in the context of the XZX surface code.
And with the sort of the simplified error model,
which I'll discuss extensions to at the end,
where we assume there's only error on two cubic gates
because those are dominant for these atomic platforms.
And we say every time you do a two cubic gate,
there's a poly error with probability Pp
and an erasure error with probability Pp.
And then the parameter that sort of governs
how well this works is the erasure fraction,
which is the fraction of all the errors
that are erasure errors.
And so in the circuit that measures the stabilizers,
there's a series of two cubic gates.
And after every two cubic gate,
we illuminate the array with some light
that would give off some fluorescence photons
if any atoms had fallen down to the ground
and had these erasure errors.
And although it's obviously not included
to be stabilizer simulations,
it's important that this is fast
and you can do it in parallel across the array,
but that's true for our scheme.
And so then most of the time, hopefully there's no error,
but then when there is an error,
we use sort of a movable optical tweezer
to replace atoms from a reservoir.
And what this does is it converts this error
that happened here,
and this atom has actually been lost
probably during our detection process,
but we know there was an error there
and we replace it with an atom.
So this atom still, with respect to the code,
it still has an error at some random state.
And then we're gonna use subsequent stabilizer measurements
to sort of project it onto a specific error and correct it.
But the important part is that
because we saw this thing happen here
and we know which atom we replaced,
we know the location in the circuit of this error
that we have to correct.
And the way, so then in this circuit level simulation,
we have to incorporate this information
about where the errors are into the decoder.
And this is conceptually pretty straightforward
using the kind of matching decoders
that you use from the surface.
So for normal depolarizing errors,
if I have a string of, say, bit flip errors,
then when I do my stabilizer measurements,
I'm gonna have two stabilizers
at the end of that chain light up.
And then the job of the matching decoder
is to pair these up and infer,
okay, there was a string of bit flip errors
in between there.
And if these errors happen near the boundary,
then I can pair it to the boundary too
because the boundary qubits only have one stabilizer.
But where the logical errors come from,
the lowest weight thing that gives rise to a logical error
is that if I have a string of bit flips
that go halfway or more across my code,
then for the matching decoder,
it's actually fundamentally unclear
whether it should kind of connect this to the left
or to the right,
and if it does it the wrong way,
there's gonna be a lot of error.
So with this erasure model,
because I saw at some point during my circuit evolution,
I saw a qubit four and qubit nine and qubit 73 lit up,
I already kind of know which qubits have errors.
So this makes the job of the matching decoder a lot easier
because if I now have this sort of D over two set of errors,
if it kind of has some reason to believe
that the qubits on this side all,
it's what this glowing orb is supposed to mean
that I knew that there was an error there,
it knows which way to match it.
And in fact, it not only can it do D over two,
but it can do D minus one errors.
And they don't actually all have to be erasures
if there's some errors that were sort of depolarizing errors
that I didn't see in there.
The knowing some amount of location information
still lets me disambigate.
So it's actually very natural and straightforward
to incorporate this information into the decoder,
which is actually done by adjusting the weights
in the decoder graph based on the locations of the errors.
So in our simulations,
which we actually did using the union find decoder,
we find the following that for this noise model
of two qubit gates for purity polarizing errors,
you get a threshold a little under 1%,
which is the sort of standard surface code number.
But then by converting 98% of these errors into erasures,
the threshold goes up by a factor of 4.4 to 4%.
So this is a very significant increase in the threshold,
which you can is sort of a benefit
that you can spend in two ways, so to speak.
You can either say for the same code distance
and same physical error rate,
I now have many orders of magnitude,
lower logical error probability.
Or if I have a target, say logical error rate,
I can achieve that for the same physical error rate
with a much smaller code.
So say factor of 4, factor of 16,
in the number of qubits that you need to use
is to reduce the overhead.
I think actually the other benefit
that maybe is the other threshold that changes
that might even be more important for near-term work
is I think we also talk about a pseudo threshold,
which is the threshold where your logical qubit
performs better than just a single physical qubit
if you try to use this line here is the PL equals P.
And actually this increases by a factor of 23
for a small code.
So this big increase in the pseudo threshold
may actually be really important
for just near-term demonstrations of an advantage
with fault-tolerant operation of a logical qubit
in the first place.
So in order to get this big gain though,
you need to actually convert a pretty large fraction
of your errors into erasure errors.
And we sort of looked at the threshold
as a function of that fraction.
We think we can get up here, but there's the whole curve.
In the limit to do that pure erasure errors,
you get to about 5.1% or something.
So that's the saturated limit.
This is kind of similar to the curve
that we saw for bias poly errors.
And you can think of an ADA sort of just like the bias
to a PX over PZ over PX as sort of related
to this erasure fraction like this.
And the behavior of this curve is actually pretty similar
to bias poly noise where you get about half the gain
that you're gonna get in threshold
once this bias is 10 or 90% here.
And by the time you get to a bias of 100,
which is 99% here, you've gotten most of the benefit
that you're gonna get.
I'll point out that within this same model
and assuming bias preserving gates,
we looked at bias noise and this erasure,
the threshold you would get in assuming only two cubic
gate errors in the XZX code for infinite bias
is about 3.7%.
So this erasure conversion performs comparably well
and maybe even a little bit better if this is really large.
In addition to getting this larger threshold,
there's another benefit which is that you get
a faster sub-threshold decrease in the logical error rate.
So the logical error rate below the threshold
in a code is related to P over the threshold
and then to an exponent, which is sometimes called
the error distance, it's the number of errors
that the code can decode.
And if we fix a code distance here, D plus five,
and increase this erasure fraction,
what you see is that these lines get steeper.
Basically the error rate decreases faster
below the threshold and if we renormalize everything
so that you can just look at the slopes,
you see that it actually smoothly goes from an exponent
of three to an exponent of five.
And then just extracting these exponents,
you can plot that right here.
So also if you have a very high erasure fraction,
not only do you have a much higher threshold,
but you also have this faster sub-threshold decrease.
So these are two kind of distinct
and complementary benefits.
I will point out that as long as this erasure fraction
is finite, there is some regime
of sufficiently low error probability
where you kind of go back to be dominated
by the small fraction of depolarizing or polyers
that are left and the slope does go back to D over two,
but you can't see it in this plot.
It's at quite a low physical error rate,
but the steeper slope doesn't continue forever.
So that's the end of I guess the first part of my talk
discussing basically the, I guess why you would want
to do this, what I have tried to explain is
that this erasure dominated noise model
can give you actually a high circuit level advantage
with the surface code that's comparable to or better
than what you can get with bias noise.
I think one question that is interesting
is whether this is generic to other codes
or just special to the surface code.
The fact that codes have a larger distance
for erasure errors is actually generic property of code.
So one would expect this is true.
So, you know, people are really excited about things
like LDPC codes and stuff like that.
These days, I wonder if this could give a benefit
for thresholds in that model as well.
Another thing that we have thought about a little bit,
but not discussed, it's not considered extensively
is that the fact that, you know,
you could also describe this erasure error
as a gate with heralded errors.
And in that case, if you think about like, you know,
resource or like magic state preparation
and fault tolerant quantum computing,
where you say prepare many noisy copies
of a resource state and then distill out a better one,
having a large bias towards erasure errors,
which is the same thing as saying you can herald
a lot of the errors may actually allow you
to do some kind of predistillation
where you throw out copies of the state
that had errors before you actually try to distill them.
And this might also give you a significant reduction
overhead that you need for doing universal fault tolerant
quantum, because this resource state preparation
is a very large fraction of the qubits involved
in a computation in these kind of resource estimates
that people have done for Shor's algorithm
or things like that in the practical regime.
So in the next part of my talk then,
what I'll do is dive a little bit deeper
into the physics of neutral atom computing
and also introduce that topic in general a little bit
and then how we use kind of a specific encoding
in a terbium to get this error model
with a large bias towards erasure errors.
So neutral atom quantum computing,
you know, in the kind of schematic diagram
that you need to have in a nutshell
is that there's a ultra high vacuum chamber
where an array of sort of tightly focused laser beams
called optical tweezers holds an array of atoms.
And each atom is one qubit, you know,
coated in the spin as we previously discussed.
You can, you know, you measure these atoms
by using fluorescence on a camera,
you control gates and the execution of the circuit
using light by focusing down laser beams
that drive particular transitions in the atom
onto this array.
And then the interactions between the atoms
are realized using these highly excited states
called ribbon states, but when they're in the ground state
they're very known there.
This field has, I think, you know,
really taken off in the last couple of years.
There's now at least 25 academic groups
and I put 25 plus here because I got tired of counting
but it could even be a larger number than this.
And also, you know, really exciting for me
is four companies at least working in this area
including, you know, Colquana, Adam Computing,
Clara and Pascal.
And one of the things that's really exciting
about this neutral atom technology is that
it kind of comes with at least some first level of scale
built into it.
We sort of, it's already pretty clear
how to make pretty big arrays of qubits.
This is from Misha Lukin's group
at 16 by 16 array of atoms, 256 qubits.
And the technology for going even, you know,
an order of magnitude or so higher
than this is already pretty clear
because there's just using photonics
for the addressing and control really gives you
a lot of parallel kind of control built in.
It's a little bit different from superconducting qubits.
For example, where you, you know, 2000 qubits
means 2000 or 4000 or 6000 wires.
For example.
So, yeah, so there's a couple of key ingredients
that go into this neutral atom computing.
One is that, you know, you can actually
not only make sort of arbitrary arrays of traps
but also move them around in real time.
And this is used to create, you know, defect freeze arrays
as the starting point for these computations
which you can do in 1D, 2D or even 3D.
But the fact that you can move these atoms around
is also important for what, for this, you know,
step in our erasure protocol
where you need to replace lost atoms
by moving new atoms into the tool.
So I already mentioned the properties of these
Rydberg states.
So I'll just remind you briefly
that these highly excited states
give you these strong interactions.
And the way in which we normally talk about this
interactions is the Rydberg blockade
where if I have an isolated atom,
I can just excite it to the Rydberg state.
But if I bring it close to another atom
that's already in the Rydberg state,
then that excitation is forbidden.
And so the way in which we,
the new point I want to make on this slide
is that the way we quantify this
is with something called a blockade radius
which is the separation between atoms
under which you cannot excite two of them.
And this is related to the principal quantum number
of your driving strength.
But for kind of typical parameters,
this blockade radius is, you know,
maybe about 10 microns.
Whereas spacing that you can engineer
between optical tweezers is something
maybe between two and five microns.
So this is a really, these length scales
work out really nicely with respect to each other
because on the one hand, you know,
this blockade radius isn't infinity
so you can actually implement gates
in parallel across your array.
I can excite an atom here to do a gate over here
and the same thing over here.
But at the same time, it's large enough
that it comfortably covers two, you know,
two or maybe even three lattice sites.
So you have some kind of flexible local connectivity.
And then to extend this blockade
to generating gates between qubits,
we just need a selective excitation
of one of the qubit states to the river states.
And I'm gonna kind of flip through this
because I already introduced this.
But then we point out that kind of the current record
fidelity is around the 98% level.
Like this was, you know, this level was first achieved
by Misha Lukin's group a few years ago,
but has now been matched by, I think,
at least two other groups
that maybe not in published data
and exciting unpublished work.
There are rumors about it.
And then it's actually an exciting week
for this field because, you know, last week in nature,
I should have updated both of these citations,
but these papers were both in nature last week.
Both Misha Lukin's group and Mark Safman's group
demonstrated, you know, what I would call
the first kind of, you know,
fully featured programmable processors
based on neutral atom qubits.
What's really interesting to me about these papers
is that they have very different approaches to do that,
which illustrates there's still a lot of room
for creativity and innovation in this neutral atom space
in the paper from Misha Lukin's group.
They implemented the programming,
not actually by focusing light onto different sites,
but by actually moving the atoms
to respect each other to change their connections.
So just bringing atoms close together
when they should do a two-to-two-legate,
moving them far apart when they should not.
Whereas in Mark Safman's approach,
they did the thing that was maybe originally envisioned,
which is actually focusing the station lasers down
and saying, okay, I'm gonna site this qubit,
then this one and this one,
and programming the circuit belt.
And the fact that groups like Misha's and Mark's
and also Antoine Brouet's,
who have been sort of pioneers in this neutral atom
and your Rydberg space,
are now doing such big and impressive circuits
with these qubits,
where these neutral atom qubits is,
I think really exciting for the field,
but has also kind of created the space in which,
in which the work that I'm telling you about here
makes sense where, you know,
now I would say the fundamental fact
that maybe neutral atoms are interesting
is really well-established by these guys.
So I would describe the work I'm telling you about here
as kind of going back to the drawing board
and thinking about what would generation two
neutral atom qubits look like.
That work wouldn't be possible without, you know,
generation one being pre-affirmly established.
Great, quick question, Jeff, about these two results
that you just mentioned.
So are the gates applied sequentially or in parallel?
On these gates?
In Misha's case, they are applied in parallel
to some extent.
So there's, but I think they're basically applied in parallel.
So there's a two qubit gate happening here
and here at the same time.
And then the connectivity switches
and then the gates are done in parallel.
In with this scheme,
they certainly could be done in parallel
if the addressing system allows it.
I don't actually know if they were, but Mark is here.
I can just say that two qubit gates were done sequentially.
There are parallel one qubit gate operations.
But I think the fact that they were not done in parallel
is a question of not of physics,
but of we're not a physics of neutral atom qubits,
but of physics of these modulators here.
Okay, so for our kind of, you know,
attempt to find second generation qubits, you know,
we went back to the drawing board
and the drawing board for atomic physicists
is the periodic table.
All the experiments I showed you,
and in fact, most of the experiments
that have been done with atoms of any kind,
you know, VECs, you know, whatever,
have been done with alkali atoms
from the first row of the periodic table
because they look like hydrogen and hydrogens,
you know, we understand it very well.
But kind of the wild west, so to speak,
or maybe this is like, we're like Chicago actually now,
but the Midwest for atom qubits
is now these alkaline earth atoms,
which have two electrons,
they look a little bit like helium,
and they have just a slightly more complicated
level structure that gives you a lot of additional features
that you can use.
And so the work in my lab that we've been doing
is using a terbium, where the particular reason
we're drawn to a terbium out of these
is it's the only one with a spin, one half nucleus,
which is really natural weight in qubits.
There's also been a lot of really exciting work
in strontium, which is maybe the better developed one
of these and really exciting work on quantum simulation,
kind of many body dynamics, and also a top box.
It is, however, very difficult to include qubits
in strontium because the only nuclear spin
is a very large spin.
And so it's difficult to isolate two levels.
We, I kind of started working on this
about the same time as Adam Kaufman and Mano Andres
at Gila and Caltech respectively.
And I think this field of using,
looking at alkaline earth atoms
for neutral atom computing is now also taking off,
you know, in its own right,
sort of mirror the trajectory of neutral atoms as a whole.
And there's, you know, at least a dozen or so experiments
around the world, you know, also being built now
using these atomic species.
I don't want to be too technical,
but let me just give you kind of an overview
of what we, features we get from this level structure,
the fact that there's two electrons,
gives you singlet and triplet states,
and now you have allowed transitions to serving spin
and we transitions flipping the spin,
which is really advantageous for laser cooling.
In the ground state, there's no electronic spin.
So nuclear spin, the qubit, if you have a nuclear spin
should have essentially internet coherence.
There's also this highly forbidden optical transition,
which is the basis of optical atomic flux,
which I think will someday redefine the SI second,
but for our purpose just gives you another way
to encode a qubit.
And in fact, you can use this nuclear spin
in this, in this metastable qubit as well.
In this metastable electronic state as well,
so there's no electronic angular momentum.
And then the other feature is that within this
aterbium atom, we have an aterbium ion inside the core
and manipulating the optical transitions of that ion
within the Rydberg atom has actually turned out
to be maybe the most useful feature
that's particularly important
for these error correction items.
So I don't claim to have explained all of these things
in enough detail to understand them,
especially maybe for this more theoretically inclined audience,
but maybe the metaphor that I wanna have here
is that this is our toolkit
and these alkaline earth atoms are really like a Swiss army
where they have all kinds of weird
and potentially dangerous appendages on them
and what we've been doing for the last few years
is basically trying to figure out
if we can do anything useful with any of this error
correction work is one thing in that category.
So over the last few years in my group,
we've kind of learned how to cool and trap
aterbium atoms and optical teasers,
measured, done a lot of I would say foundational work
like measuring where the Rydberg states are
and verifying that they have strong interactions
which were not really known before our work.
And then in very recent work,
we've actually implemented finally a full set
of qubit operations in these atoms
and actually our paper demonstrating
a universal set of gate operations
in the one that we want to terbium
just came out in PRX a couple of hours ago,
accompanied by a paper from Adam Kaufman's group
with doing very similar work in the same species.
So just to kind of flash up the fact that these qubits work,
we do sort of have a pretty typical set of tricks
for initializing the qubit into one of these spin states,
for example, using optical pumping.
And then we use in our case a global RF field
to drive Rabi oscillations to do single qubit gate rotations
between these and get nice oscillations.
In our work, this is done globally.
We have only global addressing
for single qubit gates at the moment using RF,
but Adam Kaufman's group has demonstrated
optical Framon gates also with global addressing
but the path to implementing them on a single site
is clear using their approach.
And the single qubit gates work well,
the single qubit randomized benchmarking
fidelity is high, which it should be
for just doing Rabi oscillations on a spin.
We implement two qubit gates using the approach
that I described where we excite them
to the Rydberg state.
The thing that's a little bit funny for the experts
in the audience about how we do these Rydberg gates
is that the selective coupling of just one of the qubit levels
to the Rydberg state is achieved actually
not using selection rules or energy splitting
in the ground states, which are almost degenerate,
but actually using your Zaymon splitting
in the excited state.
So the reason only one goes to the Rydberg state
is because of the splitting up here.
And we use this gate protocol
that from Misha Lukens group that I described earlier
to implement our two qubit gates
and we get reasonably okay performance
for the very first demonstration of a two qubit gate
in an alkaline refat, it's about 83%
two qubit fidelity after reduced spam correction.
We understand this to be limited
primarily by technical noise or laser noise.
Our laser system was not especially optimized
to do this particular gate,
but it's sort of an existence proof
that you at least can do gates in this atom.
So what we're working on now is
promoting instead of encoding qubits in this ground state
encoding qubits in this metastable state,
part of the motivation for doing that
is this error correction work I described.
But the other reason that we were already working on this
is actually that this is just a better state
to do gates from in the first place
because we can get much faster excitation
to the Rydberg state starting from this state
using only a single photon transition
instead of having to use a two photon transition
for the ground state.
So this is already a really good idea
from just from the perspective of this atomic physics.
And this, so, and we actually think
we should be able to do our gates, you know
maybe somewhere between 10 and 20 times faster
starting from this state,
which should actually give us our 10 to 20 squared
times improvement in technical noise.
Which is what we're counting on to get these gates better.
Using the metastable state also actually lets you do
mid-circuit measurement very well
in a way that I won't talk about today.
So having introduced kind of a little bit
about these neutral atom qubits
and in particular a terbium then let me tell you
how we, in the context of the specific energy levels here
can implement this error model
or a qubit encoding that gives rise to this error model
where we have primarily detectable ratio errors.
So our qubit, I guess in an abstract sense
as I drew earlier you have two ground states
that store your qubit and then the Rydberg state decays
and gives rise to errors.
But now we use a metastable state
not the ground state to encode this qubit.
And it's the triplet P zero state
which in a terbium has a lifetime of 20 seconds.
So it's a very long one state even though it's not infinite.
So now let's kind of account for where the errors
from this state go.
So it turns out and these numbers come from just
you know atomic physics, the matrix elements
that we pretty much know for a terbium.
34% of the errors are actually gonna come back down
to the real atomic ground state.
And when they do we can detect them by using fluorescence
on this you know singlet S zero to singlet P one transition
which is a closed cycling transition
as negligible probability of decaying
into these qubit states.
And also this light is not resonant
with any transitions starting from this state.
So applying this light is both not gonna put these qubits
back into the computational space
but is also not gonna kick any more qubits
out of the computational space.
So being able to efficiently detect these
converts these decays into erasure errors.
Another big chunk almost two thirds of the sort of decays
from this Redberg state aren't actually really spontaneous
decays at all but their black body photon induced transitions
to other Redberg states up to these very high principle
quantum numbers.
You have kind of microwave frequency transitions
between states and those can be driven by thermal radiation.
And these we can detect by converting them after a gate
we can apply a pulse of light that's resonant
with the Etterbian plus transition
inside the core of this atom.
And that through a process called auto ionization
within like 100 picoseconds creates an Etterbian plus ion.
And that Etterbian plus ion we can also detect
with fluorescence on its cycling transition
which also is not gonna disturb qubits in this state
because this light is also very far detuned
from transitions.
These ions are not trapped.
So it's a little bit harder to detect them
but we think with very realistic parameters
this can be done with at least 99% fidelity.
And then these also become erasured errors.
I'll also point out that this auto ionization process
doesn't actually care about which Redberg state you're in
it detects both these other Redberg states
that can be populated by black body transitions
but also this original Redberg state here.
So you also catch any leakage from the gate
into this state with this approach.
So then in the end only 5% of the errors decayed
back to this computational space
where they could cause qubit errors.
So basically with this encoding
and given the way these energy levels
or these matrix elements work out in Etterbian
we've engineered a qubit that has a really high fraction
of errors it's these erasured errors.
The reason this works well maybe,
or sorry, so to understand kind of quantitatively
how this, you know where these numbers come from
I've first plotted here just the branching ratios
of where it decays from a particular Redberg state
I think it's a 75S, 3S1
and what you see is that you know about most of the errors
about two thirds of the errors are black body
or one third are radiative
but among these radiative errors various selection rules
tend to favor higher J states and whatnot such
that actually the fraction of these decays
that go back to the qubit subspace is 3P0
is just very small.
This is just a combination of all these selection rules
and angular momentum tendencies.
But actually there's another, so it's only five
if once you include the multi photon decays
of these states it's 5% they come back to 3P0
but it actually turns out that if those decays happen
in the middle of the gate they have a relatively high
probability 70% of being re-excited to the Redberg state
by the rest of the gate because our laser is still on
and those re-excited qubits also are converted
into ions at the end of the tech decalations.
So when you kind of multiply this 5% and 70% together
then it actually in the end is only 2% of errors
that end up back in the qubit space
or another way is that 98% of the decays become erasures.
Sorry, can I ask you a naive question here?
Yes, please.
So is there one relevant number or two?
One relevant number seems to be what percentage
go back to the qubit space?
And then is there a different number
which is what percentage do you actually detect
and is there a middle number
and how do you think of that one?
Yeah, so I would say there's what percentage go back
to the qubit space and what percentage don't
and those add up to one.
And but then the ones that don't go back to the qubit space
are they're definitely kind of lost from your computation.
So then if you can detect them it's very beneficial
that they're these erasures,
but it's not necessarily the case
that you detect all of them, right?
So there's a question, what is the detection fidelity
for those atoms that leave the qubit space
when there's an error?
And we think that, I mean, detecting atoms,
like single atoms are actually quite bright
in terms of fluorescence.
So for these ground state atoms we're quite confident
that we can do this at the three ninth level.
For these ions, I think two nines
is a more realistic estimate.
So there is a question which we have not actually analyzed
in detail of what is the impact
of missing a small fraction of these events?
That is a separate number.
I'm sorry, just following up on that a little bit.
So the ones that were lost,
did not go back to the computational space,
but and you did not detect them.
So now there would be a missing qubit there.
So how does that affect the subsequent computation?
Yeah, so I mean, and atoms can get lost for other reasons too.
They can get knocked out of the trap by background gas
or they can just fall out of the trap for other reasons.
So it's not an error that's of a kind
that's fundamentally distinct from errors
that you would have to deal with
to be rigorously fault tolerant anyways.
So there are, but this is more like
kind of a classic leakage error.
So I think there's a lot of proposals.
I mean, Mark has a review paper from 2016
where he proposes this sort of knock-knock protocol
from Preskill to deal with this.
I think there are actually probably more clever
schemes to deal with this more efficiently
like in the surface code.
And I think at the end of our paper,
we cite a paper from Sutara, Cross, and Gambetta
from IBM on this sort of quick leak introduction circuit
that actually seems like it should work
with pretty minimal impact on the overhead
where you basically swap the data
and the Ancilla qubits after every cycle
to try to detect, since you detect the Ancilla
every time you can tell if it's missing.
But we have not analyzed that quantitatively,
but I think it's not gonna be a big deal if it's rare.
There's also a question in the chat from Liang.
Do you want to ask that directly, Liang?
Yeah.
Yeah, so the question is, will there be back-action error
even if there's no leakage events?
So what do you mean by back-action?
So it's like, if you don't protect a leakage,
it's more likely it will be in the zero state.
So that's on my, I think that comes up in one slide.
Okay, thanks.
So, yeah.
So, on this slide, exactly.
So, and then, so, you know, this 98%
comes from some kind of analytic calculation,
but to sort of validate this understanding,
we also did master equation simulations
of the full two qubit gate.
And what we see is that the sort of the total,
you know, the average gate fidelity gets better
as you do your gates faster and faster.
But then if you condition on
not only the total number of qubits,
but also the total number of qubits
but then if you condition on not detecting
one of these leakage events,
then it's better by a factor of 50 still.
And this 50 is this 98% cast in the inverse.
It's one over 1 over 2%.
So you get a big conditional improvement in the fidelity
by not having an erasure error,
but there's still some small probability of an error.
But this actually consists of two parts.
One, you know, one is the error from decays,
you know, back down at a qubit subspace,
which can cause, you know,
reveals information to the environment
about what the state of the qubit is now.
But also the fact that you didn't have an error
gives you this kind of no jump evolution,
which is why I think you were asking about the end,
that even if you keep running gates over and over
and you never see any erasure errors,
that eventually tells you that your qubits
have to be in the zero state,
because if the one state is the only one
that goes to the river state,
there should have been an error eventually.
In terms of this average gate fidelity,
you know, this is quadratic with the error probability.
So if the regime you want to be in,
it's conditionally very small.
But it's an interesting question of how, whether this,
because it's a coherent error,
whether it actually really, you know,
whether this average gate fidelity that's quadratic
is actually the right measure
for thinking about the error correcting codes.
And that's something I'd be happy to discuss later.
The other thing that's important though,
in the simulation is there's actually different kinds
of signals you can get for detecting errors.
The atoms can, you know,
one can go down to the ground state,
one can become an ion,
they can both go down to the ground state.
And we can detect most of those,
but if both atoms in a two cubic gate
become ions at the end of the gate,
then that we actually can't detect,
because if you create those two ions
within a few microns of each other,
they just blow each other,
you know, the repulsive force between them is so strong
that they'll leave before we can detect.
But one thing that was kind of a nice surprise,
I can't really say that we designed it,
is that the probability of creating the kind of error
that gives you two ions
is very strongly suppressed by the Rydberg blockade.
It's actually an even stronger blockade
than the normal blockade
that suppresses those types of events from happening.
So there, the probability of those things happening
is suppressed by, you know,
many four or five orders of magnitude compared to,
compared to the errors that were mainly worried
about detecting.
So that was a pleasant surprise.
So I have only one more point I wanna make,
which is that, you know,
all of this discussion so far has been
about only two-cubit errors,
and only two-cubit errors arising
from the finite lifetime of the Rydberg decay.
And I don't think this is like completely irrelevant
because that is the only fundamental limitation
of the deli cruise.
But there are other errors that, you know,
by, for sort of technical reason,
which can be as insurmountable as physics in some cases,
there's other errors that happen and what about those?
You know, what's our budget for those?
Or do we have to worry about those eroding our advantage?
And so the one of these,
the one type of error that we studied in our paper
is initialization and measurement errors.
And we have a pretty good handle, you know,
in our system and in other neutral atom systems,
this can be done at the sort of 99% level.
And just by inserting those
into our circuit level simulations,
what we found is that they have actually
pretty minimal impact.
The, you know, the solid lines show the kind of
the logical error rate for 0%,
for no initialization measurement errors
and the dashed lines are half a percent,
which is reflective of some state of the art value.
And these don't matter very much.
And that's consistent with other simulations
of the surface code that, you know,
those kinds of errors are way less important
than two-cubit k errors.
But there's other types of errors
that we haven't considered in detail,
but I want to just kind of lay out
or thinking about them, maybe spark some conversations.
The first is that, you know,
the new era that we've introduced
by going to this metastable state
is that we have to worry about
the finite lifetime of that state.
You know, it can decay.
We, in the ground state,
we would have had an almost infinitely lit cube
and now we have this finite lifetime.
This one, I really don't think matters very much
because, you know, what happens when it decays,
it goes to the ground state and that's an erasure error.
So I think to the extent that these erasure,
and it's just a single-cubit erasure error,
and we'll detect it with our normal protocols.
So this one is, I think, is not going to matter at all.
There's also an issue of doing, you know,
locally-addressed single-cubit gates
using Raman transitions, the level diagram
looks similar to these Rydberg states,
but you're not going to very highly excited states.
And here, I think it's likely that you'll have essentially
a similar fraction of conversion into erasure errors
from these, because the selection rules coming
from these intermediate excited states
are similar to the Rydberg states.
But the one that's more important is coherent errors.
So, you know, lasers are noisy,
atoms move around and that gives rise to Doppler shifts.
And this can cause problems for both single and two-cubit gates.
And these are actually in current implementations,
you know, among the dominant errors.
And I think the same thing actually applies for ion traps.
And here, the vision that we have for how to handle these,
which is sort of an open, you know,
research question for us right now,
theoretically and experimentally,
is to use a technique known as, you know,
composite pulse sequences to drive these gates.
And the idea is that instead of, say,
doing a single two-pi pulse to the Rydberg state and back,
you actually do a series of maybe pi pulses
or pi over three pulses or maybe some optimal control sequence,
optimal control drive sequence,
which executes some more complicated and longer trajectory.
You spend more time in this Rydberg state on average,
but does it in a way that the fidelity of the final gate
is robust to, like, a higher order
against errors in the parameters,
like, for example, the Rabi frequency of the laser
or the detuning of the laser.
And so what these composite pulse sequences let you do
is trade robustness against rotation errors
for a longer sequence.
Basically, trade, you know, coherent errors
for spontaneous errors,
which would arise from the longer sequence.
So actually in the, you know,
my understanding is that in trapped ion cubits,
there's been a lot of work on these kinds of gates,
but they have not actually realized
a significant benefit in practice
because the fact that you make these things longer
increases the gate error rate
from emotional heating and things like that.
So it ends up being a wash, basically,
whether you do this or not.
But in our case,
because we have this conversion of spontaneous decay errors
from the Rydberg state into erasure errors,
we would actually much prefer
to trade rotation errors for that.
So I think this might create a new perspective
and a new argument for the use of these kinds of pulse.
So we're actually trying to understand
how well these work for Rydberg states, not.
But maybe the final thought here is that, you know,
you actually have to,
if you want to have this very advantageous noise model,
you have to consider not only the error you want to,
but actually all the operations
and think carefully about them
to maintain the significant bias towards erasure errors.
So I guess maybe have two very thin concluding remarks here.
The first is that, you know,
the fact that this works in this atomic species,
171 of terbium,
actually kind of involves a number of what appear to be
miracles about, you know,
how these atomic properties work out,
which is really unique and nice.
And the second is that this project
that we embarked on with Shruti of really trying to code
and Shmoen and Yue of really trying to co-design,
you know, physical qubit encoding
and an error correction code was actually very difficult
at first to really learn to speak each other's language,
but I think was very productive in the end
and very exciting.
And I think that this kind of work is, you know,
the system level thinking is something
that I hope can be reproduced, you know, across the field.
So with that, I'd like to then,
I thank the Ethereum collaborators at the beginning.
I'd like to thank my students
who actually did this experimental work here
at the end of our talk.
Here's a picture of a neutral atom quantum computer.
And then also thank you for your attention.
Oh, great.
Thank you, Jeff.
That was a beautiful talk.
It's a real, for speaking of the tradition,
it was a real window into, you know,
the experimental way of thinking about things
as well as really beautiful ideas.
So let's see if there are any other quick questions
before we move on to the panel.
Hi, Manuel.
Hi, Jeff.
Beautiful talk, really good results.
I'm wondering if there's nothing that prevents me
from doing this in analog quantum simulation as well, right?
I mean, I could save up just an encoding
from a meta stable to a Rydberg state
detect these kinds of errors the same way
and try to enhance my fidelity by just post-selection.
Yes, I think post-selection would definitely work for you.
I think the challenge, the thing that's a little more favorable
about circuit model computing is that, you know,
we bring all of our population back down
from the Rydberg state periodically during the computation
and that gives us a chance to create
and clear out these ions.
I think at least in these kind of quench experiments
where you're doing now, you wanna have a large circuit depth
but that's kind of continuous evolution, you know,
including the Rydberg states.
And if you make any ions or anything like that
in the middle, you're totally screwed.
But post-selection at least, you could figure out
after the fact if it happened.
So like a heralded thing you could definitely do.
Yeah, I wonder about similar questions for coherent errors
because I mean, some of the laser errors
you can classically detect, I believe, right?
Say if you have amplitude noise, specifically for Rydberg
say intensity noise is extremely hard to stabilize
because it's so fast.
But say you have extremely good phototeid,
you know what it is, right?
So you could post-select like that and detect it.
Maybe you could also select it with like a bunch of kind
of Sula atoms that will see it or something like this.
I think there might be something about it.
Might not be 100% maybe you get another 10 or 20% like that.
I think those are really interesting ideas, yeah.
Umesh, I think you're muted.
Sorry, Steve, you have a question.
Hi, yeah, thanks for the wonderful talk.
So you did your work with the XCZX code.
Do you know if there's any real reason
why that's a good code to use in the context of erasure
or if other codes might perform similarly?
I have asked that question to Shruti as well.
And I think the answer is no.
There's no reason to believe that it would work better.
I think they were already working with it
in the context of their biosomal stuff
and had all the infrastructure set up.
Thanks.
Are there, is there any other question
before we move on to the panel?
Okay, so Jeff, if you could stop sharing your slides
then we could maybe get the panelist videos pinned.
Okay, in the meantime, there's a question in the chat.
Is there any benefit of continuous illumination
where errors would appear stochastically, but immediately?
I see, so if I, instead of pulsing on the illumination
to detect these errors at the end of a gate,
I keep it on continuously to try to detect
exactly the moment the gate that it occurs.
So I think that's a good question.
Exactly the moment the gate that it occurs.
It's hard for me to think about what the benefit
of that would be because I think at the end of the day,
the qubit is depolarized basically.
I mean, you could maybe argue that if you saw
that the error happened in the first 10% of a two qubit gate
then you sort of know that the error probability
is very small, but I don't,
that's a very small amount of additional information
and I don't know.
It's an interesting idea though.
Okay, great.
Can we get the panelists videos?
Okay, perfect.
So let me start introducing the panelists.
We have a really fantastic panel today.
We have Ken Drang from Duke University.
He's actually a professor in ECE physics and chemistry
and he works on quantum computer architecture
on quantum control and of course,
quantum error correcting code survey
appropriate for this particular topic.
There's Ignacio Sirac who is at the Max Planck Institute.
Ignacio is a theorist who works broadly on everything
you probably had seen in your own panels before,
but for our particular talk today,
I think what's interesting is that many of the original
proposals, experimental proposals for experimental
quantum computing came from Ignacio in his work
with Peter Zoller.
So great to have you here.
We have Liang Zhang who's on the faculty at Chicago.
He's a theorist and he works on sensing,
communication, quantum computation,
but with again, a special emphasis on quantum error
correction, which is so welcome, Liang.
And then Mark Sethman from Wisconsin
who is an experimental atomic physicist and Jeff
already spoke about how important his experiments are
to setting up the foundations of what Jeff
has just been talking about.
So it would be good to start with the reactions
of the panelists to Jeff's talk.
Maybe in some order, maybe Ignacio, do you want to start?
Yes, of course.
Well, first of all, thank you very much
for this beautiful talk.
It was, I think, very clear for everybody, physicists,
computer scientists, at this part of the talk.
And I think it's a very nice idea, a very good idea.
And I think that it would be needed at the moment.
So it's a method to trying to scale up our systems
and reduce the errors.
And in that sense, I mean, the way I understood it,
I think it's a good idea that has a practical application.
I mean, I have some questions, maybe I'm not so urgent.
Don't know if I should ask them now or later on.
Yes, please, please do.
Yeah, so my question is about the time scales.
So somehow, maybe I'm not up to date,
but I thought the Rittberg gates are very interesting
because they can be very fast as compared to some other gates,
like collisions.
They have other advantages, but that's one of them.
And however, I guess, when you start to measure and do
the measurements and wait until you have the result in order
to act, and in particular, in the case
that you would lose the atoms and they go away,
you have to replace them, then this will take some time.
And so I'm wondering to what extent
this diminishes some of the advantages that the Rittbergs
have or if, I mean, it's something that is independent.
Should I answer that?
Yeah, so yeah, that's a good question.
And in fact, when we showed a draft of this paper,
it turns to Mark's up, and that was also his first question.
And as a result, I think it's like supplementary section S27
or whatever has a little bit of a sketch of this.
But I think the essence of it is, though, that the Rittberg gates
are very fast, maybe a few hundred nanoseconds.
But this erasure detection, because you're not
trying to keep the atoms, we do this detection in a source.
We just blast the atoms in the resonant light.
That actually can also be very fast,
just at the level of a few microseconds,
is how long that would take, because these
are strong transitions.
That's very different from the current state of the art
from, say, doing a measurement of an ancillicubit
where you want to measure what its spin state is,
but you also want to keep the atom,
so you can reuse that ancillicubit.
In current implementations, that's, depending on the system,
somewhere between like 20 and 100 milliseconds.
So that's actually very slow orders of magnitude.
So what we work out in the supplement to our paper
is that if when you get to measure the ancillis, which
you do less frequently, destabilizer measurements,
you have to do this slow readout with current technology,
then all this erasure stop isn't going
to change the cycle time at all.
And in order for it to become the limit to the cycle time,
you would have to speed up that ancillic measurement
by about two orders of magnitude.
So this is a known problem, and people are working on it.
The other operation in here that's not that fast
is replacing the atoms.
But I think that's not going to turn out
to be a big limitation because you don't
have to do it right away.
In the circuit that I drew, we did it right away,
but there's certainly no point in doing it
until at least you finish the next round of stabilizer
measurements.
So you can actually batch a number of these atom replacements
indeed at the same time.
And the work of Misha and others has shown that parallel atom
moves are essentially free compared to single atom
moves because it's just the acceleration time
of some whole pattern.
So I'm also pretty optimistic that that's not
going to affect the operation time.
OK, thanks.
Thanks, Ignacio.
So who would like to go next?
I can go next.
Oh, great.
Mark.
So Jeff, thanks for the great talk.
I think this is the second time I
heard you give this talk, and it was even better
on the second time.
So I enjoyed hearing you go through it again.
Actually, I want to say I think one of the aspects
that you didn't emphasize, although in fact is implicit
in this architecture, which is very valuable,
is that you can recool individual atoms.
Could be the atoms you replaced or any other atom
without affecting other atoms in the circuit in a way that
is really very difficult in other elements.
Because you can take your triple P0 atom,
put it down to the ground state, and cool it nicely
without affecting any other qubits.
And that's actually a very important feature.
Thinking about cooling, I would also mention that I agree
with your rough estimates about the impact on cycle time
from the atom replacement.
There probably has to be some amount of recooling included
in that that may add to your time budget.
But these are details that will be developed
and need to be worked out over time.
Yeah, I mean, I would just say overall,
I find it a very interesting proposal.
I'm still trying to find the missing, the hole in it
that hasn't been discovered yet.
But it does look very promising for further study.
Let me stop there for now.
OK, thanks, Mark.
So can I add a comment?
I think this is a really nice work.
And also it comes timely as a kind of a co-design
in terms of the hardware and also efficient error correction.
And I think what's kind of impressive is just not
like you identified the error, but also you kind of engineer
and try to convert the error into a really more favorable
erasure error so that you can actually significantly
boost the performance in terms of using more favorable error
correction.
So yeah, really nice result.
Great, thank you.
Thank you, Dan.
Ken, it's all yours.
Yeah.
Well, Leong took one of my main talking points.
But I think the conversion duration is amazing.
And I think other platforms should take the time
to think about how to do it.
Because I don't think, yeah, I think the community is a little
bit missed the fact how much better erasures are for error
correction if we can reinitialize and replace.
The second thing I'm super excited about
is the part with composite pulses.
So it's a quick, I don't know, minor correction,
which is like the one qubit composite pulses we use all
the time.
But two qubit called positive pulses and ion traps.
Yeah, we do suffer from this heating rate.
And I think if you could demonstrate the kind of two
qubit composite pulse, which is like one qubit composite pulse
related to kind of using type interaction suggested
by Jonathan Jones a long time ago,
it could be the first system where it actually helps.
And I think that in itself would be pretty cool.
So should I think that the amount of erasure
just goes like time?
And do you have a sense of what the trade-off
should be between kind of over rotation angle
versus extra erasure?
Yes, well, first of all, thank you
for making that point about two qubit data errors.
I was thinking about two qubit data errors.
But yes, that's true.
The trade-off, so I would think about it this way.
So if I had, if I was not thinking about erasure conversion
and I was just, or types of errors
and I was just generally concerned about the error rate,
I could have the situation where I had, let's say,
50% of my error came from an over or an under rotation
and 50% came from spontaneous to change.
And in that case, if what my composite pulse let me do
was, let's say my error probability is small,
so my composite pulse is going to suppress that coherent part,
basically, to zero.
But at the expense of doubling the length of my gate,
so I now double the spontaneous part.
In many contexts, that's the error probability
is the same, the total error probability is the same,
so you don't map.
But for us, the first case would basically
be described as, let's say, only 50% for erasure conversion
because the spontaneous part is only half my total error
and then the other half is just a qubit error.
And so I could go from the case of having an error probability,
2p and 50% of erasure conversion to 2p and 100%
or 98% of erasure conversion, that's way more favorable.
So I don't know exactly how to quantify that trade,
except to say that I think you would almost always
want to take it here.
There's one other thing about it,
which is that actually when you talk about composite pulses
for trapped ions, you're actually
kind of trying to suppress two errors at the same time.
One is leakage and the other is that given
that there's no leakage, you have the right fidelity gate.
And we have a slightly more relaxed condition here,
which is that we can ignore leakage.
We can only optimize over the fidelity when you come back
because any leakage just becomes erasure.
So we might even be able to have more efficient composite
pulse gates than what are possible in other systems
because of that.
Yeah, sounds like a great showdown.
Great.
Let me just ask something more general here.
So again, going back to a very naive view of maybe
a either layperson or a theorist view of where errors come from
and this very general argument that's made that if you had
isolated qubits, you can make them almost perfect.
But as soon as you start interacting with them,
which is what you need to do for gates,
you must have that's where errors start creeping in
and you must have the current rates.
So let me just try to ask you, Jeff, as well as the panel,
in general.
So in view of this talk, I'm trying
to see how to update my thinking about this.
So here's one thought that I have, which is, OK,
so single qubit gates are always much more
reliable than two qubit gates.
And this argument that, well, if you want to implement gates,
you have to control from the outside,
and therefore, there will inevitably be errors.
It seems to apply equally to single qubit and two qubit
gates.
So is one way to think about what you're saying,
if you were to extrapolate it, is it an argument saying,
look, there's no reason why two qubit gates shouldn't
be as good as single qubit gates?
And maybe this argument that one usually thinks of in the abstract,
it really applies to just controlling stuff.
But maybe when you have two qubits interacting with each other,
well, in your case, it's through the Rydberg blockade.
And maybe that doesn't introduce errors.
It's just a metastable state.
And you can, by converting it into erasures,
you can get rid of those errors or make them much more
extractable.
So is this a good way to formulate this as a more general principle?
And is there hope that we can pursue it in many other settings?
So if you have comments or anybody else on the channel,
I'm just trying to understand how I should think about this more
generally.
So maybe I can say something here.
So first, I mean, it's a question just look at the history.
I mean, single qubit gates have been done already for 60 years, 80 years,
or even longer, where there's two qubit gates that are much longer.
So there must be a reason for that.
And so now I can analyze what is the reason.
And I guess that there are different aspects.
One aspect could be that if you want to do a single qubit gate,
you use what we would call a classical field to interact with it.
So this is this laser field, microwave field, which is full of photons.
So it's not acting a single system with a single quantum system.
It's one single system with a huge quantum system that
has very little fluctuations, basically, classical.
Whereas when we want to do two qubit gates,
they would have to act one system with this quantum
with another system that is quantum.
And then it's much weaker the interaction.
And I don't know, that could be one way of looking at it.
But I'm sure that there are other ways of looking at that.
And so sorry, Ignacio, just following up on that.
So now when you have something like this red-bed blockade,
should I think of the moral of this story as saying,
well, that's pretty reliable in itself.
The noise really comes from other sources.
And maybe that's not inherent.
So what's the take away?
I mean, is there another step beyond that?
Well, I don't know.
I guess there are many ways of seeing it.
But if you see, if you have a single atom
and you put a huge electric field,
then you make it rotate.
I mean, something will happen.
And we'll do gates with that.
And now the idea that you have to do it with another atom,
and you have to create a huge electric field, what you do
is that you put it in the red-bed state,
and you put it very close.
And then it's a huge electric field
as if it was an external one.
And this mimics a little bit this other situation.
And that's why it's very fast and kind of robust.
Maybe I can add a comment.
Actually, I think Jeff mentioned this right
at the beginning of his talk that the way that two qubit gates
are done with neutral atoms or trapped ion qubits
involves taking the physical system out of the qubit space,
out of the computational space, and coupling it
to some other degrees of freedom that are not entirely
stable, so either to the emotional state
where we have more susceptibility to noise in the case of ions
or to these Rydberg states that have a finite lifetime
with respect to spontaneous decay.
It's that finite lifetime that leads to errors in these systems.
And so, I mean, I think it's because of this,
it's intrinsically more difficult,
and certainly the history bears this out,
that it's harder to perform two qubit gates with low errors
than it is one qubit gates,
but I would not characterize that difficulty as fundamental.
I mean, I think there's an opportunity and a path
forwards where the two qubit fidelities can be as high
or higher than the one qubit fidelities that we see today,
but there's more work that needs to be done.
There's more engineering.
I mean, there's also other ideas, for example,
utilizing these very long Rydberg states,
the circular states that the Hirotsch group has pioneered,
and Jeff also has a nice paper on.
There's other papers also, which allow you to take advantage
of the strong interactions of these Rydberg states
while reducing the decay rates by orders of magnitude.
So there's a path there to at least some paper
get to 10 to minus five, 10 to minus six,
two qubit gate hours.
So it's going to be hard, but certainly there's a direction
one can explore.
I want to connect to a different part of this question,
which in terms of like the theory world
versus the real world,
I think one really nice thing about Jeff's work is that the,
and then actually Liang Zhang is doing similar work
on other error models, is that error has structure.
And so normally in these most quantum error correction papers
for a long time, people were like,
oh, we don't know anything about the errors.
We just randomly select them from the polygroup
on support over the weight of the operation.
And that's a great thing to do if you don't know anything,
but actually any real two-qubit gate, single-qubit gate,
the errors have a lot of structure.
And I think it's been really nice in the last,
I'd say the last five years,
seeing control theorists and experimentalists
work with error correction theorists
to actually make better codes based on that structure.
And I actually think there'll be a lot more work
along that direction in the next couple of years.
So following up on that, Ken.
So are there already very general principles emerging
or is this still in process?
Where would you characterize that effort?
How would you give the big picture?
I would say it's still in process.
I mean, I think erasure errors is a good,
like new thing, like, oh yeah,
we had all erasure errors, it was good.
To me, the one question that Jeff's paper answered for me
was like, what fraction of erasure errors
do I need to have some gain?
And it turns out that gives you kind of a sense
of when it pays off or not.
In this work, unrelated work with cat codes,
with gate transparent errors, that's really neat
because you can think about basically making
almost classical codes where the nodes
are just so biased one way or the other.
But I think what's missing is,
yeah, anyway, this is just my goofy example,
which is what if your error model was,
you just drop gates, like occasionally,
you just don't apply the game.
I can make a normal error correction code
that fixes poly errors that fixes that error,
but I don't actually have the tools to make a code
that, yeah, I don't have the tools
to make a code based on that idea.
And I think if we could start to make tools
that were not us making poly errors
and then finding the right match to the physical errors,
but actually based on the physical errors,
so it'd be quite nice.
Nice.
Yeah, so can I also maybe add a related comment
is that I think that's in Jeff's system,
what's interesting is that there exists a very clean
Q and D measurement to detect the erasure error
without causing a significant,
any damage to the remaining system,
which of course needs to be experimentally demonstrated,
but this seems like really promising
that basically you have all the other different colors
of laser to detect those states
without compromising the other system,
which at least in some of the earlier work
in superkinetic circuits, usually when you measure
something, you will have a T1 induced decay and so on.
So the system starts to collapse like decay faster.
But if what kind of like this in the system,
you can kind of detect those erasure error
without causing much damage,
is actually a very efficient way to extract entropy
from the system, which is really nice
to actually for the control and error correction.
Yeah, I'll ask one other question,
which I guess going back to these,
what you mentioned with the bias preserving gates and so on.
So is it the case that the erasure errors that if, okay,
so let me first save my understanding.
My understanding was that you need the bias preserving gates
to make sure that when you're doing,
this recursive error correction or something like that,
when you can contaminate and so on,
that you end up getting those bias errors again,
right at the next level.
So if you were doing the same thing with erasures,
do you need now erasure preserving gates
or how would you see this, if we thought through this?
I mean, what I'm about to say might be very naive,
but the way I think about bias preserving gates
is really that it's natural that like a ZZ gate,
the Z errors commute with a gate.
So if you have a Z error during the gate,
it's gonna be a Z error at the end,
but that's not true for control X, for example.
So, but you can make it control X out of Z gates.
Now you have Hadamards that flip X to Z errors.
And so the thing about erasure errors
is it's just you just highlight a qubit
and say this qubit has an error,
and then you can track that through gates.
It doesn't, it's not like that error magically
pops up on another qubit.
You can track the spread,
even if you wait a while before you detect,
you can track an error here,
it turns into errors on these qubits.
So it just kind of goes through gates
in a pretty natural way.
If you had an error before a gate,
you still have an error after a gate.
Let's see, okay, great.
Well, can I add a comment to that?
I think that in principle,
there's still some related discussion
about how to do for tolerance,
even in the presence of erasure errors, right?
Because in principle, when you have an Ansela measure,
some like a syndrome associated with erasure error,
you might still propagate those erasure errors
to some other like a coupled qubit.
And there's also some like work related,
if you can use flat qubits or other better designs
to make sure that they still do the error collection
for tolerantly.
And I guess I probably here,
it's maybe have a mixture of erasure
and the depolarization error,
maybe depending on the fraction
of these different types of error,
maybe you may want to optimize the different codes
and circuit designs to get a better performance
in terms of hardware efficiency
and thresholds, yeah.
Which I think it's probably related
to Steve Flamie's question
that are there better codes compared to the XCGX codes
which are maybe better fitted for this kind of
different ratios of erasure and the depolarization error.
And I think that actually will be something interesting
also to look into for serious, especially, yeah.
I'm so glad you brought up flat qubits
because I wanted to ask
if you or somebody else wants to talk about,
it seems like there you are trying to,
with flat qubits also move towards erasures, right?
So it would be interesting to sort of speak about these two
and can you say something more general about that?
Yeah, so we think we had a paper on the correcting,
again, another type of erasure error
which is about like a superconducting circuits
at this cosmic ray events,
which you can think is that the entire coding block
has an erasure, but the way to solve this
is to have a next level of coding
and connecting different chips
and once you detect the erasure,
you can use a second level of coding
to correct that erasure error.
And then there was some discussion,
at least in our paper,
we also think about using flat qubits to suppress them.
But okay, it's just at the beginning
because we only consider the erasure error,
but in practice, there could be a combination of erasure
and other gate errors.
Maybe they are more efficient or general design
to correct those, but that's only,
okay, a little bit part of the thinking so far,
but maybe they are more general,
and maybe Ken can add more comments on this one too.
Yeah, well, I think,
well, okay, so, okay, so at first glance,
why is erasure, erasure seems terrible,
like you've totally lost your qubit,
but what separates it from, say, like a leakage type error,
where you've also totally lost your qubit,
is that you know where it occurred, right?
And so flag qubits, in some sense,
that's what they do,
is they let you know which set of C-naughts
had a two qubit gate error, right?
And so, unfortunately, in the flag qubit scheme,
you can't actually check everything,
it doesn't work out,
but it's somewhere between, like, erasure,
I think that's a nice way to think about it,
it's kind of between erasure and the other way,
because we at least have scoped it down
to a narrow region of errors.
Yeah, maybe a related comment is that,
maybe during error correction, one could do it adaptively,
depending on what you see and decide what to do next,
then the kind of the flag qubit schemes
will have this kind of in the design,
so that will probably give a more efficient,
like a strategy in correcting errors.
That's, actually, that's very interesting,
I hadn't thought about flag qubits as being,
in between erasures and other things,
but I think I see, I kind of thought about other,
maybe like less efficient schemes for detecting erasures,
or for detecting out of the week,
where maybe you have to use some additional qubits,
and what I wondered about those is how you would characterize,
this spectrum from depolarizing errors to erasures,
and do you know, because with erasures,
you have this double, this like higher distance, right?
Remember the steeper slope,
so do you know where like flag qubits fall on that spectrum,
like you get a larger effect of code distance, or not?
There it's, I think, a little bit orthogonal,
because the erasure helps you,
even if you have like a code capacity model of errors,
from just sending information through a channel,
and flag qubits just makes sense when you,
when you're like building the gadgets to look for parity checks.
So it's a little bit, yeah, it's not quite apples to apples there,
but I do think the thing which is important
is that the decoder side of an error correcting code,
the more information it has, the better it can do.
And so erasure is an example where it has a lot of information,
flags give us more information than not using flags,
and then I would say that's the kind of the things in between.
So like if I have, yeah,
if I have some extra promises on my errors for some reason,
I can actually in my decoder use that to make the threshold better.
Yeah, maybe to Jeff's question,
I think it's maybe more related to like a code design,
like rather than just using surface code,
which indeed like it can correct up to 50% erasure,
but maybe when it's kind of having a relatively high erasure,
maybe there are other codes which can also do something quite well,
like a quantum polynomial code or other codes.
So, but then maybe there's a trade-off between what kind of code
that you want to do with the minimum physical resource overhead,
and also practically there's also a question
about the connectivity of the hardware,
like how to kind of find the code
which matches best with the hardware design, yeah.
Because I think for the code items case,
it's probably your less constrained with the nearest neighbor coupling,
which I think it's a great opportunity to look for
like a not just the nearest neighbor coupling codes,
which maybe you can have even like a more favorable,
like a resource overhead for error correction.
Great.
Shall we see if there are any questions from the audience?
Let's see.
I got somebody.
Maybe if you have, could you raise your hand if you have a...
Okay.
I see there was an interesting question from Rob Cook
about the ions interacting with the river states.
Okay.
Okay.
So, Rob, could you could you ask the question directly?
Okay.
Let me read it out.
Then it says, Jeff mentioned that two Eterbian ions induce immediate loss.
I'm curious if there's any interesting collisional physics
between the Rydberg states and the free ions and all photo electrons.
I'm guessing this isn't that much of an issue.
Else you would add a background electric fields.
Yeah.
That's an interesting question.
The one we consider this a little bit in the supplement to our paper,
we thought about it some more,
but the one thing that is close to being a problem that isn't quite a problem
is that when you create this Eterbian plus ion,
it's got all these ground state atoms around it in this array.
And actually this ion is not stopped.
It has a small initial velocity because it gets the recoil momentum
from the electron, but you kick them.
So now this, and it's moving at like three and a half meters per second.
And as it, so it's moving, if it's moving in the plane of the array,
it could come and go by one of the other tweezers.
And then the question is, is it going to get captured there?
Because actually the kind of the scattering cross-section for an ion,
a ground state ion is not that small.
And this is one of these things that just turns out that a terbium is in this
kind of Goldilocks Perkins zone where if this ion was going very much faster,
we wouldn't even be able to detect it before it moves too far.
And that would be the case for strontium,
which has a similar recoil energy, but it's half the mass.
But if it was going slower, it turns out this,
this capture cross-section for the neutral atom and the ion is strongly
velocity dependent.
And if this ion was going just a little bit slower,
then it would actually have a pretty high probability of colliding with this neutral
atom. So this three and a half meter per second is kind of like magical where
it's going fast enough. It's not really going to collide.
That collision probability is like 10 to the minus five or something like that.
So, you know, below where it matters.
But, but it's still slow enough that we can detect it.
Perhaps I could follow up on that.
This question is deep in the atomic physics weeds,
but let me ask Jeff anyway.
So if I have this ion going past from detecting an erasure error,
if there's any vector shift of the triplet p zero or vector polarizability
of the triplet p zero qubit states,
I might deface a bunch of qubits in the array.
Yeah, I didn't, I didn't consider that.
I can check it.
My intuition is that, you know,
junior junior and those guys are doing, well,
no, it's trying to get Andrew levels to stop it,
but 10 to minus 20 level with these qubits and the turbulent.
This is really my guess is that it's not only not going to not matter,
but by like 10 orders of magnitude, but I'll, I'll, it's worth checking.
It's worth checking.
Well, excellent.
So I think this probably brings us to the end of the time we have today.
So let's see.
Jeff, thanks again for, for wonderful talk.
And thanks to all the panelists for really great discussion.
This was most enjoyable.
So I should say that next week is our last colloquium of the spring.
It's Lenny Saskin who will talk about quantum gravity and the quantum extended
chest during pieces.
So see you next week and thanks everybody again.
Thank you.
Thank you all very much.
