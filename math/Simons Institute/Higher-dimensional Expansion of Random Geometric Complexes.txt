Yeah.
We're going to have the open problem session this afternoon, so if you have any good open
problems that can be presented easily without much background, so it would be great.
So we're just going to have an informal session where people can come and present
open problem quickly. So I think it's nice if your open problem doesn't turn into a talk,
so something short that we can describe. And then later people can contact you about
why it's interesting and make sure more of the background. With that, yeah, our afternoon session
the next speaker to be, and we're all excited to learn about your new work. Okay, yeah, thanks,
thanks for the invitation and thanks everyone for coming to the talk. So okay, today I'll talk
about two-dimensional expansion of random geometric complexes on the sphere. And this is
based on joint work with Sikilu Siddhanta Mohanty and Elizabeth Yang. So if you were here for the
previous talk, it's like the rematch, you know, this is like the second in sort of a series of
works. But okay, here we're going to be asking questions that are a little bit different than
we were in the last talk. So I'll start with a story that's hopefully familiar about expansion,
and then I will go to two-dimensional expansion and I'll introduce that notion and I'll talk about
our results. Okay, so let me say that a graph G
is a gamma spectral extender.
It's the second eigenvalue of its normalized adjacency matrix, which I'm just going to lazily
to know with lambda 2 of G is upper bounded by gamma. And this is interesting in situations where
gamma is strictly less than one, right? So if gamma were one, then this would correspond to the
situation where there is more than one component in the graph. When I said gamma is less than one,
it says that there are not more than one components in the graph. And in particular,
this is interesting when we are actually thinking about a family of graphs.
Gn1 for each n in the natural numbers, let's say, are all n sufficiently large enough, and we're
thinking of n as the size of the graph, the number of vertices. Okay, so if you have such a family,
which are gamma spectral expanders for gamma, which does not depend on n and is bounded away
from one, then this gives you a bunch of consequences for the graphs, which are pretty
nice. So this means that, for example, the graphs are like, no, they don't have any sparse
partitions, vertex partitions, no sparse cuts. And you get that random walks mix fast.
So these are kind of useful properties for graphs to have. It's like a measure of how well connected
the graph is. And, okay, so, and a priority, like if you'd never thought about expanders before,
it might seem to you that in order to be well connected, it would be necessary for a graph
to be dense, right? So like if you try to think about examples, the first example that might come
to mind is the clique on n vertices. Okay, it's pretty easy to show that this is an excellent
spectral expander. In fact, you can take gamma going to zero with n. But it turns out that actually
graphs with degree, which I'm going to denote by capital delta, order one exists. So, you know,
maybe the first analyze examples of this were random graphs even, but they're also
more structured examples too. But here is basically a theorem which says that, you know,
the property of being a good spectral expander is really, really not rare at all. So here's a theorem.
And one side of the theorem is due to Alon. And the other is due to Friedman.
And really, I'm going to state the TLDR version of the theorem, okay, which basically says that
random delta regular graph
has the best possible spectral expansion.
Okay, I could be more precise, but this is the TLDR version. So not only are there constant
degree spectral expanders, okay, but actually they're extremely abundant, like most delta
regular graphs will be really, really good spectral expanders. Okay, so that's the story for
graph expansion. And now the question is, what is true for hypergraph expansion?
So, what is the parallel story
for hypergraphs slash
simple complex?
So, okay, so I'll start by defining a analog of expansion that will be true,
which is designed for simple complexes and for hypergraph, but I'm going to define it in terms
of graphs, because I think if it's your first time seeing this definition, it's easier to
absorb it that way. And if it's not your first time seeing this definition, you know what I'm
going to say anyway. So, okay, so here's the definition. So I'm going to say that a graph
is a gamma local 2D spectral expander.
It satisfies two conditions. The first condition that I'm going to ask for
is I'm going to ask that the graph is connected,
which is more or less what we'd expect from an expander. And then the second condition
that I'm going to require is I'm going to require that for all vertices in the vertex set of V,
if I take the graph induced on the neighbors of this vertex,
and I look at the second eigenvalue of the normalized adjacency matrix of that graph,
then that is bounded by gamma. Okay, so this graph, this graph induced on the neighbors of
the vertex, this is called the link of the vertex V. Let's just do one or two examples
so that we have this notion in our mind so that we understand what's going on, right?
So consider, for example, this graph.
Okay, so in this graph, let's first consider what is the link of this vertex V over here,
right? So the operation that I said I would do is I would look at the neighbors
of V and I would look at the graph induced on those neighbors. So in this case,
the neighbors of V are all of the other vertices in the graph, right? And so the link
of V is just going to be two parallel disconnected edges. Okay, let's also look at the link of the
vertex U. Okay, so here the link of U would just be what? So we take the neighbors of U,
which are just these two guys, and we delete the rest of the graph, and we just get this edge by
itself. Okay, so is this link operation clear? Yes, right, this is a stronger definition than
the definition over there. Now, really like what? Okay, so this is what it means to be a gamma local
two dimensional spectral extender. And you'll notice that like if I'm thinking of every triangle
in this graph as being a hyper edge of order three, right, this link operation is a reasonable
operation in the hyper graph, right? It's just looking at all the hyper edges that contain V,
you know, deleting everything else, and then removing V itself and looking at the graph that
it leaves. Okay, so this is this is why this kind of makes sense in in simplicial complexes and in
hyper graphs. Okay, so this is a gamma local 2D spectral extender. And instead of telling you,
oh, let me tell you, okay, let's let's look at just one example. Any, any easy suggestions for an
example? Yeah, the complete graph, exactly, that's what I wanted to say. Okay, so let's just observe
that the complete graph on vertices should satisfy this definition at the very least, right? We have
one example that is easy to think about. If I take the complete graph on vertices, and I look at the
link of any vertex, that's going to be the complete graph on n minus one vertices, right? I'm just
deleting that graph and looking at the remaining graph. So, okay, so and I already told you that the
complete graph was a good spectral expander. So we're satisfying the definition of a
gamma local 2D spectral expander for a good choice of gamma. Okay, but rather than tell you,
and it turns out that there are examples of constant degree sequences of gamma local
2D spectral expander, but before I get into that, let me just tell you, instead of telling you any
applications of these objects, like there's a whole program right now that's heavily dealing
with the stuff, let me just tell you one amazing theorem, okay, which to me is
really like a thing that makes these objects very special. So this theorem is called the trickling
down theorem. Okay, and some version of this theorem was proven by Garland back in the 70s,
more recently developed by Oppenheim. And the theorem basically says the following thing,
it says that if gamma is good enough in a gamma local 2D spectral expander, then global expansion
is witnessed locally. Okay, so it's saying that if G is a gamma local spectral expander
for gamma less than a half, let's say the value is half minus epsilon, then
G itself is going to be a spectral expander as well. Okay, so if I look at the second eigenvalue,
not exactly of the adjacency matrix of G, but of a version of the adjacency matrix of G in which
every edge is weighted by the number of triangles that it participates in,
then the second eigenvalue will be at most gamma over one minus gamma, which is less than or equal
to let's say one minus two epsilon, you can get a little bit better than this. Okay, so what this
means is that if you're a gamma local spectral expander for gamma bounded away from half, then
you are also going to be a expander, meaning that like global expansion is witnessed locally,
which is kind of a weird thing, right? Like in an expander graph, you know, you think of
expansion as a very global property. So it's pretty amazing that you can have these objects
where you can witness global expansion locally, I think. Okay, any questions at this point?
Okay, cool. So here we are. So I said, okay, so we were asking what was going to be the
parallel story for hypergraphs and splishy complexes. So I've told you what 2D expansion is,
and now we should ask, you know, what are examples? Okay, and is this property of 2D spectral
expansion rare or abundant? Okay, so known sparse spectral extenders,
so we don't have that many constructions of sparse 2D spectral expanders. So
the maybe a flagship example is the Ramanujan complexes.
Yeah, so let's let's let's take a variety of constant degree would be ideal. Okay, but I'm
going to eventually like move the goalposts by a lot. Okay, so so the Ramanujan complexes
are indeed constant degree. These are analyzed by a couple of groups of people, Lee and Cartwright,
and Tsuk, and Lubosky, Samuels, and Vishne, all kind of in the early 2000s. Okay, so these
were these were the the only known examples of sparse 2D spectral extenders, which also
satisfy this trickling down theorem condition. More recently, we've had a couple of additional
examples, right? So we have coset complexes. Maybe the first work on this was by Tali and
a couple works came after that. And recently, there's a there's another example,
which which looks at Kali graphs of random codes due to Louis Golovitch, I guess I don't see him in
the audience, but you know, he's he's around. So in all these cases, we have we have sparse graphs,
but you know, these these are very, very structured graphs, right? They come from algebra. In each
case, there's some kind of group that is lending structure to the graph. Okay, so the sparse examples
we know, come from groups. And, okay, I know that one of the themes of this workshop is supposed
to be structure versus randomness. It's not structured versus randomness in the way that
I mean right now, like right, like usually structures randomness, you say like an object,
you can decompose it into structured and random parts. But here I really mean like structure
versus like randomness, right? Like, can we come up with a is it necessary for things to go perfectly
right in order to get a two dimensional spectral expansion expander? Or could you come up with a
kind of higher entropy distribution over examples, where not everything has to go exactly right,
right and still manage to get this property. So the question is, are there natural examples
distributions over 2d spectral expanders?
Of course, yeah, any time.
You have to mention, you know, by time, each time.
Oh, right.
Yeah, lift.
Thank you, really.
Okay. So this is a, this is a another construction in which you start with a really good,
with a really good complex, but then you can take a random lift of this complex,
similar to like taking lifts, random lifts of graphs, you can randomly lift complexes and
also get a constant degree.
Okay, but okay. And so we want natural examples of distributions
over 2d spectral expanders.
Okay. And we're going to have two goals, right? We want to have two things. We want to have sparsity
on the one hand, right, bounded degree as small as possible. And on the other hand,
we still want to kind of be in the regime where the trickling down theory theorem kicks in and
the local spectral expansion is much better than half, right? We want that we're in this
regime where global expansion is witnessed locally. So, okay, so what's your first guess
of a thing to try here? I mean, like, you know, like Delta regular graphs worked pretty well for
one dimensional expansion. So why not two dimensional expansion? Okay, so that's the first,
that's going to be our first try. So what happens if you take Delta regular graphs?
Yeah, the links are disconnected. So in particular,
if the degree is less than root n, then the links are mostly independent sets.
Right, if I take a Delta regular graph and Delta is less than root n,
then in the neighborhood of vertex, I'm not going to see very many triangles.
I will not mostly not see triangles, right? It's going to look locally like a tree with
branching factor Delta or Delta minus one. And so now when I take the link of V, right,
what I get is I just get an independent set. This has the worst possible spectral expansion,
the, the, this is only a one local spectral expansion, which is just like a trivial statement.
Okay, so this makes us sad.
Okay, that didn't work. Now the next thing that you might try is you might say, okay,
there's depending on who you are, you might try different things. But one thing you could
try is you could say, okay, here there weren't any triangles like no problem. I'm just going to,
I'm just going to take a Erdoshrani hypergraph, right? So I'm automatically going to make sure
that there are a lot of triangles. That's not going to work either.
Right. So if you, if you include every hyper edge independently with probability P,
again, what's going to happen is that if the average degree is less than redone,
then there won't be many examples of the following thing.
You won't have hyper edges that overlap.
Okay. This, this structure won't happen very often. What this means is that if I look at the link
of the vertex, it's just going to look like a bunch of disconnected, you know, little butterflies
like this, right? And again, the link will be disconnected. It's not like the, the link when
they're only being independent set, right? It's just going to be like a matching.
Okay, this is a, this is a computation that's a little bit dull. So I'm not going to do it,
but you can check it for yourself. So neither of these things went well.
Okay. A different thing that you could do, which, which you say
once tried to do is you could take a, you could take a Cayley graph
with random generators.
Right. This is like maybe one way that you could motivate this is you say, well,
the algebra construction seems to work pretty well. So let's try to have something that borrows
some of the, you know, structure of an algebra. And here you can guess
graphs, which are two dimensional spectral expanders in some sense.
And the, with degree, let's say polylog in N,
but you only get half local spectral expansion.
Okay. So, so you get graphs that have non trivial two dimensional expansion properties.
Again, I'm not, I'm not going to explain exactly what, what, uh,
you know, what, what that means. But in some sense, like we really wanted, I really wanted
a distribution over graphs, which we're in which the trickling down theorem would kick in.
And these graphs will not have the trickling down theorem kick in.
So the motivation for this work is exactly this, like the, the, my kind of structure versus
randomness, right? Like, can I find a natural distribution over sparser two dimensional
spectral expansion expanders where the trickling down theorem actually will kick in.
And my benchmark, okay, I would have loved to get a constant or polylog or make a degree,
but instead let's move the goalpost and just ask for something better than root N,
since this is the best that are kind of like natural random constructions can get.
Okay. And the idea for, for this was to maybe try to look at random geometric graphs. So,
okay, so I'll, I'll define a specific class of random geometric graphs,
but the idea of using random geometric graph is a little bit more general as I'll explain
in a second. So, okay, so here's a definition. So to sample a graph G from the class of random
geometric graphs on the d dimensional sphere, which I'm going to denote g o n p with a subscript of d.
The first thing that you do is you sample vectors independently.
From the uniform distribution over the sphere in d dimensions. Okay, so I've got my sphere.
I'm going to sample a bunch of vectors.
Okay, so first I sample points. And the second thing that I do is I add an edge
between i and j, if and only if the inner product between the vector vi and the vector vj
exceeds some threshold tau, which I choose as a function of p. So, so tau is chosen
such that the marginal edge probability of every edge is p.
Okay, I'll turn it. Okay, what does this, what does this mean in this situation? I just take,
I look at v4, I draw the spherical cap of measure p around it.
And then I just add an edge for every vector that fell inside the spherical cap.
Okay, and you can do exactly the same thing in any metric space, actually, right? You,
as long as there's some notion of a distribution over that metric space that you can,
that you can come up with, right? You, you sample points uniformly random and independently in the
metric space, and then you take a ball of a certain measure around every point and you just
include edges for every other point that falls in that ball. Okay, so this is the, this is the
distribution. Okay, the main thing that we're able to do is we're able to prove that these graphs
in some parameter regimes achieve the two-dimensional spectral expansion. So here's a, here's our theorem.
So what we're able to prove is we're able to prove that for every epsilon,
there exist numbers c and delta greater than zero so that
if you sample a random geometric graph in c times log n dimensions
on n vertices and with edge probability 1 over n to the 1 minus epsilon, so in particular this
implies that the average degree and the maximum degree is something like n to the epsilon.
Then with high probability,
d is a 2d half minus delta spectral standard.
Is randomness only in v1 to vn?
Yeah, only in v1 to vn.
So I guess my question about this is just, it seems strange, right? Because the usual
graph theory, if you take a new method graph like this, it's going to have worse expansion
properties than a random graph. So like what? Okay, what's going on here? Yeah, that's a great
question. So it's crucial when I do this that the dimension which shows into the specific
logarithmic in n, I mean I have a narrow window of dimensions in which this is going to happen,
it's very important the dimension is logarithmic in n. Oh, it's not like fixed dimension in n,
right? Exactly, so okay, so let me let me say a little bit more about this, I think that's a
really great point. So the study of geometric random graphs is pretty mature in the setting
where the dimension is fixed and you think of the number of vertices growing to infinity,
okay? But in that situation, the graphs have pretty poor global expansion. And let me just
illustrate why with the example where d is 2, right? So I'm just looking at the circle.
Okay, so it is 2, right? Then basically what I'm doing is I'm taking a random discretization
of the circle, it's gonna look like this. The points are some kind of Poisson process
on the circle, right? And then I'm connecting points that are kind of nearby.
What is the spectrum of this guy gonna be like? It's gonna be like kind of like the
spectral of the spectrum of the cycle. In particular, there are really sparse cuts,
right? Like the half-space cuts are super sparse. This will persist for any constant dimension,
okay? And it will only stop happening once I take the dimension logarithmic in n.
And now it's no longer the case that my discretization of the sphere is so faithful that
the half-space cuts are sparse like you would expect. So this is a yeah, this is this is why
this works. But why do you need to down the dimension? I can't, isn't dimension kind of like
log n because Johnson and Mishra? Okay, okay. So this says why I need the dimension to be at
least log n. And you're asking why can't the dimension be larger than log n. Okay, this,
I can say something a little bit more precise about this in a moment. But basically, the problem is
that once you, once you take the dimension too high, your graph becomes more and more
similar to an air to serenity graph. I guess this was said in the previous talk.
And the way in which, okay, so if the way in which you're measuring how far is it from being an
air to serenity graph is in the global expansion notion, then that happens when the dimension
exceeds logarithmic in n. So, okay, let me say something more, I guess I can say something more
precise later. But basically, if you want the dimension to grow faster than logarithmic in n,
then you're having to take the average degree to be something like root n in order to get
expansion. So that's, that's the issue. Okay, that's a great question. We don't really know the
answer. I think the answer should be yes. But analyzing this was, I'll say more about it when
I get to the proof. I'll point, or I'll talk about why the, why the, why it's hard to analyze higher.
Okay, one question you can ask is, is this tight? And the answer is yes, it's tight.
So, it's like, you're not going to get a better analysis of this, of these graphs, unfortunately.
Could you help to replace this here with something else? Yes, I do hope to replace this here with
something else. Yeah, okay, so, so right, so I tried to highlight that I said, I said,
suggestively, then you could have done this with any metric space, you didn't have to start with
this theory. Okay, and I think that it's possible that if you have, that there would be another
metric space, where instead of having to take your average degree to be growing polynomially
in n, right here we have for any small, like arbitrarily small polynomial in n, we can get it,
but we can't get it to be like n to the little o one. Otherwise, we lose the delta becomes too small,
right. And, and I think that there could be other metric spaces where this might not happen.
And at the end of the talk, I will say, like some requirements that we know that these metric
spaces have to satisfy.
Any questions?
Do you think that was like, not any odd for questions, that was like any other questions?
Okay, so I was thinking, you can take just any two major ones, think of the finals as little
planes actually include them all together to get something that looks different,
maybe look at that and sub sample. Yeah.
And ask yourself, will we get something? Okay, yeah, that's an excellent question. So,
so maybe I'll, so you can, you can take a existing high dimensional expander,
and you can turn that into a metric space in some reasonable way, the same way that a graph
in the binary, and then you can sub sample that and ask if you're still going to get
high dimensional expander. I think the answer should be yes. In fact, like the way that we
prove this will give one way in which to prove what you were saying, if you were complex,
satisfy certain properties. So I think there is a hope for that, but like really like, okay, so
aesthetically, what I was hoping for was like a natural example of distribution over to
the standard. And like, depending on what what your taste is, like natural may or may not include
start with a already like a remind young public, which to me, and I'm like already
not sure if that's right, but not very satisfying.
Then my question was, like, is it really a random thing?
This one, though, is what I was just thinking, whereas I was wondering what makes this construction
more rich. But okay, then we can now say we can take it off. Yeah, let's not take it off.
Okay, but I think I think it's a nice bridge into what I'm about to say anyway, which is about
how we prove this. So let me give it a couple of words about the proof with an eye towards
like generalizing this construction to other other spaces. Okay, so
basically what we what we did, the way that the proof works, is we think about the sphere itself,
like without discretizing it, we show that in some sense, the sphere is a two dimensional
expander. And then we show that the discretization inherits this property with high probability,
as long as the graph ends up being dense enough. Okay, so so our proof is really about random
restrictions. Okay, so let me define what a random restriction is. So
this fancy H is a graph, a possibly infinite graph.
Then we say little h is an n vertex random restriction.
If it is given
by sampling vertices v1 through vn independently from, let's say, the stationary distribution
of a random walk on fancy H. Okay, for example, if your h was vertex transitive,
then this is basically the uniform distribution. And then taking h to be the vertex induced subgraph.
Okay, that means that basically, I'm going to keep these vertices and all edges between them
and I'm going to throw away the rest of the graph. This is an n vertex random restriction.
So my claim to start with, in order to motivate why I'm even telling you guys this,
is that this random geometric graph is in some sense an n vertex random restriction of the sphere.
It's not exactly, but of the sphere in which I put edges between vertices that lie within
each other's spherical cap. So if you take h to be the graph with the vertex set being the sphere
in d dimensions and the edge set being the set of all pairs of vectors u, v such that the inner
product of u and v is at least tau. Okay, then g from the distribution geometric random graph
in d dimensions inverters using edge probability p is a n vertex random restriction of the stage.
So what we're going to do is we're going to study the properties of random restrictions of graphs.
And let's also like say one more thing, right? So if I'm looking at the link
of a vertex v in the random geometric graph distribution,
right? This is actually also a random restriction of h, but where instead of taking the vertex set
to be the whole sphere, I take it to be a spherical cap. So I would take h to be a cap
around some vertex w of measure p, right? And then the edge set would be the same thing.
Right, so by studying properties of random restrictions, we're going to be able to say
stuff about the links. And that's how we're going to prove local spectral expansion.
Okay, let me say the theorem. I'll say the TLDR version out loud and then I will write the formal
thing down. This is also due to due to us. So basically what the theorem is going to say
is that if I have a graph fancy h, which it has a sufficiently good mixing of random walks,
okay, then that will be reflected in the spectrum of a sufficiently dense random restriction.
Okay, so if I have a big graph, capital H, where random walks mix super fast,
then the spectrum of a random restriction of this graph, which is not too sparse,
is going to be good. Okay, that's what that's what we're going to say.
Okay, so here is the theorem. So if h is vertex transitive,
h is the transition matrix,
or transition operator,
or the random walk on h,
has a unique stationary distribution.
And, and this is the most important property, that
pH is contractive in total variation distance. So I want that there exists
c and lambda, so that for all integers k
and starting distributions mu,
the total variation distance. If I sample a point from u and then apply pH for k steps,
or a case of random walk,
the stationary measure is in most c times lambda is okay.
Okay, this is satisfied, for example, when capital H has a nice log, so above inequality.
So if this holds, then with high probability, an n vertex random restriction,
h has
second eigenvalue of the normalize adjacency matrix bounded by
the maximum of lambda, right, which is kind of like what this, this is the rate of contraction
of random walks in the original graph. And here I'm going to have a term that's going to penalize
me if the graph ended up being too sparse. Okay, so here I have some kind of polynomial in log n,
and this c guy, and then divided by square root the expected
degree of a vertex in h.
Okay, so this is saying that as long as the expected degree is at least poly log n,
the second eigenvalue of the random restriction is going to be at most lambda, where lambda was
the contraction factor for random walks in the original manifold. Okay, so the spectral properties
of the random restrictions are inherited from the contracted properties of random walks on the
original graph. Does this, does this make sense? I know this is a long theorem statement.
Part where you compare the lambda with poly log n. Oh yeah, okay, okay, okay, so, so like really,
that's a bad sign. So, okay, so, so really the important thing here is the lambda,
okay, right, lambda is what I want you to think, but you, it makes sense that if the graph was too
sparse, like if h, if I end up, if n is really, really small, and like h ends up being really,
really sparse, and I shouldn't get any guarantees about the second eigenvalue. So this second term
here penalizes you if the expected degree in h is too small.
And in your theorem, is this the second term of order lambda or like it's significantly
larger than lambda, smaller than lambda? It all depends on, it all depends on the
expected degree of vertex in, yeah, but in your random geometric spherical model.
Oh, lambda, lambda wins, in the Prandtl regime that's good for us.
It's much larger in the Prandtl regime that we consider, because, because for us this is like,
you know, n to the epsilon, and this is some polylogin n, and the lambdas are constant. So,
so this ends up, this ends up working for us. But it's also kind of like restricting us from
going sparser in some sense. Like if you optimize it so the two terms are equal, you don't get
anything. Yeah, we're not going to get anything. I mean, in some sense it's, okay, yeah, let, let
the, but it's, it's, okay, it's making sense, right? Like
you would have something that's like a fantastic extender, but if you sub-sample it in so sparse
that you're only, you don't see any, any edges, you're not going to get an extender. So,
Okay, so like, truly, I don't think we, we didn't get the optimal power of vlogging here, but in the
same, okay, so, so, yeah, so I can say something pretty concrete about why you need at least some
kind of vlog if you're going to do something like this. So, okay, so let's, it's helpful for you
to think about this here, but what I'm saying is equally true in, in any method space, right? So,
here is v1, let's say, okay, and this is the neighborhood of measure p around v1, right?
And the degree of v1 is just going to be how many other points landed inside this neighborhood.
Okay, so the degree of v is basically the marginal distribution is binomial with n-1 trials
and edge probability p, so that's probability p. And, you know, the, the, these degrees, they're like
a little bit correlated with each other, but they're still not that correlated. So really,
what they behave like is they behave like roughly like, and roughly independent samples of binomial
with parameter p. If this p is less than log n over n, then we know that these don't concentrate well
and some of them are zero, right? And then you can't have an expander because you have isolated
vertices, right? So it's the same reason that your connectivity threshold for gmp is like log n over n,
that's what's going on here. Yeah, so you could, you could say like, well, maybe if we like erase
all of the like little isolated vertices and we like snip all of the trees off of the giant
component and blah, blah, blah, things are nice. And the answer is like, maybe, yeah, but like,
you know, that's for, for future work. Okay, so, so log n seems necessary, but we're not even close,
I guess, like, we're not close to that for, for other reasons. So this isn't the thing that limits
us, I think. Okay. All right, other questions? Okay, looks like not. So, okay, so now, now that I've
said this, this theorem and, and this about the proof thing, maybe it's, it's, it's hopefully what
I'm about to say next isn't going to come as a surprise. Okay, so what is our strategy?
We take the graph fancy h for which we're going to do the random restriction
to be the spherical cap.
Okay, so really, it's, it's this graph, it's like the graph where my vertex set is the cap
of measure p around, let's say, without loss of generality, the, the indicator vector for the
stress coordinate. Okay, and my set of edges is all u and w with inner product.
And I'm going to analyze what do, how quickly random walks contract in this space,
and I'm going to apply that that theorem. Okay, now,
what is a random walk in this space look like? Basically, we're here, we're inside the spherical
cap of e one, right. And what does random walk look like? It's just, it's just I start, you know,
I start here, and then I hop to a random place inside the intersection of the cap around e one
and the cap around the point that I started off, right. So, like maybe I go here and then,
you know, like maybe I go here, etc. Okay, so, but what is this, this is really like applying
successive convolutions of a cap of measure p around my current point, similar to what
Cp was saying earlier in her talk. So, we just have to analyze the, the contraction properties
of its operator and we do that via coupling with Brownian motion. This is maybe not like
that it's pretty clean way to do it for us, but maybe this isn't the only way that you could
have done it. But anyway, we get results that are pretty tight. So, okay, the cool thing is that,
like, the spherical cap when the dimension is large is not that different from the sphere.
So, more or less what we can do is we can, we can think about the sphere. Anyways,
let me say one more thing about that.
Okay, so I've got, I've got the sphere. And let's say that I'm only considering
the restriction to the spherical cap. If D is sufficiently large, then basically,
all of the probability mass within the spherical cap is here at the boundary of the sphere.
Okay, so if I sample two vectors, u and w, uniformly at random,
in this high dimensional sphere, they're basically, like, we can think of them as sitting on the
boundary. So, like, we can think of u as basically being equal to tau times
one plus some perpendicular components.
And we can think of w similarly as tau times e1 plus
perpendicular components, where these are both uniform over a sphere of dimension one smaller,
right, the sphere perpendicular to the first coordinate vector.
So, so more or less, like, when we're analyzing the, when we're analyzing the spectral properties
of this graph, it's enough for us to analyze the sphere itself. Okay, then we're in a, in a more
vertex transitive setting, we have to do some kind of hacks to, like, get over that, but that's not
such a big deal. This is one of the things that's annoying, though, if we want to analyze higher
dimensional simplicial complexes, like, we, we did have to do this, I mean, you know, here it's
like not like exactly we're in the lower dimensional sphere, we're only approximately in the lower
dimensional sphere. If I was looking at the intersection of two caps, which is what would
I, what would I need to do to get to three dimensional expander, then it would be more
annoying to do this hack. Okay, not clear that it's worth it, since anyway, we only get polynomial
degree, right?
Also, you need to get the one third.
What?
So you got something that's one half, but if you're going to tell me you need that one third?
Yeah.
Okay, we need to get better, that's true, actually. Yeah.
I think it might work anyway, because, okay, I'll say one more thing. So why do we manage to get
from, why do we manage to get the half, that's, that's coming in a second. So, okay, so, so the
u and w that come from the cap, they're like coming from a sphere in one lower dimension,
but automatically their inner product and their edge probability is boosted a little bit, right? So
if p is equal to the probability that two vertices or two vectors a and b have inner
product at least tau, let's let q be equal to the probability that u and w have inner product at
least tau conditioned on u and w are both inside a spherical cap around same vertex, right? What we
want really is we want that q is much, much bigger than p, right? p was the, the ambient edge probability,
q is going to be the edge probability inside the link, right? This is like the probability that
a triangle closes. And the whole point is that we want q to be large, right? Like the average
degree inside each link. So the average degree inside the link is going to be np because this
is the number of vertices that fall inside the link times q, right? And this is better to be at
least one, otherwise we're never going to get a connected graph and therefore we're not going
to get a good spectral expansion. So, so why are we able to get this? The reason we're able to get
this is that we can relate q to the, to the probability that two vectors on a lower dimensional
sphere have inner product less than tau, okay? So, so, so why is that? So let's, let's look here. So
as I said, like basically we can model u and w as sitting in this level set. They have, they have
tau projection onto e1 and then the rest is a random vector. And you can see that the event that
u and w have inner product at least tau with a little bit of calculation, this is equivalent
to the probability that u perp and w perp have inner product at least tau over one plus tau.
So, okay, what's happening is that the, because if tau is large enough, right, this is a
significantly smaller threshold that I have to exceed, okay, like my, I'm like moving the goal
post once I'm inside the link, it's like not that difficult to satisfy the inner product
condition anymore. And so if tau is omega one, the edge probability, like q is bigger than p by a lot.
Okay, this is, this is where also, by the way, how I'd like answering your question from before,
if the dimension exceeds log n, then actually tau is little or one, and then this probability
doesn't change by very much. So p and q are roughly the same order. And the condition that np
squared is at least one is exactly the condition that p is one over root n. So, so this is a,
this is where this comes up. I'm glad you're still awake to hear the punch line.
Okay, so, right, so what was I, what was I saying? Okay, so if you were doing the three
dimensional, right, like the point is that once I take another intersection, things are even better,
I expect. So I think things will work out, but I'm not really sure. Didn't think about it too hard,
because as I said, didn't seem worth it to do these hacks. Maybe I shouldn't call them hacks on
live television. The, okay, so, right, and, and, okay, just a parenthetical note, right, so it
turns out that for this graph, the parameter lambda that we can take is, okay, so on the sphere,
for the tau sphere, lambda would be equal to tau. And this is exactly, if you go through the
computation, you can see this exactly satisfies the half, right? So this, our link eigenvalues
are going to be tau over one plus tau, and our global eigenvalue is going to be tau, but you
can always see that this is less than half, because tau is less than one. Okay, let's see, maybe I should
stop talking about the proof and say some stuff about open questions, unless there are questions from
you guys. So you can really choose any position, and you're on the one on H? Yeah, pretty much.
How about the one that just,
that just maps into the, into part? The one that directly takes you to pi? Well, then it's like
that corresponds, that's your corresponding complete graph when you do the restriction, so
then it's fine. Well, we don't see the random one.
Well, it's the vertex in this subgraph, so like you'd inherit the weights too,
and then I guess like if it takes you, you know, like the, the one that takes you,
there's the uniform distribution is like, let's wait to everyone. So the, I think it would,
I think it would be true that you'd get the complete graph if you do the restriction of that.
Sorry, that was probably too fast to follow, but we'll take it online. Thank you,
but instead of saying that this, so this vertices on the cap, or
approximately on a sphere, can you say that they're exactly on the ball of a slightly distorted
metric in the sphere, maybe an ellipsoid, and then, so just say that when you go to
links and sort of lower and lower dimension, you don't, it's not that you're approximately
on some ball, you just change the notion of a ball, but you have this fact,
you know, let's change the, let's say, I think the answer is yes, but I think it will still,
like the thing that's, the thing that's annoying is that you're no longer in the vertex
transitive case, and that will continue to be true if you distort the metric. So like,
I think even if you phrase it differently, like the issue, like the exact typical issues that
come up will be the same. It's not, it's not the end of the world to address them, but it's like,
it's kind of annoying. Yeah. Okay, I'll try, I'll try, I have like a couple minutes left,
so I'll try to end with some open question. So I think, okay, really in my mind, there's like one
primary open question. So some open questions came up from the audience as we were talking, like,
could you get higher dimensional expansion? Not that I defined what that was. I think like,
another interesting question is like co-boundary and cosystolic expansion, but okay, so, so these
are also interesting questions, which I don't like, I thought about them a little bit, I don't
know how to really answer them. But, but I think this is the biggest, the biggest open question in
my mind. So here's the main open question. This is the shot close question. Is there another geometry?
Which is half minus epsilon, or half minus delta, let's say, local spectral expanders
with degree n to the little low one. Okay, for the reasons that I argued there, I guess like,
it's not reasonable to shoot for better than logarithmic and n. But I mean, we can only get
n to the epsilon for fixed epsilon. So this is a, this would be like already a major improvement.
And it's also very necessary if you want to take these objects and use them downline in, in
applications. So, okay, so this is the question. And we thought about this, we've been thinking
about this question for, for a while. And we can come up with a couple of criteria that your
geometry would have to satisfy. So I will say, I guess I have time to talk about one of those
criteria which comes with a nice picture. So, okay, so you must have some curvature.
It doesn't seem like necessary to have positive curvature, could have negative curvature,
but it seems like we need the space to be at least a little bit curved. And in order to explain why,
let me give you an example of a non-curved space. Okay, so think about the, here's a counter example.
Which is going to be clear, hopefully, how to generalize. Think about the torus
in d dimensions if you want to. But I'm thinking about the r by r by r torus
in d dimensions. Okay, so, so the edges wrap around here, right, like this, this
you can identify with this edge. Okay, let's look at what the length of the vertex is going to look
like. Let's say, okay, so, so here, so here's the length of the vertex. And suppose that the,
that the side length of the box around this vertex is one. Okay, so the thing that dictates
whether this graph globally is a good expander is the ratio between r and one. Right, if r is small
relative to one, this expands pretty well. If r is larger, the expansion is terrible.
Right, just like the, just like the grid, right, like the, the expansion of the grid gets worse
as you take the grid side lengths larger and larger. But if I just look at the length of
the vertex here, I have no idea whether r was larger or not. Right, like, just by looking at
local neighborhoods, you have no idea what the global scale of the graph looks like.
Therefore, there is no way to witness global expansion locally. The links embed into a
non-expanding graph. Okay, anytime where you have flat curvature on your manifold,
you can take the flat direction, stretch it out, keep the links the same. Okay, and then
embed them. I mean, that's an, and then those links embed into that non-expanding manifold. Okay,
so curvature seems necessary. Some other conditions maybe seem necessary that, like,
take a little bit longer to explain. But, but I mean, I'm still hopeful. Like, we, every
couple of months, we come up with a new candidate that we try and analyze here. So if you have an
idea, I'm hoping to hearing it. Okay, so thanks. Thanks so much for listening.
Questions?
So what about, I mean, I know you care more about geometric stuff, but let's say k-d-graphs,
if you look at the balls around the vertices, like for a wild group or something, you have good
expansion. It's possible if it works. Yeah, I'm not sure.
Other questions? What don't you expect from negative curvature, like the neighborhood
should be, like, sparser than we want, rather than more, like, concentrated, and then it would be,
like, worse. And for you, it's not how it works. It's not clear that that's how it works. Yeah,
maybe, again, like, I'll chat, I'll chat with you offline about it, but, but actually I have a little
with the opposite intuition, like, that in a negatively curved space, like, locally, things
are pretty good, but globally, you can turn from that side.
I guess it will converge to this thing, right? Like, this thing? The sphere, like, this version of the
sphere, would it? Yeah, yeah, so, okay, then I erase it, but like, instead of here, that's you put us.
The inclusive. Yeah. Oh, but I mean, if you, so then you're in the regime where you're fixing d,
and then you're taking n larger and larger, right? No, I want, I want your remember, n
grows over. Oh, you want n to grow along with. Yeah, I'm not sure.
Like, oh, did any of that, but that would be further. We don't actually have, like,
limiting, like, it was a constant degree to, like, some graph or something, but I'm asking you,
if in this case, there is something. I don't think so. I don't think so, but I didn't think
very much about it, but I think, I think the answer is no, should, should behave like,
probably like sparse error training graphs. Well, this result, your result shows that you
can separate distinguish right between error training. No, but I mean, in this sense, it's
probably, I'm not sure.
Questions?
Not this. Thanks.
