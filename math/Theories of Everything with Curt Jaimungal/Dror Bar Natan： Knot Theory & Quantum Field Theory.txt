Drawer Barnatin is a professor of pure mathematics at the University of Toronto,
previously specializing in knot theory and more recently specializing in finite type
invariance and Kalvanov homology. We touch on quantum field theory's connection to knot theory,
how to learn mathematics in general. We also touch on Ed Whitten as Drawer Barnatin's advisor for
his PhD was Ed Whitten and Drawer gives some anecdotes about Eric Weinstein as while Eric was
pursuing his PhD. Drawer served as an informal advisor to Eric at Harvard. Professor Barnatin
taught a course in linear algebra that I took while I was at the University of Toronto, although I
was a terrible student and I regret that I skipped the majority of his classes because he had an
exceedingly intuitive approach. For example, there are vector space axioms can I do and they seem
extremely disconnected from any reason or motivation. So what the professor would do would be before
introducing a particular abstraction he would say imagine seven times a vector instead of imagine
eight times a vector and then generalize it which gave the abstraction some concreteness.
For those new to this channel, my name is Kurt Geimangel. I'm a filmmaker with the
background in math and physics dedicated to explicating what are called theories of everything
from a mathematical physics perspective but also the possible connections consciousness has to the
fundamental laws of nature provided these laws exist at all and are knowable to us. This particular
podcast isn't one to be listened to or at least you can but you're going to be frustrated as
what's being referred to our one-dimensional objects embedded in a three-dimensional space
in some configuration and the verbal description leaves much to be desired. If you're listening
to this on Spotify or iTunes, I recommend you click the link in the description and follow along.
You can also download a PDF of the written notes from this podcast in the description as well.
There are a couple sponsors of today's podcast. The first is ALGO which is an end-to-end supply
chain optimization software company with software that helps business users optimize sales and
operations planning to avoid stockouts reduced returns and inventory write downs while reducing
inventory investment. It's a supply chain AI that drives smart ROI headed by a bright individual
named Amjad Hussein who's been a huge supporter of the podcast from his early days. The second
sponsor is Brilliant. Brilliant illuminates the soul of mathematics science and engineering
through bite-sized interactive learning experiences. Brilliant's courses explore the
laws that shape our world elevating math and science from something to be feared to a delightful
experience of guided discovery. You can even learn group theory which is what's being referred to
when you hear that the standard model is based on U1 cross SU2 cross SU3. Visit brilliant.org
slash toe that is TOE to get 20% off the annual subscription. I recommend that you at least try
the free trial and don't stop before four lessons and you'll be greatly surprised at the ease at
which you can comprehend concepts that you previously had a difficult time understanding. If you'd
like to hear more conversations like this then do consider going to patreon.com slash
curtjimungle. There's also a crypto address and a PayPal address if that's more up your alley. I
now do this full time and the patrons are what allow for me to put out this content on such a
frequent basis. Every dollar helps certainly and regardless I'm grateful for your viewership.
Again that's patreon.com slash curtjimungle c-u-r-t-j-a-i-m-u-n-g-a-l. Thank you and enjoy.
I actually had you as a professor I believe in linear algebra.
I hope you didn't suffer too much. My favorite aspect of the course was that you would use the
word seven. You give an example of a number so you'd say a certain field number times a vector.
Well let's call it seven so seven times a vector would be so and so. You know it can get to extremes
right. You may end up doing things like the limit of e to the minus seven seven goes to infinity
is equal to whatever it is zero right. What's your field of research?
Not theory and from a very algebraic perspective and related algebra things. It used to be
much more not theory and quantum field theory and it slowly slowly moved to to being much more
algebraic and the relationship with quantum field theory became more and more remote.
Okay what's the relationship let's forget about the remoteness what's the close relationship
between not theory and quantum field theory? It's a bit hard to explain and so much surprising
and that's why that's why it took a long time to discover. Let me give you two answers that are
somewhat unrelated and one answer is that quantum field theory is about particles
and not theory is surprisingly also about the motion of particles so you know let's draw a simple
minded knot so by the way is this visible my screen yeah okay so here is a simple minded knot
it's actually the so-called trefoil knot but well whatever it is now what you can do is you can
so it's living in in in r3 and I could set up my coordinates so that this coordinate will be called
t maybe you want to call it z but you know I like to call it t right now and the two coordinates
here are x and y and then I can slice this knot at equal time slices so basically I draw a plane
which leaves at some fixed height so at an equal an equal time plane and slice the knot
and then the slice is four points and as I move these points in time so as I move the slice in
time the points move and they actually do a little lovely dance and the dance is as follows so if we
call these points a b c and d then they start at a certain position a b c and d right and then as I move
well you can see that c and b trade places so they play a dance they somehow dance around each other
a b goes this way and c goes that sorry b goes one way and c goes the other way and after a little
while they trade places so you have a c sorry a c b d and as you move the plane even further
they trade places again so again you will have a b c d and if you move a little further again
they trade places so you'll have a c b d and then something funny happens
uh uh points uh well points well by this time this is point b is over here and point c is over
here and point a is here and d is here and what happens is that point c moves towards point d
and point a moves to point point b and they coincide and when they coincide they annihilate each
other because if you look at the plane which is above the the level of the knot um you no longer
see any point so what happens next is that these points move towards each other these points move
to each other they meet and bang they annihilate and after that there is an empty set and in fact
i started at height uh at a certain height had i started below i start with the empty set and
instead of annihilation of pairs you get a creation of pairs so a pair of points get created here
and a pair of points get created here and then they perform this nice dance so this was this
particular knot but the same story is true for every knot every knot can be written as a certain dance
of points in which uh sometimes pairs get created sometimes pair meet and annihilate each other
and in between these times they all dance anyway uh physics in general is about the
motion of particles and it's not too surprising that there is a relationship physics also sees
things like creation and and annihilation of pairs of particles and i mean so again it's not so
surprising but this is very very very loose and gee the other explanation i can give you is even
more loose uh but even but but in some sense more um closer to the truth
uh well okay maybe you've heard that quantum mechanics is related to path integrals
so uh you know in classical mechanics uh you know if a path goes from point a to point b
sorry if a particle goes from point a to point b it goes along a specific path
um in quantum mechanics it has a probability of going uh from a to b and you compute this probability
by doing a certain integral over the space of all paths so the particle or i mean i'm again even
even my interpretation of quantum mechanics here is a bit loose because interpreting quantum
mechanics is a very hard thing but with this loose interpretation you are sum over all probabilities
of going this way or that way or that way or that way and by adding up all these probabilities or
more precisely integrating over the space of all of those probabilities or more precisely
phase factors or um amplitudes uh you you can compute the probability of going from a to b
that's quantum mechanics now uh likewise quantum field theory but in quantum field theory it's
essentially the same except the integrals are more complicated but the point is that already
these integrals are not like integrals over rn right rn is uh the collection of n
tapples of points a1 up to an so a point in rn is determined by n real numbers
whereas a function the space we are integrating over now is the space of all functions and a
function is determined by infinitely many real values you need to know the value of the function
here the value of the function here the value of the function here the value of the function here
the value of the function here so to determine a function you need to know infinite infinitely
infinitely many real numbers.
So the integral that you end up doing
is not 157 style.
Sorry, now I'm using Toronto-style language here.
It's not one variable calculus in integral,
which is, again, one variable integral.
It's not this.
It's not second-year calculus
when you do n-variable integrals.
It's infinite dimensionals, so infinitely many variables
integration.
This is what we need, OK?
So quantum field theory ended up developing a bag of techniques
for computing infinite dimensional integrals.
And what these techniques are is a whole different story,
but quantum field theory ended up doing this.
And then it turns out that infinite dimensional integrals
are useful to topology.
So the way that quantum field theory
becomes useful in topology is indirect.
It's not that the quantum field theory itself is useful.
Even this is a lie, because sometimes the quantum field
theory itself is useful.
But the main path by which quantum field theory
becomes useful to topology is not direct, but a bit indirect.
Quantum field theory developed a bag of techniques,
and this bag of techniques applies elsewhere.
Anyway, why are infinite dimensional integrals
useful to topology?
The answer is, again, a bit complicated.
But let me try to summarize it.
So let me bring a note.
Here's a note.
OK?
Great.
Well, actually, that's not a note.
Why not?
Because what I'm showing you is a curve in R3.
A note is an equivalence class of such curves.
So this is a curve.
If I deform it a bit, it's still a curve,
but it's considered the same note.
If I deform it some more, it's still a curve,
but it's the same note.
So a note is not a single curve, but an equivalence class
of curves.
Now, suppose you want quantities that
are associated to notes.
You want to be able to tell notes apart,
so you need quantities that would be computable from the notes
and that you would use them to tell the notes apart.
OK?
Now, it's very easy to compute quantities
that are associated with curves.
So if you have a curve, you can measure
how much it bends, so the radius of the bending.
OK?
Again, these things have technical names, the curvature,
maybe the torsion.
There are various names for various quantities
you can associate with curves.
So I can look at the maximal curvature of this knot,
of this curve, and the maximal curvature
will be the place where it turns the most,
and that's probably around here for this particular curve,
around here.
OK?
Now, but that quantity is a quantity which
is associated with curves.
It's a useless as a quantity for knots.
It's useless as a quantity for knots,
because if we're talking about knots,
you know, I could deform it, and the maximal curvature
could go somewhere else, could change, could grow,
could shrink, could differ.
Right.
So the way to get quantities that are associated with knots
rather than with curves is to integrate
over the space of all geometries.
So instead of picking the quantity
for a specific geometric manifestation of the knot,
you integrate over the space of all geometries,
and then the result will be a quantity associated
with the knot.
Unfortunately, the examples are very, very hard.
So you know, the famous example, the example
that got written, well, one of the examples that got written
very, very famous in topology, or among mathematicians,
is the so-called Chern-Simons theory,
or Chern-Simons quantity, which is in itself
some integrals.
So you integrate over R3.
So the quantity itself is a final dimensional integral.
Right?
So you integrate over R3 something
called the Chern-Simons form, which is something called,
which is a wedge dA plus 2 thirds a wedge a wedge a.
Whatever that is, but this is related to stuff
appearing in second year multivariable calculus
in the most advanced versions.
OK, so wedge products and differential forms
and exterior derivatives and stuff like that.
Then you take this quantity, and you multiply it by i,
multiply by k, exponentiate it, multiply by another quantity,
the so-called holonomy along the knot of a, whatever that is.
So I'm throwing names because I really have no choice,
because giving the precise definition
is a matter for a graduate class.
Yeah, yeah.
OK, and this is the quantity that you associate
with a geometric knot.
And then to get an invariant of knots,
or to get a quantity associated with the knot itself,
you integrate it over the space of all such a's.
And I didn't really tell you what these a's are,
but it's an infinite dimensional space.
So I guess I haven't really answered your question
by giving an example, but I just told you
the examples are hard.
OK, anyway, well, they're hard, but they're
exactly of the nature that physics knows how to study,
or that quantum field theory developed techniques to study.
And so one can study it using quantum field theory techniques
and get all sorts of quantities relevant to knot.
The so-called Jones polynomial is an example,
and then there are many other examples.
Are there any relationships between knot theory and gravity,
so general relativity, let's say?
I'm not an expert.
The answer is yes, but don't ask me to detail it.
As I told you, I've actually moved away from that subject.
What got you interested in knot theory originally?
This subject.
So mainly you were interested in physics?
Well, OK, I was a math undergrad, but then during my time
as an undergrad and then a master's student
in Tel Aviv University, I never finished the master's.
I moved on to doing my PhD in Princeton.
But during my time as a master's student in Tel Aviv,
I got more and more interested in physics.
And there is just wonderful mathematics inside physics,
so I got interested in physics.
And the fundamental question of physics, right?
What are the basic laws of the universe?
I mean, you can't ask a better question.
So I got more and more interested in physics
and decided to do my PhD in mathematical physics.
And I ended up working with Ed Whitten, who
is a very famous physicist or mathematical physicist
or mathematician, depending whom you ask.
And originally, my interest was in physics.
Or my aim was to do things that were relevant to physics.
Somehow, that's not what happened.
At the end, you don't choose your career.
Sort of the career leads you.
So I wrote a thesis which was somewhere
in between math and physics.
What was it called?
What was it called?
Perturbative aspects of the Chern-Simons quantum field
theory.
And that's an approximate title, because I don't exactly
remember.
But then it turned out that that was
relevant to something called finite type invariance.
And I thought I had new things to say about finite type
invariance.
And that's completely a non-theoretic subject.
So I told myself, I'll give myself a break from physics
and write a paper or two on finite type invariance
and then come back to what I really care about.
And that paper or two became 10 years with several papers,
many papers, and many results.
And then a chance meeting led me to yet another subject.
And you know, what was the other subject it led you to?
Well, no, the next subject after finite type invariance
was Chuvanov homology.
Anyway, you don't choose what you do.
You at most try to steer it, but it steers you much more
than you steer it, like the subject.
Do you have an intuitive explanation for people who,
let's say they're at the second year level,
what is a homology?
What's homology?
It's a tough, a very tough question,
because it's many things, or it's related to many things.
And I could answer it in many levels.
You know, let me give you a summary answer.
Sure.
OK?
So often in mathematics, there are two ways
to describe objects.
You can describe them implicitly by telling what equations
they satisfy, or explicitly by constructing all of them.
Let me give you the simplest example.
So consider the unit circle.
Sorry, I kind of care about the points of the unit circle.
So here is the unit circle in the xy plane,
and here is a point xy.
The implicit description of the circle,
so the implicit is by writing an equation which the points have
to satisfy, and everybody knows what the equation is.
The equation is x squared minus, sorry, plus y squared minus 1
is equal to 0.
OK?
But there is also an explicit description.
And again, I'm not sure which one is why the naming is such,
but it doesn't really matter.
Namely, I can construct all the points of the circle.
And the construction is you set x is equal to cosine of theta,
and y is equal to sine of theta.
OK?
Now, this is a general feature in mathematics,
that many things can either be constructed,
that's the explicit, or be defined by an equation.
Now, if you restrict your attention to linear algebra,
if you do the same thing, but in linear algebra,
then the situation is that you have some space in the middle,
maybe it's a vector space, and inside that space,
you want to describe things.
Actually, I'm not sure why I'm drawing it curved,
because if it's linear algebra, I should be drawing it straight.
But anyway, you want to describe things,
and there are two ways to describe them.
One way is by saying these things,
well, you construct a map into another space,
and you say these things are the things in the kernel of this map.
So I don't know what, let's call this map phi,
and these things are the kernel of phi.
And then, so this is the implicit description.
This is you describe the things by an equation.
And then there is also the explicit description,
so you have yet another vector space,
and you have a map, let's call it psi,
and you say the things here that are,
well, the good things, like the points of the circle,
are the image of psi.
So you say these are the things that are the image of psi.
And this corresponds to the explicit view.
You construct the things, you present them as an image
via some operation coming from some,
you know, maybe some smaller dimensional space.
OK.
So, OK, here's a funny thing.
So if your construction is reasonable,
then everything you construct solves the equations.
In other words, everything in the image of psi
is contained in the kernel of phi.
So once you have a situation like this,
you know that you're going to have that the image of psi
is contained in the kernel of phi.
And in fact, if you're constructing the exact same things,
then the image of phi will be equal to the kernel of phi,
not just contained, OK?
And then you will be saying that the situation is exact.
Things got exactly the things that you've constructed
are the things that you described by the equations.
But that's not always the case.
Sometimes you have equations defining something,
and you want to know, did I construct all the solutions?
So sometimes this inclusion is strict.
And then you want to measure by how much or how much did you fail?
So how much did you not construct everything?
And that's exactly the difference between the kernel of phi
and the image of psi.
And in linear algebra, the difference is quotient.
So you take the quotient space.
And this is the so-called homology.
So somehow, homology measures, so this measures,
how good are your constructions?
Did you construct everything, or did you leave some stuff out?
Is the kernel, the implicit, the description,
does it contain more things than you've constructed?
So somehow, homology measures, how good are your constructions?
And this is true in general.
And then there are lots and lots and lots
and lots of specific examples.
And to some extent, it's a bit of magic.
I'm not sure I can give you a really good explanation why
there are so many examples and why so much of mathematics
end up having close relationships with homology.
Maybe this is a partial explanation.
Maybe this is a partial explanation.
So basically, you always want to measure
how good is what you've done.
Are you able to talk about what cohomology is,
and then homology's relationship to cohomology?
Or is that a whole other kind of worms?
It's another kind of worms.
But in some sense, it's not interesting.
In some philosophical sense, it's not interesting.
So when I describe cohomology, it's essentially the same.
So the difference between a homology and cohomology,
more than anything else, is a difference of conventions,
whether your map goes from left to right or from right to left.
It's from this perspective, from this very far away
and very imprecise perspective, there's really no difference.
Professor, what do your average days look like?
For example, do you wake up at the same time?
Do you drink the same type of coffee?
And it has to be this particular type of coffee?
Do you eat at a certain breakfast?
What does it look like?
Paint the drawer picture.
Well, I'm a lazy bum.
And I'm a believer in the, at some point, a revolution.
Before that, people used to work seven days a week,
12 hours a day, or maybe with a short break on Sunday morning,
depending which culture you came from, I suppose.
And then at some point, there was this worker's right revolution.
And somehow the working week shrunk to 40 hours a week or so,
like five days of eight hours a day.
And I'm essentially a believer in that.
And furthermore, I'm a lazy bum.
So during my eight hours of work a day, five days a week,
I take coffee breaks, I browse the web, I cheat.
But anyway, I wake up at 6.30, I exercise, take a shower,
have breakfast, check my email, do all sorts of unimportant things.
And then around nine, usually, I start working.
I work until five or so.
I'm sorry, I'm very dull.
And then I try to have fun for the rest of the day,
whatever that might be.
During the pandemic, it is a bit harder.
And then on the weekends, I try to go biking, hiking,
puddling, mostly things that don't have to do with work.
I'm lying a little bit.
Because occasionally, there is a reason to work on the weekend.
And occasionally, I do have a meeting with somebody,
like I have an Australian collaborator,
so often our meetings are in the evenings.
So I mean, I've given you an idealized story.
The truth is a little different, it varies.
When you're working from nine to five,
how much of that time is spent thinking versus reading someone
else's paper versus answering departmental emails and requests?
Like, what is work?
Because work, obviously, for a professor, you have to teach.
You also have to do research.
There's perhaps 10.
Work is all of that.
And it depends on the season.
So basically, when I teach two courses, so you know,
there's basically fall, spring, and summer.
You know, fall, semester, spring, semester, and summer.
OK?
In the fall and the spring, I teach either two or one course.
Per semester, in the summer, I don't teach.
So when I teach two courses per semester,
that's almost all my attention.
Almost everything I have goes into that.
And the rest goes into the regular meetings
that I have to keep.
So I have several graduate students
that I work with, several collaborators,
and I'm trying to keep weekly meetings with them.
So there is a certain amount of weekly meetings
that I have to keep.
When I teach one course, I have some breathing room.
And when I teach nothing, namely during the summer,
then a higher proportion goes into research.
Even in the summer, the proportion is not very high.
Firstly, is your ideal situation
to simply be a research professor?
I know some professors don't.
They treat teaching as an aside.
They don't particularly like to do it.
I'm curious about your case.
Well, ideologically, I mean, I see myself as a teacher.
Even the research, in some sense, is to support teaching.
Even the research is so that I will remain in shape.
And even more so, and that the community will remain in shape.
So if I do research, I am able to teach graduate students
how to do research.
And then they become stronger mathematicians
and can teach mathematics better.
So I even see research as mostly supporting teaching.
So I definitely wouldn't say that I see everything as research.
I mean, research, again, teaching is very significant.
Well, that's the theory.
In practice, waking up every morning
and preparing your class and this marathon of teaching,
where you have to prepare your class,
you teach, you feel terrible about the things you missed during class.
You don't even have time to digest it
and you have to get ready for the next class already.
So, you know, it's very hard.
I mean, people who haven't done that
don't appreciate how hard it is.
But it's emotionally very, very difficult.
And after a semester of teaching,
you have no idea how much I'm looking forward to the break.
So it's not like teaching is, you know,
when I teach, I want out.
Do you find that to be true even if it's an undergraduate course?
The reason I ask this is that I imagine, as a professor,
the undergraduate material you could teach with your pinky.
You don't have to think much about it
because, first of all, you've mastered it.
Second, you've taught it for years and years.
I do not teach the same class for years.
Like, I think the maximum I've ever repeated a class was like three years.
So it's not.
But even then, you always have to prepare.
You always blunder. I always blunder.
Maybe perfect people don't.
But, you know, I always blunder here and there
and then I have to figure out ways out.
I always find new things to do, new ways to do things.
I always, even when I repeat,
but most of the time I don't repeat.
And the material is sometimes hard, you know?
And even when it's not hard,
so, you know, on day one of a class,
you teach of a linear algebra class,
you teach the axioms of a vector space.
I obviously know these axioms, right?
I obviously, when I see a vector space, I know it.
When you give me a vector space, I know what to do with it.
But that's not the same thing as getting up in front of a class
and listing the axioms and making sure
that you haven't forgotten one small axiom somewhere
or that you've raised them perfectly
so that even people who have never seen these things
before will understand.
So, I mean, even day one of a very basic class
requires preparation.
We're going to talk about mathematical techniques.
In fact, I believe when we emailed, you said,
you don't like the emphasis on theorems and proofs.
You like, well, we can talk about that later.
As for teaching techniques, I'm curious, what have you gleaned?
What have you found to be productive
and perhaps even some behaviors you find to be unproductive
that you see other people, other teachers,
making the mistake of doing?
I mean, there is so much.
Starting from completely silly things
like make sure that you're heard.
If the classroom is large and you speak without a microphone,
people in the back are not going to hear you
and then they're not going to follow.
Make sure that you can be seen.
So, if you write tiny letters with a faded chalk,
people can't read what you write.
The class is a failure, no matter what you say.
And then there are higher level things
and basically everything should be motivated.
So, I mentioned the first class in a linear algebra course.
So, you can give by this class by saying,
you can start this class by saying,
let F be a field.
A set V is called a vector space if,
and then you can write property,
you know, operations and properties
and you write item number one to item number 15,
where 15 is, I don't know, some axioms like alpha times,
beta times a vector, you know, whatever.
So, one of the axioms of a vector field,
of a vector space,
and then the question arises why and who cares?
Right, right.
Okay, so, if you start your class this way,
people aren't going to like it.
So, before this, there has to be a little section,
I don't know, here, where you explain,
well, for such and such reasons, we care about vector spaces.
So, there has to be motivation first, okay?
And that's true not only in the first class
of a linear algebra course,
it's true in every class.
Whenever you say something, there has to be a reason
and the people who are listening to you
should know that you've thought about it,
should know that you're telling them these things for a reason, okay?
And that's often omitted.
It's often too easy to just follow the definitions
and the theorems and never say what they're good for.
And not only you should say these things,
you should say them upfront.
Likewise, when you write a proof,
it's often easy to, well,
often there is a picture behind a proof.
The proof is telling some story
and you want the picture of that story in the minds of the students.
But it's often too easy to reduce it to a list of formulas
or a list of statements.
So, a higher level thing is to make sure that everything is motivated.
What else? What's another higher level aspect of teaching
that you find is a common mistake among the teachers that you see?
I think maybe this is the biggest,
not showing the motivation clearly enough.
And, by the way,
that's part of the reason why research is a necessary part of teaching,
because you have to know the motivation.
You can say vector spaces are a great thing
unless you've actually worked with them,
unless you've actually seen them used.
So maybe vector spaces are easy enough
so that you can, by third year,
you understand why you learned about them in first year.
But if you want to be able to say something intelligent about homology,
you have to have used it.
The only way to use it is in research.
So, that's what I meant by you have to be in shape.
Being in shape also means
remember what can be done with these things.
Do you find that you're more creative as you get older,
or that your creativity peaked at a certain age?
You know, I wish I could answer.
The truth is that I don't really remember.
You know, I mean,
it's very hard to remember myself at any other age.
So, I mean, certainly I have matured,
so the little I can remember from 20 or 30 years ago
is that I have matured, okay?
I'm more systematic, perhaps, than I was.
Am I smarter? Am I more creative?
Am I stupider? Am I less creative? I'm not sure.
What do you mean when you say you're more systematic now?
Can you give an example of how you would look at a certain problem now
or solve it versus 20 years prior?
And also, a sub-question, do you see being systematic as a good thing,
or do you see there being innovation in being disorganized
or creativity embedded there?
No, systematic is generally better.
So, I used to be much more into the low-hanging fruits.
So, and I was lucky that I found some.
But basically, I'd wait for opportunities and then use them, okay?
And now it's much more, I have a long-term goal.
You know, I want a theory of something.
You know, your podcast is a theory of everything, right?
I just want a theory of something.
I want a theory of something because I feel for various reasons
that it ought to be there.
And then I worked very hard on, well, very hard to my standards.
I already admitted being a bum.
So, you know, I worked at least for a long time
trying to find that theory.
So, even at a much lower level.
So, I think when I was first year student, you know,
math theorems and proofs were pressed into my brain.
It was essentially an external process, right?
So, I went to classes and people pushed theorems into my brain.
And some of them seemed like complete formalities,
like a sequence of deductions that I could have,
that I could memorize and I could verify,
but were essentially created by aliens.
I had no idea how they came to be.
And some theorems immediately became a picture.
I immediately understood that the proof describes
a certain geometric reality.
I could see the intuition behind the proof.
It's not always geometric though.
It's just you see the intuition.
Not necessarily geometric.
But I could see somehow the motivation.
I could imagine how the proof was discovered.
I see.
I would get the feeling that I understood the proof.
And that was essentially a dichotomy.
Some proofs or some concepts were of that type,
made by aliens and pushed into my brain by professors.
And other concepts, theorems, proofs, whatever,
were clearly intuitive and made sense,
and I understood them.
And that was it.
That was the world.
These and there were those.
And then sometime later in second year,
I actually did my BA very, very, very quickly.
So basically I did it in essentially two years.
So when I say second year,
you should compare it maybe with year three and four of most students.
So somewhere later in late in my studies,
I realized that if I think harder
about the theorems or the concepts that appeared to be alien,
then sooner or later I see the picture behind them.
Sooner or later I realize that I am able to move them to the other class.
The class of things that are motivated and understood.
And then I became systematic about it.
So every time I learned something new,
I may learn it from a book that describes it as a formula.
And then I'll struggle to see the intuition behind the formula.
And so even at that level, it's something that took learning.
Again, when I got to the university,
when I was in first year,
I took classes and I got good grades in all of them.
And nevertheless, there was a point where things became better.
Where I realized I can actually understand everything
and not just repeat everything.
And that came later.
What does that process of finding the intuition behind,
let's say a formula that before you previously simply had to memorize
as handed down from the gods or the aliens?
What does that process look like, the process of finding the intuition?
Does it look like, I'm sure it's a variety of these,
but does it majorly look like you copy it down with the pen and paper precisely
and then you draw arrows and you say, okay, so what's an example of this?
What's a counter example of this?
Or is it you go on a bike ride and you keep thinking about it?
What does that process look like?
Perhaps it would be edifying if you took a specific example.
But if you can't, then just give the broad strokes.
What does it look like from the outside?
From the outside it looks like I'm sitting on my chair and staring into infinity.
Or sitting on my balcony and looking at the view.
What does it look like inside?
It's very hard to answer without going through specific examples
and the specific examples are far too complex.
I don't know if they are far too complex, but they fit in a math class.
And the other thing is you will never see these examples in my classes
because when I teach something I try to motivate it.
So you will not see the alternative.
You will see the motivated way of explaining the implicit function theorem
or the change of variables formula for multidimensional integrals.
You will see the motivated form.
You will not see it.
I will not be showing you the unmotivated formulas.
For you much of the time when it's difficult for you to understand a proof
a theorem or a concept is it most of the time because it was unmotivated?
Well, the intuition is not explained often.
So in textbooks often the intuition is not explained.
Now some intuition is just getting used to the concepts.
So sometimes intuition is not just playing around until you are used to it.
You know, you've just landed in a foreign city.
You've just landed in Athens.
Your hotel is two kilometers from the Acropolis.
You got there at night.
You wake up in the morning.
You want to go to the Acropolis.
You punch Acropolis into Google Maps and Google Maps tells you,
you know, go left, go right, go straight, go left, go right and here's the Acropolis.
You follow the instructions.
You didn't learn Athens.
You've learned the way to the Acropolis.
To learn Athens in the afternoon after the Acropolis you have to wander around
and then you have to continue wandering around for the next two weeks
and then maybe you're starting to have a grasp of what Athens is.
So there is similar, there is something similar in mathematics.
Or in science, or in fact in every field of learning.
But in mathematics, you know, you don't learn...
I mean, if you learn a proof of something,
if you learn the proof of the most interesting result in a third field,
you haven't learned the field.
Okay, you've just learned, and especially if you follow the directions by somebody else.
Okay, the learning happens if you wander around later
and in a way you have to do it on your own.
Right, this wandering around.
So wandering around in mathematics means you experiment, you change the conditions,
you ask yourself what if you remove, what if you add conditions,
what happens if you look at analogous situations.
But these things...
Well, so like part of tourism you do with a tour guide
and a part you have to do on your own.
And so some part of being systematic, if you wish,
is doing this on my own thing in a systematic way.
Okay, this realization that I can't just jump into a topic by following somebody's guidance.
I must sit down and think and ask myself many, many, many, many more questions
and go on sidetracks until eventually I'm comfortable.
Now in this example, let me see if I'm understanding it correctly.
Let's say Athens, Athens is the city, and then you have the Acropolis, which is a landmark.
So that's almost like a landmark theorem and then the directions are the proof and the steps to get there.
And you're saying the learning comes from the wandering.
Would you say that when you first land in Athens, you should wander?
Or should you learn the major landmarks and the directions to get there?
Perhaps forgetting the directions.
You just get yourself there so that you see what the landmarks are like later you wander.
The analogy would be you learn the major theorems and then later you try and get the intuition by playing around with them.
Or do you play from the get-go?
I mean, there has to be a bit of both.
Basically, if you land in Athens and start wandering randomly, you may end up wandering randomly in a completely uninteresting direction.
Plus there is a fear factor and you may fear the bad neighborhoods.
Interesting.
But there actually is an analogy in mathematics.
I mean, you fear doing certain things and the guidance can help you overcome the fear.
But maybe fear is not the right way.
I mean, think that you have to explore Athens when it's dark.
Okay, so you simply don't know which way to go.
You just land.
I tell you, think about vector spaces.
You have no idea what good questions to ask are.
What are the good questions to ask?
So you need guidance or else you will be stuck in one neighborhood and never realize that there is more than that.
But you know, after the guidance took you from here to there, you have to go back and explore and explore more and explore more and until it's a solid and not just a single path.
So you have to do your own Feynman path and to grow over the space of mathematical ideas?
There is a bit of that, yes.
What's your greatest strength as a mathematician or some of your strengths?
Who says I have strengths?
What's your greatest weakness?
I'm a lazy bum.
Are you being modest when you say that?
Because I imagine someone who has an accelerated degree, a two-year degree that should ordinarily take four years,
it's not as if you spend the majority of your time playing video games and being unindustrious.
You'll be surprised.
I mean, of course, I didn't play video games.
I was back in the 80s.
Video games came later.
But I definitely killed a lot of time.
So who are you comparing yourself to when you say that you're slothful?
Gee, I compare myself to who I want to be.
Anyway, what are my strengths?
I think I'm decently good at explaining things to others.
Often that involves thinking about them.
I mean, explaining is not just reproducing what you've heard somewhere else.
I mean, it's first of all being creative about these things.
But you know, ask the students, ask the people who listen to me in lectures when I give research lectures.
It's very hard to...
There's a famous story about Einstein that somebody complained to Einstein about his problems with math.
And the reference was to high school math.
I never got high school math.
And Einstein said, let me assure you that my math problems are way bigger.
Meaning what? What did he mean by that?
Einstein spent all his life struggling with mathematics.
Not just high school time.
Okay, so in a similar way, Gee, I'm sorry.
I mean, it might sound like I made some analogy which I'm not making.
I understand, trust me.
But still, you know, I like to implement...
I like to compute things, implement them on the computer.
Which means that I spend a lot of time programming the concepts I work on.
Turning them into running programs.
You could say it's my strength.
But when I look at it, all I see is that I'm struggling and struggling and struggling and failing.
And here and then, here and there, there is a minor success which gets superseded a little bit later by somebody else.
So, yeah, maybe one of my strengths is the ability to implement things.
To actually, you know, turn mathematical concepts into running programs.
But it's also one of the things that keeps me depressed so much of my time.
Some people who are watching this, they may be discouraged because they weren't particularly great at high school mathematics.
Or some people may have struggled during their undergrad.
What advice do you have for those people?
If they want to continue to learn mathematics or physics at a graduate, perhaps even larger than graduate, Ph.D. level.
Do they just continue plugging away and not feel discouraged because they feel like an imbecile on a daily basis?
What advice do you have for them?
There is chug along and there is realize when it's not working.
And I hate to tell you that I don't know.
I mean, I can't give you a clear recipe to decide which one it is.
Different people are different.
Some people struggle and struggle and struggle until they succeed.
Sometimes some people don't.
Or they struggle and struggle and struggle and don't succeed.
You get feedback.
I mean, some people depend on daily feedback.
Some people can live with a bit of feedback once a year.
I'm not sure I have a clear answer.
What did you learn from Ed Whitten with respect to mathematical techniques or meta-mathematical, so how you should think about math or physics?
Well, I learned specific mathematics from him.
I learned like many other people, I learned this formula from him.
And how to read it and how to use it, interpret it, work with it, do things with it.
But I learned many other specific things with him from him.
I learned from him.
People think of him as a great researcher, but he's also a great expositor.
So when you come to his lectures, he is able to make even very complicated things appear simple.
The problem is that often you will not see that because he's turning the hyper-complicated into merely complicated.
So you don't automatically realize what a great exposition work he had done.
And I think I learned, well, I don't know if I learned that from him, but I strive to learn that from him.
And then in other ways, there are senses in which I didn't learn anything from him.
Namely, he is so much more talented.
There's just no other way of saying it.
The guy is smart.
The guy is extremely productive.
If you try to imitate his brilliance, you will fail.
At least I failed.
If you try to imitate his productivity, you will just be disappointed.
So in some ways I haven't learned.
There are certain things I haven't learned from him and I just simply couldn't.
Do you mind describing your relationship with Eric Weinstein?
When I was speaking to Eric, he said that you acted as a de facto supervisor to him.
Because Raul Botte wasn't particularly, although it was officially his supervisor, he didn't act as the supervisor for whatever reason.
It was a strange relationship.
I don't understand it.
And I'm curious, what do you think he means when he says that you acted as the supervisor?
You gave him advice or?
I don't think it's correct.
It's more like a coach.
So Eric was very unbalanced.
And he still is unbalanced.
And I don't mean emotionally unbalanced.
I mean, he has great talents in certain directions and fear of others.
And there are certain things that one has to do, certain type of computations, certain type of writing.
Basically, he has fear of turning his theories to concrete.
And that's part of the reason why he had difficulty with Raul Botte and basically with the normal system in Harvard.
And my role was to help him pick something concrete out of his big theories and push it enough so that it became a thesis.
And I also helped him with some computations.
At the end of the day, there was some, like, so his big theories lead to something very concrete and specific, which was a tiny part of the big theories, but we extracted that tiny part and made it more concrete.
And then it became a question, you know, some big matrix, 500 by 500 or whatever it was, is it positive definite or not?
And I helped him, I helped him decide that.
And that was decided by a computer, like you ended up programming it?
That one was decided by a computer.
But that's the kind of thing he would have never done on his own, because he has this fear of, in a way, it's a fear of testing your theories, because they might fail.
But in another way, it's just inability. He never learned how to do it.
He never got good at turning the very abstract into concrete.
So, yes, so that was my role.
Have you read his recent Geometric Unity papers or heard about Geometric Unity at all?
I have heard a tiny bit about it. I have not read it. I'm not in a position to comment.
You talked about it's difficult to make a concrete or take something concrete from a theory, at least in his case.
Do you mind giving an example of his strengths or an example of a brilliance of his that you saw?
He intimately knows the small groups and their representations and how they interact with each other.
What do you mean by the small groups?
There are several families of so-called symmetry groups.
So, there are S-U-N and S-O-N and S-P-N, and then there are a few sporadic ones.
There is G2 and F4 and E6 and E7 and E8, and it doesn't go beyond 8.
So, these are abstract things. These are groups.
They have so-called representation theory.
So, each one of them can be written as a group of matrices, can be realized within the world of matrices, but in many ways.
So, Eric intimately knows these groups and their representations and how they sit inside each other for small values of N
and well, for all of those sporadic ones, which are all small.
And there are symmetries and there are so-called outer symmetries.
I mean, he knows all about these things.
Now, this is relevant to physics because, you know, electroweak theory is somehow about S-U-2 and the grand unified theory.
So, strong and electroweak is somehow about S-U-3.
So, no, no, no, no, no, I have it wrong.
Sorry, electromagnetism is, well, electromagnetism is U1 and then, you know, as I told you, I actually have not thought about these things for many, many years.
So, I could be saying things wrong, but that his strength is that he knows these things really, really, really well.
Or one of his strengths is that he knows these things really, really, really well.
Earlier when we were talking about particle physics, well, not theory.
You mentioned that for every knot, there's a corresponding physics story of particles dancing and so on.
Is the opposite true? So, does the arrow go in the other direction as well?
For every particle interaction, is there a corresponding knot?
For every particle interaction of a specific type in three-dimensional space, yes.
But you have to be very specific. You have to be, you have to specialize a great deal.
Physics mostly happens in four-dimensional space.
If this can be easily explained, why is it not simple to just extend the three dimensions to four dimensions and come up with answers,
much like in vector, we're talking about vector spaces, it's trivial to go from R3 to R4. The same theorems apply.
So, the answer is, first of all, certain things do extend.
So, you know, if knot theory is to some extent about the dancing of points in two-dimensional space,
so, you know, here is a very rough kind of correspondence.
So, knot theory, or ordinary knots, is roughly about one-dimensional things, or curves, or whatever, in three-dimensional space.
And as I mentioned before, it can be interpreted as points dancing in two-dimensional space.
So, points are zero-dimensional, so you can interpret knot theory as zero-dimensional things.
Right? The single point is a zero-dimensional thing. So, it's zero-dimensional things dancing in two-dimensional space.
Now, you can lift this one-dimension up. So, you can say, let's, instead of zero-dimensional things dancing in R2,
you can think of one-dimensional things dancing in R4.
And one-dimensional things dancing in R4 somehow correspond by the same correspondence, looking at sections,
they somehow correspond to two-dimensional knots, so two-dimensional things are noted in R4.
Now, you can ask, now, what exactly are one-dimensional things dancing, sorry, not R4, I wanted to add one.
This is in R2, adding one, you get R3. Sorry. So, one-dimensional things dancing in R3.
Now, you can ask yourself, what are one-dimensional things dancing in R3? And fortunately, I'm ready for that.
So, here is a one-dimensional thing. And here is another one. And, right, this is a loop, it's a one-dimensional thing.
That's another one, it's a loop, it's a one-dimensional thing. And they can dance around, they can fly in R3,
and they can fly in interesting ways that don't exist one-dimensional less, namely, they can fly through each other.
So, yes, there is a theory of one-dimensional things dancing in R3, and it corresponds to a theory of two-dimensional spaces
noted inside four-dimensional space. So, to some extent, the answer to your question is yes, there are higher-dimensional analogues of knot theory.
It turns out that some features are different. It turns out that in some ways, and they're actually qualitatively,
the theory of knots in one, unlike vector spaces. So, vector spaces, once you learn R3, R4, you've learned R to the 50.
Okay, but here there are qualitative differences. So, some things from the study here translate to things here and vice versa, but not everything.
So, myself, I've spent most of my time actually on these two. Okay? But then there is higher, there is even higher stuff.
By the way, so, you know, linear algebra, when you take a first class in linear algebra, and maybe also a second class in linear algebra,
you get this impression that everything is the same in all dimension.
Yep.
But that's not true.
Even in linear algebra?
Even in linear algebra. So, well, not quite linear algebra, but in algebra. Okay?
So, you know, there is something called the real numbers, and then you can add i, add i, and you get the complex numbers.
And then you can say, let's add j and k as well, and you get the so-called quaternions.
But the quaternions are different than the complex numbers in that this is a commutative ring, and this is a non-commutative ring.
And then you can add, gee, I forgot what's the standard name for what you add.
So, I don't know, add in stands for nameless, because I forgot the name.
And you get octenions, and the octenions are no longer even associative.
So, it's still sort of a ring, namely it has a multiplication, but it's not even associative.
And then the process ends. There is just nothing beyond that. You can't continue.
So, even in linear algebra, so that's not quite, well, I don't know, even in basic algebra.
Not all dimensions are the same. One is not the same as two, is not the same as four, is not the same as eight, and not the same as sixteen, where things no longer exist.
So, it's just not true that things are independent of the dimension.
They're only independent of the dimension in the first two years of undergrad.
Now, in this example, we're going to different spaces by adding one, or by adding whatever extra variables there are there.
In the previous example, we're extending r4 to r5, or rn to n plus m.
So, we're in the same space, in a sense, except adding an extra dimension.
So, what's an example that holds for r3? Or actually, sorry, let's make this even simpler.
What's an example that holds for r2, but doesn't hold for r3?
Let's say r3 to r4. What's an example of a quantity that drastically changes as soon as you go to r4 and becomes extremely complicated, but in r3, it's trivial.
The motion of points is fundamentally different in two dimensions versus in three dimensions.
So, let me explain.
Suppose you have two points, a and b, in two dimensions, and you want them to trade places by moving.
There are two basic ways they can do it.
They can trade place by b going above a, or they can trade places in sort of the opposite way by a going above b.
So, a will go to here and b will go, sorry, a will go, a will go take the upper root and b take the lower root.
These two ways are somehow fundamentally distinct.
And to use technical language, you cannot homotop this way to that way. They're not homotopic.
You cannot continuously deform this way of moving to that way of moving.
But in r3, that's not true anymore.
So, in r3, points can trade places in only one way.
So, let me find two points for you.
So, I suppose they're not points, but they're close to points.
So, here is red and blue.
So, in r3, they can trade places going this way, or they can trade places going that way.
But there are many other ways they can trade places.
They can trade places by going that way or that way.
And in a sense, all of these trading places ways are equivalent to each other.
Because if you trade places, if the two points trade places, they span a circle.
So, basically, let me use this, right?
So, you know, they span a circle by, I wish I had a third hand, maybe.
Sure, sure, sure, sure.
Okay, they go like that. That spans a circle.
And the other way of spanning a circle is by going the other way around the same circle.
But now I can flip that circle over and that homotop that deforms one way of trading places to the other way of trading places.
So, in r3, so, in r2, there are two distinct, sorry, this can spell,
there are two distinct ways of trading places.
And in fact, two are easy, they're actually a little bit more.
But anyway, in r3, so, if you're talking about three-dimensional space,
and you have two points in the three-dimensional space, there is only one way,
or one, or all way, or let me say differently,
all ways of trading places, all ways of trading places are equivalent.
Again, the basic point is that a way of trading places is a circle,
where one point goes along one arc and the other point goes over the other arc,
and you can switch from one way of trading places to the other by flipping the circle.
You can't do it if you're only in the plane.
Since trading places is actually fundamental, it makes, by the way,
it's also fundamental in physics.
So, you know, you might have heard of bosons and fermions and how they behave
with respect to trading places, and then there are anions that behave yet differently
with respect to trading places.
So, basically, trading places or how things behave under trading places
is a fundamental thing, and it's different.
It's different in two dimensions than in three.
Right.
Likewise, there are things that are different between three dimensions and four.
And so, it's actually this kind of myth, or not myth,
but this thing that you learn in first and second year that
Rn is the same Rn, no matter what n is, is not actually true.
Have you heard of the hard problem of consciousness?
I have not heard it in this language, namely...
No, no, no, it has nothing to do with what's written on the board.
Or maybe it does, but not as far as I see it.
This is sort of a clear philosophical problem, right?
But with the added prefix, and it's clearly hard,
but as a phrase, the hard problem of consciousness,
I don't know what you're talking about.
There's only one consciousness that I need to explain, and it's my own, right?
From my perspective, you're not conscious, right?
You just react to things that I do.
If I tell you you're stupid, you get angry at me.
If I give you a compliment, you get happy, but these are all...
I don't see consciousness here.
So, there's only one consciousness that I have to explain,
and only one person that I have to explain it to, and it's myself.
But anyway...
Having said that...
Yeah, I'll keep the rest private.
It keeps you up at night.
Different things on different days.
Whether she loves me or not.
Whether I can complete the computer program I'm writing or not,
or why is it not written.
My classes for tomorrow, the theorem I don't know how to...
I don't know how to prove the state of the world, COVID,
different things at different times.
Is there a through line?
So, let's say one that you've been thinking about for 15 years or more.
You know, I'm politically aware and always been.
I've always been.
So, how to make the world a better place.
But of course that's a subjective thing.
What is better to me may or may not be the same as what is better to you.
So, in a way, when I'm saying how to make the world a better place,
it's more like how to make the world more like how I would like it to be.
Yeah, so I think about it a lot and mostly my conclusion is that there's nothing I can do,
or there is much I can do.
My powers are limited.
What direction would you like math research to head?
I think I mentioned that I like computation.
I mean, it's not the only thing of value in mathematics.
There are things in mathematics with great philosophical value,
regardless of whether they're computable or not.
But those parts of mathematics that can be computed are often not.
And often people just don't care about computations.
They pretend that, well, they think of computation as something,
as a technical thing to live for the graduate students.
And I see it as a much more integral part of the work.
So, if I were to change, I would make computation a bigger part.
Computation in the sense of implementation on the computer,
not in the sense of writing by hand.
I would make it a much bigger part.
And that happens at many levels.
So, when you teach a class in linear algebra,
you teach about how to compute the determinant.
Well, along with it, there should be a computer program to compute the determinant.
And there should be a realization that there are maybe two formulas for the determinant,
but one of them is way better than the other if you try to implement it.
So, I would emphasize computation much more.
Yeah, maybe that would be the main change.
Do you have a philosophy of math?
So, for example, there's Platonism and Formalism.
Anything I would be an ultra-finitist.
But it's much more graded than that.
So, you know, there is a hierarchy of complexities of things,
or sizes of things in mathematics.
So, you know, people in set theory and logic talk about large cardinal axioms.
And then there are parts of mathematics that have to do with uncountable or bigger than uncountable.
And then there are infinite parts.
And then there are just infinite without being uncountable, right?
And then there are parts of mathematics that are finite but huge.
So, you know, Ramsey theory, you have, you know, you prove that if a graph is large enough,
then it has properties, certain properties.
But large enough means really, really, really, really, really, really, really, really, really huge,
bigger than the number of atoms in the universe.
And then there are, you know, parts of mathematics that happens where the numbers are between 10 to the 6 and 10 to the 12.
I don't know what a computer can do in a fraction of a second to what a computer can do in an hour, okay?
And, you know, maybe you can go a little bit beyond to 10 to the 16.
I don't know, 10 to the 15. I don't know, okay?
So, there is this whole grading, and to me, the bigger things are, the less important they are.
And because they're farther away from our everyday experience.
So, they may have philosophical value and some of them do.
So, I don't want to completely say anything about large cardinals is not interesting,
or anything about infinity is not interesting.
But the bar is higher.
For something that genuinely uses infinity to be interesting,
the bar is higher than if the thing is in the range of sizes that we can actually hold in our hands.
So, that's what I mean by an ultrafinitis.
I mean, sort of the, or so this is a form.
So, an ultrafinitis would say the only numbers that exist are the numbers I could count to.
And maybe by extension that my computer could count to.
So, maybe my computer can count up to this number, but 10 to the 100 does not exist in some sense,
because my computer will never be able to count there.
Okay, will never be able to get there.
So, I'm not extreme.
I would say sometimes there is value to this one too.
And sometimes there is even value to infinity.
And sometimes there is even value to the bigger form of infinity, but the bar has to be higher.
The philosophical value bar has to be higher the bigger the sizes.
These things are always interesting.
These things are interesting only on special circumstances.
Now, I'm not sure.
Do you classify yourself as a pure mathematician?
Definitely.
Okay, so for a pure mathematician who seemingly doesn't care about reality, it's supposed to be...
Again, the more valuable math is the math that I can turn into a computer program.
It's still pure math in the sense that I'm not looking for an immediate or a specific application.
But both from the application perspective and from the pedagogical perspective,
I still prefer the math that can be implemented, because the math that can be implemented,
even if I'm not looking for an immediate application, is the math that has a higher chance of being applied.
Because in the application, you end up computing.
And also, from the teaching perspective, from the pedagogical perspective,
there is more value to the education you give if it comes with implementation,
because you want your students to be able to implement the things that they learn.
Yes, sorry, what were you asking?
Oh, I was saying, you ultimately care about the practical aspect of math,
even though you're operating in pure mathematics?
Ultimately, yes, though possibly very indirectly.
So maybe the way I'm doing, or maybe the way I care is in that the pure math I'm doing
enables me to teach graduate students who will then become college instructors,
and their students will be able to actually implement the mathematics they're learning.
So I mean, it doesn't have to be immediate and direct,
but yes, I do care about how at the end it gets applied, or how at the end it becomes applicable.
Great, now we just have some audience questions I've asked people beforehand.
What are some questions you have for Professor Jorbar Netten?
So the scientific mystical philosopher, that's the username he wants to know, he or she wants to know,
why the unreasonable effectiveness of mathematics?
Sorry, why is mathematics unreasonably effective?
I don't know, ask God.
She may or may not be able to give you an answer.
Yeah, it's weird, we live in a predictable universe.
Right, when we throw something up in the air, it always goes through the same trajectory.
Nearly, I don't know, I mean, that's a miracle.
Roman Gekwad wants to know, have you ever had your IQ scored, and if not, what do you believe it is?
No clue, and also IQ scores are very limited in many ways.
There is a fun story that I've never verified it, but I heard it.
That when the people who introduced IQ tests first introduced them, they ran them on test subjects,
and the women, the females, got higher scores than the men.
That was back in the 50s, or I don't know exactly when, but that was a long time ago.
They figured, hey, that's impossible.
And then they tweaked the tests a bit, they changed the nature of the questions, they asked them differently,
until they finally were able to make men equal, or maybe better, I don't know, than women.
And again, this is true to a certain system of education in a certain time, right?
Men and women did not receive the same education.
It would have been easy to distinguish, because they came as their backgrounds were different.
So you could arrange such things.
That's about the value of IQ scores.
I mean, to some extent they measured something, to some extent they were cooked to measure whatever people wanted them to measure,
not much to learn from these measurements.
And therefore my experience of the universe, do you think you would gain much of an understanding of an exploration from those fields to mathematics?
Again, biology?
Biology, psychology, and philosophy.
I mean, they're all interesting fields, I'm not sure what more to say, you know?
Do you think there are insights there that if you were to read a biology book, a psychology book, perhaps a philosophy book,
do you think there are insights to be gleaned from there that you could then apply to mathematics?
There are connections here and there.
Much of mathematics is independent of that.
Obviously, biology motivates parts of mathematics.
So parts of mathematics are about modeling biological systems.
When we compute the Fibonacci numbers, we give examples of the reproduction rules of rabbits.
And then there is much more like that.
Likewise for psychology, and of course philosophy.
So, you know, there are bits of mathematics of great philosophical value.
So, you know, Gedel's theorem says that there will always be things we will not know.
Or differently, you know, there is no algorithm to decide to find the proofs of theorems.
These things have philosophical value.
For most mathematicians, it's not a day-to-day working relationship.
My own mathematics benefits very little, if any, from contact with biology, philosophy, and psychology.
Okay, Phil Thomas wants to know what led him, what led you, professor, to the field of mathematics,
and is your field something you think virtually anyone can learn?
What led me there?
You know, it's a question about my life story, right?
So what led me there is a sequence of coincidences and misunderstandings.
I don't know, a number of things, but so to some extent I was good at that.
And that pushed me to do more of it, and that's a very common answer for very many things, right?
People tend to do what they're good at.
Misunderstandings? You said misunderstandings as well?
Yes, to some extent I misunderstood math.
So, you know, there was a time that I really liked computers and electronics,
and I at some point realized that there is a lot of kind of hard work in it.
So writing a computer program is very hard.
You have to write hundreds of lines of code and then debug them and whatever.
And similarly for electronics, right, you have to place those transistors on the board
and solder them together, and you always get it wrong.
And you know, there is a lot of hard work where mathematics is so much easier.
All you have to do is to prove theorems.
That was a complete misunderstanding.
I mean, I thought it was like much purer.
At the time I didn't realize how much hard work is involved.
So in some sense I got to mathematics by misunderstanding, by being lazy and not realizing that I'm getting trapped into a place where laziness is not always enough.
And also at the end I ended up liking those parts that I first thought I was going away from, namely computation.
So I ended up, I mean, computation to me was like a childhood thing in a later acquired taste.
There was a period in between when I thought I was pure, I would never compute anything.
Right, what else did I answer?
There was a second part to the question.
Do you believe that your field, either the field of mathematics or your subfield, the one that you're working in currently,
do you feel that your field is something that virtually anyone can learn?
No.
So some amount of mathematics everybody can learn and everybody should learn.
And maybe it's more than what is taught in our current educational system.
But, you know, I will never catch up with Ed Whitten, okay?
The amount of stuff that he knows, the depth by which he knows things, I'll never get there.
I'm not talented enough to get there.
It might be that in my 30 years in the subject I've learned things that most people also wouldn't be able to catch up with.
So, you know, there is the Moshizuki proof of the ABC conjecture, disputed.
Okay, Moshizuki claims it's a proof other people think it isn't.
I will never be in the position to understand that.
I mean, possibly if I started early enough, I'm not even sure.
I happen to know Moshizuki. I remember him when he was young.
We were friends when we went to graduate school together.
So I remember him when he was young.
He was smarter and faster than me.
It's conceivable that after 20 years he's gotten so far that I will never be able to catch up.
It's equally possible that there are people who would never be able to catch up with me.
They said some amount of mathematics is useful to everybody.
You know, we're now in the midst of a pandemic.
Understanding what's the exponential function is something that is basic knowledge.
It's something that every human should have.
Because it actually dictates if it tells us something about how to behave at this time.
Likewise, making various probabilistic estimates.
If the chance of such and such type of side effects is one in a million, what does it mean?
What does one in a million means in practical terms for me?
Should I fear it or is it so remote that I can ignore it?
So basic numeracy is an absolute necessary ingredient for everyone and can be reached by everyone.
When you mentioned that you'll never be able to catch up to Whitton and perhaps some people won't be able to catch up to you and so on.
What is that a function of biology? You mentioned the word talent.
Is that just innate? Is what's innate called intelligence?
Is that associated with IQ? What are you referring to when you say you won't be able to catch up?
It's a number of things. You can call it talent.
You can call it IQ if you wish, but of course there is no numerical measurement and it's a much, much more complicated thing.
So so much of how far you can get in mathematics depends on how much you concentrate on mathematics.
So you know, maybe when you're, if at a certain part of your life you're able to completely isolate yourself and think on nothing but mathematics,
you'll end up knowing more mathematics than if you are also a human alongside.
Okay, so so so so talent is also circumstance and and luck and in some sense being narrow minded.
So now being narrow minded helps in certain ways.
Speaking of Eric Weinstein, you mentioned him.
Okay, so so so he was very unbalanced.
He had great talent for certain kind of things and great fear of others.
And maybe he would have been a much more famous mathematician physicist now if if it wasn't for these coincidences.
So I don't know.
I mean, you could call it innate, you could call it learned, you can call it many things, but it's a very complicated combination of things.
Tyler Goldstein has a question.
Do you think we need to make new research institutions that aren't linked to academia?
And I imagine what he's referring to is something that Weinstein has brought up and Wolfram as well.
They're not huge fans of the peer review process, or at least they're not huge fans of only thinking that work is worth it if it's gone through the peer review process.
And while that's associated with academia, I'm curious if you have any thoughts as to that.
Well, there are things outside of academia.
It's not arguably most research is done outside of academia.
I mean, you know, there are a lot of a lot of research is done inside companies inside.
Well, a lot of research is done outside of the peer review process.
But overall, I think the system is reasonable.
Sorry, overall, you think the system is reasonable?
I think the system is reasonable.
So, you know, you can say bad things as much as you want about peer review, but it's better than the alternatives.
It's better than having no review.
You can say maybe mathematical should be more hierarchical.
Okay, there should be big bosses who determine which theorems we should be proving.
And then they should have their smaller bosses who would be responsible to specific theorems.
And they would be the big bosses of other people who would be proving dilemmas.
You could say something like that.
It sounds like it sounds ridiculous.
It would work just as ridiculously as it sounds.
Current system is, you know, given the limitations of humans is reasonably good.
I believe there are some arguments to say that what you just outlined is somewhat like it happens.
Given grant agencies, they have some field that they fund.
And so then the research tends to go toward there so you can view them as the large boss.
And I believe Lee Smolin makes some arguments saying that that's why string theory is as large as it is,
even though it shouldn't be because we haven't seen much success from it and so on.
So do you see what you just outlined as the ridiculous scenario?
Do you see that as having any bearing to how it actually is?
So, yes, of course there is bias generated by the granting agencies, by the powerful people.
Be them the granting agencies or the editorial boards of journals.
So, you know, if you have a result that somebody on the editorial board of an excellent journal likes,
you know, that's close to the field of somebody in the editorial board of an excellent journal,
then your result is much more likely to get published in this excellent journal.
And then you're much more likely to get tenure and then more work will be produced in that field
and you'll advise students who will produce even more work on that field.
Yes, it all exists.
But it's not clear that you can think of a better alternative.
And also there is still some amount of academic freedom.
So the granting agency decides once every five years what my funding will be.
In those five years, some of my time I do what is necessary to get my next grant.
But some of the time I also do things that I think are useful, that I think are useful.
Likewise, you know, some of my career is determined by which journal I publish in,
but some is determined by other factors, okay, by the results that aren't published,
by the lectures that I give in conferences in China.
So it's much more multidimensional than just having a boss and the boss is the granting agency.
Even the fact that the editorial boards are different than the review boards of the granting agencies
allows, makes some variety.
And again, what is the alternative?
So, you know, one stupid alternative would be to say, to allow anyone to say,
I am a mathematician.
Here is my proof that Pi is rational.
Or that you can divide, you know, here is my proof of the Goldbach conjecture.
Who's arguing with me?
I'm saying it's a proof.
My word is as strong as yours.
It's much better not to lose control completely.
You just used the word mathematician and I'm curious, what do you see as a mathematician?
Is it someone who's a professor of math?
Is it someone who gets their master's or PhD?
Do you have to frequently publish research in order for you to call yourself a mathematician?
Like what gives, obviously it's a nebulous term, but it's not extremely amorphous.
It's defined.
So what do you define it as?
It's hard to say.
You know, many definitions don't have sharp boundaries.
In fact, only in mathematics there are sharp boundaries between things.
Either the series is convergent or it is not convergent.
And there is a very, very specific criteria to decide it.
Only in mathematics there are sharp boundaries and now you're asking a metamathematical question.
You're asking a question about mathematics, not a question in mathematics.
So there aren't sharp boundaries.
So basically all of the above, you know, if you teach mathematics, if you think a lot about mathematics,
if your way of thought is abstraction, proof, you know, then that's mathematics.
But you can always find the borderline things that will be hard to decide.
Here's a reason why I'm asking, because not everyone can call themselves a doctor.
I know that some people, they get their MD and then they're no longer practicing researchers.
They no longer are connected with the field, but then they'll give health advice,
resting on the laurel that they have an MD and they'll say, well, this comes from a doctor.
And I'm sure the same occurs in mathematics, though to a much lesser degree, same with physics.
Someone may say, well, I'm a physicist because I have a degree from a university,
but that in physics, but that doesn't necessarily make you a physicist.
It doesn't mean that your physics work should be taken more seriously because you have that degree.
So if you're looking for a set of thumb rules to decide which experts are to be trusted and which should not,
be them experts in mathematics or experts in medicine,
well, the answer is these questions are complicated.
So, you know, in medicine,
you probably have to look for the bigger name doctors, the more, you know, the more established doctors,
because the system actually does, on average, promote the better people, even if not always.
And you should look for, was it said by most doctors or by one doctor who hasn't practiced for a long time?
And the same is everywhere.
Yes, there are PhD in mathematics, people with a PhD in mathematics who shouldn't have gotten their PhD.
Yes, there are people, brilliant people who did brilliant PhDs in mathematics,
and 10 years later lost their mind and are speaking nonsense.
I mean, there is no look at this certificate and if somebody has the certificate, then they're trustworthy.
Ashley Schipp wants to know, is there a connection between knot theory and protein folding?
And is it possible to use, well, if so, is it possible to use the artificial intelligence research that has been done on protein folding to inform knot theory?
There is a connection between protein folding and between DNA and stuff like that.
In knot theory, people, it's not very strong.
Like, in general, people like to say things about it, but the actual research, at least my own research is very different from what you would end up doing in biology.
Of DNAs or proteins.
By the way, in some sense, DNA, the study or, okay, so DNA molecules are really, really, really, really long strings, chromosomes are really, really, really, really long strings.
Namely, they are of length billions, or the ratio of their length to their width is billions.
Okay, because they have billions of amino acids along them, right?
And so, you know, if you were to make it out of rope, it would be very, very, very thin rope, which is very, very long.
But yet this rope is compressed into a tiny, tiny, tiny, tiny regions.
So they get knotted.
And even your experience from a much smaller knot, so here's an electric cable.
If you have a little bit of an experience with letting loose of cables, not tying them nicely, as I tied this.
If you let it loose, it gets knotted and it's nearly impossible to untie.
Chromosomes are much worse. They're much, much, much longer in relative terms.
And they get cramped into tiny, tiny, tiny spaces.
So how come they don't get knotted?
And if they do get knotted, how come during various parts of the life of a cell, so when cells split, they split.
They somehow manage to untie themselves and disconnect.
So how is it happening?
There must be some huge brain inside every cell that can untangle knots which are much, much, much, much more complicated than the knots that you can untie.
Or, you know, the knots that you take hours to untie sometimes.
So there must be something there. And there is something there.
And there are enzymes that actually, you know, so when DNA molecules need to unknot, to untangle, there are enzymes that allow one molecule to pass through the other.
So basically you want to pass one string through another so that they will come to position.
There are enzymes that cut this one temporarily. It gets cut, it passes to the other side and it gets reconnected.
So in a sense, biology invented anti-knot theory enzymes.
And they are necessary because of knot theory.
Because if there were knots, you wouldn't be able to divide cells.
So biology invented some anti-knot theory enzymes that allow things to pass through each other, that nullify knot theory.
So in some sense the most interesting knot theory that happens in biology is somehow anti-knot theory.
That's a lovely story, but in practice there isn't much.
Jack Disart wants to know, is math a human construct that we have devised to better understand the universe?
Or is math a set of universal, unchanging principles?
I'm a human. How would I know about, you know, how non-humans would see math?
Or how, you know, I don't know.
But this said, I mean I don't want to answer the philosophical question, but I want to answer some practical things that come out of it.
Every bit of math. So, you know, pick up a math book here.
Every bit of math in this book, and not just this.
Every bit of math in every book was created by a human.
That's a good thing to remember. It means that there are no miracles.
It means that, you know, when you see a proof and you have no idea where could it come from,
well you have no idea, but somebody had this idea and you may be able to find it.
So, it's often extremely instructive to try to reverse engineer somehow,
not just understand the proof, but understand how humans with all their failures could have found them.
So, it's actually useful to know that math was entirely created by humans at the end.
Do you think we don't spend enough time in books talking about the missteps?
So, for example, here's how the person arrived at this proof. They originally thought of it like so. This didn't work.
Or would that just bloat the book unnecessarily?
It would probably bloat the book. But motivation should be there.
So, some story should be told. Like when you present a proof, there should always be some plausible discovery story.
How could this proof have been found? It doesn't have to be a historically accurate story.
But there has to be some story explaining that this proof could have been found by humans.
It wasn't created by some supernatural entity.
This question, CX770 asks, does he believe in the concept of God?
No.
Okay, this one's a tangential question. Bill McGonigal asks, Ramanujan said that he learned from his goddess.
Others suggest, well, Ramanujan said that he learned from his goddess by meditating and praying.
Or it came to him in dreams, some of his formulas, esoteric formulas.
What does Professor Jorbar Neton make of this?
Can't speak for Ramanujan. I tend to be a rationalist, atheist, no miracles.
If I think hard about something, I either solve it or don't. But there are no miracles in the business.
Another question, this one, this is my question. Are there some relatively elementary concepts that you've recently had a new insight on?
So for example, I don't know if you know my brother Sebastian, he's a professor at U of T as well in math finance.
He was telling me, Kurt, you know, only recently, maybe last year, I realized the triangle inequality is actually about a triangle.
He would only just use the formula. Also recently, okay, the Jacobian determinant, the Jacobian is actually like, it's like a linear map of what is that point doing, close.
Instead of just, okay, think of it as an abstraction of slopes and stuff. Yeah, so do you have an example?
Sorry, of what? Of things that I've only recently understood?
No, no, no, of something you have understood you've used before, but then maybe you've come to a different understanding of you and you say, oh, that's an interesting way of thinking about it.
I didn't see that before.
It happens all the time. Well, I wish it would have happened all the time. So when it happens, it's a great day, not all days are great day, but there is the occasional great day.
You know, every time you learn something has higher meaning, or more meaning than you thought, it's great news.
It happens all the time in the life, in the professional life of a scientist, though not every day.
Okay, well, I'll give you an example, which somehow, which I think also partially involves you.
So a few years ago, I was asked to give a 10 minutes talk or something to a, I don't even remember what was the event, but you were there.
Oh, you remember that?
Mentorship event or something like that. I don't remember.
That's what that was, that's right.
I thought, what would I talk about? And I, well, okay, the truth is that I have some list of ready made topics that I can talk about, you know, if you ask me to talk in front of high school students, I sort of have a number of topics that are ready.
Okay, but nothing really fit. And also, sometimes I get bored of myself.
I have to occasionally add something. So I said, okay, why don't I talk about a prisoner's dilemma?
And so I mentioned the prisoner's dilemma in five minutes or so. And it's a very light map, right? It's hardly math. It's so light that it's hardly math.
But then I realized that it has profound, I guess the economists have always understood that. They always thought it was important.
But I realized it only relatively recently, so that was, what is it, three, four years ago, I only then understood how profoundly important it is.
So, you know, the prisoner's dilemma is some numerical example where you have two sides, two people, A and B, and they can choose whether to cooperate or betray.
Each one chooses independently, whether to cooperate with the other or betray the other.
And if you look at the numbers, each one, no matter what the other does, would prefer to betray.
And as a result, both of them would betray, and the overall outcome is worse to both of them.
Again, I'm not describing the whole thing. I'm not telling the numbers. I'm just saying there is this situation where if each one acts selfishly, it's worse for both.
And, well, it has huge philosophical implications. You know, there is a whole political class that believes that people should be acting selfishly,
because if each one acts selfishly, it's actually for the best, for better for everyone.
That's somehow the foundation of the belief in free markets.
So, yes, I am actually still a great believer in free markets. I actually still greatly believe that free markets are a good thing.
But there is a little asterisk. Sorry, that's not duality. That's an asterisk.
So there is a little asterisk, and the asterisk says, provided somebody arranges so that free markets, when each one acts selfishly,
you will not be led into prisoners' dilemmas. You will stay out, and your selfish acts will actually benefit the overall good.
So, I mean, this asterisk here is the prisoner's dilemma.
Ah, sorry. Well, okay. Yes, yes, yes, I get it.
Anyway, and you think it's a small asterisk, but the more you think about it, you realize it's actually a huge asterisk.
It completely changes the picture, and it's much, much, much more common.
You think of it as an esoteric example, but then you realize that it's everywhere.
So it's actually a very, very big asterisk. And I think I realized it relatively recently.
I'm honored that you remember that. I didn't think that I'm honored to even be a footnote in your memory.
Oh, don't be silly.
Okay. What have you applied?
Also, this question comes from a Harinavis.
Harinivis. Sorry.
Harinivis, he asks, has drawer applied the principles of mathematics to some other domain of his life?
Like, let's give an example of a non-trivial example.
Well, okay, I mean, I can give two answers.
Okay. One answer is, well, I don't know if it's the principles of mathematics, but you know, people once, there was a claim going on and going around.
And in fact, it's still going around that the Bible contains predictions in code.
So if you read the Bible with letters keeps, if you read every 10th letter, you will occasionally find a word.
That's not surprising.
But the claim was that these words were that that it actually happens more than statistics would predict and that these words have messages in them.
And there was a group of people in Jerusalem that published a paper and in fact, it passed peer review.
But so they published a paper in which they supposedly proved this fact.
They did careful statistical analysis, again, careful in quotes.
And showed that the effect was real.
And therefore, the Bible must have been written by God because no human could have put this information into the Bible.
It's information that did not even exist at the time that the Bible was written.
And so God wrote the Bible.
Furthermore, it's the Hebrew Bible.
So there is a God and that God loves Hebrew more than she loves Japanese.
This would be the greatest discovery.
If it was true, it would have been the greatest discovery in the history of science and I'm not kidding.
There's no longer an argument between Atheism and Thaism.
Thaism wins.
My role was to debunk this.
Is this real life or not?
I don't know.
I, me and a group of people read critically these assertions that were good enough that they passed peer review in a respectable journal.
And we found gaps.
And the gaps are, well, we're good enough that the same peer reviewed journal accepted our paper too.
Okay.
So, you know, in some sense, my role was negative, right?
What could have been the greatest discovery in the history of science, a proof that there is God and it's the Hebrew God, was debunked.
Maybe it's a negative discovery, but well, I mean, even maybe my effect, well, my part was negative.
I've killed a great discovery, but that's also a valuable thing to do.
I mean, to kill.
The other thing is, otherwise, mathematics plays an extremely limited role in my life.
Other than my professional life, if you cut out the eight hours a day that I work on mathematics.
Mathematics has no role in my life or hardly any role.
So, I mean, you do.
And in fact, also almost no role in your life.
So, basically, you know, if you, you give change in restaurants, you, so, so, or in when shops, sorry, you add 15% tip.
That's about the math you need in real life.
Multiplication you don't need, but, but if you're a carpenter, you probably need it, right?
You need to tile, you know, if you're a trade person, you need to tile a certain room.
This is the room is five by six.
How many tiles do you need?
So, multiplication trade people need, but people like me don't need.
Trigonometry, well, maybe architect need, architect needs trigonometry.
Some parts of engineering, and we get more and more esoteric, need higher and higher mathematics.
Without the very high mathematics, without general relativity, GPS satellites wouldn't work.
So, in some sense, we use mathematics every day, but we're not the engineers who designed GPS satellites.
We're designed by a group of, a very small group of people somewhere in some, you know, design bureau, and they use high level mathematics.
So, so, so, but my real life high level mathematics is almost nothing except once, sorry, the Bible codes and another time.
And I've given talks with this title.
So, the title was the hardest mathematics I've ever really.
And if you Google this title, probably you'll be able to find a video of that talk.
And you'll find the one time where it was actually in my professional life, but in a meta way.
So, not my research, but something about how to present my research.
I ended up using very high level mathematics, hyperbolic geometry, in fact.
So, one time it happened.
Anyway, what was your next question?
Why does it take 360 pages of abstract symbols to prove that one plus one equals two?
Occam's razor says we can cut this, we can just say one plus one equals two because every single last one of us believes it does.
So, what do you make of that?
Why is it so complex to prove something simple like one plus one equals two?
Do we even need that?
And this question comes from Roy Dobson.
So, let me answer a different question.
You know, so one plus one equals two.
So, the reference is probably to work of Russell.
Basically, if your axiom system is very, very, very basic and very, very primitive in a way, very, very simple minded,
and you want your axiom system to be very, very, very basic, then it might be that even proving one plus one equals two would be difficult.
But it will be a part of the build up.
And later you will be proving harder things.
But the truth is I see.
Okay, but let me give you another example.
Okay, so, you know, there is the intermediate value theorem.
The intermediate value theorem says that if you have a function, continuous function, so if f is continuous and it's negative here and positive here,
then at some point it crosses zero.
We teach it in week seven, maybe, I don't know, of undergraduate calculus.
Maybe it's just week five or maybe it's week nine.
I don't remember.
Okay, you can ask, this is so idiotic.
Right.
Why does it take five or seven weeks to prove the obvious that if you go continuously from here to here, you'll be crossing zero?
And the answer is that when you prove the intermediate value theorem, you are not actually proving the intermediate value theorem.
You are actually testing your definition of continuity.
You're testing your abstract setup.
You're testing your abstract setup, your language, you're testing, you know, the machinery of for every epsilon there exists a delta such that blah, blah, blah.
If you could not prove the intermediate value theorem, you'd be rejecting the abstract system, not the intermediate value theorem.
You'd be rejecting this, not that.
So the seven weeks were not about the intermediate value theorem.
They were about this, for every epsilon there exists a delta.
And then you can ask, what's for every epsilon there exists a delta for?
Why do we need this?
Well, we don't need it to prove the intermediate value theorem.
The intermediate value theorem was testing this.
What do we need it for?
And the answer is, well, there are later things like e to the x is equal to sum of x to the n divided by n factorial, which is this, by the way, looks innocent and people think they understand it, but it's actually a miracle.
And this miracle depends, well, doesn't depend, but there may be other ways to reach there, but you will believe this miracle after you've understood this system.
So the answer is, in a way, we didn't go from here to here to here, but more like this was known.
And it validated this, and then we could use it to do other things.
