Welcome everybody to this special lecture during our masterclass on KK theory and physics.
It's a great pleasure to have Nigel Ficksen of Penn State as our distinguished speaker
Well known among many other things he did for his contributions to KK theory and index theory and non-linear geometry.
And he will be speaking here giving us a rapid tour to non-linear geometry
called from integrally questions and a spectral theorem to the index theorem and beyond.
Thank you very much Bram.
It's great pleasure to be here again, really enjoy my visits to Vienna.
I first of all have to apologize to everyone because before this lecture was scheduled it was a free afternoon.
Now it's not a free afternoon.
I mean it's still a free afternoon for you guys if you want but at least I have to stay here for the full hour.
I was asked to give the graduate students something they would enjoy and in my country that usually means beer.
So I need to apologize especially to the graduate students, no beer, just mathematics.
So okay that's the way it is.
My goal is to describe to you a construction of Alenkon, our mathematical godfather in this particular part of mathematics
called the tangent groupoid.
And for the most part I'll talk about old stuff but this particular construction continues to have an interesting life
and so I hope that by the end I will show you some new occurrences of Alen's tangent groupoid in current mathematics.
So that's my goal.
So for the most part a relaxing tour through old things if you are now or even you count as an old guy.
It used to be my student but now he's an old guy.
You can tell he was my student because of the hairstyles.
Alright, so for the most part some relaxation and at the end I'll be out of time so I'll be in a frantic hurry
and I'll just rush some new stuff by you but for the rest of the time just a relaxing tour.
Alright, and let's begin at the very beginning really of Hilbert's spaces and such things with Hilbert.
Why not?
And his spectral theorem.
And so this is supposed to be something which every graduate student has learned.
Probably every graduate student in this room has learned it but I discovered giving this lecture elsewhere that it's not the case anymore
that people learn the wonderful theory of Hilbert, this particular wonderful theory of Hilbert.
So it's all about operators like this that Hilbert was interested in.
Integral operators, he would say equations.
So there's some space and if you were Hilbert it would be maybe just a domain in the plane like this.
It would be also interesting to study an interval, let's call this domain, omega.
In these modern times of course we study manifolds, maybe here's a Riemann surface sigma.
And I'll just call them collectively M and the only thing we need to know about this space is that it's compact
and now the operators that we want to consider look all of them like this.
You take a function f and you feed it into this integral equation as it's called an integral operator and you get out a new function.
Okay, just like that.
So this function k should be, of course it's a function of two variables and it should be reasonably smooth, reasonably differentiable
so I'm imagining this space M is some sort of space on which you can differentiate.
It turns out not to matter how smooth k is as long as it's reasonably smooth.
One of the great virtues of the theory of Hilbert is that in the end it's so simple, it doesn't depend on any details at all.
There's nothing really left to it.
Now what kind of an operator is this?
Well as long as k is reasonably smooth it's an operator on the, at that time, recently invented Lebesgue space as L2, like that.
This is only 10 years after the thesis of Lebesgue so everything was brand new back then.
And what Hilbert was interested in, thanks to correspondence and discussions with Fredholm actually, are those k's which satisfy this symmetry condition.
If the k is complex valued, as I indicated here, then we should put a conjugate.
And you're supposed to think of k xy as a sort of continuous matrix entry.
In the position xy you have the number k of xy and so the matrix is self-adjoint if it satisfies this equation here.
And of course what Hilbert proved is that inside of the, inside of this gigantic infinite dimensional inner product space there is a complete
orthonormal, that's supposed to say orthonormal system, consisting of eigenvectors for the operator t.
So I guess they really functions and they satisfy equations like this.
And it was a huge breakthrough because the argument, the proof of this as you know, certainly if you're in this workshop you know, is very, very, very simple and it depends on hardly anything.
And it conceptualizes a whole bunch of wonderful things.
First of all in dimension one there was the work of Sturman-Lewville about a hundred years before.
And then in dimension two there was all of the difficult controversy surrounding the Dirichlet problem and the associated equation, the eigenvalue equation, the Helmholtz equation.
But then in the hands of Hilbert's students, particularly Hermann Weill, many more things were brought to light.
So for example there was at the time a famous problem of Lorentz having to do with the asymptotics of the eigenvalues that appear in this problem here.
And this is the work of Weill.
And the list goes on, there's only room for one more thing in the list, so let's just put here again a subject which is very new at the time, the representation theory of compact groups.
I'm thinking of the work of Peter and Weill and the Weill character formula, all of these wonderful things, they all really evolve naturally from this wonderful theorem of Hilbert.
And so this is 1914, something like that, a little while ago, a hundred years ago.
And so I'd like to kind of tell you a little bit what happens over the next hundred years.
So nearly everything I have to say involves the following change of viewpoint in which rather than focusing on one operator T like we did there and there, and that's really what happens in all of these applications, I want to consider them all at once, all of these integral operators.
So the Hilbert space is fixed, L2 of M is fixed, but the K varies, and now we have a large collection of operators, and together collectively they form an algebra of operators, an operator algebra, because if you have two of these operators, say T1 and T2, and you compose them as you're supposed to multiply operators,
and then there's a formula which tells you what the integral kernel for T is in terms of the integral kernels for these, as everyone knows it's very easy, it's like matrix multiplication, it's like this.
And so we get an algebra, an algebra of operators, an operator algebra.
And one of the things that Hilbert and especially student Schmidt discovered is that it's very convenient from a conceptual point of view to close this algebra up, consider not exactly these operators, but all of the operators you get as limits of these operators.
So the norm closure in the algebra of bounded operators, if we want to be precise.
This is an example of a C star algebra, it just means it's a norm closed algebra of operators with one extra condition.
And the operators you get are what are called compact operators.
And there's some notation for it.
I'll try and write it clearly enough so you can actually read it. It's always called some K, K of L2 of M, and you get to choose the font, you put your K in like that.
And it's worth pointing out, and this is really, this is part of the beauty of Hilbert's theory, is that this thing is, it's independent of almost everything.
Once you've done this, the only thing that is left is L2 of M as a Hilbert space, only.
Not as a space of functions, but this thing just as an abstract Hilbert space.
And we know there's only one Hilbert space out there, they're all isomorphic, so there's only one of these algebras, and so there's only one theorem of Hilbert, and there it is.
And it's so beautiful and simple, a theorem, so this theorem is true, the one that's up here for any self-adjoint compact operator.
And because it's so general, the proof has to be easy, or it has to be false, one or the other. In fact, it's true, and so the proof is easy, and the whole thing is like floating down a river.
It's a marvelous, marvelous accomplishment.
Okay, but once you've closed this up and you've forgotten that L2 of M was functions on M, it's just a Hilbert space, then you're sort of drifting away from geometry and applications,
and you're in the abstract world of Hilbert spaces, algebras of operators, and there's a danger that you sort of drift so far that you can no longer speak to anyone anymore because it's all too abstract.
And if you're Hermann Weill, this is not a problem, of course, you can keep your eye on the prize and continue to do great work, but for the rest of us, this is a little bit seductive.
All of the interesting details get sucked away, and you're just left with this abstract c-ster algebra of compact operators, and that's not altogether good, although mostly, mostly it is.
So what I want to do is describe how you can adjust the definition of this c-ster algebra, this abstract algebra, add to it in such a way that connections with geometry become a little more evident, a little more manifest,
and that's the theory of Constangent Grupo that I want to describe.
First of all, before I get there, I want to set a goal in sight.
I'd like to describe something which will cause us to maybe want to tinker with the construction of the compact operators here.
And so first of all, let's introduce another character into the picture, Von Neumann, and let me coin a term here, the Von Neumann symbol, this is not standard, it's just for the purposes of this lecture.
So what Von Neumann studied were, among many other things, self-adjoint unbounded operators on a Hilbert space.
For example, it's very natural to want to understand, as we saw in this example, here the, here's the Holmholtz equation.
And here maybe we want to study the Sturm-Leville equation.
And so there are differential operators floating around like delta or, so d is a Sturm-Leville operator and we all know what they are, so I won't write down the formula.
And they all satisfy some equation like this, the self-adjointness equation is this.
But of course you can't apply d, whether it's this one or this one or a more general one, to any function.
The function has to be differentiable before you can differentiate it, so the most you can hope for is to have this on some dense collection of functions.
And at this point we might as well just be talking about an abstract Hilbert space, although we're secretly only interested in this particular one.
And what Von Neumann figured out is that before you can seriously talk about spectral theory, seriously ask what is the spectrum, what are the eigenvalues of this operator.
You have to add more to the definition of self-adjointness than what I've written here.
And I won't tell you exactly what Von Neumann said, I will just summarize the story in the following thing.
So Von Neumann's definition of self-adjoint operator, let me say it leads to, can be summarized as the following thing.
If you have an operator d, and if it's self-adjoint in the clever sense that Von Neumann defined, then it gives rise to what I'll call Von Neumann's symbol, which is a homomorphism of algebras like this.
So this is a morphism of, well the algebras in question are c star algebras.
This thing here, in case you don't know, which there's no reason why you should, is the algebra of all continuous complex valued functions on the line, on this space, vanishing at infinity.
We'll see this C0 notation in just a while, see it again.
And the idea is that if I have a function, here are some interesting functions.
Suppose you have a complex number lambda, which is not a number on the real line, it's a real honest complex number.
Then according to everyone's conception of self-adjoint operators, lambda should not be an eigenvalue.
The first thing you learn about self-adjoint matrices is that all of the eigenvalues are real.
So it should be the case that if I stick d where lambda was, I should get an invertible operator, and that's what Von Neumann insists.
All of these d minus lambda inverses should be well defined, and they should assemble to give a homomorphism here.
Now, I cheated a little bit by using the compact operators here.
So let me just say, generally you're supposed to replace this algebra of operators by a bigger one, but in our particular case,
for the particular operators I mentioned up here, everything I've written down is perfectly correct.
All of these operators are, as they say, elliptic differential operators, which is what I'd like to tell you about next.
Okay, so there we go there.
So that's Von Neumann's appendix, if you like, to the theory of Hilbert.
The operators we're really interested in are typically differential operators, and this is how you deal with them.
The properties of differential as opposed to integral operators can be reduced to properties or reduced to the study of integral operators by means of this construction.
And it's very important, by the way, that you make this completion here that I was mentioning right here, this closure,
because it's difficult to say at the outset exactly what the operators are with that are in the range of this map, except that they are compact.
Are they actually smoothing operators where the K of x, y is a three times differentiable function?
That's a tricky business, but the fact that they're compact is not tricky at all.
All right.
So the reason I called this Von Neumann's symbol is I wanted to compare it to suggest a comparison with something which is well known from differential equations,
which is the principal symbol of a differential operator.
All of the operators we're interested in, I'm interested in, in this lecture are differential operators.
So if your operator happens to look like this, there are some coefficient functions, they're all linear operators, partial differential operators.
So they all look something like that.
Then the PDE people do the following thing.
They attach to this operator a function, and the function is on, it's a function of two times as many variables as x is.
So if the original operator was defined, let's say, on some, I don't know, K dimensional manifold or on some open subset of RK or whatever,
then there are K x's, but also K new variables, XI, and of course, you all know what you're supposed to do.
The first thing you do is you forget about the lower order terms, so if the lower order terms just go away, you don't care about them.
And the next thing you do is you make a function out of an operator.
Each time you see a derivative, you replace it by a variable XI, and you just do this thing here.
So this, as far as XI is concerned, this is a homogeneous polynomial, but the homogeneous polynomial changes as X moves around,
because the coefficients of the homogeneous polynomial are the same as the coefficients of D.
That's the principle symbol, and one says that D is elliptic.
The word that I used over there, if this function, it's just a function, is a proper function.
The image of a compact set is compact, so I'm assuming here for simplicity that M is, let's just say here some K manifold.
M, I like that better, so assuming M is compact, so this is what elliptic means.
If D is self-adjoint, the thing that von Neumann was interested in,
then first of all the symbol can be thought of a little more abstractly.
These new variables you can think of as being cotangent variables like this.
So sigma is a function on a space of two times the number of dimensions of M.
It turns out that the correct interpretation is that it's the T star of M,
and self-adjointness corresponds to this symbol being real valued.
And the reason I want to write that down is to make the following comparison with what we see over here, this thing.
Namely, if you have a map from one space to another, then you get a map in the other direction on function spaces.
So each time I have a function on the real line, I can pull it back by this map, and now I get a function on this space.
And so there's an algebra homomorphism from functions on this space to functions on this space given by this construction.
And because of properness, what it actually turns out to be is a map from the functions which vanish at infinity on R
to the functions which vanish at infinity on T star of M.
So this is by, let's see if I can squeeze this in here, by pulling back functions, by pull back.
So now there are two symbols on the board, one due to von Neumann.
Let's give him credit for that, and one due to Mr. Principle.
And the question is, what do the symbols of Professor Principle and Professor von Neumann have to do with one another?
So far, so good? You're happy?
Oh, well, this is a real variable. I'm describing coordinate functions on a...
Yeah, okay, good.
Yep.
That's for you.
Very good.
And so what I'd like to describe is technique, due to Alencon, which interpolates between these two things.
It's a rather remarkable construction. It's rather incredible that you could do it.
Okay, good. Just checking the time.
First, so I shall construct more algebras of operators, or maybe it's better to say more C star algebras,
by generalizing this formula here. Let me call this convolution.
This way of taking two kernel functions to create a third. I could have called it matrix multiplication,
but convolution is a little more suggestive of the direction in which we want to go.
So I'm going to call this type of product, which is an associative product, convolution.
And let's build a whole bunch more of these things.
And I want to start from the following ingredient, geometrical ingredient.
Namely, I want to imagine I have a group, G. I'm thinking of it as a Lee group.
It doesn't have to be, but I'm thinking of it as that.
And I'm imagining now it's acting on a manifold M.
I'll show you a couple of examples in a moment.
But first of all, suppose you have a group acting on a manifold.
I want to build the following space.
I'm going to use this funny times picture symbol, L times, I guess.
And it's the following thing. It's the best way of describing this is as triple.
So it's a point from the manifold and then an element from the group and then another point from the manifold.
I won't write that down.
And there's just one equation that these guys have to satisfy, which is that M1 is G times M2.
M1 is where you go if you move M2 by G like that.
So it's a little redundant to write M1 and G and M2 because M1 is determined by G and M2.
Nevertheless, this is the most convenient thing, but you could forget about M1
and then you'd see that this as a manifold is just in the obvious way by projection onto the last two coordinates.
Just G times M, no difficulty there.
Now, suppose I have two functions on this space.
They should be reasonably smooth.
Like I said before, it doesn't particularly matter what kind of smoothness they have,
but they should be reasonably smooth and I'm going to be doing some integration.
So let me assume these functions decay sufficiently fast at infinity.
Let's suppose, for example, they're compactly supported.
Suppose I have two compactly supported functions on this space.
Okay, and now I'm going to convolve them to get a third, figure out some version of this formula.
It's not very difficult, so let's call the functions K1 and K2.
So now I'm defining some new product in a way which is a little reminiscent of what happened before,
but involves the group and it's just the following thing.
It's actually integration over the group of K1, let's call this M3, M1, G1.
Oops, let me leave that out and make it look even nicer.
K2, M2, G2, M3.
We're integrating over all of those G1s and G2s such that G1 times G2 is G.
This is a lot like usual convolution.
Okay, so it defines a product on the space of reasonable functions on this reasonable geometric space,
which is really just this space here.
It's associative. How could it not be so natural?
And let's just look at two examples.
I have to mention one because if you happened to wander into this lecture but you weren't part of the conference,
then you might ask yourself what is the conference about.
Well, it was about this thing, is about this thing.
I would say this is a fair summary.
It's about the physics underlying or related to or which can be expressed by this particular example.
Suppose the group is just the integers.
Okay, and the manifold is just the circle.
Suppose the action is just rotation of 1.
The generator of z is just rotation by theta that way.
If you take these kernels that I was just describing and form the world's most natural completion of this collection of kernels,
you'll get yourself a c-store algebra.
So this is just some c-store algebra, whatever that means.
It doesn't matter.
Closure of let's say we'll just take the smooth compactly sported functions on g times m like that.
And what this is, it has a name.
It's called the rotation algebra.
It even has a symbol.
It depends on theta in an extremely interesting way.
A theta.
There you go.
So you get extremely interesting algebras from the world's most simple to describe examples of actions here.
This talk is not about this example, but the rest of the conference is about this example.
And if you did not go to Emil's talk yesterday, then shame on you and you should ask him to show you his incredible videos,
which changed my life.
Those are just such amazing videos.
They will make you believe in c-store algebras just seeing Emil's videos.
All right.
But we're not talking about these examples.
Instead, talking about these examples, it's a very boring example from a geometric point of view.
At least it seems to start out that way.
I want to consider the following example.
The group will just be Rn.
I guess I was calling it Rk.
So I'll stick with that Rk like that.
And the manifold will just be Rk.
And the action will just be translation, except for one thing.
And this is the one thing which makes this kind of interesting.
So the action.
If you have a group element, which is really just a vector in Rk,
and you act on a manifold element, which is really just another element of Rk.
Actually, there's a whole bunch of actions, and I want to consider them all at once,
so I'll put a little s there.
What you do is you just translate by s times g.
You're allowed to do that.
That makes sense, and that's a well-defined action.
And let's try and figure out what's going on here.
There's a huge difference between s equals zero.
Well, there's no action at all, really.
Nothing is moving anywhere.
And s is not zero.
And if s is not zero, what happens?
Well, this space, the one that's up there, I said before you can forget about m1,
because m1 is completely determined by g and m2.
But in this particular case, you can also forget about g,
as long as you don't forget about m1.
You can remember m1 and m2 and forget about g,
because g is completely determined by m1 and m2.
You can solve the equation.
If you know what this is, and you know what s is, and you know what m is,
of course, you can solve for g.
It's just m1 minus m2 over s.
So the manifold is just m times m.
It's isomorphic to m times m.
If you build this c star algebra, that is to say you take functions on here,
really functions on g times m up there,
and you make a c star algebra using this convolution multiplication.
What you get is our old friend, L2 of m, the compact operators on L2 of m,
maybe Hilbert's old friend, like that.
All right.
Well, it's interesting that the action went away.
The fact that we were using some linear action goes away,
and now you can make this for any manifold whatsoever.
This thing exists for any manifold.
We built it using some linearity properties.
This action depends on m being a vector space,
but the final result doesn't depend on m being a vector space.
And something kind of similar happens when s is equal to zero.
So now there's no action whatsoever.
Let's just write down what the product is in this case to kind of see what's going on.
So what I'll do is I'll forget about m1,
because m1 is determined by m2, or in this case m3.
Just write it like that.
Here's what's going on.
This is just g1 mk of g2.
And what we're integrating over is maybe I'll use additive notation,
all pairs g1 and g2, which add up to g, like that.
That's exactly the formula from above specialized to this case.
So what's going on is kind of interesting.
What you see is the multiplication here is just point-wise multiplication
as far as m is concerned, but it's usual convolution,
the thing you learn about in Fourier theory in g.
And the two don't speak to one another because there's no actual action anymore.
And when you build the C star algebra, what do you get?
Well, if you want to make convolution multiplication a little more transparent,
you should take the Fourier transform,
because Fourier transform converts convolution multiplication
into point-wise multiplication.
So under Fourier, this becomes an algebra
where all of the multiplication is point-wise.
So it's just an algebra of functions.
And the right way of thinking about this is this is t star of m.
It's some space.
I mean, it's g hat, so to speak, the Pontragon dual of g,
which is just a copy of rn, times m, which is just another copy of rn.
But the invariant way of thinking about it is like this.
And it pays to think about it the following way,
thanks to the following calculations, which I'll tell you about now.
Time for some technology.
Okay.
I know the pros do this all at once, but we'll do it.
So I've built a family of c star algebras.
They're all built in the same way.
Namely, you start with functions on this single space, g times m.
They all start from functions on g times n,
but on this space of functions on this one manifold g times m,
there is a one-parameter family of multiplication operators.
And what you get from the multiplications is a little bit interesting.
Here you have compact operators.
Here you have point-wise multiplication as it happens.
Functions on the cotangent bundle.
So here's the beautiful observation,
which is the whole business here.
We have one space which is functions on g times m
and then a whole one-parameter family of multiplications.
And I can, leading to a whole one-parameter family of c star algebras,
maybe I could stick an s here.
And we get out of this what c star algebras would call a continuous field,
a continuous family of c star algebras out of this.
And you can guess the definition based on, more or less,
based on what I just said.
And it's invariant.
It's invariant under the natural thing which acts.
What acts on this thing?
Well, on the face of it,
what acts on this thing are morphisms which are compatible with translation,
which is to say not very many morphisms of the manifold to itself.
But in fact, all of these c star algebras have another life.
This is just functions on t star of m,
and so you don't need the linear structure to write down this answer.
Nor do you need the linear structure to write down this answer.
So on each of the five c star algebras,
you can act by the diffeomorphisms of m.
And if you take a continuous section,
and you apply diffeomorphisms to the individual values of the continuous section,
in the natural way here and the natural way here,
then you get another continuous section.
That's remarkable.
It's not very difficult.
It actually follows from Taylor's theorem.
But it's kind of interesting.
It's invariant under this group.
This huge, this huge group.
So we started, it was really necessary from this point of view to start with rn
and use the additive structure.
But in fact, in the end, you don't actually need it.
You can globalize to any manifold.
By making a construction on coordinate charts
and then just gluing them together in the usual way.
And the way that c star algebras tend to package this is they build
what's called the c star algebra of the tangent group.
I'm talking here at the level of c star algebras,
but there's an analogous geometric construction at the level of spaces
which produces something called the tangent groupoid.
It's a groupoid, so it's hard to like a groupoid unless you're French
and then it's different.
So by this thing here, I mean it's a c star algebra
of continuous sections of this family of c star algebras.
And you tend to want these sections to vanish
at plus or minus of s equals plus or minus infinity.
Small detail.
So this is some new c star algebra.
It's built up, it's like a bundle of c star algebras
and it's built up out of compact operators
and it's built up also out of this one exceptional fiber
which is functions on the tangent space,
the cotangent bundle rather.
And the other big theorem,
how are we doing, fine,
is that the two symbols that I introduced before,
something coming from spectral theory
and something coming from just examining the coefficients of an operator,
something let's say geometric, the principal symbol,
maybe in this context interpolated, r interpolated,
maybe interpolated.
So what do you do?
Well, it's very easy.
For s, if s is equal to 1,
then we could study the von Neumann symbol of the operator d.
And if you want to attach a symbol to some s which is not 1,
but on the other hand is not 0,
well, the symbol should be valued in the compact operators
and so we could take von Neumann's symbol here.
On the other hand, 0 should just be the principal symbol.
Oh, I made a small typo here.
So first of all, d is supposed to be elliptic.
Let's say it has order q as it turns out,
it should be s to the q here like that.
So what do I mean by that?
I mean, if you take a function here,
so we have all of these symbols
and let me use this notation here,
the fiber of this family of c-ster algebras at s,
if I define a morphism from c0 into this fiber algebra,
it's written over there using this formula
or exceptionally this formula,
then each element here gives rise to a family of elements over here
and they vary continuously, they're a continuous section
in the sense we just described.
And that's the story here.
So this thing is new sub t, d of say,
just new sub t, d.
So the fact is that s goes to new sub t,
not t, it should have been s to the q.
My apologies.
Or alternatively, the classical symbol of d,
I'm saying that this is a continuous section.
I'm just trying to explain a little more precisely
in this box what this interpolation actually means.
It's a beautiful fact and it relates to things
which are on the face of it not connected whatsoever.
This is an observation of Alain.
And let me try to explain what it's good for.
Any questions while I do the housework here?
If I have time at the end,
sort of despairing of that at this point,
I want to make the point that it's good for many things,
but there's one famous construction.
Which deserves to be mentioned first and foremost.
Which goes like this.
Which is a construction relevant to index theory
in the sense of a tieran singer.
One of my fond memories of the Schrodinger Institute
is sharing an office with Ralph Bott.
It was a little earlier in the year,
maybe in August or September, so it was very hot.
And so Bott had stripped down basically to his underwear.
And so I shared an office with an almost naked Ralph Bott.
And so then I explained to him what I'm now about to explain to you.
In fact, I've met Bott several times.
And each time I explained to him what I'm now about to explain to you,
he was never tired of learning this particular fact.
Or he just didn't remember it.
So in geometry, when you're hunting around for interesting elliptic operators,
what you find is that the operators tend to have this form.
First of all, they don't act on functions.
They don't act on scalar functions.
They tend to act on vector-valued functions.
Indeed, they tend to act on sections of vector bundles.
Let's just say vector-valued functions.
And they have this interesting off-diagonal form.
Or if you want to carry favor with the wrong sort of crowd,
you could say they have some sort of supersymmetry.
So it's not one operator.
It's a whole matrix of operators.
And it acts on vector-valued functions, not on scalar functions.
That's how you're supposed to think about it.
And the whole thing is self-adjoint.
And going back to the constructions that I was telling you about,
well, now they're a little fancier.
Not by much.
There are just some matrices involved.
Maybe this is a 2n by 2n matrix.
So each block is an n by n matrix.
And the symbol now is made up of matrices of symbols like this.
I used a curly m to distinguish it from that n.
And the same thing for the von Neumann version.
Otherwise, it's exactly the same.
And these two have some special symmetry
as a result of the symmetry here.
And one way of saying it is that they are graded homomorphisms.
What it means is that if you have an even function like in calculus,
f of x equals f of minus x, if you have an even function,
it goes to a matrix which is block diagonal.
And if you have an odd function, it goes to a matrix like this one here,
which is block off diagonal.
And that's kind of logical because block off diagonal times block off diagonal
is block diagonal, just like odd times odd is even.
So it all makes sense.
And so these maps send even functions to diagonal matrices,
odd functions to off diagonal matrices.
And there's another little calculation.
It's not very difficult.
Let me use this funny notation with square brackets.
Where are we going?
The square brackets are supposed to mean homotopy classes
of supersymmetric homomorphisms, morphisms of c-star algebras.
And you could do it.
So each individual morphism gives an element in this group,
this space of, it is a group actually,
but the space of homotopy classes.
And the same thing down here.
So homotopy classes of graded or supersymmetric morphisms of algebras.
You can calculate what this is.
It's kind of fun as long as n is big relative to the dimension of the manifold.
For example, at least the size of the dimension of the manifold.
This is something that the topologists understand.
And it's something that Raoul Barth understands.
It's the k-th here in the sense of a tier and Barth and Husserbroek of t-star of m.
And this is something that everyone understands.
It's just the integers.
There's one component of this space for each of all morphisms, for each integers.
And the way it works is that if you have an operator d,
giving rise to a von Neumann symbol,
new of d, then this just corresponds to the index of this operator d plus that you see right here.
The index in the sense of Fredholm.
The guy who talked to Hilbert who started the story a few blackboards ago.
So we have all of the ingredients.
It's like one of those cooking shows.
Everything is spread out on the table now for the index.
And what the index theorem does is it identifies this integer with, in some way,
expresses this integer in terms of information, which comes right here.
Which is exactly the class in here.
So let's say we have sigma d here.
This corresponds to, well, by good fortune, the element it corresponds to is called sigma d.
The symbol class of d.
And the whole purpose of the index problem is to express this.
In terms of this, I forgot to say it, but at the very beginning,
or at the beginning of this discussion here,
that one of the clues which suggests that index theory should be an appropriate application
of all of this stuff that I was mentioning,
is that the index of an operator doesn't depend on the lower order terms.
You can throw them away.
The only thing that's really important is the principal symbol.
And so you expect some interesting relationship.
And the fact that everything is so conveniently presented in one object,
this c star algebra, the tangent groupoid,
leads you to believe you cannot be more than a few steps away from the proof of the index theorem,
just by expressing everything in like form.
We have the index here.
We have the symbol here.
These things can be continuously varied one into the other.
How hard can it be to prove the index theorem with everything presented?
Well, the answer is not very hard.
It's not very hard.
Let me close by just saying one thing which will,
for the purposes of the experts,
just to show you that this subject is still alive.
Everything that I've told you dates back at least to my early visit here,
where I met Bralbot and so on and so on.
It dates back at least, I don't know, 20, 30 years.
So let me just very quickly show you something interesting just on this one board here.
I hope I showed you something interesting already,
but it will be familiar to many people in this room.
This, on the other hand, is a very interesting puzzle.
And then we'll stop.
So this is maybe ancient history,
but this particular topic is still very much alive.
And one of the places I find this really fascinating,
and I'm really trying to learn this,
and it's torturing me, it's very complicated,
is in the theory of some beautiful work that Bismuth is doing,
Bismuth's theory of the Hypoeliptical Aplassian.
And I don't have time to just indicate the ingredients,
to do more than just indicate the ingredients here,
so interesting how it works.
So here are some interesting features that are relevant here.
One is that there are operators that are not elliptic,
as you might expect from the title.
And the world's smallest example is this family of operators here.
So rather than calling it s, wasn't I?
s squared times d squared dy squared,
with a minus sign, plus y dx, plus y squared.
So Bismuth wants to understand this family of operators.
What he really wants to understand is the following thing.
You're supposed to exponentiate this operator,
solve the heat equation, as if this was the Laplacian,
and then calculate the trace, the heat kernel,
the trace of e to the minus t times this operator,
which is called L.
In Bismuth's notation, it's L1 over the square root of s, like that.
And the idea is to let s go to zero
and reduce the computation involving some symbol,
some principal symbol.
So this is the principal, the Bismuth's version,
if you like, of the principal symbol.
Anyway, there's a lot more going on here.
It's not a function by any means.
If you have a function which is polynomial,
and you take its Fourier transform,
it becomes a constant coefficient differential operator.
So another way of talking about symbols
is as families of constant coefficient differential operators.
And I'll show you what Bismuth's operators are.
They look like this.
There's some discrete parameter involved.
It's the family of all of these,
for this particular guy,
minus d squared dy squared plus y dx plus n squared.
And it may not look like it to you,
but this operator is very much simpler than this operator here.
It took me a whole year to understand this point.
But you're supposed to think of this as relatively intelligible,
and this thing is being mysterious.
And there's a deformation parameter,
which at s equals zero gives you these things here.
And the other part of the ingredient
is a new version of constant tangent groupoid.
And let me just tell you what it is for x-mets.
This is the deformation to the normal cone.
I'll leave that blank for a moment.
So there's a certain construction,
which if you apply it to the diagonal embedding of m into m times m,
gives you the entire theory that I spent the hour telling you about.
What you're supposed to apply this to is not m sitting inside of m times m,
but v sitting inside of m times m.
So what's going on here?
m is the space on which this operator lx, I forgot to tell you,
but you're supposed to think of it as acting on s1 times r.
This is the x variable, this is the y variable,
and this v is s1 times z.
It's a set of closed geodesics, if you like, for this particular manifold.
And so there's a construction,
which is a little more complicated than the one I described.
It gives you a new groupoid, a new family of c-ster algebras,
and that new family is very relevant to the problem of relating this family of operators to this family of operators.
So this subject is very much alive, it's quite a puzzle.
Thank you very much.
Thank you very much.
Any further questions or comments from the audience at this moment?
What would be the range of these indices?
Would it be integers?
So the problem, it's rather fascinating.
It's conceivable that there is an index, family of index problems, available here,
but this is not a problem about calculating indices.
It's a problem of computing.
I mean, of course, this is just a toy model case that we're discussing here.
So some super trace of e to the minus t times,
I gave you just a scalar operator here,
but there's some spinorial generalization, which I'll write like this,
and rather than writing one over the square root of s, I'll just write b, like that.
So what Bismuth wants to calculate is something like this.
The reason he wants to calculate this crazy thing is the following thing.
First of all, he's able to show by supersymmetry that this quantity is independent of b.
As you expect in index theory, there's some rigidity.
This quantity is independent of b.
Secondly, you can actually compute the limit as b goes to zero,
which is the same thing as the limit as s goes to infinity.
And when you do that, you get the ordinary trace of e to the minus laplacian,
well, in this case, of the circle, t times laplacian,
up to some constant, which I may have gotten wrong.
So he wants to calculate not the, an index, not an integer value,
he actually wants to calculate this trace of the heat kernel,
and this tells, gives you information about the entire spectrum, not just zero.
Of course, you can only do this in very special cases, like these,
you can do it in less special cases than this.
He's interested in locally symmetric spaces,
in understanding this quantity on a locally symmetric space,
just like Selberg was.
Yeah.
This is the first part of your talk,
and you have considered this as the summation of these numbers.
I shall function on t star n, or into the operator by the problem.
Can I interpret that as just the quantization of this vector structure on t star n?
Yeah, so, yes you can.
I'm not qualified to talk about quantization
when there are physicists in the audience,
but you can read the works of Klaas Lanzmann, who is qualified,
and you'll find him discussing very closely the,
the extent to which this counts as an actual quantization.
So according to the experts, not me, experts in quantization, this counts as one.
Yeah.
Yeah.
Since the story is still totally quantized,
so do you think it's wrong?
Yeah, that's an extremely interesting question,
and the person who's best qualified to answer that is Bob Yonkin, who's in the back row.
It's possible to not only apply this to pseudo-differential operators,
and I understand the index of pseudo-differential operators,
it's possible to characterize what a pseudo-differential operator is
using this family of ideas,
and that's what Bob did in joint work with Eric Fenner.
So yeah, there's a very strong connection between the theory of pseudo-differential operators
and the tantric groupoid.
Any further questions?
If not, let's thank Nigel Womans.
Thank you.
Thank you.
