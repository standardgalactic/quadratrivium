It's a great pleasure to introduce Marcus Miller.
He's going to be leaving us for the University of Vienna.
We very much missed.
He's going to be speaking on operational interpretation
of entropy and free energy without the thermodynamic limit.
Okay, thanks very much.
So thanks to Wayne for inviting me.
It's really nice to be back at Western in the philosophy crowd.
So I'm really happy to be here.
I'm enjoying this conference a lot.
I'm not talking about guitars now,
but about how we can operationally interpret the free energy
and also entropy without going to a thermodynamic limit.
Now when I say entropy, then I mean the von Neumann entropy of quantum states.
And this is, I would say, a fundamental result.
And some of it is based on work added with former students of mine,
with Michele Pastner and Jakob Schalo,
and also some work with my colleague Matiola Svaglio.
Here's a quick outline.
So you should recognize this picture.
It's very, very similar to what Nelly has talked about.
And I will, in fact, repeat a few things that Nelly said as well.
And I'm quite happy that she prepared the stage in some sense
by explaining a few things to you that I may actually use in my talk.
So in the first part of my talk,
I will give you the standard view on entropy,
which is basically that entropy becomes only meaningful in a thermodynamic limit,
where you have large systems, many particles, or you do averaging.
Then in the second part, I will introduce thermodynamics as a resource theory
in the same way that Nelly talked about it.
I will repeat some of it and also put this in context to the first part.
The third part will be the actual result,
which gives you some kind of new one-shot interpretation of the free energy and the entropy.
Now, one shot here is always to be contrasted with many shots or many particles.
So it's something that you do with a single particle and a single cycle of an engine,
and not in the thermodynamic limit.
Now, in the last part, that's a bit work in progress and some speculation
whether this might tell you something more fundamental about quantum information in general.
Okay, so let's start with a standard view.
In the following, I will always look at a scenario where we have a fixed background temperature T
and in a standard box in a gas paradigm, we would also have a fixed volume, for example.
And then there's this folklore idea that you learned very early in the physics studies
that if you have spontaneous thermodynamic processes,
then these spontaneous processes will always decrease the free energy.
So it's the Helmholtz free energy U minus TS that's relevant in the setup,
and thus free energy can never increase spontaneously.
If it decreases, then sometimes this means that you can really extract useful work from the system.
So by computing the free energy differences in many cases,
this will tell you how much work you can extract from a system via a heat engine, for example.
But all these statements, as we know from statistical mechanics,
and if we just look very closely at our thermodynamic systems, are just statements on average.
On average means we average over a large number of particles and we go to the thermodynamic limit.
Actually work, as we've heard before many times today, is a random variable.
So if you have a fixed process, then, you know, for example, you have a gas in a box
and you insert a piston, you try to raise a weight,
and then you ask in the end by how much did I actually raise the weight,
then the amount of work that you get out from such a process fluctuates.
So in general, it is a random variable.
You have a certain probability distribution over the different possible values of work.
And now, in standard thermodynamics, we usually have systems that are so large
that in some sense the law of large numbers kicks in.
So the distributions that we have, for example, over different values of work that you could extract,
are just very, very strongly peaked around the averages.
So if n is, for example, a particle number, then the free energy would be proportional to the number of particles,
but the spread would be really something like the square root of the particle number.
So we are strongly peaked around the average.
And that means we can just replace the values, like w, just by the averages.
That's what we do in standard thermodynamics.
And this is where the standard prescriptions that we learn in standard thermodynamics will really apply.
So when we say that in some processes the extractable work is really given by the difference in free energies,
then this is a statement that's only true in a thermodynamic limit,
where the number of systems involved with particles is going to infinity.
Because then these fluctuations will actually become irrelevant, they won't really matter.
And the law of large numbers is basically what tells you that.
Now the question is, and this is also one of the motivations to really look at these resource theoretic approaches,
is to say, but what happens if you really have just one particle or a few particles?
So what if you have small quantum systems, or maybe as many, but they are strongly correlated,
so that we cannot just say it's like having a large ensemble of independent particles.
What do we do then?
Then we have a problem, namely the work that we really get, for example, in a cycle,
might have so strong fluctuations that the amount that it fluctuates
is actually comparable to the actual amount of work that we get out.
So that relates also to what Nelly said about the weight that you raise,
and then classically in the usual thermodynamics, it doesn't really wiggle very much.
So you can neglect the fluctuations, but in the quantum regime for small systems
you cannot really neglect these fluctuations.
And so let me see.
Now when you actually look at small systems, then we find that the free energy can sometimes tell us something,
but it will not always tell us the full story.
So here's an example called Lando Erasure.
You start with a system which has two possible configurations, for example,
a particle where you don't know if it's sitting in the left or in the right of this potential.
And what you want is you want to do a process that resets the particle to be definitely
with probability one in the left half, for example, of that potential.
Now, can we do that just for free?
So can we basically erase this bit for free?
Well, Lando Erasure's principle tells us that it's not possible,
and to see why it's not possible, all you have to do is to compute the free energy difference,
and then you will see that in this process the free energy has to go up.
So this means that this process cannot occur spontaneously.
We have to invest some work to do it, and the free energy really tells us that this is the case.
Now, this looks all fine, and this is one way to see why erasing a bit costs you energy,
but then you can make things a bit more complicated,
and for example, look at a puzzle that Bennett has come up with.
So let's say you don't have only two possible configurations, but many others,
and the left two, these two, still have probability one half,
and then you know that the others are not occupied.
So that's basically the same as here.
You just introduce a few empty slots, and then you think,
well, maybe I cannot go exactly to this reset state where I know the particle is, but maybe almost.
So maybe it's like with 99% probability there,
and then with 1% probability spread over all the rest of the levels.
That would also be pretty good.
That would be pretty good resetting of your system.
But if you compute the difference in free energy,
which amounts to computing the entropy difference of both sides,
then you find if n is really, really large,
then the entropy can actually go up in that process, and so the free energy can go down.
So suddenly this criterion would tell you that this is a possible transition.
So that's something that could, in principle, appear spontaneously.
So if epsilon is very, very small, and this would be really surprising,
so why don't we see these processes happening in nature,
or how can we rule them out?
That's what I call Bennett's puzzle here.
So Charles Bennett has come up with something like this before.
Well, so if epsilon is like one in a million,
and you're having this bit that you want to erase,
then just give yourself a few extra levels,
or maybe you have them already,
and then if you just wait,
and then suddenly after a while you're almost sure up to one in a million
that the particle must be in this lot,
that's just not something you would expect to find.
It's very close, in some sense, to the standard Landau erasure.
Of course there are subtleties, and I'm going to be subtleties now.
All I'm saying here is that the standard condition of the free energy decrease
cannot really rule this out,
but we will see that there are now, if we look at,
so okay, as I said, the free energy determines the possibility of state transitions
only in a thermodynamic limit,
so we cannot really apply it to the situation.
So for small systems we will get additional constraints,
which are these constraints that Ozonelli talked about,
and they will indeed solve Bennett's parcel
and rule out that such a transition is possible.
So I will come to that in a second.
The insight that standard entropy and standard free energy
only apply in a thermodynamic limit
also is reflected in information theory.
If you ask about the meaning of entropy,
for example, for data compression,
here's the usual story that people tell.
So to see entropy, on the meaning of entropy,
you need n copies of a quantum state,
so you have n independent copies of one state,
and what you want to do is you want to compress it
to a fewer number of quantum systems.
Now you ask, how can I do that?
Well, one way to do it is to say I want to project it into some subspace
such that the probability to be there is almost 1.
It's 1 minus epsilon, and epsilon goes to zero.
And the subspace, the dimension of the subspace, should be small.
I want to compress it to something smaller.
How small can I make this?
And then turns out the smallest value that you can get,
if you take the dimension of the subspace and take the log,
is basically n times the entropy plus some small corrections.
And s is exactly for Neumann entropy, so trace of rho log rho.
So what this tells you is that if you want to compress quantum information
in the limit of many copies,
for Neumann entropy tells you how well you can compress your state.
But it's only in the limit of infinitely many copies.
So it's not true if you have just a single quantum state
or strongly correlated quantum states.
So this is basically what we,
with the standard view in quantum information in thermodynamics on these quantities,
that you have to have large ensembles to see the entropy at work.
Good.
Now, how does it fit into the resource theoretic framework?
What does it tell us for single copies of quantum systems?
So here's the paper that Nelly also talked about, who's also here.
So, yeah.
So in order to talk about thermodynamics for small and strongly correlated systems,
you would like to formulate it as a resource theory.
And this is the picture you've also seen in Nelly's talk.
So what you do is you say, well, I have some initial system, initial state,
and I want to transform it to some final state.
And to do so, I allow a thermal reservoir, so this is the thermal bath,
and some energy preserving unitary.
And then what we also, and I've seen this in Nelly's talk,
can do is we can borrow an extra system from somewhere, a catalyst.
So we borrow the catalyst that's somewhere involved in the process,
and we get the catalyst back in the end unchanged
and can give it back to the bank, for example.
So these would be processes.
We say, I have my state of my system,
I have the thermal state here of the reservoir, and I have a catalyst state,
and then let's do a big unitary on all of them,
and this unitary should be energy conserving,
so it should commute with the total Hamiltonian.
And when we do this, and then we forget the thermal reservoir,
we just trace out the bath, we want our new state,
and also we want back our catalyst in the end,
which is the same as we had before, so the catalyst doesn't change.
Now, what are the rules of the game of resource theory?
So the rules of the game, as we've heard before, are what are the free operations?
And the free operations here are that you're always free to bring in a thermal bath.
So some system, which is in its thermal state.
So whatever its Hamiltonian is, the state of the system will just be the Gibbs state.
Then you're allowed to do strictly energy-preserving unitaries,
and then you can also discard systems.
You can basically throw stuff into the thermal bath and forget about it if you want.
And anything that's not thermal is then a resource.
So the advantage is that this is really a clear game with clear rules.
It's just mathematically really rigorous,
and you can work out what's possible and what's impossible in this framework,
and this gives you really nice insights sometimes on which transitions are possible,
what kind of efficiency can you have for heat engines, and so on.
So what you also find, and that's also maybe a nice insight of the resource theory,
that if instead of a thermal state you allowed another state to be free,
then the resource theory would just become trivial,
and everything would be possible,
and all transitions between all states would be possible.
That's like an indirect justification of the Gibbs states and why they're important.
That's related to what Owen talked about, about complete passivity of the thermal states.
Good.
Now, we've set the rules of the game.
We now know what our resource theory are,
and then we can work out what kind of transitions between states this theory allows.
So suppose you have initial state rho of your system here.
You want your final quantum state, rho prime.
When can you get from rho to rho prime by a process like this?
And now, this has been fully characterized in the case,
where both rho and rho prime are incoherent,
so where they are block diagonal in energy.
Now, this is of course a restriction.
It means you have states which are quasi-classical,
but it's good enough to talk about Landau erasure, for example.
In Landau erasure, as I've talked about,
you just have probabilities one half, one half,
and you don't have coherences between these levels,
so that is covered by the setting.
So for block diagonal states,
it turns out that you can say exactly when such a transition is possible.
Namely, the free energy, the alpha free energy of your initial state
must be larger than or equal to the alpha free energy of your final state.
And this must be true for all alphas.
So these are exactly the free alpha,
the free energy that Nelly has talked about.
And in the case that alpha is equal to one,
it turns out that this is just the standard Helmholtz free energy.
So the free energy of a state rho is just the expectation value of the energy
minus temperature times von Neumann entropy.
It's u minus Ts, that's again just the standard free energy.
For all other alphas, there is an equation that gives you that.
And what you do is you have to compute a certain relative entropy
between the state and the thermal state,
namely the really relative entropy between these two states.
And all these quantities for all real numbers alpha,
they all have to go down.
Good.
Now, first thing to see maybe just for fun as a warm up
is to see that this really solves Bernhard's puzzle.
Again, as I said, it would be surprising if a transition like this
for very small epsilon would be spontaneously possible.
The standard free energy difference, Helmholtz free energy difference is negative.
So that suggests that it's a possible process,
but we just agreed that it actually should be impossible.
And indeed, we will find that it is impossible
according to these alpha free energy conditions.
So in this plot, you will see the difference in free energy alpha
for the different alphas.
And now for alpha equal to 1, as I said, here is the standard free energy
and the difference is actually negative.
So that would tell you maybe it's actually possible,
but for larger alphas you see, oh, the free energy alpha difference is actually positive.
And since it's positive, it violates the condition
and it tells you that this here is an impossible transition.
So in this framework, according to the rules of the resource theory,
it is impossible to transform a state with these eigenvalues,
these probabilities, to a state with those probabilities.
Good.
Now, what's about work extraction in general
or the opposite, we want to perform some work on a quantum system
and ask how much work do we have to spend.
So as I said, in both cases, work is a random variable.
And here is a paper where, yeah, it's behoridesk in Oppenheim
and they have worked out what the extractable work in work cost is in this resource theory.
Now, in order to model this, you have to say what do you mean by work,
as we've heard before in the talk,
and one way to model work in the quantum regime is to say,
if I want to extract work, what I do is I have this quantum system in the ground state
and I just raise it to the excited state.
It's like a weight that I raise.
That would be work extraction.
Spending work would just be the opposite.
It would be going from an excited state to a ground state.
So in addition to the other setup,
you will have this what people call work bit and you raise it
or you lower it to spend or to extract work.
Then you can ask, well, how much work can I get here
or how much work do I have to invest if this is the energy difference here is W?
What is this W?
And one way to say it is the following.
You say, well, maybe I don't get exactly the excited state,
but maybe I get very close to the excited state.
So in order to have a reliable amount of work in the end,
I want a very high probability of ending up in the excited state.
Maybe it's larger than 1 minus epsilon, this probability.
Then you can work out what this means
and it tells you that if you have a state row,
then the work you can extract from it in the end
turns out to be given by this rainy free energy again
and in this case the zero rainy free energy.
There's a little epsilon which tells you that it's a certain version
of this rainy free energy where you allow an epsilon error.
Also, if you say, what if I just have a thermal state
and I want to create a state row, how much work does it cost me?
It turns out that it's f infinity, so for alpha equal to infinity,
one other of these rainy free energies that tells you how much that is.
In general, you find that f zero is much smaller than f infinity
and the standard free energy is somewhere in between.
This tells you you need a lot more work to create a state
and if you have the state and you want to get back that work,
you can only get back part of that work.
So this is like a fundamental irreversibility
that you get in this framework if you work on a single quantum state.
However, and it's something that Rob has shown first,
the nice thing is that in the thermodynamic limit in a certain sense
you recover standard thermodynamic prescription.
In this case, the simplest setup when you say this is like the thermodynamic limit
is to say you have n copies of your state, many independent copies,
and then you ask what's the work cost in extractable work
and you evaluate these f zero and f infinity
and say divided by the number of particles
and that turns out to just converge in the limit to the standard free energy,
to the usual Helmholtz free energy.
This tells you that in the thermodynamic limit finally,
if you allow a little bit of error but you have a large number of particles,
you really get the standard thermodynamic prescription
that it's the standard free energy that has to go down.
Again, here we have a situation where f becomes important
but in the thermodynamic limit.
Okay.
Now,
what all this suggests
is that for Neumann entropy and free energy are really for many particles.
Can we have a meaning of them for one particle?
That will be something I'm talking about now.
So, this is in this paper here,
which relies a lot on some earlier work with the students I've shown you.
So, here's a picture of the setup
and it's very close to the one we've seen before and also Nelly's talk.
So, we have our system initially and our system finally
and we want to do a transformation.
We also have a catalyst that we can borrow
and that we have to give back unchanged.
And one way to think of a catalyst is really it's just part of your thermal machine.
You have your system you put around a big thermal machine
and you do a thermodynamic cycle.
And part of that machine is this quantum state
and since you want to preserve your machine
you also want to preserve the quantum state here.
Everything happens in a thermal bath
so you have a thermal operation all in all.
You have a work bit where you can spend work or extract work.
So, that's exactly as before.
But now there's something new.
What we do now is we say,
well, what if we allow that there are correlations
that build up between the catalyst and the system.
So, that's different from Nelly's setup.
As Nelly mentioned in the talk as well.
Because in Nelly's setup we would have them uncorrelated
and decoupled in the end.
The sense that we get the catalyst back unchanged is the marginal state.
Exactly. Yes, exactly.
So, if you trace out everything here
the marginal state of the catalyst is exactly the same.
Now, when is that a good model?
So, I'm not saying this is somehow better than Nelly's setup or anything.
It depends on the situation you want to look at.
And I guess here's a situation where correlations are okay.
So, think of a small biological machine maybe here.
And the machine is acting on small particles,
single particles, one after the other.
Maybe it's like a stream of particles that's just flowing by.
And then the machine is acting on one particle.
One particle just goes away and never comes back.
Then it's acting on the next particle.
It goes away and never comes back.
And so on.
So, then all that you need is that your machine is preserved
so that this quantum state just stays the same locally.
But you can have correlations built up with these systems that just go away.
That's perfectly fine for some situations.
And some situations may not be fine
and you may want to actually have them decoupled.
For example, when these systems become correlated with a machine,
they become indirectly correlated with each other.
And maybe this is something that you don't want
because maybe there's a goal that these particles have to do something else elsewhere.
But if this is not the case,
then we can really allow these correlations in the first place.
The question is if you allow this.
Can this really help?
And the answer turns out to be yes, this can help.
And here's a little example.
So let's just first look at the standard setup
where we have a system that we want to transform,
a work bit where we spend energy and a catalyst for the machine,
and a catalyst can be anything,
and it must remain product with everything else.
That's the standard setup that we heard about before.
So A now is just a qubit.
It's a two-level system and it starts off in its thermal state.
So a thermal state in this case where you have energy zero and energy,
well, in this case it's chosen like that.
It doesn't really matter.
But it turns out that this is the thermal state
two-thirds for the ground state and say one-third for the excited state.
And what we want to do is we want to change the state and heat it up
so that the probabilities become one-half and one-half.
That's the goal.
And this is not free, so we have to spend energy to do it
and we will draw the energy from this work system.
It starts out excited with some energy difference delta
and it ends up in the ground state.
Now, all in all, we want a transition like this.
So we have the thermal state on A,
we have the state of the machine of the catalyst
and that will be the same initially and finally,
and the excited state going to the ground state.
Now, what do we have to check on to compute?
Well, we have to compare the alpha-free energies of the left-hand side
with the alpha-free energies on the right-hand side.
And for all alphas, the left-hand side has to be larger than the right-hand side
and if you work that out and compute the alpha-free energies,
you find the smallest possible amount of work that you need to do that
is just 0.4 Boltzmann constant times the temperature.
You can just work that out.
Now, if you just compute the standard free energy difference,
you would find, oh, actually 0.06 KBT should be enough.
But we've just seen that the standard free energy is not sufficient
to tell us what's possible.
We have these extra constraints
and these extra constraints lead to the fact that we need more work
than we would naively think we did.
Now, what if we modify the setup and now allow correlations to build up?
And here's an example where you see that this can really help.
So our machine, our catalyst, is now also a qubit.
It's just a two-level system
and the two energies of that system are just the same
so there's no energy gap
and it will start in the same state as it ends
but in addition, you will have these two systems become correlated
and the local state on A is really the one we want
and the local state on M is just the same as before.
So you would have a transition like this.
You start the thermal state on A, the state of the machine,
excited state here.
You go to this new state on MA
and the ground state on the work system.
Now, how do you do this?
Well, here's an example how you would do that.
I'm not sure how illustrative that is
but it's an example here.
So this is a state of A and M,
so of two of these qubits,
your machine and the system
and it turns out to be a correlated state of the two.
So there's a probability of 1 over 10,
that if the ground state on A
and the zero state on M, and so on.
Now you will find that when you compute the state on A,
so you sum over M,
so you sum over this here,
gives you 1 over 10 plus 4 over 10 is one-half.
You sum over that, you also get one-half.
So you get really the state that you want on the system A
and if you sum in the other direction,
you will get the state on M,
which is the same as you had initially,
so 1 over 10 plus 3 over 10 is like,
so that's like 3 over 10
and the other one would be 7 over 10,
so that would be the state of M.
So that's a possible transition.
And it turns out that this transition
can really be achieved by a thermal operation.
You can really do it.
And if you do it and you ask,
well, then how much should this delta be,
how large does this energy have to be that I invest?
Turns out that 0.26 KBT is enough.
And before we had 0.4 KBT,
so we can do better, we can like save some energy.
How would you check this?
Well, you check that this is possible
by using these Lorentz curves that Nelly talked about.
You see, oh, here's the initial state, the blue one,
and the final state here of the machine
and A and of the work bit is actually below.
So yes, it's a possible transition
and it has helped you.
Now, the question is obviously this.
This is just an example,
but if I have larger, better catalysts,
then what can I do?
What are the ultimate limitations and possibilities of this?
And it turns out that the ultimate limitations
and possibilities are exactly those given by delta F
by the free energy difference.
So in this case, we can really go down further
until the free energy difference like 0.06 KBT here,
but we cannot go down further.
So in this setup,
you would have that theorem here.
So you can go from initial state to a final state
with a process like the one I've shown you
for block diagonal states,
if and only if the standard Helmholtz free energy
goes down in that process.
Good.
So in a way, this gives you a single particle
or one-shot interpretation
of what the Helmholtz free energy means.
So the second laws that you have in situations
where you do not want these correlations,
they actually collapse just to the standard second law.
Only one monotone, one law,
when you allow these correlations to build up.
So here's a result here formulated.
Mathematically, so if your initial state's product,
your final state will be,
so this is without the work bits,
the initial state, final state,
state on M is the same,
the state on A is as close to the state I want,
like epsilon close to the state I want
and epsilon can be as small as I like.
And then this is possible
if and only if the free energy goes down.
Now one thing to notice, you can do this,
in fact, you can ask how big do these correlations have to be
and the answer is they can be as small as you want,
in fact, but they cannot really be zero.
Also here's a conjecture.
So this is only for block diagonal states.
It's quantum states that have no coherence in them.
But we conjecture that this should actually also be true
in the presence of quantum coherence.
Something similar like that should be true
and that's what we're looking at the moment.
Now yeah, this is maybe a funny comparison.
So this is somewhat reminiscent
of more old school thermodynamics.
Like think of Boltzmann's Sturzzahl Ansatz
where you have a box of gas,
you have particles colliding
and then the assumption is that the particles that collide,
that the velocities of these particles
are initially uncorrelated and they factorize.
But obviously after the collision they will not factorize
and you will naturally get correlations.
And this is a bit what's happening here.
So the system ages, so to say,
by creating correlations.
What about work costs?
So I've just shown you an example where we invested work.
What can we say here?
Turns out that the work cost of some process
becomes really just the Hamilton's free energy.
On a single particle you have exactly zero fluctuations.
So this is again what we want.
We have initial state, we have the final state,
the machine is preserved.
From an excited to a ground state.
And then you can do it with an energy
that is basically the difference in free energies.
A little bit larger,
but any value that's larger than the difference
will allow this transition.
So basically it's delta F that you need.
Also the machine is finer dimensional
and it's exactly preserved.
There are some correlations with the other system.
So in a way,
this is really surprising I would say.
Because the system that you send in,
that could be a very,
well it's a probabilistic system
with a lot of probabilities.
It's high entropy possibly.
And what you get is here are very controlled proxies
where you exactly flip the work bit from excited to ground.
And there is no probability distribution
over the work you spend.
You just spend this work exactly with probability one.
And you can do this by having a very clever catalyst
that allows you to do this on a single copy of your state.
I think it's quite surprising that this is possible.
Now what about extractable work?
Extractable work is not quite as nice unfortunately.
And for the same reasons that in Nelly's work
you have to talk about near perfect work extraction.
So it's the same reason.
So if you extract work,
you can really extract the standard Helmholtz 3 difference
from one copy of your state.
But what you need is,
you have to deal with a little bit of fluctuations
that you can not make zero,
but you can make them as close to zero as you want.
So in quantum information jargon
what you would need is called a max entropy sink.
What you have is you would have a bunch,
like a state here that's your ground state.
It has probability one for the ground state
and zero for the others.
And what you want is you want,
yeah, excited state,
but you have to kind of move some entropy,
some max entropy to some other level.
So, but epsilon can be a smallest possible.
Let me not go into details here,
maybe skip this for now.
Let me just say that the fluctuations now cannot be zero.
But you can make them as small as you like.
And the fact that they cannot be zero
turns out to be related to some versions
of the third law of thermodynamics,
which would however be another story to tell.
Good.
So where does it follow from?
Well, here's a theorem on majorization
that's basically the workhorse of all of that.
So we've seen in a picture,
I've shown you these Lorentz curves
that tell you whether a transition is possible or not.
And this is a result of majorization
that tells you when does a state P
majorize another state P prime,
but you allow this catalyst P y prime
that remains unchanged.
And then it turns out that the standard
n entropy and what's called the Hartley entropy,
it's a number of zeros or non-zero entries.
They basically tell you whether this is possible or not.
But again, let me skip this.
Let me talk instead about this here.
So this is another way to fuel the result physically.
So that's another way to do this
with Matteo Los Tagliou's earlier work,
which is kind of a weaker result,
but it also maybe tells you what the physics behind is.
So again, you ask if I have a thermal operation,
I want to go from initial to final state,
can I do this, yes or no,
but now I allow this extra proxies where I have catalysts,
maybe three of them,
and they have to be given back exactly as before,
but allow correlations to build up between them.
Can this actually help?
And the answer is yes, this can help,
and transitions that you would not otherwise be able to do
can sometimes be done if you allow correlations to build up.
And one way to see this is that you,
if you do it without this,
you would have strong fluctuations for your work extraction,
but when you do this,
you somehow move these fluctuations into the correlations
between the different parts of your system.
And from the point of view of standard thermal dynamics,
this is actually a bit surprising if you think about that.
So if you start uncorrelated and you put in correlations,
what this means is you put in free energy into your system.
So the free energy of a correlated system
is always larger than the free energy of the uncorrelated system.
So you would think, well, here I put in free energy,
that should make it even harder to do the transition,
but it turns out that in this context it actually helps you
and allows you transitions that would not otherwise be possible.
You can trade fluctuations for correlations in that sense.
Good.
Now, what I've shown you so far is really about thermal dynamics,
but we think that this is really just a shadow
of some more general properties of von Neumann entropy
in quantum information theory.
But this really you don't know yet,
so this is just speculation.
So there might be similar results in quantum information.
And usually when you look in quantum information papers,
when people consider all kinds of tasks,
they say, well, what if I want to do decoupling of quantum information?
So like in this paper, I have a big system correlated.
I want to put it in a state that's decoupled
and how much do I have to throw away to do this?
And then the answers to these questions
always depend on entropic quantities that are like entropies,
that are very strange entropies.
So like the max mutual information, for example,
or these rainy entropies that have shown you f0 and f infinity
that are usually the ones that have show up here.
But so typically this task we ask,
I want to do them only once.
I want to do them on one copy of a quantum state.
What can I do?
These are usually the one-shot entropies.
But given what I've shown you in this thermal dynamic framework,
you may wonder whether really if you just have one quantum state,
whether in some settings it's actually the standard
for Neumann entropy can really attain some operation interpretation
if you allow correlations to build up in your setup in some way.
So why is this interesting?
So one reason for us to look at this
is that you have all these high-energy physics people
that tell your story about entropies.
Namely, the standard entropies, the for Neumann entropy,
has a space-time interpretation.
So this is something that some of you may have seen.
This is in the context of string theory.
We say, well, I have a field theory, a quantum field theory
on the boundary of some space-time.
And then I can compute the for Neumann entropy of a region, for example.
This is really the standard for Neumann entropy
and no other entropy.
And now they claim that this has something to do
with some gravity theory inside of this, so it's in the bulk.
And they claim, for example, if I compute the for Neumann entropy
of something on the boundary,
I will find something like the area of a minimal surface
in the interior of that region.
Now, you ask, why should that be the case?
Why is it the for Neumann entropy
and not one of these other entropies?
And what people also want to say,
well, here on the boundary, I can give this entropy a meaning.
I can tell you something,
that it means something about data compression on the boundary.
And maybe I can go then to the bulk
and give it also a meaning, an operational meaning.
But the problem is, of course, you need many copies on the boundary
to give standard entropy a meaning.
But you have only one copy of your space-time.
So in this context, what you would really like to have
is a one-copy operational interpretation
of what S means on the boundary
to say something about the theory in the bulk.
So what's possible here?
So I don't know yet,
but I will just give you, well, actually two examples
of what you do know.
So here's just a very abstract situation
in quantum information where you say,
I have an initial state.
I want to transform it to a final state.
And I want to do it by a big unitary operation.
Now, I have helper systems,
catalysts that helped me achieve this.
So I have some other state,
that I get back in the end unchanged.
And if a third state,
actually maximally mixed,
that I also get back in the end,
but I allow it to become correlated.
What can I do if I allow this?
Now, first thing to see is that
if you look at the intro piece initially
and the intro piece finally,
then you will easily see
that the entropy of your blue state here
has to go up.
It's actually one line calculation.
So initially you have a sum of the three entropies
that's here.
And then finally you have this big state,
whatever that is, and that's the same entropy,
because the unitary doesn't change the entropy.
And then there's a property called subeditivity,
which tells you this is less than or equal
than the sum of the entropies.
And you see that this means
that on A, the entropy has to go up.
So this is necessary.
But is it sufficient?
It turns out to be actually sufficient here.
So if and only if the entropy goes up here,
can you do this process?
So this is already an example
of a single shot or one particle
meaning of the phenomenon entropy.
Now, obviously what you would want to have here
is that we don't have three systems, but just two.
You really just have a catalyst,
you do something to your system,
you get your catalyst back,
and you can use it on other systems.
And here's a conjecture that we think is very natural,
and we were already surprised to see
that nobody has looked at it.
So namely that the phenomenon entropy
has the following meaning.
So if you have two states
of the finite dimension,
a different spectra,
the entropy, say they have both the same,
so the rank is the number of nonzero eigenvalues,
say this is the same,
then really when you compare the entropies
and the entropy of the second one is larger
than the entropy of the first one,
then this tells you that you can have a unitary
that transforms row to row prime
and leaves your catalyst unchanged.
So this is a conjecture that we are not able to prove,
but it would be very interesting
if it would give the phenomenon entropy a new meaning
just in the context of unitary quantum mechanics.
What we really found unbelievable
is even the classical version is completely open.
So classically, if you just look at probability distributions,
you say, well, I have my probability distribution P
and I have a catalyst Q,
I do a big permutation on both of them,
and then I want to get back my catalyst
and on the first system I want to get the other state.
When can I do that?
Answer if and only if the Shen entropy goes up.
That seems to be true as well,
but it's not proven and nobody seems to have looked at this.
And we've tried like half a year to prove it unsuccessfully,
so if you're bored on one afternoon,
you can sit down and have a try.
But maybe you can see the appeal of that
because it would give the phenomenon entropy
an interesting meaning and interpretation.
Okay, so let me stop here.
So what I first shown you
is that the usual interpretations of free energy
and the phenomenon entropy,
they're only meaningful in the thermodynamic limit.
We have many particles.
Or information theory,
you have many copies of your state.
Now I've built here on the resource theory approach,
and when you do that,
and you do the usual notion of catalysis,
then you will find that you get these extra second laws,
these additional constraints
that all these infinitely many free,
alpha free energies have to go up in your process.
But then I've shown you that
if you allow correlations to build up
in a way that in some situations doesn't matter,
and others it may,
but if you allow that,
then in fact this restores the standard second law
in its usual form.
So this gives the free energy
and operational meaning for single particles in the sense.
And then I've shown you some conjectures
how that would,
how some related results in quantum information theory
would wait there to be found.
Okay, that's it. Thank you.
Questions?
Just on Bennett's puzzle.
I'm just going to get some insatiable out of puzzling.
I mean, the normal way one thinks about
to add out several types of situations
would say, if I've got two to the n bits
in which I store information,
then for sure the system is either
in one or two of possible memory configurations,
but it's definitely not in the other models.
The normal way to think about that as a principle,
that's only more useful in results
than just having a one-bit memory component,
which is an important way out of those.
So, I mean, should I be surprised
that if I have this very large,
definitely empty memory,
I can use it to do things that
could at all do a very similar thing?
Well, it's a little bit of a taste
whether you should be surprised,
but maybe it's more surprising
if you think of it in doing it this way,
so let me go back to it.
It's fast enough.
So, what you can say is you would say,
let me choose my epsilon really small,
like 10 to the minus 100.
No, sorry.
But n is very big.
So n is very big.
Well, n has to be really big, yeah, sure.
But, well, not sure if 10 to the minus 100
is illustrative to think it really intuitively,
but say ridiculously small epsilon.
And then, obviously, n has to be taken
very, very large for that puzzle
to be surprised, to make any sense
and to give you delta s, large, and zero,
but let's make it really ridiculously large,
but still finite.
Now, what you have is in this process,
you do this process,
and then you measure, is it really in the first one or not?
With ridiculously high probability,
you'll find that, yes, obviously, it's in the first slot.
So the probability can be ridiculously close
to one that you can really trust it.
And what you've had is you had n,
so you had a resource with n minus 2 empty slots,
and you made it into a resource with n minus 1 empty slots.
With a probability where you can really bet your life on it.
So, still, I mean, it's really big, the system, yeah?
So it's still quite surprising,
and it's somehow interesting to see
that the alpha free energies will rule that out.
So when I take it, it's there.
It's not as if, you know,
being at the fact that doesn't do this
doesn't in any way let me get around that time.
Even if I did have the power to do this translation,
it's still the case if you believe it.
And a fantastically big memory that I had as empty
was already a resource I could be used to do.
Yes, so, I mean, this is not really a paradox
for anything that we know about standard thermodynamics, even.
That's true, yeah.
I see the curiosity of the fact.
Yeah, yeah.
It goes somehow against intuition, I'd say.
I'll just make a follow-up.
So Judea and Pearl is a nice criterion
for what you can have as a satisfactory resolution of the paradox.
Which is that once you've got it,
you should be able to go back and see
why your intuition was mistaken.
So in that sense, I don't think you've really resolved this paradox.
So I didn't share your intuition that this should be possible.
And I don't think it's the case that when you look at your resolution
and say it's about all these conditions,
that you can go back to your intuition and say,
ah, what I was mistaken about, the reason I thought it was impossible
was because implicitly I was assuming catalytic conversion,
not non-catalytic conversion.
But when you expressed your intuition,
there was no mention of whether catalysis or failure of catalysis was involved.
So it seems to me, you know, unless somebody,
you know, a priori, I have no intuition about whether this should be possible.
You know, you work it out for the catalytic and non-catalytic cases,
you get the answers.
I didn't have strong intuitions either way.
Well, so first of all, obviously,
this is not my goal here to solve that particular puzzle.
I was really just trying to illustrate
what these alpha conditions can yield to you in some cases.
And obviously, it is not a paradox.
It's not.
There are many copies of the left-hand side.
You can go to many copies of the right-hand side.
You can.
Or you use these correlating catalysts that you talked about.
You can really do that.
So it's not in that sense a paradox.
Still, I think, if you look at the one-shot situation,
like you're just born once,
and you only live to ever do this experiment once,
and then you pass away.
Then what this tells you is you can really,
you can really, you know,
if you could do it in one shot,
without any other changes in the world,
then it would be,
I still think that would be really surprising.
And the resource theory tells you
you cannot do it in a one-shot manner.
So really, to do this transition,
you need many copies of the first,
you need many copies of the second,
and ridiculously many, and then you can do it.
But if that's all you can ever do,
and you do it only once,
and then if the absolute is really small,
that's the only thing that matters to you.
If it's one in a thousand, it will probably work,
and then it's done.
In the single-shot case, I still find it surprising.
And in the single-shot case,
without having any other changes,
including correlations with other systems,
it's indeed impossible.
It's a bit the intuition.
That's, again, yeah.
Any questions?
Maybe a really basic question,
but I was confused when discussing the process
to heat in this quantity.
You said the goal was to heat from a finite,
from another ground,
heat from temperature to temperature.
But it seems like you're describing
just heating it from T to T plus some value.
It doesn't really seem like you want
the infinite temperature at the end.
I was like, does infinite temperature...
I mean, I just want you to understand how...
Yeah, I mean, so here it's completely...
It's just a state that I want to get.
Yeah, so it doesn't really matter
that it's infinite temperature or what.
You just say, that's the state I want, can I do it?
But regarding...
Yeah, so the thing is that,
as long as your temperature is finite,
you will always have a higher occupation probability
in the ground state than you have for the excited state.
And once temperature goes to infinity,
it really becomes equal.
So in that sense, that will be the infinite temperature.
At the end of your talk,
at the very end, you showed these two parallel conjectures,
which seemed...
you had to find a possible version.
They looked totally analogous.
Prior to that, you made it seem as if your results...
that the intuitions about this sort of result would be different,
depending on whether you're thinking about it quantumly or classically.
The intuitions would be different?
Yeah, that you had a slide where you said it's exactly the opposite.
And so it's not this part, but an earlier one,
going back further.
Yeah, it's like the opposite.
Keep going back.
Further? Which part?
Yeah, in the new one shot of dictation of free energy.
Okay.
Maybe...
Forgot costs?
Although the one before, maybe...
this.
I'm not sure where it is.
There was a slide where you said the core mathematical result here
is Px tensor Py prime is majorizing Pxy.
Yeah, this one.
And so the very next slide you sort of said
was not what you would expect from a quantum perspective today.
Am I right about that?
Oh, no, so here's...
I talked about something surprising there, but...
That's not relating to the previous slide.
No, it's something you...
All I said is, hey,
standard thermodynamics with standard free energy tells you
systems with correlations have more free energy
than systems without.
So if a pump in correlations
increases the standard free energy
from left to right.
So I think that this makes it even harder to do the process.
Now, the surprising thing is that it makes some transitions easier here.
So standard thermodynamics just would tell you
that this makes it even harder, but it really helps you.
That's a surprising thing here.
Does this have to do with the fact that the conditional
and the quantum link are negative?
No, this is more related to the fact that
this inequality is just wrong for the other F alphas.
So for the other alphas...
So this is called sub-editivity here.
Yeah.
That you just look at the system separately
and then you add up the free energies
and it's less than the total free energy.
Actually super-editivity, I should say.
It's called super-editivity.
And the other F alphas are not super-editive.
So this is block-dagen also.
It's basically classical here.
So that's why I don't think it has something to do with it.
It's about sub-editivity.
Mostly.
This is just...
The correlations say they can be classical.
Yes.
Yes.
Again, this is all so far proven in the case
that I mentioned here,
where all the states are block diagonals.
So let's say it...
Hopefully I say it here.
Block diagonal energy.
These are really diagonal matrices,
so you can just read them as occupation probabilities.
If you have coherence in the game,
then it becomes a bit more involved.
And we think there is a version with coherence,
but it's a bit subtle about what's possible and what not.
And there, I think you do need entanglement,
not just classical correlations.
Sorry?
Can you come back to this slide?
How to do that?
Oh, yeah.
That was it later.
Yeah.
So, I guess the motive of this question
was to say that I'm allowing the transition...
I'm enabling the transition
by creating correlations
within the calculus itself.
But since the free energy of that correlator to calculus
is actually larger,
then you're not exactly stealing from the equation.
Yeah, well...
At least according to the free energy dimension.
I want to touch on something that I had in mind before.
What happens if you think of it
from a very operational perspective,
in the sense that what happens if I insist
that I would like the states to be...
I would like the correlation to be destroyed.
Yeah.
No.
So, this is a good question.
The thing is here, this is a very different setting
because here you cannot reuse these systems.
They are just stochastically independent.
You do this once and then they're used up.
So, that's why the paper was called
Stochastic Independence as a resource.
You have stochastic independence
and you kind of burn it and then it's gone.
So, I shouldn't even call these catalysts.
It's like an extra system.
It's like a big oven where you put in...
Instead of coal, you put in stochastic independence
and you burn it and then you get up correlations
and then it's over.
So, with the stream of particles,
then we can think about it as...
In the scenario when you have the stream of the fine...
Yes.
...and one direction, then we are actually really
independent in continually as a resource.
Exactly.
That's a good way to see it, yeah.
So, in fact, we got this result
when we were trying to prove the other one.
So, this is not really what we wanted
because that's not a cyclic process.
Whereas the other one can be seen as cyclic to some extent.
So, when you think of this machine here,
let's just...
Where is it?
Yeah, here.
No.
Yes, yes, here.
So, in a way that's cyclic.
It's cyclic in the following sense.
Namely, locally your machine
stays exactly the same.
Yeah.
But you bring it, you assume that whatever comes in
and that's fresh new stuff
is initially uncorrelated, exactly.
And so, in the old picture,
that would be the resource that you also get
would be this initial independence.
And it's a real difference here
because here can we say,
well, maybe I have this biological machine.
I just act on these particles once after the other.
This is fine for me. This is cyclic enough.
But obviously, in the other case,
that's not cyclic enough.
You can do it only once and then you're done.
You've burned your stochastic independence
and then there is it here.
And then it's over.
I mean, I find it interesting because
this kind of tells us that
moving in fresh tanks and products
can act as a resource.
But also, our intuition also tells us
that if we have correlations,
we can exploit these correlations.
So, is it both?
I think both can be a resource
depending on the circumstance.
I think correlations remain a resource
if you have many copies.
In the thermodynamic limit,
because then you find the standard prescription
of the free energy,
and they still tell you that it is a resource.
Whereas in the thermodynamic limit,
stochastic independence loses its resource character
in all situations.
I mean, that's a way to see it.
Rob, would you have one more?
Sure.
So, this is very interesting.
Do you believe that this notion of catalysis
where you allow correlations
is a better mathematical representation
of the intuitive notion of catalysis
that you just see in old thermodynamics textbooks
that says, you know, you cannot use the machine over again?
Because in some sense, you know,
you just ask, what do I need
to be able to use this machine over again?
Do I need the correlations with everything else
in the same, you know,
if I'm only interacting with the machine
with the next system,
because other systems are not normal?
It seems not.
So, would you go so far as to say this is
the right way of mathematically formalizing
the intuitive notion of catalysis,
and therefore the lesson is that
all the usual thermodynamic relations
do apply into the single shot region?
No, that's not what I would say.
Absolutely not.
I would say it depends really on the task you have in mind.
Because the task here,
if you think of this little biological machine,
then, you know, just taking this intuition
one step further, if it's really biological,
it does something quickly to that particle.
Yeah, then whatever you do here
will typically create correlations.
It's very hard to avoid fully
that in the end you have correlations.
So in many cases, you will get them.
But they have big disadvantages.
For example, maybe these systems here, later on,
so for example, they are strongly correlated.
You want to heat them up
with like 50-50 probability for up or down.
But maybe they are so strongly correlated
that they're actually all up or all down.
And maybe these have to do with the working
of something in your body,
and you want that it's robust under errors.
And if you have 10,000 of these molecules
working in your bones somewhere,
maybe some of them should collapse and others not.
But here, maybe they all collapse individually.
So it may be a big disadvantage
to have the correlation between the individual systems
mediated by the machine,
depending on the task you have in mind.
I guess that's something.
You could make them arbitrarily small,
but then you would need a really large catalyst.
And even if you have them small,
you would kind of still,
enough particles would still collect them up somehow.
So it's always a trade-off to be made,
where you can trade one thing for the other.
Like, smallness of the catalyst
versus size of the correlations and so on.
So there's something I'm missing here,
which is if that stream of particles is considered
as the systems I'm doing work on,
then, you know, in the standard way,
by imagining a building a heat engine,
it's going to put marbles on a shelf.
You know, marble one comes by and put on a shelf.
There's kind of nothing in the description
that suggests that in the future,
all those marbles that I lifted earlier
are going to come back and do operations on them,
and the fact that they're correlated,
it's going to be relevant.
That was just, you know,
there's nothing in the definition of the problem
that suggests I'm ever going to see those marbles again.
So why do we care about that correlation?
Well, it really depends on the context
in which you do this transformation.
So suppose you want to use this machine
to maybe you're very bad in deciding in the morning
what kind of breakfast you have, okay?
We have two kinds of breakfast.
Sometimes you have oatmeal and sometimes you have cake.
And you want to make sure that sometimes you have oatmeal,
sometimes cake, quite unpredictably,
but in a mixed way so that you don't get bad digestion.
Not always oatmeal or always cake.
So then what you do now is you say,
well, for every day I have no marble,
I have like a coin and let it prepare by this machine.
And, you know, so I always have,
like there's oatmeal or there's cake,
and it's always 50% probability.
And it's put into little boxes
and every morning you open one of these envelopes
and then there's the next one, next day, okay?
And then, oh, randomly,
if you just look at one single day,
then yes, randomly 50% will give you a cake
or 50% will give you oatmeal.
But if you have very strong correlations
and maybe you have any of them, yeah,
and then might be so strongly correlated
that in fact they're all the same,
all cake or all oatmeal.
And I guess it's not what you want.
The first day you open the first envelope, oh, oatmeal.
The machine worked very well, it randomized.
I didn't know what happens, very nice.
You eat it.
The second day you say, oh, oatmeal again, oh well, okay.
But then after 100 days, you will be really annoyed.
And so this would be a task that would just,
this would not be helpful.
And I guess, I mean, depending on where you want to use that
and how you want to combine these systems with each other,
what kind of task you want to accomplish with them,
it's okay or it's not okay to have the correlations.
Other questions?
This is probably something I want to be able to answer myself
if I've been tracking the technical details, right?
I've sort of lost many people
how much of what's going on here
is because of small systems in single shot aspects
and how much of, and how much squadron chemical
versus gas at all.
And I didn't even sort of disentangle that, so to speak.
Yes, so all I said so far, you can read fully classical
for discrete classical systems.
So just to have probabilities of configurations,
and I don't know which of the configurations I have.
In the Guantan case, you have to deal with
more subtle questions and nearly talked about this.
The problem is in the Guantan case,
we don't have these nice Lawrence curve pictures
to tell us whether thermal transition is possible or not,
but it's much more involved.
And it's really open so far with this theorem
that I've shown you that you can do this
for the free energy, whether that's possible or not.
And right now it seems that it's in a subtle way impossible.
You need an infinite dimensional machine system.
It seems in the Guantan case to really do it.
And then the question is, is that fair or not?
Can you really control these infinite dimensions?
That's very subtle.
But all that is in principle that I've shown you,
in principle classical.
Okay, let's thank Marcus.
Thanks.
