So, fine.
Well, many of you asked about references and as a first input, I wrote down on the table
some basic references to what I've said, essentially, I mentioned two books.
One is the book by Breuer and Petruccione whose first edition goes back to 2002.
This actually is not simply a book on OpenQuantum System.
I would say it is the book on OpenQuantum System.
So, a very valuable publication.
There is also a more recent contribution, a smaller amount, but pointing to some more recent result if you want.
A book published by Springer by Rivas and Welga.
And, of course, I've been biased a bit by the lecture notes I prepared for my students in Milan.
So, some part of the stuff and some other references you can find there.
And maybe this doesn't cover exactly everything with that out, but maybe for the end I will give you a printout of a more detailed reference.
These are the basic ones.
More enjoyable and more wide in scope.
So, we were dwelling with our projection operator technique, which I introduced, essentially, to say, OK, so it is, in principle,
possible given a microscopic dynamics to write down is formally the exact evolution equations.
In particular, I pointed to the fact that one introduced a projection operator, which is useful to the extent to reduce the amount of signal freedom you are taking care of,
but still not to give you the dynamic of the reduced system.
And you can come, even though I didn't go through the details, but you can believe me, that you come to a closed evolution equation for the so-called relevant part of the statistical operator.
Rho of t is now an operator referring to both system and environmental degrees of freedom.
In the procedure I mentioned before, I came to actually to one kind of evolution equation, which was of integral form.
So, it was the integral between 0 and t, where variable of the tau over a certain kernel, depending on t and the variable tau, acting on p applied to Rho of tau.
So, this is a proper integral equation with an integral kernel, in principle, very difficult to solve.
You can go to a similar path, so always use this idea of projecting the equation of motion on the relevant part, which is called p of rho,
and the other contribution, which altogether make up the whole statistical operator.
This is sometimes called irrelevant part. Relevant or irrelevant is simply reflecting our viewpoint.
This is what we are looking for in order to recover the system state. This is what we are not using.
Now, you can go through a similar path and end up with an equation which is local in time.
So, we come to an operator that we call still k, since it depends on only one time, so there is no conclusion here, which acts on p rho of t.
So, this is a local, an equation local in time.
So, there are two analog reductive techniques, which needs to form all exact equations, the one in integral form, so called the Nakashima-Zvansik,
and the other in a time local form. This is called the time convolutionless approach.
Now, what do they have in common? They have in common the fact that for both of them, by taking the partial place with respect to the environmental degrees of freedom,
we end up with an equation which gives us exactly the evolution in time of the object we are interested in, which is the reduced statistical operator of our system.
In the one case, this will be written as a kernel, which is not exactly this one.
Now, we are taking the partial trace, so it's the transformation of these objects.
So, I'll use a different name.
So, I need to be able to come over properly called Nakashima-Zvansik kernel.
This is an operator at the level of the system only, still depending on two times, acting on the statistical operator of the system, which depends on the lapsing time.
So, in principle, to solve the equations, you need to know the state of the environment through time.
So, this is typically associated to a notion of memory, and we will try to see that this might be the case, but it's not necessarily the case.
On the other side, we end up with another contribution, which I write with a vertex TCL, to mean that this is actually an operator at the level of the system only, which provides you local in time evolution equation for your statistical operator.
Of course, there are a lot of details in between, but what I want you to find out is that there are essentially two classes of evolution equations which you can find out, the one in integral form, the other in time local form.
If you retain all perturbation orders, sorry, these are exact equations.
So, in particular, they present not only trace, but also positivity, and they are completely positive.
The problem, or the physical piece, arises when you start doing a perturbation expansion, and typically what is done is to proceed to a second order perturbation, which for the Nakajima-Tzpanzi kernel leads to an equation of this form, which you might have encountered before.
So, you retain an integral over time, and what you have here, second order means that the operator L of T, the Ljubiljan introduced before, which was given by the commutator with the interaction term in the interaction picture, appears twice, essentially.
So, the structure of this equation is as follows, the trace over E of the commutator of the interaction Hamiltonian time T, commutator with the interaction Hamiltonian interaction picture time tau with rho s tau tensor rho a.
And this comes about because you have applied one projection operator, which in standard form means take the partial trace, obtain the new state, and make it tens of product with a fixed state on the side of the line.
So, these are double commutators, and I end up with this.
So, you see this is a typical structure, but this structure doesn't warrant complete positivity.
So, one can indeed see that if you end up at this perturbation level, which is in second order in the coupling,
this is the first approximation, which doesn't necessarily guarantee that your time evolution is well-behaved.
A similar result can be obtained for TCL.
If you go to TCL after the second order, you obtain the very same expression where here you recover your statistical operator at the given time.
This corresponds to the fact that indeed this has to be a local time evolution equation.
So, also from the simple fact that at the second order, you obtain different equations, you realize that they are in general not exact.
So, they don't give you the exact time evolution.
It's a matter of convenience, which one you're going to consider.
Of course, the one local time is easier to be evaluated.
This integral equation generally has better properties as far as convergence is concerned.
Perturbation expansion is suppose you put in front of your actual term a coupling term lambda, a dimensional, which you later equal to one.
This is the order of the equation.
Now, this said, I go for the first example, which is meant as part of a tutorial.
So, I try to introduce them set-wise along the lectures.
This is an example, which I will present sketchily, but where you first see the appearance of these concepts I introduced before.
So, what do I take as an example?
Suppose you have a two-level system.
So, we have a two-level system and the separation among levels is fixed by an energy state, say, omega zero, kk bar equal to one.
So, this is our system in our picture.
And this system is coupled to a collection of bosonic modes characterized by certain frequency, omega k.
And there is a certain coupling whose strength is given by certain constants g sub k.
In principle, you consider many of these modes, possibly the numeral set, which is a possible model to mimic the action of a proper bosonic field.
So, in the essence, we have a two-level system coupled to bosonic reservoir.
The Hamiltonian of the system, in this case, can be taken of the following form.
This is sigma z, a standard Pauli operator.
This is the difference among levels.
For the environment, I consider a set of harmonic oscillators with frequency, omega k, and I introduce the annihilation of creation operators such that the commutation relations indeed corresponds to bosonic modes.
And now, the point is how to characterize the coupling.
We characterize the coupling as follows.
We take a coupling which commutes with the system Hamiltonian, which means that this interaction will only affect the coherences but not the populations of our system.
The interaction can be written as sigma z times, in the sense of terms of product, we are the operator part of the side of the environment.
We take a sum over k of g k v k dagger plus g k complex conjugate v k.
So, this is a linear combination and v and v dagger essentially kind of position.
This is the choice of this coupling is due to the fact that this leads to a typical defacing effect so affecting the coherences of our quantum system.
It is a quite widespread situation if you consider a system which might be an atom or whatever, coupled to a complex environment which only affects the coherence.
We don't drive excitations, we don't exchange direct excitations with the graph, but the dynamics is affected through the coherence.
Now, the point is that this simple situation can be exactly solved and still be of interest in many situations.
Also, quantum biology is typically taken as a relevant coupling when you want to describe the coupling between your system and the environment.
The fact that H has commuted with interaction allows you to exactly solve the dynamical equations, which means that in this case we can write down the map and check all the properties we introduced before.
So, let me start on this side of the blackboard.
In principle, what you have to do if you are able to is to obtain rho s at time t as a partial trace over the environment, which means over the bosonic modes in this case.
Given the overall interaction Hamiltonian, which is now known since we fixed all elements, take an arbitrary initial state for our two levels system,
hence a product state, which is soon to describe the bosonic modes, which we take generally to be of thermal state.
So, e to the minus beta h e, beta is the inverse temperature over the contribution, which is the trace, US e t mega.
Now, the point is that this can be exactly evaluated because this object is essentially known.
The idea behind this is that given the value of sigma z, you have a certain action and displacement actually on the state of the map.
If you accept this, you easily find out that a matrix representation of the state in the whole time is such that the diagonal matrix elements, which represents populations,
so in a sense the classical degrees of freedom, if you want, stay unchanged while the off diagonal ones are modified by multiplication by a certain function,
which is now convenient to write in this form e to the minus gamma of t.
Okay, I'll slow down.
So, maybe I will put less content, but okay.
No, no, it's fine.
I mean, it's more important to convey something than to come to a certain group.
So, I hope up to now everything is clear.
Of course, there is, I mean, there are in between a lot of calculations, but this you can take for granted if you want or find, for example, in test books.
Now, the idea is to concentrate on the main part.
So, I give you the idea why this can be evaluated, essentially because the Hamiltonian of the system commutes with the interaction on the problem.
So, in a sense, this is a simple situation.
So, given that this can be calculated, you indeed obtain this operator and evaluate the partial phase coming to the following result.
The idea is that, indeed, this is a two-dimensional system, which means that all that you have to check in the essence is how this matrix element behaves in time,
because they are on the diagonal fixed by phase preservation, and how these elements behave in time, because they are fixed by the complex conjugate.
Essentially, you have to take care of the two elements.
This is fixed because a sigma z commutes with interaction on the problem, so it does not evolve in time.
So, in the very end, you have to evaluate this matrix element.
So, everything boils down to put here this element, which would be the off-diagonal element, and see how it evolves.
So, this is something that you can trace.
If you do this, you end up with an effect which is simply multiplying the initial value times this function.
How does this function look like?
So, this function, gamma of t, can be written as follows.
It is given by an integral of a frequency with a certain function, which I will now explain more in detail,
j of omega, which is called spectral density.
And this contains the information about the modes of the system by coupling to the environments.
We come to this in a minute.
I am going to finish this expression.
A contribution which actually takes into account the fact that your environmental system has, in principle, a finite temperature.
This is given if you walk out the calculation by a hyperbolic cotangent with arguments, beta, omega over 2.
And another factor which is 1 is cosimos omega t divided by omega square.
Of course, this comes out of the calculation.
The important fact is this takes into account the properties of the environmental state.
It's a thermal state and dependence of temperature appears here.
And this quantity, this so-called spectral density, can be written formally in this way.
So what you end up with is a calculation in which if you replace j of omega by a sum over k, k, label the modes of the environment,
and you take the coupling constant square, so g sub k times the direct delta omega minus omega k.
So you would, if you insert this expression here, you end up with a sum over k of modulo square of g k and this different function evaluated omega k.
This is what exactly comes out of the calculation, because as said in principle, our expression for the environment is not over a continuum of modes,
but over the numerable set.
Then it's kind of a way to take the, if you want the limit of infinite modes, you take this whole expression as a whole.
It contains information both on the frequencies on the side of the environment and on how strongly each frequency is coupled to the environment.
And it contains information both on the environment and on the couple.
So it characterizes not only the environment, but also the dynamics of rule.
So this is the object, which is usually modeled.
Since in order to know it in detail, you should really assume that you have a finite or the numerable set of modes.
For each of one, you are able to measure the coupling strength, which is really in general not feasible,
but it can be taken as a general mode in which you have a lot of bosonic modes coupling with your system,
so many that you only give an overall effective description, and characterizing the properties of this function gives you information about this couple.
Typically, what you would add is the fact that if omega is very large, this function has to go to zero,
meaning that your system is coupled to the environment only after a certain frequency.
So it responds, it is affected only by frequencies up to a certain finite frequency, which is a kind of realistic situation.
There is no coupling at any arbitrary end, only to a certain end, and the coupling gets down if energy goes up.
The further information that you get is given a certain frequency, how strong is the coupling?
And this is reflected in the amplitude of this function at the given frequency.
So it's a way to effectively convey information about the system and the environment when they are coupled together.
So it's an effective way.
And the name spectral density comes from the literature. It's not related to the mathematical concept.
Now, we have the map.
In principle, we already know that this is a completely positive map, because if you believe this was the exact calculation, this is so.
But maybe in this case it is convenient to also have a look at the tri-metrics.
So let me see it in a specific example.
So if we now evaluate the tri-metrics, what we have to do?
As I said, it was a n-square times n-square-metrics.
So this time it's a 4 by 4 matrix. Yes, please.
Typically, the behavior of low frequencies is very relevant.
And there are classes of spectral density, which are actually characterized essentially by a choice of cutoff.
And she's usually not so important.
I mean, whether the cutoff is exponential or the power-low doesn't make a big difference.
But the behavior at low frequency, which is typically taken to be a power-low, it's quite important.
And depending on this behavior, so the behavior of spectral and low frequency, there are different behaviors for this system.
So let's take a look at it.
Yes, in order to have converges of this integral function.
So this is somehow, these are, if you want, constraints that you put on the reliable J of omega.
So integrating up to infinity is usually solved by taking the suitable cutoff in this function.
And the behavior for small omega is checked also depending on the time.
This is a phenomenological modeling essentially.
So what we obtain from the exact representation is that if we put all information in this expression,
this is where somehow we have the freedom to play in order to describe different environments,
which all share the fact that they do affect our system simply by a modification of the coherence.
In a sense, it's a specific order which allows to consider a phenomenological function.
Now, as I said, the geometry is, in this case, 4 by 4.
The first block would be our map phi applied to the element 1, 1.
So this is how a state which is characterized by value 1 in the upper state and 0 everywhere else is transported.
This is the first block, then you would have another block when you apply phi to 1, 0.
And the same for the other two.
Now, if you apply it to 1, 1, what happens?
It remains as it is.
So it's just 1, 0, 0, 1.
It is not transformed.
The same goes for the other block here.
If you apply it to the metric which is only one at the bottom right corner, it remains equal to itself.
The only effect is in the coherences.
So in this op diagonal, you have the transformation of the op diagonal term, which is multiplied by itself by this factor e to the minus gamma.
So you would end with 0, 0, e to the minus gamma.
So this is an explicit representation of a tri-metrics.
So you have to diagonalize it.
You obtain the eigenvalues and you can end up right down your transformation in Krause form.
In the following setting, this Krause form is quite simple.
You essentially have two Krause operators, sigma z and the identity.
So you have the first contribution with the Krause operator, which is sigma z.
Where the time dependence is all in this three factor, which are essentially the eigenvalues.
This is always a quantity which is always positive because this one is less than one.
The other contribution is a three factor, which is one plus the exponential of this function times the Krause operator, which is the identity.
If you have a look, what you realize is that the diagonal matrix elements are not affected by the others.
So as a result, what you see is that in this simple case, you end up with two Krause operators, which essentially describe the phasing.
They are obtained by diagonalizing these all.
And these two coefficients, if you want, can be reabsorbed in the Krause operators.
The point is that they are positive.
So you can take square root here and square root here and the same.
So the example is very simple, but is instructed because everything is under control.
More than this, even the exact evolution mapping, where you check the complete positivity in Krause representation,
you can also look for the time, the variability of this object and see whether you can find out the expression of the evolution equation.
And now we have only performed an exact evaluation of the operator at time t.
Let's look at the evolution equation, because in the general case, you will not be given the exact mapping, but maybe you can have or look at the evolution equation.
Now, if you do that, this time by simple calculation, but still by calculations, we find out that the time evolution of your system operator looks like this.
So we know that this can be either in the integral kernel form or the time local form.
For this model, it turns out that this can be written in time local form.
Here you have a contribution, which is simply the free Hamiltonian.
And the other contribution depends on this function, gamma of t, which is determined.
And this determines the derivative with respect to time, which is determined at the spectral density, which contains the information about the environment,
both its temperature, the available frequencies, the coupling to the system.
And as the special form, you have this operator sigma z, rho s sigma z.
Let me put an adjoint, even if it is meaningless, but just to point out the structure.
Of course, this sigma z is self-adjoint, but this points to a more general structure.
Minus one half, I write another nonsense, but simply to put into evidence a structure.
This parenthesis means the anticommutator, so it's operator times rho s plus rho s times operator.
And this is, in this case, simply the identity.
But what I wanted to put into evidence, we have an operator, the adjoint to the right, and this structure.
And this comes out exactly.
Now, I've not said that these quantities are necessarily positive.
This is maybe not necessarily the case.
And just put it into evidence.
However, we know that whatever the expression of these quantities provide, they are evaluated as such.
And the solutions of this equation for any arbitrary initial state are well-defined transformations,
so sending positive operators, preserving trace, and moreover, a collection of completely positive transformations.
Now, this, in a sense, is maybe enough to point to a simple example, which you can work out by yourself,
which have all the ingredients, the structure of the mapping, the associated geometrics,
the related crowds representation, and you can also find out the evolution equation,
and this special operator expression up to you.
Now, as a second part in this introduction to open contour system,
I would like to focus on possible sensible evolution equations,
because in general, you will not be able to perform an exact calculation as I performed here.
Now, this is the exception rather than the rule, of course, even though it can be inscribed.
Sigma z, yes, yes, for sure.
This is the free contribution, yes, thank you.
Okay, so now I move to a general possible treatment of evolution equations.
So, this part I will call the Limb-Bled theory.
And, yes.
What you mean for any strength?
The constraint is only that the integral of the spectral density is convergent, so it makes sense.
No, this is not correct.
So, what I've said is that I take an initial state, which is a factorized form,
or s times 0 times the state, which is a thermal state, so I've used a specific class of states.
I'm not saying that for an arbitrary form of the environmental state, I'm able to calculate.
But given that, if you were able to give the expression, in this case you formally are,
to give the expression of the state of the environment with elapsing time,
and this in general will evolve itself.
As a matter of fact, the state at the initial time is factorized,
but with elapsing time, the state gets correlated.
And, of course, there are cases in which the state of the environment really does not change in time,
but that's another point.
They get generally correlated, this is in the first instance,
and in the second instance, even if you trace out over the system figures of freedom,
suppose you can do that, you have a state of the environment which changes itself.
In a sense, the situation is in principle such that you can exchange system and environment.
It's labeled that we give to the system.
The point is the labels are motivated because we have a two-level against the whole set of bosonic modes.
But depending on what you're interested in, I will, if I may, because it looks like I'm not going so fast,
but if I may, I will come to a situation which you are interested in the dynamics of the field,
which is an eight-dimensional interface,
and you affect it by letting this mode of the electromagnetic field interact
with atoms going through a cavity in which the whole system is stored.
So, really, system is the use of freedom you are considering and interested in.
Typically, the idea is that system is simple and environment is complicated,
but it's not up to you in the sense.
So, Limba theory and generalized mastery question.
So, here, Limba theory is most textbook and generalized is a bit more, not up to date,
but it's more recent than it is if I may.
So, we said, if the initial state is factorized, we do where the quantum dynamical method exists.
But how does it look like?
Apart from very special cases, you don't know what the structure is.
So, you may look for results which give characterization of classes of these.
In particular, consider a situation in which you take your initial state,
you go to a state at a later time, s, by applying your ad, pi of s,
and then you go still forward in time for another time, let's call it t,
and you end up with rho s at time s plus t.
Because overall, your path was s plus t.
And suppose that this is the same as making it in one shot, so applying them at pi t plus s.
What you're saying in a sense that in order to proceed onwards in time,
you only need to know the state at the given time.
Now, this picture corresponds to a so-called semi-group composition law.
That is, the time evolution up to time t plus s can be equally well obtained
by taking the evolution in two, and if you can split it in two, you can split it in even more pieces, obviously.
Over times t and s, the only constraint that t and s have to be positive.
So, onwards in time, here we are introducing irreversibility.
So, for this class of objects, which is not a generic class,
but a special composition law, it is a general mathematical results
which tells you that it can be written as the exponential of t times a certain operator,
which I call l, it's often called mubillion.
If you want, this is a super operator.
Usually, instead of the word super operator, I have used the word map.
Map sends operators to operators.
These objects are often called super operators.
Operators send vectors in the space of vectors.
Super operators send operators to operators.
I usually use the word map, but another equivalent terminology is super operator.
So, this is a general result, but the point is, do we know how l looks like?
Otherwise, it's kind of a rewrite in our view.
Now, for a physicist, you really want to look at these objects,
whether you can combine these objects with some physical intuition.
Now, on top of this, so on top of the semi-group composition law,
you further ask that each of these maps is CPT.
It is to say, completely positive entries per serving,
and this is not a difficult part, but it's still relevant.
The idea is that it's written in this expression.
For t equals zero, you recover the idea.
So, t equals zero, you are not changing the state and proceeding forward in time.
Now, in this setting, you have a beautiful and well-known result,
which is also a theorem and becomes a theorem when you introduce suitable hypotheses.
I will be more sloppy, of course, but the basics are there.
So, yes?
How it appears?
You mean how to go from this line to this line?
Okay. On the one hand, it's a theorem.
On the other hand, think about numbers,
functions which combine in this way are given by explanations.
So, this, in a sense, is enough to get the idea.
This is the answer.
I'm not imposing the semi-group condition, actually,
but this is a nice point.
I say, if I have a semi-group composition, then I will go on.
Of course, one might ask the question,
when do I have a semi-group composition law?
This is a simple question.
Sorry?
Yes, sure.
This is the motivation for my lecture's attention.
So, coming to a normal conversation.
But nevertheless, this is a very interesting situation
because we can characterize it.
And moreover, when does it apply?
A typical situation in which it applies
is the situation in which there is a clear separation of timescales
between system and environment,
which means that, in some sense, the environment
relaxes very quickly with respect to the typical timescale of the system.
So, that, in a sense, the system is always interacting
with the new environment you want,
the environment which is in this state and just remains in peace.
So, typically, in this situation, such an approximation holds.
Of course, to hold it exactly, one should check it in principle,
which is a very difficult point.
But it looks like, in many relevant situations,
this is a sensible approach.
And this can be seen by the fact that we will come now
to a characterization of these operators in this setting,
and they describe these questions.
But, of course, this is not the most general situation.
And when I will come, if I ever come,
to characterize the different quantum processes,
as I said in the morning, the idea is given a quantum process,
see how it might look like,
see what are the related illusion equations,
distinguish about processes which have or do not have
or imply memory effects.
And these memory effects are for sure related to this property.
In fact, what I said here, I said,
in order to go onwards in time,
you only need to know the state at a given time.
This was preparing the ground for a violation of this condition
before preparing the ground for the idea of matter.
But, of course, I have to introduce the memoryless case.
So, in this connection, there is a beautiful result,
which was obtained in 1976.
So, it's by now exactly 40 years old.
This year, there was a conference celebrating
this 43rd day of this theorem,
which is known also called as GKSL theorem.
It stands for Gorini, Kosakarsky and Sudarshan,
which in a paper which appeared in the same year
as it was available,
it essentially characterized this generator
under different quantum processes.
This is a most well-known result,
but it essentially says that...
Yes?
Sorry?
These objects are called the quantum dynamical semi-group,
and the theorem will characterize this area.
No? It's not reversible.
Wait a second.
You think you put a minus here,
but if you put a minus there,
in general, you will not have a transformation
which is completely positive 37.
So, this only happens if you have a unique evolution.
So, it's not reversible.
So, when you speak about an inverse,
if you want to speak about an inverse in this setting,
you mean not only a linear transformation,
but a linear transformation which stands states to states.
And in this sense, it is not reversible.
So, this argument holds in the classical setting.
So, you can speak about semi-groups in the classical setting.
And indeed, this theorem is a kind of quantum counterpart
of a characterization that you have in the classical set.
So, let me state.
Let's say that the collection of pi of t,
or positive t,
is a quantum dynamical semi-group.
That is to say, it's characterized by this collection of statements.
So, composition as a semi-group,
CPT at each time,
at time equals zero's identity.
If, and all if,
this operator L, which is typically called generator,
can be written in this form.
L applied to rho is equal to the commutator
with an operator which is self-adjoint.
Last contribution, which is the following form,
is the sum of the finite or the denumerable,
depending on the dimension of the space of the system,
set of operators with certain numbers,
which can be interpreted as rates,
because they are positive,
and certain operators,
let me call them L sub k,
L sub k, rho L sub k dagger,
minus one half L sub k dagger,
L sub k,
anti-commutator with rho.
As you see, this is exactly of the form that I obtained before,
apart from the fact that here,
we are in particular saying that these coefficients have to be positive.
These operators are often called Liemblad operators,
but of course, this is a matter of taste.
And they are the natural point in which you insert some idea
of what the interaction between system and environment looks like.
They can typically be, I don't know,
raising or lowering the operator with respect to the
ground or excited state of your atom,
which are describing as system and atom,
or they can describe collisions,
if the dynamic is identified as collisions,
and this rate can be exactly the rate
with which photons are emitted or absorbed,
or collisions can take place,
or a certain kind of collision.
So this is not only a mathematical result,
if you want, it is.
It is also the place where you can put in a lot of physical information.
This is perhaps the fact which has made these results so important.
And it allows to describe a huge amount of interest in dynamics.
The statement is that the solution of a question of this kind
is a semi-group of completely positive 3% maps,
and if you have a semi-group of really 3% maps,
then the generator has this form.
The generator, L of rho, is actually what we usually introduce
to write down a so-called master equation.
So what happens is that the derivative of this theta time t
is equal to L applied to this theta time t.
This is more standard terms.
The derivative of rho s of t is equal to L applied to rho s of t.
And this is what we typically call a master equation,
and the name comes from a classical section.
So the equation in which you find the balance
between the contribution of different terms.
It can be seen as a gain term, and this is a lost term.
Please notice, and I come back to the question by Professor Singh
that even if rho is fewer, this state is typically mixed.
So this contribution leads you,
it's a typically inquiry and contribution
which drives fewer states to mixtures.
So in this sense, even if you take as a particular situation
the initial state which is fewer,
you are led to introduce statistical evidence.
And more, yes please.
I wouldn't say so, but on the other hand,
first of all we still have to define memory
because it's a difficult topic.
And also I'm not aware of general characterization
of nonlinear evolution equations
that we still need to define.
It's difficult to answer, there is little work on the subjects.
On the one hand, of course, the linearity of quantum mechanics
plays an important role.
And if you start in this setting,
the evolution equations are naturally linear.
So there is no restriction in this aspect.
We are in the same kind of motivation.
The linearity of the equations
starts from a microscopic dynamic.
So in the space there you have linearity
and use linearity for more.
Of course you might have no linearity
if you have a complex system
and you have introduced some, I don't know,
kind of wind field description
which all depends on the state,
but then that is kind of a, I mean,
an approximate description which
does not come from an exact microscope.
So we have introduced these basic results
and now what I would like to show
is that, okay, this is a theorem
that we cannot leave.
I will say something in the one direction
in order to point to a generalization of the result.
Okay, so something in the one direction means
suppose we have the solution
of an equation of this form,
let us show that it is a completely positive time evolution
and use this as a starting point
to the generalized master equation
or generalized evolution equations.
So...
The idea goes as follows.
We write this generator as a sum of two contributions,
one which we call relaxing
and the other that we call jam.
What we take as a relaxing contribution,
the idea is that the relaxing part applied through
is the combination of the unitary term
if you want unitary in the sense that
if you only had this contribution
you would end up with a unitary term evolution
and the other term which is the anticommutator.
So the anticommutator of sum of a K LK dagger
a K anticommutator.
This expression can also be written as
minus I an effective Hamiltonian applied through
minus rho times the adjoint of this effective Hamilton.
It is to say the last term
does not disturb a purity.
It decreases the trace in general
but does not disturb purity.
It is the same effect that you would have
if you should introduce this called optical potential.
So a kind of a noce of a joint Hamilton
which is an effective way, for example,
to describe the absorption of the system.
So this is what I call the relaxing part
and the idea is that the evolution
due to this contribution,
so the evolution of our finite time
this is the potential equation
is E times T L relaxing applied to rho.
So this would be the same group of contractions.
It does not destroy purity
but it decreases the norm.
The other contribution is LJ
is a jump contribution.
This is the incoherent part.
The term which I've written as LJ applied to rho
is equal sum over K LK rho LK dagger.
Please notice that actually
this object determines the whole Limblest structure
because this term can be related to this other one
as the joint action of the observables.
This I give also as a...
I'm not going into this detail
but just to those who are perhaps already aware
of the Limblest structure just point to the fact
that in the essence this map
which is by construction CP as you can see
but it's generally not trace per setting,
so it's CP, CP.
This is essentially the error of all the expression.
So we have these two parts,
the contraction term and the jump part.
Now the statement is that one can check
that the time evolution map can be written as follows.
There's a sum of contributions
in which you have first only a relaxing part
then the convolution between a relaxing part
and jump part.
Now I write it in more detail.
So you take the convolution of these two objects
and I'm going to write it in more detail
but let me write it in combat right now
and you go on inserting each time
and add a convolution.
So this is formally an exact solution
and you can check it by derivation
which is also in a...
if you want in a derivation expansion
let me write it more explicitly here.
So the state at time t can be written as follows.
You take the map,
slide to the state at the initial time,
which means the first contribution is t
times t l r, like the initial state,
so this contraction,
plus this infinite amount of contributions
is distinguished by the number of insertions
of this map lj, this jump part.
So it's a sum from 1 to infinity,
the integral between 0, t and dt n,
the integral in dt 1 between 0 and t 2,
of what happens?
So essentially this integral
translates this convolution.
Now the integral I'm convoluting is obvious.
So you have r at time t minus tn,
then I have the action of lj,
r times tn minus tn minus 1, and so on,
and so forth, tlj at t1 applied to os 0.
So what I have done is writing down
the formal exact solution of the master equation,
so given that the master equation is in limblat form,
so that I can split it into these two pieces.
R of t is the solution for a one-piece only,
and the exact solution is obtained by inserting
at intermediate times this,
which I call jump operators,
and lj takes into account the action of this limblat operators,
which are meant to describe the microscopic interaction events
which drive the dynamics,
so I don't know, exchange of a photon or collision or whatever.
This expression is indeed a solution of the master equation.
If you take the time derivative and you have not patience,
you find that this time derivative is just l applied to rho at time t.
And it provides you also, in principle, with a kind of interpretation.
If you want so, the dynamics is a sum of contributions,
described by the fact that you have first a relaxing evolution,
then a jump, a relaxing evolution, then a jump,
and the overall dynamics is the sum of these evolutions,
which would be known if you could say,
okay, at time t1 I had the first jump,
at time t2 I had the second jump, and so on and so forth,
and some overall possible number of jumps,
from 1 to infinity,
and overall possible intermediate times,
for integrating these cases.
Now, the idea is that, given this expression of the solution,
it's easy to generalize the limblat result
with a so-called independent case.
So up to now, we had a proper composition law of a semi-group,
and all that mattered was the elapsed time.
So the map actually did depend on a single time.
Now, supposed to relax a bit the condition we had in the theorem,
and we sent our limblat operators
to possibly a collection of time-dependent operators.
So lk becomes t-dependent.
Gamma k also had allowed to become time-dependent,
and it is not so strange.
We've seen in the example we have considered in the pure-the-phase model
that indeed these coefficients appearing in the structure of the equation
was the exact one, it was exactly in the limblat form,
however, it allowed for time-dependent coefficients.
So it's, in a sense, natural to allow this finalization,
but still keep them to be at any time possible.
Now, if you do this,
you have also to modify your r of t,
which now goes over to an object which depends to an initial and a finite time.
The point is that, in a sense, we are introducing time-dependent rates.
So instead of a situation which is homogeneous in time,
it's a situation which is no more homogeneous in time.
So at what time you are considering quantity, it does matter.
So this object is replaced by a two-time object,
and instead of simply the exponential of our l of r,
we have the time-ordered exponential in zero and t in the tau of l of tau,
sub-r of tau, because now our limblat operator is generally dependent on time,
and the same goes for the rates, so we are finished.
I notice now that, please forgive me,
I somehow introduced the rates in the definition of the operators along the way.
It's not an important point, but just to be precise.
So with this replacement, we end up with a map
which is still of exponential form,
and it can be formally written as the integral between zero and t of the tau,
our time-dependent limblat structure.
So everything as before, only l depends on time,
so instead of multiplying but the elapsed time, I integrate it over time,
and since they are operators, I also have to take care of ordering in time.
Now, how does this object look like?
Now, the statement is that this object allows for the very same expansion, if you want,
and here what I do is simply replace the object depending on one time
by the object depending on two times,
so this relaxing path does not only depend on the elapsed time between t sub n and t,
but on the initial and the final time and in between the decaying rates, for example, do change.
On the same footing, this lj will now generally become time-dependent operators
because they keep the same definition, but here we insert time-dependence.
So as you see, in such a way, you can almost with the same effort,
apart from operator ordering, introduce a structure of generator
which leads to a time evolution, but is it still completely positive?
Yes, why? Well, simply for the fact that where do we read complete positivity here from the solution?
So I said this is the solution of the equation, but one should have already asked,
but yes, but is it CP? Yes, it is completely positive.
Why? Because it is sum or composition of completely positive maps.
So from the structure of the solution, we immediately see that it is completely positive,
but this remains true if we replace this object with time-dependent objects.
So by the very form of the solution, we can conclude that indeed this is a well-defined evolution.
So seen in these ways, it's kind of a simple generalization.
What do these two approaches have in common?
A feature that I would like to stress.
Yes, please.
The commutator vanishes this contribution.
No, because you need both terms for trace preservation, you need gain and loss.
So what can happen is that this contribution can be the identity,
and the anti-commutator boils down to minus rho.
This is a situation in which, for example, when the moon-level operators are unique.
But otherwise, if one is zero, the other has to be zero for trace preservation.
So they come together.
Yes?
One could make the statement, in a sense.
If you want, you would call this a dissipative term, and it's the noise term,
in a sense the whole Limblat theory, the open quantum system theory,
came about because of looking at a sensible distinction at the quantum level of dissipative effects.
Because one had a problem with preserving Heisenberg uncertainty in relation,
if you want to straightforwardly use a classical approach.
So one should be more careful and go through a more...
The origin of all these, in the sense...
If you have a quantum system, introducing dissipation is not so obvious,
because there are constraints on the noise in order to essentially process the Heisenberg uncertainty.
So this comes about from a proper quantum treatment, bringing this with itself.
But if you go phenomenologically, then it can be dangerous.
And in a sense, these are results that, of course, many others, which tell you,
okay, which are the sensible path, even if you go from a phenomenological approach,
but still these have all relevant functions.
I don't know whether this answer...
Gamma K is positive.
Here, because when I want to say that Lj of T is still a CP net,
as I stress, this is the key point.
If I want Lj of T to be, again, CP, you might say, okay,
the fact that L sub K is independent is not so important, because we have the adjoint here.
I can agree.
But if these coefficients are not positive, this is not the CP net.
Because we learned from the Krause theorem that the net is CP if and only if it can be written in Krause form,
which means that these objects have to be positive.
And then you can forget about them, in a sense, putting them into the square root of this object is partly here and partly there.
So that's what I use.
And they have all to be positive.
You are asking now whether they were always positive in that example.
No, they were not.
As I said, depending on the spectral density, they are or they are not.
Which is in mind.
Conclusions will follow.
Now, the point which is related actually to this question, as I said before,
the semi-group means I go from time zero to S,
applying them at once, and then I start from here.
I don't need any other information.
I come to the state at the final time.
Or I do it in one shot and nothing changes.
This is the semi-group composition.
Now, what I'm saying with this enlargement is that if I go from here to here,
I hope you can see, I introduce a map phi from time zero to time S.
And in order to proceed onwards, I need a map which starts from S and goes to time T.
Or equally well, I can start from time zero and consider the map, sorry, going from zero to time S plus T.
So what do they have in common?
Essentially, they have in common this fact.
Let me consider S and T as initial and ending time.
I can split this evolution into pieces and if to, of course, many others.
On the same footing, I can split the semi-group evolution into pieces.
Here I have the constraint that tau is in between S and T,
and here I have the constraint simply that the times are positive, sorry.
And what do they have in common?
They have in common a property of divisibility.
So I can always split the time evolution into pieces, as many pieces as I want.
And each piece, so each of these operators is CPM.
But it tries preserving, but the focus now is on...
So these two evolution, homogeneous in time if you want, semi-group or time non-homogeneous,
generalized one, share the fact of being divisible and divisible in terms of a CPM.
And this is one of the notion that will be used, that we use in order to deduce the notion of memory in quantum mechanics,
so in the dynamics of these arguments.
I now want to provide you an idea of generalizations so deeper,
or at least wider generalization of this Limblat result.
Here I've pointed to the fact that you can allow for time-dependent coefficients,
provided they are always positive.
And now I will also try to give some conditions which you might introduce,
even a general local in time or memory kernel mastery question,
in order to guarantee that your time evolution is equal to 5.
This situation, everything is under control.
The theorem is an if and only if.
But what about considering other situations, as was asked before,
in which we don't have this composition law?
So the problem is still all.
So let me start.
So first question is time-local master equations.
Yeah, I will be, in a sense, brief.
I've given the general idea and I'll point to conditions which warrant complete positivity.
These same conditions which warrant that these phenomenological,
possibly phenomenological, evolution equation are indeed well suited to describe quantum dynamics.
As we learned from the projection operator technique,
it is generally the case that you can write down your evolution equation in a form which is local in time.
Now, building on the result by Limblat is generalization,
and on the fact that we have different constraints that we have to keep in account.
We have to preserve emeticity in the operator, trace,
and then positivity, and in particular complete positivity.
Now, the difficult part is complete positivity, as you might have understood by now,
but emeticity and trace essentially fix the general structure of a time-in-local equation.
And the structure is amounts essentially to the Limblat one, apart from corrections,
which means that time-in-local, I will call PCL,
to remind from the fact that these kind of equations might be obtained,
exploited in the general techniques of projection operators local in time,
as a contribution, which is of a Newtonian form.
This would be also therefore an isolated system.
And another contribution, which indeed, if you want, is in Limblat form,
but in order to say Limblat form, you should say something special about the coefficient.
So it's a kind of a generalized Limblat form, which will allow for time dependence in the operators.
You still have the action to the left and of the joint to the right,
and the anticomutator tab is essentially determined by trace preservation.
So emeticity and trace preservation has the structure as to be of this form.
And that's also the reason why they appear always together.
So you take here k dagger of t and k of t, anticomutator with rho s.
So it can be shown that it's easy to show that actually an equation which preserves emeticity and trace has to be like this.
And now the question is, okay, but if we also want this to be a well-defined dynamics.
So in the first instance, this should generate time evolution of time 0 to t,
which has to be completely positive trace percent.
Are we able to give conditions in order to obtain this?
Otherwise, I stated, do we have the most general conditions on the time dependence of these objects
in order to warrant this property?
So the answer is no.
There are only partial answers.
One has already been written there.
It will come clear when I give the conditions for these objects.
In specifying the equation, the operator structure is for sure relevant, is most relevant,
but the operator structure, so this L, a joint, anticomutator, is essentially fixed by trace preservation and emeticity.
No, in general, no.
And now I've come to the condition for this coefficient which should help in clarifying this point.
So given this structure, what about, if you want, I can put it like this,
what about p or cp of this collection of maps, which is in my definition a quantum process?
Now, one statement is already known.
If we say that the coefficients are positive at any time, then we have two things.
This collection is cp, not only, it is also cp divisible, which is something more.
So we also have a composition law.
This dynamic is well defined and we know the composition law.
This is cp plus cp divisible.
Then another result is known.
If the sum over k of this gamma k of t times the modulus of Un,
Un is a possible complete orthonormal system in the space of the system.
So in here, I take the Limblad operator, these matrix elements with respect to an arbitrary orthonormal system in the space of our system.
The non-diagonal terms, so I ask n different from m.
I take the modulus square of this quantity, sum them with these coefficients,
and I ask this quantity to be positive.
It's maybe the meaning is not necessarily obvious, but this is a condition which actually came about as a result of work done by Kosakowski actually in the early 70s.
At this time, there was the idea to move from the classical to the quantum setting in describe dissipative evolution.
For example, consider semi-group evolution, in analogy with classical semi-group evolution.
So coming from classical semi-group evolution, one considered a collection of mappings which would be positive.
In the first days, it appeared that positivity was enough.
Then it was realized, thanks to Proust, that actually complete positivity was the key point, but only later.
The first result by Professor Kosakowski were about the characterization of semi-groups built up by single-map, which were only positive.
And he came up with this condition, which is obtained by analogy with conditional classical Markov processes, which is a constraint.
If you want, this can be seen as the transition rates and the matrix element of the transition operator, if you think about your interaction as a transition among levels.
And this condition is such that the map is positive, so the time evolution is positive, and in particular, it is still divisible.
Divisible because this came about looking for semi-groups, the quantum analog of semi-groups in the quantum setting, but only asking for positivity.
So the visibility amounts to the fact that you compose forward in time, but each piece that you compose is now simply paused.
So this is essentially what is known up to now.
So in particular, if these coefficients become negative at a certain point in time, we don't know about complete positivity at the time evolution, apart from special cases.
What are the special cases? For example, the one I've considered before, because that was an exact evolution.
So one can point to choices of spectral densities leading to coefficients, which at certain times do become negative, but such that the overall dynamics is verified.
So in this sense, these apparently simple part of the structure, classical in a sense, because there is, contains in itself information which is still not completely extracted about the possibilities of the well-defined dynamics.
Now I go for something about well-defined evolution with a memory kernel.
So in the time-local case, we know that evolution has to look at this, the evolution equation, because of emitticity and trace preservation.
With memory kernel, the subtleties are more delicate.
There we know very little.
But there are a few developments which I want to tell you about, because they might help the intuition.
So the idea is memory kernel, master equation.
So what we know is that, of course, there should be something like this.
If you speak about the memory kernel, the derivative of the statistical operator is equal to the integral over a certain kernel that you might, for simplicity, take to depend on the difference between the time arguments applied to the state at a time down.
In principle, you are following the state a long time in order to have a solution.
It's for sure natural, given this convolutional structure, to look at the Laplace transform quantities.
This is natural because then this convolution goes over to be there, and it really turns out to be a natural strategy.
So with this object, it's also natural to keep in mind this one.
So this, by the head, I denote the Laplace transform, and I take u as the variable in the Laplace quantities.
So the derivative amounts to u times the Laplace transform of u.
And here, to the right side, we have the Laplace transform of the kernel times the Laplace transform state.
So it looks formally simple, and here you can also formally solve for rho s, depending on u as a function of the kernel or the initial situation.
Of course, a formal solution. You need to know how does k has to look like in order to provide you a sensible evolution.
Now, how to do that? In order to do that, I step back to the classical situation.
I have pointed to the inlet result, which is something like the derivative of rho as a gain term and the loss term.
But as said, this result was essentially obtained starting from studies building on the classical analogy.
And what is the classical master equation of what we call the Pauli master equation or rate equation?
It's an equation for the, these are the components of the probability vector.
So in the classical setting, instead of a statistical operator, you have a probability vector, so a collection of positive values summing up to 1.
And the typical rate equation would look like you have a gain term in which you start from the side j and you have certain transition rates from side j to side i.
This is the gain term and the loss term would be the situation in which you are in state i, but you go over to a certain state j.
And here I'm summing over j, of course.
Now, this is often called the Pauli equation.
And in the essence, if you want the inlet result, one way of looking at it is looking at it as a kind of quantum counterpart of this Boltzmann-like or gain loss-like master equation.
The point is that here you are only taking care of populations, so they would be the diagonal matrix element in a given basis.
Here you only have to take into account coherences.
This makes all the difference.
Now, given this viewpoint, one could ask, do we have some other relevant input in the classical setting for processes which can be considered to be non-Marcovian to show up memory effect?
Because in the classical setting, in a sense, we have an idea of what Marcovian means, at least of the level of classical.
So the idea is, step back also for this situation to a non-Marcovian classical situation and try to build on that in order to come to well-defined and, in a sense, physically well-posed or connected to a physical interpretation evolution equation.
So what we have here, the idea is that we have certain classical states, one, two, three, four, and we can jump in between them according to certain rates.
Now, one can slightly generalize the situation, introducing information to a stochastic matrix which gives the probability to jump from one side to the other.
For example, PIJ is typically non-stochastic matrix and the matrix element of this object gives you the probability to jump, say, from three to two, from three and so on.
So this gives the probability of jumps.
And this is one piece of information, which is already there in this Marcovian description.
And then there is another piece of information, in principle, tell me when I'm going to jump.
And this could be, in the Marcovian case, would be described by a simple rate, meaning that in order to know when you're going to jump next, it is not important to know how much time has elapsed since the last jump.
This is a way to describe the Marcovian situation, and in this sense, the other piece of info that in general you should provide, which is, in a sense, fixed in a very specific way for the Marcovian setting,
but in a more general setting as to be provided as, tell me when I'm going to jump.
And there might be a situation where the probability to jump at a given time depends on the elapsed time.
This is typically so with buses, for example.
So one point is the probability of jumps, and the other is the time in between jumps.
This quantity is characterized by transition probabilities.
This quantity is characterized by a so-called waiting time distribution, which for the Marcovian case has to be of the following form, so exponential form.
This is the only situation in which the time, which has already elapsed, does not provide you any information about when the next jump is going to take place.
So the idea is, okay, but let us consider other situations, different from this one.
So we still give a certain set of probability to jump from here to there.
So once I am in three, I have set the probability to jump to four, to one, or whatever, but I also need to provide information when is the next jump going to occur.
So I replace this object with a general setting, and actually these objects always come together with this companion, which is the so-called survivor probability.
Which is the probability not to have jumped up to time t.
So one minus the integral of the waiting time distribution, survivor in the sense this is the probability not to have jumped up to time t.
So it is one, time t equals zero, because we start from there, and then it basically goes to zero.
So these two objects, in a sense, come together.
And indeed, the idea is that we can exploit these objects together with these stochastic metrics in order to obtain a class of classical evolution equations, which feature memory, in the sense of they give rise to a process which in the classical setting is normal.
Then from there, we will move to the quantum setting.
In order to move there, we will replace functions by operators.
And when we replace function by operators, there is something special which happens, operators generally do not commute.
So if you have a function which is a product of different functions, and each of them is replaced by an operator, you have to pay attention which ordering you are going to take.
And what we will see is that depending on the ordering, you will adapt with different kind of evolution equations.
So there is kind of wider freedom in the quantum setting with respect to the classical one.
So let me close coming to the classical evolution equation, which is the say, no Markovian counterpart of this equation, and tomorrow I will go on to the quantum set.
So just to complete the evolution equation I've written there.
It's now generalized to the following one.
Besides the sum, I introduce an integral over time because these rates will generally be time dependent.
So I will add a sum over j, the integral between 0 and t in tau.
My transition rates now are generally time dependent.
So instead of w ij, I will write w ij depending on t minus tau times j times tau minus, this is the game part, if you want, the lost part will be the reverse part.
So w ji t minus tau pi of tau.
Now, the crucial fact is that there is some information in the classical setting how these objects will look like.
And these objects can be, in a sense, easily understood if you re-express them in terms of pi, this f, and this g.
And then this will be moved on to the point.
Since I have only one minute, meaning if I would need a lot of time, it's better to waste these seconds rather than start in tau.
So thank you for your attention.
Sorry.
The reference given here covers today.
And the end of today and beginning of tomorrow will lead other references, and these I will provide tomorrow.
