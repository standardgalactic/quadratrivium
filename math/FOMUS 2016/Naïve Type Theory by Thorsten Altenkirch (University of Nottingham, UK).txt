Thank you for organising this thing and also for inviting me.
I wanted to present some ideas behind type theory.
And I called this naive type theory a bit.
The title is a bit inspired by Halmau's book on naive set theory.
So I think it's quite important to understand the intuitive ideas behind type theory and
not so much to focus on formal representation or on a particular system with lots of Greek
letters.
Yes, okay, we want to implement type theory, then we have to be very precise, but for humans
it's often the ideas are more important.
Yes, as a general remark, by the way, I'm quite happy to have questions in between and not
just at the end, obviously if it gets into philosophical discussions then we should postpone
it to the end.
But if there's a question or a short remark, quite happy to have this during my talk.
Okay, so this cartoon was a colleague of mine picked it up somewhere on the internet with
the already modified line and put it on my door, I don't know why, maybe it's a general
warning.
It depends on what guns you have ready.
Okay, so what about, let's talk about set theory, so in a naive sense.
So here are just a few lines, okay, I have the set of natural numbers which I usually
write like this, and then I can express some facts like 3 is an element of the set, 3 element
of n, and I can say 3 plus 4 equals 7, I hope that's correct, and I can express more general
statements like for all natural numbers, so here obviously I say for all x such that x
is in the set then x plus x equals 2 times x, that's the proposition, and that's sort
of like how we use maybe set theory in everyday life.
So how does this look in type theory?
It's completely different, I have n which is now a type of natural numbers, I say 3
is a natural number and notice here that I use a colon and not the element symbol.
And I write 3 plus 4 triple equals 7, triple equals means it's actually definitionally
the same, it's no proof, 3 plus 4 is 7, this is something you can just calculate, that's
true.
But I can also write something like this, I use a funny symbol, big pi, x colon n, so
I always talk about typed things and that's the x plus x equals 2 times x, so that's type
theory, easy, huh?
What's the difference?
Oh, the slide, what's the difference?
Okay, here is one point, and I'm not sure how to say this best, I wrote some words on
the slides but I'm not sure they really explain this very well.
I would say in set theory the elements come first, you have some elements lying around
and you say, oh, I take them and make a set, okay, so you collect them, you can be in my
set and the cup can be in my set and I put them all together, they're there already.
In type theory, the types come first, I have a type, type of natural numbers and then I
create elements and this element is a natural number, nothing else, it is by birth, it's
a natural number and it stays a natural number all its life, so that's type theory.
So this has got some consequences in the way we present the theory.
In set theory, if I say something like A element A, this is a proposition, like A is
even or x plus 2 equals whatever, it's a proposition, it's something I talk about.
That is because I've just collected my elements and now I talk about them, like this element
is a human and this is a cup, so this is my set, I'm talking about it, whereas A colon
A is a judgment, that I talk about a natural number is not a proposition, I'm talking about
a natural number, that's it, it's not a question, you give me a natural number, is it a natural
number, yeah, that's what it is.
So when we write A colon A, that's a judgment, now it's difficult to explain sometimes to
people not used to this distinction because in predicate logic we have only one judgment
which says this proposition is true.
In type theory we have several judgments and this is static information, so one judgment
is this colon but this triple equality I showed before is another judgment because if I define
pi to be 3.14 then pi is 3.14 and that's not something we can question, you can say if
pi would be defined as this then what is the case, this is not something which you postulate,
it's either the case and it's not something we argue about.
So there was a mark already made, this analogy, if you know a little bit about programming,
it's similar to distinction between statically typed language and dynamically typed languages
in programming, if you have a statically typed language and we know x is an integer then
we know that x is an integer, this is the static piece of information which a compiler
will already check for us, whereas if I am in a dynamically typed language like python
I get an x and now I can say is this x an integer, it's something which is a dynamic
piece of information in a way corresponding to a predicate, to a proposition where a static
idea is a judgment.
So as a consequence this seems to be a sort of, I'm giving some negative information here,
what I'm telling you is something like this is not expressible in type theory, it doesn't
make any sense, here I'm saying if for any x of x is in A then x is in B, this I can
say about this epsilon relation but not about the colon because I cannot argue either x is
in A, that's it, that's just something which is a static piece of information which I'm
not predicating about, I'm not saying if it were that the natural number is a natural
number this doesn't make sense and every object by birth belongs to a particular type, not
to another type, so the question whether a natural number is although a Boolean doesn't
make sense, natural numbers and Booleans are two different things, so that's it, end of
story.
Okay, so there are certain things which you, if you learn set theory it's one of the things
which people usually start with, like subsets, intersections and unions, these are not operations
on types, there is a way to represent them in type theory but not as operations on types
because here as I say already I have to talk about the elements, if I take an intersection
of two types I'm thinking about these elements which I have collected before and then I say
okay, half of you too may be collected the same elements but it doesn't really make sense
about types, the same for the union, these are not operations on types and as Mark already
said there are lots of stupid questions you cannot ask, for example what's the answer
to this, what's the intersection of the union of the natural numbers and the Booleans, what's
the answer, I'm now set theory, if I write union, look, if I write union I'm in set theory,
what's the answer, it depends on the encoding, it depends on how I represent things, it's
an intentional question and it's a really stupid question, I mean basically anybody
asking questions like this, you should throw out of the class, this is a dirty question,
I should ask this. In type theory, one of the advantages of type theory is that you cannot ask
any stupid question, like why are you late, no it's not a stupid question, I'm not, in type theory
we don't ask these stupid questions, so you cannot ask stupid questions, it's more disciplined. Now
there are operations which are fine and here the function type is fine, that's construction
type theory, the Cartesian product is fine and the disjoint union, so plus here, I write plus to
construct the disjoint union which is really a bad word, because a disjoint union you think of,
it's a union but it's made disjoint, we don't have a union here, so really the one operation we have
is the disjoint union which we may call by its categorical name coproduct and indeed all these
constructions, all these extensional constructions which we can do in type theory, they can be
specified precisely by their categorical properties, whereas these cannot, I mean these are intentional,
they're not really mathematical operations in a certain sense. So here just for explanation,
let's have a look at the difference in behavior between an intentional and extensional operation.
If I write a funny squiggle b for isomorphism, that means I have functions in both directions,
whose compositions were inverse to each other or equivalently, we can talk about projections,
whatever you prefer, then certainly AB is isomorphic to CD, because all what matters for final sets
is the numbers of elements, how we call them isn't really such an important property, I mean if
you like A better than C then fine, use this one, that's a choice. But now if we do a union,
so in both cases if we do a union with A, then the result is no longer isomorphic,
because this one here has two elements and this one has three elements. Whereas if we do a disjoint
union, so we always label the elements where they come from, do they come from left or do
they come from right, then each of these has three elements and so hence the isomorphic. So all
these extensional operations, all the operations definable in type theory preserve isomorphism,
they preserve this extensional behavior of types. So I mean the slide maybe just summarizes what
I've just said, so operations like union and intersection or the subset I would call intentional,
what do I mean by intentional? Intentional is to look at objects how they are built,
extensional means to look from outside, to look what we can do with them. And mathematics I think
much more naturally works in an extensional setting. So the behavior of these operations
we've just seen or there depend on the intangible properties of objects, whereas the other three
operations, they are extensional and their behavior, their behavior is independent on the
choice of interpretation, choice of encoding. It's an interesting paradox almost that in what's
called intangible type theory, which is a type theory which was introduced by Pierre-Marty and
Leuph, all operations are actually extensional in the sense they preserve isomorphisms. However,
it's called intentional type theory because you can never take advantage of this. So you may have
two objects and we have seen examples already, so Mark yesterday talked about two functions which
are extensional equivalent. Actually, they are extensional equivalents because we have no
observation to distinguish them, but they're not equal in intentional type theory. Intangible
type theory is a very cruel system because it doesn't give you any way to look into objects,
but it doesn't pay you back by identifying them. So it's fundamentally incomplete. I cannot
distinguish two things, two objects, but they're not equal either. This is what I would say is a
fundamental incompleteness. So in a way, to me, this is the main point about homotopy type theory
which has not so much to do with homotopy theory is that we can go one stop further and have,
as a consequence of this univalence principle, really, we get the payoff. Isomorphic types are
equal and in general, extensional equivalent objects, we can actually show to be equal. So in
this sense, it's a consequence, it's a sensible step from intentional type theory. After having
been disciplined, being very disciplined, not never to look into objects, you want to have
the payoff. Then they should be equal. If I'm not allowed to look into them, then they should be
equal. All I can see is their extensional behavior. Okay, so let's look a bit in how things are made
in type theory. So here I ask the question, what is the function? And if you've studied your set
theory, you know a function is a set of pairs with certain properties. Now, that's not the view
in type theory. That's a particular encoding of something I wouldn't even call a function. What
is a function? So if I write f colon a, a, b, a function to me is like a box and I can put
something in. I couldn't put an a in and on the other side, I get an f a in b out. That's a function.
Very basic idea, I think. Hence, in, in type theory, functions are primitive objects. They're not
reduced to sets of pairs or something. A function is, is a primitive concept. So yes, indeed, type
theory has more primitive operations than set theory, which is the element symbol. So if you
really like, really impressed by the beauty of, of, only have one operation, then maybe type
theory is not so good. But I think it goes too far in set theory. So my understanding of a function
is not, it's not a set of pairs. It's a box. I put something in and I get something out. And, and to
write them, we use the language of lambda calculus with mark, which mark yesterday has already
presented. And here's an important concept for me. My understanding of a function is that if I put
something in, I get something out. Yeah. And I would say that a function which doesn't function
shouldn't be called a function. Yeah. So I mean, there's no function deciding the holding problem.
Yeah. I put the program in and I don't get the Boolean out. So that's not a function. I don't know
why anybody would call this a function. Now, when we look at functions in type theory, we come
across functions like this. So let's fix an a and a. And now, given a natural number, we can
construct first of all a to the n. I mean, the type of n tuples. This is actually a dependent type,
as Mark introduced yesterday. For any natural numbers, I have a to the n, n tuple. And now, let's
consider, let's just repeat a n times. So that's a function. I put in an a, I get out a tuple. But how
can I, how can I describe the type of this function? So it seems that the arrow isn't really
enough. So I have to use this pi type, which really is a dependent function type. It says, if you
feed in an n, you get out a tuple of length n. And here in this kind of functions, the domain, the range
of the code domain depends on the input, on the domain. So I write pi n colon n a n to mean a function,
this function tube. So what I feed in is a natural number. And what I get out is a tuple of exactly
this length. Now, why is this called pi? That's another story. And I'll get to this. There is a
alternative notation, which is maybe in a way nicer. So we sometimes write n not arrow a to the n. So
we use the normal arrow, but we allow to bind something on the left. Tradition in type 3 is to
use pi. And the ordinary function, the non-dependent function type is just a special case. So here I
use this bar to denote an anonymous variable, which I don't want to use. So a over b is pi,
something in a, which I don't use b. So if there is no dependency, this b does not actually depend on
the input. As a special case of dependent functions, you get back your ordinary functions. So we can
play the same game with pairs. And here's an example. What's a list? A list is a tuple of arbitrary
length. That's what we understand, a list in computer sense. They're not usually defined this way, but
that's one way to define them. And then I say, okay, a list is a pair. I choose the size, the
length is a natural number, together with a tuple of exactly this length. So for example, if I have a
pair, it's 3. And then I have just a tuple, a 3-tuple. So that's a list. Or, okay, for 0, I can only use
the empty tuple. So these are all lists. So I can explain lists via pairs of a number and a dependent
type. And here you use sigma for the dependent pairs, for the type of dependent pairs. And again, the normal
product type, the Cartesian product, is just a special case, if the second part doesn't depend on the first
one. Okay. Now, so this idea, hang on, what about this joint union? So this joint union, I can actually
represent using sigma types. And actually, that's where the sigma comes from, if you think about it, sigma is
just a big plus. So now I can use my big plus to construct little plus. And how do I do this? I say a plus b is a
dependent pair. The first component tells me whether I'm left or right. And I just use a Boolean. And if I'm, if
the Boolean is true, the second component is an a. If the Boolean is false, the second component is a b. And here, I thought
this curly bracket looks more mathematical. But in type theory, I can also write this as an if-then-else. I could write this
as, okay, now I'm running out of space already, as this way. If b, if you're more used to programming, then maybe you prefer this
way of writing things. If you have done any functional programming, that's what's going on here. So now, for example, and I
can't do everything, but for example, injections are definable. So in left form, if I have an a, I got an element of a plus b. And how do I do
this? Oh yes, I should say something about this. Here, a function application, I just write as juxtapositions. In mathematics, we often
write function application, like, I think I, I did, this is already mentioned, I explained earlier, I write f of a. But there are these
brackets, and why do I need these brackets? So in functional programming, you just just f of a. And if you need to, you put brackets
outside. In functional programming, and also in type theory, actually in type theory, there's a bit of a mix, both are used. So in left, so how
do I define this function in left? I say in left applied to a is defined to be true, a. So if the first one is true, the second
component has to be an element of a, but that's fine, right? I couldn't, I couldn't, if I write true here, I couldn't put a b in here, but I can put an a. And it
should be no difficult now to see how I can define the second injection, and then there's also, you can program this, for example, doing case analysis. Now I have to
find plus using sigma, and I have said before, pi, which is actually dependent functions, they are sort of products. And if you look at this,
then maybe it should be, you should have the idea that I could define products in the same way, right? If the pi is a big product, then I can also make small
products. So yes, so what is this? I can say a times b is a function, actually, dependent function given a Boolean returning either an a or a b, depending on
this Boolean, yeah? So this function has an element of a, which you get out if you apply it to true, and it has an element of b, which you get out if you apply it to false. So this is really, I mean, in a way, now we
understand why pi is written pi because it's actually related to product in the same way as plus is related to sigma, yeah? And there's some, so we've seen, we have now two definitions of
products, right? One uses sigma types and one uses pi types. And this is interesting behavior of the product that can be used, seen both as defined by the introduction behavior and by the elimination behavior. I don't want to dwell on this. So here, what we've seen so far, a little overview over some basic concepts and
type theory, I will have a more complete list at the end of the talk, but in the moment, we have seen pi types. The pi types as a special case are functions, but if you've seen another special case is products. Then we have sigma types, which as a special case again, produce
products, but we can also use them to represent this joint union. And then we have seen some inductive types, such as two example. So what are inductive types? Inductive types are types which are generated by some constructors.
So the Booleans are just generated by saying a true and false is in Boole. The natural number is an infinite inductive type. We just say natural number is zero, or given a natural number, I get success of n, which is another natural number. So we have these inductive types, and they are quite interesting and open-ended concept, anyway.
Okay, any questions so far? Okay, we're good.
So one important aspect of type theory, and Marc has already mentioned this, is this idea of propositions as types.
So set theory, and Vladimir has already talked about this, is actually always comes together in a package with classical predicate logic.
So what is a proposition in predicate logic? We understand it as saying what is true. So propositions are related to truth.
Now, and I'm happy to discuss this more, but let me say a few words about this.
So I think the notion of truth, I understand if it's related to the real world.
But I personally find it problematic if we talk about mathematical constructions, which take place in our mind. They're not really real objects.
And while there's this sort of analogy, which is used in classical logic, that mathematical or our mind full constructions are sort of very similar to objects in the real world.
We pretend they were real, so hence we can say, oh, this is true about the natural numbers, or this is true about the reals.
But they're not actually real objects. They are something we have constructed in our mind.
So I'm not convinced, or basically don't understand, that we should really use this notion of truth in this context.
And as Mark has already said, we can actually, we don't need to, we can do mathematics without having to refer to this idea of truth for objects which only exist in our mind.
And instead, and here you use a slightly different word, we define what is evidence for a proposition, which is really, I have a proposition and I tell you under what circumstances I would accept this proposition.
And I may sometimes say it's true, but this is just a way of speaking by true, I mean I have evidence for it.
And in type theory, and this is I think in my view rather nice, we don't need this extra apparatus of predicate logic on top, but actually type theory already comes with its own logic,
and this is achieved by this idea that we can translate propositions into types.
That means for every proposition we assign the type of evidence.
So for every proposition I give you a type, which is the type of the evidence for this proposition.
And I accept the proposition, or I can't even have a lab since the proposition is true, if there is evidence for it.
What else?
So here is the little table, which is very trivial, which is the translation.
Here on the left I have propositions, so this is implication, conjunction, disjunction, universal quantification, and existential, and I have written them in a sort of typed predicate logic.
And if I translate this, so what's evidence for a implies b?
It's a function from evidence for a to evidence for b.
Because if you want to convince me that if a then b, and you can give me a function whenever I put in evidence for a, on the other side evidence for b comes out, I accept that if a then b.
What's evidence for conjunction?
Evidence for conjunction is the product of evidence for a and evidence for b.
What is evidence for disjunction?
A or b is the disjunction union of evidences.
Either you can present evidence for this, or you can present evidence for this.
That's exactly what's presented by disjunction union.
And what is evidence for universal quantification?
It's a function.
If you put in an a, an x and a, you get out evidence for b of x.
So here we employ our dependent function type.
So let me just give an example.
If I want, what is evidence for, maybe I just turned this around.
So what's evidence for, let's say,
we had this x plus x equals two times x.
It's, it's a machine.
I put in a natural number and outcomes approved that x plus x equals two x.
Okay.
Now I haven't really explained what is a proof of inequality, but actually I can tell you a proof of inequality for natural numbers.
The number is just the fact that these two are identical.
So for any number I put in, so for example, if I fit in three here, I get a proof that six equals six.
No surprise, right?
Now the main thing is that I have actually constructed this machine which does this procedure for every number.
So I understand universal quantification via dependent functions,
and I understand existential quantification via a sigma type.
So what is evidence that exists in x and a, such as bx?
It's a, it's a pair.
It's an element of a, my witness, and a proof or evidence that for this particular witness,
my predicate b holds.
Yeah?
So can it be understood if there is a situation that you have a proof of any situation when you can construct a dependent variable?
Yeah.
Yes.
So if I, if I mean, let's see how, so to prove excess.
So if I want to prove, so if I want to prove there exists a number which is even,
then I understand this as the sigma type, and evidence would be, for example, I give four,
and here I have a proof that is even of four.
Now I use the other notation.
Okay.
Yeah?
So to prove an existential statement, I provide a witness and a proof that this statement holds for this witness.
Okay.
Okay.
Let me just give two simple examples.
The first one is propositional logic.
So I want to, to prove some tautologies.
So I want to prove that a and b or c implies a and b or a and c.
And to make it a little bit more interesting, I also want to do some predicate logic.
So if the existing x and a, such as either b of x or c of x, then either the existing x and a such as b of x holds, or the existing x and a such as c of x holds.
Yeah.
And these are tautologies.
Everybody knows, I hope.
In some cases, I can also turn the implication around.
So these are all logical equivalences, but I haven't actually introduced equivalence.
I just prove implications.
I wanted to compare.
So how do I justify this classically?
And how would I justify them type theoretically?
So the first one, a and b or c, I don't want to read it again.
How do I prove it?
I mean, how do I give evidence for it?
You could say it's obvious, but okay, if I, if I'm in my class and I want to explain this, so I draw this truth table here and you see the last three are true in both cases.
And if we then look up the truth table for implication, you see it's always true.
And this is a tautology.
So if you use a truth table, that's how things are done.
Now, in type theory, there's a completely, I would say, different way to justify this, which is maybe just interesting to look at this.
So if I have, I want to add, what I have to do is I do this translation, which is, as you see, it's just replacing symbols.
So I have a product of a times b, distant union c, and now I want to output, although a distant union of a times b and a times c.
So now how do I write a function?
I analyze the input.
So the input of this function here, I try to define this function and I know the input will be, oh, there's a bracket missing.
Oh, where is this bracket?
No, here's the bracket missing.
That's horrible.
So I know it's a pair of an A and there is a distant union.
And if you remember, distant union where a pair, a dependent pair of a Boolean and then an element.
So either it's the first component here is true and then we have, I write little b to indicate it's the element of big b.
So we have four c and these are the only two possibilities and we have an element of c here.
So these are all what are written here with the missing brackets are the all possible inputs of this function.
And in the first case, I want to produce a distant union.
So I have to first say I go left or right.
So I go left or right through and I have already an A and a b.
So I just have to put them together to make an element here.
If I had four here, then I know this little c is a big c and I can produce again an element of this distant union.
Just this time I have to go right and produce an element of a, c.
So that's quite a different flavor justifying these two things.
Here I analyze the truth and here I give evidence using this translation that the proposition is presented by a type of evidence.
So if I find an element here, I know it holds because I've just given a generic case of evidence for it.
So I was a bit stuck on the slide.
I thought it was a cool idea to do this classical constructive thing.
But when I do predicate logic, I think how do I actually do this?
It seems I need infinite truth tables and they take such a horrible long time to write, to be honest.
So I think these are called first order structures actually, never write them.
So the best I can come up with is on the meta level just to argue again why this is true, which is sort of a bit unsatisfying.
Because in a way you just do the same.
Don't make any progress because I mean, so how can I do this?
I can say, oh, this could be only false if this is true and this is false.
If this is true, then there must be an element of a that either one of them holds and in either of these cases, one of them must hold.
But this is actually the proof I will do in type theory.
I'm not sure.
I'm not really sure how I can do this, but maybe it's my lack of classical education.
Fun.
Yeah, but I want to do a semantic.
I want to give a semantic argument here.
I mean, yes, you can do this in actual deduction, but I wanted to say why is this true?
I'm not semantically looking at the truth understanding.
Yeah.
Fun.
Sure.
Yeah.
Okay, but then you have to justify each step semantically.
Okay, fair enough.
Anyway, let's go to the type theoretic version, which I think is much clearer.
And here, using propositions as types, I just have to write a function of this type, right?
So, again, analyzing this input, I know I have a pair and the only thing which varies here, which only thing where can analyze is the Boolean, which is part of this plus here.
Whether it's true and that have an element B of A, it's dependent in this case, or a fault and have an element B, which should be called C, bad.
This I should call C, which is A of C of A.
And depending on the input, I produce an output either here or here.
Maybe you notice already that actually this proof is very similar to this proof.
And actually, they are both instances of a more general fact, namely that sigma is in a sense associative.
If you have two nested sigmas, you can move them in and out.
And that's really what's going on here.
But what I want to point out, there's a very simple way in the propositions as types explanation to write down why this is a tautology.
Yeah.
This translation here means we believe, we can give the evidence for this implication as a function from this type to this type.
And what I'm doing here, I'm defining such a function.
Yeah, so I define, this is my definition of this function.
I could have even written colon defined.
So I define a function by, I analyze the input and for each possible case of the input, I tell you what the output is.
And that's definition of a function.
So we just wanted to remind you it would be great if you use the microphone then everybody can hear everything.
There's another question there.
No, I was just trying with using the microphone was trying the microphone.
Good.
Okay.
Okay, so now, as you may know, I want to tell you the full story and not keep quiet about this.
And there's a difference in this evidence based view and the truth based view.
And that is a certain things are not valid in the evidence based view.
So, if I say not a is a arrow empty and here's the empty type, and the empty type is a type with no elements.
Now, a means I have a function from a to the empty type.
Now, that means that there is nothing in a, because if there would be something in a, there would be something in the empty type, and the empty type is just intentionally left empty.
So, now, what about the principle of excluded middle?
Can I give evidence for a or not a?
So what would I need to do if I translate this?
For any type a, I have to give an element of a plus a arrow empty.
I also have to do this for every type and types are extension also I cannot actually look into a type.
Then I would the only way to do this is I always say left and always give evidence for everything or I can refute everything.
And that's clearly not the case.
So, it is clear that such a function doesn't exist and similar or equivalently the rule of indirect proof, if you want to prove a, then prove that not a is false.
So, not not a.
It's not, there's no evidence for it.
And actually you can show that these two principles, if we assume them for all propositions are logically equivalent.
So, they are not provable.
This is a bad thing.
I actually don't think so.
I think it's actually a good thing.
For example, the excluded middle not provable, I can actually express some interesting facts.
The excluded middle means that something can be decided.
So, for example, in type theory, I can prove that for all natural numbers, the excluded middle holds for the prime predicate.
Or not prime.
So, I can actually prove this.
So, this is like an instance of the excluded middle, but it's not a tautology.
It tells me that prime is decidable.
That for any natural number, I can either prove that this number is prime or I can prove that this number is not prime.
If I have another predicate, for example, I write hold here, which means I view my natural number as a presentation of a program.
And the question is, does this program hold or not?
Then this version is not provable because this is not decidable.
So, by not having the excluded middle as a tautology gives me a richer language to talk about properties of predicates,
which especially as a computer scientist, I find interesting.
So, there is a difference between prime and hold.
One is decidable and one is not decidable.
And this is not something which is expressible in classical logic.
Now, I have this remark as well.
While the excluded middle is not, there's no evidence for it.
If I double negate it, there is evidence.
And this is quite a clever proof.
If you want, you can try to think how I can write a function of this type.
It's a function because not goes into the empty type.
So, you really have to write a function.
You do a proposition as types and coding.
And write a function.
There is a function of this type.
And this gives rise to a way to understand classical reasoning within type theory.
It's called the negative translation.
So, in a way, how does this work?
If a classical person says there exists an X with a property,
I think no, that's not what they mean.
They mean it cannot be that there isn't one.
And if a classical reasoner says A or B,
I understand this, that they mean it's not the case that both are false.
And this translation explains classical reasoning.
And you can summarize classical reasoning as somebody who can never say anything positive, right?
Because this is what's called the negative fragment.
Everything is negated of constructive reasoning.
Okay.
Now, there is not just excluded middle.
As you may have heard, the other source of evil is the axiom of choice, right?
So, let's have a look at the axiom of choice.
So, I use this particular presentation.
I write here R as a relation.
So, I'm somewhere in between sets here in type theory.
And I say, okay, for all X and A, there exists a Y and B such that a certain relation holds.
So, for example, these are socks, paired socks or something.
Then, there is a function from A to B, a choice function,
and A chooses a B such that for all X and A,
the X and the B chosen by the choice function are related.
Okay.
So, that's one of the many ways to represent the axiom of choice.
So, it can be, because it's in predicate logic,
we can very easily translate it into type theory.
So, here I predicate, for me, it's just a family.
It's a function from pairs into types, which for any pair gives evidence.
And here, I translate this.
So, for all this pi, so this is a dependent function returning pairs,
and I want to have a pair of a function,
or I forgot this should be a pi and another function.
Now, I want to write this function.
C gets an argument F, and F is of this type.
It's a dependent function.
And as an output, I have too many Fs.
I want to return a function from A to B.
Now, I can get this function from A to B,
because my input was a function, a dependent function,
which for any A gives a B together with this thing.
So, I compose my function with the first projection.
What's the first projection?
If I have an element of a dependent pair,
I can project out the first component and get an element of A.
So, that's the definition here.
So, I just combine this with the first projection,
and now I want to show that this function here is actually correct in this sense.
So, this is a dependent function again,
and I can obtain this second function,
taking the same function,
but this time projecting out the second component.
And here, I've given the type of the second projection,
which is a dependent type.
So, if as input I have a dependent pair,
then the output is an element of B
as the first projection of the input.
So, the second projection here is dependent,
and if you do the calculation,
you see that using this rule here,
you just get exactly the right type.
Yeah?
How does this go together with that in set theory,
the axiom of choice actually implies the law of excluded middle?
Oh.
It does, and it's so easy to write down in type theory.
Are you guys saying something different, or how does that work?
Yeah.
Yeah, what happened?
Oh, all right.
You knew my next slide somehow, yeah?
Huh?
I thought axiom of choice is evil,
and I've just done something evil?
Does this mean that type theory is evil?
So, no, I've been cheating.
So, if you look at this,
this is not, I don't do any choice.
My premise told me,
gave me the choice,
because I represented the existential as a sigma.
So, the choice was already in there.
I just had to take it out again.
This is not what we mean by axiom of choice, right?
There's no choice made.
The premise gave me already the information about this function.
So, here, we notice that there's a difference
between types and propositions.
So, types can carry information, right?
Sure.
Propositions, not.
I mean, propositions, maybe they hold, maybe not,
but there's no information.
I mean, I cannot look at the proof of a proposition.
That's the nature, I would say, of a proposition.
So, a proposition should be really a type with no information.
So, here, that's what we do.
We say, okay, a type is a proposition.
If it has at most one element,
means there's no information.
And I can write this down.
Let's say it's a type,
and to make this type of proposition,
I show that every two elements are equal.
So, that means that either it's empty or it has one element.
This is exactly what Vladimir talked about,
as a sort of truth values.
The truth values are these propositions.
Now, if you look back at the logical operations,
we can show that they are closed under being propositions.
And here, I abuse notations a bit.
So, I say if I have a type and a propositional family,
then the pi is also a proposition.
And why is this?
Because if you think about it,
if the codomain of a function has at most one element,
then there is a most one function.
But the operations...
Sorry?
Yeah, you need functions.
Yes.
Yes, yes, you need functions.
That's fine.
Actually, this is equivalent to function extensibility.
So, plus and sigma are not closed under prop.
So, why is this?
I mean, we've just seen it, right?
If I have a sigma type,
whose first component is something interesting,
like natural number, for example,
sigma exists a prime number,
sigma n not prime of n.
This is not a proposition,
because there's more than one proof of it.
Three and three is a prime.
Five is a prime and so on.
So, we are in the same situation here.
We have a type and a propositional family that is a predicate,
but the sigma is not a proposition.
And that's exactly what we exploited
in this doubtful proof of the action of choice.
So, we have to, if you want to represent proposition
and propositional reasoning in type theory,
which is actually quite useful, it turns out,
then we need to do something.
And the answer is something which is called,
was known under different names,
but is now called propositional truncation.
And it's an operation given a type A,
which produces a proposition.
And the understanding is this is a proposition
which just says this type is inhabited.
So, we have a function, I call it eta,
because sometimes people use single bars,
from A to the truncation of A,
which means if I have an element of A,
I know that A is inhabited.
But the truncation of A is a proposition
that means any elements are identified.
So, in a way, I can use it as a black box.
I have a proof, and now I put it in the black box
and can't look at it anymore.
You know there is a proof, but you don't know which one.
I'm hiding it, secrecy.
So, we can now define our propositions as types translation,
and we have only to modify disjunction
and existential quantification.
And in both cases, we hide the evidence.
So, here the evidence is whether we are left or right,
and I'm not telling you how I did this.
I did it.
And here, I'm hiding the witness.
So, existence means, yeah, we have a proof,
but it's hidden which witness we used to provide this proof.
And now, if we go back to the axiom of choice,
so I now have to, since I used exists twice,
I'm now inserting this propositional truncation operation
to make sure that my axiom of choice is a proposition.
And once I have done this, it's no longer inhabited.
So, the proof from before doesn't work because I'm here.
If you read this, it says, for every x and a,
I have some b which has some properties,
but I don't tell you which one.
And from this, you secretly can produce a function from a to b.
So, maybe you think because you're a secret about your function,
you could hide the fact that you're lying,
because I have never told you which b is the output.
So, how can you now define a function which gives this,
even if you hide it?
No, I think you're lying.
So, this is just a lie.
And as lies do, one lie implies another lie,
and that was mentioned before.
There is a construction due to Diakonesco,
where you have a clever choice of a,
you can actually prove that from this axiom of choice,
you get excluded middle,
which we know cannot hold in our evidence explanation.
So, hence there cannot be no evidence for this axiom of choice.
And I always wonder why one should believe in it,
but maybe somebody can tell me.
Yeah?
With this proposition lines,
how would a proof look like that proves that this junction is commutative?
Okay, yes, you can do this,
because you go from, okay,
so maybe I should, the question is,
what can I do with a truncation?
And I can do something,
as long as what I prove from it is a proposition.
So, if you have a, and you have truncated a,
so this is a function eta.
So, now I want to write a function from truncation of a
into another type, b,
but I know that b is a proposition.
So, because b is a proposition,
it is okay, I can define a function.
So, if I have a function from a to b,
then this lifts into a function from truncation of a to b,
but only if b is a proposition.
So, as long as I stay in that proposition,
and using this, you can then prove that,
exactly what you asked for,
that a plus b implies truncation of a plus,
no, I don't, truncation of a,
oh yeah, no, a plus b,
what do I do now?
B plus a, oh yeah, truncation of b plus a, that's it.
So, because the result here is a proposition,
you are fine, it's enough to define a function
from a plus b to this, and this you can do.
But here in this case, it's,
this here is not a proposition,
so you cannot apply the sort of elimination behavior here,
this eliminator, this is called,
I mean, there's always a question,
how to define a function out of a type,
and I haven't really given you lots of details on this.
Okay.
So, now let me say a few words on equality types.
So, if you want to do...
May I just question on propositions also?
Yes, sure.
I just feel a bit confused between these like two different,
apparently different notions of proposition, right?
You talk proposition as, say, type of its evidences, right?
That's one thing, and different thing when you use this truncated form,
so it's like at most one element, right?
And I have actually, first thing,
whether we should really distinguish it as, you know,
just different meaning of term proposition,
and more specifically, what you do is like false proposition,
so if it has no evidence,
does it also, how say, collapse into one,
or you may have many different false propositions?
Okay, let me have one first.
So, I would not say it's a different explanation.
It was a refinement.
So, I said propositions as types,
so I want to translate, okay, what the propositions from logic into types, yeah?
But then I observe that doing this translation in the sort of naive way,
I get more than I asked for, yeah?
So, in a way, I'm saying that translating existential quantification
as a sigma type isn't really correct, yeah?
Because it's not what I mean by existence.
So, I actually have in type series these two operations.
Existence, which is propositional,
and sigma, which is not propositional.
And this really, in many cases, it's quite subtle, which one you use.
In a way, you have two existential operation.
One is informative, and one is non-informative.
But when I translate propositions,
now that proposition has two meanings,
I mean propositions in the sense of logic.
If I translate propositions in the sense of logic into type theory,
I should always use the non-informative exists,
because in logic, there isn't informative sigma.
So, in type theory, basically, I get all the logical operations.
I had logic, and I get some more, namely the sigma.
And this can be quite important.
So, for example, if you formulate, if you translate a logical statement
like this, this continuity principle,
browse continuity principle into type theory,
and you use sigma, then that's wrong.
It's actually unsound.
There's a reason proved by Martin Escado.
You really have to use the exist there.
So, really, this fine-tuning where to use exist and where to use sigma
is quite important.
The second question, I mean, yes, there is false,
which is just the empty type,
and it is already a proposition, so there's no need to truncate it.
And there's exactly one, if you see in New Orleans,
there's exactly one empty type.
I think in real life, you would like probably to say that there are
many different false propositions.
Yes, there are, but they're all equal.
I mean, yeah.
Once something is empty, it's empty.
Yes, it's extensional, exactly.
Just a short follow-up question.
So, two propositions that are not equivalent are then two different types.
Is that correct?
Because I at the beginning understood that propositions
are all somehow elements of one type that we call propositions,
but no, every proposition is a type.
Is that how it is?
It's just a follow-up question.
Yes.
So, the only propositions you can write down in the empty,
without any assumptions, is the false proposition
and the trivially true proposition.
But obviously, we often look at propositions where we have assumptions,
like this tautology, we assume that A, B and C are propositions,
and there's more interesting stuff happening.
But two not equivalent propositions in type three are two different distinct types.
That's true.
Okay, thank you.
A further question.
Yeah.
Regarding such translations,
in intuitionist first order logic,
it can be made precise that you can translate every classical proof
into a corresponding one with some more negations, etc., etc.
Can there be made anything precise in type theory?
I guess it will get non-obvious if you have inductive types or something like that.
Yeah.
Because of, I mean, if you only look at propositions,
then everything works as before.
I mean, there's no, it's just you do the same game in a different context.
If you look at types, then it's not clear
how you can apply the negative translation in general.
If you have what we call proof relevance,
then it doesn't really work, I would say.
There was another question, yeah.
I just want a short question.
Somebody just asked whether two equivalent propositions denote the same type.
No, you just asked the negative version.
He asked whether two not equivalent propositions are different.
Okay, then I want to ask the positive versions,
must two propositions that are equivalent denote the same type?
Yeah, that's true.
So that's actually, it's an instance of univalence.
And it tells us that if A and B are propositions,
then if they're logically equivalent,
which by the way means just that you have an implication this direction
and one this direction, then this is itself logically equivalent
to them being equal.
This is also called propositional externality
and it is a special case of univalence.
Because I mean intuitively, the only thing about a proposition you can see
is whether it's inhabited.
So if two propositions are logically equivalent,
the inhabitants is the same and that means they are the same.
Sorry, does it hold in all type theories?
Or is it just with univalence?
Yes, exactly.
It doesn't hold an intentional type theory here
because intentional type theory doesn't allow you.
Intentional type theory only identifies objects
which are constructed the same way.
And that's not the case.
Okay, so a few words about equality which I have already used,
but then I mean the explanation here,
I hope it's already clear by now,
so if I have two elements of a type A,
I have a new type A equals B
and this is the type of reasons that A is equal to B.
And we always have a raffle,
we always have that A is equal to itself.
In intentional type theory,
this is understood as an inductive definition
in the sense that this equality type is generated by raffle.
So raffle is the only way to prove an equality.
The same way, it's the only way to do a natural number as zero as successor
and the only way to construct a Boolean is true or false.
So the only way to construct an element of an equality type
in intentional type theory is raffle.
And this has all these consequences we had seen yesterday,
so if you have two objects which are not defined the same way,
I cannot prove them equal because I can only prove things equal
which are actually themselves.
So the equality in intentional type theory is very, very poor.
Nevertheless, here is a question which was on vogue
when I was doing my PhD, it was open.
The question was, is equality a proposition?
It means, are any two proofs of an equality equal?
A proposition is, any two proofs are equal.
Now, I didn't really tell you everything about equality,
in particular I didn't tell you how to write functions out of equality
because this is ugly and a long slide so I didn't really want to present this.
However, this question here was then settled to the negative
by my friend Hofmann, Martin Hofmann, Thomas Streicher,
using the GrupoEAT model.
So what they did is they say,
okay, here we have an interpretation of type theory
which justifies all the principles of intentional type theory
and we use GrupoEATs, what's a GrupoEAT?
A GrupoEAT is a glorified equivalence relation
or it's a category where every morphism is an isomorphism.
What else can I say?
And in this GrupoEAT model, types that interpret as GrupoEATs
and equality as a homestead, you can have non-trivial equality types
and hence this is not provable in the theory.
This was a while ago when was this?
Okay, this was in the 90s.
And in HOT, it's sort of straightforward.
We can sort inside the theory.
We don't have to go to a model.
We can observe that equality of sets and the word set here
is actually intentional as you'll see in a second.
So equality of certain types which we call sets isomorphism.
So we say two sets are equal if they are isomorphic.
The equality of sets is given by isomorphism.
And then if you think about it,
bull equals bull, how many ways are there?
It's a stupid question, but how many ways are there
to show that bull is isomorphic to bull?
Two?
Either you use true to true and true to false to false
or you swap them.
So this type here has two elements
and hence it's not a proposition, end of story.
So in HOT, you can actually prove
that equality is not a proposition.
So it's a stronger statement.
And you can do this inside, you don't have to refer to models.
So this observation gives rise to a hierarchy.
I think the hierarchy is actually a hierarchy
because they are different ones.
But this is an interesting hierarchy.
I just wanted to point out there are different ways of number them.
This one is the one in the book
and this is the one Vladimir likes.
So I leave it to you to understand the relation
between these two numbering systems.
Apparently this one comes from homotopic theory
which also uses these levels.
So we have prop here.
So prop is exactly what I said every two elements are equal
and then we have set.
So when we say set, we're not talking about set theory
but we mean a type which is of the form
that every equality is a proposition.
So every two elements is a proposition.
And we can actually extend this here one level down
and say a type is contractable if it has exactly one element
and there's a little proof that the definition for prop
that I gave you in this one is actually equivalent.
That's one of the first proofs you do in homotopy type theory.
And then you can continue this.
So why do we call them sets?
Because equality is propositional.
That's what you usually expect.
So for example, natural numbers are set.
So any sort of run of the mill
which you find the garden variety of types, they are sets.
But then clever things like the universe of all sets
of all small things is not
because equality we have seen is not propositional.
Bull equals bull is not a proposition.
So if you do clever things,
then things are not propositions, they're not sets anymore.
They become group voids.
So what's a group void?
Now in type theory, not in category theory or in GPD
is something so that all equalities are sets at least.
But then you can continue this and have something
where all equalities are group voids.
And this is then called, I should have written this one.
This is a two type, a three type.
So this is called a one type, a zero type,
just using these numbers here.
Or these numbers, whatever numbers you like.
So this is a hierarchy of types
which arises naturally in homotopy type theory.
And it is in a sense important.
And let me just give you a little idea for equality types.
So here obviously for prop but also for contractable types
equality always holds, so it's trivial.
Once you have a set, the equality on sets are equivalence relations.
And if you go to group voids, there are group voids that hence the name.
So we have here, and what happens the next level,
we have something that's called a two group void.
It's a group void whose home sets are group voids.
And you get into this higher category theory here very quickly.
So in hot, equality is not inductively defined,
but a type always comes with its equalities.
And the structure of equalities is what's called a weak omega group void.
So what we think of, we don't think just of the elements,
but we think of a type together with all the equalities,
the equalities and equalities of equalities and so on.
And the structure, I mean, if you're on the low level,
if you know it's a set, then it's an equivalence relation, yeah?
Maybe just to avoid confusing people,
it's worth pointing out that a prop is not necessarily
understanding the same way in all even type theories,
based proof assistance, for example,
in COC prop, it works in different ways,
just distinguished by being impredicative and being stripped away
during program extraction.
So if people want to have a slightly different,
well, related exposition of what is wrong with axiom of choice,
maybe a good reference is simply this article of Martin Love,
one hundred years of axiom of choice, what is wrong with it,
when it's just stressing that the point is the distinction
between intentional and extensional,
because this presentation is tying it very tightly
to this specific understanding of what prop is.
Yeah, I never, okay.
So thank you for this remark.
I should have said this, if you use COC,
they mean by prop something really weird
and better forget about it.
I have to think a bit more.
I know this article, and I think it's a bit of a confusion,
so I mean, I know it's Pierre Martin Love,
but okay, I'm, okay.
It's, yeah, it's strange.
I mean, okay, that's not cool thing.
I have another question.
Concerning this interpretation, look,
I think in any event, if we have a type of whichever level,
we say the term of, okay, we think of type as kind of space
or a group point, and term in any event is a point, right?
Yeah, element, yeah.
Element is a point.
Yeah.
And all this, what you call structure,
like group point, right?
Yeah.
You say exposed through equality type.
Yes.
I haven't said this, but almost.
Yeah, so it makes kind of, you see,
on one hand you have just like a single type,
you think it structures.
You think only of the elements.
You only think of the elements.
So in an intentional type theory,
you only think of the elements.
And then you construct a new type,
which is the equality type,
and you only think of the elements.
So really, in intentional type theory,
you should have this principle
that every equality is a type,
because it's constructed inductively.
So that's an incompleteness.
It's a strange incompleteness of intentional type theory.
No, some strange thing comes this.
Maybe just, you know, kind of geometry and imagination
which doesn't really fit logic.
But if you think about elements as a point,
suppose you have kind of higher order type,
you know, two group points, right?
But still, elements are points.
So all these past homotopies,
they live somehow, you know,
in one sense they live inside, right?
But in different sense, you expose them
in a different type, like equality type, right?
So how it goes together.
Okay, I haven't really understood the question,
but let me try to answer that anyway.
So first of all, I haven't really talked about topology at all.
Okay, okay.
However, there is a way,
if you really want to understand what weak omega group weights are,
then it's a good idea to ask the homotopies theoretician
because they have studied this already
and the sort of feasible definitions
of what an omega group weight comes from homotopy theory.
And that's what Vladimir used is the Caen complexes, basically.
The Caen complexes are a way to make precise
what is a weak omega group weight.
It's just like downside that it's classical,
but that's another longer story which is still ongoing.
Yeah, okay, thank you.
So this is at least one of my last slides.
So I just wanted to point out
that the understanding of types and homotopy type theory
is the whole thing, the whole structure.
And then we have morphisms or functions between the structure and so on.
And we can model in this world, model very interesting things.
We have a very existential equality
and we can introduce something which is called higher inductive type
and I think there's a workshop on those.
And you can lots of exciting things.
Some people use it to do synthetic homotopy theory,
but I have been using them for other more mundane computer science purposes.
But I don't really, I mean,
we have defined the syntax of type theory in type theory
without ever talking about partial elements or pre-times.
And we have given a definition of what is the partiality model
which you can use to represent partial computations in type theory.
So there are many applications of these higher inductive types
and what's the idea?
If we have an inductive type, we say,
oh, we generate all the elements by giving some constructors for elements.
But once we view a type as this big structure,
then why only giving elements,
why not also giving equalities because they're part of the structure.
So we freely generate the structure,
not just giving elements, but also equalities.
And that turns out to be a very powerful concept
together with univalence.
Okay, I'm more or less at my end.
So first of all, here's a little summary of what we have seen
and what we have not seen.
So I have talked about Pi and Sigma types, inductive types.
By the way, using what I have just said,
this propositional truncation operation is an inductive type,
a higher inductive type.
That's maybe surprising.
Equality types, as I say,
they basically give us access to this higher structure of types.
And I have really talked about universes.
So some people write U and I write type
and we have a hierarchy of universes
and you can make it bigger if you like.
And I didn't really want to talk about this today.
So here I have some sort of tentative conclusions.
To me, type theory is actually constructive in two ways.
It's constructive also in the sense
that I think this idea of collecting elements
is very sort of, what to say, platonic
because the elements are already lying around.
There somehow is this universe of elements
and I am just going there and collecting things.
That seems to be a weird idea to me.
To me, I construct things,
because mathematics is not sort of something which I find.
It's something which happens in my head.
So type theory is constructive in the sense
that a type gives you the elements.
The elements are not lying around somewhere.
And it's constructive in the sense
that truth is replaced by evidence
and it gives rise to proposition as types and universe.
So I think modern type theory,
I mean, homotopy type theory
is really a truly constructive theory.
And here a little preview,
which is a little book I'm writing
which I haven't yet finished,
but I thought I'd make a free advert or something
or whatever you could call this.
Okay, thank you very much.
Thank you.
