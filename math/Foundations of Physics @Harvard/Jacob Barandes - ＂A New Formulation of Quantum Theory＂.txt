So hi everybody, welcome. I'm Jacob Berndes. I'm at Harvard University. I'm going to be talking about a new formulation of quantum theory.
This is an ambitious talk, so I want to get started. We're going to move fairly quickly.
Okay, so the structure of this talk, I'm going to start with some motivation. I'm going to talk a little bit about the theory of stochastic processes, which will be a major part of this talk.
Then I'll connect this up with Hilbert space pictures and Hilbert space formulation.
And, and this will be the heart of what I'm going to try to explain in the talk is the correspondence between these two pictures.
Then I'll talk a little more about dynamics in particular about crusty compositions which I'll define and then I'll talk about a uniterity and its emergence.
And I'll talk about interference, how that shows up in this formalism. I'll talk about decoherence and I'll introduce the notion of a division event. I'll talk about entanglement.
And then we'll talk about the measurement process measuring devices and I'll introduce the notion of an emergible, and then we'll have time for discussion.
Okay, so motivation or why should you care about any of this.
So, I probably don't have to convince many of you here that there are problems with the textbook to rack bottom axioms for quantum theory they're abstract or complicated.
But, you know, more to the point they're ultimately incomplete independent of fundamental way on notions like measurements that are not defined by the axioms themselves.
And this issue leads directly to the measurement problem. There are many ways to formulate this and there are different variants of it but the one I'll be using is the measurement axioms the born role wave function collapse don't unambiguously specify which
can act as measuring devices. So the one hand applying wave function collapse breaks uniterity.
And that means deciding whether or not to apply it can make an empirical difference and this ambiguity is pressing.
Now that we, you know, are developing the technology to develop mesoscopic scale devices.
On the other hand trying to drop the measurement axioms makes it difficult to see how to get definite outcomes or probabilities out of quantum theory.
It's instructive to line up the measurement problem with another very important problem in theoretical physics the black hole information loss problem. They're both decades old problems they both at least superficially look like a violation of uniterity they're both very hard to study empirically
in the lab. They both effective theory that may be part of quantum gravity.
Both of them have at times generated wild metaphysical speculations.
They produce very interesting spin off results.
I should mention that the measurement problem or at least attempts to understand it better have led to at least one Nobel Prize in recent years. No Nobel Prize is quite yet for black hole information loss but maybe someday.
So, you know, your reaction to, you know, the measurement problem the measurement axioms.
And this whole problem of interpreting quantum theory might be to try to assign a physical meeting directly to the ingredients of overspaces.
The reason to be skeptical of this is that quantum theory contains a relatively unrecognized form of gauge invariance.
If you let VFT be any time dependent unitary operator the following gauge transformation leaves all the empirical predictions of textbook quantum theory invariance.
So we transform the state vectors away functions by acting with VFT density matrices observables transform by similarity or conjugation transformations by by the time evolution operator of the system transforms as a so called correlator.
And so you'll notice that it contains a time to equal zero on that right factor of the dagger that you need that in order for the predictions to be invariant.
And if you figure out what this means for the Hamiltonian you find the Hamiltonian transform was just like a non-ambient gauge potential to this was first at the first example I have found of this being carefully defined was in a paper by Harvey Brown in 1999.
And you know the interpretation here is, if you think of taking the systems Hilbert space and fibering it over one dimensional manifold parametrized by the system's time.
Then you know time evolving state vector is a section on that fiber bundle.
And this unitary local and time unitary transformation is independently unitary rotating each of the Hilbert space fibers independently so really is a gauge transformation in the mathematical sense.
And then the time evolution operator is really a correlator between Hilbert space fibers and Hamiltonian is a connection, it's a flat connection that means that you can gauge transform to any Hamiltonian that you want.
In particular, it means that all the ingredients of Hilbert space picture a gauge dependence, and that makes their physical meaning highly suspect.
You can use this to map any state vector trajectory the Hilbert space to essentially any other state vector trajectory, which casts doubt on on taking it seriously is containing gaussian variant physical content.
Okay, so another motivation, and this is an analogy from history. So, in replacing Ptolemy's instrumentalist model of the universe which one could regard as just like an instrumentalist mathematical mechanism for making predictions about observations of planets with all the
particles on epicycles, you know, in replacing that with, you know, the Keplerian Copernican Galilean picture, it wasn't just a simplification.
It also provided a clear physical picture actual bodies moving in space.
It would be improvement on the direct phenomenon axioms if we could do the same thing for quantum theory, and I would argue that there isn't another kind of principle we all know the principle that if your theory is telling you that you're located in a special place in the
space, maybe you've made a mistake, but I would argue the other Copernican principle is that yet instrumentalist theoretical frameworks are useful, but we shouldn't settle for them.
We shouldn't settle for them forever getting the physical picture right really matters even if it takes centuries to see why. I mean, maybe Galileo couldn't have imagined it.
But, you know, today we can build rocket ships and go into space and visit those orbs and that wouldn't have been possible. If we've been using the Ptolemaic system.
So this is another motivation, somewhat more practical motivation so when modeling a real world macroscopic system, you know this could be a turbulent system a biological system, systems and social science finance that be it's
probabilistically one often uses the theory of stochastic processes I'll describe that in a little more detail later comes from the Greek stochos or stochastic goes for a more guess in modeling systems like this one often resorts to various simplifying
applications to make these models tractable. So a big one is the Markov approximation which I'll define a more detail later and also a little less well known as divisibility again I'll define those a moment.
I'm going to show in this talk there exists a highly useful formalism for modeling stochastic processes without making these approximations. There's like an analytical mechanics, a general framework for being able to model stochastic processes without those approximations that's been waiting
to be solved all this time, and this formalism will actually turn to be quantum theory itself. And this potentially opens up a whole new application for quantum computers and maybe in the shorter term quantum simulators as an efficient tool for simulating
generics stochastic processes, identifying and removing hidden assumptions is often a key step toward discovering new science.
And by avoiding the assumption of a Markov or divisibility approximation will be crucial for deriving quantum theory. So then I said another way, non Markovian indivisibility seemed to have been the key missing pieces the secret sauce for obtaining a physical picture.
And that's why arguably earlier efforts like the Fendi's Nelson stochastic approach to quantum mechanics, didn't ultimately pan out.
So there's been a lot of work in recent years, suggesting that quantum theory is fundamentally non Markovian, just as an example of paper from a few years ago by Lincoln dummy.
So later review has standalone probabilities joint probabilities conditional probabilities and marginalization work.
So PV is a standalone probability of some proposition e p of f is the same thing for some proposition f PV and f is the joint probability p of e vertical line or p given f is the conditional probability be given f.
And a simple factorization relationship between joint probabilities conditional probabilities standalone probabilities given by this formula here.
And there's a notion of marginalization. If you take joint probabilities and some over the possible values of one of the propositions, then you end up with a standalone probability for the other.
If we insert this factorization relation into this into this marginalization formula we get the marginalization formula in this form.
The standalone probability of e can be obtained by taking the standalone probability of f multiplying by the conditional probability of e given f and then something over f. This will be an important formula in our work ahead so just signposting that we'll need that.
Okay, now this talk is not going to be limited any specific kind of quantum system when I say quantum theory and then quantum theory quite broadly not the quantum mechanics of particles.
But I will find it useful here to talk about a single particle thought experiment it'll be good invitation, motivating why we should be thinking about non Markovian indivisible stochastic networks in the first place.
So we're going to consider the double slit experiment. And in this experiment one imagine setting particles one at a time toward a wall with two slits that observing with a particle arrives on a projection screen.
So here's a schematic picture of the diagram. The slits will assume a very close together. Let's let a broadly speaking denote various possible starting conditions be denote which slit the particle goes through and see where it lands.
Another classical assumption is that the conditional probability p of b given a which slitted enters given initial conditions times p of c given be where it lands given which slit you multiply this together and some over which slit you get the conditional probability
of C given a where it lands given its initial conditions, but it's important to note that this formula does not follow from the general marginalization rules it is not a mathematical identity.
The probability requires making a Markovian or divisibility assumption. This is an implicit assumption that's been lurking there.
This predicts the following pattern over many repetitions if you make that assumption that over many repetitions you get a histogram it looks a little bit like this and just a note this is a blend of two distributions you get like to bump like distributions from the two holes and if the
holes are close together they merge into this distribution here.
And indeed this matches observations from microscopic particles like stones.
This is done by contrast one instead observes what looks like an interference pattern. This is what the histogram looks like distinct locations where the probability of the particle landing is very very small and other places where it's very very high.
But does this mean that the particle is really a wave or that the particle somehow goes through both slits on each run of the experiment. Well, I'm going to show in this talk that one can account for this pattern merely by allowing the dynamics to be non Markovian or indivisible.
And this is because I'm not going to make this assumption. I'm going to allow this relation, which, you know, from a purely mathematical or, or, you know, fundamental standpoint, one has to justify, I'm going to drop it.
And we're going to see that we can get the same results without it which is surprising but very interesting. Right now let me talk about the theory of stochastic processes a little bit.
We're going to take our basic model for system described by this theory of stochastic processes to have the following kinematical axiom that means what its configurations are.
Every system in this framework will have a configuration, it'll be denoted by Latin letters ij and so forth, running from one to n in some configuration space composite systems and their subsystems will be defined by Cartesian products just like in classical physics.
By the way, I'm going to use a discrete notation here for the configurations I labeled one through n but keep in mind n could be countably or uncountably infinity after replace summations with integrals, probabilities with probability densities and so forth.
But it'll make the notation too complicated and also have a lot of time.
So this is a reminder of x and y or sets their condition product x cross y is just a set of all the order pairs little x comma little y for a little x and little y belonging to those two sets.
And so what we're saying here is that if you have a subsystem a with configuration I subscript a and subsystem B with configurations I subscript B, then the composite system AB has configuration given by the order pair.
That's all we need to order pairs of configurations like you would do classically.
The axiom for these systems are going to consist of the assumption, the axiom deposit that there exists to a suitable level of approximation, well defined conditional probabilities.
Gamma sub ijft. These are conditional probabilities for the system to be in its ith configuration to T, given that it is in its jth configuration at some initial time t equals zero.
Now, a little bit I'm going to explain why t equals zero is not a special or unique time, but for now we're going to be assuming some initial time t equals zero.
And that's that's it. Those are the physical axioms of kinematic laxium and a dynamic laxium.
I should just make a note here that the stochastic property conditional probabilities here.
You can if you if you wish you can model a deterministic system with them by degenerating them to trivial probabilities of one and zero so this is actually more general than deterministic dynamics.
So we have just two axioms now the kinematic laxium identifies the physical stuff the configurations although it will depend on what system you want to model.
This isn't going to give us the answer about what the fundamental configurations of all the universe are like in classical physics you propose a model and you have to pick the configuration space.
The dynamical axiom will see treats closed and open systems quite equitably and neatly generalizes classical deterministic time evolution to the probabilistic stochastic case.
And the metaphysical level behind all of this formalism, there is just a system with an underlying configuration evolving through time along some trajectory that we don't know in a configuration space, but we lack.
You know, a framework to say definitively deterministically what that trajectory is and that's why we're resorting to these conditional probabilities.
The claim is that one can derive all of quantum theory in general from these physical axioms via a set of mathematical identities and theorems together with some standard choices and composite systems and stochastic dynamics.
Now although the physical axioms here are simple.
One of the tasks that we have to do is re derive the whole formalism quantum theory quantum theory is complicated and will take many steps. And this is akin to asking a burn again with a simple physical model to account for and derive the whole Ptolemaic system to persuade it a Ptolemaic astronomer that that this
really does give an equivalent description, you have to go through this task, even though the goal is not is not to treat Ptolemaic astronomy as the fundamental thing.
So let's get started.
We assume a standalone probability distribution at the initial time, and these are probabilities they're not negative they sum to one, and we assume a standalone problem. We're going to study standalone probability distribution at a later time T.
And I should say later or earlier I actually haven't specified whether T has to be bigger than or equal to zero. I'm not making at this point any time asymmetric assumptions.
The probabilities are linear related by the conditional probabilities according to that marginalization formula that I told you to look out for earlier.
Unfortunately, this is a linear relationship, you can stick in arbitrary choices of initial standalone probabilities those are fairly adjustable, and then the dynamical conditional probabilities, telling you what the standalone probabilities are at the other time T.
One says that these conditional probabilities describe a stochastic process and this is, I would begin the discussion of stochastic processes.
I can actually write this in matrix notation that marginalizing formula naturally lends itself to matrix products, and again defining gamma IJ to be those conditional probabilities that introduced earlier this matrix equation be written very neatly as a column vector p of T equals this end by
matrix gamma of T times p of zero the column vector of initial probabilities. Right so gamma of T consists of conditional probabilities that means it satisfies the two basic properties that define a left stochastic matrix its entries are non negative.
And if you sum on I fixing J fixing J fixing what you're conditioning on I gives a probability distribution, and if you sum on I which corresponds to something down the columns you get one.
So far this is all textbooks stochastic process theory you find this in textbook like Ross's textbook.
Okay, but now we're going to say as general as possible. I'm not going to assume that gamma T describes a Markov process. This is in contrast to the assumption made almost immediately in textbooks and most phenomenological applications is to cast a process theory, to the extent that when
people say stochastic they often mean Markovian in particular I'm not going to assume things like a discrete time Markov process a discrete time Markov chain which is a special kind of Markov process in a discrete time Markov chain which is frequently used in
mathematical applications, you assume you can discretize time into integer steps of some basic time scale delta T, and then write the stochastic matrix for n steps of time as the product of some fixed constant stochastic matrix, multiplied by itself and
times. That simplifies things a lot but we will not be making that assumption.
A less well known approximation is called divisibility this is more recent to be introduced in the research literature a little more recently.
But this is the statement that for all times t bigger than t prime bigger than zero one has the following semi group property or composition law satisfied by stochastic matrices.
So, gamma of t here is our original stochastic matrix gamma t prime is the same time dependence stochastic matrix but evaluated at the other time t prime.
And the question is, does there exist another matrix gamma of t from t prime that's that T arrow t prime.
And then you win stochastic matrix exists that satisfies this identity that means it's got to be non negative entries and its columns have to sum to one.
In general, such a matrix won't exist that connects the stochastic process across those two times.
That means in general systems will have indivisible stochastic dynamics, and just to make clear that this isn't some strange or unusual assumption.
It's very simple two by two smoothly time dependence stochastic matrix it is non Markovian it is indivisible fact it's just a Gaussian time.
It's not periodic. Everything is perfectly smooth and differentiable here tau is some fixed timescale constant and T is the variable. Here's another example this one's periodic so it exhibits recurrences.
And these are the generic case, you can just provably show that if you try to write down a composition lie it will fail. If you try to write down that intermediate gamma matrix that intermediate stochastic matrix.
It will have non it will have negative entries it will not be a valid stochastic matrix, and this is the generic case.
Okay, so I promised I would show that there's a connection here to the Hilbert space formulation let's construct it. So the first step is to take that annoying inequality and solve it by expressing gamma.
So the first step is to take the JFT the i jth entry of the stochastic matrix as the absolute value squared of the entries of some new matrix. This is not an axiom or positive it's a mathematical identity.
I could just take the square root of the entries if I want that certainly exists, but I can be more general and regard them as the absolute value squared anything else.
Now, you don't have to use squares you can use fourth power fifth power, you just won't get Hilbert space picture, but I encourage you to consider other reformulations I mean we're starting from a new set of axioms.
I want to study other possible mathematical representations I encourage you to try that, but our goal here is to start with our physical axioms and reconstruct the Hilbert space picture in particular and for that I want to I want to write down this identity.
Right this matrix fate of tea is guaranteed to exist it is not unique.
The other property this property that the columns of the stochastic matrix sum to one becomes the following summation condition this will be important.
Okay, so this is this summation condition you square the entries and some down the columns and you get one.
So the mathematical entity that we just wrote down can be expressed rather succinctly in terms of what's called the sure had a more product. This is defined by naive matrix multiplication entry wise matrix multiplication.
The sure had a product of two n by n matrices has IJ entry that's just the product of the IJ entries, no sum.
This is, you know, kind of a strange product we don't usually introduce in textbooks not widely known probably because it's basis dependence.
But in our case we have a configuration space that picks out a preferred basis anyway so we don't care.
So write this basic identity then is the statement that gamma factorizes as a sure had a more product of its complex conjugate with itself.
This is the same statement, and this is useful, you know, conceptually and notationally in some circumstances.
In particular, so remember I noted that the theta IJ of T this weird new matrix I introduced underlies gamma does not uniquely. So this identity is not it does not uniquely define what what the n by matrix that it is.
So theta is like a gauge potential.
And in fact there's a corresponding notion of gauge invariance you can take theta and you can sure had a more product with an arbitrary n by n matrix of time dependent phase factors at the level of entries that means I'm refacing each entry individually by an arbitrary time dependent
phase factor. So these are like local in time on that fiber bundle that I talked about before.
So I'm going to talk about the gauge invariance which as far as I can tell hasn't ever been yet carefully identified literature turns out to be really important for understanding dynamical symmetries although I will not have time to talk about that in this talk but I could talk about it
at the Q&A people questions. So it turns out to be useful.
All right, so again we'll construct the Hilbert space picture I'm going to introduce an orthonormal basis. This is just a standard basis that sort of Cartesian looking basis with basis factors that are labeled by the configurations one through end of the system.
They're simple. They're complete.
That's the one by one identity matrix. If you want to work think in terms of bras and cats. You know this is the same statement in terms of bras and gets. And so this is more familiar to you.
I'm nervous about using bras and cats because I don't want to hide my cards I want to be upfront with everything that I'm doing and bras and cats wall useful can sometimes hide what you're doing.
So, then I'm going to define a projection value measure this is a set of diagonal projection matrices that each have a single one in the ith entry along the diagonal and zeros everywhere else.
Each of them is given by an outer product of one of these configuration basis factors with itself.
They're equally exclusive. They square to themselves their projectors their item point, and if you multiply different ones together you get zero. They're also complete. They satisfy that completeness relation. They have trace one as well.
And this is them expressed in bracket notation. If that's more, more familiar to people.
Okay, we're not going to do a very important set of manipulations.
If I take my theta matrix, which remember was this matrix of entries that underlie the stochastic matrix, and if I sandwich it between projection matrices in this way.
There's PJ data dagger PI theta and the P, and I'm going to work in terms of bras and cats here just to make them mathematical manipulations a little more familiar to people.
Well, I get this construction. I see the appearance of the IJ matrix element of theta itself I see the appearance of the complex conjugate of the IJ entry of theta.
Okay, the IJ entry because of the adjoint it transposes and complex conjugates.
And then of course this becomes out of a square divided by J times the Jth projection matrix. If we take trace of both sides using the fact that projection matrices have trace one, then we get this identity here.
If I use the cyclic property the trace I can move the PJ to the other side, where it meets its counterpart and because of idempotence idempotence it squares to itself.
Let's start with this simpler relation here. I'm going to write this again on the next slide.
Right. So this is that relation using the basic identity that relates theta and gamma. This says that I can express my stochastic matrix, its entries in terms of this interesting working trace.
This is a new result.
I've scoured the literature. It has never been written down before at least not for this purpose.
I can talk about it. It showed up once in a paper as an intermediate step for a completely different set of arguments.
And this new result is a dictionary. It translates between stochastic process theory that's the left hand side and Hilbert space formulation it's the right hand side.
What I'm going to do is show how this unspools the Hilbert space picture of quantum theory. I'm going to call it the dictionary or the stochastic quantum dictionary and all that follows.
This is important equations, what's boxed and large.
All right, let's go back to our standalone probabilities again remember this marginalization relationship we had.
Well now I'm going to insert the dictionary in for that stochastic matrix in that marginalization formula.
I insert it, I get this expression here, and now I'm just going to manipulate it I'm going to move, use the linearity of the trace to move the initial standalone probabilities and the summation inside I get this formula here.
Now I'm going to use the stochastic at cyclic property of the trace to move a dagger to the other side.
And then I get this.
All right, well this may not look like much, but let's introduce some notation.
I can write this as the standalone probability at the later time T the I standalone probability is given by the trace of the project or times row of T, where row of T is state of T row of zero theta dagger of T it's a similarity or conjugation
transmission of some other matrix row of zero, and row of zero is given by the following spectral decomposition it's a sum on the initial probabilities multiplied by the corresponding projectors it's a diagonal matrix.
It's basically the diagonal matrix with the initial probabilities along the diagonal.
The addition on theta guarantees that at all times the trace of this new matrix is one.
Now, you may be tempted to call this a density matrix and we will, and we'll argue them but it is, but these are certainly properties we're familiar with density matrices.
All right, now let's let a be a random variable on the configuration space of our system. That means it's some has some spectrum of magnitudes or values.
And basically what this means is that if the system is in its ith configuration, then this random variable has a I as its value, but we're going to be general and allow these values potentially depend explicitly on time as well.
Well this is the standard formula for a computer mean or expectation value or statistical average of one of these things it's the weighted average over its over its magnitudes over the standalone probability distribution at the given time.
And now what I'm going to do is insert the formula we already obtained for the ith standalone probability at t into that formula, insert that there.
And then after some simple manipulations you get this very familiar looking formula which says the expectation value is given by trace of aft row of T.
So here aft is given by the following specially composition. It's again a sum over the projection matrices it's the ith projection matrix multiplied by the ith magnitude of this random variable summed over I.
This is a diagonal matrix here with the magnitudes along the diagonal.
Let's suppose, as a simplifying case, that the standalone probability distribution at the initial time is pure.
So, and this is pure classically pure I mean, we know with certainty that the system is in its jth configuration at the initial time.
That means that P of zero the probability vector the initial time is just the jth configuration basis vector for some J.
Well, we know that the initial density matrix is a diagonal matrix with the probabilities on the diagonal in this case there's just a one in the jth entry.
And that means that the initial density matrix is the jth projector which is this outer product of the jth configuration basis vector with its adjoint.
Well, if I plug that into the definition of row at other times T, but I see I get this very neat factorization over on the right hand side.
That is, if I introduce a new new column vector psi, psi here is defined as state of T, multiplying that initial probability vector.
Then row of T factorizes at all times its rank one and it factorizes this new and by one complex valued column vector with its adjoint.
So this with our marginalization formula which gave us the final probability vector the real final probability vector you can see that effectively what we've done here is kind of taking the square root of our stochastic conditional probabilities, and
evolving with kind of their square roots gives us this and by one column vector psi instead of our probability vector.
Okay, so it's again so we're evolving by the square root of gamma.
Now, if I take that special form of the density matrix and plug it into the formulas for the formula for the standalone probability side, I get this very familiar looking formula right this is the born rule.
If I plug it into my formula for expectation buzzer and variables I get our familiar formula that you sandwich the matrix representing your random variable between the state vector and sadjoint.
So here that by construction, the ith entry of this and by one complex vector we've introduced psi is really just the name for particular entry of the theta matrix which underlies our stochastic, our stochastic matrix.
Psi is a whole the whole column vector is just the jth column of theta of this of this matrix theta.
So whatever psi is, at least in this approach, it's, it's not like a physical object. It's a piece of a law like or no logical thing theta, which, again is a non unique way to represent the system's dynamics.
Well these degrees should look familiar we see 30 years playing the role of a time evolution operator rose playing the role of a density matrix notice at generic times not diagonal.
A of t plays the role of a state vector away function and a of t plays the role of a self a joint linear operator representing say an observable although at this point, we were only talking about diagonal observables but we'll we'll come back to that question in a little bit.
One can even at this point go ahead and define a Heisenberg picture where you fixed state vectors and density matrices at the initial time, and move the, the time evolution operator over to your, your random variables, they will no longer be diagonal now.
We'll talk a little bit and talk about why this is useful for our purpose. So again, the random variables may depend explicitly on time, given the magnitudes may depend on time but now in the Heisenberg picture they also inherit a time dependence and implicit time dependence from the time
evolution operator, and you can check that this, this change leaves all of our probability and expectation value formulas and variants.
Okay, let me talk a little more in detail about dynamics now.
In particular, given a time evolution upper theta and remember I haven't made any special assumption assumptions in a theta theta is generic. It's just, you know, square roots essentially of the entries of our stochastic matrix and I wasn't making any simplifying approximations but stochastic matrix.
But given any such data one can always define a set of new operators k one through KN, each of them is defined by taking one column of theta and setting all the other columns to zero, there are n ways to do this.
And I'm labeling them by a subscript beta that runs from one through n.
And below the this matrix representation I have the definition directly in terms of entries this is something you can always do. It's important to note, I haven't made any special assumptions here you can always define k matrices this way.
Well, you can check that the summation condition on theta, which is a little bit awkward to write down in terms of these new matrices becomes a very nice matrix looking.
It becomes the so called cross identity. If you take the adjoin of the beta if cross matrix multiply it by itself and then someone beta you get the identity matrix, which you can think of as a generalization of uniterity and we'll be coming back to uniterity
moment.
But you can also show that these, these, these new matrices which are called cross operators like I said, they give another way to write down the time evolution of the system. So instead of row of t being theta of t row of zero theta dagger you can write that same time evolution this way as well.
This sort of time evolution is well known in quantum information theories called the quantum channel, also known as a completely positive trace preserving that.
Now, what's special about quantum channels is something called the Stein spring dilation theorem so just to remind you like the Hilbert space formulation we've introduced here is just useful mathematical representation we're not giving it any deep metaphysical
significance so we can, we can modify this much as one it's a highly malleable tool.
In particular the specific cross operators that that we defined here, they're not unique.
Although you can always write down the ones I happen to write down you can write down others, a given, you know, quantum channel may not necessarily have a totally unique set of press operators.
But, but more over what you can do is you can, you can dilate the Hilbert space you can actually increase the dimension of the Hilbert space if you want, as long as you hang on to your projection valued measure and they continue to satisfy the standard properties as long as you can p1 through pn, you
actually free you enlarge the Hilbert space representation if you wish.
And then the Stein spring dilation theorem, which goes back to 1955 proves that by dilating the Hilbert space formulation appropriately, and you can accomplish this formally by say adding an extra degree freedom.
You can turn a crousty composition into unitary time evolution on that larger Hilbert space.
And in fact you can show that the maximum size of the Hilbert space you need if you started with an n dimensional space is n cubed, which can be implemented by adding a single degree freedom with n square values or two degrees of freedom we use with n values.
There are many ways to do this.
But merely by dialing the Hilbert space formulation if needed, one can always assume that your stochastic matrix is unistochastic this was a term introduced by Robert Thompson in 1989.
That just means that the entries of your stochastic matrix are the squares of the entries of a unitary matrix you, and this is the same thing expressed in all of our various mathematical forms.
That last one is our dictionary.
And that means that state vectors, if you have one and density matrix more generally evolve according to this unitary time evolution operator.
Okay, so let's just stop for a second. It's important at this point to take a moment and just, and just talk about what's going on here a little more depth.
Let's talk about composite systems for a moment and I want to talk a little bit of an interactions, which will play a very important role in what lies ahead.
Let's talk about composite system AB, consisting of two subsystems as a Cartesian product configuration space, right that's the configuration space, or the dynamics of a composite system is more complicated.
If the two systems a and b are fully independent, totally independent systems with their own stochastic dynamics, then the composite systems to cast matrix will naturally factorize as a tensor product.
And this is where tensor products make their appearance.
What allows us to do is neatly identify when an interaction has occurred. If this factorization breaks down we can say up that must mean there was some sort of interaction director interact between the systems.
We can phrase this breakdown factorization also at the level of the composite systems time evolution operator theta AB.
Right. The breakdown of this, at least if this breaks down among all choices of theta AB is going to correspond to the breakdown of the factorization for the stochastic matrix.
However, even if the time evolution operator theta B for the composite system the composites that we've you know imagine we have this larger system, even if that time evolution operator refactorizes in some later time.
The stochastic matrix for the composite system.
Maybe it once factorized, then the interaction happened to stop factorizing. It will not generically return to factorizing at later times.
Even if, you know, I don't know the systems are separated or something like that if there's a notion of physical location in space, that sounds like a containment and we'll have more on that later.
Anyway, so the time evolution operators give a beautiful general and precise way to define what it means for interaction to occur we can, we can talk about interaction occurring as the breakdown of this factorization at the level of the time evolution operators theta.
Whereas with stochastic matrices is a little hard because if the systems ever interacted then you won't return to factorization.
Generically if we dilate a given system to get you know this larger Hilbert space where we have unistochastic dynamics, the resulting time evolution operator the unitary time evolution operator on that bigger dilated Hilbert space will not generically tensor factorize.
Physically what this is saying is that if we take a system with some stochastic dynamics and we regarded as belonging to a composite system, maybe a composite system evolving unistochastically with some unitary time evolution operator.
Well then generally we're going to find is that our subsystem is interacting with the other systems in that sense that you don't get factorization of the original systems time evolution operator with the rest of the system.
So naturally we'll call the system open in that case.
By contrast, if the system you started with if you are given some system that is already evolving unistochastically its stochastic matrix is already determined by some underlying unitary operator.
And you attempt to regard it as part of some bigger system. What you'll find provably is that the time evolution operator that bigger system will tensor factorize.
That is a system already of all the unistochastically cannot be interacting with other systems and this gives a nice way to define what we mean by a closed system.
It's a system of all the unistochastically for some choice of underlying time evolution operator you.
All this stuff is going to just spill out assuming you as a smooth function of tea as physicists will assume that the rest is now inevitable. You can define a generator a self a joint Hamiltonian in this way.
Here each bar is being introduced just if you care about units. If for whatever reason we'd always been measuring units of energy in the right one we never would have needed each bar.
If I go back to the death, the definition of side T at times T, then the, and I take this definition of the self a joint Hamiltonian, then what unrolls is the Schrodinger equation.
If I take the time derivative of the time evolving density matrix of the system I get the von Neumann equation.
The time derivatives of Heisenberg picture random variables I get the Heisenberg equation. You can compute expectation values you get the Aaron fest equation and I want to just point out, each of T, this generator of the time evolution is not generically to diagonal diagonal row of T density matrix
the generic times not generically diagonal Heisenberg picture random variables not generically diagonal.
And these brackets here are genuine commutators, they're not croissant brackets I'm not making some tortured correspondence to classical Louisville theory these are actual commutators showing up even though our system is some configuration and some configuration space.
That's surprising.
At this point you might be skeptical that we can empirically access more than just diagonal observables AFT. Now, if your goal is just to use a Hilbert space representation to study phenomenological examples maybe this is enough you're only interested in in random variables which have diagonal representations but if our goal is ultimately to try to get all of quantum
theory out, we're going to have to deal with non diagonal observables to which will come to that.
Now just think about, let's let's talk about another signature hallmark empirical feature of quantum theory interference let's come back to that question that interference I've talked about earlier.
So to start, we can use uniterity, we have closed system we have you know unitary evolution to define a relative time evolution operator, unitary time evolution operators right there invertible, they're agile and other inverses.
We can define u of t and you dagger at some other time t prime to define this new unitary operator u of t from t prime over the left. If I rearrange this definition I get the following composition law.
So we see the time evolution operators do compose right you. So the first over on the right u of t prime here. This is the time evolution opera that takes you from zero to see prime.
u of t takes you from zero to T. And then we have this unitary operator that takes us from t prime to T, we can describe the evolution either using the single operator on the left or the composite product of operators on the right.
Well, if you take that new unitary matrix we just defined and square its entries, you get a matrix that is genuinely stochastic in fact it's unistochastic.
Wait a second, doesn't this mean that the stochastic dynamics does compose well you can define this matrix, but what you'll find is that it does not correctly give you the composition behavior.
You'll find the overall stochastic matrix that evolves through zero to T is not expressible in terms of that matrix we just defined, and the stochastic matrix that takes you from zero to T prime.
In particular, if we compute the discrepancy, the difference between them, you get this expression right here, in terms of the Hilbert space ingredients that we just introduced earlier.
This is exactly the set of cross terms that describes interference, it's the exact same formula we use for interference but we see it's emerging here, just from a discrepancy between the systems true indivisible stochastic dynamics, and this sort of nearest approximate
divisible Markovian counterpart.
So interference is just showing up as this discrepancy between indivisible roughly speaking non Markovian dynamics they're not quite the same concept, and divisible or the Markovian approximation.
So notice we didn't need to invoke waves here.
I wasn't talking about single particle systems this just told some general whether you're talking about qubits and have a discrete configuration space or whatever.
In this perspective it was actually unfortunate accident of history that so many early examples of interference resembled wave interference in 3D wave functions after all don't usually even live in 3D space Schrodinger knew this in his 1926 paper introducing undulatory mechanics is undulatory theory wave mechanics.
He noted that wave functions live in configuration space. He was willing to he was committed he was willing to take that seriously but it's worth noting that we've always known they don't really live in physical space anyway.
In fact they're more abstract systems don't even have a continuous configuration spaces, you know this, we don't know that particles are the fundamental ingredients of reality could be fields it could be discrete things qubits who knows, and these systems, you know, qubits
particularly don't have continuous configuration spaces at all but they certainly do have interference.
Okay.
But from a practical standpoint we now have a framework, because the framework can accommodate non Markovian indivisible dynamics. We now have a way to provide a theoretical justification theoretical grounds for why the Markov approximation turns out to work so well and
phenomenological applications, rather than what most textbooks do is just assume a mark of approximation and just guess and check. We can provide like a maybe a better explanation for why that guess and checking has been working all this time.
So now, we're going to take our system to be a composite system. There's a subject system in an environment this is our a and our be now.
I suppose for simplicity that for each configuration I have the subject system, the environment has a corresponding configuration that I'm going to label by the configuration I have the subject system.
So the way you should read this is the environment has witnessed that the subject system is inside the configuration. Now you can generalize this you can allow for degeneracy, it just makes the math messier but there's nothing that that
changes. For simplicity I'm going to assume there's just one environment configuration for each of the configurations I have the subject system, although the environment could have been more configurations beyond that.
And I'm going to assume that whatever the stochastic matrix for the overall system is, it has somehow yielded a classical correlation.
So the joint probability at time t prime for the subject system to be in its i prime of configuration and the environment to be in its e prime of configuration is given by this simple formula here.
I'm going to assume that there's some probability distribution over the subject system to be in its configurations and and whichever it's in the environment with probability one has is going to be in the configuration corresponding to it and
probably easier otherwise.
I'm going to assume at least for a brief moment of time there's no interactions between the subject system and environment, because if there are then I won't be able to factorize their evolution.
And that's even true classically right if you have a system that is in the midst of interactions with something else, you can't talk about the dynamics of just the system by itself.
So at least for a brief moment in time I'm going to assume that the interactions are weak enough that we can ignore them.
Well, then if you just marginalized over the environment at later times t bigger than t prime notice, I'm not invoking partial traces or anything like that this is a classical joint probability so I'm just marginalized by marginalized over the joint probabilities at the later
time t at later yet later time t, then from a simple calculation what you find is you get the following expression over here on the right hand side, a linear relationship.
Right, this gives a linear relationship between the final standalone probabilities of our subject system and the standalone probabilities at that intermediate correlating correlating time t prime.
And this is that our stochastic dynamics does in fact divide at t prime, due to the correlating interaction with the environment there's a new division event at t prime playing the role of t equals zero it's the new T equals zero t prime.
And this is what a division event is it's it's a break in this stochastic dynamics it's the appearance of sudden appearance of divisibility at some time due to correlating interactions with external system like environment.
And this is going to be ubiquitous if you've got a big open system in contact with a strong contact with the environment. And, and, you know, basic ideas if the environment is checking in on the system over some time scale some characteristic time scale you're going to get these division
events repeatedly and this is going to produce the Markov approximation.
And this is going to be the same time t prime if you do compute the reduced density matrix of the subject system you'll find it's momentarily diagonal, just at t prime. And this of course is the coherent so we learn.
Number one is that the off diagonal entries in a generic density matrix, you know, forgetting about coherence or whatever the off diagonal entries more generally, the called coherences for state vectors they correspond to super positions of state vectors.
And this is the core defect of having non Markovian indivisible dynamics and decoherence itself is just what the routine leakage of correlations out of the environment looks like when you see it through the lens of the Hilbert space formulation.
And this explains why missing non Markovianity and indivisibility was such a hindrance, it really is the secret sauce, it is, it is what those off diagonal entries of your density matrices are and if you pretend that all this stochastic dynamics is Markovian you're going to miss that and you're
not going to be able to construct all the quantum theory.
So we're going to go back to this case of two systems interacting. And again just as a reminder even classically during an interaction systems have non factorizing dynamics, you know maybe there's a potential energy that doesn't cleanly factorize for the two systems.
So, in our case we're going to consider two subsystems again a and b. If they're not interacting with each other from the initial time t equals zero up to just before sometime t prime, then again the composite systems to cast a matrix will factorize little tensor factorize between two systems.
However, again, if an interaction takes place against gamma the stochastic matrix of the composite system encodes cumulative statistical information and that means that after time t prime, at least until another division event, the composite stochastic matrix will
factorize. This already looks like entanglement as I mentioned before. But notice if there's a division events, suppose that some later later time t double prime bigger than t prime, maybe the environment or some other big external system comes in and classically
correlates with at least one of the two systems, then you're going to get a division events at that t double prime. That means the overall stochastic matrix of the composite system is going to have a division event it is going to divide at the time t double prime, happening after the interaction
Well, if there are no further interactions after t prime, then that new, genuinely stochastic matrix gamma a b t from t double prime will tensor factorize between the two systems and we'll get a restoration of factorization between the two systems.
That is, the division events, decoherence causes a breakdown entanglement just as expected. Okay, now we're going to talk about the measurement process we're almost at the end.
And as a motivation for why why we're going to be able to act why we should be interested in non diagonal observables and how to access them. I'm going to start with introducing the notion of an emergence.
And again, in a moment I'm going to explain the terminology, it's, it's to be distinguished from vehicles and I'll say in a moment what I mean by that.
All right, so notice if we're given a random variable a for a system and if you want to think of the system as a particle a could be the position of the particle.
Well, if you think about a particle for a moment. Right. If you know exactly where a particle is, but the dynamics is stochastic, you don't know where it's going to be even an instant later, and that means, like velocity and momentum are not going to be sharply defined.
If you want to look for the nearest thing to something like a velocity momentum and a picture like that. Here's what you do. You take your random variable you go to the Heisenberg picture and you take its time derivative.
You do that use the product will you get this formula here for simplicity I'm assuming a has no explicit time dependence like a position would have no explicit time dependence, but you can put that in if you want to make things messier.
And then you can take a limit he goes to zero. And what you get at the end of this limit is a new operator a dot that is not generically diagonal it is self a joint, and it does not commute in general with the original random variable a.
You get a non commutative algebra, and we get non diagonal matrices showing up this new operator a dot mixes configurational information from a. It mixes it up with law like information from theta, the time illusion operator into this emergent amalgam and that's why I call it
that. This is to be contrasted with my random variables which I guess Bell might have called vehicles, be able from like ontologically to be. They are the way the system really is they reflect what's actually happening with the systems ontological configuration.
The tools are the sort of soup of mixture of of configurational and, and, and, and law like information is a mergeable combination which is what emergent combinations have I call them mergeables.
And again, an example of a is the particles position and a dot is its velocity then may die would be its momentum.
And these are actually useful. Yeah, for example, if I take.
If I want to compute the rate at which not a itself is changing with time which I can't because it's evolving stochastically but if I compute its expectation value, and then compute the rate at which that changes with time you can see that that this day HDT gives a very clean way to write down those those those formulas.
Okay, so this motivates talking about non diagonal objects so that we don't yet know what their interpretation is that, but that's going to be next.
So let's consider a general self a joint emergible. This could be a dot it could be something else for a subject system. This is a specially composition which we have because it's self a joint.
This is a new set of project projectors they form their own projection value measure but they will generically not be diagonal in the configuration basis. And at this point I don't have an interpretation for them at least this point the talk but we will in a moment.
It also is eigenvalues, but I'm not going to be for zooming any meaning for these eigenvalues at this point at this point they're just mathematical notions.
We have a measuring process. Now we need three systems, our subject system, and then if you want you could take the environment and break it up into a measuring device in the environment.
And I'm going to define explicitly what I mean by these systems I'm not going to take measuring devices to be axiomatic primitives I'm going to tell you to measure devices.
In particular I'm going to start by saying that a measuring device has to have at least enough configurations it's got to have these result configurations d of alpha a configuration corresponding to each possible value of alpha.
So what you have to do if you want to make a measurement is you have to set up a measurement process. Can we do this. It depends on whether we have the lab equipment and technology. Right, that's a question for an experimentalist can they set up a stochastic dynamics for the overall system of the right kind.
That's the second important ingredient that measuring device has to have the third is just that has to be a robust environment producing division events those are the three requirements for measuring device, but in particular there has to be some stochastic process that will carry out the measuring process.
And, you know, this isn't some bespoke Jerry rig Jerry Mandarin stochastic matrix.
In fact, you can base it on the kind of unitary operators that you looking up at textbooks. We model Von Neumann measurements all the time in, you know, in the textbooks. Here's an example of a very simple choice of unitary operative that will do the job this is something you'd find in a textbook.
And, but the interpretation here is different. I'm regarding this matrix, it's entries when you square them as giving you the entries in this stochastic matrix so that really, there's just this stochastic process for the whole system unfold.
Those are the eigen projectors of our original self a joint, a mergeable. And these are unitary operators that align the configurations of the measuring device. That's D, and the environment, respectively, to the measurement results.
Plug this system with its configuration, and this stochastic matrix into all of our formulas are probability formulas what comes out is exactly what you'd expect.
After you marginalize you find that the probability for the measuring device D to end up in its result configuration the alpha prime at t prime is given by the absolute value squared of something.
And that something is the subject systems when you function in the emergent was I can basis. And this is of course the textbook formal.
Moreover, if these are probabilities we can condition on them, and you can condition on them in the usual way, if you condition on the measuring devices results at t prime, and then ask, what is now the subject systems probability to be in its IF configuration.
What you find is is given by the formula we had before trace of projector times something like a density matrix, but this is a modified density matrix.
And the correct density matrix that gives you the answer this is something you calculate it's not a postulate, you just do the computation of what spits out is this formula here where this new density matrix is given by the eigen projector of the thing that was measured, which is the
outer product of its state vector with itself, its eigen vector with itself. And of course, this is just textbook wave function collapse. That is, if you want to say what the subject system is going to be doing.
So the density matrix for the subject system is going to be this collapsed density matrix. Okay, so we're at the end. There are some further points discussed in the paper, and we can also talk about them in the q amp a.
This approach attempts to avoid any radically speculative benefits like policies. I'm not proposing that there's some giant wave function evolving in some huge dimensional configuration space.
I'm not proposing that there are multiple universes. I'm trying to be as metaphysically humble as possible we have a system in a configuration space, and the dynamics is a slight generalization of the kinds of stochastic dynamics that we often encounter, instead of getting classical systems,
measuring devices here are not axiomatic primitives their ordinary systems.
They have configurations but all systems have configurations now, what makes a measuring devices what you'd expect if you were an experimentalist, you have to have enough configurations enough resolution, you have to have the right to be able to set up the right dynamics and you need a good environment to ensure division events.
And this resolves the measurement problem there's no special status for measuring devices.
We get a clear interpretation of the uncertainty principle from all of this and I can describe this right now in words.
I can remember if you have it like a system with a definite value for be able let's suppose it's a particle particle has a definite position.
You know what this is you know the position of the particle. Well, you don't know what its position is going to be in the infinitesimally later because the dynamics is stochastic. And that means we don't know what the measurement of something like a momentum is going to be.
And this shows that we measure the emergible corresponding to momentum. The, the analysis we just did in the measuring process shows that after that measurement concludes the subject system, and this is just a stochastic dynamics we're just letting it do its business.
It pushes the system into the midst of a new indivisible process, a new indivisible process represented by the systems density matrix being an icon projector of momentum.
The indivisible process is non-trivial has a non-trivial probability distribution now over the configuration space and now we won't know the particles initial, we don't have the particles position anymore.
But that that new indivisible process that the subject system has now entered is exactly right so that if you were to try to rerun the measuring process again on the same system a second time for the momentum, you get the same result on your measuring devices you've got before.
So we can see how you get the measure, the certain principles showing up in the story. You can carefully check all the various no-go theorems, and again we can talk more in the discussion Q&A about how this navigates no-go theorems but for example,
the measure specter is avoided because we're not claiming that all observables are variables. PBR is avoided in a way similar to how they're avoided in Bohmian mechanics, in particular, the measuring process is not a passive process of revealing pre-existing facts in general.
It does alter the system being measured, and we can talk about Bell and all that other stuff later.
In particular, connected to Bell's theorem, one can argue and we can discuss this later, that this approach is no more or less dynamically non-local than textbook quantum theory already is.
If you were to view the textbook quantum theory is already non-local, the cat's out of the bag and you accept Bell's 1975 generalization of his original 64 theorem, well then this is just as non-local.
If you dispute the premises that lead to Bell's stochastic generalization of his 1964 no-go theorem, and you dispute therefore that textbook quantum theory is non-local, you believe it's local.
This jumps through exactly the same hoops, and this is therefore local in the same sense.
This framework is a very interesting relationship with special relativity. It's compatible with special relativity, where you replace t equals zero to t equals t prime with Cauchy hyper surfaces.
This is an interesting new light on the role played by the complex numbers, and has a number of other arguable improvements over other interpretations of quantum theory, future directions.
So this approach gives new ways to think about maybe even generalize what we mean by dynamical symmetry and we can talk about that in the Q&A.
It'd be interesting to explore the ramifications of this approach on algebra first approaches, including the famous problem of unitary and unitary and equivalent representations of cistern algebras.
I mentioned this at the very beginning, and this is maybe one of the more exciting practical consequences. It would be interesting to see if one could develop new algorithms for fitting
Hilbert space representations of stochastic processes, maybe machine learning, maybe some, you know, cool trick for doing that, for simulating real world systems beyond the Markov approximation with potential applications throughout the sciences.
It would be very interesting to take this highly general framework and actually sit down and construct specific models for different kinds of quantum systems, particles and distinguishable particle systems can that matter qfts and so forth.
It's possible that with a clear picture now you could develop new intuition about the systems that we didn't have before.
It'd be very interesting to know if you could take this framework go back to those two axioms that we introduced and maybe imagine modifying them and seeing if you get a new theory that generalized quantum theory.
In fact, I have a suspicion which could be unfounded that this might be a necessary step toward a theory of quantum gravity in, in a gravitational theory in general relativity, the metric tensor is not given to you a priority.
It is a dynamical thing you're supposed to find. And if you're not given the metric tensor in advance, you don't know what time like in space like even mean, and that puts some pressure on those initial axioms.
If you're already in the Hilbert space picture, which I would argue bakes in those axioms, that may not be the right place to stand if you want to find the way to do quantum gravity you may have to go back to those earlier physical axioms, revisit them, adjust them to make them
more compatible with gr, and then proceed from there.
And in particular, I haven't yet and this just buck me by ignorance. I'm unaware of any fully general comprehensive credible probabilistic forget quantum mechanics probabilistic generalization of general relativity.
Arguably that would be an important step toward toward thinking about this. That's it.
All right, and I'm trying to get my chat window also open separately.
Okay.
Great. Okay, so the first hand I have is from Johannes hello Johannes, how are you.
Hi, Jacob. Thanks very much for this really fascinating talk, and very much and China covers a lot of ground.
So everything made total sense to me up until the point where you claim that you get the uncertainty relations out of your story.
Okay, very very interested how you get that because what you're doing is giving a very general framework of transition probabilities.
And that applies to classical mechanics as well. So I think everything you said is totally compatible, you can give classical and quantum transition problems that you can give a similar account.
And I felt like you're giving you are doing something like a very general stochastic hidden variable theory. Yes, that's my impression.
You're exactly right. Yes, so it is stochastic hidden very highly general stochastic. Where do you get, where do you get the non commutativity of your observables.
And then, I think there's another problem with defining. So if a is the position observable let's say, then you said that the momentum is just the time derivative of that is the time derivative of the Heisenberg picture version of that.
Okay, maybe that's different, because that usually doesn't work if you have the bomean case right, that's not what you get up to.
Let me actually, this is a great opportunity to talk a little bit about how this differs from bomean mechanics so this I'm really glad you brought this up.
And let me just also add that, like I said the secret sauce here is not making the Markovian assumption.
If you reimpose the Markov approximation which you can do in this framework because Markovian evolution is a special case, you can do it by taking your time evolution operator and sticking in lots of projection matrices.
And if you do that what happens is the coherences disappear, things diagonalize, you don't get non commutativity. So, like, it does look almost like what we're doing is classical.
I did a wonderful conversation a while ago with Emily, who I think is here, who, you know, pointed out, I think, you know, incredibly astutely that there is something a little non classical about fundamentally indivisible or non Markovian dynamics and I'm willing to exceed to that like,
that that may be a little bit non classical at the dynamical level, and a lot of really non intuitive stuff happens then a lot of our intuition around probabilistic dynamics comes from the Markovian assumption if you drop it.
Things get very unintuitive. And so it's not such a surprise that we didn't realize that there was this potential connection to quantum mechanics. Now let me talk a little bit about the connection to bomean mechanics.
So, bome mechanics is also defined on the systems configuration space, but bome mechanics proposes a deterministic law through the guiding equation the guiding equation tells you directly what the particles velocity is.
And to think of a law that tells you what the velocity is, as a law that tells you, at each step in time, what the particles position is going to be at the next infinitesimal step.
So this is a continuum limit of a deterministic permutation type matrix dynamics. So it's a spec it's a degenerate case of stochastic dynamics where your stochastic matrix trivializes you just get zeros and ones and it's a permutation matrix, and you take an appropriate
continuum limit. And that's that's, that's what happens in bomean mechanics. What's a little strange in the bomean approach is that that, you know, deterministic version of the stochastic matrix that deterministic permutation matrix is not what you get your wave function
the way you do here, you get the wave function out of imposing the Schrodinger equation separately and letting the wave function evolve according to Schrodinger equation with the standard unitary time evolution operator.
And then you take that time evolving wave function, and you stick it into the guiding equation and it defines your deterministic dynamics.
And then to get probabilities that you have to make an initial equilibration hypothesis and there's all this other stuff you have to do.
The approach is just much simpler as fewer ingredients, you don't need an initial equilibration hypothesis, the probabilities are just built right into the laws.
However, you do give up determinism. On the other hand, it's not entirely clear how to generalize the bomean approach beyond certain, you know, relatively simple systems and retain that determinism.
There are many, very many highly compelling ways to do that. So you do lose a little you lose the determinism but you gain a high degree of generality, and it's somewhat axiomatically simpler.
And then you're not trying to figure out whether wave functions are ontological or nomological, you know, in this case that's that's sort of beside the point. So I don't think it gives me a better pictures what's going on here and the connection to bomean mechanics.
And what about the uncertainty relations.
Right, so the claiming that that comes out from the non Markovianity, then right.
What do you get for the commentator relation to get a value for you get the standard commutator relation if the dynamics you're, you know, if you have like a particle system and the unitary operator that you're using is the standard p squared over two m plus to be that you know this this framework
is mathematically equivalent to the Hilbert space framework you're going to get the same, you know, commutator where what's going on here that's different from what you'd usually think of in a classical system is.
Number one, the non Markovianity is important. Right, that gives you these non diagonal emergibles you got a non diagonal momentum operator. But the second thing is, if you go through that measuring process that I went through, at the end of measuring the momentum
you know when you condition on the measurement results, the correct density matrix now to use for making those conditional probabilistic statements about your, your particle system is is a density matrix that's now in the midst of this new indivisible process defined by the
eigen projector of say the momentum.
That's this weird non classical thing going on the measurement was not a passive operation. It took the system from being in a configuration where you knew its position exactly.
It put it into a new indivisible stochastic process represented by that I can project the density matrix is now this non diagonal I can project that indivisible process is exactly right.
So if you measure momentum again you get the same result, but now there's a non trivial probability distribution over the configuration space of the system, and now you will have uncertainty over where the system is.
And this generalizes from particles and even generalizes past one of the observables being a vehicle, you get the same kind of reasoning for two emergibles also.
What you can't do is get an uncertainty principle between two vehicles, because they're diagonal and they can be with each other. Okay, so one of them, at least one of them has to be the mergeable but that makes sense because the uncertainty relations do depend on the two observables not commuting.
Thank you. I do have more questions but I guess I'll let all the people ask the question stepping again later in.
Thank you. The next hand I have is from Steven Steven please go ahead.
Hey, Jacob. Thanks, thanks for the talk that was very interesting and I think I need to take a little more time to digest some of it to fully understand.
I actually was going to ask about the relation of me and mechanics but since you answered that I'm going to ask the question that I that is always on my mind when someone talks about a new approach to quantum mechanics.
I'm still justified in asking this specifically because you made the analogy to Copernican versus Ptolemaic celestial mechanics.
So, it's certainly true that for you know Copernicus, the Caplarian Copernican model and the Ptolemaic model were directly, you know, indistinguishable from the point of view of astronomical observations from Earth.
However, could certainly, you know, it was certainly within Kepler's or Copernicus's capabilities to perform a thought experiment where there's an observer in space.
And clearly, you know, the two models are perfectly very, very different to that observer. So, my, you know, my question is, do you, do you know, do you see is, or do you claim that there's, there's
some empirical way of distinguishing this from other, you know, formulations of quantum mechanics that don't invoke an objective collapse of the wave function.
If not, I suspect the answer is no.
The answer is, is the idea that this is, you know, the hope is that there's some more fundamental theory, for instance, you mentioned this leading to, you know, ideas in quantum gravity, where this approach would yield, would either yield a theory where other
approaches don't or would yield an empirically different theory.
Great question. Let me just say right off the bat. This is a no collapse interpretation, meaning that collapse is not imposed as a, you know, as an axiom.
That paper I said it before, the Glickadami paper that was published in 2020.
I didn't just cite it as just a random example of a paper that explores non Markovianity and quantum theory. I cited it because the paper shows something that I think we all suspect would have suspected or maybe some of us already
know that wave function collapse if you impose it strict literal textbook wave function collapse has empirical consequences.
Right, it actually produces somewhat different empirical results from assuming that we don't have exact collapse. Now, if the systems you're dealing with are very large macroscopic systems, we would expect those discrepancies to be, you know, extremely
small, very, very, very hard to measure. And then you're in the regime of, you know, Kepler imagining that he's floating in outer space, right, thought experiments that are not physically achievable.
But maybe one day we can achieve them and we are, we have much better control over small quantum systems and maybe we're going to be able to probe regimes in which these distinctions could arise.
Which is empirically different from textbook quantum theory in being a no collapse interpretation, however, that means that it's going to be probably empirically indistinguishable from other no collapse interpretations like a ready and quantum mechanics, bowling
mechanics, but it would make it empirically different from approaches like grw that impose something like an explicit collapse posture.
And this question of empirical distinction is, is subtle. I mean, yes, it does differ from textbook quantum theory in that it's a no collapse approach that doesn't treat collapse like a fundamental physical process.
But beyond that, I'm not 100% sure, except to your later point, this point you made referring back to how maybe we need to go back to these lower level axioms and generalize them to get maybe a theory that we need to talk about quantum gravity.
So I think we need to have some conversations with some of my colleagues in, in, you know, work in quantum gravity on this. And actually there's some agreement that this might actually be a promising way to think about things.
So if ultimately go back to those axioms and we modify them. Maybe we have to change that's dynamical axiom and we can't talk about stochastic conditional probabilities from one slice of time to another we have to do something different.
We might get a different theory and that could lead to different empirical predictions.
So to your point that, that no collapse theories without collapse are empirically distinct from theories with objective collapse that's, this is something that for you know I am an experimental physicist.
This is something that for like 15 years, I've been trying to correct, you know, my colleagues on where, you know, who don't appreciate that point.
And look at dummy paper what they do is they imagine a sequence of measurement like dynamics conducted by mesoscopic devices that are sealed up in the box. And these devices they do like end measurements and then at the end of big macroscopic measuring device does a final
correction at the end. And they compare this what would have happened if each individual and each of the individual each of the individual measuring interactions had been a collapse.
And what you find is you get to distinct empirical answers at the end. Only the distinction becomes extremely small if the mesoscopic devices are big and strong contact with the environment.
Yeah, this would in principle lead to empirical differences but again I don't think it would lead to a clear empirical difference between this and other collapse approaches.
Thanks.
Thank you very much, Eric. Nice to see you.
Jacob hello.
That was a very rich talk that and really stimulating and provocative. So I have a lot of questions but I will just both technical and conceptual but I'll maybe I'll stick to my most, I'll start with my most pressing one and add myself to the end of the queue if there's time.
So you be the setup depends on starting out with what you're calling a configuration space. Yes.
This seems to me and you then develop what seems to go very ingenious attempt to solve the measurement problem.
Based on this dictionary you build with the Hilbert space formalism, but I worry that you're kind of building in the solution to start, because the assume assuming this classical configuration space seems to be assuming that there's always a kind of
privileged way to carve the world up into subsystems.
And that that to me is something that seems to be exactly up for grabs and say strongly entangled systems and strongly entangled quantum systems. So I wonder if you can just say something about that.
So that's a great question not only I think it's a good question but it's a question I've actually talked about with some of my other colleagues before. Again, if I'm not incorrect I think Emily and I talked about something quite similar to this.
Yeah, it's an excellent, it's an excellent question.
And, you know, because the conclusion of this approach is a resolution of the measurement problem somewhere in the premises must have been inherent the thing that yet, and I'm not, I'm not going to deny that right I'm not going to make some argument that you can deductively get a conclusion
that's not inherent in the premises. The question is whether the premises are more reasonable or more credible or simpler or in some sense, more physically transparent than the premises we've been using before and I would argue that they are.
We were talking before about quantum gravity if you start with a Hilbert space picture. It's not clear what new mathematical framework, you would need, you know, you need maybe something other than Hilbert space picture it's not clear.
If you have more physical axioms maybe this gives you different knobs to turn different things to think about for finding a generalization so.
Yeah, so I'm totally agree with you inherent the premises, you know are the things that do lead to the solution to the measurement problem in this approach.
The question about how to pick a configuration space is extremely interesting.
One way to think about or one way that I like to think about the problem of trying to find it a good satisfaction interpretation of quantum theory even separate from the measurement problem is that it's going back to that analogy to make astronomy problem again.
I think astronomer could say, look, in order to get like empirical adequacy in a Ptolemaic model you have to have so many epicycles.
This is an incredibly ornate extremely complicated model, and then Copernicus comes along and Copernicus claims that this that you that he can recreate this whole thing with a really simple picture orbs moving along very simple curves.
I think a totally reasonable reaction by tall make astronomer would be.
This is impossible the system is so over constrained and over determined you're never going to find a simple solution that jumps through all of the hoops right.
You know there was a huge over seemingly over determination problem.
And this is where I'm not a historian of science we do have some historians of science here perhaps someone can fill me in on this but my from what I remember Kepler tried like 40 different curves, looking at all of the you know astronomical figures he got from
Tycho Bray right and, and finally it was ellipses with the sun at one focus that that worked and it's remarkable that he could be that precise because so many planetary orbits are so close to being circular but.
He managed to do it he managed to find a solution and, and now we would argue it's the ideal solution to that, what we thought was an over generation problem turns out not to have been.
So, in a sense the Hilbert space axioms right there's so many of them there's the Hilbert space, you know, you propose the Hilbert space you've got a system with a density matrix, you have to, you know, propose self a joint operators and Eigen values and all the stuff that we do measurement collapse and, you know, it's this very
complicated framework.
And, and you know the way I have seen the problem with trying to interpret quantum theory is trying to do it with Kepler did find a simple picture that somehow finds it's it solves what seems like a incredibly difficult over determination problem.
And hopefully this is one such solution but you know probably not the unique solution. If you take this approach, however, then you're over determination problem becomes an under determination problem.
Because if you're simply handed a quantum system, you do experiments, you collect empirical data, and you work backward and figure out what kind of Hilbert space will kind of Hamiltonian are going to do the job.
Now your question is which basis is is going to be the basis for the configurations what's the configuration basis. This is now an under determination problem.
And, you know, let me just say that, historically, we've always had under determination problems right the other determination of theory by data is an old problem and if we can go from an over determination work or seemingly over determination
where we don't know that there's any kind of picture back to the, the old time of under determination problems I regard that as, as incremental progress at least.
But, but now I want to make an analogy between this problem and what one finds in classical physics, because I would argue there's actually a very similar under determination problem in Hamiltonian mechanics, classical Hamiltonian mechanics and you and I talked a little
about this by email.
So this this will hope this will probably be just a reminder for you but maybe new for everybody else in classical Hamiltonian mechanics if someone just says, I have a, here's a Hamilton, each, it's a function of a bunch of cues and a bunch of piece.
You could say, Well, all right, I can write down the canonical equations of motion, I can write down Poisson brackets I can start calculating things. And maybe someone is even nice enough to say certain complicated combinations of these ingredients, instrumentally correspond to certain
measurement results. They just tell you certain combinations are going to be certain measurement results. You go okay that's that's really that's very interesting and then someone says okay.
So, what is this system exactly. What is the configuration space of this system.
Right, I mean, you could do canonical transformations very general canonical transformations.
They can mix up the cues and peas they can even mix time in. And in fact, that unitary local and time v of t unitary transformation introduced wait the beginning is not the same thing as canonical transformations, but it does play a metaphorically
analogous role. Just as you can do these local and time unitaries on all the fibers of your, of your, you know, of your fiber fiber bundle over spacing, you can do canonical transformations and different canonical transformations give you different cues.
And the different cues might seemingly label very different looking configuration spaces.
And so you have again a massive under determination problem.
Now, I'm not claiming I have a solution to that problem I'm only claiming the quantum problem is basically about as bad.
If you want to claim that there is a preferred canonical frame in the Hamiltonian framework.
If you notice the Hamiltonian becomes much simpler in a certain canonical frame, and it looks like it's got a nice notion of locality and various other nice properties.
You might propose, okay, my working hypothesis is going to be that I'm going to pick that canonical frame to define my configuration space and again you might have the same situation and quantum theory, but the truth is you could never know for sure.
So, I guess so I just want to add that one of the reasons I really really like this work is because I think it's always a great thing when you have a new formulation of a physical theory, it gives you new ways to think about old problems.
Even if you aren't able to address what you're calling the under determination problem. I think what you, what you've done so far is fantastic.
That's incredibly nice of you to say, I think at the very least it provides, you know, in addition to the pathological formulation and the, the, the, you know, vigner,
that's a new mo orial products, you know, space face space approach and quasi probability approach. You know, it's like a distinct new way to represent systems which, you know, exactly you said even if all it does is provide new forms of intuition or new ways to calculate
things I think could be good, that's always useful. Yeah. Thank you. Yeah.
just to say by the way that those slides and interactions
are very much inspired by your work
on thinking about interactions and classical theories.
I agree with you that thinking serious
of an interactions is really very important
for making progress on this.
Thank you.
Lev, you're next.
Thank you, Jacob.
Yeah, you know, I believe in many worlds
and I don't like it.
I would like a picture with one world.
I don't want to be in many places,
but I kind of failed to see how I can see coherent picture
with one world.
And the main thing, I cannot see coherent picture
with electron being in one place.
So use, I want to, and I think you repeated many times
that in the end of the day, there is this configuration space
and there is, you talk about particle
and there is a position.
So in every, if I have just one particle,
it's always, apparently your picture has a position.
So you're considered to sleep experiment,
it's complicated, but similar one, it's much center.
So what is a picture?
I start with a particle, it entered.
Then in the middle, if I understand you correctly,
it's in one of the arms.
And if it's a one of the arms,
how I can get this interference,
I have no idea how it knows that the other arm is closed
and not closed, because there is a locality interaction
in 3D.
And here we have no one particle, even no entanglement.
I don't ask you to do a GHD, which I have to understand.
Just can you have kind of a intuitive picture
in your language of bug-centered and performative?
If the particle is only in one arm
and we have, we know the physics, we understand.
Now, now it's kind of a beam splitter
and somehow how it knows that the other arm
is closed and not closed.
This is why it's matter.
If it's in one arm,
they do have kind of picture without the mathematics,
without, you know, I want to fill it with waves.
Everything is, I understand it.
I say everything are waves, and then I think interference.
And everything, there are some kind of tangled waves
and I can understand GHD.
But when you say no, it's not waves,
it's a particular position of the state all the time.
I have no intuition.
Can you help me?
So, Lev, again, it's a great question
and really great to see you.
And I assumed when I saw you visiting here
that we'd be talking about the Machsender interferometer.
So the answer I'm going to give you
is going to be potentially a little disappointed.
So, you know, ultimately the predictions made by this model
manifest the agree with the predictions made by quantum theory,
but this approach uses these indivisible non-Marcovian maps,
which as I showed, do produce the same exact, you know,
predictions as you get from waves and interference.
But I also noted that indivisible non-Marcovian dynamics
is extremely non-intuitive.
So you've asked me for like an intuitive picture.
How is it that this is all happening?
And the answer is it's happening due to this indivisible law
and indivisible stochastic laws are really confusing.
They produce clearly very confusing results
that are not intuitive.
I don't have a good intuitive picture for you.
What I'll just say though is, you know,
the question of what one regards as intuitive or unintuitive
is very much a matter of preference.
I'm not going to argue that the Everettian picture
is incorrect by any stretch.
You know, the Everettian picture
is another approach to quantum theory.
The Everettian picture makes some things more intuitive
and it makes other things, at least for some of us
mere mortals, much, much less intuitive.
So the question is where are you going to put
the unintuitive stuff, right?
On the one hand, here, a lot of the sort of strange,
somewhat unintuitive behavior is baked
into these strange, indivisible stochastic laws.
Or you could take that unintuitive behavior
and propose that there are large numbers
or infinitely many decoherent universes
and then you have to make sense of
what does probability mean in that kind of a universe
and that's a difficult problem.
And ultimately, I think it's a matter of preference
which one finds preferable.
I don't have a knockdown argument.
But in the end of the previous answer,
you mentioned the past integral and other things
which for me are not interpretation in any way.
There are a few people who believe so,
but for me and for a majority,
I think that as far as I know, it's a mathematical tree.
So then it looks like what you propose,
it's another mathematical tree, which works,
but it doesn't tell what is really there.
You could be correct.
Like I was just mentioning with Eric
and like Eric pointed out,
you know, even if all this ends up being
as a new mathematical representation,
that's still useful.
But where this differs from the pathological representation
is it does lend itself to a picture
where you have systems on configuration spaces,
but the dynamics is really non-intuitive
and maybe very difficult for us to visualize.
And I'm, that's okay with me.
It might not be okay with everybody.
Thank you.
Yeah.
Barry, hello.
Yeah.
Hi, Jacob.
Unfortunately, there's noise outside my apartment.
If it's too loud, I'll spout out.
But if it's okay.
I'm not detecting any noise at all.
It's from outside on our side.
If it's okay, here's what I had in mind.
First of all, I love,
it's always hard to follow Lev question.
I was wondering about this,
but it was connected to Lev's question.
How exactly are you thinking about laws
and probability in your account?
And who is Ptolemy and who is Copernicus
comparing your account to let's say
Bohmian particle mechanics?
Bohmian particle mechanics gives a pretty clear picture.
If you adopt the, well, any of what is the formulations,
you can have a good understanding
what probability means in those pictures.
Are you thinking of the laws as an additional elements
over and above the sequence of configurations
so that they are, it's a,
so what's called a non-humian account?
And how are you thinking about probabilities
that certainly not frequencies or ordinary propensities?
Yeah, so let me just say that as I was developing this approach,
I was intentionally trying to remain even-handed and agnostic
about one's metaphysical orientation toward laws.
I believe that there is a principled way to look at this
from a humian attitude toward laws.
There's some, if you want to resurrect the notion
of a humian mosaic, there's some human mosaic.
The universe is some sequence of configurations,
a trajectory in some huge configuration space.
We don't know what that trajectory is,
but what we physically embodied,
epistemically limited human beings
have somehow been able to develop
is a framework with a set of descriptive,
summarizing best system summary type laws
that let us make probabilistic predictions
about what we're going to see.
And in some cases, those probabilistic predictions
can become highly deterministic
in certain limiting, classical regime type situations.
One could also be a primitivist, a non-human about laws.
One could say that the laws are fundamental pieces
of the architecture of the universe
and that they generate in some sense, the humian mosaic.
And one could take, I find it a little bit challenging
to be a dynamical productivist about laws
in a relativistic universe,
but here's perfectly fine.
I don't know if Eddie's is still here, but Eddie's here.
One could take a primitivist view
that's like minimal primitivism
or Emily also has a similar formulation
when we think about laws acting all at once
in the entire tapestry or human mosaic
and cause it to have a particular trajectory there.
So I think one could take either view.
I'm not attached to either of them.
I'm also not committing myself to one view
on the meaning of probability.
I think it's possible to take different views
on what the probabilities here are doing,
whether they're probabilities in the sense
that we have many similar versions
of whatever system we're studying
and we try to give them some kind of frequentist gloss
or whether we regard them as credence type.
I haven't put in, at least from my point of view,
any strict condition on what I mean
by probabilities or laws here.
In fact, I view the flexibility here as a virtue,
although it might be nice to have a physical theory
that finally gives us an answer as to the right way
to think about laws and the right way to put probabilities.
I hate to disappoint you,
but I regard it as a feature, not a bug,
that this is pretty flexible
and you can serve whichever view you want.
I completely agree with that.
That was what was really with my thinking behind my question,
despite what you may have thought.
Okay, yeah.
But I do, well, one thing,
it does seem to rule out Maudlin's kind of approach
because it seems to be essentially not stochastic
in your sense.
Right, right.
Okay, but I wasn't looking at it to settle an issue
about the metaphysics of laws,
but I was very puzzled about
how you wanted to think about probability in your account.
And also to make the point back to your analogy
between Copernicus and Ptolemy,
which was very striking,
that when one gets used to a Bohmian picture,
one sees it kind of through the eyes of Copernicus
or a Kepler or Newton.
And this looks like it's a kind of,
what you've done is to get rid of a bit of ontology,
whatever though, quantum state represents
and replace it by a more complicated kind of a law.
Yep, that's a fair characterization.
What I would just say is that,
obviously among the different interpretive approaches
to quantum theory,
the one that shares the most with is the Bohmian approach.
Right, it has a lot in common with it.
This is a kind of hidden variables approach.
And what I'll just say is,
when I first encountered Bohmian mechanics,
it was just incredibly elegant, incredibly beautiful.
I was coming out of,
Hamilton Jacobi theory
and it just seemed like this incredibly beautiful.
It wasn't in Bohm's paper, I'm sure.
Right, I mean, I don't know that I encountered it originally
in Bohm's original papers.
Eventually I got to his papers
and then to De Broglie's early, whatever.
But so, I mean, I do think it's a little more
axiomatically complicated,
although you get some benefit from that.
You get determinism back into the story again.
And you get, as you correctly argue,
a pretty nice way to think about probabilities.
If it generalized to other kinds of systems,
I would be really excited about that.
But I've been around a long time
and it does seem that the Bohmian approach
is helping itself to certain very special features of systems
of finitely many non-relativistic particles
that we just don't seem to have for other systems.
So I'll tell you is,
I'm not trying to rule out other interpretive approaches.
If someone comes along and gives an approach
that's just as simple as non-relativistic Bohmian mechanics
that applies more generally to systems
that may have continuous,
may have discrete configuration space
as a nice general framework
that's roughly as simple and as elegant,
I will be very impressed by that.
But I've never seen it.
And in the meantime,
if I'm not gonna sit around waiting,
we'll try to proceed in another way.
I would argue that this is somewhat more axiomatically simple.
As you pointed out, it has less ontology.
The state vector is not proposed as a form of ontology.
Although I will note that in some Bohmian approaches,
the state vector is not regarded as ontological either.
But the axioms are simpler.
And the downside is that you don't have determinism anymore.
At least you don't have predictive theoretical determinism.
We don't have a model that we can use
to make deterministic statements
about what's going to happen.
Yeah, thanks.
That's very, very tough and clarifying.
Thank you.
Thank you.
All right, John, you're next.
John, can you hear me?
John, we're not getting any audio.
Yeah, John, we're still getting any video.
I'm so sorry about this.
In the interest of time,
I think we're gonna have to put you to the end of the queue.
And if you can figure out your audio issue,
we can bring you back.
I'm so sorry.
And that's great.
We can bring you back.
I'm so sorry.
And I might suggest if you can't,
just post your question in the chat
if worst comes to worst.
Yeah, I see in the chat you mentioned you'll come back.
Thanks again.
All right, Chip, hello, it's good to see you.
Hi, good to see you too.
Sorry I've been so busy lately,
but there's been a lot going on.
No worries at all.
I have a question along the lines of Leves.
Just as he was wondering,
how does in your system,
and how does the particle know
that the other wing of the interference,
the moccasin and interferometer is open,
I was thinking about the same thing
in the double slit experiment.
You started with the double slit experiment
and was saying that it was the Markovian assumption
that leads to problems.
So I'm curious in your system,
how does the non-Markovianity
give you the right result in the double slit experiment?
And really I'm trying to track like to what degree
is it the non-Markovian nature of the dynamics
that allows you to get the right behavior?
And to what extent is it just like the complexity
of the kind of stochastic dynamics you have
that gets you the right behavior
after passing through the slits?
So I'm thinking like at the moment
when the particle is going through the top slit
or the bottom slit,
how is it that its future dynamics are determined by
whether the other slit is open?
And it seems to me like maybe what's going on
is that the stochastic dynamics are so complicated
that a lot of the mess of the wave function
has been absorbed into the dynamics.
And so it's kind of in the dynamical law there
that the other slit is open
and that there's like what you would have thought of
as some of the wave function passing through that slit.
Is that right or how should I understand
what's going on just after passing through the slits?
So this is a great question.
As you noted, when you drop the divisibility assumption
you get the interference pattern.
And in fact, you can, in that discussion
of correlating environments and division events,
if you have a bit or a particle or a tag
or something like that that is able to detect
which whole the particle went through the environment
comes in, the interference pattern goes away.
So all this stuff just comes out from the mathematics.
The question about the interpretation
of what the laws are actually doing is,
and I'm sorry to punt again,
but I don't have a good intuition for it.
Maybe at some point I will,
but all of our intuition is really based
on certain very simple pictures
of how probabilistic laws work.
Here we're considering a more general kind
of probabilistic law.
I don't have a good intuition for how they work.
And in fact, if you look at textbooks on stochastic,
this was one of the most surprising things about this.
I spent most of my time working on this project
with my jaw wide open.
I couldn't believe that things I thought,
I couldn't believe the things I was seeing.
The first thing I did when I thought,
well, it looks like we get distinctly
quantum mechanical behavior
by dropping the Markovian assumption.
Well, let me go back and look at the textbooks
and see what they have to say about non-Markovian dynamics.
And they basically say nothing.
If you look at the textbooks,
they just immediately assume the Markov approximation
or they work with Poisson processes or something like that.
And so I started looking at the research literature.
I'm like, surely someone has systematically examined
non-Markovian processes and it hadn't been done.
As far as I can tell,
there has never been a good general framework
for stochastic processes outside the Markovian approximation,
let alone outside the divisibility approximation,
which is similar, but not quite the same thing.
So we've been missing out on like a century
of building intuition
for how indivisible non-Markovian processes work.
The first step is to at least write down
a general framework for them, right?
Think of it this way.
We had Newtonian mechanics, we had these laws.
And then we reformulated Newtonian mechanics
in all these different ways,
analytical mechanics, Lagrangians and Hamiltonians
and all this stuff.
Once you have analytical mechanics,
there's certain things that you can now do
that would have been very hard to do directly
from the Newtonian formulation, right?
If you're trying to guess new laws of physics,
if you have some new system
and you don't already know what its laws of physics are,
if you have a Lagrangian,
if you have an understanding of Lagrangian mechanics
or Hamiltonian mechanics,
you have all these knobs for constructing
in a systematic careful way new kinds of dynamics, right?
Again, this is a history point,
but my recollection is that when Hilbert got
the Einstein-Hilbert action like a month
before Einstein wrote down the field equations,
he did it by constructing a Lagrangian, right?
It gave this powerful way to predict new laws of physics
that would have been hard to do otherwise.
What this framework is essentially doing
is providing an analytical mechanics
for these highly generic non-Marcovian stochastic dynamics.
And interestingly, we had it all the whole time, right?
It's like discovering,
oh, the mystery person the whole time was, you know,
but because we haven't thought of it that way,
we've missed out on like a whole century
of taking advantage of this to develop a kind of intuition
for indivisible stochastic,
highly generic stochastic mechanics
that we have developed for classical mechanics,
deterministic dynamics.
So I guess my answer would be,
maybe we need to wait 50 years working with these systems,
thinking about them in their stochastic realization,
seeing how they work,
and eventually people will just find it so intuitive
that there'll be explanations that I can't describe.
Let me put it in a slightly different way.
So when I talk to, you know,
colleagues here in the physics department here at Harvard
about quantum mechanics,
they talk about state vectors and wave functions
with all this intuition.
They talk about just feeling
how a wave function should behave in different situations.
And if you tried to, you know,
present some of the intuitive explanations
that they talk about to someone who, you know,
a highly capable, very highly trained physicist
from before the quantum era,
they would go, that's not an explanation,
that makes no sense, right?
And when you talk to people who are Everettians,
and again, perfectly, you know, alternative approach,
but you know, some of the explanations,
they say, oh, it makes sense in the Everettian approach.
I look at it and I'm just like,
that doesn't make any sense to me.
But people do develop intuitions
from working in a framework,
and then it just feels intuitive to them.
I know that's kind of a disappointing answer,
but I'm arguing we've just opened the door
on a whole new way, a whole new set of kinds of dynamics,
and we need time to understand
what have they worked with them intuitively.
Does that, it's not very satisfying,
but that's the best I can say.
I appreciate it, thanks Jacob.
And yeah, I hope it doesn't take 50 years to get it.
I hope not.
A more concrete story for this kind of experiment,
but yeah, I appreciate it, thanks.
Robert, you're next.
Thanks, can you hear me?
I can hear you quite well.
Okay, so, well, thanks for a very interesting
and stimulating talk.
And so more than questions,
I want to make a few comments.
And so you haven't emphasized this very much in your talk,
but I would like to emphasize,
let's say different perspective on what you're doing
as far as I can see it,
that I hope could enrich maybe the discussion
and taking this into account.
So for quite some time,
we've understood that we can describe
classical and quantum systems in a unified way
by talking about completely positive maps
and state spaces in a more abstract way.
And so of course, yeah,
so that's been known since the 1970s or so
and more since the 2000s,
this has been pursued particularly
in the foundation of quantum theory
in terms of generalized probabilistic theories and so on.
So I would like to emphasize a bit that perspective
on what you were presenting
and seeing what you and others think
about this kind of point of view.
So basically from that point of view,
as far as I can see what you're doing
is you're taking your classical space space,
you're embedding it in a,
sorry, your classical state space
and you're embedding it in the quantum space,
quantum state space.
And then of course you have a description
of evolution and measurements and so on
that applies in that context
and that you cannot specialize to the quantum version.
And then you can think of going to the Hilbert space
and evolution operators of a quantum theory
as a kind of square root, right?
Because the state space of the quantum theory
you can think of this space of self-adjoint operators
and Hilbert space basically as a tensor product
of the Hilbert space with its own dual.
So your step of going from the gamma to the theta,
matrices is this kind of square root really.
And then it's very natural
that you can express everything in this language
of quantum theory and Hilbert space.
So yes, so I think it could be worthwhile
to develop this point of view
on what you're doing further
and maybe that can add more to the story here.
And so let me make a second remark
and which is about the intriguing part of your talk.
So yeah, and in fact, it's a bit related
to previous questions and particular of Charles
about the divisibility.
So I also found this very remarkable when you were saying,
okay, so if we drop this Markovian
and the visibility conditions,
then we can model processes
that are proper to quantum theory, say, in this framework.
And so this, okay, so it might sound a bit crazy
what I'm going to say, but so the way I was immediately
thinking about this is in terms of temporal and non-locality
because so what means non-divisibility
is really that you're dropping the condition
and that you have in classical physics
of well-post-initial data and an evolution
that you can describe in time,
in terms of ordinary differential equations of finite order.
And so as far as I understand it,
this dropping the divisibility condition
really corresponds to dropping that.
And one way to think about it is as a locality condition
not in space, but in time.
So now if you think of
trying to describe a quantum theory
in a framework that looks, let's say as classic as possible.
So for example, if we think of hidden variables,
theories and approaches,
then while we know that for this to work,
they have to be non-local in space.
So now the question I would like to pose
and which is suggested by a lot of things that you are saying
is can we restore locality in space
but drop locality in time?
And so some of the things that you were saying
seem to indicate that that's exactly what's happening.
So thinking of dropping the divisibility condition
as dropping locality in time
and that's being able to use this classical looking framework
to describe quantum processes.
So I just want to throw this out.
Let me quickly respond to a couple of these.
There were a lot of important points there.
Let me see if I can respond to them quickly
and we are running a little short on time.
So let me quickly first just say that
I spent a very long time marinating
in the Koopman von Neumann-Süderschen
formulation that lets you work with classical theories
and quantum theories in a somewhat more similar way.
I've also spent a lot of time thinking about
generalized probability theories.
This is distinct from the Koopman von Neumann-Süderschen approach.
It's also distinct from generalized probability theories
which are generally presented as instrumentalist tool sets
where you treat measurements by agents as primitives.
So it does differ from those approaches.
This is not to say that there's not huge value in those approaches
but if you're looking for a non-instrumentalist approach
that's what we're trying to do here.
You also have this question about the question of the dynamics
and questions about locality in space versus locality in time.
We're running a little short on time.
Let me just quickly say that I have had conversations
with again some of the folks who are here
about the question of what's going on in time with these maps.
If you don't have deterministic initial value type problems
and you don't have more co-meanity,
then something is going on with the dependence on time
that's not trivial.
I'm not going to have time to talk in detail about what that is
but maybe we could talk offline about that.
I think I got to all of your points
and I want to make sure we have time for at least one more question
because we have...
So now Gabriella already asked a question and so did Eric
but David has not asked a question yet.
So if it's all right, can we go to David?
This is a way of trying to press a little further
but maybe we've begun to get to an answer to it
with this nonlocality in time associated with non-Marcovian stuff
but what I was going to do was just try to press a little further
Lev's question and Chip's question and so on
and maybe this is a way to do it
and maybe the comments of the previous questioner
about nonlocality and time have something to do with this.
Here's a way to press their question a little further.
Compare it with Bohm's theory again.
In Bohm's theory, I've got a particle on its way
into a double slit experiment
and what's going to happen later on
the probabilities later on of landing at certain points
doesn't just depend on the initial position of the particle
and the setup of the slits and the screen.
It also depends on the initial wave function of the particle.
So another way of pressing Chip's question and Lev's question
might be to say what corresponds in your picture
to that sort of additional degree of freedom
corresponding to the setting of the wave function.
That is a really precise question
and now I understand a little bit more
maybe what people are trying to ask about
and now it's a precise enough that I can provide an answer.
So you can still do preparations in this framework
but what's going on in a preparation now
has a different interpretation.
So in the standard textbook quantum mechanics approach
to prepare a system in some particular quantum state
you do maybe some initial measurements
and then you identify some collection of systems.
You identify the ones that come out a certain way
and now you know the initial state vector for the system
and I don't know.
Depending on whether one wants to take a Copenhagen-ish attitude
toward the wave function or treat the wave function
like a physical thing I don't know
but certainly in the way that we talk about it
in the textbook approach the idea is
the state vector is something physical in that approach.
We've set the system up with some initial state vector
and then we evolve it forward and in the Bohmian approach
at least if you're going to take a sort of
flat-footed Bohmian attitude in regard to the wave function
as additional furniture in the world
when you prepare the system
you have the particle it's in some initial configuration
you also prepare some initial pilot wave or wave function
and then they evolve together.
So the question is what's going on in this picture instead.
You of course can run that entire preparation process
in this picture it's just the interpretation is different
but let me get that at the point.
So what does it mean to say that the system
has some initial wave function?
Well if the system has some wave function at some time
t whatever your time you want to start your experiment
called t-naught.
Well that wave function corresponds to a density matrix
it's a matter product of the wave function with itself
you get some density matrix and you'll notice
that density matrix has off-diagon luxuries now
it has coherences.
Those coherences are telling you that your system
is in the midst of a stochastic process
it's in the midst of the indivisible stochastic process.
So when you prepare a system in some non-trivial
state vector or wave function
what you've done is you've put the system
into some indivisible stochastic process
that's going to have certain consequences
and those consequences are when you run the experiment
you're going to get certain results that come out.
So I'm asking for more
than I can get a quick answer to now
and I apologize.
We can talk offline about this too.
Help me just one more step
in the last minute available to us.
Forget about a preparation
because the only thing in the universe
is this particle and these two slits
and the fluorescent screen.
What is it?
The density matrix is presumably
not some independent thing.
The full physical situation of this particle
is given by its position.
Yes, the physical position is given by its position
but whether you take a humian
or primitives view on laws
the description of what this particle is going to do
is still described by some law
and that law is some stochastic matrix
and remember how the wave function shows up.
So it's going to be like in a
normal logical Bohmian approach.
It's going to be like in a normal logical Bohmian approach.
So they're going to be kind of two normal logical layers.
There's going to be the density matrix
and then how the...
Oh, but remember the density matrix
they're unified.
So the density matrix is really made out of theta, right?
So wave functions and density matrices
they were defined using this data matrix
that gave this Hilbert space representation
of the stochastic dynamics.
So the wave function isn't this extra ingredient
that's been added on top.
You actually don't even ever need to use it.
It's just a convenient tool.
The density matrix will have a sort of
NOMIC interpretation.
Yeah, it'll play a normal logical role.
It's a collection of your initial probabilities together
and it's got the time evolution wrapped up inside it.
Yeah.
So in a way you can think of...
That helps a little.
Okay, I'll have to think about it.
The kind of object that I regard it as
is something akin to whatever the metaphysical
status is of like a Hamilton's principle function.
It's a mathematical appurtenance that lets you
predict things,
but it's not like a physical object.
You can't hold a Hamilton's principle function.
But it changes in time.
So the Hamilton's principle functions.
Uh-huh, right, right.
Okay, good, good, good, good.
Good. Thank you, Jacob.
All right.
Eric, if you can take one more question,
then we'll have to quit.
Okay, well then I'll give you the really simple,
really simple technical one.
Do you have an idea how this can be...
Pardon me.
Can we possibly extend it to systems
whose space-space don't admit probability measures?
That's a great question.
And the answer is I don't yet know.
Okay.
I don't get an answer to that.
The only thing I can say at this point is,
and let me just...
I'm not going to punt completely.
I'm just going to say that, you know,
my view is that when you encounter some physical system
and you want to model it,
inevitably you're going to do some kind of coarse-graining
because we don't know the ultimate constituents of reality.
That coarse-graining can mean making something continuous discreet.
It can also mean taking something discreet
and pretending it's continuous.
And the argument here is that there's certainly enough
resources in this approach already
through an appropriate level of coarse-graining
to model all of the real-world systems that we know about.
Once you have a Hilbert space picture for one of those systems
and you understand the Hilbert space picture
to now be this mathematical pertinence, the set of tools,
you can do all kinds of things in the Hilbert space picture
to simplify your story.
You can take various limits in the Hilbert space picture.
You can take thermodynamic limits.
You can do all kinds of stuff with it,
but as long as you're not trying to ascribe physical meaning
to the Hilbert space ingredients,
then it's totally fine to do that.
I was thinking about probabilistic reasoning
and cosmology in particular.
And there the coarse-graining you're talking about
actually won't solve the problem.
It does not solve the problem.
And my answer to questions of making sense of probability
in large universes, I don't have a clue.
Join the club, man.
All right.
Well, it's good to see everybody.
We're going to have to call it quits.
I'm sorry we went a little over.
So stay tuned.
There'll be an announcement about our next speaker.
Our next speaker is going to be Wayne Mirvold,
who will be talking.
I'll send an announcement with all the information about that.
So stay tuned.
It's good to see all of you.
Thank you for spending this past couple of hours.
And if you have follow-up questions, please let me know.
Be happy to hear from all of you.
It's great to see everybody.
Be well.
