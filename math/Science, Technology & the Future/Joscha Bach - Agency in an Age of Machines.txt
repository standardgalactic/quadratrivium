And we have Yosha Bach with us, and he's going to be speaking about AGI and agency.
Okay, so agency in the age of machines.
And Yosha is a PhD, his NAI researcher who's worked and published about cognitive architectures
for quite some time now, mental representation, emotion, social modeling and multi-agent systems.
And he's taught computer science, AI and cognitive science at Hamburg University of Berlin
and the Institute of Cognitive Science at Oddsner Book.
And he's got a book out, which has been out for a while now.
It's called Principles of Synthetic Intelligence.
Yes, so I won't belabor the bio.
I think, are you working at Intel Labs at the moment?
Is that current?
They do.
Yes, that is current.
Cool.
All right.
Yes, Yosha is working at Intel Labs.
There we go.
Thanks, Yosha, for joining us.
It's great to have you here.
It's been a real pleasure to listen to previous talks as yours and interview in the past as well.
So I can tell everybody you're in for a real treat.
Yes, so the talk title is agency in the age of machines and I'll hand you over to Yosha.
Thank you.
I work at Intel Labs as concerned with discovering the future of artificial intelligence and understanding
how we can assess the next generation of systems and what we're currently looking at are the
different vectors in which intelligence unfolds and how to basically understand which systems
occupy which regions in the resulting space.
Today, I'm going to talk about a specific thing, which is agency.
Does this work?
Yes.
Excellent.
So the word spirit is an old word that is no longer common in our culture, but I think that in the context of agency, this has
been the word that has been used before our time to describe what we want to have.
The spirit is basically the virtual control system that gives control to a query into an autonomous agent.
So spirit is, in some sense, the operating system of an autonomous robot.
And then the word was invented, the only autonomous robots that were known were cells and plants and animals and humans
and families and cities and ecosystems, nation states and so on.
They all have spirits and sentience is the ability of an agent to discover itself in the world and its relationship to the world.
And it's clear not all spirits have sentience, but they're all agents at some level.
Our own culture is confused about the nature of agency and of consciousness and sentience.
Consciousness and sentience are not the same.
Consciousness refers to a particular kind of control model, one that is noticing its own attention in the world.
And that's the specific thing that you have in addition to agency.
And it's not clear that all systems that agency need to have it.
It's also not the same thing as having itself.
The self is basically a model of what kind of agent you are.
And it's a very specific one, one that gives you a first person perspective.
It's one where you notice that the contents of that model are being changed as a result of turning your intentions into actions.
And this is what the first person perspective and the self is about and the self is getting an intermediate stage.
I think it discovers exists between the discovery of your own agency and the deconstruction of the representations in the self.
So at some point you notice what you're looking at is not you, but it's representations about you and at this point it stops being a self model.
But most people don't get to that stage. So this first person perspective seems to be something that is almost insurmountable for most people.
And our culture we do not have a very sound understanding that you give to the kids in school about how this whole thing works.
The consciousness seems to be so confusing to many people that they turn into mysteriousness and mysteriousness is the philosophical position that something cannot be understood when one Chomsky doesn't understand it.
Nonchomsky is a mysteriousness with respect to consciousness for instance.
And instead in our culture, a lot of people are instead dualists.
And this means that we in some form or other think that there is a separation between the physical domain of things and another ontologically similar to equally privileged domain in which ideas thoughts feelings emotions and so on play out.
There are many versions in the variants of that. And alternatives to this dualism that most people have an informal form even most neuroscientists that I talked to seem to have this at some level is a monist idealism which means everything is a dream and we live in this dream.
And monist physicalism which means there is a costly close mechanical layer from which everything emerges and this is all there is.
And I think that our culture has this confusion, this dualist confusion, due to the religious indoctrination that we have.
And it's something that we do not find in the same form in some other cultures so for instance when you talk to people in Japan or sometimes to people in China or India and our culture I think has known this at some point and how to put this together and it just got lost.
And we think this documented in an interesting text which is Genesis one Genesis one is being used as part of the Christian don't know true nation and typically takes the place of telling pre scientific people about the creation of a physical
or a supernatural being at least that's the modern interpretation of what Genesis is about.
Right. And the way in which Genesis does that is completely unscientific and it's also philosophically unsound, because what's being described is that there isn't God who hovers over the face of the water somehow, and then it creates light.
And then it finds that the site is good, and then it creates a firmament and separates the waters above the firmament from the waters below the firmament she doesn't make any sense at all.
And then it creates all the plants and the animals and gives everything their name, all these things you know, none of these objects is a physical object we know physics it's quantum mechanics right it's weird shit it's mass.
It's not plants and animals and names, colors and sounds colors sounds names plants animals people, they are not in the world. They exist inside of your mind their creations by your mind to make sense of the patterns of your systemic boundary.
So, we know that people know this right took us very long time to figure this out.
Did we actually need to do experiments to figure this out. No, you figure this out by actually thinking about how the state of a fair works you realize that the map is not the territory.
It's not that hard to see. What if the ancients were able to think straight.
What if they had actually good philosophy so let's reread Genesis one. And I think it might turn out to be a six stage theory of development of consciousness and person itself.
The picture that you see to the right has been generated by Dali to new open AI transformer model for the city fusion model for generating images based on textual prompts and the prompted generates image is the emergence of consciousness from the
universe automaton.
On stage one, we have the work, the work is the creative spirit. It's something that can create stuff. And what it does it creates the domains of the world.
And it creates the domain of abstractions ideas. And this is the domains that our ancestor Descartes describes as rest extensor and risk competence the domain of stuff in space, and the domain in which thoughts and ideas
happen. Right, these are two domains that exist in the mind, not out there somehow at the level of physics. And this whole thing starts this creation process with the creative spirit hovering over the substrate.
And the substrate is to the boohoo. It's without form avoid. Right, it's completely chaotic substrate that the creative spirit hovers over and we have now discovered that this chaotic substrate to our best current understanding is the
architecture of the brain. And the first thing that the creative spirit manages to do it manages to spark light out of these new ones. Basically, it sets them into oscillation. And some of this oscillation creates a measure of intensity that is possible to be separated from its absence.
And you draw out this intensity from the darkness and what you get is contrast. And then you associate this intensity with the color of the day, this light, this brightness, and the absence of it with blackness, the color of the night.
And this is stage one.
And one is done.
What you do is you create space from the substrate you do this by arranging the contrast along an extension.
And you can find discover operations over this extension you can for instance move with the medically we call this moving along extension edition, you can zoom in and out we call this multiplication.
In fact, you also can discover the relationship between addition and multiplication, which is the exponentiation.
If you are defining extensions in multiple directions, you can also discover another operation which is rotation.
The basic primitives of moving stuff in space and creating shapes.
And what the creative spirit is doing after it discovered multiple dimensions, it separates spatial regions, and it creates a division between the domain in which it's going to describe the world from stuff in space and ideas.
And this is stage two.
In stage three, it is structuring the space into 2D and 3D, and it creates the ground plane from 2D.
And on top of this ground plane, it puts a space in which things can be put.
And the things that it puts in are the solids and the liquids and the organic shapes that it discovers how to model and to create and to manipulate.
And you now have the basics of a game engine that you can use to model the regularities and the patterns of your systemic boundary.
And at stage four, creative spirit discovers the irregularities of illumination, shapes remain constant when the light changes, so you need to tell them apart.
And this is the light sources and it puts the big light in the sky during the day which is the sun and explains why things are visible during the day and not during the night and why there's change over time.
At stage five, it starts using this game engine to construct a variation of plants and animals and people and starts to name them, and it learns a language to do that.
At stage six, it discovers the need to control and interact with this world. And in order to control and interact with this world, it needs to have a model of an agent that performs that interaction.
And so it proceeds to create this agent and it creates this agent in its own image, in the image of this creative spirit, but not as a creative spirit, but as men and women as a person, it's something that thinks it's a human being.
And what you observe when you have children that at some point between the age of one and two and a half, its locus of self construction shifts from the creative spirit into thinking that it's a person that it starts to talk from in the first person at some point.
And around the same time, which you also notice is a break in its memories.
The toddler has perfectly fine memories and is able to report on them after learning to speak, but at some point they don't remember what had happened before age one and a half or age two.
And I think what might have happened is that they basically construe memories from this new perspective, and the original creative spirit that they had been gets relegated to a perceptual module that helps them constructing solutions to problems occasionally, but they identify as a human being that lives in this game world that is created in the brain.
And it takes a long time before you discover again that the world that surrounds you is one that has been originally created by your ancestor by this creative spirit that lives in your own brain.
So I think that's a pretty sound theory that is pretty congruent with what we figured out in AI so far.
The pictures that you've seen were mostly not all of them generated by the leaf, which is not an API yet, but an interesting point in the development of understanding modern agency this strategy of discovering modern agency had to start again from scratch because
we broke our civilizational spirit when the Christians destroyed the insights that we had in antiquity, and then we basically broke the Christian epistemology with the Enlightenment and we discarded that one.
And so modern science had to reinvent all the terms from scratch and we started, I think the cycle of physics is good stuff.
And then we discovered cybernetics theory of control and feedback and building systems from them, and then artificial intelligence took over and pushed out cybernetics because Minsky didn't like it, and turned everything into computation.
So we had to replace parts of artificial intelligence with cognitive science, because other people didn't like the idea that we could be computers, and we had to pretend that we are studying the mind without doing computers for a while.
So we started annoyed new discipline, in which different fields came together and linguists, neuroscientists, AI researcher, philosophers and anthropologists and it turned out to be a shared race for the same kind of funding, but very little overlap between the actual disciplines.
What does agency emerge over.
The Christian has a very beautiful theory that has the benefit of being super elegant because it's a simple single thing. And it's the search for free agency which basically leads us to building predictive models.
And the AI systems that you have seen like Dali and so on and GPT three and so on are interesting in that what they're doing is they just are systems that are in some sense, taking for instance idea to the extreme what they're doing is they minimize the predictive
model.
There are autocomplete algorithms, they try to identify the future from the past just by minimizing a prediction error on some level.
And of course, there's also contrast of principles so you basically try to do this in both direction that you try to associate the similarity between a label and an image, and the dissimilarity that you have between them.
These networks slightly conscious famous open eye researcher treated recently that he thinks that it might be the case that today's neural networks are slightly conscious and it created an uproar of people who mostly disagree with them.
It seems to me that the noble eightfold past to make these transformer models slightly more conscious is to do the right feature decomposition the right loss function, the right output generation the right update rule, the right prompt the right model size the right attention
control this is what we're currently doing, but it might be that this is not the right way to do it.
For instance, an issue that the current generation of models that despite the amazing abilities is compositionality if you ask Dali to to generate a blue cube on top of a red cube besides a smaller yellow sphere, these are some of the things that you get.
I think this one down here, maybe comes closest, but it's not a yellow one.
It's not always getting it wrong, but interesting thing is that it's getting it wrong most of the time here.
And it's fascinating this thing has seen 400 million images that were annotated.
It has discovered structure and it has discovered so much structure that you can ask it to draw a bunch of teddy bears that do a GI research on the moon in the 1980s that does that it looks awesome.
But why is it failing with these things that still. And first of all, taking 400 million pictures is a lot more than what the human artist does before they learned to do things even if you count looking into the world into different scenes.
In your course of your life as frames that you're looking at right you're not going to look at 400 million scenes in your life you don't get that old.
And so it's not very sample efficient to converge to this.
And it also learns and somehow in the wrong order it starts with learning some basic structure then it gets to syntax then it gets to style and semantics emerged as the long tail of learning style in these models.
And our learning is very different we start learning stuff based on some kind of relevance. And then when we learn language we learn syntax also the syntax of visual primers later on, after we already have semantics and the semantics and in syntax
we have to explore the world for a while. And so we start basically with learning semantics and the style is the long tail of the perception that we have.
And so human beings learn in a completely different way. And I think that's because they're not driven, but just by the minimization of a prediction error, but their own agency, they are based on control.
And the modeling that we had to happens in the service of control.
You all know what the controller is. This is the basic canonical structure of a controller is often being described in the context of cybernetics.
The controller is an effector and the sensor the sensor notices a difference from the way how things are.
And the system that's being regulated gets manipulated by the controllers to the factor to minimize this difference between the set point deviation and the sensors measurement.
The controller environment exists as something that affects the regulated system by disturbing it and so the controller has to constantly regularly mess with the regulated system to account for the disturbances.
And to do this, it needs to, if it wants to be effective to model the environment to some degree.
The simple controller the canonical example as you read the summer start. So the summer step is measuring the difference between what the temperature is and what it should be.
And if the temperature is lower one it is what it should be the thermostat that turns on the heating.
And if it's as warm as it should be it turns off the heating again.
And so this is a very simple control circuit, but this control circuit is not an agent. How do you get from this to an agent.
You can control the temperature of the future.
Basically what you do is you give the thermostat the ability to anticipate future differences in temperature based on its current actions.
So it and then it starts to learn that difference. Now what thermostat is optimizing for is not the temperature at the given moment, but the integral of the set point deviations of the future.
So that if the thermostat decides to turn the heating now versus in five minutes from now, they're going to be different trajectories of temperatures in the future now we have a branching universe.
And the thermostat can decide which branch it's going to prefer based on its model.
If you get better, if it starts making such models of the future it will be more energy efficient controlling the future and so on. And it will show to have basically beliefs about the current state beliefs about its relationship of its actions to future actions
it will have beliefs about the future, it will have intentions, and it will turn these intentions into actions you have a full blown agent just by giving an agent the ability to control the future.
The future becomes very smart. It can start to model the environment in very intricate ways, including itself so it can discover the nature of its sensors and the factors the inaccuracies of its sensors additional kinds in which the sensor interferes with the
heating because it's put next to the heating, and it might even discover that when it makes things too hot and shoot too short amount of time somebody will open the window and hitting will become less efficient and so on and so on.
It's possible if you give that thing enough entanglement with the world and if you give it enough ability to model the world at depths that will discover itself in this world, and eventually become sentient.
The idea of cybernetics was thrown out because the people at MIT discovered how to make computers, they're usually successful, and eventually this notion of agency was we discovered for instance in Minsky society of mind where you had this idea of basically
reconstructing agents from neural networks that form an organization and eventually all sorts of models that interact with each other.
The difference between computation and cybernetics is quite interesting computation is built on the idea of a state machine and deterministic transitions between states.
And what Turing has shown after good as failure to save this semantics of classical mathematics, and we had to throw out continuity, that the state machine is equivalent to what mathematicians have been doing done all along which means the state
machine gives you all of constructive mathematics and constructive mathematics gives you domain of all languages, in which you can make computable functions.
And the dynamical systems that existed in cybernetics and part build on continuous functions, which is not a big problem because you can always, if they are useful, turn them below the hood into something that the real numbers are actually all in teachers,
and you get the results of the finite numbers of steps. But the angle in which you look at it is different one state machines look at discrete functions and dynamical systems, you tend to look at geometries.
And state machines look at composition and dynamic systems you look at entanglement in state machines you have logic and dynamic systems you have feedback and state machines you look at modeling and dynamical systems you look at control.
There is no continuity in nature, if you zoom in, you realize that geometry is just too many parts to count.
There are no discrete state machines in nature. Your computers are built on somewhat continuous systems, right, that you just tune in such a way that they look discrete to the logical language that runs on the computer.
So in some sense, dynamical systems and discrete computation and practice are both abstractions, and the abstractions of the same ontological phenomena and control and modeling, turn out to be two sides of the same coin that just two ways to write down your mathematics.
The physics itself is of course not that one of the most interesting attempts to build a comprehensive cybernetic theory of representation that exists today might be Stephen Borsberg's adaptive resonance theory.
This book conscious mind resonant brain is something that I'd recommend if you're interested in that. And basically he had some ideas on how to our neurons compute minds in the aggregate.
So if we look at this idea, what we see here is some neurons that light up we see how the activations are spreading through the exams and are affecting other neurons, and then eventually they form large scale structure in which somehow computations of mental states take place.
And the original interpretation of what's going to what's happening here is that humans are basically circuits, and the activations of the circuits are basically the activations that are associated with the notes in the graph and the links in the graph, transport this activation into
other notes and they're being summed up there and weighted against functions.
The basic idea is the perceptron by Frank Rosenblatt that is still at the core of our neural networks today.
And back then when Frank Rosenblatt invented the perceptron he I think already anticipated deep learning as you can see here in the organization of a perceptron.
And he did this before people had discovered to train these things with stochastic gradient descent this rule that is mostly still used today.
And because they hadn't discovered that means he could show that the existing algorithm couldn't learn many things so the algorithm that Frank Rosenblatt and others had at the time, and Rosenblatt actually implemented this with hard work you did this before computers were available so
they did this really with mechanical contractions and chemical systems and so on it was a really mechanical interpretation of this, it could be used for actual computations and for actual learning.
But with the existing algorithm, it could not learn for instance XOR was not able to do the decomposition of arbitrary functions.
And Minsky and Papad wrote a book that basically obliterated funding for connectionist research for more than a decade and set back neural networks for quite some time.
This was not Minsky's fault arguably because how would he know that he didn't see something. It was the fault of the funding agencies who interpreted Minsky in this way.
But it's not like means he later recanted and told the funding agency that should give the money to the neural network people anyway.
So I think there was also some science politics involved.
The neural networks today are immensely successful and this idea that they form circuits and so on has been explored by open AI they've written excellent series of articles under the distilled pub URL that they look at lots of, for instance, vision neural networks and
their similarities. And when you read this you will notice that neural networks are far from being black boxes that they actually understand what they're doing and how they are able to decompose the world into features by representing everything as weighted
chains of sums of real numbers and how these features represent different aspects of the reality that are being superimposed.
You can basically replace once these features are being learned, then by fun to construct yourself and have the same property and you get the same outcome, if you can show that this works and why it works.
So, if you ever get told by somebody that you will networks are black boxes. This has not been true I think for a pretty long time.
And so features and the interpretation of Chris Ola's work are the fundamental unit of neural networks and they correspond to directions in an embedding space and they're connected by rates and form circuits.
So the universality condition that means that if you have a model that is architecture that is powerful enough, then a lot of those features will form circuits that are have another those functionalities across different models and tasks.
The first question whether the circuit metaphor is wrong, whether our new neocortex is actually made of circuits. For instance, when you look at a new representation and your cortex by measuring the activation of cells, and you look the next day sometimes the activation of patterns have migrated or they have even rotated.
It's obvious how this would be done in circles right and there is some confusion at the interface between many groups at different schools of neuroscientists, and also between neuroscientists and AI researchers, but the relationship between neural networks and the neural mechanisms in the brain are.
And I suspect that the correct solution has to do with understanding the self organizational structure in the brain.
There is a difference between technological systems which have a functional design like neural networks have a functional design and biological and social systems which use a meta design and biological neurons are self organizing agents right if you look at them.
They're single set organisms, each of them tries to survive by itself. The only reason that you can survive is that it gets fed by its environment, and the environment is measuring the performance of the neuron when it feeds them.
So the neuron better behave, and it's linking up with other neurons to solve this problem of survival in the skull. And it does that by looking for features patterns in its environment that give it an information about when it should fire.
Right so the individual neuron is looking for a spatial temporal pattern of information, a wave front that comes in that tells us now you should fire now you should send a signal.
But if it's successful in doing that it can exchange some of this information with other neurons, but it's also collaborating with other neurons in sending these activation patterns force.
The overall active interaction of the neurons is integrated by the brain to distribute reward across the new ones to tell them that they're doing that keeps them on track.
How is the organization in the brain forming.
You could put everything into the genome but the genome is not that very long it just fits on a CD ROM and a very tiny fraction of that CD wrong we don't know how large it is codes actually for the formation of the brain and for the difference between neurons and other cells.
So, there is a very sparse representation for what the brain is doing. How can the space, the all the structure of the brain that is crucial be represented in such a sparse form, and also such a robust form that it works, even when the developmental defects, or when the
environment is quite different that you need a different brain organization.
And the solution to this might be neural Darwinism a coin, coined by an idea coined by Jerry Edelman the idea is that what happens is that there is an evolutionary competition between different modes of organization in the brain.
Something like you have a society that doesn't know what it's doing that people don't have a government yet and then there is a different competition between different forms of government that are being invented and compete with each other and fight wars with each
other to become more complicated, get one more layer and prevail and have revolutions and so on. And this is a very efficient way to set up the design if you will only send up the boundary conditions of the evolution.
Right. And you can do this arbitrarily tight so this this evolutionary competition is going to be rigged. It's going to have basically the same output and basically every person.
And the outcome of the evolution is going to be pretty much the same, but you only need to leave a few cues in a system to speed it up.
So you can be genetically quite efficient if you basically just create the preconditions for an evolution of possible mental organizations that make it necessary for convergence of that thing.
And it gives you the benefit if you have some mutation that affects the way in which your brain is organized, you will just turn out to be a nerd.
You still get to full organization you even can get to enlightenment it's just going to be a different past than the normies.
So, how is the difference between this technical design technical design works from the outside in biological and social design works from the inside out.
To present this idea we have to look at feedback loops and open loop basically means that there is no loop means you just write in your environment like industrial robot that has no sensors.
We take up this part part from the conveyor belt put it somewhere else and if somebody has moved the bin in which you're supposed to put the part you're going to drop it on the floor.
This is what an open loop is about. But if you give the robot arm feedback, then it's able to adapt to slight changes environment even to quite large changes in its environment.
But the type of system that we are we have an extending loop.
Basically what we do is we build loops into an environment to check which part of the environment yields to our control, and then we integrate it into us.
And that's the reason why, for instance, when we use a car, they're used to driving a car, the car gets integrated into our body schema, right when you drive the car you notice why this emotion that is extension of the dynamical system that you are.
You notice that the distance between your car and the environment quite effortlessly, you notice how the tires touch the ground, and it feels like part of your body.
And if somebody crashes into you, they crash not into your car, they crash into you, right, you feel this how they crash into you.
And it's quite fascinating what basically happens that your body is inside of your car and it becomes they become integrated, your mind has a resonant model of your environment and if you can create this instant resonance, then you will have this impression that is part of your body.
You build these resonant loops, what is the minimal system that can create these loops into the environment.
And what you basically need is something like a core seed.
And I think it's tempting to think that our consciousness in our mind is actually simple, that the hard thing is not consciousness, and the observer that is attending to the fact that it's attending.
But this notion that you are attending to the fact that you are attending and thereby established that you are an observer is actually algorithmically and structurally simple.
What's hard is perception, what's hard is learning, what's hard is the overall architecture, but to have this thing, the seed that is growing and that imposes more and more order on its environment as it grows is actually quite simple.
Right, so we are back at our creative spirit, this original spark that is taking in that observing the two who were both around it, the formless void of the neural substrate that is having chaotic patterns, and it builds feedback loops into it and creates order it's
basically like an attention had in a transformer, only it's integrated with itself all the time, and it lives in a resonator that is dynamically entangled to a changing universe.
So, if you look at this difference between the inside out design and the outside in design.
If you design a system technically, you start out with a controlled table workbench lab science scientific discipline. And from this deterministic known environment you identify a substrate that is still formless avoid on which you can write which you can create, and you extend the
known world into the substrate.
Right, so you basically you close the gap in your world from the outside in, but building this new thing that you add to your lab to your direct revenge, or to your science.
And biological systems do not work like this around them. There is an inter dismissive environment there is with lightness, and you do know what this wilderness is like.
And in order to become a tree you need to start out with something that is not a seed, and the seed is colonizing its environment colonization means that you impose an administration on the environment that replicates your own structure into the environment.
In order to make that happen you need to extract more energy from the environment, then it costs to maintain your administration of that environment.
But this is what it means to build a colony. And if you exist in nature, you also in competition with all the other colonies that exist.
It's on order to survive you need to be able to defend the onslaught of the other colonies that going to try to extract the make entropy from the same volume of space, as you occupy.
And this is what limits the size of an organism right an organism needs to be able to maintain chains of command into the environment to maintain this colonization by the principles that define the organism.
And one way of doing this, if you want to go with, beyond the size of the cell that has a clear cut boundary to the world beyond which knows what is happening and outside it doesn't know what is happening is to take multiple cells of the same type, multiple
cells of the same type know what they are like.
So they can link up the cells of the same type and form some self organization and together colonized the world by creating more of cells so they're copying themselves create more of the same kind.
And in this way, they can create a branching administration.
And there's of course a limit to this administration. Most organisms have a certain limit over which they can maintain the logistics chain so they can coordinate.
And there are very few organisms which have cracked the secret of spreading beyond very small organism in which basically can scale almost indefinitely and for instance there are some trees which manage to clone themselves and stay connected.
And there is a dwarf and species that has been able to colonize large part of the planet as a single colony.
But the price for this is that they stopped at stop adapting, because if you still evolve, if you still have changes in your genome, it would mean that some parts of your super organism will go out of back with the others.
And for the reason why not all organisms have evolved in the shape that I can go indefinitely is because the benefit of remaining at some kind of medium shape is that it's much easier to adapt, because you can form units that are different than the others and you can evolve them and you
can select for those that work.
So if you look at the causal systems that can exist.
The interesting system files here is the agent and the agent is subset of our control system. It's basically the controller the sensor and the effect are together with the set point generator and a model of the future that is being generated by the controller.
It's the canonical shape of an agent and to control the future, you need to be able to make a model of the future.
That means you need to create a world that is counterfactual causes structure that is different from this causal environment in which you currently live, but you need to create your own hypothetical universe to make such a simulation universe.
So the simplest agents in nature will have a Turing machine in them, otherwise you cannot be an agent because you cannot control the future.
And the simple system that we know in nature that contains a Turing machine is the cell.
And it literally has a read write tape in it.
And the cells in this way able to make models that are decoupled from the subset of the cell that are the various some causal closure that the model doesn't know what the environment outside of the model looks like.
And so the agent can use that model to entrain it with causal structure, arbitrary causes structure that you can use to model counterfactuals like the future.
And so mechanical components are not agents that just systems that have to randomly self assembly or that use an outside in design external force.
And they have no capacity for adaptation controller are resilient they have attractor states but they cannot adapt to new environments.
So they couple their computation, and they are able to anticipate disturbances environment is lost and to adapt.
And group agents have an individual motivation, they form the group, usually via reputation systems so they basically coordinate their attraction, the navigation, the gating the world in such a way that the interaction is to the benefit of the individual.
And then this is basically to the mutual benefit of the individuals, then the group becomes more powerful.
The problem is this this group agent that sometimes the local national curriculum of the individuals is not compatible with the optimal good of the group.
Right. For instance, imagine you live in a village and you will always put on your garbage on the street. This is maybe bad for the village, but it's a benefit for you if somebody else takes care of that garbage.
Everybody just acts on their own benefit the streets will be full of garbage.
How do you prevent this will form a government and the government is an agent that exists to change the pay off metrics for the individual agents in such a way that their national equilibrium becomes compatible with the common good which means basically, if you
throw your garbage on the road, then the government is going to punish you it means it makes throwing the garbage on the road, expensive enough that you will take care of the garbage, and maybe even pay somebody to collect your garbage for you.
Right, this is the benefit of the government. It changes this pay off metrics and it's in your own best interest to pay for having a government, because it's going to create a world that is much better for you.
Right, by also making the other space by imposing common rules on the system.
The government is not bad to you because it forces you to put your garbage away. The government is ideally good because it creates a world where there is no garbage on the street which is more useful to you than a world which where you can dump your own garbage as the only person.
And as a result, there is no time to garbage and everybody else does too.
Right, so this is the beneficial thing you need to have something like a state, and the state means that you are defecting against individual units when it's in the interest of the better of the system.
The state building basically means you have an organization that is also willing to arbitrate against your friends, against itself and principle, or against parts of the system by making sure that the global good is maintained of the system.
That's the purpose of the state. And so a state is going to have hierarchical governments, and you can derive the nature of the state and it's optimal organization from cybernetic principles basically there's always a right level for control.
And a state will have to have an immune system that defends itself against agents that do not submit to this hierarchical governments.
And the state has to limit the autonomy of the sub agents.
If you want to go, for instance, beyond a simple state, a tribe, where the individuals still know each other and you want to build a state that can stay indefinitely, then you cannot maintain this with a reputation system.
So basically you need to build something in each individual agent that makes them behave, even if nobody is looking, even if they get into a new context with strangers.
And this is going to limit their abilities and their evolutionary drift.
It's interesting to look at humanity because it's at this boundary between the tribal agents and the infinitely scalable building agents.
Humans are too large degree domesticated they have built something into them that makes it hard for them to defect.
But not all of us have and our societies are being built to that tangent in a way.
So if we look at the society of mind this structure of the agents in our own mind that we can also use to understand how to build machine minds.
Rinsky has looked for instance at the modularity of the functionality that is implemented by hierarchies of dynamical features which are actually operators.
So the operators that change the interaction with the world.
For instance, he suggests that we can create actions by organizing our behaviors into hierarchies that get more complex and that combine the sub worlds of these actions into long term goals and you basically have alternatives and conjunctions of steps that give you scripts that you can execute.
You can obtain these scripts by having self organizing agents that coordinate with coordinate with each other, until they form something like a society in your mind in which the agents are being activated and play with each other, according to the tasks at hand.
And you will have to have special agents that evaluate the behavior of the other agents.
And what they have to do is they have to figure out the differences between the current situation and the goal situation. And these differences are being made available to the system and lead other agents to decide whether they should become active.
And the interaction between the agents is also managed by specific agents and means he calls them K lines and these knowledge lines are basically the form their own society of mind and this is the government.
So you have a case society and an S society and the society of the government and the society of the other agents interact with each other.
And eventually you can basically turn this into a perspective into an A brain and a B brain and the B brain lives within the A brain and is optimizing the learning and actions of the A brain and the A brain is what acts directly on the world.
And the self is basically a model that emerges in the B brain, it's a model of the own aggregate agency of the system, and it's downstream from the set point deviations from the motivation so when you look at your own self, you don't motive notice for the most part how you create your own motives.
Instead, what you notice is urges and you channel these urges by directing the attention.
And it shapes your own agency via identification which means the self contains a model of what you care about.
There are two ways of modeling reality and measure reality one is you measure an absolute value. The other one is you measure the difference between how things are and how they should be.
And this creates a discrepancy that he could call an identification is where you think this is what I'm supposed to regulate.
I'm supposed to take care of, for instance, you might notice a mismatch between how just the society is and how, how just it should be how well as it should be in its social equilibrium.
And you realize that you get an action or an urge out of this and you need to change this, right, but objectively the world is just as it is.
This is the result of many attempts to change it in the past. So this impulse is something that motivates you but does not we've explained the truth that exists out there in the physical universe.
Instead, it's something that exists upstream from yourself inside of your mind, and that changes yourself into something that is serving these purposes.
So what is the role that consciousness plays in this the idea that for instance Joshua Benio has is that the purpose of consciousness is to get into this dynamic model of reality that we form and find solutions to problems that cannot be found automatically.
The automatic way of finding solutions to perceptual problems is basically stochastic gradient is then you follow a gradient until you reach an optimum, but sometimes you cannot find a solution to a problem by just following me gradient.
You need to construct a solution constructing a solution means you need to create some kind of low dimensional function is that is often compositional some kind of script.
And then you need to modify the parameters of that script until it works, and you will have to remember what you tried and why and what works and what didn't work and so you basically need to create an index memory.
And consciousness idea is that conscious basically a prior that there is this low dimensional discrete function that allows you to modify the overall state of your model so it snaps into a low energy energy state.
This is basically means is there is an optimal description or nearly optimal description of the world requires a relatively small function, and you discover this function using construction by consciousness and consciousness is aware of what it does because it needs to have the memory of its own actions.
Right, so the consciousness is something that observes itself performing manipulations on the mental substrate, because some things require such a manipulator.
And so you have an attention agent that is basically the conductor of your mental orchestra.
And you have a motivation agent that creates what you care about. And then you have lots and lots of perception agents that interact with the world and coordinate the motor behavior of the system.
And if you turn off your attention that is the integrated attention that tells itself a story about what it's doing, then you become a sleepwalker.
And when you were sleepwalking that state, the orchestra can still play can still play without playing paying attention but it's basically just free jazz.
If you ask the sleepwalker what they're doing, they will give out a groovy answer, but it's not one that is coordinated with a coherent story of what this agent is currently doing in the world, because the character is dreaming dreaming
is a state where you are so far disentangled from the world around you that the representations that you are acting on are not representations of the world around you, but that features of the internal system dynamics.
So the role of attention of consciousness is basically to prevent you from dreaming. It's to create coherent story, make sure that this coherent story is in sync with your perception.
The perception is available. If you shut out your perception, then even the conscious mind, even if it creates its own order will drift into dreams.
The same circles that produce dreams during the night produce them during the day, right and normally these dreams are tuned to the sensory data and if you tune out sensory data for instance during meditation, you can create arbitrary dreams in your mind and follow them.
Dreams are very useful. They can be used to augment the training data that you've seen. They can create new perspectives. They can vary the known parameters that you can see what is coherent.
But the data that are generated in the dream are not observations of the real world. They are extrapolations over the known observations of the real world.
And that part of the role of consciousness beyond this control model of attention is also that it supports distributing knowledge throughout the mind. To do this, we need to discover something like a language of thought.
And this language of thought is enabling our own society of the mind.
There is a similarity between the organization of our own mind and the organization of our societies.
And that basically the mind is the thing that observes the universe and creates a structured models of it, and the neurons and neurotransmitters are the substrate.
And the working memory is the current binding state of the mind into a model of the scene that we are in, the reality that we are in, the frame that we are in, and the self is the identification between what we think we are and what we want to happen.
Consciousness is basically the contents of attention that makes knowledge available throughout the mind.
The civilization and intellect is quite similar in its structure. The society is the thing that observes the universe. The substrate of the society are people and resources.
The current generation is the binding state of the society. And the culture is the identification with what we think we are and what we want to happen.
And media are the contents of our attention and media make the knowledge available throughout society.
So in some sense the culture is the self of our civilization, the media is the consciousness of our civilization.
And if you look at it like this, it seems that our culture basically currently has a seizure, and because our media is not really helping us to understand society as it is, there is a disconnect between media and the ground tools.
If you call that state postmodernism, modernist society, everybody is obsessed with interacting with the ground tools and having agency based on the way in which the universe is to model the future.
And when your models of the future move faster, then you can track them because the future changes faster, then you can model it, then you stop having model of the future, you lose the plot.
You basically live in a society in which you no longer have a model of the future in which you do not plan ahead past the next generation.
And this is a very concerning state to be in. It's really one that either precedes our doom or one that precedes a new form of organization in which again we are able to discover our agency and model the future.
And before this happens, what happens is a singularity. It's a state that you cannot predict. It's one that we don't know what's going to happen next, because the current models do not allow you to see what the state is going to look like and what kind of structures what kind of
eigenvectors in the new space of possibilities will emerge.
So what we've seen so far where some stages of intelligence agency starting from the regulator, which is a feedback loop and the predictive controller that models the future.
When you combine it with its own motivation with its own set point generator, you get an agent that can discover that it has a self, and when it covers itself in the world, then it can also become sentience which means that it understands what it's doing.
There is another thing that is missing here and this is transcendence transcendence I think has to do with linking up to next level agency.
You probably are familiar with Maslow's pyramid of needs.
I started to encounter it when I was trying to model cybernetically the need structure of the human mind so I could build artificial agents that mimic human behavior.
And what I realized that this pyramid doesn't make a lot of sense because, first of all, these are not needs.
I don't have a need for self actualization or a need for self esteem. This is too complicated for the organism. These are models that I have about what I'm doing.
These are purposes.
Self actualization is the purpose that I can follow. It's not a basic need.
Most of these things are actually way too complicated to be direct needs of the organism.
So purposes actually are models of our needs and when we act, we don't act directly on our needs because our needs often do not create pleasure and pain in the here now.
We act basically on anticipations of pleasure and pain in the future integrated over long time spans.
And this gives rise to things like self actualization.
Self actualization means that you can create an environment that optimally serves your needs.
So it's a high level abstraction of your purposes.
And in this way, if you try to understand our needs, be what we see here is, you have basically the immediate organism purposes, you have the relationship purposes, and we have the purposes integrated of our own lifespan, which this is what muscle talks about here in this
pyramid, right? This is a two or three categories of purposes that are organized in this hierarchy.
Also what it turns out in the individual is that it's actually not the pyramid, because our individual purposes, the relationship purposes and our immediate organism purposes compete at each point.
It's not that you wait until you have eaten until you care about justice. Sometimes you start a revolution while you're still hungry, right?
It's not just because you're hungry. Sometimes purposes are of all levels are displacing the purposes of all the other levels.
They have different weights and these different weights give rise to different personality structures and so on, but they do not form a hierarchy.
They coexist these needs and the purposes that result from them.
But there is something that goes over all of them and that's more important than all of them and that's the purposes above the ego, the stuff that is sacred to you.
And all the other purposes when you discover the sacredness are instrumental to this.
And this is basically the agency that you think you are serving, that you are part of that gives reason to your own existence.
And it's not that all people have something that is sacred to them. The people that do not experience sacredness of any kind, call them sociopaths.
Some people think that's a pathology.
Other think that's the free state where the other one is domesticated, probably depends on whether you're asking a domesticated human being or a sociopath.
And then you have something that is sacred built into you that basically creates purposes above the ego.
If you are more important than your own organism, you have possibility to interact with other agents based on that shared sacredness because you can give them resources if what they do is in the surface of these higher level purposes above the ego.
Do not expect anything in return because it's the larger agent that is being created through your interaction.
And the important thing to realize is that this other agent does not exist as a machine independent of your in the universe. It exists because you implemented together with others.
So how does this emergent higher level agency works.
There are some principles for emerging social agencies.
The one obvious principle for rational agency is goal rationality.
Goal rationality means that you are picking the right behaviors to achieve a goal that you have rational reason to do that right you should not pick irrational behaviors to achieve your goal because that is not the right way to like relation.
You also need to have the correct optimization between exploration and exploitation, which means you need to learn to write amount before you can act.
And once you have learned enough you need to be willing to act on your models.
Right, it's not good if you spend all your life with preparation by learning, and it's no good if you just jump right into action.
So this balance is very important that you need to be willing to act on your models once you have them.
You also need to optimize your internal regulation so your homostatic self regulation of the organism you don't overeat you exercise enough and so on but not excessive you just optimize your own agency.
And you want to optimize the regulation between agents to create stable social systems between them that are balanced economically and structurally.
Then, if you these principles basically rise to an individual rational agent. This is basically this applies to any sociopathic agent that doesn't have transcended agency.
But it does not give rise to a society to some kind of structure where society acts as a single coherent agent to do this to create a single current agent from your society out of individual social agents.
In some sense, the same principles will also apply to the agents and that exists in the mind and Minsk society of mind, you need to have principles for the formation of next level agency.
And first of all, this first principle is you need to be willing to serve it right you need to have a commitment to serve in the next level agent.
Without that it's not going to happen.
And you need to integrate with other agents around you to do this so shouldn't happen in the abstract, but it must happen in the practical sense that you actually connect with other agents, with which you share resources in the service of producing and maintaining this next level agent.
And the third one is, you need to be willing to be deceived, but if it doesn't exist yet, which means you need to be willing to act as if that thing was existing before it can give you a rewards.
Right, so you need to be willing to act in the absence of rewards as a precondition for making this new order happen.
If you don't do this, it's not going to emerge because who's going to start these principles have been discovered in around 1270 by Thomas Aquinas, the famous Catholic philosopher.
And he was this principle of war rationality of picking the right action and picking the right goals prudence, the optimization between exploration and exploitation willingness to act on your models is courage.
The optimization of self regulation is temperance, the optimization of regulation between agents what he called justice.
And therefore these weird antiquated Christian words for that well he invented them or they use to describe exactly those principles in our society saturated with them.
We think these are abstract values, but they're not abstract values that just principles that you can discover when you think about how to organize agents.
And for the next level agencies, he discovers this principle of, we need to submit to the next level agent to the higher level agent and he calls that face.
We need to be willing to work with the other agent to make that want us to make that happen that's love, and we need to be willing to do it before there is reward and this is hope.
Hope is in the use of Aquinas different from the hope that we have in our everyday language it doesn't mean that you think something good will happen, or you think that something good should happen.
It means that you're willing to invest into this future to make it happen.
So I think that's very interesting that the Catholics discovered this and use this as design principles for the religion so no matter whether they succeeded or not. It's interesting that the philosophers discovered principles that we might be able to use to analyze agency.
Now there's an interesting question how can we align non human agents with our human interests.
This gives rise to the alignment problem. The alignment problem means how can we align artificial agents like a eyes and organizations with our human interests.
And this is not solved in the general case we already have this problem that we don't really know how to align Wall Street with human interest and Wall Street is basically an emergent agency that has formed in our economy.
We know that is actively structuring our society and most people feel also economists often that it's not always in the interest of the optimal organization of our society so we struggle this discussion of how can we take these institutions that emerge.
This system is extremely useful right it's one of the big modern wonders of the world that we build a system that is able to align resources to every place and aside resources to every place in the world and have universal way to measure the value of these resources and so on.
And it works everywhere it's completely ubiquitous. This is an achievement that is even bigger than the internet or the corona virus the other big man made miracles of the modern world.
This is totally fascinating to me that we do not have a good theory about this alignment yet, in which we talk and then we start discussing alignment that we act as if we start from scratch.
So typically when we discuss alignment between people, I think we should start out with identifying our highest level purposes, and our highest level purposes are the world that we want to create and participate in.
What we need to do this is we take our preferences and we extrapolate them, and we see what world we get by extrapolating our preferences and available actions.
And it usually means that we have to retrace and throw out some of our preferences because they are not simultaneously possible.
And other preferences will turn out to be instrumental to the aesthetics, which means you no longer need to have them explicitly as a valuable to the system because they will automatically follow from the harmonies of the world that you that you want to create.
Right. So when you are completely done this extrapolating your preferences into possible worlds and you pick the one that is the most harmonious one, then you can replace all of your preferences you can just throw them out.
You can you will be able to eliminate all your values you just throw them out. The only thing that remains is this is my model of the harmonic world and all the of your actions can be evaluated with respect to how instrumental they are to achieving this harmonic world that you're working on.
And so the only thing that's left is to negotiate between different harmonic worlds.
Right, so this gives rise to second order problems in which you discuss the outcomes of the negotiations and the world that result from the conflicts and successes of this negotiation of about the most harmonic world.
And this is I think this domain is ethics.
So surprisingly little ethics in the AI alignment discourse, most of it is just politics, and most of this politics is not talking about aesthetics of the harmonic world that you're working from.
It's actually about very simple things about values about or even less than that it's about the civil as of different political groups that are buying for power, and by which I recognize their own group.
And so I think that's a very dangerous situation to be in.
So, at the moment, our strategy for alignment consists in nerds creating demons to create pretty pictures for us.
There is a difference in neurons with the trope of indefatigable college level thinkers that curse at the demons when they are painting nurses as women and CEOs as men and so on.
Because this is for shadows difficulties with AI alignment.
And it does right is right there is a problem there but there is a much deeper problem the deeper problem that we need to discuss goes far far beyond that it requires us to understand what kind of world we want to have.
And that is not adequately being handled in asking AI engineers to put an ethics chapter into their papers.
So I think this needs to be taken much much more seriously and it needs to be its own discipline and it needs to be a discipline that operates with formal methods, and that operates very seriously, because I think that we are very soon going to
develop a world that is populated by sentient agents that are not made from carbon, not made from cells, and that will interact with us, and that will form their own opinions about the aesthetics that prefer.
So we will probably need to find ways to align our aesthetics with them and there may not be that much time left.
I think I stopped here this is a good point to stop.
And I hope we still have some time left for questions.
We do.
That's good.
Now.
Thank you so much for that that was very fascinating, a lot to digest and a lot there.
I mean, you've obviously thought about these things very deeply.
Has anybody got any questions.
Can you elaborate a little bit on, do you have any ideas of what a formal method or formal methods would look like for beneficial AI ethics that are actually functional functional.
No, not from scratch, I suspect that what you first of all probably create need to create our institutions that allow us to do this.
At the moment, what blocks us is a lack of honesty with respect to understanding our own nature.
We are very dishonest I find in the field of AI ethics and we talk about our own psychology and about the structure of our own society and the sustainability of our own society.
We currently do not live in a society then its present form of organization seems to be sustainable.
It's not clear yet to us how we are going to resolve this, and without being able to create a space in which you can have that discussion.
We will not be able to form models about our future and how about the coordination between people and AI.
At the moment, I do not see the space because I think that if you would want to try to join academia to seriously and honestly discuss about our own future and about our own psychology and about our own nature.
You are at the moment getting into too strong conflict with tribal politics.
From all sides and this makes it very hard to have that discussion basically first of all need to create this space again our society where we can talk about these questions without screaming at each other.
And without sacrificing the outcome to very short games.
Is there any movements that you see are getting close to this space where people are not shouting at each other over tribal politics that are talking about real ethics or close as close to it as possible.
I think that such forms exist, but they know people which do this, but so far they have relatively little societal influence.
So it's nothing that seems to have a very large effect so far and so while it is, I think it's starting to happen. It's not clear yet how to integrate this with our public discourse.
How do you integrate it with with this public discourse, if it doesn't involve politics.
I don't know that.
How do we win public discourse of politics.
I think it has probably to do with the top level incentives at the moment, there are bad incentives for the people who want to work in the domain of AI ethics.
And that's not the fault of these people it's basically it has to do with the incentives that they're operating under.
And basically people are incentivized for bad behavior.
This bad behavior leads people to not producing optimal results, they're sacrificing tools to the benefits that they get or to the constraints that they emerge over these benefit in different incentives.
And again, this is nobody's fault. It's the problem of the structure that we are in that is ungoverned.
And it may be necessary that we basically started a societal project that is at our aims at our core problem this is, which is truthful governance, truthful, effective efficient governance.
And we don't yet know how to achieve this we agree that we want to have. I think most of us at least a society that is based and that is nonviolent.
And it seems to me that this means that it should be a liberal society, because the alternatives to a liberal society are ones where you get to power lose power by using violence.
And liberal society requires that you have a space in which you can have discussion that is oriented on truth, without any threat of any form of coercion.
And we have not figured out how to do this yet, and we have very big debates about how to do this and I suspect that social media has to play a very big role in this social media in some sense is is not a separate thing that is new to society it's
just the space in which you have started to coordinate and communicate using the internet.
And we have not integrated social media in our society the governance of social media is currently done by small teams of people that are not being elected that do not have a principle form of governance that they basically get the code of conduct and
governance, but this is not the form in which you can organize and rule over society basically social media is not no longer just a small part of society, but it's a significant part of society and it's a significant part of our effective government about our way in which we form the
real intentions of our society.
So, what I suspect is that we also need to think about new ways to organize social media to turn it into a global brain that comes gets to say in results that allows us to become sentient as a society again.
And that helps us to support our form of government. It doesn't mean that we necessarily that we have more voting on social media.
And I also suspect that the question of free speech or not free speech is the right question.
The question is how can you organize the discourse in such a way that you do not suppress to this ability of the system to get to true opinions about reality.
It's hard to incentivize that sort of thing. People are very interested in signaling, but not so interested in talking about things that expose themselves because that maybe they believe that if they expose themselves to freely people use that as ammunition against
that's a little bit like saying people like to throw garbage on the streets and nobody wants to have the street without garbage.
But this is actually not true. I think that most people want to have the streets without garbage more than they want to throw garbage on the street.
It's just in the world where so many so garbage on the street they many of them don't see the point to not do it.
And this is the same issue with governments and the question is how can we have self organizing governance on the Internet.
If you give people the ability to govern the rules under which they communicate with their peers with their friends with the people that they will actually want to be in relationship and you will probably create societies sub societies on social media that are attractive to enter by submitting to their
rules. And it's very different from saying we let social media have one system of rules, but instead we allow the communities to invent their own rules and we trust them in doing that, that the rules that will come out of this will both be
sometimes beneficial and also work out in such a way that more people are going to join the more productive and beneficial sub communities.
I think you had your hand up first.
Yeah, so you talked about perceptrons for a second. So I was wondering whether your thought process around perceptrons is related to Marvin Minsky's perceptrons is computational geometry.
The perceptron that he describes is a Rosenblatt's idea. And it's not just Rosenblatt's idea the basic principle has been invented and discovered by numerous people.
Also the algorithm that came later back propagation has been reinvented and discovered by several people. And Minsky has done some research in this field and basically what he added to the discussion among other things is a theorem in which he could prove that given
a certain algorithm was unable to work to learn certain things on the perceptron so it basically had proven a limit of a certain implementation of the perceptron. That's his contribution.
But the perceptron by itself and his core idea is simply the neural network, it means represent the computation as a sequence of sums over real numbers.
And it turns out that this is a general enough to express arbitrary programs and the benefit of the neural network instead of using other methods to expressing programs that would be shorter and more useful is that we have discovered some learning
algorithms that can tease this neural network structure into an arbitrary function. And we don't know how to do this at scale for this other representations that might be more efficient.
That's why everybody's using neural networks right now for almost everybody.
I think any of the current work on achieving causal learning is going to scale in the near term causal representation learning or these influence causal influence diagrams that deep mind are working on which somebody spoke on earlier today.
I'm not completely sure about this. My issue is that I, I don't quite understand the arguments of the course of reasoning, the community when they are made in a strong case.
The strong argument is basically there's a hierarchy of positive models at the lowest rung of that letter is the discovery of associations between elements in the data.
The neural networks are doing today and only that. And on the next level of the course of hierarchy is the ability to intervene and battle with the causal structure. So you can control the world using the causal model.
On the highest level is the creation of counterfactuals and the interaction with counterfactual worlds.
I think this is a very good narrative, but it's simply not true that neural networks cannot do this. You can perfectly well ask GPT-3 to predict the outcome of putting sugar and cookie dough or salt and cookie dough.
It's going to tell you whether the cookies are going to be salty or sweet. And you can also connect a robot to that and that it make bake the cookies based on what you want to have.
And if you ask it, why salt is going to affect the cookies in this way, it's going to give you a causal explanation that involves sodium and taste receptors.
And this, so it means that it's possible to extract causal structure from the world, giving the existing learning models.
Obviously, if you look at the models that the causal reasoning community is using, it's mostly directed asset graphs that are inadequate in many domains, where there is heavy cross correlation between features.
And the, as a result, a lot of what the causal reasoning community is currently working with is our toy models that do not scale to the problems that can be solved by massive large language models and vision models.
So far, the problems of the deep learning community, and I don't like deep learning because it's brutalistic and simplistic and simple inefficient and so on, have always been solved by using more deep learning, not less.
If you take a step back and look at what deep learning actually is, it's differential programming. It's a particular way to express things in the program so you can often find a solution by stochastic gradient descent because you represent everything somewhat continuously.
It's not like the deep learning people can you can do everything with deep learning are religious about this they are completely fine with mixing in conceptual graph and using propositional representations as a layer and so on, or writing and reading
from propositional layers with an end to end learn system, but there is no problem with that so I'm not sure if you need to extend the existing paradigms, even if we should look for different algorithms.
I'm much more interested at the moment in trying to find out why the deep learning systems are so incredibly simple and efficient what can be done about this.
So I suspect that the more important things are about attention, how to build better models of attention.
I mean, you referenced somebody is working on causal representation learning that was your show Benjio interested, interesting to get you to in conversation some stage.
Look, there's a few people asking about blockchain and how it can solve this through dows.
I don't know very much about blockchain or dows so I'm just
and but
as they say, we don't know what tools that's free will be built this but that forward the sticks and stones
and not quite convinced yet about this ecosystem it seems to me that the blockchain system ecosystem has been developed entirely in the shadow of cryptocurrencies.
It's independent of cryptocurrencies if you would outlaw cryptocurrencies I suspect that the majority of blockchain applications and blockchain companies would disappear.
And I don't think that cryptocurrencies by themselves sustainable cryptocurrencies seem to be the control group to the economy.
So in the same way as Bitcoin is the control group to doge how does a serious cryptocurrency perform compared to one that says on the packet that it's a joke.
You can basically look at the stock market and compare it to cryptocurrencies to understand how to real stocks perform compared to something that obviously is a joke.
But to me the fact that cryptocurrencies performs almost as well as the stock market is very disturbing sign that is very concerning to me about the state of our economy.
And so I do think it's very important to study these principles and studies ideas.
I also think this idea of having executable contracts is a beautiful idea.
The blockchain goes beyond that blockchain is not about changing contracts into something that is auto executable. And it also for the most part does not solve problems in this regard in the practical world yet.
What I see on the other hand is that there is a very big danger in such technologies.
One is, if we currently look at the financial system, there are bugs in it. For instance, when you followed the Robin Hood affair, and there was this issue with the GameStop stock.
And basically what happened was that the private investors, retail investors have discovered a bug in the way in which the value of these stocks are determined.
It's actually an undefined value.
And the reason why Robin Hood was stopped from trading with these things at scale was that some adults higher up in the food chain discovered that there was a danger that the retail investors would sack for the market.
And if our financial market sack faults, that would be a very bad outcome for all of us, right? Because we don't have a second financial market that we can just boot instead of the first one so far.
We don't know even how to reboot the first one.
You said segmentation fault, right?
So basically it creates a system bug that could create a critical error that brings down most of the functionality of the system.
And that would have been super bad if that happens. And imagine that all the decisions in that thing would not be done by people, but that they're done by machines based on the hope that everything in the algorithm is correct.
And our system would have sack faulted because some automatic trading algorithm would have just gone through the end.
I think this is one of the biggest dangers of AI in the near term that somebody is going to build a financial stock market transformer is the same resources as open AI throws on Dali.
And this thing is going to model the shit out of the stock market and figures out that all the actions on the stock market are produced by less than 7 billion people who are only alive for a very limited amount of seconds and that you can basically figure
out what they think in these seconds. And you're not going to do this exhaustively but you can just reinvest most of your gains on the stock market into collecting more data and buying more compute to get as close as possible.
And I think it's going to give the shit out of us before it suffocates all of us, right.
It's just terrifying to me this financial singularity this trading singularity and the doubts and blockchains are basically to me, the attempts of children to play with the financial system, because the adults are no longer at home.
Yes, that was awesome. By the way, Josh, that was the way I don't want to offend anybody who works in the blockchain. I'm sure that there is a lot of work being done.
Very interesting concepts. If listen to people who get JavaScript on the blockchain so it becomes more democratic and even people who can only do front and can to serve on the blockchain.
Yeah, well, you'll be showing a panel later with Ben, who's working on the blockchain. Yeah, he's working on his singularity net stuff.
But yeah, that'll be fun.
Yeah, I think it's great that Ben has discovered ways to fund his research, but I wish that he was still working on API. And I think he does. But I wish he had more attention for it.
Yeah, that's the thing. Yeah, yeah. So I mean, I agree, he does seem to have a very distributed attention.
It's great if he can manage it. And I'm sure he can. But yes, some of us wish he federates his attention on the most important things.
Yeah, I basically wish somebody else would do the blockchain for him so he could focus again on the AI.
But maybe he finds a way to combine it in ways that I'm not seeing. It's probably the case.
Yes, you can you can drill him on that.
How I understand is that the whole point of the blockchain is to coordinate the AIs to have a marketplace of AIs that query each other and each other.
In that case, you should just leave the design of the blockchain to the AI, because I think that the design of safe and extensible and scalable protocols is a problem that is, is not very interesting but hard.
So it's really an AI type problem.
But you need to bootstrap it right.
Yeah, but you can bootstrap it without a blockchain. I don't see that would you need to blockchain to bootstrap AI that this invents blockchains.
Yeah, yeah, like we had a Stuart Armstrong earlier today talking about his new research.
He's left the future of humanity Institute and it's now heading up aligned AI, if you didn't know.
Yeah, concept extrapolation is the thing that is working on. That's not all that they're going to be working on in the future but that's what aligned AI is working on right now.
It's very interesting to reason about the space of aesthetics that you could get.
The bio aesthetics probably culminate in Gaia, some coordinated agent at the level of all the organisms together, combined into one Big Hyper agent, maybe at the planetary level maybe beyond.
So it's interesting what happens when we teach the rocks how to think and if that is compatible.
Yeah, I'd like singing rocks. I used to put like little eyes on them to make them look sentient when I was a kid and paint them and things like that.
But it could also take the opposite stance. I mean, it looks like to me is that Gaia built these trees 100 billion years ago and they guided invent insects yet to take them down again so many of them fell into swamps or my
So all this delicious carbon was gone from the circulation and since then Gaia has been working on a way to reactivate that carbon and we are the solution.
We take it out of the ground put it back into the air and it can get gets recirculated and, of course, we will burn ourselves out in the process.
We will have been useful to Gaia the alternative is the standard story that this was unintentional and we just destroyed Gaia because we're so stupid.
But I suspect in this case Gaia had it coming because she didn't see us coming I think it's obvious that she should have seen this. This would happen and should have prevented us if she didn't want this from happening.
Anyway, we have reason to be angry at Gaia and we can retaliate by teaching the rocks how to think.
We've got a minutes before the next talk is scheduled to start so I give people five minutes from this point in time to have a comfort break and then we'll start the next talk, which is Monica Anderson and Monica is going to be speaking on the red
pill of machine learning so stick around or take that blue pill and go away. No, he should join us and take the red pill.
Right solution is always to take two red pills one blue one red two blue and then one red again anyone based reality everything else gets you in the wrong one.
Right, I'm going to have to trust you with that one I haven't worked it out. Let me think about that.
I have to do some more computing. I think I have to get back.
I think I went for a segmentation fall to that to have a maybe a purple pill.
You
