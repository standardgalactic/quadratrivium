All right, well, thanks, Masaki, for that introduction.
And thanks also to Kyoto University for inviting me here.
It's wonderful to be back in Kyoto.
I always had a great time in Japan in general.
Wonderful place.
So today is the first lecture.
In this series, I want to sort of start from the beginning and
hopefully get up to some work that I'm currently doing on
quantum spin chains, where I have some ideas which seem to be
orthogonal to what physicists actually do.
Maybe there's a good reason for that.
I'm trying to figure that out, but certainly one thing that
that work has led to is unitary representations of Thompson's
group, F and T, and a new way of constructing knots and links,
all knots and links from these groups.
So I take that as a positive indication from the mathematics
that maybe my ideas, this block spin renormalization idea is
not completely stupid.
And that's my aim is to get to talk to that work, which is I
find the most exciting right now.
But this is a course, so I have to start at the beginning, and
I realize probably have a diverse audience.
So I'm going to really start at the beginning with Hilbert space.
And so there will be quite a few people in the audience who
know absolutely everything that I'm going to say are possibly
better than I do.
So to them, I apologize, but there is one thing that you might
find interesting anyway, which is to follow the way that I
presented over my career.
I have spent at least 20 years trying to explain the basics of
phenomenon algebras and sub factors to diverse audiences.
And I've come up with a lot of tricks.
And for those of you that do know it, it will be quite useful
to me and to everyone to ask some of the people who don't know
it what they actually understand from my lectures.
I wouldn't really appreciate any feedback on that score.
Because it's all very well to find what you think is a great
expository trick, but whether it actually communicates
something or not is unclear.
Okay, so there's a lot of ground to cover, so I probably
will not give too many proofs, but I will try to give examples
that illustrate the theory in all cases.
There's no time for even single, difficult proof.
So that's the facts of life for these talks.
So let me therefore start.
As I said, I'm assuming almost nothing.
And I want to begin by defining a Hilbert space.
And my notation will always be the Hilbert space itself as
vector space will be written with this letter and it's in a
product which is a sesquilinear positive definite form on the
Hilbert space will be written with these pointed brackets.
Okay, so what is it by definition?
It's a complex vector space.
And this, as I said, is a sesquilinear positive definite
in a product, whether it's linear in the first variable and
anti-linear in the second or the other way around is a matter
of convention, but mathematicians and physicists generally
have the opposite convention.
So I will choose probably swap conventions.
I hope that isn't too confusing.
So be ready to see it either linear in this variable,
anti-linear in that one or the other way around.
My colleague at Berkeley, Richard Borchards, once came up to
me very excited in the corridor and told me he had found the
reason why it had to be one way or the other.
But I've forgotten what it was.
So the point is that it's complete in the metric coming
from the norm, and the norm of electric psi is the square root
of psi xi.
Any Cauchy sequence in that norm has to converge.
All right, well, if you don't know what Hilbert space is or
you're totally unfamiliar with them, then that doesn't
communicate very much.
So let me give you some examples and non-examples.
Possibly the most illuminating will be the non-examples.
So the first thing is in finite dimensions.
So in this course, typically these
phenomenon algebras, the real interest is, of course, in
infinite dimensions.
But whenever you see a new notion or something, your
first reaction should always be, what does it look like in
finite dimensions?
And then you can start to attack the more interesting
infinite dimensional question.
So well, in finite dimensions, there's a very easy
theorem of linear algebra that says that you can find an
orthonormal basis of the Hilbert space in vectors which
are orthogonal and unit vectors, which means that the
Hilbert space actually might as well be c to the n,
consisting of vectors z sub n and the inner product of z
sub n with w sub n is sigma of z n bar w n.
At least right now, the convention is that it's
conjugate linear in this one and linear in that one.
So that satisfies all these properties and completeness as
trivial in finite dimensions.
Right, so now we've turned to infinite dimensions, and let
me give two examples of non-Hilbert spaces.
So first example would be c of t and t inverse.
So this is polynomial, Lorentz polynomials in one
variable t, and you could insist that the powers of t
be unorthogonal, and this is not a Hilbert space, and the
reason is that it's not big enough.
That's because these vectors in here are just finite linear
combinations of these basis, orthonormal basis vectors, and
it's easy to see that that's not complete.
All right, so here's another non-example, something that
looks more complete.
And so this is formal power series in the variable h.
So you could, again, you could decide to make the powers of
h unorthogonal.
And this is also, so remember, this is all all power series.
There's no restriction on the coefficients of the powers of
h, and this is also not a Hilbert space.
And this one is just because it's too big.
All right, so what we search for is the Goldilocks.
The Goldilocks space in between, and that is precisely
little l2 of the integers.
So this is square integrable functions, sigma c n squared
n belonging to z, that is less than infinity.
And this is just right, and it is a Hilbert space.
Basically, if you'd chosen other summations on the
coefficients with non-volving sums of squares, you would be
wrong also, you wouldn't get a Hilbert space.
All right, so there's a theorem, oh, by the way,
until further notice, I will also assume that my Hilbert
spaces are separable, which means there's a countable
dense subset, since of topology.
And there's a theorem that says any Hilbert space is
actually equal to little l2 of z, and I'll put that in
quotes.
And this simply means that there is an
orthonormal basis, i.e., there exists an orthonormal basis.
And once you've got an orthonormal basis, then you can
put it in by ejection with z.
This is an infimdimensional course, and therefore it
becomes little l2 of z.
So these are the first few elementary facts about
Hilbert space.
So the theorem is that any Hilbert space is little l2
of z.
So in fact, it's sort of amazing that there's only one.
But that's sort of fake also, because that one Hilbert space
up to abstract isomorphism may occur in lots and lots of
different ways.
And I'm not getting hit at myself.
And this is seen in a very interesting fashion in this
very example, which is to say, so here, but this Hilbert
space may appear very different.
And the first example is sort of staring us in the face, which
is the Fourier series.
We can think of little l2 of z, square integral functions on
the integers is actually the same as little l2 of the
circle.
And here, I'll have an notation as t equals s1, and it's the
circle in a set of all complex numbers, or modules one.
Well, in thought, let's just sort of e to the i theta.
OK, well, so we all know from undergraduate courses that the
Fourier series, if we take a sequence, what I call the cn,
here, and I form sigma cn e to the i n theta, this is a
function that goes from this space to this space.
These are now l2 functions on the circle.
And it is a unitary.
Let's introduce the term unitary.
So u from one Hilbert space to another is a unitary, if it's
a surjective isometry.
So to say that it's surjective means that it's onto, and then
surjective means that it's preserves the...
I have lots more to say about unitaries later on, but in this
initial definition, it just means that it's what I meant here
when I said equals.
Two Hilbert spaces are equal in the sense if there's a unitary
from one to the other.
The Fourier transform or Fourier series, so this we call this
f from l2 of z to l2 of the circle is a unitary, or f.
OK, so on the one hand, we have this model for the Hilbert
space, which is the l2 of z.
On the other hand, we have this model of the Hilbert space,
which is capital L2 of the circle.
This is a little bit more complicated to deal with because
you have this notion measure, and you have functions.
The elements of this Hilbert space are functions defined up to
sets of measures 0.
In some sense, one of the strengths of the Hilbert
Fourier transform is it allows you to deal with these
equivalence classes of functions by simple sequences of
honest functions.
Right.
Any questions at this stage?
OK, so I hope everyone, now if you didn't, if you're
from the apology or something, didn't know where the Hilbert
space was, then you have some slightly better idea now.
Just in passing, I'll add the important notion of weak
convergence.
Weak convergence.
And so convergence in the Hilbert space simply obviously
means convergence with respect to this metric,
to the sequence of vectors converges to another one if the
distance between them tends to 0.
There's another important notion which we will see is quite
significant, and that is we say xi in some sequence of vectors
tends to another vector if and only if this is the definition.
This is weak convergence.
If the inner product of xi in with eta tends to xi eta for
eta.
OK.
So this is a much weaker notion of convergence.
For instance, if you take l2 of z and you take cn,
sorry, you take the basis vectors which are the functions
that we would say this would probably be a notation that I'll
use.
So let me epsilon n of n equals delta nm period.
So this is the function which takes the value 1 on the
integer n and 0 on the others.
And this sequence, epsilon n, actually tends weakly to 0.
If it's provided n tends to infinity in any sense, this
sequence will tend weakly to 0 because an l2 function,
so the coefficients, I have to all tend to 0.
So it tends to weakly to 0, but obviously it doesn't
tend strongly to 0 because the strong convergence preserves
the length of the vector, these unit vectors,
and they tend weakly to 0.
So an important concept for Hilbert's base, which is a bit
different from ordinary convergence, the weaker notion
of convergence.
So basically the idea is a sequence weakly converges,
just sort of disappears from sight.
Whereas if it converges weakly converges to 0 if it
disappears from sight, whereas it strongly converges to 0
if it really comes into 0 in every neighborhood.
So the next thing, I'm working towards definition of
phonomenal algebras, and phonomenal algebras are
algebras of bounded operators on the Hilbert's base.
So we need to begin to study the notion of
bounded operators.
I guess we might as well start complete generality
and consider two Hilbert's bases, h and k.
These are Hilbert's bases.
And then an operator from h to k is simply a linear map.
This doesn't need any specification, but to say that
it's bounded means that it's bounded means.
Let's just see what that means.
And that's that a, if we look at all of the unit vectors
in h and we take the length of axi, then this is less than
or equal to soup or unit vectors less than or equal to,
is less than infinity.
And if it is less than infinity, then we call that number,
if we call this k less than infinity, then k is
by definition the norm of a.
So it's well known.
So I think that I was supposed to give some exercises for students.
So an exercise, first exercise, we'll be to show that a bounded,
not only if a continuous.
And here for continuous, I can use either of these topologies,
or either of these notions of convergence, weak or strong.
And second exercise is answer the question,
how do there exist operators that are not bounded?
This is a bit of a can of worms, actually.
That's something to think about a bit.
Can you actually exhibit an operator from Hilbert space
to another Hilbert space which is not bounded?
Any questions about that?
This is sort of obvious abstract nonsense of operators on Hilbert space.
And so as I said, I want to give examples,
and the examples are supposed to be well chosen,
but perhaps before I give any examples,
there's some special kinds of operators that we're going to be interested in more than others.
They're going to play a very significant role,
so let me introduce them one at a time.
So sorry, we're still staying in the abstract realm,
but actually the first kind of operator we already have an example of,
and that's unitary.
So an operator is unitary if it has already defined it.
It's a surjective isometry.
Now the next class, I'm not going to be able to even define
without introducing a structure on the bounded operator on Hilbert space,
which really goes back to the structure of Hilbert space itself,
but I'm going to sort of, this is going to be one of my little skips
is to not really properly explain why.
So I just have a theorem to every A belonging to,
piece of notation, B of H equals the set of all A, B of HK,
set of all A from H to K, which is bounded.
So given two Hilbert spaces, B of HK, this is Cooley K,
not to be confused with this K, which was a bad choice anyway.
Let me change that right away.
This notation, and if we're talking about, if H is equal to K, then we just suppress it.
So the set of all linear bounded operators from H to itself is just going to be called B of H.
So, given every bounded operator from H to K, there is a unique A star,
which goes from K to H, such that, and it's uniquely defined by this condition,
XI eta equals XI A star theta.
One way there's always this adjoint going back the other way,
and this sort of follows the abstract nonsense that I'm leaving out is that
between any two vector spaces, there's always an adjoint which goes backwards from the duels,
and the thing about Hilbert spaces is that the dual of all bounded linear functions
is isomorphic to the space itself in a product.
So that's a little theorem at the beginning of Hilbert space.
Anyway, I'm sort of assuming it and to prove this theorem.
It's going to be more interesting to us.
So all of this to say that B of H is an involution.
A going to A star and it's conjugate linear.
So this is a very important piece of structure on Hilbert space,
and we're seeing it on the algebra of bounded operators.
Between Hilbert spaces is this conjugate linear involution defined by this formula,
and its existence and definition doesn't actually depend on which way I'm assuming
the conjugate linear with us in this variable or this variable.
Either way, it comes out to be the same operator and this map is conjugate linear.
So A going to A star is simply a conjugate linear.
All right, so our reflexes are supposed to be building up as we have this notion.
First thing, you've never seen it before, what does it mean in finite dimensions?
Well, in finite dimensions, with the Hilbert spaces C to the N,
every linear operator has a matrix with respect to an also normal basis,
and this operation is nothing but the conjugate transpose.
Conjugate transpose.
In finite dimensions.
But in infinite dimensions, it's also the conjugate transpose in infinite dimensions.
But I have to be a bit more careful here, so the first thing is that any matrix A
has a matrix form, any A belonging to B of H has a matrix form, which is AMN,
and by definition is AM is an also normal basis.
Okay, so it's sort of obvious that any bounded operator is specified by a matrix
and the star operation is the conjugate transpose for this matrix.
What's, of course, much more subtle in infinite dimensions is what matrices define bounded operators, right?
That's sort of not so obvious at all.
If you give yourself an infinite matrix, when does it actually define in the obvious way
a bounded operator on Hilbert space?
Well, that sort of gets back to this sort of Goldilocks question.
What matrices, the Goldilocks matrices define bounded operators on Hilbert space?
Well, that's not so clear.
Yeah, okay.
Well, so throughout these lectures, I reserve the right to put it either way,
an ongoing exercise to figure out which way it should be.
To keep you on your toes, fix a convention from the start and fix all of my formulae
so that they're actually correct.
I'll do this also when we come to groups.
There's some inverses that should be one way and the other,
and I will make it an exercise to make all of the formulas that are right actually correct.
So inverses and left and right actions, well, I'll write down what I like
and the exercises for you to correct the differences.
All right, so we spent quite a bit of time on this adjoint,
and the main thing that I'm going to use it for is to define the next kind of operators.
Well, actually, let's go back to unitaries.
So the conjugate linear condition is simply that lambda,
if lambda is a scalar, then lambda a star equals lambda bar a star.
Linear, in the usual, well, in the additive sense,
but when you multiply by scalars, you have to take the complex conjugate of the scalar.
All right, so let's revisit unitaries.
Well, it's fairly easy to see that unitary is the same thing as this.
We can actually say what the unitary condition is simply in terms of this nice little formula here.
U equals U, 1 equals U star U.
And since I'm doing this a bit out of order, I seem to be in the slightest space.
Obviously, there's a definition which is beginning to be made,
which is forget the second part.
And now is this right?
One of the parts.
I guess it's U star U equals 1.
This is by definition.
There's two notions, unitary and isometry.
And remember, a unitary is a surjective isometry.
And if you write it down, you'll see that this condition is exactly the one that says
that it preserves the inner product.
I think this is correct.
Okay, now, so all of this to get to the next very important definition which is self-adjoined.
And this is, if we're talking about B of H, the algebra.
It's an algebra, obviously, you can compose and add bounded operators.
Self-adjoint means that A equals A star.
So there's a unitary isometry, self-adjoint, A equals A star.
So this means I can spell it out.
A psi equals A psi eta equals psi eta.
You know, so it only makes sense for operators from the Hilbert space to itself.
It wouldn't, you know, A wouldn't make sense if it was different Hilbert space.
Okay, and before I give you examples of this, let's do this very special case.
And this is probably going to be the most important kind of operator in the whole,
the course dealing with, from the moment I was at, is among self-adjoint operators.
So it's these very special ones that we call projections.
P is the condition that it's self-adjoint and is, what's the word?
Item-potent.
When you square it, you get it back.
So these conditions, self-adjoint item-potent, is extremely restrictive.
And it's part of this basic structure of Hilbert space that this is the same thing as,
if and only if it's the orthogonal projection onto a closed subspace.
As to say somewhere in your Hilbert space you've got a subspace,
if you have Xi here then P of Xi is the nearest point in the subspace to Xi.
So this is often the case, you know, we have a simple formula giving the structure of the operator
and then some more verbose but perhaps more revealing sentence about what it is.
With unitaries it was some kind of generalized rotation and for projections it's orthogonal projection.
So this means that our reflex once again, finite dimensions,
what does it look like in finite dimensions while a projection, according to a basis,
you can take an orthogonal basis which starts with the subspace P contained in H,
starts with a basis element being in P and finishes off in the orthogonal complement
so we see that it looks like a bunch of ones and then a bunch of zeros down the diagonal.
The projection looks like in infinite dimensions it's the same thing
except that there may be infinitely many ones here and infinitely many zeros there.
So that's a very important kind of operator and they can occur,
they can occur, well let's see how they can occur in a rather more complicated form later on.
We'll see so many projections as we go that I don't even need to bother.
So those are the three most important ones.
Let me add this last one which I'll say a bit more.
Partial isometry which is somehow the union of the notion of projection and unitary
and this is simply the property that UU star is a projection.
Or we could sum that up UU star squared equals UU star.
There's the box formula for it but it's the same as lots of other ones,
UU star U equals U.
Well anyway, this could be U star UU equals U star.
I don't want to overload the poor student with exercises
because there could be hundreds of them in this little beginning
but this is sort of a level of exercise.
But the point is the structure of a partial isometry is that we draw
sort of nonlinear representation of things.
Here's the Hilbert space H, here's some subspace P
and a partial isometry between Hilbert spaces will take the vector here,
we're foggingly projected onto there and then isometrically ended to U of P
in some other Hilbert space.
This is called the initial domain and this is called the final domain of the partial isometry.
It's a very nice sort of geometric idea.
Alright, but we have to get back to self-adjoint operators
because there's a non-trivial theorem about them.
So let's immediately say what they are.
Remember we have this reflex in finite dimensions.
Well in finite dimensions there's this famous and often much used notion
of the diagonalization of matrices and according to that
any self-adjoint operator in finite dimensions is diagonal with respect to some basis.
So there exists a basis with respect to which the operator looks like that
with the lambda i's being real.
So that's good, now we know what all self-adjoint operators look like in finite dimensions
at least with respect to the appropriate basis.
And the first really non-trivial thing that we're dealing with here in this course
is what happens in infinite dimensions.
Is there an analog in infinite dimensions of this simple diagonalization of matrices?
The answer is yes, but it's a bit of a big deal.
I am of course going to assume that you already know most of the things about us
called the spectral theorem.
So contrary to what is taught in some physics courses
not all self-adjoint operators are diagonalizable.
In fact it's sort of rare.
But there is a theorem called the spectral theorem
and I'm going to sum it up with some version of it which says the following.
This version has the advantage of actually being relatively easy to understand
and manipulate without really understanding the proof which is good and a bad thing.
It's bad and it's not really kosher to do mathematics when you don't know how to prove things
but it's good in that proof doesn't necessarily help you figuring it out in examples.
The spectral decomposition is often is not by good guesswork
and by following through the details of the proof.
So anyway what is it?
Well it says that given A equals A star
then we can view the Hilbert space
then H becomes isomorphic
so there's a unitary transformation to L2 of some measure space
in such a way that A corresponds to multiplication by F.
This is the operator MFG.
You have to unravel this notation on X.
So it's unravel this notation.
G is supposed to be a function, an L2 function on X mu.
F is some function so about what kind in a second.
Then we take the multiplication operator MF takes MF of G
and evaluates it on a point X simply by multiplying F of X by G of X.
It's sort of a nasty looking formula but let's go to find dimensions.
What does this mean?
Well here, let me forget some multiplicity subtleties.
The measure space in this case would be the vector space of all.
The measure space would be the set from 1 to N
and the function would be the eigenvalues.
The main thing about the function F is that it's L in front of you.
So it's a trivial observation that if you have a bounded function,
essentially a bounded function,
then this multiplication operator actually gives a bounded operator on the Hilbert space.
It's just a trivial inequality with the integral.
And F of X is real value for almost all X.
You can fix it up, it misses.
And the norm of A is the norm of MF.
And this is another one of these interminable exercises
which is the essential supremum of F of X.
So in finite dimensions this essential supremum becomes trivial
and the norm of a matrix is just the largest eigenvalue and the absolute value.
So that's a very important question.
We have two things.
The spectral theorem says that there are two notions that are equivalent.
One is a self-adjoint operator on the Hilbert space
and the other is a multiplication operator
from one L2 space to another.
The spectral theorem says that given any A,
there exists a measure space such that A,
such that any unitary operator from this Hilbert space to this one
which transports A to multiplication by F.
Okay, so we want an example.
And a good example is to be obtained by the Fourier coefficients
and seem to be blanking on how exactly to do that.
Okay, so we should do that over EG.
If you have...
No, that's too complicated to start with.
It's sort of a bit tricky to get an example right away
because the example I want to give you is a multiplication operator on an L2 space.
Okay, where the spectral theorem is rather trivial
because it's just the statement that the operator exists.
So let me put that off as a debt to give you a nice example
using Fourier transform of how...
No, let's do it right like this.
The thing is I'm going to do it in reverse.
And for some reason that's the way I prepared this to do it in reverse.
So what I'm going to do is I'm going to sort of...
This is a pretty crazy thing to do but I'm going to start off with an operator
which is already diagonalised.
We already have its spectral decomposition
and then I'm going to convert it to a more complicated operator
in a different Hilbert space.
So let's take L2 of t
and let's take Mf to be...
We're going to take some f belonging to L infinity
or S1 of t.
I'm going to take Mf.
Okay, so that's a perfectly fine operator
which is beautifully diagonalised on L2 of t
and it's going to be complicated by taking the Fourier transform
and the fact of life is the f Mf f inverse.
So I have the Fourier transform and I remind you that that goes from L2 of z
to capital L2 of t.
So f inverse goes the other way.
So this isn't going to be an operator which goes from...
This should go from L2 of z back to L2 of z.
It's just I've taken this operator
and I've transported it back to little L2 of z using the Fourier series
and the answer here is this is the first result in Fourier series.
This is the operator of convolution by Fourier series.
On f itself.
Possibly up to some complex conjugates and minus signs.
This is more how we want it to be.
So to see this as an example of the spectral theorem
and it's a very important example,
we could just say that if someone gave you the operator of convolution
on little L2 of n,
then its spectral decomposition would be multiplication
by the Fourier series given by that sequence.
And slightly more advanced,
if you took instead of taking L2 of the circle,
if you took L2 of the real line,
then this is also a thing except that we'll be talking about Fourier series
rather than Fourier transform, rather than Fourier series.
So that's certainly, well,
it's a bit of a different world which we have to come to in a second.
All right.
So there we are up to, yeah, sure.
Since does this formulation help you to get information about this spectral
eigenvalues equation?
Right.
So for that, I guess I haven't really prepared this,
but let me say it quickly.
Operators in general on Hilbert's base,
even self-adjoint operators don't have any eigenvalues,
but they do have spectrum,
and the spectrum of any operator of A,
well, let's use X belonging to B of H,
is by definition the set of all lambda belonging to C such that
X minus lambda times the identity is not invertible.
So obviously if you have an eigenvalue,
and there's no way that there's something in the kernel,
so it's not invertible,
but in general, infinite dimensions being not invertible is much more subtle
than just having an eigenvalue,
and you can see that immediately by these L infinity functions.
For instance, if you took multiplication by X on L2 of 0, 1,
this actually has no eigenvalues,
and the eigenvalue would have to be a sort of point mass at some point,
and they just don't exist in L2 of 0, 1.
But it is true that this Mx minus X, Mx,
the operator Mx minus Z times the identity for Z belonging to 0, 1,
is not invertible.
Even though there's no eigenfunction, it's not invertible.
So this is the spectrum, and in general the spectrum,
this is called the spectrum sigma of X,
and in general the set of all, the spectrum of multiplication by F
is the set of all essential values of F.
Since everything's defined up to sets of measures 0,
there's some values that you can throw away,
but there's some values that keep on coming back at you,
even if you try to throw them away,
and that's the essential values, and it's the spectrum of M of F.
Good if you like to find the essential values
as being the spectrum of the multiplication operator.
Once again, in finite dimensions, everything is very simple.
The spectrum is just these eigenvalues.
Does that answer the question?
Up to questions of multiplicity, you can realize that X mu
as being the spectrum of the operator with some special measure.
There's more detail on the spectrum than I want to go into,
probably more than we'll need, of course.
Well, I know we're near enough if you want to work on the subject.
All right, I'm going to stop and have a 15-minute break after the hour.
I'm not doing very well.
I have a total of 45 bullet points to cover in this lecture,
so I'm up to 10.
We're not doing very well in terms of time.
That's a problem I'm going to have to solve somehow.
I have to say I expected it.
I wasn't so optimistic that I could get through all this stuff
at this level, at this speed.
But now I want to have possibly the most important words so far
with the connection with physics.
So this is what every mathematician should know about quantum mechanics.
He should probably know a lot more, but at least this basic stuff.
It sort of motivates the entire study of von Neumann algebras,
and it was certainly one of von Neumann's main motivations
for introducing von Neumann algebras, connection with physics.
So the idea is that for very fundamental reasons,
almost philosophical reasons,
if you have a quantum system,
the idea is that a state, whatever that means,
a quantum system is a unit vector in some Hilbert space.
Now, I need to say a lot more about this statement
in some sense what I mean by state,
and indeed I'm really talking about what are called pure states,
and I don't really mean a unit vector.
I mean a one-dimensional subspace.
A one-dimensional subspace is going to be defined by a unit vector
up to a phase, right?
And this is one of the main sources of confusion
and lack of understanding by anyone about quantum mechanics.
Some people understand it better than others,
but this ambiguity of states represented by vectors up to a phase
can be very confusing, right?
So without telling you what a state is,
except for some intuitive notion that you might have,
system is in a state of it is what it is.
Let me say that the second part of the correspondence is observables.
So if you have a state of the system,
so to be immodest the whole universe,
to be more modest maybe an electron somewhere,
then you can try to observe certain things on that system,
such as the position where the electron is,
what its charge is, how fast it's going and so on.
And the idea is once again for fundamental, almost philosophical reasons,
given to us by the founders of the subject
and sort of verified by experiment to amazing precision,
observables on the system are given by self-adjoint operators.
I'm going to have to say more about this later, but I will.
So self-adjoint operators on the Hilbert space,
so we all know what they are,
and they have a spectral decomposition, A,
so this is charge or something, spin or something like that,
and under that spectral decomposition,
the Hilbert space will be L2 space of some measure space.
The measure space might as well more or less be the spectrum of that operator,
so make it be the spectrum of that operator.
I'm simplifying multiplicity questions here.
But some L2 space, the spectrum of A is the values of the observable.
When you actually make a measurement say a spin,
you might get a number 25 and a half or something like that.
Well, the set of all possible values that you might get
is called the spectrum of the operator.
And the elements of this Hilbert space,
this is the Hilbert space of states of the system,
the elements of the Hilbert space are called wave functions.
And here we go with the thing that ties it all together is this magic formula,
which is A psi psi.
So this is an observable, and this is a state,
pure state, whatever that means.
A bit more about it will be revealed when I say what this is.
So this is a purely mathematical thing, we have a Hilbert space,
we have a unit vector in it, this number A psi psi,
first observation is it's defined on the state,
not just on the vector, because if you change this by a,
if you change psi by a number of modulus one,
you get the same number, okay?
It's a real, because this is self-adjoint,
you can bring this over here by complex conjugate,
you get a real number.
And as I said, this is the magic word,
the magic words that tie everything together
make some sense of the word state and observable,
and it's the average value
of the observation of observing A,
A, if the system,
I haven't said what that is either,
it's prepared in the state, sorry, in the state.
So the sort of idealistic thought experiment
is to imagine a physicist in this lab,
and he's looking at a meter,
and he's preparing the system,
and then he's observing something using some piece of apparatus,
and every time he prepares the system he'll get some number,
which gives the value of, say, where the particle is,
what it's been is,
and in quantum mechanics this is not a deterministic thing,
it doesn't always get the same thing,
but if he does it again and again,
sets up the system in the same state,
prepares everything in exactly the same way,
then this number is the average value
that he's going to get after repeated measurements
of the observable A.
Okay, so this is what ties it all together,
this shows that Hilbert's base, you know,
is the essential mathematical ingredient
for studying quantum mechanics,
and this gives us sort of the beginning of a dictionary.
This is a mathematically precise formula,
and this is a whole lot of vague and precise words,
but this sentence here,
the average value of observing A
is the thing that allows you to make a bit of sense
of what a state is, what a system is,
what an observable is,
and how this correspondence goes between Hilbert's base.
Lots of good books on the subject.
One of them, written by von Neumann himself,
goes into a lot more detail than what I just said,
particular projections, play a role,
and of course we have the spectral theorem
playing a magnificent role in the center
of this structure.
So for instance, we all know there's this duality
between position and momentum,
and that duality is completely explained mathematically
by saying that you have L2 space.
The position is being observable,
you get one L2 space,
if you take momentum as being the observable,
you get another one, and it's actually the Fourier transform
that goes in between these two.
So L2 space, it's the same Hilbert space,
but it's realized a different spectral decomposition.
So you can have wave functions in momentum space,
wave functions in position space,
representing the same physical system.
It's just the spectral theorem applied to different operators.
Okay, so I'll stop there.
I was certainly at full 15 minutes of break,
and then we'll come back to the next topic.
Next topic on the list, yes, is unbounded operators,
and since I've said that you can't actually exhibit them,
there's obviously some kind of clash that's going to occur
right at the start of the second part of the lecture.
Okay, thanks.
Going again.
To our lecturers, a long time is long for me,
so I'm not even sure I'll make it till the end.
So just to ease back into the area
just to get our minds back in the same game,
let me tell you about a very nice structural theorem
which ties up all of these kinds of operators
that I was talking about, and that's the so-called polar decomposition.
So we all know that if you have a complex number,
the polar decomposition, we just write it as a phase
times its absolute value.
Well, there's a completely analogous theorem for operators,
which is the so-called polar decomposition,
which says that we take any operator,
this is any element of B of H.
In fact, it can be any element from one Hilbert space to another,
and it has a unique decomposition as a phase,
whatever that means, and I'll explain the second time,
it's an absolute value, and I have to explain what that means.
Well, the point is, what about the absolute value?
If you take A, the linear operator between two Hilbert spaces,
and you take A star A, then this is sort of
tautologically self-adjoint, right?
This is equals A star A.
And it's also tautologically, the spectrum is positive.
The spectrum of A star A is contained in plus.
That's, you can prove, I mean, you know,
it has to be that way, right?
By intimidation, if that's not true,
then mathematics falls apart.
So it's certainly true, you can also prove it
without most difficulty.
Therefore, by the spectral theorem, you can take a square root.
The main function of the spectral theorem
is to actually take functions of operators,
because if you realize an operator
as a multiplication operator by F,
and you have some function that you can take of F,
and you take multiplication by that function of F,
and that allows you to take any kind of,
all sorts of functions of operators.
In particular, the simplest one,
perhaps the most useful one of all,
is simply the square root of a positive function.
So this is the absolute value of A.
And then, once you've done that,
it's a simple exercise to discover
that there is a unique partial isometry.
U is a partial isometry.
U is a partial isometry,
and it goes, remember,
partial isometries are just,
you inject onto a subspace,
and take it isometrically onto another subspace,
and this one takes the image of the absolute value of A,
takes the closure of that,
and sends it isometrically onto,
sorry, the image of A, right?
So once again, it takes the image of the absolute value of A,
and sends it isometrically onto the image of A,
and you have to take closures in order to get,
help with spaces.
And those words, if you just write them down,
allow you to prove this partial decomposition tautologically.
There's really only one U that fits the bill.
You know exactly what it has to do,
or exactly where.
This is a unique decomposition.
It's unique with respect to U being a partial isometry,
whose initial domain is the image of the absolute value.
Or its final domain is the image of A or whatever.
Okay, so in final dimensions,
this is a non-trivial and useful theorem,
known under a name,
probably several different names for decomposition of matrices.
I can't think of any of them right now.
Maybe someone in the audience knows what this means
when you write a matrix as a,
partial isometry times a positive matrix.
I give values of the absolute value of A
called the singular, that's the singular value decomposition.
This is the singular value decomposition of matrix.
All right, now as promised,
we're going to talk about unbounded operators.
So as I said in the beginning,
it's a real challenge to find an unbounded operator
from one Hilbert space to another one,
one that's not continuous.
And it's somehow not of all that much interest in the subject.
Well, let me be honest and say that
if you take the appropriate axioms of set theory,
it follows that every operator between Hilbert spaces is bounded.
So what am I talking about?
This is supposed to be useful and versatile,
not depend obviously on set theory.
So it's not the same notion.
What a Hilbert space person means by an unbounded operator
is not an operator that's defined everywhere.
So an unbounded operator is going to be an A
and it's going to be a linear operator from,
but not from one Hilbert space to another,
but from a subspace of the domain.
Which is some subspace of Hilbert space to some other Hilbert space.
It doesn't have to be defined everywhere.
That's what allows it to be both unbounded and interesting at the same time.
So that's the definition of an unbounded operator.
It's simply defined as a...
And to avoid trivialities,
let's suppose that the domain of A is dense.
Dense subspace.
So if you want to get examples, we have them immediately.
We have two ones that we're going to deal with immediately.
So example one.
Remember that if we have L2 of a measure space,
then we said that if you have a bounded function,
you can define a multiplication by an operator that's obvious.
But if you have an unbounded operator,
sorry, a function that's not bounded,
you can still define an operator
and it will be one of these unbounded operators.
So suppose we have F, which is a measurable function.
One X U.
Then we can define MF from L2,
now, from this domain of MF to L2.
And the domain is simply all functions for which it makes sense.
So the domain of F.
A set of all G's, such that the integral of F and X,
G of X,
in other words, if we try to define a multiplication
operated by multiplying by F,
it spits out another L2 function.
So we sort of meet these unbounded functions
in a most natural way if we try to extend
multiplication functions from bounded functions
from L and F functions that are more general.
So I mean you really might have a function like this.
Real life, right?
Your orbit space might be L2 of the unit interval
and you might have this function, one over X.
You might say, well, I really want to consider this
as something to do with Hilbert space,
an operator on Hilbert space.
Well, you can, but you just have to restrict it to make it.
Another example is BDX from L2 of R to L2 of R.
So now you see I'm doing something which is really not allowed.
I'm saying there's an operator from all of L2.
Well, it's not because not all L2 functions are differentiable.
But I might, this would be an abusive notation
and the actual meaning of this formula would be
DDX and the domain would be all functions
that are first of all differentiable
and second, if you differentiate them, then that's L2.
So I might just, this is a shorthand for an operator
whose domain is held to differentiable functions whose image
under differentiation is also held to.
And the Fourier transform is the key to understanding this
because that will Fourier transform diagonalizes differentiation
and makes it multiplication.
So that rather mysterious question of what functions are differentiable
and how L2 derivatives simply becomes Fourier functions
whose Fourier transform is as far as this condition is concerned.
So there's a nice example of mathematics in action.
What else do I have to say about an operator?
No, right.
So just to define something as an unbounded operator
doesn't mean that you've nailed it in terms of Hilbert space
because it could be that even, you know, you can define it
on dense domain but Hilbert space still doesn't see it at all.
And so now there's a sort of section which is due to von Neumann
about closed unbounded operators.
And the idea is this, that if you have an operator A in its domain,
so this is an unbounded operator,
then let me just sum it up by saying this one.
You want to define an operator A star, right?
You get a Hilbert space and every time you have an operator
you better have a star.
So you can talk about self-adjoint and so on.
So the way you define this is it has to have a domain, A star.
And the domain of A star obviously has to be a suitable eta
such that if you take, such that the following formula makes sense,
so that A psi eta.
Remember the formula for the star morally is this.
You've got to be able to pull this from the other side.
So if you take this and if this is a bounded linear functional of psi,
then by Hilbert space structure that means there is a vector A star such that,
well, instead of all eta such that this is a bounded function of psi
and in this case,
if this happens to be a bounded function of psi,
then by Hilbert space duality there has to be a vector A star eta
such that this will hold.
A little game being played here.
It takes a bit of getting used to it a little bit,
so there's really nothing to it.
You take the, given any unbounded operator A,
you take the set of all eta such that this function here defines,
it's a linear functional on the Hilbert space,
if it defines a bounded linear functional,
then there has to be a vector A star A to by Hilbert space duality.
So that formula is true.
That defines A star.
So this defines A star.
However, if you just give yourself any old operator to find on dense domain,
guess what?
You're not going to get any athers for which this gives you a bounded linear function.
You've got to be in special situation for such things to exist
and obviously what you really want is for the domain of A star to be dense.
These are the operators that Hilbert space is going to be extremely relevant to.
You have an unbounded operator with dense domain for A star.
What are these?
So the star operation actually makes sense.
And there's a very simple, well, very clever structure due to for moment.
If the domain of A star is dense,
then we can take the graph of A,
which is the set of all A psi of A,
psi A psi,
such that psi belongs to the domain of A,
and this is a subspace of h plus k,
vector in h, vector in k,
and then the graph of A, the result is the graph of A closure.
The result of the graph of A is the graph of an operator,
actually if and only if, called closure of A.
So let me say it all again.
If you have an unbounded operator on the Hilbert space with a certain domain,
then typically the adjoint won't be defined anywhere.
And I've run into this more time than I care to think about.
I've wanted to study some operators and the adjoint doesn't make sense,
in which case you sort of give up.
But if there happen to be vectors for which this is true,
this is a bounded linear functional,
and there's enough of them, in fact it's dense,
then what you can do is you can close the operator A,
which enlarges its domain,
and you get an operator which is said to be closed.
I think we're closed means that the operator is closed.
So the definition, adA is closed,
if it's the graph of an operator, if graph of A.
So in general, given an unbounded operator A,
it may or may not have a closure,
if it does you say it's pre-closed.
A is pre-closed.
This is to say, so there's two alternative conditions,
one is that the domain of A starts dense,
and the other is that the closure of the graph is the graph of an operator.
So what happens in practice is that you give us,
suppose you were just given, let's go back to differentiation,
then you might say, well let's just look at Schwartz space.
Schwartz space, infinitely differentiable things,
vanishing rapidly at infinity, beautiful subspace of L2,
but we can certainly define derivative on that.
But on that space, the domain is not quite big enough for it to be closed.
Not big enough for it to be closed, so you want to add some more stuff.
What do you have to add? Well, that might be a bit mysterious.
But the point is, for that operator, it is pre-closed.
The domain of the adjoint is also Schwartz space,
at least contains Schwartz space, so it's density defined.
So you have a proper closed self-adjoint operator.
A closed operator is the closure.
Why the big deal about closed?
And here comes some miracles to the von Neumann.
So here's the von Neumann miracle.
So miracles.
Obviously we're going to define a equals a star
for unbounded operators,
which means, by definition,
a domain of a star.
And also, whenever this makes sense,
or you would ever excite it in the domain of a.
So self-adjoint has to be not just this thing, this is called symmetric,
but the domain of a star has to be equal to the domain of a.
So being self-adjoint for unbounded operators is a bit more than for bounded operators
because of the domain.
The miracle one is that the spectral theorem works as before.
So if you have a self-adjoint operator,
where we mean the domain of a is equal to the domain of a star,
then there exists a Hilbert space, L2 of x mu,
and there exists a function this time, not necessarily bounded,
but a measurable function such that there's a unitary transformation,
the Hilbert space you're given to the L2 space,
that the operator becomes multiplication by f.
So that's sort of amazing.
You can prove it by reducing the unbounded case to a bounded case
with some kind of transform.
But this is the one that really convinces you that you're onto a good thing.
Just the polar decomposition works on the nose as well.
So in other words, given a general closed unbounded,
this is a closed unbounded operator, closed,
then there exists a unique decomposition
where this is a closed self-adjoint positive, well, in fact.
So here's the theorem.
The first miracle is that if a is a closed unbounded operator,
remember this is something to be dealt with with a lot of caution
because a has a domain, sends vectors in the domain to other things.
Well, it turns out that the domain of a,
if you apply something in the domain of a,
you end up in the domain of a star,
and a star a, the domain of a star a is the domain with adjoint,
and this is a self-adjoint positive operator.
So already, just considering this guy,
and the fact that it all, everything works out that, you know,
it just turns out that this is an honest self-adjoint operator,
is in my opinion an absolute miracle.
But once it is, then you have no trouble taking it square root by the spectral theorem,
and then this becomes as simple as before, a bit of a formality.
Okay, so you can always do this.
This is a partial isometry.
This is the square root of a star a.
And now in a much more subtle context,
because as I said, this is closed unbounded,
you have to show a star a is actually honestly self-adjoint
in order to be able to apply the spectral theorem to take it square root.
Terrific.
So, for example, now we can have a real live example,
which gets us into some of the classical stuff.
If we take L2 of r, and we take IDD, no.
Right, IDDX.
Maybe one over IDDX exercise.
And this is, on its domain, which I've said before,
is a self-adjoint operator.
I want to do its polar decomposition.
Okay?
Well, its polar decomposition is the, all I want to say is that,
you know, it has positive and negative spectrum,
so the polar decomposition will be non-trivial.
All I want to say is that this u part is the Hilbert transform.
And this is the u part of polar decomposition.
Now, obviously the way to see it is to do Fourier,
to switch it into Fourier space where this becomes multiplication
and where the polar decomposition is obvious.
If you don't know what the Fourier, the Hilbert transform is,
it's the Fourier inverse applied to the operator
with multiplication by minus 1 and plus 1.
Right.
So, besides the mathematical reasons for studying unbounded operators,
and to me the most amazing is just this absolute miracle
that A star A on the nose is a self-adjoint operator,
if A is closed.
We need to connect it with physics,
because what I said about physics was a little bit of a lie.
Observables, self-adjoint operators, well, observables in physics,
typically are going to have unbounded values.
If you're looking at the position of a particle,
and it's moving on the real line,
then this position can be anywhere from minus infinity to plus infinity.
So, you have to be able to handle operators,
self-adjoint operators with a spectrum which is infinite.
Well, this is just made for the job.
So, physics, physics.
So, I have a question about spectrotherm.
Yeah.
In the form of a to an a in it.
Yeah.
Algebra in it?
Yeah, it is.
What it really is, is, so if we have A in the main of A to H,
and A equals A star,
the spectrotherm says that there's a unitary from H to L2
of some measure space.
So, U A star is multiplication by F.
F.
Therefore, now you can deduce the appropriate multiplicative properties
if you like, just by factors given by this unitary.
So, the second question is,
that is a distance power, right?
So, is it easy to construct lambda?
Of the, from the spectral theorem.
So, the answer is that it's an existence theorem,
and I said it before, but let me say it again.
Often, you obtain the spectral decomposition by a lucky guess.
There's a theorem that proves that it exists,
but to actually apply that theorem in practice may not be so easy.
The theorem, well, first of all, with the unbounded case,
the first step is to reduce it to bounded operators,
because then you get rid of all these horrible domain problems,
and then you prove it for the bounded ones,
and the bounded ones, the proofs always go by studying the algebra
generated by the self-adjoint operator.
Then you use the structure of that and so on,
but, you know, it's all very abstract nonsense.
So, if I just give you an operator and ask you for a spectral decomposition,
well, it's a bit stuck.
In finite dimensions, it's very simple, right?
You just give it a matrix, say a self-adjoint matrix,
you want to know the spectrum,
you just figure out when the determinant of lambda minus a is zero,
and you're done.
Well, in infinite dimensions, you have to understand things a lot better.
You know, diagonalizing, doing the spectral theorem of a single operator
might be a sequence of five papers in the annals of mathematics, right?
Right now, am I at least halfway through today?
Not yet, no.
So, I have now at least done the Hilbert space preliminaries.
So, we'll be able to make great progress
in knowing what we're talking about quite rapidly now.
We have all of this vocabulary.
We have the vocabulary of operators,
self-adjoint, unitary, partial asymmetries, projections.
We have the spectral theorem,
we have unbounded operators,
and their spectral theorem and the polar decomposition.
All of that is now defined.
So, now is the time if you're confused by something to ask a question
on any of these concepts.
Because we're going to start using them.
That's it.
I'm turning to the board,
giving you a chance to think up your questions.
Right, let me ask for questions again.
No questions.
All right, we go on to the definition now for women out there.
So, now I'm going to take the same attitude
as I did right at the beginning of the lecture.
So, assuming no one knows anything about them,
just as no one was supposed to know anything about Hilbert space.
But now, I'm going to take a modulo of the fact
that we know everything about
everything we've done so far, self-adjoint, operators, and so on.
Okay, so, the first thing is a definition
of a formalin algebra.
So, M subset of BLH is a formalin algebra.
Well, first of all, it's an algebra.
Let me not say that because it's, you know, sort of implicit.
So, algebra simply means I can compose and add.
I can multiply and add the elements in it.
This is the composition of operators.
So, there are three conditions,
two of which are pretty innocent.
Well, one of which is really innocent.
You're going to assume it's unital.
Now, the second one is M equals M star.
This does not mean that it consists of self-adjoint.
Operator means that if an A is an A,
A is an M.
So, that condition may look innocent,
but we'll see that it's very stringent, very powerful.
And the third one is simply that it's closed.
I have to say what I mean by this closure.
This is in point-wise.
It means that if you have a net, A n tends to A,
and if you have an A, it's right.
So, one of the moment's motivations
for introducing these structures
was to try to do some algebra in infant dimensions.
And obviously, in infant dimensions,
you're going to try to set yourself up
with the best situation you can to start with.
And this is pretty nice.
We'll see that this is an extremely important
niceness condition.
And this is, you might as well assume things are closed
if you want to get anywhere.
So, it's a natural definition.
And we'll see that it ties up with physics very nicely.
It's not quite clear yet.
Although, remember that obviously M equals M star
means that self-adjoint operators
is a very simple theorem that says X is
A1 plus A.
Any operators or any combination of two self-adjoint ones,
so, you know, this condition means
that self-adjoint operators, physics that's observables,
are going to be very important in the study
of formulae and algebra.
All right, so now let's have examples.
One example.
Let me start with a non-example.
Oh, sorry. Let me start with an example.
This is a really trivial example.
Total logically, you take all bounded operators.
They satisfy those three conditions.
It's my first theorem that I've actually proved today.
All right, now, example one is a non-example.
It's a very important non-example.
If you take a set of matrices
which consist of A, B, C, which have zero
in the bottom left-hand corner, okay,
this is an algebra.
It contains the identity.
It's obviously closed because we're in fine dimensions
and it's not self-adjoint.
This fails to be a problem.
All right, the examples of things that are
formulae and algebra are, of course, A, B, C, D.
Because that's the two by two matrices
and that's B of H when H is dimension two.
That's a formulae and algebra.
I'm thinking of all these things acting on C2.
And the other one is the diagonals, A, 0, 0, B.
This now satisfies all the conditions together with this one.
This is a formulae.
A non-formulae and algebra.
All right, example three, example two.
Now we're going to let H be inferred dimensional
and we'll take a basis.
So let's take the set of all matrices.
Okay.
So suppose we take a basis of Hilbert's base
and we take the set of all operators
which are the form, some matrix,
but eventually everything's zero.
This is an algebra.
Oh, it fails to contain the identity.
Well, I said that that was an innocent condition,
so let's go on.
Let's go to the joint.
Okay, but it does not.
This identity thing is sort of trivial
because you can always throw it in and consider the example.
But what it really fails on is being closed
because obviously anyone can figure out examples
of small chunks of matrices which converge
to operators that are not small chunks of matrices.
So this fail, this is a non-example.
For a different reason from the finite dimensional one,
this is a non-example because it's not big enough.
It's not closed.
Well, we can try to close it.
The first attempt to close it would be to say,
well, make this basis independent.
We could take all finite rank operators.
This is just operators that,
with respect to some basis, look like this.
It's obviously an algebra.
Fails to contain the identity.
It's self-adjoint, but most of all, it's not big enough.
It's not closed.
Then we can take one step further.
I have to use a little bit of functional analysis now
and you can look at all compact operators.
Making it bigger.
Compact operators are all operators
which are norm limits of finite rank operators.
They are operators.
Well, there's other ways to define them.
But the point is that this is still not big enough.
In fact, it doesn't contain the identity
and it's certainly not closed in the topology
that I said had to be closed.
It's closed in the norm topology on operators,
not in this topology.
And the only way to actually make this big enough
to be a phonomenal algebra is if it's all of b of 8.
So once again, let me go over these things
by example and non-example.
If you want infinite dimensions in the 2 by 2 matrices,
these things, these sort of non-self-adjoint things
are not phonomenal algebra.
Nothing of our theory, we say,
is going to apply to a simple algebra like this.
Three-dimensional algebra matrices.
It's not in our sites.
They ruin Japan.
On the other hand, if we take the diagonal matrices
that's self-adjoint, closed by the law,
therefore it's phonomenal algebra.
Infinite dimensions things that not only have to be self-adjoint,
but they have to be big.
They have to be closed.
That's the topology of point-wise convergence.
So if we start off trying to get a phonomenal algebra
by taking all finite size matrices
and closing it, then if we try to close it,
we're going to end up with all of b of h,
which is tautologically an example of a phonomenal algebra.
Our first job is going to be
to construct more interesting examples of phonomenal algebras.
And we can get one rather immediately
from...
Here we go with example three.
I'm going to state this as an exercise.
So if we take L2 of x mu,
let's make it even more concrete,
L2 of our favorite t,
L2 of the circle,
then I claim that L infinity functions in the circle,
acting by...
Yeah.
Remember if we have an L infinity function
of x on the Hilbert space,
the L2 function by point-wise multiplication,
then I claim that this is a phonomenal algebra.
And obviously there's an assertion
for arbitrary measure spaces as well.
So let's just check that it satisfies the axioms.
Well, first of all, it's an algebra.
If you add and multiply functions, that will work.
It contains the identity.
The constant function equal to 1 is in infinity
and it acts, obviously, by the identity.
It's self-adjoint because if you take Mf star...
Mf bar, that's a sort of intimidation result.
If that wasn't true, then the whole world would fall apart.
Finally, the fact that it's closed in point-wise convergence.
Well, that's a measure theory exercise, I guess.
We'll see another way of doing it, much better later on.
So there we've got examples of phonomenal algebra.
