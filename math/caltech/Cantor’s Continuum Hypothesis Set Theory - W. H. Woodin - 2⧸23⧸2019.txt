I want to thank Caltech for the invitation, it's great to be back, I first walked these
halls 45 years ago as a freshman, not this hall, but the previous incarnation of the
school.
Okay, I want to talk about the continuum hypothesis, I'm going to start slow, so I want to introduce
you to the ordinals, these are the trans-finite numbers, everything's a set for me, so simplest
set is the empty set, that's the ordinal zero, the ordinal one is the set is the only number
for the empty set, and take these two, you've got the ordinal two, in general with alphas
and ordinal, alphas just a set of all ordinals which are smaller than them, you can see that
from the example, to get the next ordinal alpha plus one you just add alpha to the set, omega
that denotes the least infinite ordinal, and it's the set of all finite ordinals.
So using the ordinals in a trans-finite induction, you can present the universe of sets, what
you do is you trans-finitely iterate a simple operation, the power set operation, so given
a set x, p of x denotes the collection of all subsets of x, and then you can form the
cumulative hierarchy of sets, so you start with nothing, at a successor step you take
the power set, and at a limit stage you just take the unit, and it's not hard to show that
this is an increasing sequence, if x is a set then x belongs to be alpha for some alpha,
that's a consequence of the basic axiom, the other way you could do it, you could define
this as the conception of sets, this construction, and then you could back out the axioms, either
way it's fine. Okay so let's look at these, we want to understand all mathematical questions,
our questions about the universe of sets, it's nicely layered for us, so why don't we
just look at it level by level, maybe it's not so hard. So v zero is pretty simple, that's
the empty set, v one, that's got to be the power set of v zero, and then v two's got
to be the power set of v one, well look those are just the first three ordinals, v three
has four elements, not an ordinal, v four has sixteen elements so far, not so bad, v
five has sixty five thousand five hundred and thirty six elements, so it's gotten a
little bigger, and then v a thousand, that's a lot, v omega is infinite, we just jump up
to the first infinite ordinal and it's a set of all hereditary finite sets, the structure
of v omega, that set with its membership relation is mathematically identical to number theory,
each structure can be interpreted in the other. Now as a set theorist, I think this is a more
natural structure, probably everyone here would disagree with me on that. Alright so
it's going beyond omega that takes you from number theory into set theory, so I want to
introduce you to cardinals, concepts that are equivalent in the finite split in the infinite,
an ordinal which is rigorously defined as a cardinal if every smaller ordinal has smaller
cardinality, so you can't find a bijection between alpha and a smaller ordinal. The
finite ordinals are all cardinals, omega is a cardinal, for each ordinal alpha, omega
alpha denotes the alpha infinite cardinal, so omega zero is omega formulae but we never
use that notation, omega one denotes the least uncountable cardinal, omega alpha plus one
is the least cardinal bigger than omega alpha. By the basic axioms of set theory, for every
ordinal alpha, omega alpha exists, so there are lots of cardinals, there are as many cardinals
as there are ordinals. It's also follows from the axioms, the set theory that if you have
an infinite set, then its cardinality is measured by a cardinal, so there is an alpha such that
the cardinality of x is omega alpha, that requires the axiom of choice.
Alright, so now we come to the continuum hypothesis, it starts with the theorem of cantor, if you
let n be the set of natural numbers and r are the set of real numbers, they do not have
the same cardinality, different sizes of infinity. Assuming the axiom of choice, the cardinality
of the reals is omega alpha for some alpha, which alpha. The continuum hypothesis is the
assertion that if you have an infinite set of real numbers, then either it has the same
cardinality as the natural numbers or it has the same cardinality as the real numbers.
So, assuming the axiom of choice, the continuum hypothesis is equivalent to the assertion that
the cardinality of the reals is omega 1. And yesterday, we were told there are only two
kinds of equations. That's an equation, it doesn't look like a differential equation,
so maybe there's a third kind, but enter the house of set theory at your own peril,
this question has driven many people crazy. The continuum hypothesis in mathematics is
a Wikipedia scholarship, I'm sure it's all correct. In 1900, at the second international
mathematical congress in Paris, Oprah Poe's now famous list of 23 questions, to guide
mathematics for the coming century, the problem of the continuum hypothesis was the first
problem on the list. In 1904, at the third international mathematical congress in Heidelberg,
Connick gave a lecture in which he claimed to solve the age. All of the parallel sessions
of the congress were canceled so that everyone could attend Connick's lecture. The announcement
was a sensation and widely reported in the press. The grand duke, the first of Prussia
had Felix Klein explain the entire matter to me personally, but the proof was wrong.
I'm not quite sure how that played out. Many others tried to solve the problem of the continuum
hypothesis and fail. Hilbert apparently thought he had approved for two years. The problem
of the continuum hypothesis quickly came to be widely regarded as one of the most important
problems in all of modern mathematics. In 1940, Gertl show that it's consistent with
the axioms of set theory that the continuum hypothesis be true. So you can't refute the
continuum hypothesis. And then the modern arrow set theory arguably began in 1963 with
Collins announcement at Berkeley on July 4th, that it's consistent with the axioms of set
theory that the continuum hypothesis be false. So you can't prove the continuum hypothesis.
The fact that July 4th is Independence Day, I'm sure it's a coincidence. What we say,
what these two results tell you is that the continuum hypothesis is independent of the axioms of set theory.
Now, I want to say a little bit about Collins method, nothing terribly technical. The ZFC
axioms, those are the axioms you would back out from that cumulative representation of
the universe of sets, just as you would back out the piano axioms from the standard conception
of arithmetic. These axioms formally specify, let's call them the founding principles for
the conception of the universe of sets. But you might be studying the universe of sets,
but you can look at toy universes or models. If M is a model of set theories, that's just
an algebraic object, then what Collins showed is that M contains blueprints for virtual
models of ZFC which enlarge M. And these blueprints can be constructed and analyzed within the
initial model. If M is countable, then every blueprint constructed within M can be realized
as a genuine enlargement of M. Collins proved that every model of ZFC contains a blueprint
for an enlargement in which to continue my hypothesis is false. So if there is a model
of ZFC, then there's one in which to continue my hypothesis is false. She's passed the
accountable model. Collins method also shows, but that was a slightly later discovery, that
every model of ZFC contains a blueprint for an enlargement in which to continue my hypothesis
is true. So what Collins really solved was the extension problem. Girdle, and I'll say
more about what Girdle did later, solved the submodel problem. Just so you have, the way
you have to keep in mind is a model of ZFC is an incredibly complicated object, so complicated
that you can't prove it exists. But what Collins, this is a leverage game. You can't prove the
model exists, but if you've got a model, you can use it to build other models. Okay, what
about Collins method? It's much more chaotic than just CH. The method has been vastly developed
in the five decades now since Collins' original work. Many problems have become to be unsolvable,
including problems outside SEP theory. There was a Whitehead problem, group theory, Kotlanski's
conjecture, Seusslin's problem, Burrell conjecture, and it's still happening. More recently, the
Browne done this, the more automorphous the problem. So it doesn't happen a lot that problems
that arise outside of SEP theory are shown to be unsolvable, but all of these did. Okay,
maybe it's time to give up. Maybe it just was a good attempt at SEP theory, maybe the
liquidity of unsolvable problems is an indication that it's an incoherent perception. We got
misled. Well, large cardinal axioms, which are axioms that assert the existence of very
large sets, are not proven by Gerl's second and completeness theorem. The large cardinal
axioms are falsified. So I'll make my standard prediction. No contradiction in the existence
of infinitely many wooden cardinals will be discovered in the next thousand years. Not
by any means whatsoever. I don't care if we encounter an extraterrestrial civilization
that's been doing mathematics for a billion years. Okay, that's a pretty precise prediction.
It's a prediction about our universe, next thousand years. A thousand years will know
whether I'm right or not. Now, the real claim, of course, is that there's no contradiction
in the existence of infinitely many wooden cardinals. Now, these statements cannot be
formal proof. They can only be refuted. This suggests there's a component in the evolution
of our understanding of mathematics, which is not formal. There's mathematical knowledge
which is not entirely based on proofs. So the skeptic is going to say you are comfortable
with arithmetic truth. So you have to regard any statement about the numbers as either
true or false. The claim that the existence of wooden cardinals does not lead to a contradiction
in arithmetic statement. Now, you could say it's not interesting, but you have to say
that it's either true or false. No one has come up with a method for arguing that large
cardinals don't lead to a contradiction except for the conception of the universe of sets
in which those large cardinals exist. Those don't seem to be any other way. Okay, so now
we can get trapped by the skeptic. All right, I'll buy that. Do you have this conception
of the universe of sets as populated, well, I guess you're like wooden cardinals that
are populated with wooden cardinals, whatever. Tell me whether CH is true or not. How can
you have a conception of the universe of sets and declare it as meaningful and talk about
the existence of really big sets if you can't answer the most basic question about small
sets? So now the tables have turned and we have to investigate CH. Well, in mathematics,
if you've got a complicated problem and you're trying to figure out the answer, why don't
you look at special cases? But how do you look at special cases for the continuum of
offices? You're not going to run off calculations. Well, you can look at simpler sets. So I'm
going to talk about, I'll call them the simplest uncountable sets. Now, this is not an entirely
standard definition, but it's the equivalent standard definition. First of all, let's look
at V omega plus one. That's the first stage in the cumulative hierarchy which is uncountable.
That's the canter set in disguise. Well, we'll call a subset of that to be a projective set
that can be logically defined in the structure of V omega plus one epsilon. We're really
talking about a second or another theory. But we also want to talk about relations. So
let's make the same definition. A binary relation on V omega plus one is a projective set if
it can be defined as a binary relation in the structure of V omega plus one epsilon. Okay,
so let's now state the continuum hypothesis. Let's not talk about reels. Let's talk about
V omega plus one. So the continuum hypothesis is equivalent to this statement that says
that if you have a subset of V omega plus one, think the reels, then either A and V omega
have the same cardinality, V omega is really N, or A and V omega plus one have the same
cardinality. But we're shifting to V omega plus one because we have a natural notion
of a simple set. Well, we've defined the simple set so we can projectify this statement. And
that's the projective continuum hypothesis. If A subset of V omega plus one is an infinite
projective set, then either A and V omega have the same cardinality, or there's a bijection
that has a binary relation, which is a projective set. Now you might say, why don't I have
here? Well, all the countable binary relations are projective. So if you have a bijection
between A and V omega, that's a countable binary relation and V omega plus one, it's
projective. So no need to isolate that. So all we're doing is taking the continuum hypothesis,
which is a statement about all sets, and we're localizing it to simple sets. Now we can do
the same thing with the axiom of choice, because that was another issue that Hilbert raised.
So first I have to tell you what the axiom of choice is. So if you have a subset of X
cross Y, arbitrary sets, a function is a choice function for that set. If we're all points
in A, if there is a point B which when paired with A is in the set A, then F picks such
a point. So it's just this. You've got X. These are not drawn to scale. This is A. And
that's the function. Okay, but we can projectify this. So in the axiom of choice is the assertion
that for every subset of X cross Y, there's a choice function. Well, we now can projectify
this. And we can define the projective axiom of choice. If A is a subset of V omega plus
one cross V omega plus one is a projective set, then there's a function, which is a choice
function for A, and which is a projective set. So in each case, the projective C, continuum
hypothesis, or the axiom of choice, we just replace set by simple set and we get statements.
Why don't we investigate these? They did. These were investigated in a slightly different
guise in the early 1900s. They're both unsolvable. In fact, in an amusing coincidence, if you
look at the actual constructions of Girdle and Collwin that showed that the continuum
hypothesis was unsolvable, shows these are. In Girdle's universe L, that's how he showed
that the continuum hypothesis was consistent, the projective axiom of choice holds and the
projective continuum hypothesis holds. So you're not going to refute either of these
from the ZFC axioms. In the actual enlargement of L provided by the blueprint, which Collwin
defined for the failure of CH, the projective axiom of choice is false, as is the projective
continuum hypothesis. It doesn't look like we've made much progress. We decided to look
at some special cases, and now it's all over. But the intuition that they had answers is
that it took a long time to get to that point, but that happened. So if you go beyond the
basic ZFC axioms, you have the large cardinal axioms that assert the existence of very
large sets. These such axioms assert the existence of large cardinals. So I'll give you some
examples. Measurable cardinals, wooden cardinals, and then a whole bunch. Names aren't so exciting.
The strength increases as you go down. This is not the historical order. These were discovered,
and these, these, these, these were discovered, and these are much smaller. Now you wouldn't
think that large cardinals would have much influence on small sets. Like take the projective
sets. Why would the existence of some enormous infinite set tell you something about the
projective sets? Remarkably, it does. So if you have infinitely many wooden cardinals,
then the projective continuum hypothesis holds. And then, rephrasing a more significant theorem,
Martin and Steele showed about a year later, if there are infinitely many wooden cardinals,
then the projective axiom of choice holds. So both of those problems remember were unsolvable,
but you bring in infinity and you get the answers. So this is looking now promising again. In
fact, I would argue we now have the correct conception of V omega plus 1 in the projective
sets. This conception yields axioms for the projective sets. These determinacy axioms are
in turn closely related to and follow from our cardinal axioms. So what really happened
is you have the piano axioms for arithmetic. V omega plus 1 is really second order number
theory, and we now have the analog of the piano axioms for second order number theory,
and it gives a rich, robust theory. Well, this looks good. We did the special case, so
let's go on. So I have to introduce you to the definable power set. If x is a set, p
of x denotes a set of all subsets of x, such that the set y could be logically defined in
the structure of x epsilon from parameters. So we don't necessarily take all the sets,
we take those which are somehow intrinsic to the set x itself. The collection of the
projective subsets of V omega plus 1 is exactly the definable power set. But wait a minute,
we have a conception of the universe of sets as iterating the power set operation. What
if we were placed the power set operation by the definable power set? And that's exactly
what Gertrude did. So let me remind you about the cumulative hierarchy of sets. Remember,
it was, we start with nothing. We iterate the path, we, at a successor step, we take
the power set at a limit stage, we take the union, and V is the class of all sets that
you catch this way. So we're just going to replace the power set by the definable power
set. This is, gives you Gertrude's universe L, which I've already mentioned. You start
the same way, but now at a successor step, you take the definable power set instead of
the full power set. At limit stages, you take the union, and then L is the class of all
sets x, such that x belongs to V alpha for some alpha. Okay, now this is a much more
slowly growing hierarchy. In general, L alpha is not V alpha. Okay, but this suggests an
axiom. The axioms of set theory tell us, for every set x, there's an alpha for which it
belongs to V alpha. Why don't we just formulate that axiom, but use this hierarchy instead?
And that's what Gertrude did. The axiom V equals L. Suppose x is set, then x belongs to L. So
there is an alpha, such that x belongs to L alpha. So iterating the definable power set
gets all sets. And Gertrude showed that if you assume V equals L, then the continuum
hypothesis holds. Now jumping forward to Cohen's forcing, if you have a Cohen blueprint for
V equals L, then the axiom V equals L must already hold, and the blueprint is true. Adopting
the axiom V equals L completely negates the ramifications of Cohen's method. This looks
great. There's a problem, obviously. Scott, before Cohen's work showed that if you assume
this axiom V equals L, there are no measurable cardinals. There are no large cardinals. There
are no wooden cardinals. Well, that's a little disturbing to me. The axiom V equals L is
false. So it was like a bait to switch. We got a great axiom V equals L. It's immune to
Cohen's method, but the universe of sets is supposed to have everything in it. It's supposed
to be transcended. And so the very principles we use to generate richness in the hierarchy
of sets tells us we have to reject this axiom. So what are we going to do? Well, somehow
the projective sets led us to the axiom V equals L and the construction, rewriting history.
Maybe we can, if we generalize the projective sets, we can find the ultimate version of L
or maybe the ultimate version of the axiom V is L. So the trouble with the construction
of L is you're iterating the definable power set. Maybe it should be a richer operation
and maybe the large cardinals give us an enhanced definable power set. And we should really be
iterating the enhanced definable power set. But then what's an enhanced definable power
set that all starts to look murky and very complicated? So let's try to do an end run
around it. So I'm going to talk about the universally bare sets. Now the projective sets are built
from below. You just define the omega plus one and you extract the projective sets. We
really want to get at a generalization of the projective sets that's not from below
in any sense. It should couple into the universal sense if it has any chance of giving us or
leading us to the correct generalization of the equal cell. Okay, so I'm going to work
in Euclidean space. It doesn't really matter. If you take a subset of our N, it's universally
bare. If it's continuous pre-images in any topological space, have the property of
bare differ from an open set by a meager set in that space. So if you vary the space omega,
this is either trivial or non-trivial. If omega is a discrete space, well it's pretty trivial,
if omega is Rn and you take pi to be the identity, you see that A has to have a property of
bare. And if you replace omega by the stone space of a measure algebra, you can get
Lebesgue measurability. So this is kind of a universal nice property. Universally bare
sets are Lebesgue measurable. They have the property of bare. They have all the classical
regularity properties. They seem quite interesting. Now if you assume D equals L, something very
strange happens. Every set of reels is the image of a universally bare set by continuous
function. So the universally bare sets are not closed under continuous images, but the
projectives sets are. Well, that suggests maybe it isn't the right notion. Or maybe it
suggests we need to bring in large particles to clarify the picture. So you can relativize
the construction of L in any set. And particularly you can relativize it to a set of reels. All
you do is you start instead of with the empty set, you throw the reels and the set of reels
in at the bottom, then you just iterate the definable power set. And then L of A and R,
that's L relativized to A and R, is a class of all sets you can generate this way. And
we're going to be interested in the sets of reels there. They're in some sense all the
sets of reels that must exist from the axioms of set theory without the axiom of choice
given that A exists. Very complicated set of reels. Okay, and these are in the right
context the ultimate generalization of the projective sets. So if you assume there's a
proper class of wooden cardinals, what in steel showed that if A is universally bare,
then A is determined, just like with the projective sets. Steel showed that if you have a
universally bare subset of R cross R, it has a choice function which is universally bare.
I showed that if you have a proper class of wooden cardinals and A is universally bare,
then all the sets you can generate from A by relativizing veritable construction are
universally bare. So in particular they're closed under continuous images and in fact
L of A and R models A and D where A and D is the axiom of the terms. So in the context
of a proper class of wooden cardinal, the universally bare sets are a very rich collection and they
have tremendous structure. So I want to talk about measuring the complexity of the
universally bare sets to motivate how we're going to use them. So I'm now changing a
standard definition slightly just to make things a little easier. If A and B are
subsets of the Euclidean line, we'll say A is weakly wage reducible to B if there's a
continuous function on the irrational such that either A is the preimage of B or it's
the complement of the preimage of B. So the idea here is that if you know A, if you know B,
modulo that function which is the only accountable amount of information in it, because it's
continuous on the irrationals, you can compute A. So from B you can compute A. It's a
complexity notion. We'll say that A and B are weakly bi-reducible if each is weakly
wage reducible to the other, just an equivalence relation, and so we can take the equivalence
classes. Well, it doesn't seem like a terribly useful notion until you localize it to the
universally bare sets. Soon there's a problem, this is combining many theorems, so soon there's
a proper class of wooden cardinals, then the weak wage degrees of the universally bare
sets are linearly ordered by weak wage reducibility. And this linear order is a well order. So
all of these very complicated sets are laid out for you in a hierarchy that's well ordered.
And this is a very fine hierarchy. The borrell sets don't even occupy the first omega one
level. Okay, so that suggests that we might be able to use the universally bare sets to
get the ultimate generalization of the axiom of the equals L. We wanted some kind of enhanced
power set. We've got a rich structure here. How are we going to use it? Okay, I'll have
to make some elementary definitions. Set M is the transitive set. If every element of
M is a subset of M, each V alpha is a transitive set. If M is a finite transitive set, it's
in V omega. And V omega is just a union of finite transitive sets. A transitive set is
like an initial segment. A member of a member is a member. That's what transitive is. A
sentence is a sigma two sentence. If it can be written in the following form, there is
an ordinal alpha such that V alpha satisfies psi for some set in psi. Something happens
in a V alpha. It's an incredibly powerful sigma two can say a lot. All questions of mathematics
classically discovered are classically looked at as sigma two sentences. Okay, so we wanted
to define V as ultimate L without defining L. How about we define V as L without defining
L? So for each N alpha, we could intercept all the transitive sets. That must be empty.
M such that N models ZFC minus power set. Now ZFC minus power set or the ZFC axioms we
could throw out the power set axioms, keeping infinity. You can still prove that every
infinite set has a cardinality. We want alpha to be contained in M, and we want the
ordinals of M to be alpha. So that's N alpha. We just intercept them all. There may be
none. Now the following are equivalent. The axioms of L, which I've already defined,
for each sigma two sentence V, if V holds a V, then there's a countable ordinal alpha
such that N alpha satisfies V. So all you have to do is, these are like, these are the test
models. And if you could reflect every true sigma two sentence into a test model, that's
equivalent to V is equal to N alpha. Alright, so that will be the template for trying to
generalize V equals L, but we need test models. Well, we're going to start with HOD. This
is another important definition of girdle. HOD, that's the redditorially ordinal definable.
It's the class of all sets X such that there exists an ordinal alpha and M is the alpha.
X belongs to M and M is transitive. Every element of M is definable in the alpha from
ordinal parameters. HOD is the class of all sets. This is a metal M, because it's not
for quarter. It's the class of all sets that there exists a transitive set M such that
X is in M and every element of M is definable in V from ordinal parameters. That's why
it's hereditarily ordinal. Now, you can work this definition of HOD in models where the
axiom of choice fails, no use of the axiom of choice. Girdle did that to give another
proof of the consistency of the axiom of choice for the axiom of set figure. At least that
was, I think it was 10. So, particularly we can work or define HOD L of R within L of
A and R where A is a set of reals, possibly a universally bare set. So, the notation as
the base of set of reals, HOD super L of A R is the class HOD as defined within that
universe. The axiom of choice always holds when you compute HOD and so that will be true
even if L of A and R is a model of the axiom of determinacy which denies the axiom of choice.
And the first connection between determinacy and large cardinals was this theorem was
solivated over 50 years ago now. Soon you have a set of reals and the axiom determinacy
holds in L of A and R then the omega 1 of D which is a measurable cardinal in the HOD
of L of A and R. HOD of L of R, A, this is not going to have all the reals so there's
no reason to expect that this is the omega 1 of this model and it's not, it's a large
cardinal. Well, this is looking promising. It was a slight problem. I suppose there's
a proper class of wooden cardinals and that A is universally bare then omega 1 of B is
the least measurable cardinal HOD. There's not enough richness in the HOD of L of R, L of
A and R below omega 1. So our test models can't be counted. We can't just cut HOD of
omega 1. So we need to look elsewhere. Well, a natural ordinal that comes up is theta of
L of A and R. That's a supremum of the ordinal so there's a surjection of the reals onto alpha
such that pi is in L of A and R. This is a kind of measure of the size of the continuum
from the point of view of L of A and R. And it really is very much like omega 1. Omega
1 is the supremum of the ordinals onto which you can map omega. So we're just moving from
surjective images of omega to surjective images of the reals or the power set of omega. Well,
something interesting happened. Suppose you have a proper class of wooden cardinals. A
is universally bare and kappa is greater than or equal to the theta of L of A and R. Then
kappa is not measurable. So below omega 1 and above theta nothing's happening. All the
excitement, if there is any excitement, has to be between omega 1 and theta. But there's
some indication that there's plenty of excitement because if you have a proper class of wooden
cardinals and A is universally bare, then the theta of L of A and R is a wooden cardinal
in the hog of L of A and R. So that ceiling in the large cardinal is actually a wooden
cardinal. And this is a symptom of a deep structural relationship between large cardinals
and average cardinals. Well, now we're all set to state the axiom B as ultimate. The
existence of a wooden cardinal, and I'm not going to define it, is expressible by a
sigma 2 sentence. Wooden cardinals exist in B. If A is universally bare and there's a
proper class of wooden cardinals in the hog of L of A and R, then there's a wooden cardinal.
So we've reflected a sigma 2 sentence true to B into the hog of L of A and R. So that
suggests our axiom. The axiom asserts there's a proper class of wooden cardinals. That's
to make the universally bare set behave properly. And then just for every sigma 2 sentence B,
if B holds to B, then there's a universally bare set A such that the hog of L of A and
R satisfies B. These are test models. And because of the weak weight-reducibility hierarchy,
if A is less complicated than B, in fact, the hog of L of A and R is computed inside
the hog of L of B and R. So these really, in some sense, fit together. So that's the
axiom or the candidate axiom. Now, because it's form-stated this way, and because these
sets are all models of A-D, we can now couple into the enormous theory that's been developed
in the scripted set theory in the context of A-D. A bunch of that took place here at
Caltech and at UCLA. So you can prove ax- you can actually prove links from this axiom.
So you can prove the continuum hypothesis. You can prove B is equal to hog. So suppose
there's a Cohen blueprint for B as ultimate L, then the axiom B as ultimate L must hold
the blueprint's trivial, just like that. Adopting the axiom B as ultimate L completely negates
the ramifications of Cohen's method, just like B equals ultimate L. But we have an issue.
The issue is we rejected B equals L because there was a large cardinal that denied it.
So we have that same problem. So we need to look at that. So I need to tell you a little
bit about the language of large cardinals, how they're formulated now. If you have two
transitive sets, a function from one to the other is an elementary embedding if it preserves
truth. So whenever you choose points in x and a formula, if x thinks that formula holds
at those points, that holds if and only if y thinks that same proposition holds about the
image points. The embedding preserves truth. Now, isomorphisms are clearly elementary
embeddings, but there are no non-trivial isomorphisms between transitive sets. All right, so I'm not
going to define, in fact, I'm going to define one large cardinal that was on my list, extendable
cardinals. It's nicely phrased in elementary embeddings. So first of all, I'll show you
something. Suppose that you have an elementary embedding from B alpha to B beta, then the
following are equivalent. It's not the identity, and that's one. And two, it's not the identity
on the ordinal. So a non-trivial elementary embedding from B alpha to B beta has to move
an ordinal. The critical point of J denotes the least ordinal. Remember, the ordinals are
well-ordered. So if J is not the identity, there has to be a least ordinal that J doesn't
send to itself. J has to be increasing. And then that leads you to the definition of an
extendable cardinal, which is Reinhardt's thesis. Delta is an extendable cardinal if
every lambda is bigger than delta. There's an elementary embedding from the lambda plus
one to the J of lambda plus one with critical point delta and J of delta bigger than
lambda. It's pushing up. Okay, so a few more definitions. I apologize. Transitive sets H
gamma for every infinite cardinal. Gamma H gamma denotes the union of all the transitive
sets M of cardinality less than gamma. So H omega is V omega. H omega one and V omega
plus one are logically given structures just as V omega plus one, number theory R. H
omega one satisfies all of the axioms of set theory of set power set. For any infinite
cardinal gamma, remember, gamma plus was the least cardinal bigger. H gamma plus will
satisfy ZFC minus power set. So this is another way of stratifying the universe of sets
besides the V alpha's. Now a transitive class, we want to talk about transitive classes,
but how do you talk about classes when we're supposed to be talking about sets? We're
talking about classes. Just like you talk about set of odd numbers in a number theory,
even though that set is not a number. So a transitive class is an inner model. It contains
all the orignals and all of the axioms of ZFC hold in that class when interpreted in
that class. Now that's not first order. But there's a nice equivalence, a surrogate
definition, which is first order. And that is M is supposed to transitive class containing
the orignals, and I'm thinking of a definable class. Then the following are equivalent. M
is an inner model, so this is happening. And second, for every infinite cardinal gamma,
M intersect H gamma plus satisfies ZFC minus power set. And the point is that this is a
first order condition. That's a legal statement. So if I have a definable class, it's illegal
to check that, but that holds, I say that holds. L and HOD are inner models. Okay, so
we're trying to get at, V is ultimately L, doesn't deny large cardinals, we need some
measure that an inner model is close to the parent universe. Now the many roots toward
this, actually many independent roots were taken, but a particularly elegant one was
student Hamkins. Suppose N is an inner model, remember it's an inner model ZFC, and delta
is an uncountable regular cardinal, like omega or omega one. Well, omega one, that's
O is an uncountable. N has a delta cover property that every subset of N of size less than
delta can be covered by a set in N of size less than delta. Small sets can be covered
by small sets. Now this would be trivial if delta was omega, we don't use this in the
case of omega. N has a delta approximation property if for all sets X, the following
are equivalent, it belongs to N, and all of its small pieces belong to N. So if you have
an arbitrary subset of N, and it's intersection with every set in N of size less than delta
is in N, it must be in N. Now if I used omega here, there's only one inner model I can
satisfy, and that's V. Okay, so that's why we don't use omega. Okay, so Hamkins proved
a remarkable theorem that N, N0 and N1 both have the delta approximation property and
the delta cover property. Suppose they do, suppose their intersections with H delta plus
are the same, then they are the same. So inner models with the delta plus cover and approximation
property are necessarily definable inner models from a brand. Okay, but then, so that's the
weakness theorem, and then Hamkins also proved the universality theorem, which I'm only stating
for extendable cardinals, that's the only large cardinal I can find. Suppose N is an
inner model with a delta cover and approximation property, kappa is bigger than delta and kappa
is an extendable cardinal, and kappa is an extendable cardinal in N. Now this fails to
generalize to the axiom i0, and we want to get at all large cardinals. So we need a third
property, and that's the delta genericity property. Suppose N is an inner model, sigma is a subset
of N, then N bracket sigma is the smallest inner model such that M, such that N is included
in M and sigma belongs to M. It always exists. Suppose N is an inner model and delta is
strongly inaccessible, that means 2 to the kappa is less than delta, or kappa less than
delta and it's regular, then N has the delta genericity property for all small subsets
of delta, N bracket sigma intersecting delta, that's a CFC model is a common extension of
N intersecting delta. So we're just bringing in another property, and the reason to bring
this in is we get the full universality theorem. If N has the delta approximation delta cover
and the delta genericity property, suppose the axiom i0, and this is just a sample, holds
a lambda for a proper class of lambda, then in N the axiom i0 holds a lambda for a proper
class of lambda. All large cardinals go down. Okay, now that's an obvious conjection. The
ultimate L conjection. Suppose delta is an extendable cardinal, then provably there is
an inner model N, such that N has the three properties delta cover, delta approximation,
and delta genericity, and N satisfies V is ultimate L. Now by the universality theorem,
if you can prove this theorem, V is ultimate L is compatible with all large cardinals.
Now right by the universality theorem, there's no Scott's theorem. Now I want to stress that
the ultimate L conjecture is an existential number for all statements. It's saying provably
if it's undecidable it's false. An existential statement about numbers which is undecidable
is false, for true you verify. So the ultimate L conjecture must be either true or false,
unlike CH, it cannot argue that it's meaningless. You're arguing that an existential statement
about the integers is meaningless. I'm not saying that this is an interesting number
of theoretic statements, I'm just saying you can't say it's meaningless. Alright, so we're
now in an interesting situation. I would say set theory faces one or two futures. The ultimate
L conjecture, first of all this is now my claim, I'll be controversial, the ultimate L conjecture
reduces the entire post-colon debate on set theory, on set theoretic truth to a single
question which has to have an answer. It's an existential number of theoretic statements.
Future one, the ultimate L conjecture is true. Then the axiom B is ultimate L is very likely
the key missing axiom for V. There's no generalization of Scott's theorem for the axiom
by the universality theorems. All questions which have been shown to be unsolvable by
Cullen's method today are resolved by a large cardinal analysis. So V equals ultimate L,
like V is ultimate L, the reason it negates Cullen's method is you can think of the
universe of senses having two dimensions, it has a width and it has a height. The height
is a large cardinal issue, the width is how many reels do you have, how many sets of reels.
V is ultimate L, as does V equals L, binds the height to the width. If you change the
width, you can't recover the axiom without changing the height. What Cullen's method
does is it changes the width and preserves the height. So that's why it's useless, it
cannot affect the axiom. So that's future one. Future two is the ultimate L conjecture
fault. Well then the whole program to understanding V by generalizing the success we had in understanding
V omega plus one in the projective sense fails. It's back to square one. So which is it? Now
I realize I have to show a picture. And to me, I really think the set theory is at a critical
crossroads. And we have to resolve this conjecture and the future will be very different. I happen
to think that in this future, obviously I'm a little biased, the set theory will really
start to get off the ground. The trouble with set theory now is you work from the ZFC axioms,
almost everything is independent. How do you build a rich structure theory that everything's
independent? It'll be like walking up to a cathedral that you want to investigate. And
there's a doorkeeper. And the doorkeeper says, well, you say, where's the door? The
gatekeeper says, where would you like it? Then you enter the door and say, well, what's
over there? What would you like to be over there? There's no structure. Set theory with
forcing is a study of the landscape of possibilities. It's not the study of what is. And so you
can't develop a rich structure theory in that setting unless you can somehow eliminate forcing.
So that's just why we want it. If this happens, all right, it's really that square one. So
in set theory, I think we are either going to face a bright future or a dark future. Now
a bright future is like the sun rising and a dark future is like the sun setting. So
that's my picture. All right, now this is Caltech. You might look at that picture and
say, yeah, it's a nice sunrise or sunset picture depending on the future. It's not Earth. It's
Mars. And this is from the land Earth. And so this is what the sunrise or sunset, I'm
actually looking at which it was, appeared to be the land of the clue, of course, is
the sun seemed a little bit smaller than it should be. But other than that, so that's
where we are. I think it's an exciting time in the subject. And hopefully before too long,
we'll know the answer. So thank you.
So any questions?
This is from a non-expert. If the ultimate L conjecture is false, that doesn't preclude
the fact that there could be another conjecture.
Yeah, there could be. But if the ultimate L conjecture is false, it depends how it's
built. But it's saying that the whole program of how we build enlargements of L fails. So
the whole program we use.
So the way things work now is you build L early. Scott comes along and says there's
no measurable cartons. Then you can enhance the definable power set and build an enlargement
of L with a measurable carton. But wait, then there are not two measurable cartons. So there's
a whole area of set theory that constructing ever more complicated enlargements. You can
build the enlargement for one wooden carton. But in that enlargement, there are not two
wooden cartons and so forth. But all of these constructions share a similar feature. And
it's hard to imagine a construction that doesn't. And what the ultimate L conjecture
does is really distill out that feature. So if the ultimate L conjecture is false, you're
at a threshold in the large cardinal hierarchy which is beyond analysis according to the
way that we currently do it. There's another issue which I didn't dwell on. The large
cardinal axioms go beyond I0. But then they enter a realm where the notion of large cardinal
was so large, it denies the axiom of choice. Wait, I said the only way to verify the claim
of large cardinal is consistent is to have it true and be. The strongest large cardinals
we know of deny the axiom of choice. Does that mean we're denying the axiom of choice?
For big sense. And if you go down that road, not only is it dark but it's foggy and you
can't do anything because you give up the axiom of choice, everything falls apart. So
there's the issue in that if you take that view seriously, that the only way to authenticate
a consistency claim is by an existence claim. And the only way to have existences in this
is deny the axiom of choice. These axioms should be inconsistent. That follows from
the ultimate outcome. Could it meet the countable choices too but not the full axiom of choice?
Yes. So these just say the axiom of choice fails way up high. So down here where things
are nice, you might have the axiom of choice but once sets start getting too big, you get
tired and you can't choose. But more interestingly, the reason these axioms haven't been refuted
is it's incredibly difficult to do a construction without the axiom of choice. The ultimate L
conjecture, which is a ZFC conjecture, wipes them all out. So something else will have
an important clue is the ultimate L conjecture. It will be the first time that a large cardinal
axiom, well motivated by large cardinal principles, has been shown to lead to a contradiction
by a deep structural argument. All inconsistency proves so far that relatively easy. And that's
kind of a strange tribe. Every theorem has an inconsistency as well. And there are very
deep theorems, even in that theory. But no one has proposed a large cardinal axiom which
leads to a contradiction by a deep structural argument that just hasn't happened yet. So
that, in the future one, will be a whole wave of such results.
And then you missed this, but what does the vehicle of the L say about the continuum
of axioms? It implies the continuum of axioms. And it implies, curiously we don't yet know
it implies GCH, the generalized continuum of axioms. And that's because V is ultimate
L is an N run. We don't know how to build the model yet. Once we know how to build the
model, we'll have all the tools to analyze it. The idea was just to come up, to try to
guess what the axiom is, maybe prematurely, before knowing how to build the model. And
the reason to do that is to open the door for an anti-theorem. In other words, you don't
want to spend all your time developing a theory. You want to allocate some resources to showing
that it cannot be developed. So in the abstraction of the ultimate L conjecture, it's a level
playing field. Each side has a chance, right? You could work and try to prove it, or you
could work and try to just prove the Scott's theorem. So that's the situation. That's why
I think, we're going to act, this will come to a conclusion. Hopefully it won't be 12
volumes of, and probably many journal pages. But I think there's enough additional evidence
for that dichotomy. There are other theorems that suggest that the ultimate L conjecture
is true. I'm not telling you the whole story. But it's a hard one.
So the nice thing about an unsolvable problem is that you can change your mind. So 20 years
ago, I was advocating actually L2, and I had arguments for it. You know, you have to go
to the evidence. And the trouble is, a lot of those arguments are, I would call them
R-centric. The view is numbers and the reals, that's all the action. And they're always
sets up there, but who cares about it? So why should your intuitions of V be determined
by local intuitions? They're only capturing a small piece of V. You don't want to overgeneralize.
So I would argue that those intuitions that suggest C is L2, they're misleading because
they're based on a very local view of the universal sets, ignoring the global structure.
I mean, it's a lot of work that's been done. But there are fairly, I think, good arguments
against the C is L2. Just how old is the conjecture? Four years. Okay. Well, it's refined. There
was an earlier incarnation of the conjecture eight years ago, but as one investigates,
one realizes it wasn't quite right. The issue is, the ultimate L conjecture, as I stated,
is reflecting sigma in two senses. There was an idea that you should be able to reflect
sigma in three senses, which is more complicated, provided you chose the right test model. And
that seemed like a very solid intuition, and it was made for a more useful axiom. And then
I showed that it's not right. You have to only reflect sigma in two. So in this form,
so that's three years old, two years old. And there's progress on it. I mean, there are
partial results. It's not like nothing's happening. So that's complicated so far. Any other questions?
What's the feeling of how many years until it's finished?
Well, I can always, I mean, look, someone in this room could refute the ultimate L conjecture
tomorrow. And I don't have to resign my job, because if you show wooden cardinals are inconsistent.
Well, I did say I would resign my job, but I was at Berkeley at the time. I'm not quite
sure what the ethical choice would be, because I could resign my merit status. So I think
we're going to know within a year whether the current methods can achieve it. It's just
so, the dichotomy of possibilities has sharpened so much that I really think it's either going
to work or there's going to be an anti-pairing. But, you know, every now and then, what you
think is a sminer or loose end that has to be worked out that blows up and becomes the
key problem. We don't have enough understanding here. What we know is, we know that the construction
of ultimate L has to be very much different than the current construction. And the question
is whether you can lay that out and do it. And that's under what?
So which future of sense can you give to the person?
Well, I think I kind of indicated that. I'm still going with future one, because I don't
have an alternative. The future two is so dark. I mean, it's a little bit like, here's
a depressing thought. The projective sets and the correct incarnation of second order
number three. This is not the algebraic geometric projective set, but second order number three,
those questions were studied in the early 1900s. By 1925, they looked really hard and
I don't know the answers and stuff like that. And now Solver took maybe 80, 90 years. But
you look at the ingredients of that solution with things they had even imagined. Well,
what if we're in the same situation now? That figuring out C.H. is going to involve
ingredients we haven't even imagined. So that's the problem. How can we be so confident that
we really are on the cusp of figuring this all out? You know, it would be kind of a nice
period of history or a sector of history to live in. But there's no real reason. Maybe
there's something that's going to be a thousand years from now when they're dedicating the
next building. You know, they'll talk about those school or set theorists from a thousand
years ago. They just had no idea. But that's possible. But right now, there are enough
partial results and there's a clear line. So I think there's a good shot at it. Check
out our podcast on that.
