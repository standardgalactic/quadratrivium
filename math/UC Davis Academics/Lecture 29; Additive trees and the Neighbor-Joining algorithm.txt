Okay, so last time we were talking about ultrometric trees,
and we said if the data's ultrometric or closed ultrometric,
it really seems like there's no explanation for that,
except that the data really is somehow reflective
of passage of time since divergence,
and therefore you can build an ultrometric tree for that data,
and you can feel confident that tree is the correct tree.
Now there was a question that came up later about ties.
If you look at my strict, maybe I didn't say this,
but the definition of ultrometric tree
should have said that the numbers along a path
from the root should be strictly decreasing,
and so that's a very narrow definition of an ultrometric tree.
You can have situations where the data is such
that you get no contradiction to the ultrometric conditions,
but you don't get strict decreasing either.
Anyway, I just want to say that when the data is pure,
when you take any triple and the max is a tie
and the min is strictly less than that,
that's a special condition which will allow strict decrease,
and all this is to say that when the data
really looks good under the right circumstances,
you get a unique tree, and I don't have time, unfortunately,
to spell that out completely,
but I want to make that point that when you look at the data,
if it has the right form, you get a unique tree,
and the only reasonable explanation
that people have been able to come up with in the last 30 years
as to why that would ever happen
is that it is, in fact, following the true evolutionary history
of the distances that generated those numbers.
Okay, so ultrometrics are the gold standard if it happens,
but often the data is simply not ultrometric
or not close enough to ultrometric to feel
that the differences are just due to some experimental error.
So the next level of assurance,
or feeling that your data reflects evolution
and it's not ultrometric, is if it's additive.
Now, what is additive data?
Additive distances.
And I'm going to do this by way of an example.
A, B, C, D, A, B, C, D, 0, 3, 7, 9, 0, 6, 8, 0, 6, 0.
Okay, we have four columns, so we should have four leaves
on the tree.
If we were trying to build an additive, an ultrometric tree,
we would have four leaves.
And previously I told you that a tree on four leaves,
or on n leaves, if every internal node has two children,
no matter what the shape of that tree is,
and here's one example, it has to have three non-leaf nodes.
And if the tree is ultrometric, you're
going to put numbers at these nodes.
So you only permitted three distinct numbers
in an ultrometric matrix.
Well, this one has a 3, a 7, a 9, a 6, and an 8.
That's five distinct numbers.
So already you know this is not ultrometric.
Try as you might.
Well, you just have too many distinct numbers for one thing.
There's no way you can possibly get an ultrometric representation
of this distance, these distances.
And if you look at the triple condition that I wrote down
last time, you'll find violations.
Very simple.
Okay.
Nonetheless, these numbers have some properties
that make you think that this might reflect evolution too,
even though it's not ultrometric.
And the reason is this.
Let me show you the tree.
Chalk is awful.
OK, so what does this tree got to do with biology, by the way?
Well, it's obvious.
It looks like a dog or a horse or a deer or something.
So that's biological, right?
So OK, so anyway, if you look at the distances on the edges
and you calculate the distance between any pair,
let's say from A to B, that's 2 plus 1, which is 3.
And you look that up in this table, that's 3.
OK, A to D, 2 plus 3 is 5 plus 4 is 9.
And you look up A to D, that's 9.
So and if you check it out for all pairs,
you'll see that's true for all the pairs.
So this tree is what's called additive,
and this matrix is called additive.
And a tree T is additive for a matrix M
if the distance between i and j, and by distance,
I just mean you sum the length along the unique path
between i and j.
So dij comes from this matrix, equals
the sum of edge lengths between leaves i and j.
And this has to be for all, all pairs i, j.
Then that tree is called additive,
and the matrix is called additive.
And when you have data that's additive, again,
it looks like the only reasonable explanation for why
it's additive is that it has something to do with evolution.
So that we're saying, well, I haven't drawn this with a root,
sort of the beginning in history.
But you could, you could just say, well, maybe this
is the ancestral point in time.
But at any rate, between what we're
presently looking at a sequence B,
and we're looking at a sequence A,
and there's some difference that we're
noticing between them, and it has some measure nine, A to D.
And we're able to lay out a tree where
the individual edge lengths are such
that it satisfies this property.
Then you can say, well, I think really what was happening
is that between the moment in time when A and B split off,
this thing accumulated two differences, two mutations.
But along here, we only had one, OK?
So the point I want to point out is
that if evolution is tree-like, and that's
in the kind of models we're looking at, that's believed,
but the molecular clock doesn't hold,
so that you can have faster mutation along one branch
here than the other.
The molecular clock would have told you
that if this is a point back in history,
say time is going that way, then the number of mutations
you should expect along here is roughly equal to there.
Well, actually, one is roughly equal to two.
But you know what I mean.
If you make these numbers such that one is a very different
number than two, this doesn't look at all altimetric.
It does not look consistent at all
with the molecular clock thesis, because you have an edge here
where you have more mutations than along there.
So nonetheless, if evolution was tree-like,
but you can have different rates of evolution
on different edges, you should get additive distance.
If your pairwise comparisons, if that number
is proportional to the total number of mutations that
happened, so let me just start at the beginning.
Suppose this is true evolutionary history,
like that, A, B, C, D, E. Now, under the molecular clock
thesis, the number of mutations on this edge
should be roughly equal to the number on that edge.
And in fact, the number on this single edge
should be roughly equal to the total number from here down to B
or from here down to C, which should be the same.
But suppose this is the true branching history,
but the molecular clock was broken that day.
And you have, I don't know, five mutations along here,
one, two, three, four, five, but only two there, OK,
and two there, say, two, five, two, four there, one, three.
I'm just making these numbers up, two, four, OK?
So the clock ticks at very uneven rates,
and particularly, the real violation of the molecular
clock idea is when you look at things
since they deviated, the number of mutations on one side
is very, very different than the number of mutations
on the other.
I mean, look at this split here.
This only has two mutations going that way,
and this has five.
Down to there is nine.
Here's total seven, 10, 14.
Totally not ultra-metric, totally not
consistent with the molecular clock, OK?
Nonetheless, ABCDE, ABCDE, if I have some, let's say I compare
A and E as sequences.
And if I have an alignment method where the distance that's
returned is proportional to the total number of mutations
on the path between the two, so let's just say not only
proportional, but it's actually equal.
Let's say I have some fantastic alignment method
where when I look at the two sequences,
I can see for absolute certainty
that between A along this path from A to E,
we had 7 plus 4 is 11, OK?
I don't know how they're distributed anymore.
If it was ultra-metric, I would know that half
should be on one side.
Well, that'd be a problem because 11's an odd number.
But half should be on one side and half should be on the other.
But we don't have that anymore.
All we know is that since the time that A and E split,
the number of mutations on one side plus the number of mutations
on the other side adds up to 11, OK?
So we learned that.
Now, let's see whatever.
Let's just a little bit of a big example,
but let's just see what we get.
2, 7, 9, 12, 16, for A to B from A to C is 7, sorry, 2, 7, 9, 12,
14, A to D is 2, 7, 9, 10, OK?
B to C, B to C, that's an easy one.
That's a 6.
B to D, 7, 8, B to E, 7, 9, 13, OK?
Oh, C now is next.
C to D, 2, 5, 6.
And C to E is 2, 5, 7, 11, D to E is 1, 3, 7, OK?
And then E, we already know that.
So these are zeros along the diagonal.
So let's say I have some ability to look at pairs of sequences
and I get this data out or something
that's proportional to this.
I could multiply by some constant everything.
The claim is that because this data actually
came from a tree where it actually was the distance data,
well, let me just back up just a minute.
Let me just ask a trick question here.
Is this data additive?
Yeah, of course it is because I started with a tree.
The definition was if you can go from here to some tree
where the totals match the matrix, then it's additive.
But if you started with a tree and you actually
calculated the distances that way,
then of course it's additive because there's the tree.
So this is additive.
Now, the claim, though, is that if you're given data
it's additive and you don't know the tree it came from,
then you can actually constructively find the tree
and the tree again will be unique.
And it's hard to think of why you would have data that was additive
along with this uniqueness theorem if this wasn't actually
the true evolutionary history.
Now, one thing is true, however, I should say,
when you create this tree you don't know what the root is.
You actually, you look at this tree as an unrooted tree.
We don't know that this was the point back in time.
What you learn is that between A and E,
you had that many total mutations,
but you don't know what the beginning of time was necessarily.
Biologists use different methods.
Biologists use different tricks to try to figure out
where the root actually might be.
The principle when being something called out-group.
Anybody here use out-groups in your work at all?
A couple of people.
So basically the idea is that if you're studying fungi,
I don't know, you have some kind of commonality
between the A through E that you're studying.
And in addition to that, you throw in some organism
that's very different from any of these things.
You know, you throw in a giraffe.
It's very likely, here's the giraffe,
if you really believe that the split back in time
that went ultimately to giraffe and went to fungi
was before there were splits in fungi,
then it probably, that split should come in
at the root of the fungi.
So this is an out-group element,
something that's so totally different.
Now, if you want a tree of all life, yeah.
If you just look at the table,
commonality and what the tree gets is unique.
Oh.
Or not something like the black.
Okay, the question is how do I know that this tree is unique?
Mathematics.
I mean, as a theorem, I haven't proven in the class
and I don't have time to prove it.
But so I'm just asserting it.
It's something, just a claim in terms of this class
that if the tree, if the data is ultrametric,
then the tree is unique.
There's one tree that works for it.
Do we consider that kind of thing a tree?
Oh yeah, this is a tree.
Definitely this is a tree.
This is one I drew where we don't know the root
and that one I drew saying this is true evolutionary history
so there is a root.
We don't happen to know what it is,
but there is one and that helps to explain
where this data comes from.
That was just in terms of explanation.
When you actually go from this data backwards,
you don't know what the root is unless you do things
like this out grouping idea.
And with an out group, you pick something that's very different
from all the organisms that you're studying
and then the belief is that it should relate to all of those
through the root of what you're interested in.
So that when you build this whole tree as an undirected tree,
you see how the giraffe connects to the tree
and then that is what you decide is your root.
Now if you want to build a tree all of life,
and what I'm going to say in the next 10 minutes
is going to be on the exam,
if you want to build a tree all of life
and you want to root it,
your out group has to be something like a rock.
Anyway, that's supposed to be a joke.
It's hard to use a rock as an out group,
but it's something that's not living.
All right, so I am going to tell you an algorithm,
hopefully say a little bit about,
enough about it in the next 20 minutes,
so that you could actually go backwards here
and do this construction.
But is it clear, A, what definition of additive is
and B, why if your data is additive,
it's suggestive that you have evolutionary,
some evolutionary truth in hand.
I should have mentioned also similar to ultrometric data,
additive data is quite rare.
If I just throw up some numbers,
it's very unlikely that it's going to be additive.
So when you have data that really is additive
or close to additive,
it's highly suggestive that it was because
it's really reflective of the number of mutations
that have occurred along paths between species,
between organisms, even if there's no molecular clock.
So all we have to assume to get additivity
is that evolution is tree-like
and that the data we have is somehow proportional
to the number of mutations that actually occur along that.
That sounds like an awfully good exam question to say,
what are the necessary assumptions,
the biological assumptions for additivity to happen
or for ultrometric stuff to happen and so on.
Not solve this problem type question,
but do you understand the larger picture kind of question?
Anyway, I promise that I would say what was on the exam,
so I don't know.
Okay, so how do we actually go from here to here?
There is an algorithm that's very widely used
called neighbor joining.
At first this algorithm may sound a lot like
the one we discussed last time,
the single link kind of algorithm,
but it really is quite different
and to some extent quite mysterious.
And the nice thing about this algorithm, by the way,
is that if the data is additive,
it will definitely give you the tree.
But if the data is not additive but not too screwed up,
it gives you features of the tree.
What do I really mean by that?
It was, in truth, some additive distance.
And now you take each element here
and you change it somehow.
This used to be a 14, now it's a 15,
this is a 6, it becomes a 5.
You know, the kind of things that really happen
when you collect data or because your measurement,
your alignment doesn't give you something
that's perfectly proportional to the true distance.
So you start perturbing these things
and you can actually prove theorems
that say that if you don't perturb these data terribly much,
you will get back the same tree,
at least the shapes, not with the same numbers,
but the same topology as what you had
or what you would have gotten
if you were working with the real numbers.
So this algorithm has a very nice property,
a kind of resilience property.
And again, I won't have any time to prove any of that.
But so here's the basic idea of the algorithm
is if by some magic, here's your data,
and if by some magic you can figure out
that these two nodes are siblings,
as they have the same parent, okay?
In neighbor drain they're called neighbors.
I think siblings is a better...
Anyway, if you can figure out that these two are siblings,
then you should start to construct the tree like that,
B and C, and here they are as siblings.
And then we'll build up some kind of artificial distances
from this node to everybody else who is not yet in the tree,
and we'll just continue.
If you have an operation that lets you say,
lets you know for sure that two nodes have to be siblings
in the tree you're trying to build,
then you pick the two that are siblings,
put that into the tree you're building correct,
or invent some distance between this node and everybody else,
and continue in that way.
And that's the neighbor...
That's the high level of the neighbor joining,
but I have to tell you the details.
How do you correct these?
How do you find out who are neighbors?
What's the most obvious suggestion
as to who should be neighbors?
So you haven't put anybody else...
Anything in the tree yet.
You're at the very beginning of the algorithm.
Who do you think ought to be neighbors?
Smallest distance.
Smallest distance, right.
And that works if it's ultrametric.
That's what we did last time.
And when the data is ultrametric,
that's exactly...
You find neighbors that way.
Unfortunately, I'll show you an example.
When the data is merely additive but not ultrametric,
it's no longer true that the smallest distance
in your matrix actually identifies neighbors.
So you have to be a little trickier than that.
Here's an example.
One, two, three, four.
One, one, one, four, four.
Well, if you look at the pairwise distances,
from one to two has distance three.
And that's the smallest distance between any pair.
All the other distances are either five or six.
Or actually, here's one that's nine, okay?
So one to two is the smallest pair.
But leaf one and leaf two are not siblings.
They're not neighbors, okay?
These two guys are neighbors.
These two guys are neighbors.
But one and two are not neighbors, okay?
This data certainly is additive.
Here's the tree.
I mean, I could write it up in a matrix
and that would be additive.
But if I then looked at the matrix,
I didn't have the tree in hand and I said,
okay, the smallest distance in the matrix,
which is three,
I'm going to make one and two neighbors.
I'd be wrong because here's the tree.
One and two are not neighbors, okay?
So it's no longer what's, you know,
that greedy kind of approach,
that naive first step that we were able to do
in building ultrometric trees.
And I said there almost anything you do works.
That no longer works here.
All right, but instead the following thing does work.
So this is, I want to be consistent
with the notes I'm following here.
They call this thing a little d.
And big d is going to be something else.
Okay, so little d distances.
d i j equals d i j minus r i plus r j.
And what do you do?
What's r i?
r i is equal to 1 over l minus 2,
summation k contained in l, d i k.
Okay, so what is r i really doing?
This looks like the average distance.
I'm sorry, this is the total, not quite the average yet.
This is the total distance in the original data here
between i and everybody.
Okay, so you take the sum of all those distances
and then you divide by the number of leaves minus 2.
It's kind of like an average.
Okay, I don't know why minus 2.
If this had been minus 1, you know,
I could say that's an average,
average distance between i and everybody else other than i.
But that's why I say this is a little bit mysterious.
So what this is looking like is,
I'm taking essentially the distance between i and j
and subtracting the average distance,
something like the average distance
between i and everybody and j and everybody.
And that's my capital D.
And the theorem that can be proven,
if i and j have the minimum Dij value.
So you take all of the pairs there
and you calculate this capital Dij.
If they have the minimum Dij value
among all the Dij values, then it is true.
i and j are neighbors in the additive tree.
If the data is additive,
data that's additive,
you do this transformation as given here,
and intuitively all I can tell you is
you're kind of subtracting off the average distance
from the distance that's given in the matrix given over there.
You're subtracting off the average of i to everybody
and the j to everybody.
But this minus 2 gives me headaches.
Someday I'm going to have to go through the proof very carefully
to find out why it's a minus 2 and not a minus 1.
But this tells you how you recognize your first pair of neighbors
by doing that.
If we had time, I'd actually go through this example,
but this is way too big to try to do on the fly.
You might try that at home.
Okay, so now you've identified two that are supposed to be
neighbors in the tree you're building.
So presumably if we did this,
we'd find BC as the pair that has the smallest capital D value.
I said earlier the high-level idea is that you now compress these somehow,
B and C, into a single new node, an artificial node,
and give it some artificial distance.
Okay?
Well, what distance should you give?
Let's call this new node K.
This was I, this was J.
What do you think K, the distance from K to each of these other remaining,
ought to be?
Okay.
Well, if it's perfectly additive, if the data is additive,
let me go over here.
Let's just imagine we know this is part of the tree.
Remember, the tree is unique.
We haven't figured out the rest of it,
but just imagine now that by some magic,
I can see what the rest of the tree is.
Okay?
I haven't figured that part out yet,
but there certainly is one there.
So here's our new node K.
And here, let's just say some arbitrary,
oh, they're leaves, M.
Okay?
I want to invent DKM for the purpose of the algorithm.
I want to say what is the synthetic distance that I should now use
for the continuation of the algorithm
that talks about the distance between K and M,
and I'll do this for each of these, each of the leaves.
Okay?
Well, how about this one?
DIM plus DJM.
Okay?
So that's that plus that minus Dij.
So that plus that minus this is this thing twice.
We'll divide that by two.
So if the data is additive,
and this and I and J are siblings in the tree,
from here onward, it has to have exactly that distance.
Okay, let's see if we can see that over here.
I haven't done this ahead of time,
so I sure hope this works out.
Suppose we just have determined that B and C are neighbors.
Okay?
And now I'm trying to get a synthetic distance
between K and E, let's say.
And I'll also do one for K and D and so on.
But I'm saying that the distance between K and E
should be the distance between I and E.
What was that?
B and E.
B and E, that's 13.
Plus J and E, that's C and E.
That's 11.
Minus the distance between I and J, that was B and C.
B and C is 6.
Okay, so that's 24 minus 6 is 18,
divided by 2 equals 9.
What do we see in the tree between here and E?
3 plus 2, which is 5 plus 4, which is 9.
Okay?
It works.
It's mathematics.
I mean, if we didn't make any mistakes here, it has to.
So I'm just trying to explain the logic here.
If we absolutely got this thing right,
these are siblings in the unique additive tree,
then the distance between here and M is something we know.
We know what that should be.
Okay?
We know what that ought to be,
and we can figure it out.
This is what we did here.
So the algorithm again, what's the algorithm?
The algorithm is you use these capital Dij's,
which are based on the little Dij's,
and the R's which are based on the little Dij's,
to figure out a pair that's a neighboring pair.
Put that in the tree that you're building.
That creates a new node, K.
Now you have to figure out what the distances are for K
in the tree that remains, which you do this way.
So now you basically have a new problem.
What's the tree on these, on the new distances,
the existing old little Dij's plus the new little D?
You want to find a pair that's supposed to be
a neighboring pair of those.
So on that data you do this capital D calculation,
find out a pair that's supposed to be a neighbor,
do that, and keep going.
That's the neighbor joining algorithm.
There are ways of streamlining the actual calculations,
have the capital Dij's change as the new node is put in,
and so on, but I'm trying to just highlight the logic
and not the optimizations to the algorithm
that make it look a little more mysterious.
Now one thing to note, and the theorem is that if the data
is actually additive, this algorithm finds the tree.
But one thing you can note is that the algorithm
is the algorithm.
It doesn't matter whether the data is actually additive or not.
You can run the algorithm on any kind of crazy data.
And as I mentioned earlier, this algorithm has some nice
properties that when the data is not perturbed too far away
from true additive data, the algorithm will return the tree
that was the additive tree for the unperturbed data.
And it has some other resiliency properties,
so that in fact people really run this neighbor joining
algorithm quite a bit.
So the neighbor joining algorithm is kind of the
classic algorithm for distance data.
When you have pairwise distances,
or if you took it from a multiple alignment or whatever,
and you've taken your sequences and turned them into distances,
this is the typical algorithm that people run.
Now in Clustall, it builds a guide tree.
Does anybody remember what they call that guide tree?
Nobody here from New Jersey?
They call it the NJ tree.
They look just as things are zipping by as you're using Clustall.
It's not from New Jersey.
That's the guide tree.
So I told you way back when I was talking about Clustall,
they built a guide tree and it's kind of like the single link,
but not exactly.
Well now you know exactly what they do for the guide tree in Clustall.
They do neighbor joining.
The neighbor joining algorithm,
like all good algorithms in this field or any field,
there are 100 variations on this general algorithm.
I mean people have to eat.
You've got to write papers in this business.
So somebody will find some small modification of this algorithm
and show why it's better than all the others and so on,
and somebody will say that's untrue and it keeps going.
But basically this is the idea,
and I don't know in Clustall whether they're using the pure neighbor joining method
or not.
Okay.
That's distance-based evolutionary trees.
I happen to think that distance-based methods are the most effective.
They really work.
Neighbor joining really works.
But since we're building trees here,
these are the evolutionary tree methods.
What we've been talking about is distance-based,
where you take your sequences and you use them just to get a matrix of numbers
and then you work on those numbers somehow.
There's another set of methods where you use actually the sequence data.
You never throw it away.
You don't just turn the sequence data into numbers.
And the big method there is called parsimony.
I don't have time to talk about it.
I'll put some notes on the web that talk about parsimony.
The only thing I'll say about parsimony is that those methods are very, very slow.
So there are people who believe that distance-based methods are the work of the devil,
but they work fast.
And this goes back to the very first lecture.
We have always this conflict between what seems biologically the most sensible model
and what the computational constraints are.
So some people believe you shouldn't throw away the sequences
and just punch them down the numbers.
Your algorithms should work on the sequences themselves
because that's where all the real data is.
And then there's another set called maximum likelihood
where we have a statistical evolutionary model of what creates trees.
And given the data, you try to find the best tree that fits the data at hand.
These are also very, very slow.
So in the past, people had not really tried much maximum likelihood.
And today they are.
Parsimony methods can work in some limited contexts.
And most people just use distance-based methods.
And no matter how much bad press there is about them, these things seem to work.
Okay, and they're very fast.
All right, one more little comment.
I only have 10 minutes.
Six minutes.
There's another thing that shows up in evolutionary tree algorithms,
which is called bootstrap values.
Have you worked with bootstrap values before?
Okay, just a little bit.
Bootstrap values are misinterpreted.
So without telling you what they are, I'll tell you how dangerous they are.
Okay, so what happens?
You get a tree out and you get some number on an edge
and it's written on a node.
And I actually don't work with these enough to know whether a big number
or a small number is good.
But anyway, I think it's a big number.
But anyway, you get out of what looks like a good number
and people misinterpret this as saying this is some kind of measure of reliability
or truthfulness of this tree.
If your bootstrap values are good,
then like a significance value in statistical testing,
you can have a large confidence that this was the right answer.
And the only thing I want to say today about bootstrap
is that that's absolutely the wrong interpretation.
Don't overdo that.
What the bootstrap is doing is telling you something about the consistency
of the computation.
So what bootstrap is part of a larger set of techniques called resampling techniques.
So in particular what happens in bootstrap is you take your data.
Here it is.
It's supposed to be one, two, three, four, five leaves.
So instead of using this data, you pick columns randomly,
five of them with replacement.
So it's possible that you pick each one of these columns.
But it's also possible you pick this column twice or this column three times and so on.
Anyway, you build up synthetic data that has the same size as the original data.
Each time you pick one of those synthetic data sets, you run the algorithm.
You get a tree.
Now we can define whether an edge is in the same...
If we have two different trees, we can actually make a definition
of whether a particular edge is in both of those trees.
Because an edge separates, I take this edge, it separates BCD,
some set of leaves from some other set of leaves.
So even if I shaped this portion differently,
if it down here is BCD and up here is the same set,
I say that edge is actually there.
So if I had two different trees and I take away this edge
and I still get BCD and the other thing,
then I say that that edge is actually in both of these trees,
even if the trees look quite different.
So in Bootstrap what you do is you make all these synthetic data sets,
just as I was describing.
You run the algorithm for each synthetic data set
and you look to see how many times a particular edge appears
in all the different trees you built.
The Bootstrap value is then calculated based on that number.
So large Bootstrap value is good.
You saw that edge many, many times,
even though you were doing weird things with the data.
Somehow the message is that edge wants to be there.
That edge is really part of all the different kinds of trees
that you would get if you had this kind of perturbation of the data.
All right.
So a lot of people interpret that as,
well, that edge is really there in the right answer.
And the point is that all the Bootstrap value is telling you
is consistency over the set of executions you get.
And the algorithm may be producing garbage.
The tree may have no connection whatsoever to truth or history
or any of that stuff.
And so Bootstrap values should not be misinterpreted
as saying that you hide Bootstrap values
when you got the right answer.
