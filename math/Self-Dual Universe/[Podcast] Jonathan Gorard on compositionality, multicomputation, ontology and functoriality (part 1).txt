Okay. So I'm here with Jonathan Gorard. Thank you so much for joining me. And I just want
to mention that I learned of Jonathan when, you know, he was the co-director of the Wolf
from Physics project in about, you know, October, November, 2020. And I've been following
your publications ever since. And I've always been very interested in your and your own
research approach. So really brief intro. What do you have a professional role right
now? Are you affiliated with any universities? Sure. Okay. Well, okay, first of all, thanks
very much for inviting me to come be here with you today. It's a really great honor.
And thanks for that very nice introduction. So yeah, okay, so my official role is so I'm
a, I'm an academic mathematician. My current position, my primary position is at the University
of Cardiff right now. So I'm a Cardiff, I'm the director for the Center of Applied
Compositionality, which is this new, when I say new, I mean, we officially launched like,
less than two weeks ago, research center that's trying to do a kind of promote and foster
interdisciplinary research between science, technology, kind of fundamental mathematics,
but unified by this belief in compositionality, which is a kind of collection of broadly
philosophical ideas that come from category theory. So the sort of ideas to do with
trying to understand processes and the algebra of how processes compose and the algebra for how
subsystems compose, and you know, how you can describe kind of emergent features of a system in
terms of the, in terms of the kind of both the properties of the constituent components, but
also the properties of the syntactic language by which those components kind of hide together.
Well, I'm sure we'll talk more about that later on, but that's kind of so that that's that's the
the main thing I'm trying to do right now is to start up that that center. I also, you know,
I also have a joint position between the Math, Physics, and Computer Science department at
Cardiff University. I'm also a sort of visiting research fellow at the University of Cambridge,
which is where I was just before I moved to Cardiff. I was previously a PhD student and
researcher there. And I also do sort of random bits of like consulting work for various companies.
So I'm also a, I'm a research fellow at Wolfram Research and happened for some time. I also do
consulting work for some other tech companies. I'm in the process of like starting to get
involved with a weird biotech startup that's trying to do like on the fly sort of
brain scanning for determining things like hormone and neurotransmitter
quantities and stuff like that. So I have a bunch of random side projects, but that's the that's the
main thing. Excellent. It is extremely exciting. So before we dive into more technical topics,
I just want to know a little bit more about your own mathematical education. So
at what age roughly did you get interested in math? And did you have like a particular
problem question topic that you enjoyed? Yeah, it's a really interesting question. I mean, I
let me try not to go, that's a question with many layers. So let me try not to get to go
into too much of a monologue there. But so the reality is that actually I didn't,
I didn't realize I was interested in math until until quite later on. I mean, I know a lot of
people, there are a lot of kind of math prodigy types who like, you know, at the age of six or
something think that they want to be mathematicians and they do lots of stuff. And that definitely
wasn't me. I was someone who was interested in lots of different things. And I was generally good at
I was generally good at most subjects. And it wasn't it certainly wasn't the case that math
stood out as something I was particularly good at or interested in. So, you know, I was,
ever since I was a kid, I was interested in, I got interested in computer programming quite
soon, quite at the age of about four or five. I remember kind of messing around with sort of
simple writing little simple bits of code and kind of getting interested in algorithms and stuff.
I was really interested in what I guess I would now characterize as engineering, like, you know,
trying to take stuff apart and figure out how things worked. I was interested in science more
generally. I remember spending a lot of my kind of early teen years reading popular science, you
know, reading a lot of Carl Sagan and Stephen Hawking and, you know, Jacob Brunovsky and that
kind of stuff. But I was also really interested in literature and philosophy and lots of other
so it was there was definitely no, I was very kind of split brain as a kid I was no there was no
clear to right if you'd asked me what I wanted to be when I was like 12 or 13. I probably would
have given you 50 answers and I don't think a single one of them would have been mathematician.
But so what actually changed was it was really all the. Okay, let me not say all of it, but
a really big moment of change for me came when I was about 1415 as a consequence of a specific
teacher I had, he was called Richard Bridges, Dr. Bridges. And he had unlike all the other
mathematics teachers I've had up until that point, he had actually been a mathematician or he'd been
a mathematical physicist he'd he'd sort of his background was really in sort of mathematical
aspects of conductivity theory he developed with his PhD supervised he developed kind of the first
properly rigorous theory of quantum transport and magnetic fields anyway to kind of worked in
that area for a while, and then decided he didn't really like academia and became a school teacher
and he was incredibly inspiring. And he was the person who showed me that if you like mathematics
was kind of the key interdisciplinary piece that unified all my different interests right so you
know why was I interested in programming, well it wasn't really because I was interested in writing
code because I was interested in designing algorithms and analyzing how they worked improving
that they were correct, you know why was I interested in philosophy, well really it was
because I enjoyed constructing kind of rigorous analytic arguments you know reasoning from
premises to conclusions constructing proofs you know why was I interested in physics well
it was because I was interested in the idea I was captivated by the idea that you could
construct a kind of robust mathematical quantitative model that described the natural world and so on
so I and you know and even things like my interest in poetry I came to realize that was really
the reason I liked poetry is a particular form of literature was because it was a way of
expressing creativity within a very rigid set of constraints and again that's really kind of the
essence of mathematics and so suddenly it was like I had a almost religious revelation and so
from about the yeah about the age of 14-15 I suddenly realized like no it wasn't that I had
a million different interests it was that I was interested in a core feature of many many different
areas and that core feature was really mathematical and then I got you know I so my my earliest
interests happened to be in sort of bits of discrete mathematics and also complex analysis
purely because they happen to be close to things I was interested in for other reasons so I got
interested in discrete mathematics via kind of computer science and wanting to understand
algorithms and I got interested in complex analysis completely by accident because I was
interested in fractals I was interested in the Mandelbrot set and Julia sets and I wanted to
build I was I spent like a summer getting really obsessed with writing code for doing like deep
Mandelbrot zooms and I wanted to understand how these algorithms like the power algorithm and things
that let you predict like where filaments of the Mandelbrot set were going to terminate I
wanted to understand how they worked like why those things worked and to know that I needed
to study complex analysis so I so those are kind of the bits of mathematics that I got interested
in earliest but that was kind of by accident if you like anyway so there's more I can say but
I can leave it at that for now. Well that's that's fantastic did when you when you say you're
interested in discrete math and complex analysis did you feel any tension between
discreteness and continuity or was it just two things that you had to handle at once and
there were just different aspects of math? That's a really interesting question I think I did I
think I did feel attention right because for me discrete mathematics always felt very intuitive
and maybe this is because I spent you know my back because I spent my formative years back
when my brain was still forming I spent programming computers so somehow discrete mathematical
structures got quite deeply ingrained in a way that I think continuous mathematical ones didn't
and so you know ideas from abstract algebra or from graph theory or from logic felt very intuitive
to me whereas ideas from analysis or topology or geometry really didn't you know I I know a lot of
I spoke to a lot of professional mathematicians and it's very very common for people to say that
they think in pictures right they think in a very geometrical way and and it's kind of that's a really
good power tool if you could because you know the the visual center of the brain is a really big thing
right if you can co-op that to do mathematical reasoning that's really powerful but for whatever
reason that's just not the way that I that I think I don't I don't really think in a particularly
geometrical way I think in a very I think in a very combinatorial way I think I I tend to think
about things in terms of sort of the logical structure of how things connect rather than some
kind of pictorial or diagrammatic representation of what's going on and so yeah I I think I definitely
did feel attention in that you know discrete mathematics for me felt very intuitive but
almost to the point of feeling kind of boring right I knew I could understand it I knew I could
do it but somehow analysis because it was much more counter-intuitive was also much more mysterious
it seemed much more magical right when I when I read a proof in discrete math I would kind of
think well yeah obviously that had to be true whereas I'd read a proof in complex you know I'd
read the proof of you know there's this remarkable theorem of complex analysis which I still you
know whatever 10 years 10 plus years later I can't get over right this this result that says
um if you have a you know that if you look at functions not just over the real line but over
the whole complex plane then if you like then the only functions that don't blow up the only
functions that don't have singularities somewhere are constants right every function every other
function that you can define has to have has to have singularities um it as long as you define it
in this sort of in this way that it is complex analytic that's a really really count for me
that was a really counter-intuitive idea and I've read the proof you know a hundred times
and it still surprises me um and so analysis for me always seemed much more magical precisely
because I found it much harder to understand um so yeah I definitely did feel this tension and and
maybe because of that I was perhaps in my later career so to speak became I was maybe more receptive
to the idea of using discrete structures where previously people had used continuous ones where
I whereas I think if I'd had a more traditional way of thinking about stuff uh that maybe wouldn't
have been such an obvious conclusion so interesting that you mentioned how you think because I was
going to ask a question about that so it's funny because I also think in that way I think in terms
of like the logical structure and and just abstract connections with no like visual imagery
associated per se although you can always induce some visual energy I always thought of that as
being more like local structure right so geometry is more like a local structure of like this global
logical properties that appealed to me more because I wanted to see how like more of like
a top-down approach to the system um but interesting so all right let's move into
the applied compositionality center center for applied compositionality
I have a just a quick question clarifying question because I know what compositionality means in
linguistics theoretical linguistics but you mentioned that it's more of a category
theoretic notion maybe you know the idea of like composing morphisms so can you just briefly define
what you mean by compositionality absolutely I mean so actually I should say that like um you know
it is a it is terminology that comes from linguistics but then so does a lot of category
theory terminology right even notions like uh well I mean the notion of a functor right goes back to
like Rudolph Kahnap and the Vienna Circle and that kind of stuff and and yeah compositionality is
another term that category theorists shamelessly stole from like theoretical linguists of the
20th century so that's that's a very good connection to make but yeah I mean the idea is
basically the same right it's the the idea being that um so uh when you've got some if you've got
some big complicated system right so some so in linguistics that might be a big you know a long
sentence or something you know the linguistic idea is if you've got some big sentence and you
want to know what it's mean I mean what is it semantic meaning then you can really decompose
that into two distinct pieces right there's uh so category theorists have this kind of dogma of
like stuff and structure okay so the stuff is the the individual like morphisms right the individual
pieces of words that carry the indivisible units of semantic meaning and then the structure is the
grammar that describes how they compose together how you can chain them together to make to make
bigger things and the the principle of compositionality in linguistics is saying that as long as you know
the semantic contents of the morphemes and as long as you know the grammatical language that
lets you chain those morphemes together you should have everything you need to know to be able to
infer the semantics of the sentence from the semantics of the individual morphemes to make it
up and the principle of compositionality in category theory is just taking exactly the same
idea and applying it to a much much wider class of things so yeah the idea is you decompose everything
into stuff and structure so you have you have some indivisible components which in in category
theory would be your objects and then you have some kind of syntax some algebra that describes how
those components are chained together in order to form larger things and so compositionality
is yeah it's both a kind of philosophical principle but also a sort of actual like
mathematical methodology for analyzing complicated systems and so for instance you know the way I
got really convinced that this was a fruitful thing to kind of dedicate myself to studying for
several years was this realization that actually if you take uh what I by kind of trying to investigate
the foundations of physics at a progressively deeper level I came to realize that really the most
foundational theories of physics we have are actually compositional theories and that's not
immediately obvious in the way they're traditionally formulated but it's one of these things where if
you think about it in the right way it becomes kind of clear right so one example of this is in
in quantum so quantum mechanics is you know one of along with general relativity kind of one of the
two most fundamental theories we have of physics and quantum mechanics the way people often go on
about how quantum mechanics is really weird and counterintuitive and whatever but actually I think
the essence of quantum mechanics lies in one sort of idea and it's the idea of well separability of
parallel composition so what I mean by that is if you've got in classical physics if you've got two
systems you know you've got two particles right and they're described by classical equations of motion
so you have some equations that for a given set of positions a momenta will tell you where the
particle is going to be in the future or where it was in the past and so you have some each particle
has its own kind of space of states what in classical physics you call a phase space and you
want to know if I bring them together and I let them interact and I've got some composite system
what's the phase space of the composite system and for classical physics the answer is really easy
it's it's what's called the cartesian product of the two phase phases you started with basically
just the you know you pair off all the positions at all the possible positions I'm going to enter
the two particles and that's the and that's your new phase space and that's fine and
that encodes the fact that classical physics has what's called separability so when you've
got this composite phase space you can always recover the phase phases of the systems that made
it up right so you can always trace out if you've got a system of two particles you can always trace
out one particle and just look at the system as though you know and just look at the other
particle purely in isolation and that's because the cartesian product as an algebraic operation
is separable you don't lose any information when you compose them that way but in quantum mechanics
that isn't true right if you take two particles that are described by the Schrodinger equation
you combine them together they can become entangled and when they become entangled what that means
they are their composite system is non-separable you can't neatly trace out the state of one
particle because there is no state of either particle in isolation there's only a state there's
only a single state of the composite system and again if you think about that algebraically
all that's all that saying is that the tensor product operation the operation by which you
compose the basis of states is not cartesian so there's like this spectrum of you can have
cartesian tensor products which are kind of like fully separable and you can have chronicle tensor
products which are kind of fully non-separable and quantum mechanics is all about the sort of
liminal space in between the two where you have classical physics on one end a kind of maximally
entangled quantum physics on the other end and really all the weirdness of quantum mechanics
I think can be explained in terms of the non-cartesianness of this tensor product operation
and this is not my own I mean I'm parroting this view but this is not my own view this is
an idea that really goes back to Bob Kocker and Sampson Abramsky from 2008 it's the idea behind
categorical quantum mechanics that you can define so what I think that's telling you is that you've
got the really quantum physics the the transition from classical physics to quantum physics is
all about just changing the way we compose processes in parallel right normally we're used
to the idea that if you've got a process A and a process B and then you can obviously you can
compose them in sequence you can do A then B but you can also compose them in parallel you can do
A and B you know side by side non-sequentially and we're used to the idea that that operation that
parallel composition that tensor product has separability but as soon as you relax that condition
you get quantum mechanics and so so I came to discover that really there was a way of thinking
about quantum mechanics that was just in terms of an algebra for processes it was just in terms
it was just an algebra for how different processes compose in sequence and in parallel and in
particular how parallel processes can compose in ways that encode non-separability at the same time
you know okay again if you look at the other most fundamental theory of physics we have general
relativity um again it's normally formulated you know just as quantum mechanics is normally
formulated in this very abstruse way to do with Hilbert spaces and Schrodinger equations and
all that kind of stuff in the same way general relativity is normally formulated in terms of
Bernstein manifolds and Einstein metrics and all that kind of stuff you know all that kind of stuff
but really when you when you strip back all the complexity you realize what it's ultimately about
is this invariant quantity which is or this invariant data structure which is the causal
structure of spacetime so spacetime has a causal structure a thing that tells you for any
give for any pair of events whether those events are uh time like separated space like separated
or light like separated and that causal structure we know is invariant under what are called
conformal transformations so under transformations that that maybe distort lengths but preserve
angles and those conformal transformations encode not just the transformations in reference frames
that you get in special relativity but also transformations in gravitational frames uh
that you get non-inertial frames that you get in general relativity so all really all hinges
on the structure of this course on the nature of this causal structure and the
invariance of this causal structure and again if you formulate it in the right way you discover
that's really again just a statement about processes and how they compose that basically
what you're saying is we have this algebra but now so now you just as in the quantum case you
can if you've got events uh you know suppose you've got two events you could compose them
sequentially you can do event a then event b and you can always do that independent of what
your reference you know what what the what the causal structure is but if they are not
light like separated or time like separated only then could you compose them in parallel
right so it's it's so only events that are space like separated can be composed together in
parallel so again you've got some kind of algebra that's telling you how you compose
processes in sequence in parallel but unlike in the quantum case in relativity you've also got
this additional restriction that says actually the tensor product operation the parallel composition
operation is only defined for space like separated events and so it turns so okay that's just giving
examples from from kind of fundamental theories of physics but there are many many other examples
from chemical reaction networks to biological ecosystems to process algebras to linguistics
all kinds of things and there it suddenly struck me as it has struck many other people and this is
kind of the you know this is why applied category theory has kind of taken off in the last few years
as a field that an awful lot of these really foundational questions we have about science
and technology and reality and whatever can be boiled down to questions about the syntax and the
grammar for how processes compose together and that's really the idea of compositionality the
compositionality is the kind of philosophical conviction that if you like that thinking purely
in terms of processes and how they compose in sequence in parallel or components and how they
compose is a fruitful way of thinking about lots of different areas in science excellent thank you
for that little tutorial because I I've always wondered what it is about compositionality that
motivates you so much so all right let's start diving a little deeper so you mentioned and I
heard intimations of multi computation in in what you're speaking so yeah let's so the applied
compositionality focuses on what you say is the multi-way nature of computations and I have an
intuitive notion of what multi computation is in distinction from like a single way system where
you have like just like where you're kind of describing like a classical system with sequences
right and there's one thread of time and this kind of thing whereas in multi computation
I've read that you have you know not a single thread of time but you can have you know and
this is what I'm getting into is is there is there a finite thread of time in a multi-way system
or can it be arbitrary it can be like an arbitrary thread of time
okay when you say an arbitrary thread of time do you mean are we talking about
lengths of time in terms of like in terms of temporal extent or kind of numbers of different
branches of time yeah like that sort of like the arity so like the numbers so I mean with a multi
computation it's sort of unbounded right the arity in terms of the number of threads you can
compose together in parallel I mean that's kind of the power of multi computation right you can
that you can start to when you analyze multi computations mathematically you can start to make
statements about sort of infinite limits of what happens if I take a if I take a tensor composition
of like an infinite number of branches what what kind of structure do I get and you get see you
get these very interesting mathematical structures that sort of inherit spatiality from infinity
group voids and things and there's this clearly a very rich theory there that we're only just
beginning to uncover I should actually by the way just as a point of use kind of skipped over it
a little bit but I think it's an important it's an important connection that you made there that
the connection between compositionality and multi computation so if you like I like to think of
compositionality as being like the kind of pure syntax and multi computation as like the kind of
concrete computational semantics right so for any anytime you've got
anytime you've got a sort of a language like you know category theory you can talk about that
language in purely abstract terms about like you know what's the what are the algebraic rules
by which things can compose but you can also talk about it in terms of like can I design like a model
of computation that explicitly actuates this this this you know this grammar this language or whatever
and that explicit computational model is multi computation multi computation is the kind of
in my opinion is the compositional semantics for these so-called symmetric monoidal categories
these categories where you can compose processes in sequence in parallel and and exactly as you
say the idea there is you know we're all used to in sort of our classical physics intuition and in
our classical computation intuition we we're all used to the idea that you can compose
processes or computations in sequence what we're maybe less okay with but I think we should be
more okay with is the idea that you can also compose them together in parallel that you can
have multiple threads of time that are essentially tensor products together and that those different
threads and that that tensor product structure can be non-trivial in the sense that those threads of
time not only can they branch but they can also merge and so you can have kind of and that that
merging operation is the essence of this non separability thing that I was telling you about
in quantum mechanics right so separability is the case where they just kind of diverge completely
they don't interact and you could say you can always if you just have pure branching you can
always just chop off you know however many branches you like and just look at the ones that are
remaining and you haven't changed anything whereas if you have merging if they connect if they interact
you can't neatly do that anymore because any chopping off operation you do is going to lose
some information about that merging operation and that's the essence of non-separability and so
that's of course central to understanding quantum mechanics and quantum entanglement
but the idea is that actually it's a useful way to it's a it's a central way of thinking about
lots of other kinds of things as well that you know this this statement that your rules for
how you compose processes either sequentially or in parallel the fact that those rules can be
quite non-trivial in nature I think encodes a lot of the diversity in these different sorts of
fields and that a lot of it's my conviction that a lot of phenomena can really be understood in those
terms so like most physical systems in say chemistry biology and even psychology could
they're amenable to multi computation as multi-way systems like could we could we frame a physical
system as a multi-way system and a classical system at once uh yes absolutely I mean so so
that actually brings us on to it to yeah that's an important point to make right so
with all of these things these these different models of computation for instance that we're
talking about with you know ordinary classical computation or multi like oh yeah multi computation
etc from if you're just if you're a logician or something and you only care about computability
theory uh these models of computation are all equivalent right that the the the the functions
you know in the same way that we say that you know Turing machines are equivalent to lambda
calculus are equivalent to you know recursively innumerable functions or whatever they compute
the same set of partial functions so there's nothing you can do with a multi with a multi-way
system that you can't in principle do with a with a classical computer I mean in fact look
all the multi-way systems we've ever studied you know I've studied multi-way system I'm using a
MacBook Air right now that's a classical computer right um so I you know I make my living simulating
multi-way systems with classical computation it's not that multi computation does and is
capable of doing anything that classical computers can't you can obviously simulate multi-way
systems using classical computers just as you can simulate classical computers using multi-way
systems it's it's a it's a more um it's a slightly more philosophical slightly more um
ockomy posimony-y thing than that right what what we're really saying is with with any
with any mode of explanation the the operative question is almost never can this you know can
x explain y it's almost always I think more fruitful to phrase it as is x a useful model for
explaining why right it's like um uh it's a bit like so for instance we it's believed if okay if
you assume that there's nothing there's no kind of supernatural influence and if you assume that
there's nothing uh there's no kind of intermediate scale physics that kind of becomes relevant uh
you know between the kind of micro scale and the meso scale then everything we know about chemistry
everything we know about biology everything we know about psychology is explicable in terms
of elementary particle physics right it's explicable in terms of quantum field theory
because presumably if that if those assumptions are true then in principle you you know if you
were Laplace's demon you could simulate the you know the interactions of every elementary particle
inside the brain or inside some ecosystem or whatever and you could in principle predict
what was going to happen but that's not useful right just because you can in principle do that
with infinite computational power doesn't mean that particle physics is a useful model for
thinking about psychology it's the wrong level of explanation and when I when one talks about
usefulness what one's really talking about I think is um okay the way I like to think about it
is that it's kind of like you've got these two computations that are going on right there's the
computation that nature is doing and then there's the computation that our scientific model is doing
and these are not the same obviously um because it's a model but what we'd like to be able to do
is to is to map features of our model onto features of nature and vice versa right we'd like to be
able to point to something in our model and say oh yeah that's that particle or that's that flower
or whatever and so you've got to construct some kind of encoding function that maps between these
abstract computational states in your model and the concrete sort of physical states in the
natural system you're looking at and so when I talk about explanations being good or bad explanations
what I'm really saying is that the encoding function for mapping between the two is more or
less complicated right the less complicated your encoding function is the better the explanation
the more natural it is that that's the point right so particle physics is a bad explanation
for psychology because the encoding function to derive psychological features from elementary
particle physics would be incredibly complicated and what you want is something that's much more
much more sort of synthetic much more much higher a much higher level but reasons
more directly at the level of concepts that might be relevant to the brain and to human thinking
and there we one would hope the encoding function would be much simpler so uh so what I'm trying
to claim is not that multi computation can explain things that classical computation or
other ways of thinking can't what I'm trying to say is actually is the more subtle point or
the point I'm trying to make is a more subtle point which is that I think multi computation
there are certain kinds of phenomena and for instance quantum entanglements I'd say is one of
them uh which are I think much more natural to encode in terms of a multi computation than
they are in terms of a classical computation or indeed in terms of a conventional mathematical
model yeah that actually makes that perfect sense so okay we have an ordinary multi-way system
right but we also have something called a rural multi-way system yeah and
I'm wondering could you define what a rural multi-way system is because I know that an
ordinary multi-way system can can be thought of as like a concurrent application of like
specific rules that generate like multiple paths but then the rural multi-way system has like all
possible rules but that all possible refers to some kind of domain or you know and then we're
after this we're going to get into what a limiting rural multi-way system is so let's start with
this is a bit tricky and this is a point where actually that I um this is a place where I start
to have some slight philosophical disagreements with people like Steven and others who've kind of
also thought about these ideas where um I think the that it's actually really quite hard to uh to
construct a um I think constructing a well-formed definition of something like a rural multi-way
system and how it and what the boundary is between a multi-way system and a rural multi-way system
and a hyper-rural multi-way system whatever like I think those boundaries are extremely ill-defined
at present and um so I can give uh I can specify the way that I'd like to think about these things
but that won't necessarily be universal right and I think this is I think uh my opinion is that
this is still an area where we are where where there is still quite a lot of confusion and and
so the the the answers that I'm going to give here are necessarily kind of speculative right so um
yeah as you say normally when we say multi-way system we mean a multi-way system with a kind
of fairly definite rule or set of rules so the the branching and merging either comes I think you
have a single rule but it can be applied in many different places right so like with a
graph rewriting system or something you can have a a graph rewriting rule that matches a subgraph
and replaces it with another subgraph uh but then there could be many many subgraph in the
graph you're applying it to there could be many subgraphs that match the left hand side of the
rule right that are isomorphic to left hand side so you can get even with a single rule you can
get branching and merging behavior then you could start to say well what if I have multiple rules
right so if you've got a system like a Turing machine where normally there's only ever one
transition that's possible one way you can get multi-way behavior is just by having multiple
transition functions so you've basically got multiple Turing machine rules there's another way
you can get branching behavior once you're in the business of saying well let's allow that let's
let there be multiple rules then you could start to say well suppose I've got all possible rules
of a particular signature so when I say signature what I mean is uh so it's there are ways that you
can parameter when you've got these kind of infinite or not even necessarily infinite but
like large finite rule sets there are obvious ways that you can there are generally very natural
ways you can parameterize them that are kind of specific to the model in question so for instance
for hypergraph revising rules um it's not that we have a sort of encoding signature that that's uh
sort of specifies the the arity of the input hyper edges and the arity of the output hyper
hyper edges so when you say like a two three goes to two three rule that means you've got
I forget which area it is now but it's not I think it means it means something to the effect of
you've got two hyper edges of arity three and they're being replaced by two other hyper edges
of arity three that encodes a kind of infinite class of possible hypergraph revising rules
but that that two three goes to two three is what we call the rule signature and so then you could
start to ask what if we built a multiway system not with a single rule or the finite collection
of rules but with all possible rules of that signature and similarly with Turing machines
you could say uh you know there are there's a class of I forget I can't do the combinatorics in my
head but if you look at like you know the the number of two state two color Turing machines
or two state three color Turing machines or something there's some large but finite collection
of all possible Turing machines of that type of that signature and again you could say let's
build a multiway system that runs not just a single Turing machine rule or finite or a small
sub collection but all possible Turing machine rules of that signature and that is what I would
call a rule your multiway system then of course you could say well what if instead of just restricting
ourselves to a single signature we look to all possible signatures uh or even you could go even
further and say well what if we okay well even doing that is kind of tricky enough but you can
imagine doing you can imagine saying something like that and those are the things which in for
instance the papers that Xerxes and I wrote we call kind of the limit of rule your multiway
systems or the or the classifying space of rule your multiway systems and there's a nice map by
the way I mean we can talk about this more in in more detail in a minute but these rule and limiting
rule your multiway systems have a very nice mathematical structure at least if you construct
them in a particular way they they that you can interpret them in terms of infinity one top losses
and things um and uh then more recently there's this uh term that's appeared in some writings of
Stephen and and others which I'm I must admit not a fan of which is this thing called the Ruliad
which I I'm still unconvinced as a well-defined concept um but as far as I understand it is uh
a essentially if you looked at not all possible rules of a given signature or even all possible
rules of all possible signatures but all possible rules across all possible models right so not
just looking at all possible Turing machine rules but also looking at all possible hypergraph
rules and all possible whatever um a I don't really see how you make that a world of you know
because then you've got a bunch of issues with like effectively data type mismatches and whatever
I don't really see how you make that a well-defined uh mathematical or computational object but um
um another way I suppose if I were being more charitable maybe one way to think about the
Ruliad is that it is just the same thing as these limiting rule your multi-way systems I
was describing before in which case they are nothing more than these uh infinity one top
host classifying spaces that classify uh all possible rule your multi-way systems anyway this is um
we're we're we're rapidly reaching the limits of what might be considered useful to think about
but anyway yeah for the for the questions well yeah so I have similar reservations regarding
like the Ruliad which you know I've read some of Stephen's writings on it about you know all
possible rules and all possible conditions and this kind of thing and and the term all possible
sort of like stretches the imagination too far to the point of like ill definition so if we stick
to like would we say that there is many possible limiting rule your multi-way systems or could we
say that there is in principle one limiting rule your multi-way system it's a good question and
yeah you're you're right the word the the phrase all possible conceals a number of a number of great
sins right so um yeah I would say it's when you ask is there is there one possible limiting rule
your multi-way system I mean it's again it's like yes because of universality of computation right
because you know because we know that you can simulate hypergraphy writing with Turing machines
or vice versa then yeah of course the the multi-computation that is you know enacting all
possible hypergraphy writing rules is at some level formally equivalent to the multi-computation
that enact all possible Turing machine transitions uh they will they are you can think of them as
being different parameterizations or different coordinatizations of the same underlying computational
structure just as you know Turing machines and uh and you know lambda calculus are different
parameterizations of the same model of classical computation um but actually I mean that's so I
want to be clear that I think the rule ad in terms of if you think about it in terms of like okay it's
just enact it's a multi-way system that's enacted all possible rules whatever that means I do I agree
that's a useful thinking tool right because it's it's a way of very concretely encoding the other
extreme view right that like um when you're trying to understand you've got a model like
computation and you're trying to understand um natural processes using it then you can either
adopt a kind of extreme bottom-up approach where you just start with basically nothing but raw
computational building blocks and you try and build up explicitly or you can take the extreme
top-down approach where you start from all possible computations and then you kind of slice
that apart in some way to obtain the system you want or you can start somewhere in the middle
and I think the rule ad as a thinking tool in the denetian sense is quite useful right because
it's a way of encoding this idea this this top-down view if you start with all possible
computations then you make restrictions you take fibers or whatever um but beyond that I'm I'm very
I'm very skeptical about its formal usefulness precisely because it's so ill-defined um now
actually okay I want to come back to this this um all possible thing right so uh because I think
again therein lies I won't be so harsh just to call it a philosophical confusion but certainly
a something which I have philosophical issues with which is this privileging of uh touring
complete computation and this is a very universal thing that the people sorry I should say that's
a bad pun it's a very general thing that people do so um what is it okay so on the surface
what is it that's so special about Turing machines right we know that Turing machines
are not they're not the most powerful model of computation right they obviously you know
the first paper that was ever written about Turing machines was showing that there were
things they couldn't do the other models of computation could do um they're not the most
powerful they're not the they're not the least powerful you know partial recursive functions
are simpler and in some ways easier to understand um they seem to be just kind of pretty arbitrary
in this kind of space in this hierarchy this arithmetical hierarchy of different
models of computation they seem to be pretty arbitrary uh so what is it about what you know
and you know the if you looked at if you look at the space of all possible mathematical functions
and all possible mathematical relationships uh the set of relation of functions and relationships
that can be computed by a Turing machine by a universal Turing machine is a vanishingly small
subset and seems to be a pretty arbitrary subset and so you know why is it that we spend so much of
our time talking about why is it that 99.9 percent of the theoretical computer science research or
the mathematical logic research that gets done assumes that Turing machines are kind of like
the ultimate model of computation well my opinion is that it's because the church Turing thesis
right the thing that says that you know all these different models of computation are equivalent
to Turing machines is really not a statement about computation and it's not even a statement
about mathematics or logic it's really a statement about physics in fact I think it's
probably the deepest statement about physics that we know because I'd argue the thing that's
special about Turing machines is that we can build them right that we know we can we can set up
physical systems that are arbitrarily good approximations to universal Turing machines
whereas we don't yet know and I would I suspect it we it's impossible within our universe to how to
build um you know physical systems that are arbitrarily good approximation to hypercomputers
so if you like the way I interpret the church Turing thesis is it's saying that there is a
hard upper limit to the sophistication of computations that could be enacted within the
constraints of our universe the laws of physics permit and that that hard upper limit is universal
Turing machines and that has a bunch of interesting you know philosophical implications right so it
means that in effect the reason why we care so much about the mathematical functions that can be
computed by universal Turing machines is because they are exactly the mathematical functions that
are instantiated by the laws of physics or if you build a an approximation to a universal Turing
machine as a physical system then the set of possible motions of that system is in one to one
correspondence with the set of possible motions of any physical system anywhere in the universe
so in some ways I think the church Turing thesis is actually the most even though no one thinks of
it or I say no one David Deutsch and some other people think that this way but almost no one thinks
of it as a statement about physics but I think it's actually the most it's the deepest the most
foundational statement about physics that we know although the deepest law of physics that we know
sorry the reason I bring that up is is because um that's that realization or that hypothesis
means that I have deep issues with this idea of all possible rules right because if you're going
to say that uh somehow our universe the physical universe is in as an emergent feature of computation
then if you're presupposing that you know in order to to say all possible rules you have to be able
to say you you have to be able to specify what model of computation you're using or you're using
the whole infinite hierarchy of possible models of computation which I don't think is what the
Rouliat is because I think there's there's also been terms like hyper Rouliat that encode hyper
so if we start with the assumption that Rouliat is only doing Turing complete computations
then I think you I think you've already committed a huge philosophical blunder because you've
presupposed sort of the fundamentals of Turing computation whereas I think it's exactly the
other way around it's that you know um you know it's not that Turing computation is fundamental
and the and the laws of physics emerge from that it's that the laws of physics place constraints
on what computations we can enact and Turing computation is just the strongest model that
we can build within those constraints um so it's one of many uh you know philosophical issues I take
with I take with the Rouliat construction it thought about in that way but as I do think it's
if used correctly it can be an extreme it could be an extremely effective uh thinking tool
so it's funny you're almost reading my mind because I was going to raise your thoughts on
hyper computation let me just briefly say I'm very interested in what you said that the that
it's not that our laws of physics arise from Turing complete computation it's that the laws of
physics almost like give us parameters according to which we can you know build machines within
the physical universe I mean I don't want to it's veering a little philosophical here and I don't
want to you know sort of make you force you into speculations about your own philosophical that's
that's quite right I'm happy to do that so then so then would you say that because this is the funny
thing is that you said and correctly of course that you know the the the set of all computable
functions is vanishingly small compared to the set of all uncomputable functions right I mean
aren't there like uncannably infinite uncomputable functions yeah right so so how is it that a human
being can even formulate an uncomputable function if the laws of physics do not allow
for a system to you know like a system to actually realize hyper computation
yeah that's a really interesting question I mean it and that strikes to the heart of this
distinction between definability and computability right the fact that we can conceive of things
that we can't compute that we can't construct I mean like you know my favorite example of this
which for a long time I used to say was my favorite number it probably still is my favorite number
right is this thing called chiton's constant that you may be aware of right so as you exactly
to say well exactly as you kind of alluded to there are these there are uncannably infinite
you know non-computable functions there are uncannably infinite non-computable numbers right
because for the very simple reason that we know that there are uncannable there's an uncannable
number of reals and we know that the number of possible Turing machines is countable so there
got to be way way more non-computable reals than computable ones which is kind of what Turing was
arguing back in the 1930s and so so we knew for a long time that there would be real numbers that
we couldn't compute but it took a long time before we had any explicit examples and then the first
really kind of non-trivial example I think was this thing called chiton's constant which the idea
being that it's the probability that an arbitrarily chosen Turing machine will halt right if you choose
some Turing machine specification and you ask if I just arbitrarily choose a Turing machine
arbitrarily choose an input what's the probability it will halt well you can show Greg chiton showed
that if you to compute that number you would basically have to solve the holding problem
which we know can't be done by a Turing machine and the thing that's really interesting about that
is that you know the set of definable numbers is countable right because we we define things using
language using alphabets with finite numbers of symbols and whatever um we certainly don't use
you know symbolic alphabets with uncannable with you know continuous parameters or something
so uh a number of possible descriptions is countable and a number of possible computations
is countable but somehow the number of number of possible descriptions is somehow larger you know
in some maybe in some discrete measure theoretic sense is somehow larger than the number of of
computable things because every computable thing is definable because if nothing else the algorithm
that computes it is a definition you can think of it as being a definition but on top so that
they're at least the same size but on top of that we know that we can define things that we can't
compute like the holding problem or like uh chiton's constant or you know these other kinds of
things so somehow and I don't really know how to think about this um as you say it's really a
kind of philosophical speculation I I'm not going to claim to understand it completely myself
but there's some sense I think some fairly well-defined sense even if I can't define it
precisely myself that uh in in which the set of definable things is larger than the set of
constructable or computable things and so uh I think the the point you make which I agree does
seem on the surface kind of paradoxical the fact that we can talk about stuff like real numbers
or like non-computable functions or whatever that we can't construct explicit examples of
is somehow a byproduct of this general phenomenon that definability is a even though just in terms
of their cardinality the sets are the same somehow definability seems to be a weaker
condition than constructability or computability excellent yeah I won't venture to speculate
myself although I I do I mean there is this let's see I want to frame this so if we if we
talk about something like a self-generating operation I I don't know if there's any I mean we
we talk about like generating functions and generating sets and math but like a self-generating
operation where somehow you couple like minimal and maximal scales via the same structure and
operation something almost like taking a tautology on the minimal scale and sort of like modeling
it as like an an infinite order meta language on the maximal scale and saying well like if anything
self-generates it has to be that right we don't have to call it a hyper-ruliad but there there's a
sense in which like a tautology is almost like the conditions of definability in a way and yeah
what do you think about that I think that's a really I was going to say sorry I think that's a
really perceptive idea so so if we take because that actually aligns pretty well with how I think
about this so yeah since we're in the realm of philosophical speculation I want to I don't I'm
probably happens to do philosophical speculation I just want to be clear okay what I'm saying is like
not I don't have proofs of any of this this is just me you know um whatever uh talking about
stuff that I think might be might be interesting so I think it's a really nice example that you give
so because the way the way that I tend to think about this is that the reason why definability
and computability seem to be you know definability seems to be somehow weaker is actually very deeply
related to the top down bottom up thing I was talking about earlier that if you like
computability is a bottom up condition whereas definability is a top down right and so it's like
with with definability you're saying you're giving constraints it's like you've got you know you've got
kind of all possible structures somehow instantiated um and you're and when you're when you give a
definition it's like you're making restrictions on the on that space of possible structures whereas
when you're giving a construction you start with nothing and you try and build it up and so clearly
everything that you can build can be can can be yielded and you know by means of some restriction
but there might be things you can yield by restriction that you can't explicitly build up
and so your example like as you know a self-generating function or self-generating relation whatever that
would mean we could maybe write down some formal definition of if such a thing existed these were
the properties these are the properties it would have to satisfy so if you like there what we're
doing is we're starting from this ontology of all possible structures if you like some I don't know
hyperbouliade or whatever and we're writing down some rules by which you can make sufficient
restrictions that eventually you you boil it down to maybe not a unique thing but maybe some
infinite equivalence class of structures that all have those properties uh but and we might be able
to do that even if we have no idea how we would go about building those structures directly from
the bottom up and yeah I think that that I think it's a really nice way of thinking about it because
I think it does kind of get to the heart of you know the the essential you know it's like your
that your definability and computability are two different ways of looking at the same space of
structures but they are because they're looking from different directions uh the the the reach
you know definability gets greater reach but it sacrifices that at the expense of lower
essentially lower specificity whereas computation has extreme specificity but the the kind of the
price you pay for that is that you can't you can't get you can't reach as many things
nice so then let's let's it's funny because I think you're one of your youtube channels is
jontology right yeah yeah I remember that and so I mean what do you think do you think this is the
last speculation I'll ask you to indulge in so do you think in an ultimate sense there is one
ontology that takes both a top down and bottom up approach and sort of somehow creates like an
automorphism like a unique automorphism between where like you know the top down is literally
literally the the bottom the you know the bottoming up that's some you know fundamental structure
and if if you take that view then that would almost and necessarily yield one ontology as
opposed to many possible like ontologies so I mean I know it's more useful to think about
many possible ontologies but if you were to think in like an ultimate sense if we were to
sort of define minimal and maximal scales by one in the same structure wouldn't that yield one ontology
um it it may well do and that would that that's a very elegant idea it's that's not the
that would not be my hypothesis of how things will turn out like I agree it's an extremely elegant
concept that somehow that you uh it's like you've got the I don't know somehow you've got these like
two cones you know what one the top down cone from the finability and the other the bottom up cone
from computation and you intersect them somewhere and somehow you find you know if you if you kind
of push the intersection further and further and further eventually you kind of zero in on this one
unique ontology I agree that would be beautiful if that were the case um unfortunately I just
that doesn't align with my formal intuition for what I mean I have no again I'm not saying
that's wrong or whatever like this is just speculation but it that doesn't align with
my formal intuition for how these things go what I think will what I think is more likely is that
yeah that there will be a sort of infinite um okay the the reason it doesn't accorded my formal
intuition is because it's precisely because of the kind of encoding function argument I made before
it's because um yeah okay so you you might say well you know we've got the computation the nature
is doing and we've got the computation we as scientific observers are doing and if we assume
that the those are both kind of definite and determined then then surely then you know there's
going to be one unique you know there's going to be one unique simplest ontology that will allow us
to interpolate between these two um but precisely because we have freedom in how we set up the
encoding function I think the best we can ever hope for is a kind of infinite equivalence class
of possible encoding functions um and you know I think we already see that like it's a bit like uh
I don't know uh religious pluralism or something right that or like the you know the distinction
between I was at the um I'm forgetting my theological history now but like there was this
there was the distinction between like the scholastics and the mystics right and the mystics
like this kind of pluralist view and the scholastics like this very kind of restrictive view and in
that sense I'm much more of a kind of religious mystic I you know like again you see this a lot
even in existing science I mean it's uh the example I like to think about quite a lot is
is Hamiltonian versus Lagrangian mechanics in physics right so um ontologically you know like
Hamiltonian Lagrangian mechanics are mathematically equivalent um they they you know if you formulate
Hamilton's equations you can derive the Lagrange principle and and vice versa um there's no
physical difference between them there's no mathematical difference between them they are
it's not like one is correct and one is incorrect they are the same thing in terms of their explanation
of observed phenomena that they're underlying mathematical formulas it's all isomorphic um
but they they have very different ontological interpretations right metaphysically what's
happened you know Hamiltonian mechanics is much more like sort of the Newtonian interpretation of
you know a clockwork kind of evolution Lagrangian mechanics seems uh teleological right it's saying
like your your you've got some teleological principle that you're aiming towards and everything
is kind of shaping itself towards trying to achieve that principle and it's not I don't think
it's meaningful to say that one of these things is true and the other is false it's the one of
these ontologies is true and the other is is is incorrect it's that both ontologies are valid and
equivalent ways of formulating the same set of phenomena and uh you know you see that with
interpretations of quantum mechanics and my suspicion is that we that that in a sense that
that is the and maybe this is overly pessimistic and maybe there will be a true fundamental theory
of nature or whatever but my suspicion is that actually that's the best that we can ever hope
for right is that we'll have a collection of uh mathematically informally and observationally
equivalent indistinguishable models that have very different very different radically disparate
potentially incommensurate uh ontologies that that that are implied and uh I think some people
view that as kind of pessimistic that it's like that means that there is no ultimate explanation
I personally view it as extremely optimistic because I like the diversity I like the fact
that there's that there are many many different ways of thinking about the same problem and that
each one gives you a slightly different new perspective on on what's going on and somehow
I think the world is more interesting if there's uh if there are multiple ontological lenses by
which you can view the same phenomenon um but anyway that would be uh not that I do it's what
I'm saying is not that I don't think your speculation is would be extremely beautiful if
it were true it's just uh it doesn't it doesn't align with my personal suspicion of what what I
think will will ultimately happen and I think it's also your your your view is realistic in the sense
that we're computationally bounded observers so even if we can we just like we can you know
define uncomputable functions we can define a potential singular unique ontology but to actually
prove or or make definite statements about it is is almost like limited by our computational
boundedness right and we're diverse right I mean that's it's related to the thing we were talking
about in the beginning that we you know you were saying we both have in common that we don't that
you had this very nice description of like you think in geometry but only kind of locally and
you think globally in the logical structure which is very much how I how I feel about it like it's
you know or like geometrical intuition I find is useful for communicating ideas to other people
but it's not how I tend to think about it myself just like you know people often say this about
words right when you're thinking about a problem very often the thought is occurring non-verbally
and it's only if you try and do the metacognition of figuring out what it is you're thinking or
explaining it to someone else that suddenly it's like the wave function collapses and it's all in
language um and so but you know people so my point is that because we're diverse because
not only are we as observers computationally bounded but we're not even homogeneous right
as a set of observers we are extremely computationally heterogeneous we have very different uh
the the kinds of models that will be intuitive to me may be very different to the kinds of models
that are intuitive to you and so that's another key limitation I think from this idea of like
there's some ultimate theory or some ultimate ontology because it presupposes something which
I think is not true about humans which is that the you know that then even if you end up in the
situation where which might be the case that there is some uniquely uh minimal in terms of like
algorithmic information theory that there is some uniquely minimal encoding function
that defines the the ultimate ontology for you I think it's I think it would be um
you would be assuming far too great a homogeneity of human thinking processes if you were to
presuppose that would be the true for everyone else um and so it may be that in order to convince me
that your ontology that your theory of everything is actually true you might have to cast it in an
ontology that's completely you know weird and counterintuitive to you but you know somehow
fits much more neatly into my own processes of thinking yeah absolutely and let me let me just
we're I'm going to close here just because I don't want to take too much of your time but um
I wanted to get a little bit into pre geometry so let me just I just want to read a statement
from your I think it's one of your most recent papers with Xerxes on the pre-geometric
pre-geometric spaces it's page 38 and um I just found this statement so interesting
and so I'll read it for you and then I'll ask you to just comment on if that's okay
so a noteworthy feature of the synthetic geometric approach that we've used in our
constructions is that no now one does not require to make ad hoc assignments or assumptions about
geometric structures on local entities of the model such as assumptions about a background
geometric space or discretization schemes defining use pre right so instead of geometry
in the form of cohesive structures is inherent inherited functorially using global properties
and as a consequence is naturally induced upon local structures by taking sections or projections
of the total space so I just want to pause there and and say here the total space would be like
that you know the sort of would that be would that be referring to like infinity categories or
or or what yes exactly I mean so so infinity categories in the in the abstract sense
and I suppose rule your multiway systems in the concrete sense yeah yeah yeah okay great so so then
the the this gives such a natural way of linking syntax and semantics right so like and you mentioned
in the applied compositionality vision that functoriality is the key to sort of uniting
syntax and like abstract syntax of of a really a multi or sorry of a of an infinity category and
then sort of like the objects like the semantic objects which would be like the really a multiway
systems themselves that are traversing like evolution paths right and so I mean I'm still
trying to bridge that intuition because I know that like each category has objects and morphisms
and then like functors link different categories together right and then you have like natural
transformations between functors and so like what is it about a functor as opposed to like a
natural transformation that would lead you to and I I know you mentioned you know generalizations of
like naturality but like what is it about a functor specifically could we interpret functors as
sort of like the processors and like natural transformations as like as like new programs
that the processors are generating or is that just or are you are you diverging there I think
in that analogy it's more it would be more like the functors are acting like compilers right so so
that yeah the point with a functor is that exactly as you kind of mentioned it's a way of you know
if you imagine every category is defining a sort of language so when you give it an explicit
computational semantics it is very concretely it is a computational model right every object
to some data structure every morphism is some computation and then the category defines the
algebra of how those computations compose together to make programs and then when you
when you apply a functor you're changing the interpretation you're changing what the data
structures are you're changing what the computations are but in such a way that the language that the
grammar of how they compose together to make programs is unchanged or at least is changed in
a very predictable way and this is exactly how compilers work right when you when you compile
from one programming language to another what you're doing you know you're changing the underlying
data structures that are being manipulated and you're changing the elementary functions that
are being applied but you're doing so in a way that the overall grammatical structure of the
program is preserved and that's functoriality and so yeah so so the the hope is I mean this is not
you know we don't know this to be true but it's kind of again it's one of the it's one of the
many prejudices that is that is implied by by the principle of compositionality is this idea
that you can in general interpret you can you know okay if we take the linguistic example so
you can have imagine you can imagine having a category a syntactic category in a semantic
category so you've got a category where the objects are just kind of empty symbols and
your algebra is just designed it's just specifying if you like the you know the the grammar for how
those empty symbols can be linked together so you have a thing that just said you know verb
adjective whatever I don't know but and you and you've got some you've got some purely syntactical
category there's no meaning there but then you could imagine defining a semantic category where
now it's got the same obviously the grammar is preserved right because once you get once you
take the the abstract grammar and you you know and you imbue the individual components of it with
semantic content the grammar doesn't change it's just that all you've done is there's new structure
there that wasn't there before but everything else is preserved and so then you can you can imagine
building a semantic category which has the same compositional properties but where the objects
now have actual semantic meaning and that and then so then the statement that when you imbue the
the individual morphemes or the individual empty structures with semantic content the statement
of that process doesn't change the grammar is the statement that the map between those categories
is functorial that's the sense in which we say that the functoriality you know that the map
between syntax and semantics we think is is functorial in nature and so you bring up this this work
that Xerxes and I did uh primarily Xerxes I should say but it's uh so the um that's an example of a
situation where so the idea the that particular instantiation of this idea is based on the the
thought that um we've got if you like we've got algebra we've got algebra which gives us kind of
the logic of how you know how these discrete structures blew together and whatever and that's
somehow that's like our syntax that's like our grammar um and then we've got semantics what is
the semantics in physics or in a model of physics well it's kind it's it's these things like
geometrical structures right so the the um because you know the uh when we take say a causal graph
that represents the causal structure of spacetime and we talk about its manifold properties and we
talk about its decomposition into space like hyperservices or whatever it's like we're taking
this purely anonymous syntactical object and we're imbuing it with kind of semantic content we and
with the semantic contact here is topology and geometry uh the topology and geometry is not part
of the of the syntactic structure it's something you impose on top of it that's consistent with it
and therefore functorially related to it but is not implied by it directly and so the hypothesis
was that this is really an instance of this kind of general functorial map between the
if you like the the mapping between the algebra of how multi-computations are composed and the
semantics of how you from that get you know geometrical structures that you care about in physics
our hypothesis that said or that is a central hypothesis was that you know this is a this is a
an essence that this is an sorry an instance of uh of that kind of functorial transformation
between syntactic syntactic and semantic categories and that construction from the
the paragraph you described is just a particular way of of uh of kind of enacting that right so
the the principle there is you say well okay um maybe i have to go on a slight diversion about
infinity categories but so the you know the the idea is that um okay there's this idea called
growth in the hypothesis which kind of gets to the heart of like or supposedly it if true would
really get to the heart of what it means for something to be spatial uh what it you know what
it means for something to have spatial structure and have topological structure and the idea is
that it comes from the following intuition that's actually quite a quite a nice intuition if you
think about it in the right way so if you've got some space uh then you can look at points in that
space and you can look at paths between the points in that space and if you if you imagine looking
at a pair of points that have two different paths x and y between those between those points um then
you could look at the continuous deformations of those paths the so-called homotopies right
and if there if there's like a hole between those points then there'll be some obstruction right
there'll be some homotopies you can construct and some that you can't and so you can imagine
constructing what's called the homotopy space of the homotopy type um which again as a spatial
structure is another topological space a higher dimensional space where now each point in this
new space is a path in the old space and each path in the new space is a homotopy in the old
space that gives you the first sort of first homotopy space the first homotopy type of space
you started from and then you can do this construction again you can construct the homotopy
type or the homotopy space of that thing and go and again and again and again and you get these
progressively higher dimensional structures and uh when you do this in terms of categories you you
know the if you like the points in our objects and the paths and our morphisms and the homotopies
are now two morphisms that build some higher categories um and yeah so then the idea is that
this gives you a kind of an inductive construction by which you can build you can start from a one
category of some topological space and you build up to an infinity category uh that is the kind of
infinity homotopy type and then the remarkable thing is that once you've built this big ladder
to get up to this infinity category you can actually kick the ladder away right that somehow
that up to weak homotopy equivalents the structure of this infinity category the structure of this
this this uh this infinity homotopy type completely determines the topological structure of all the
spaces below it in the in this whole infinite hierarchy which is not true of any of the others
right all of the others don't have this property it suddenly only happens in the infinite limit
so then growth and deeks said well maybe therefore we don't need all we need is infinity categories
right if somehow this if these infinity categories you know so from any topological space you can
build up this infinity category that completely determines the properties of that of that space
and all of its homotopy types up to weak homotopy equivalents uh then maybe the correspondence
goes in the other direction as well so maybe it's not just that all spaces can be interpreted as
infinity categories but maybe all infinity categories can be interpreted as spaces by
the same construction so growth and deeks hypothesis is really this like this idea that
that infinity categories and topological spaces are the same thing but infinity categories are
really just a way of thinking about topological spaces in the most general sense um so this like
the idea behind this in this paper was saying okay one way we can explicitly construct this
functor from if we think about again we think about like um the algebra for how processes in
quantum mechanics and general relativity compose together if we think about that as being like
the syntax of physics and then if we think about spatial structures like spacetimes and
Hilbert spaces and whatever as being like the semantics of physics one way we can explicitly
build this functorial map between those two categories is we can start from a multi-way system
we can then introduce these higher we can introduce these homotopies between the paths to
get these to get these progressively higher order multi-way systems to until we build up
to a rule your multi-way system that is the concrete instantiation of this infinity category
and then all the lower multi-way systems are then just sections that we take through that
through that that rule your multi-way system and then if that's true and if we take the
sections in the right way then the then the then there's a possibility that the reason why because
you might ask well why does a multi-way system have a spatial structure why would a causal graph
have a spatial structure right it's a discrete thing right why why why would why would we have an
interpretation of this in terms of spacetime well then you know if this hypothesis is really true
then all that's happening is you've got this rule your multi-way system that has inherited
spatial structure from some kind of groten-dechian origin you take a global section through it you
get an ordinary multi-way system where you get a causal graph or you got a hypergraph or something
and then the spatial structure of that causal graph or hypergraph or multi-way system is inherited
from the from the spatial structure of the of the ruling multi-way system of which it is a section
and if that's true that would allow us to explicitly construct this sort of functorial
correspondence between the syntactic category and the semantic one so it's there's much more I
can say about this because I mean for instance like one thing that we're interested in looking in
in the center is this relationship so I believe and have believed for a long time that there's
that this there's really a deep relationship between semantics and causality that I think because
when you build a multi-way system there are many many different ways that you can equip
it with a causal structure right because to equip it with a causal structure you actually have to
know what the individual states and events were to be able to determine whether the events are
causally related so it seems to me that if the multi-way system is like syntax then the causal
structure is like a is a phenomenon of semantic it's a it's a by-product of semantics and my
conjecture is that actually it's not just a by-product it's the causal structure is the semantics
that you can you can always reconstruct one from the other and so I have a I'm one thing that I'm
interested in using kind of the center's resources to try to investigate is this hypothesis that
basically causality and semantics are somehow in one-to-one correspondence and so for instance one
project that we're currently working on is looking at is trying to do a kind of big inventory
of all the different notions of causal semantics that have arisen in physics and computation
in chemistry trying to encode them in the most general way using these algebras of concurrent
processes and petri nets and sigma nets and so on and trying to construct a kind of unified
compositional semantics that describes them because my hope is that that unified compositional semantics
will give us a general way of constructing these functors that let us map from syntax to semantics
or you know to put that in a computation in a computational way for a given multiway system
allow us to construct what possible causal structures are consistent with that multiway
system and which you know and which would be inconsistent which right now we don't have a
way of doing but I think there is I think I think there is a there is a formalism by which that can
be done but we we haven't invented it yet so so the hypothesis would be and correct me I just want
to have a little fun here so at the infinite at the infinity category limit
the syntax is functorially equivalent to potential semantic territory at the infinity
category limit exactly exactly so it's like so somehow yeah so it's like it's like down here
when we're scrubbing about at the one category or two category level it's like the the the
symmetry is broken right so syntax and semantics are different things and to to to take a one
category and equip it with semantics you need to basically equip it with some causal structure
that that is you know in the way we traditionally formulate it in the context of our project we
done that by equipping it with a two category structure where the individual morphisms have
two morphisms between them that represent causal relations um but yeah so in a sense the exactly
as you say when you go to the infinity category limit it's like well now you've got all possible
higher category structures are inside your infinity category so it's not so you don't you
don't just have all possible semantics all possible syntaxes but you will somehow also have all
possible semanticses and so so everything all the syntax all the semantics all the causality
that you could ever hope to have should somehow be contained in that infinity category object and and
if you take sections in a sufficiently careful way should be it should be able to be kind of
functorially inherited now yeah exactly that's a really nice way to think about it well it's a
it's a beautiful idea i'm going to be playing paying very close attention to the the center's
activities i saw a couple of specific projects and the applications to like systems biology even
is very very exciting so we're gonna i'm going to close the video now thank you so much jonathan
if you could stay with me for just like a couple minutes after i stop the recording that'd be
amazing but thank you so much thanks very much for having me but uh this is this is a lot of fun
