A lot of this work was pursued during COVID isolation alone in my bedroom, and it's really exciting to now be sharing it in front of a lot of people in person.
So, the totalist work is attention approximates for distributed memory.
It was done by myself in collaboration with Jengis Palavan, and I'm now advised by Dr. Creeman, and let's get into it.
So, first of all, why should you care.
We show that the heuristic attention operation can be implemented with simple properties of high dimensional vectors in a biologically plausible fashion.
The transformer and its attention operation are incredibly powerful, but we're heuristically developed, and the softmax operation in attention is particularly important but also heuristic.
Now the intersection of hyperspheres that's used in sparse distributed memory closely approximates the softmax and attention operation, both in theory and train transformer models that we investigate.
So SDM thus preempted attention by approximately 30 years having been made back in 1988 and meets a high bar for biological plausibility, particularly it maps compellingly to the unique wiring of the cerebellum.
So, as an overview of this presentation I'm first going to give a summary of sparse distributed memory, and then I'm going to give a summary of transformer attention.
I'll then show the relationship, and how SDM can also interpret the transformer more broadly, and then if there's time which there probably won't be.
I want to give a review of SDM's biological plausibility in the cerebellum. So we'll see how we do. I'm going to prioritize visual intuition and then try and get into some of the math.
And please, please ask questions whenever.
So first and overview sparse distributed memory. So the fundamental question or problem is trying to solve is how can the brain right and read memories in order to retrieve the correct one later.
And so there are a few considerations around this. First we want high memory capacity. We also want robustness to query noise and we're trying to retrieve a memory later.
We also in this case want our system to be biologically plausible. And we also want fault tolerance, and that we're resistant to cell death, they're on death.
So what makes SDM unique versus other memory models.
And this kind of comes from its name so it's sparse, and then it works in a high dimensional vector space, and neurons exist and only a fraction of possible locations in that space.
Secondly, it's distributed in that the read and write operations you're doing apply to all nearby neurons in that vector space.
So first getting into the SDM right operation, we're storing patterns in nearby neurons. So in green here we have our first pattern.
It's appearing in our high dimensional binary vector space will move to continuous later but for now in a binary binary space we're using hamming distance.
So the green pattern is going to activate neurons in this right radius around it. So the neurons are these hollow black circles and write itself in. And just as a side note that this pattern, it can either write itself in an auto associative setting, or it can
write itself in a different pattern point to a different pattern in a header or so set of setting. For example, that would be if I remember in the alphabet, then my first pattern here would be the letter a.
It would write in a pointer to the letter B. And then if I query for B, it would point to C, etc.
Okay, so now we've written that green pattern into nearby neurons. And so you can see that the green pattern is inside the hollow black circles, and we're also keeping track of the original pattern location, and that will be important in a second.
Yep.
What space are these now?
I don't want to get too much into biological plausibility right now, but the, you could think of the dendrites that the neuron has correspond to a vector, which represents a particular location in this high dimensional space.
So the neurons all have like an address in this space, they exist in a particular location, and then a pattern, as it's processed by sensory stimuli, kind of up the layers of the brain, will have some vector representation that will also define a location in this space.
I have the same question.
Yeah, yeah.
No, it's it's the neural population code for the two vectors would be similar.
Yeah, yeah.
Thanks for asking.
So there's addresses are similar that means the neurons that they're synapsing from are similar they have a high input similarity.
The dendrites of a neuron so what it is sensitive to.
What is activated by will be close in space.
So, so the pattern corresponds to a population coder, particular vector, and neurons that have a high similarity to or looking for patterns like that will be close in space, and therefore they'll be activated.
Mathematically, these are binary factors.
Yes.
High dimensional binary vectors.
When you say.
I'll get more into the math in a second. Yeah.
So you can just think of these neurons as they have a storage vector and they're storing this pattern inside of it. So here we're writing in a second pattern.
It has a different location in the space. And it's also activating nearby neurons.
And then it's writing itself in. And now note that some of the neurons are storing both the green and orange patterns.
And you can think of this as it's storing a set of patterns, but in reality each neuron has a one storage vector, and we're doing a summation of the patterns that we're storing.
And because we're in a high dimensional space you can think of that as a superposition of the two patterns being stored with minimal crosstalk between.
So finally we're going to write in a third pattern.
We're going to start again, and something that's important for later is note that although these patterns are ephemeral so they've disappeared.
The original locations can be triangulated based upon the nearby neurons that are storing that pattern.
So now we're getting into the right the read operation. And so in this case I have my pink queries I, and it is also going to be activating nearby neurons, those neurons are going to output the patterns that they've stored.
So all of those patterns are going to be aggregated, and then we're doing a majority operation. So what converge towards whatever the dominant pattern was.
And so in this case the query is getting one green two orange and four blue patterns, and the blue pattern dominates such that it will converge towards blue.
Also that's fitting because if you look at the location of the query, it's closest to where the blue pattern was originally written that.
So looking at this where we abstract away the actual neurons is just considered the original pattern location. And then we can, we can just look at the intersection between the original right circle, and the read circle for our query.
And so in the bottom right here I've replaced the number of patterns with the size of the circle intersections. And it's this circle intersection relationship here that will be crucial to mapping on to attention later.
So now getting into more of the mathematical formalism. Here I have my blue pattern and the intersection with the query.
And I'm defining that intersection is the number and the number of neurons in it with a cardinality operator, and then the neurons that are in the pattern intersected with the neurons that are in the query.
And now I'm going to break down this equation for the read operation one step at a time.
So, first of all, for each pattern, where, which is denoted by the law of the ball P at the far right, we're waiting it by the size of its circle intersection.
We're then summing over all of the patterns.
We're then normalizing by the circle intersection weights because we ultimately want to be able to compute a majority. So we need that normalizing constant.
And because we're in a binary space we need to map back to zero one. And so we have this majority rule function g.
Okay, I'm now going to give, and I'm actually not sure if I addressed your question with this because I'm focusing on the high level relationship to attention.
The query talks more about the way that the patterns are stored but it is the superposition. So, I'm doing a superposition all of all patterns that are stored in that activate that neuron. Yeah.
So, this is storage room where the neuron is in the space.
So it's a key value pairing. So the neuron has an address where it exists in the space, and then it's stored vector that it will return. So you can you could think of that biologically as the dendrites that activate that neuron, and then the synapses it has with effort.
It's effort connections downstream.
So this, this is the, in order to show the relationship to attention, I'm abstracting away the actual neurons.
So that's this slide doesn't show the synaptic weights, but but there's a one to one mapping between the actual the neuron perspective and the pattern perspective.
I just can't get into the SDM biological plausibility. I have slides on it at the very end, that hopefully we can get to, but I want to I want to focus on the relationship to a transformer attention here.
Yeah, so it was invented by.
He published his book on it in 1988. There were a couple extensions that were developed in a few years after and that kind of disappeared with the sense of time. And I'm still not sure why maybe have a better idea for for why that was.
And there were.
So, they were early associative memory.
Maybe we show.
Yes, yeah.
Absolutely.
So this is kind of connecting.
Yes. And it actually, I don't talk about related work here but sparse distributed memory can actually be written as a generalization of hot field networks.
So there is a special case.
Could that now.
Yes, there are still some differences, but they're very closely related.
So now giving a short overview of transformer attention for those who are unfamiliar with that.
And just quickly, I mean transformers are one of the state of the art deep learning models across many modalities right now. And so on the left here, the transformers is being used to predict the next word and generating texts in this case it has a wonderful story about
that you can read on open eyes website.
The attention operation is also applied in two different locations in alpha full two, which was recently used for protein folding prediction.
It's also used for almost every Google search query now.
It doesn't just do language processing it's also moved into image classification and generation tasks. And so, as a kind of fun one here, you have this model input which is half of an image.
And then it needs to predict what the other half of the image would look like. And you can see that here it's getting the shadows correct and some different examples and the originals on the far right.
The core thing that makes the transformer unique is the attention operation.
And I have this slide here just to show where it's doing next word prediction. That's the example we're going to be working with. And in this case the animal didn't cross the road the street because it blank.
And our query to this system is the word it, and we're then looking back at the rest of the system deciding how to pay attention to it to predict what word comes next.
And so, here our word it has connections with previous words that it's using to them predict what comes next.
And so, diving more into how that works exactly. I'm going to work with a simpler phrase the cat sat on the blank.
So, here are four things that we do in order to predict the next word. And so, first, we take each of our input words, and we create what are called keys values and queries.
So each word, aside from the last one turns into both a key and a value, and then the last word becomes a query.
And then use the query and compare it to each of our keys. And the way we do that is we use a product operation.
And we then take the size of those dot products with each input, and we normalize, and we're using the softmax function for normalization, which is crucial to the relationship that all that are so close we'll get into that.
And then finally, based upon the weights. So how much attention we decide to pay each of our inputs, which is based on how again how similar the query was the keys, we're then going to take the corresponding value that's paired to each key, and do a summation operation of each of those values.
And then from that we'll do some projections and predict the next word.
So, as just like a fictional example we're really going to work through this, the cat sat on the blank. So our query word here is the, and you could hypothetically think that that query is a vector that's going to have a high dot product or be similar to keys that are either
nouns, or they're associated verbs. And so, in this case you think it might have a high similarity with the words cat and sat their keys.
So the cat and sat value vectors, and those that the cat value vector, you might think of as containing some superposition of other animals that are related to cats, and maybe words that rhyme, for example the word Matt.
This is this is totally fictional what I'm just trying to give intuition right.
And then the word that the sat value vector that corresponds to its key we're paying attention to it contains things that are sat on. So, including that. And so, what you could have is when you when you're paying attention to both cat and sat value vectors, you're doing this summation
operation. And then you have all of the different possible vectors and superposition. And in this case I have a way to three on Matt, and then a way to one on mouse and so far and it goes on and on, but you can think that the word Matt would dominate in the summation
operation, allowing us to then correctly predict the next word that comes.
So, and I guess one piece of intuition here is what you pay attention to, and what you should extract from it are different. And so that's why we have the key value pairing, and we're paying attention to the keys and we're extracting information that are in the values.
So, quickly, just to like show you some of the notation for this because then we're going to map it on the SDM.
Here I have my query that's being updated, and it's being updated using this equation, and these are the WZ you can kind of ignore the projection matrices.
And so, yeah, I break this down more clearly here. So, first, we're doing a dot product between our keys and our queries. So that's shown by this operation to here.
And so the, the wise are the actual inputs, we then do this projection of them. And then we do a dot product.
And we have the softmax operation.
And the way this is actually defined as an exponential over a sum of exponentials. And to give you some intuition for softmax normalizes the weights but it makes large values larger.
And this relationship is crucial to SDM so it's why I'm spending a lot of time on it. So just as this demo here, I have these inputs on my x axis, and they each have some value to them, ranging from zero to five.
And the second plot here I just do a normal normalization, in which case like that largest value, which is as an index of four, normally has value five it's if it's normalized regularly it'll just have a value of 0.3.
But if I'm using a softmax operation, depending on my beta coefficient in the softmax, it'll it'll have a value 0.6. So it becomes much pointier peakier than it would if I was just doing an ordinary normalization.
And so just relating all this back to the equations again I do my softmax operation, and then I use these normalized weights to weight the summation of the value vectors, we talked about before.
And that gives you that full equation, and hopefully some intuition for it.
Okay, so how does transformer attention approximate SDM.
Well, it turns out that in a high dimensional space, if I have two hyperspheres that as I pull apart those hyperspheres, the size of their circle intersection, the number of neurons that they share will decay approximately exponentially.
So in this figure on the right here on the x axis I'm showing the hamming distance between those two circles as I pull them apart. And on the y axis, I'm showing in log space, the number of neurons that exist in that circle intersection.
And because in logs places plot is approximately linear. It means that the number of neurons in the circle intersection is approximately exponential.
This is just one set of SDM parameters, but I'm using n equals 64 dimensions which is the normal dimensionality used in transformer attention.
And do note here that the exponential approximation doesn't hold for all hamming distances. It works best for patterns and queries that are close to each other, when the circle intersection is large, but that's the regime that we care about.
Because when we do the softmax operation and then have our normalizing constant anything that's far away basically drops to zero.
So, here we have our equation for the circle intersection we can write it as approximately exponential with a coefficient outside of the exponent, and then the beta coefficient inside of it.
And there are two things that we need to do to make this relationship a good one. First we need SDM to be continuous.
So, what we need there is we need a mapping from our hamming distance into cosine similarity, where we're able to normalizing our vectors, and then taking their dot product.
And this equation here is just the linear mapping between hamming distance and cosine similarity.
We also need the beta coefficient insider exponential to be a correct value that it can fit our exponential decay. And the way we can do that is in a closed form with just log linear regression on our circle intersection.
And so I'm going to show you. Okay, so here.
I'm just redefining the attention operation we went through before into the SDM notation. No other tricks.
And this is the real money slide, where it's the relationship between SDM and attention so I've expanded out the softmax operation on the right here, and so the exponential or some of exponentials.
I have the SDM equation, really into before, and the extent to which the circle intersection in SDM is approximates an exponential is the extent to which SDM and attention converge.
So I have some some results, first in theory. And so I have two plots with different SDM settings for a small and large hamming distance that we're using the size of the circle radii's.
And in blue, I have the actual circle intersection, which I'm normalizing just just a basic normalization right, and then in orange, I am fitting an exponential with the beta coefficient using my log linear regression to the circle
intersection, and then I'm using the softmax equation.
And so you can see the quality of these approximations in these two different settings, and then the sub plots I have their, their log plots. And so you can see that in this case with a larger hamming distance that exponential eventually the approximation only holds for closer
points, but by the time I'm at a distance a hamming distance of 20 here where it drops off, you can see that my normalized weight is basically zero anyways.
So, we've talked about the relationship between SDM and attention, but how does SDM relate to the transformer more broadly. And so one way that we can look at this is to what extent do transformers use beta coefficients in their attention
operation that are similar to those for optimal versions of SDM.
So depending on what I want my SDM system to do. If I want to store the maximum number of memories possible.
Versus if I want to have my system very robust to query noise.
I will use different hamming radii. And in order to compute this, I need to assume that my patterns are random. And so this won't apply to the real world where of course data is on some lower dimensional correlated manifold.
So I can still get these values with random patterns and see how they map on for the beta coefficients that transformers decide to use.
And so, in this case I'm using the key query normalized attention variant of the transformer, which actually learns its beta coefficients so it makes it very easy to look at this.
Because normal transformers don't learn the beta coefficient you have to kind of infer what it would be from the size of its dot products between queries and patterns.
So this histogram shows the learned beta coefficients across attention heads across layers for this transformer model.
And the vertical red lines are three different definitions of optimal SDM. And so on the far left, we're maximizing for query noise, we want our queries to be as noisy as possible but still work.
In the middle we're maximizing signal noise ratio, and on the far right we're maximizing memory capacity. And you can see that the learn beta coefficients fall within these bounds, and also it skews towards max query noise, which I think makes sense because
if you're maximizing memory capacity you're assuming your queries are noise free. And if you're training a model in a deep learning environment without a distribution training data always appearing, of course that's not going to be the case.
So, beyond attention, how can we interpret other parts of the transformer. Well, there's some interesting work showing. Yeah, yeah.
Yeah, these values, the translation of the terms.
Okay, yeah, thank you. I mean, SDM and attention to use different notations so I just needed to.
The values keys and where it's come from, previous use in the information of it to be more. Yeah, something historic. Yeah, yeah. So values are the patterns of the one first.
And again those pattern pointers, you can be in auto or heterossociative setting. So the pattern pointer can even either equal the address it can point to itself.
Or exactly. Yeah. And so in a transformer setting of course where you're trying to predict the next thing, it would be heterossociate.
Yes, exactly. Yeah.
And this this work has been accepted to nurse and the paper will be out a week from today.
We don't have a preprint yet but it'll be the camera ready version is next week today.
So there's some interesting work that we set in the paper that where that other people have done showing that the feed forward part of the attention.
Sorry, the transformer as a whole. And I should have said before, this is the whole transformer architecture with each of the operations laid out.
And so we can actually interpret that feed forward layer as doing a long term version of attention. And so then we can interpret it as doing a long term version of SDM.
And by long term here I mean when I'm doing normal attention, my keys and values are a function of my receptive field, the current inputs that I'm looking at.
And this longer term attention is actually independent of my particular inputs that I'm working with, it'll store longer term memories across the whole training, multiple epochs.
Exactly, exactly. Yeah.
And that actually relates to the neuron versus pattern perspectives of SDM that I talked about a while back.
We can also interpret layer norm, which has been shown to be really important people tried to get rid of it.
And this is in the sense that in order to do SDM I need cosine similarity. So I need to L2 norm my vectors, and the key query normalized variant of attention that leads to some small improvements actually uses L2 norm instead of layer norm.
So you can kind of think of this work as retroactively predicting this improvement and layer norm approximating.
Yeah, so I basically, I look at my vector.
I compute for all the vectors a mean and standard deviation. And then I normalize by that, if you're familiar with the batch norm operation is kind of similar to that.
But it's a function of, I think you have a running average of all the things you've seen it's not just within the batch.
In the batch norm I compute the mean of everything in that training batch.
Yeah, yeah, but but they're, they're quite similar, and I think that they're putting everything on a similar scale, the same time the same way that L2 norm would.
So beyond these connections, SDM has a number of extensions that we think could be useful and further improving the transformer. So one SDM has some close relationships to vectors symbolic architectures.
There's also some work showing that you could have multiple value vectors corresponding to each key.
There are variants of self attention where you're not having every single input be its own query.
And there are other forms of external memory storage techniques.
And so in summary, the intersection between two hyper spheres approximates an exponential, and this allows SDM's read and write operations to approximate attention in theory and in the tests that we run.
And so as sort of big picture future research questions that we certainly don't have answers to yet, but I'm interested in exploring are is the transformer so successful, because it's performing a key cognitive operation.
And certainly pointing out the cerebellum or cerebellar like architectures are ubiquitous across a large number of organisms.
And so so so maybe there's some key operation that it's doing.
And given how successful the transformer has been empirically across multiple modalities is SDM the correct theory for how the cerebellum is functioning.
I think I'm pretty much out of time and want to leave time for a question so I won't get into biological possibility. It's in the appendix of the paper that will be out soon.
But it's quite exciting and how it maps to each of the cell types. And I'll just go to the thank you slide.
