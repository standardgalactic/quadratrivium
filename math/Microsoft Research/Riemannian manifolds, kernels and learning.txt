Each year, Microsoft Research helps hundreds of influential speakers from around the world,
including leading scientists, renowned experts in technology, book authors, and leading academics,
and makes videos of these lectures freely available.
Good afternoon, everybody.
Thanks for coming.
It's a pleasure for me to welcome back Richard Hartley to MSR.
He's visited us a few times before.
Richard actually needs very little introductions.
He's a very famous professor at the Australian National University.
Everyone knows that he worked on projective geometry for a long time, and he has a very
famous book, too, that's out.
A lot of vision folks have it, co-authored with Andrew Sisserman.
So there are some very little known facts about Richard.
He likes opera.
He likes to hike in dangerous places.
And he actually won a prize for drinking.
So at least two of those things are true.
Is that, Richard?
At least.
Yes.
Okay.
Thanks.
So, this is not all entirely my work, and really the fact that my name's first is simply
because I wrote the slide.
It has nothing to do with who did what.
In particular, the work at the end is done largely by my student, Sadeep, if we get that
far.
All right.
So what is a manifold anyways?
So I'm going to sort of start fairly slowly, giving some definitions and things, so those
who are not totally familiar with this sort of thing are not lost from slide one.
So this is, when I was doing research on this, I came across this, I think, in the National
Library of Australia, James Chester manifold 1867 to 1918.
From a family of manifolds in Australia, there were actually quite a lot of them, but I have
naturally enough nothing whatever to do with this talk.
So here are some examples of manifolds, which one would be familiar with, only sort of two
dimensional manifolds, and we'll come back to that slide.
So a manifold, or didn't actually give the definition, did I?
Okay.
So a manifold, something which looks sort of like Euclidean space if you don't look
too far.
So the surface of the earth, you may say, is a two dimensional manifold.
Every region around it looks like a region of the plane.
And we're not talking about geometric measurements, we're just talking about rubber sheet.
But well, for the time being.
So examples of manifolds, there are a lot of them, and some of them have not a lot to
do with computer vision, but others do.
So Rn is naturally a manifold, an n dimensional manifold, because around every point there's
a little bit which looks like a little bit of Rn, naturally enough.
So the sphere, Sn, is one which, you know, common manifold, well, like the surface of
the earth, S2, rotation space, that's one which I've given a bunch of thought to recently
and looked at in some detail.
Very simple manifold, of course, but there's a lot to learn about manifolds from looking
at SO3.
SO3 is rotation space, three dimensional rotations, space of three dimensional rotations, which
it's a five dimensional, sorry, three dimensional manifold that you can think about in lots of
different ways.
Positive definite matrices are a form of manifold, and I'll have a bunch to say about positive
definite matrices.
In fact, they form a cone, as is sort of well known from those who do convex optimization.
They form a cone in, you know, if we have an n by n matrix, then that sits in Rn by n,
so Rn squared, and a neighborhood there of matrices is like a neighborhood of Rn.
Grassman manifolds are an interesting one, I'll say something about them later.
The essential manifold sort of harks back to what I spent a lot of time thinking about
in previous years.
It's related to structure and motion and represents the rotation and translation of one camera
with respect to another.
So the set of all such possible rotations and translations is the essential manifold.
And one which I've sort of looked at a little bit recently, shape manifolds, which is sort
of an interesting thing, so yeah.
Okay, that's manifolds, I'll say a little bit more about Romanian manifolds in a bit.
But the other main topic of this talk is about inner products, Hilbert spaces, of course
they're tied up with kernels, which is the other thing on the title of the talk.
So what is a Hilbert space?
So a Hilbert space is simply a vector space with an inner product, all right?
For me, it's a Hilbert space with an inner product, so you know, dot product.
It does have one other property to really make it a Hilbert space, it has to be complete
under convergence sequences, but that's not really a very interesting or very important
to me at this point, so it's interesting, but not important.
So usually you talk of Hilbert space, you think of it as being infinite dimensional,
but that's not necessarily the case.
So infinite dimensional spaces, things like spaces of functions on the real line or on
an interval of the real line.
One of the things which a Hilbert space has, which I'll talk about a little bit, is a norm,
right?
So a norm is basically a length.
So you take some of xi squared, it gives you the length, well it gives you the square
of the length of a vector, sorry, you can get it from the inner product, x, x.
So if you've got an inner product, you've always got a norm.
The opposite does not, of course, hold.
There are norms which don't give you inner products, such as things like L1 norms, they
do not arise from an inner product.
But so norms allow you to measure distances, inner products allow you to measure angles,
right?
And distances.
So an angle, of course, you get by the well-known formula, the inner product gives you the cosine
of the angle, effectively.
Okay, so we'll talk a little bit more about Hilbert spaces in a bit.
Remanian manifolds, so okay, I said what a manifold is, what is a remanian manifold?
Well really the only thing important to me, particularly here, about a remanian manifold
is that you can actually measure lengths of curves on the manifold, right?
So here's a curve on the manifold, you can in fact measure its length.
Gamma t is a curve, you can measure its length.
To be precise, it's to do with inner products on the tangent space.
Okay, I didn't say, the way I think of manifolds, I'm taking a very intuitive view of manifolds.
For me, I think of manifolds as being embedded in some sort of Rn, like a surface in Rn,
right?
For a mathematician that's not entirely satisfactory, but for me a manifold is, think of like a
surface in Rn, the tangent plane is in fact the tangent plane, where, you know, in the
natural way, where a plane that meets at one point tangentially.
So one of the things you can do on a manifold is think of a direction in the tangent plane,
think of a tangential direction, and now head along this tangent plane, head along the surface
in that direction a certain distance, and the way I like to think of that is in terms
of riding a motorbike on a road which is very slippery, if you want to ride a motorbike
and not come off, you should arrange it so that your acceleration is always perpendicular
to the surface, otherwise you'll slip sideways and fall.
So a geodesic is a curve, which is what I'm describing here, is a curve is normal is always
perpendicular to the surface, perpendicular to the surface.
So that's a geodesic.
Geodesic can also be described as the shortest path between two points.
So if you take two points on a manifold and take an elastic band and pull, it'll settle
down on a geodesic, the shortest path, or at least locally shortest path in the way
I'll describe.
So that's what a geodesic is, a taught piece of elastic band, curve and surface accelerations
always normal to the surface, and always also a local distance minimizing curve.
You can break up the curve into little sections so that it is the shortest path from gamma
A i to B i, okay, or maybe.
Because you're talking about acceleration, so you have to assume that manifold like C2
continues.
Yes, I should have said, I shouldn't use the word smooth here.
I'm talking about smooth curves, smooth surfaces, in fact, and there is a hierarchy, but I'd
first describe as the manifold, it was a topological manifold, next sort of level of hierarchy is
what you call a smooth manifold.
So on topological manifold, you can define continuous functions.
On a differential manifold, you can describe differentiable functions.
And on remaining manifold, you can describe depth and angles and things like that.
So it is, in fact, differentiable and smooth in my way of thinking.
Okay, so here's an example, for instance, of a geodesic, and although this red line
is a geodesic, it's obviously not the shortest path from there to there, but locally it's
the shortest path, it's made up of a whole bunch of little shortest paths, and that is
the geodesic, a great circle.
Here are examples of geodesics on various manifolds.
So on a simple thing like a torus, the geodesics are remarkably complicated, in fact, on a
pretzel, even more.
And on things like cones, they're actually quite easy.
You draw a paint line on the ground and roll the cone over it.
It will, the paint will transfer to the cone in a geodesic, on a geodesic line on the curve.
That happens to be examples of them, you see.
So getting back to this thing here, there's a thing which I'm going to talk about called
the exponential map, which means that if you start here with velocity v heading in
such a direction, and you go along the geodesic for time one, at velocity v, you'll get to
a point on the manifold, on the surface, and that is known as the exponential of v.
Don't ask me why it's called exponential, it has something to do with the way these
work with matrix spaces, but it's the exponential, it's a mapping from the tangent space at
a point to the surface, to the manifold.
And the mapping which goes the other way is called, naturally enough, the logarithm map.
So a lot of algorithms, which I'll, well, talk about that when I get to it.
So geodesics and the exponential map, I've said that.
So that allows you to do convex optimization on manifolds, so I've looked at that a bit
in recent, particularly convex optimization on SO3, I've looked at it recently, because
the structure of manifolds, you can start doing things which mimic convex optimization
and optimization theory on a manifold.
So convex sets are defined in terms of geodesics.
So geodesics is sort of like a straight line, it's your replacement for a straight line
in a manifold theory, and you, the straightest, the path between two points, if the geodesic
between two points, the shortest geodesic lies inside a set, it's called a convex set,
and functions the convex, if they're convex when restricted to the geodesic.
So you can do a whole bunch of things on geodesic, on convex optimization.
You can also do things like define whether a function is, well, you can define a Hessian.
A Hessian is defined like this, if you have your manifold, and I'm supposing we have a
function from the manifold to real numbers, I can define the Hessian of that function
by starting out from the tangent space and mapping by the exponential map and then the
function, and if that map all the way across there, which is now a map from Rn or Euclidean
space to R has a Hessian, and if that is positive definite, the function is convex.
So it allows you to do all sorts of convex optimization, convex is positive definite.
So this allows you to do iteration algorithms on a manifold, there's also a gradient was
defined there, the gradient is, the gradient of this function is also just the gradient
of that mapping, same thing, and so you can define things like gradient descent algorithms
on manifolds and Newton algorithms on manifolds using the Hessian, but that's not really
what I want to talk about.
But before I get off the idea of optimization on manifolds, I think I talked about this one
here before, maybe about the Weisfeldt algorithm on a manifold, where the Weisfeldt algorithm,
a bunch of points on a manifold tries to find the point, which is their median, in the sense
that it minimizes the sum of distances to the points, and your standard sort of algorithm
for doing these sort of things or optimization in general, you knew how to do optimization
on the tangent space, because it's Euclidean space, it's flat, right, is to map, start
at a point, map up to the tangent space, do a step of optimization, and map back down
again by the exponential map, then take another tangent space and keep going. And so an interesting
area of study is when are these algorithms convergent, what is the radius of convergence,
and things like that, so we're looking at that sort of problem on SO3.
So the P and X are supposed to be the same or are they the same?
Yes, they are, yes. Just two different diagrams, I guess I've ensured in different places
on the information. So, example of this is rotation averaging, where you can rotate,
do rotation averaging on SO3, this is SO3 being a set of rotations, so this comes back to sort of
structure in motion, where you've got a camera moving around, and you can work out what the
relative motion of the cameras are, R1, also R2 to R3 methods are based on fundamental matrix
or essential matrix, you can work out R2, R1, you can work out all the pair-wise things,
but for consistency, because rotations form a group, you need this relation to hold.
So the question is, find the absolute rotations of each of the points, it's an example of
averaging on a manifold, in this particular case, the manifold of rotations. So this I think
I've shown here before, though maybe you don't remember it, if you were here, even at my talk,
but we did this with the Notre Dame dataset, where these are all different positions of
cameras, images of that, 569 images, 280,000 points, and 42,000 pairs of images that overlap
by more than 30 points. So if they've got that much overlap, you can work out relative rotation
between these, but some of them you can't, they don't see anything, so you get a sort of a graph
like that, where these are cameras, and you can work out the relative rotation from one to the
other of the camera, and then the task is to work out all the rotations, and I won't go into the
details of the algorithm, but it's an averaging process where iteratively we average the rotation
of one based on the supposed now known positions of all its neighbors, and the relative rotations, so.
But you can also encode the confidence of estimating how much rotations, right, because
yes, because depending on your reliability of the consequences, you may not be reliable.
That's true.
That's true. In what we were doing this, we didn't actually do that. Instead,
we did the algorithm. I said the vice-versa does an L1 optimization, so L1 is basically robust
rather than L2, so bad measurements are sort of not so important, and once you've actually
done it, you can work out, in fact, if there are some measurements that you got that are bad,
because you can sort of look around the loop, and if the loop doesn't add up to 360 degrees,
you know, roughly speaking, one of the estimates around there is wrong.
And so, you, the processes that you could think of for weeding out bad ones.
When I say average, I meant add up, you know, if we're playing out, which it's not, of course,
then you go around the loop, rotation from there to there, follow both directions, there to there,
there to there, and there to there, it's got to be one complete turn, right? And if all those
relative rotations don't add up to a complete 360 turn.
There's a co-cycle condition on those.
Yeah.
On those. But what do you mean by the average?
Average. Okay, well, sorry, yes. Well, you can think of it as being, well, going back to
this diagram, perhaps this diagram, think of this as being SO3, and you've got a bunch of rotations
represented by points on this. I'm trying to mimic, well, it is, in fact, the point that is closest
to all of them in some sense, well, in fact, whose sum of distances to all of them is minimized.
These would, these would all be, suppose we had a bunch of points around here, they're all supposed
to be estimates of that one, right? So, what is the best estimate of that one? It's in the,
you get, you know, in Rn, you would add up all the estimates and divide by n, right?
There should be a point that, within that set, that the geodesic distance between it and all
the others is minimized. Well, couldn't you find it? The sum of ones is minimized. No, you couldn't,
you couldn't assume they're all going to be equal, like, they'll be, you know, in a triangle,
you know, in a triangle, yes, but if you've got n points in Rn...
But if your geodesic distance is not, come on.
Sorry.
Their geodesic distance is not included in this one, right?
No, but even in Rn, it's...
You can have like five points in arbitrary configuration and make the center point be
equidistant all of us, right?
Yeah. Yes. So, you try and get the one that's minimally distant, you know, sum of distances.
Sum of squares would be in sort of mean. Some, some distance is not their squares, as I call median.
Okay?
So, yes?
So, the Weisfeld algorithm is basically iterative.
The Weisfeld algorithm is iterative. You start at the point, do an update down here and then
go back and throw that process out. And the details of exactly what you do in the tangent space,
there's a formula for it, the Weisfeld formula, which explicitly, it's an explicit update formula
for a gradient descent algorithm. And we can prove that it's convergent, you can prove it's
convergent in Rn, you can prove it's convergent on SO3, and in fact, you can prove it's convergent
on a remaining manifold with positive curvature, which I haven't really defined, but you know,
certainly the sphere one.
Doesn't this only have to hold locally, like, sorry?
So, doesn't this only have to hold locally? In some sense, because on the sphere, there's sort of this
anti-codes of the sphere, right? So, isn't this only true, like, if you were in the
upper-level sphere of the sphere?
Well, yes, you have to sort of, you have to assume that things are not too random,
expressed everywhere, okay? All right, okay, so I want to finish with that and talk about, about,
well, that was a little bit more about the kernels on manifolds, which I'd sort of
advertised how I was going to talk about. So, what is a kernel anyway? I mean,
so if you do machine learning, you're always talking about kernels, right? You're talking
about kernels for kernel SVM, kernel Fisher-Discriminals, kernel this, that, and the other,
PCA, things like that, and I'll show these a little bit. But what is a kernel exactly from
a mathematical point of view? Well, it's not exactly, but it's like a similarity measure,
right? It's defined for points in some set S, any set, and K, the kernels large when they're similar,
and small when they're dissimilar. So, think of it as being like an inner product. If you've got two
inner products, two vectors, if they're close together, their inner product is large. If they're
way apart, their inner product is zero, right? And small. So, it's a similarity measure.
It's analogous to the inner product. In fact, if a symmetric kernel is positive definite,
then it is essentially inner product, and I'll say what this means.
So, the point is that you like symmetric kernels. You like symmetric kernels because then it can
work like an inner product, and then the kernel trick allows you to do a whole bunch of algorithms
that use kernels, kernel SVM, and et cetera, et cetera. So, the point is, which are, you know,
things like SVM you use for discriminating two different classes and things like that, okay?
I'll give examples. But the point of the talk then is when can you find these kernels, which are
sort of distance measurements or, you know, between points or similarity measurements between
points on a manifold, okay? When can you do that? So, okay, I made it. I mentioned positive definite.
It's the definition of that is that if you form matrix KXIXJ as being, you know, matrix made up
of those elements, then this really means the, the, well, this is the formula. It really means that C,
K, C transfers, or C transfers KC is greater than or equal to zero. It's a positive definite matrix.
So, if you form KXIXJ, it's positive definite matrix. That's what the
thing is, and that's what the definition is. For all choices of points in the set S,
that has to be positive definite as a matrix indexed by i and j.
And the importance is that if a symmetric kernel is positive definite, then this is just like an
inner product in that there exists a mapping, phi, from the set X, I was calling it S before,
now I'm calling it X, mapped from X to a Hilbert space, such that K of X and Y is simply the inner
product in the Hilbert space, okay? That's what the thing is. And if you're familiar with the, the
hill, the kernel trick, you're looking for algorithms that only really rely on inner products. So,
you can do all these sort of things. So,
yes, you can. You can define a Hilbert space, you can define H, and you can define exactly what phi
is. It's a little immersive, immersive theory, basically. Yes, there is a way of doing it,
but normally you don't have to worry about that. And in fact, in the case they're looking at, you
seldom have to worry about constructing it, because all you're really interested about
is the inner product, and that's given by K, right? So, as long as you have K, you're okay.
So, one of the ones which one commonly uses is the radial basis function kernels, okay? So,
like this. Kernel X, Y is e to the minus X squared, X minus Y squared on sigma squared. I'm thinking
here of when you're in Euclidean space, okay? When you're in Rn. So, X minus Y is sort of like
distance. So, e to the minus distance squared on sigma squared is the radial basis function kernel,
and that is always positive definite for all sigma, right? Where this is, in fact,
is a norm in a Hilbert space, it's always positive definite, but in particular in Euclidean space.
So, that got me thinking, okay, why, right? Why is this positive definite? Why are we able to use
this to do algorithms with kernels? So, start looking around a bit. And so, the first thing you
think of is, well, maybe it's just because D is a distance, right? It's a metric. So, it satisfies
the triangle inequality. Maybe that's it, right? Turns out that's not the case. No. So, if you
have a metric space D, X, Y, defined on a set S, which is a metric space, which means that there is
a distance which satisfies the inner product, then the answer is, well, I won't say the answer yet.
Leo to the surprise. So, this being a distance is not enough. So, that means, for instance,
really, you know, the first thing is having a norm good enough. If you're on vector space with a norm,
will that work, right? And the answer is, once again, it won't. You actually need the product
of there being an inner product. So, the theorem says this, the radial basis function is positive
definite for all sigma. This is the radial basis of K, X, Y, defined by that, where this is a given
distance. This is a positive definite kernel for all sigma if, and only if, S can be isometrically
embedded in a Hilbert space. This is saying something rather different from the previous thing,
where here we're taking a norm in a Hilbert space, whereas the previous thing, right, we were taking
inner products in Hilbert space. It's not the same theorem. So, this is, so, it's not enough for
there to be a Banach space, technically. It has to be embedded in a Hilbert space, even though we're
only using the norm. If there is a Hilbert space and an embedding of S into the Hilbert space,
then you have a kernel. So, let's have a look at some examples here. If you take a sphere,
this first represent a sphere, maybe it's a circle, whatever it is, okay? So, there are ways of defining
the distance between two points, X and Y here. You can define the distance around the curve, right?
Or you can count the distance across, you know, the chordal distance. Well, the chordal distance
on the sphere is clearly a distance which is embedded in Hilbert space. The sphere itself
sits inside a Euclidean space, which is by definition a Hilbert space. It has an inner product,
it's a vector space from inner product. And the distance, if I define it as a chordal distance,
is exactly the distance in the Euclidean space. Whereas there's no way of embedding that so that
the distance, the Euclidean distance is equal to the distance around the curve. So, that means
that if I take the chordal distance, dxy equal to sigma theta on two, where theta is the angle,
that leads to a kernel in all cases. But the geodesic arc does not, does not give you a kernel,
okay? So, we want to, we're not so interested in spheres, but we are interested in positive
definite matrices for reasons that they come up in computer vision. And they come up in number
of ways in computer vision, but one of the most obvious ways, well, I don't know, maybe it's not
obvious, but one of the most common ways, perhaps in recognition, that they come up in computer
vision is through so-called covariance features, where you look at a window and you sort of take
little subwindows and you work out things like intensity across that window, and you work out
gradient across that window and various other things, and you work out how they correlate.
How does gradient correlate with intensity across this window? How does gradient correspond with
curvature of things? These were features which were invented by Fatih Porikli and Tuzel, I believe,
and they can be used, because they're covariance matrices, how these things correlate,
they are positive definite matrices, and they are used as features to describe
objects. I'll push ahead a few things here. This is the in-rear dataset, you're trying to detect
pedestrians. So what you do is you put a little, you train this, right? Put a little box around
pedestrians and you take little windows and take these covariance features. So, and then you look,
you train, you classify on these, and then you run a window over and see which windows you find
container, a pedestrian or not. And so the window is described by these covariance matrices,
by little covariance matrix. So the question is, I've got a bunch of windows which have these
descriptors, which are pedestrians and bunch which aren't. I want to distinguish between them with a
support vector machine, for instance, but the covariance matrices sit naturally on this
remaining manifold, so what we have to do is find a kernel on this remaining manifold of positive
definite matrices so we can apply a support vector machine, and then we can distinguish
between the different things. Now about positive definite matrices, people, there are lots of
ways that you can define distances between two positive definite matrices. So one of the useful
features that you might like is affine invariance, which means the distance between x and y is
A transpose x, like that. If x is positive definite, so is A transpose xA. And this,
if this were true, then it means that every point in the space of positive definite matrices,
sort of like any other point from the terms, in terms of neighborhood, and it's a homogeneous,
the thing can be, well, it gives a homogeneity condition on positive definite matrices, which
turns out to be good, and you can define an affine invariant-remainian metric using this,
so a distance function on positive definite matrices. There are other metrics, there's
logarithm metric, distance between x and y is log x minus log y, Frobenius norm. It's a useful one.
Now for the Stein metric, where given by a rather more complex formula, these are various
different metrics that one can define to look at positive definite matrices. This is a distance,
and the question is, under what circumstances does this allow you to define a radial basis
function kernel? That's the question. You want a distance function which allows you to define a
kernel so that you can then discriminate classes. And it turns out, if you look at various ones,
which we looked at, and there are a few others as well, there's just one, the log Euclidean,
which is both a geodesic distance, and what that means is it's really measuring distance along
some curve in the manifold. You're not sort of going out of the space. If you have a space that
curves around, it's no use measuring the distance across there, the shortcut, you have to measure
the distance around through the space of what you're really interested in. So the log Euclidean
is geodesic distance, and it is a positive definite kernel. And you get that it's positive
definite by applying this theorem, which we had here. It's a kernel because it can be embedded
in a Hilbert space. So this theorem allows you to sort of look at various distances and say,
quickly rule out, yes, no, this is not going to work to give you kernels because I can't find,
it's not going to be embeddable, or it's not obvious that it's embeddable in the Hilbert space,
whereas otherwise some it is obvious you can embed it and you can apply the theory. Sometimes
it's not quite so obvious, but you can nevertheless find ways in which you can define kernels.
So the log Euclidean one looks good. We applied it to various things. I'll skip over that,
but one of them is the Inria data set and Lowers-Wood here. This is false positives versus
mis-rate. It works, this is using a technique called multi-cernel learning, but still your
kernel, you're discriminating between covariance matrices, which are descriptors of windows in
the space. Multi-cernel because you're actually taking several sub-windows. You're taking good
sub-windows, which you can find what ones are good by training and adding the kernels, taking
linear combinations, positive linear combinations. So this is the best one we can get and this is
the kernel method based on positive definite matrices. There's a thing called the Euclidean
kernel, which means you don't take the logarithm. It's just actually the Frobenius distance,
given by the Frobenius distance between two matrices, doesn't work so well at all,
and others. It works well. So okay, the technique works well on support vector machines. The same
kernel you can use, the same sort of, yeah, the same kernel you can use on object classification,
where you're doing kernel k-means to, kernel k-means to group things which are the same
among this data set. You had a question? So multi-cernel learning, yes, you learned the
coefficient or you just... Yes, you learned the coefficient. So in multi-cernel learning and you
learned the Euclidean kernel, what's the difference between those kernels? So you basically have
different kernels there? Yeah, well, okay, we are using the kernel based on the log Euclidean
distance, right? We're using the kernel based on log Euclidean distance always, but the different
kernels that you're adding are kernels depending, determined by little sub-windows. So it's a
combination of little sub-windows. The same kernel, yes, yes, same kernel that plays a different
window and you're using them all to recognize that. So other things like, so this is kernel SVM,
this is texture recognition, another one here is diffusion tensor imaging, where you're actually,
so in diffusion tensor imaging, MRI imaging technique, I don't know if you're familiar with it,
I'm not too familiar with it myself, but instead of just having a color at each point,
you actually have a three-by-three matrix, a diffusion, you know, sort of describes diffusion
directions of a fluid in the brain, and each point, sort of think of as being a little ellipsoidal
cigar, and you're actually doing classification based on the three-by-three matrix there.
Now this is what we, there's no ground truth here, so I can't, haven't got a lot of basis for
saying that's the best result, but it looks sort of okay. Motion segmentation once again, okay.
So kernel dictionary learning on manifolds, I'm going to skip over this one fairly quickly,
but it's another example where we're talking now about a different type of,
a different type of manifold, not positive deformation, these are grassman manifolds.
So here's the idea behind dictionary learning, so some of this work was done earlier by
René Vidal, we're applying it to grassman manifolds here, but the point is in dictionary learning,
you have some object x that you want to describe in terms of some dictionary d, okay,
as a linear combination, close as possible to a linear combination of some set of dictionary
elements, but you want to do it in a sparse way, so you add this l1 sum of the coefficients,
okay, and you want to, you want to choose your dictionary,
which you, you know, set of features or whatever they are, so that you minimize this
over the choice of all possible dictionary and coefficients, so what dictionary best describes
my elements here, my x, now how do you do this if the thing lies on a manifold,
if everything lies on a manifold here, it doesn't make sense on a manifold, you can't add
points on a manifold, right, it doesn't make sense to multiply v and point on a manifold by v,
if you multiply a rotation by a constant, you get something which is not a rotation matrix,
right, you multiply a fundamental, well you multiply, it just doesn't, you don't have a
multiplication, so how do you make sense of that, so one of the ways you can make sense of it is
to kernelize the whole thing, where you find a kernel phi which maps everything, maps my manifold
into a Hilbert space and then I can define kernels on that, and the question is, well not so much,
the question, the idea is that now when you've mapped phi into a manifold, all the theory of
kernels works as well, sorry, when you've mapped into a Hilbert space, okay, I'll say again,
because I probably got it wrong, phi is now a mapping from my manifold into a Hilbert space,
and in a Hilbert space you can add, you can take linear combinations and the whole thing makes sense,
right, so we apply that to dictionary learning, the whole process, there's a,
there's an easy way of mapping positive, of grassman manifolds into a, into a Hilbert space,
which, sorry, grassman manifolds, I'm not going to say anything more about grassman manifolds,
because I want to hop on to the last thing, which to do with shape spaces, which I find
rather more interesting, maybe because it's the thing I'm latest looking at most, most recently,
okay, shape manifolds, maybe you should call them shape spaces, they're not always manifolds,
but they are in dimension two, what is, it's, what captures an invariant of a set of points
in Rn, so let's have a look at something like this, you have a set of points which you found
in a person's face, okay, maybe they're equally spaced points around the contour, or equally spaced
points around this contour, now you want to capture what there is about the shape of these points,
right, and so you can say these are a similar shape or these are not a similar shape, so what is
shape, first question, shape is clearly something which if you rotate, you don't change, right,
I don't change the shape of my head by rotating it, I don't change it by translation either,
and the point that in my interpretation here, I also don't change it by shrinking,
so this is, this is what you probably agree is what shape does, it's not changed by rotation,
it's not changed by translation, not changed by shrinking or scaling, this is an idea which
goes back to Kendall in a paper from 1984, and it turns out that if you define shape as an
equivalence class of points under transformations of those three types, you call that a shape,
the equivalence class, this forms a manifold, so which is rather surprising, I find, but just let's
work out how this works out, you represent each point as a complex number, supposing we've got
points in the plane, you represent points as a complex number, okay, that gives you a real
imaginary part of your two axes, so n points form a vector of complex numbers, okay,
a vector of complex numbers is the first approximation to, you know, what you might call
the shape or something, it's the pre-shape manifold equal to the set of all complex n vectors,
or m in this place, now if you center them so that you, you subtract, you got a bunch of these
things, sorry, you see, not a bunch of these things, you take the mean, the center, centroid of all
these complex numbers, their mean, and subtract it out, then you've got a centered vector here,
okay, so you do that, translation, you center it at the origin, so it's now, translation is taken
away, and you scale it so it has length one, so it's got rid of the scale, right, now what about the,
what about the rotation, that's the important one, about rotation, now the interesting observation
here is, important observation, if you multiply this whole thing, which is a vector of complex
numbers by a complex number, e to the i theta, it rotates each complex number, so it, it's a,
it preserves the shape, so what, what you can think of as a shape manifold is sets of centered
well, sets of points on the complex sphere, they're centered of length one, right, because I've
scaled length one, these are points on the complex sphere, and I take the action of rotation,
multiply, just multiplication by z, now this gives you what's known as the complex projective space,
to understand this, just think of, if you, if you're more familiar with real projective space,
real projective space, you can think of as points on the sphere, with minus one and plus one
identified, right, so all real numbers of length one, you know, minus one and plus one,
you do the same thing in complex, you take points on the complex sphere, and you identify them when
they are equal up to a complex scale, okay, so this is complex projective space, which is a manifold,
but it looks like it's just basically scale similarity, well there's the rotation similarity
as well, that's the point, scale is fine, but rotation, it's, it's, it's scale, it's scale in
terms of complex scale, but in terms of actual, if think of these as points on the plane, you're
actually rotating, right, which is, you know, it's a rotation rather than a shrinking of scale, right,
so it turns out that this has to do with a thing called the Hopf vibration, which I
studied a little bit in graduate school in topology, a very complex looking thing like this, these are the,
these are what happens if you take a point and multiply by a complex number of unit,
a unit complex number, they're, they're circles, so in other words, if I take, if I take this
and multiply by all complex numbers, it traces out a circle, right, so the complex space is made
up of a bunch of circles, and it's, it's this thing, Hopf vibration for what that's worth,
but I'm getting to towards the end now, so which is good, but the point of this research is this
work that I'm talking about by my student is that he can define a kernel on this two-dimensional
shape manifold, so which is shown here, think of, think of, as I said, a shape as being a unit vector,
well, say, forget about the fact that we're identifying the z and the z, think of it as being,
you know, you forget that because that's the way you think of these things in terms of real
projective space, so just for intuition, a shape is a vector, well, so it's an equivalence class
of vectors on the unit sphere, and if we take two of them, I want to work out what is the
distance between them, right, as shapes, well, in terms of this thing called the, the full
procrastis distance, the full procrastis distance between two shapes now, and that is the, I can't
reach that far, the perpendicular from one to the other one, now it's a good, let me convince you
that that is a good thing to think of as being the distance between these two shapes, the distance
between the arrowheads is maybe fine as well, but maybe not quite so good as this one because
this is the smallest distance between, say, the left-hand arrowhead and any scaled version
of the right, right, so between one shape and any scaled version of the other one, that is the
minimum distance, right, and it turns out, by a bit of calculation, this in fact gives you a
thing which satisfies that theorem about, about kernels, it turns out that this allows you, this
gives you the radial basis function kernel based on that distance measure on shape space, gives you
a positive definite kernel and hence you can do all the sorts of things that you like to do with
kernels. The top inner product between x and y, x and y are elements or end vectors in complex space.
Yes, unit vectors. They're unit, yes, but how do you define the inner product between two
complex end vectors? Okay, it's, if they're A i, you know, it's, you know, A i times B i is more
than a product, in this particular case it's A complex, A i complex conjugate times B i sum
of a i, difficult complex conjugate, yes. So the point is that the complex, the inner product
in complex numbers, in complex vector spaces or complex Hilbert spaces, can give you a complex
number, you know, it can give you a complex number. So you cannot forget, you cannot just say the
cos theta is equal to the inner product because what's, what doesn't make sense, that the cosine is
imaginary, so you, but it turns out you can define cos theta to be equal to that, the absolute value.
Complex number multiplication and then you just take the magnitude or whatever you want to call
that operation. Yes, yes, and that gives you cos theta and sine theta, which is that because the
fault across this distance, you use the positive definite kernel, right? Various other ones do
not. Geodies, it does, distance does not in this one, which is actually the distance between the
arrowheads does not, right? Which is curious because it does if you're in a real space,
but not in the complex space, okay? So, okay, that will, that gives you a kernel that you can now
look at and try and do things like classifying shapes, yes?
Well, you don't allow that, you say, okay, this is, okay, this is one of, there are a few, a few
weaknesses behind this in a practical sense, which I can just point out in a little bit,
but yes. But it allows, you know, and two-dimensional shapes are of somewhat limited
interest, really, to, in computer vision, but one hopes to be able to, to go to three-dimensional
shapes. And I've been talking, in fact, with Katsu Ikeuchi, who's here, yes, I said, can we
try to use it to classify different faces, which there are on the temples of Angkor Wat or whatever
he's, so, which he says they're all, they're, they're classes, so can we use this? So that's,
that's the, you know, work that I'm interested in doing, but for the present, you know, we can do
it for two-dimensional things. Now, you know, leaf database and things like these. So what are the
weaknesses? The weaknesses are, for one thing, yes, you need the same number of points, right?
And furthermore, you need them to be the correspondence between them to be made and to make
sense, right? So one can think of, you know, one can think of situations where that's possible,
like, you know, you take your nose, your two eyes, your corners of your mouth and your chin and
your ears or something like that in an image, right? But, or obviously in, in, I don't know,
well, maybe not obvious, you can, you can maybe find the stem of the leaf and then take a fixed
number of points around the stem, around the contour of the leaf equally spaced, right? So that
will give you the same number of points and the correspondence that you can make to use to, to
classify those. There are obviously other situations where this is going to be a problem, right?
One also may, there's, there's some hope of, of generalizing this work to things where you don't
have a finite number of points, but you have a curve and you're trying to work out shapes of
curves. And I think I've, I haven't followed this up yet completely, but people do talk of shape
manifolds in, in that sort of sense too, where you, it's not a finite number of points, but,
at the present time, I'm not sure of the details. To get this into 3D, there are problems too,
which means that we haven't just immediately gone and done it, in that in 2D, there's 2D space,
I said it's a complex projective space, it is in fact a manifold. In three dimensions,
this is not a manifold. It's normally called shape space, not a shape manifold. It's sort of
like a manifold with casps and things, so I don't know quite how to handle that at this point.
But, you know, one, one does research and hopes to find answers to some of these problems. So,
okay, that's, that's it with the various people who I work with. This is a different list of people,
not exactly the same list as before, but the union of the two lists is,
the union of collaborators on this work. Okay, that's it.
I'm trying to understand when you say, this is, when you say something like this is not a shape
space, it's a shape manifold. Yes. What exactly do you mean? Because, I mean, there's always
some space in which the manifold is embedded, and you can always think of that as your shape space.
It's like taking a set and relaxing it into a larger set, and it seems that a lot of
computer vision people take their features and turn them into vectors, do all the work with vectors,
and then maybe at the end they project back to the manifold.
You mean, I don't know really what the last part, but what's wrong in working that vector space?
Well, there are a number of things wrong with working in the vector space, I guess. It depends
on the particular application you're looking for. If you're talking about kernels in particular,
I'll sort of sidestep that for the moment, but if you're talking about averaging, for instance,
finding, on clustering, if you're doing things like clustering on a manifold, there's no point
finding the center of a cluster in the space, in the embedded space, because it does not represent
an element that you're looking at. And if you think of very specific manifolds like
rotation space, the sum of rotations is not a rotation, right? The Grassman manifold, it's not.
The way Grassman manifolds are skipped over it is you're applying Grassman manifolds to
represent sets of images. Well, if you do clustering with Grassman manifolds and try
and work out where the center of the most average sort of thing is, it's not going to be a member
of the Grassman manifolds, it's not going to correspond to a plane, it just won't be right.
Distances also, as I said, if the manifold in a vertical comes around and like this,
you really want to find distances around the hole through the space of possible configurations,
not across the gap, which is really just something to do with the particular way that's embedded,
right? In our end, for a different embedding, it might be completely different, but the intrinsic
thing is in the manifold itself, and so that's why you want to deal with the manifold. But the
experiments also support this. You best be doing these things with intrinsic metrics,
intrinsic distances on the manifold, okay? Any other questions? Actually, I just have one. So
when you talk about comparing ships, does it allow you to compare against different categories
of ships? And you showed a leaf, so you're saying, okay, can I distinguish between, you know, maple
against others? And even the maple leaves are not exactly the same.
Yeah, well, I mean, it depends on the granularity of your clustering, I guess, right? So we are
talking about there, and he's done the experiments where, in fact, you are, you know, taking maple
leaves and comparing the beach leaves or something like that. But if you bothered to train your
classifier on, you know, maple leaves when they're young and in spring or something like that and
what they look like, I'm sure you'd find that you'd be able to, you know, find differences in
shape as well. And as Katsu was saying to me yesterday, they weren't using this technique,
but they did, for instance, find, which was very interesting, in these faces, in Angkor Wat,
that there were differences between clusters of these faces. They all looked like people's faces,
but the clustering was perhaps based on having different sculptors who did these things. Well,
some of them first represent humans and some of them first represent gods, and so you can actually
distinguish between them. So sure, you can, you can do as fine a classification as you like.
Really, you know, well, obviously, within this.
Any other questions? Well, if not, let's thank Richard once more.
