Thank you very much, Laurent, for the introduction.
So as Laurent said, this is the title of my presentation, Beyond the Statistical Prospective
on Deep Learning, which is in fact the usual one nowadays, the toposic point of view, invariance
and semantic information.
So I just wanted to notice that there is an archive paper that is a common work with Daniel
Benka, which has been available since last night in archive.
So it is effectively a joint work with Daniel Benka, and I would like also to thank a lot
Laurent LeFoq first, who opened to me this fantastic world of toposies in 2017, and also
Olivier Carmelo, who has helped us a lot to understand some notions that I will introduce
afterwards.
So just a brief introduction on AI and machine learning for our mathematician colleagues.
I need to say that I'm not at the origin of mathematician, but I'm coming from the electrical
engineering world.
So it's in general quite far away of those things on toposies, and this is through, in
fact, it is through AI, so artificial intelligence and machine learning, that I have been confronted
to toposies and other mathematical notions, other related mathematical notions.
So what is machine learning?
In fact, very briefly, it can be, let's say, it can be divided, subdivided into three
basic tasks.
The first one is called the supervised learning, which in general is used to perform classification
or regression, and that means, for example, that you want to recognize, for example, in
an image, if there is a cat or not, so just use what are called labeled input data, that
means it's images where someone has labeled them with cats or non-cats, and so this is
used for the machine to be trained, then so you train the model by using these labeled
input data, and finally, you test it with new images, and you hope that you'll be able
to recognize even in new images that haven't been part of training data that you can recognize
a cat or not.
Regression is, let's say, related to more continuous problems.
Then there is also unsupervised learning, so you don't have any labeled data.
You have basic raw data, and the idea is to perform grouping, it can be grouping, it
can be dimension reduction or discrimination, so you don't know that what you are considering
is a cat, because in fact, this word is completely unknown to you, and what you want to do is
to discriminate, for example, cats and non-cats, based on some, let's say, for example, cats
will be in some manifolds and non-cats will be outside this manifold, and you want to
understand patterns and to discover outputs.
And then there is another one, which is maybe very closer to the way animals or humans are
behaving, which is reinforcement learning, where you have an agent that interacts with
its environment by performing actions and learning from errors and rewards.
So it's a trial and error method.
So in some sense, reinforcement learning can be considered as a kind of supervised learning
since when you have rewards or errors, in fact, there is something that says that it
is an error or that gives you some reward.
And so that's why it's quite close to supervised learning.
In this presentation, we will essentially consider the supervised learning case, because
maybe it's the simplest one to understand in terms of toposes, but also the other ones
can be understood this way.
Okay, so in order to perform these tasks of machine learning, in fact, the most popular
way of doing that and most successful is using neural networks.
So what is a neural network?
So this is an example of what is called fully connected deep neural network, where you have
in fact here.
So this is the simplest case, where you have input data here at the input.
So this neural network, and you have layers basically.
So input layer, first hidden layer, second hidden layer, et cetera.
And at the output, you have what is called output layer.
So I'm sorry, it's a little bit small, but the output of a neuron yj will be, in fact,
a kind of, you perform, you compute a linear combination of the inputs Xi by using what
are called weights, Wij, and these weights will be, in fact, learned using training data
plus what is called a bias.
So in fact, instead of being linear, it's a fine, a fine transformation.
And then you apply to this number, you apply nonlinear function phi, which is called an
activation function, and that could be a sigmoid.
So this one hyperbolic tangent rectified linear units, so which is the third possibility
and some other more exotic activation functions.
So in fact, depending on the problem, you choose the one that is best suited.
So in order to train your network, that means in order to compute the weights and the devices,
in fact, you need to, in fact, what is the most popular algorithm is called back propagation.
It dates from the 90s, and in fact, what you're doing is that you compute a loss function,
which can be based either on the Kulbach library of cross entropy or mutual information,
or maybe also some other loss functions are also possible.
Even some of them are just based on Euclidean distance.
And in fact, this loss function has variables, which will be the weights and the devices of
the neural network. And then you find them, the idea is to find the minimum of such function
by using the label data, so the label, the training data.
And the idea is to use gradient descent, and thanks to the chain rule in order to compute
the partial derivatives, then the gradient calculus becomes very efficient and can be
done layer by layer. And so this is on this figure. So you can see, let's say, a schematic view of
back propagation. So I don't want to spend too much time on that.
So you have seen a neural network based on fully connected layers, but you can have other
architectures when you try to deal with problems which are very specific.
For example, what I call the convolutional neural networks, I will come back to this
architecture later on, because it's something which is really related to a word on my title,
invariance. And the idea, in fact, is instead of considering a fully family of linear transformations
on when you are considering edges from one layer to another one, then the idea is to
restrain this to some more specifically linear transformation. And in this case, it will be
convolutions. And so you will have some layers which are convolutions followed by max pooling,
which is just a kind of, let's say, restriction. And then at the end, you will have fully connected
layers. I will explain later why we use this architecture. This one, the convolutional neural
networks, are basically used, in fact, for computer vision tasks, so for image processing, essentially.
Then you have recurrent neural networks, you know, where, in fact, here, what you have are
vectors, and the idea is to use a kind of, let's say, a kind of loop. If you unfold this loop,
then you obtain this kind of architecture. So, in fact, this architecture can be used
when you have time series, or when you have to consider, for example,
natural language processing, when you have some sentences, which can be considered as
a kind of time series, finally. But basic recurrent neural networks are not good.
They cannot be trained efficiently, because when you consider gradient descent, then the gradient
rapidly vanishes, and so that means that the loss function will not become very low, and
that means that you will have too many errors. So, the idea is to consider some other kinds of
cells in this recurrent neural network settings, which are called long short-term memory cells,
LSTM cells, and so I will not spend too much time here as well. So, as you can see, the idea
is to have not only short-term memories, but also long-term memories in order to make
the neural network able to be trained more efficiently.
Okay, so after this very brief introduction on machine learning and neural networks,
let's go into a toposive view of deep neural network. So, very briefly, what we have
is that, in fact, it's possible first to model a neural network, let's say, by using,
let's say, those grotendic toposites. It can be done in several steps. The first one is based on
the architecture of the DNN, of the deep neural network, and it will constitute the base grotendic
site. So, in fact, our way of considering it is by considering, for example, in the case of a chain,
for example, what we have seen was this fully connected deep neural network, and in fact,
we have shown that the best way of modeling it using toposes was by considering that each layer
is an object of some site. So, here, in fact, the feed-forward functioning of the network,
when the network has been trained, will, in fact, correspond to a covariant function x,
so from the category which is generated by this graph to the category sets. So,
I mean, you can see that it smells the grotendic topos, of course. In fact, this xk plus 1kw,
which is, in fact, a mapping from xk to xk plus 1, so it will be an edge here,
will correspond to the learned weights that goes from layer k plus 1 to layer k,
and it will correspond to each row in this category c op of gamma, so gamma will be, of course,
this graph of the neural network. And then the weights will be encoded in a covariant function,
so this blackboard w, from the category c op of gamma to the category of sets. So,
the idea is that at each layer, lk, so we define wk as the product of all the sets,
in fact, w, so this is essentially a matrix that goes from the layer l plus 1 to the layer l
of weights, and to the edge that goes from layer lk to layer lk plus 1, we will associate the
natural forgetting projection that goes from wk to wk plus 1. So, then the Cartesian product xk
times wk, together with this map, will also define the covariance factor, blackboard x,
and the natural projection from x to w will be, in fact, a natural transformation of phantoms.
And what is interesting is that if you consider supervised learning, which is an essential case
we will consider here, then the back propagation algorithm can be represented by a flow of natural
transformations of the functor w to itself. And in this case, in the category c of gamma,
xw, w, and x become contravenant functors from this category of sets, that means that there are
pre-shifts of a c, and that means that there will be the objects in the pre-shifts to post
c-hat. Okay, so this is the case of a chain, which is quite simple, because in fact,
in all these settings, we will have objects and natural transformations in the topos of
pre-shifts based on this simple site. Now, if you have something a little bit different from a chain,
that means if we consider the general case, then the situation becomes a little bit more tricky.
And now the functioning and the weights cannot be defined by functors on c of gamma.
So in fact, what we have done is a canonical modification of this category. And now, for example,
if you have this kind of problem to be solved, that means you have in this graph many different,
let's say, modules that converge to this object a, a small a,
then we have to perform a surgery, because considering this as a site will not work at all.
And the idea is to introduce new objects here, you can see, between all these a primate second,
et cetera, and the object a, here by introducing capital A star and capital A, right? And
with arrows that go from a star to a and from small a to capital A. And that form a fork
with tips in a primate second, et cetera, and the handle will be formed by a capital A star,
capital A and small a. And what is it? And if we reverse the arrows, then we'll have a new
oriented graph without oriented cycles. And the new category c will replace that which,
sorry, the category which will replace c of gamma will be the category now c of bold gamma,
which will be opposite to the category freely generated by this bold gamma.
And now the main structural part, so that means the projection from a product,
so the product of a primate second, et cetera, to its component, can be now interpreted by the
fact that the pre, this pre-sheaf becomes a sheaf for natural growth and dik topology, J.
And in fact, on every object x of this new category, the only covering
will be the full slice category, c on x, except a fix is of the type, sorry, a star,
where in this case, we have the covering made by the rows of the type a prime towards a star or
a second towards a star, et cetera. Okay. Yeah. So in this case, we have, in fact, basically
all possible scenarios that happened, all possible structural scenario that happened in
neural networks. So even if we consider modular neural networks, where you connect many neural
networks to some other ones, et cetera. Okay. So this is a structure, but the structure is not
enough. Now we have to consider a second stage, which is now the, what we call a pre-semitics.
And in this case, we'll see that considering just a gotten diketopos will not be enough to
characterize all possible neural networks that can be used now, and maybe to consider to
characterize also some new ones that may emerge in the future. Okay. Let's start with a simple
example, which is the example of convolutional neural networks. So this is the one that I showed
you in a preceding slide. So here, you know, the images that, because this is a convolutional
neural network is used for image processing. And so images, of course, I assume to be by nature
invariant by planar translation. For example, if you have an object in an image, and if you
shift it, of course, this object will still be the same object. And so the idea is to use this
invariance in order to learn much more efficiently. That means that you will have
if you are able to consider this invariance, this translation invariance, then you will be able to
consider much, much less weight to be learned. And that means also that you will need much less
training data, you know, in order to make the neural network learn. So in this case, in fact,
this is imposed this to a large number of layers to accept now a non-trivial action
of the group G of 2D translations, and also to a large number of connections between two layers
to be compatible with the actions of this group. So that means that even the underlying linear part
when it exists will be made by convolutions with a numerical function on the plane. So
this is the way, in fact, this action of the group G of 2D translations will be considered.
Of course, it doesn't forbid that in several layers, for example, these last ones,
the action of G is trivial in order to get invariant characteristics under translations.
So in this case, of course, the layers can be fully connected.
Some other groups have been considered also in the literature,
together with their convolutions. So, and now DNS that analyses images, they have always
they are constructed in the same way, which is several channels of convolutional maps,
and with max pooling in order to make this as an object. And all these are joined then with this
fully connected DNN in order to take the decision. In fact, this looks as, you know,
a structure in order to localize the translational invariance. And this is, in fact, what happens
in the visual areas in the animal brains. So it's really a copy of the nature.
So what is interesting also is that experiments show here that in the first layers,
we can see kinds of wavelet kernels that are formed spontaneously in order to translate contrast
and color. And the opposition kernels are formed to construct also the color invariance.
So it's these convolutional neural networks are very, very
interesting tool for major processing.
Okay, so let's go back to our toposic interpretation now. So as we have seen,
we need to take into account these grouping variants. So toposic manner to encode this
situation, in fact, consists in considering the contraverent factors from the category C of the
network. So the one we have seen that takes into account the structure of the network
with values in the topos of these sets. Okay, so because it's, in fact, of course,
it is exactly, in fact, the actions of this group G on sets are, in fact, the objects on
in the topos of G sense. So the collection of these factors with their morphisms,
they will form a category which was shown to be itself a topos by Geo in 1972. So we thank
Olivia to have informed us of this fantastic work from Geo. And it is equivalent, in fact,
to introduce a category F, which is fibered in groups isomorphic to G over the category C.
Okay. And it's, it's satisfied the axioms of a stack. So F in this case has a canonical topology
J, which is a courses one such that by the morphism from F to C is continuous. And in fact,
the ordinary topos E of sheaves of sets over this site, FJ, is named the classifying topos
of the stack and is naturally equivalent to the two CJ tilde that we have seen here.
But the Geo theorem is much more general than that. It doesn't concern only groups,
but it extends to any stack of a C. And it says that the category of covariant functions from C
to the topos of the fibers is equivalent to the classifying topos of the stack. In this case,
nothing is seriously changed compared to group if the group is replaced by a group read.
And if we consider category F, which is fibered in group weights over the category C,
or it's associated stack for our own purpose. In fact, we have also considered postsets and
postsets fiber group weights instead of group weights is something that I think that Daniel
will introduce them, but it will not be part of my talk. So with groups, we open the
the neural, the convolution neural networks with group read. In fact, what's interesting is that
we open, in fact, the interpretation of the long short term memory cell RNNs. So for example,
or what are called, for example, the attention networks, which are very powerful networks.
So it's a generalization, which is very, very interesting for us.
Then we have the language. So in fact, we have to consider now a vibration, another
vibration over F, which is denoted in this case A, and which choose an adapted language
and the semantics over every object of the architecture, augmented by a context in
its internal category F. So in this case, the objects U of the architectural category C,
together with psi of the fiber F on U, will represent the pre-semitic
context in the layers represented by U. And each one of them possesses a reservoir of logic
in the classifying sets of parts, omega of uxide. And in fact, the transmission of the
potential logics between layers and contexts for morphism alpha H will goes into the two directions.
So Daniel will explain with more details these logical factors. So pi star and so covariant
factor pi sub star and the contravariant one by sorry, exponent star, which should come
respectively from the red agent F star alpha and the left agent F alpha, so which extend
at the unit extensions of the pullback defined by the function F alpha. So you will have a
more detailed explanation in the next talk. So and they will give rules of transformation of the
formulas or axioms that will be available at one layer in two formulas of axiom to another layer,
so to another connected layer, and it can be a backward or forward depending on the pi star
that we're considering. So pi star would be a pi exponent star would be a kind of projection
when pi sub star will be a section of pi exponent star. So it's in fact one is
let's say will go to the output theories when the other one in fact will enrich
by considering some other possibilities. So it's something that will be explained by Daniel later.
So now just before considering this concept of information briefly some I would like to
show you the results of some basic experiments and I would like to thanks a lot Xavier Giraud
for that he performed all those experiments. So the first one we're done by using small networks
and in fact we want to do we have been inspired by a result from two neuroscientists.
So Moschowakis and Neuromyotis, so two Greek neuroscientists which have in fact analyzed
what was happening after the what are called the motor equivalent cells, so MEC neurons,
and so and they found that the neurons that were coming afterwards were in fact performing
let's say Boolean propositional calculus and we wanted to see exactly what was happening
if we replaced those neurons by using artificial neural networks. So we modeled the
output of the MEC cells by in fact using some activation signal that were distributed using
von Mises probability distribution function and the idea is that we have an activator A that can
take three values capital E which is the I that means it corresponds to activation of the I of
the monkey because those experiments were done on monkeys. It can be so capital H was the hand and
EH was in fact both I and N. So in fact we used very small neural networks. The first experiment
were with three layers so an input layer L0 and output layer L2 and just one layer L1 and
those numbers are just the number of neurons per layer and then we tried also four layers
and five layers in order to see what was happening. The activation function was the hyperbolic tangent
okay so just very quickly here is what is happening so those circles in fact on those
circles you can see the activation of one neuron so in some hidden layer so in fact this is cell
one cell two cell three and cell four you can see that for example this is the way they are encoded
the blue curve represents in fact the response of the neuron when it's the I okay the red one
the response of the neuron when it is the hand and the green one when it is both I and N okay.
So well and then when the curve is dashed it means that in fact the sign of the output of the
hyperbolic tangent is minus one and if it is not dashed it is plus one okay. So as you can see here
when the curve is red in fact the response the sign of the response depends on the angle
but when it is blue or green it doesn't depend on the angle so we cannot in fact deduce any logical
behavior when it is red but we can deduce here a logical behavior when it is blue or green if it
is blue so it's I we can say that I implies minus one it is the sign minus one and when it is
I and H it implies also minus one if we contrapose those two implications then we obtain that one
implies the hand so that means that if we have so so if the output of the of this cell is positive
then it means that it was the hand that was which was the activation at the input right.
We can see the same for example for cell 4 here so we have only one curve that doesn't change sign
it is the blue one so it means that I implies in this case plus one and if we contrapose this
implication it means that minus one implies hand or EH so you can see that here this neuron
performs already a proposition Boolean propositional calculus all right with three hidden layers then
the network generates complete triplets of cell that means that in this case a triplet will be
sufficient to conclude in any case because the triplet here always in fact has this kind of behavior
so one implies I one implies H or one implies EH minus one implies etc okay so from these
this behavior of the three cells of the triplet in fact we can conclude we can conclude in any case
because okay when you have a configuration that implies the force it means that in fact
this configuration never happens basically and for the other configuration we can always conclude
without any ambiguity all right what is interesting here in this experiment is that we have used
two different encodings for EH or EH at the output the first one where we encoded in fact
these three activation activities sorry by using just an interval right and this was
E was at one end of the interval EH at the other end and H in the middle okay and the problem is that
in this case it was very hard to make logical cells appear but then by using this
encoding then of course this encoding respect in some sense the group of symmetries of the
problem because you basically you can exchange EH or EH right and in this case in fact of course by
using it so then the logical cells were appearing much much more easily so it is something that
in fact shows that the fact that the action of this group is very important in this case
is the symmetric group okay then also we have done some experiments when instead of considering
three classes we are considering more classes so in this and we were considering what we
called the logical information ratio which was the number of decidable logical propositions at each
layer divided by the number of logical propositions that can be
generated by in the theory and in this case you can see that when you go from the input layer
to the output layer then this logical information ratio increases and at the end basically you can
you can decide everything in the theory okay we have done also some experiment
based on predicate calculus and so in this case we have considered very quickly
three bars two or three bars it could be a red bar and red bar or also a blue bar
in our experiments and we were considering an interval so a line and or it could be also
a circle so we have tried also to consider a line a module or something and in fact
for example for two bars the questions that were asked were so the first one the other disjoint
or ii is one bar included in the other one or yo can they intersect but the shortest is not
included in the longest so you can see that compared to the input layer where you just
some senses can region of this interval or of the circle in fact the propositions that
that i involved in fact predicates and not just coming from proposition calculus
with three bars the same questions but of course with more possibilities in fact for example so
these are the first results if we train with bars of respective lengths five units and three units
for r and g and if we test with the same lengths then in fact let's use this figure maybe rather
so here in fact a bar can be just encoded by using the center of the bar and the
the the lengths of the bars in this case what we can see is that in fact the testing looks
almost perfect i mean it's because basically we are able to i mean it's because the testing
is done with the same lengths in fact we don't ask to the neural network to generalize in any sense
but if we ask it to generalize then for example by using still for training the bars of lengths
five and three and for testing for example with links four and six and also we can exchange
the bars now for example the longest one in training becomes the longest the shortest one
in testing then as you can see the results are not so bad but it's quite blurry here here here
here and here so it's i mean of course it's not bad but of course it's a little bit
worse than in the preceding case what is interesting also is that if we are considering
for example less than three layers then the neural network is not using logics to perform
the task but it's using Fourier analysis but from three layers and beyond then it's a logical
analysis that is performed all right what is interesting is that disconnection and inclusion
only are the most frequent outputs and in order to decide the inclusion only in fact most neurons
instead of of training to decide it directly they eliminate the two other possibilities
d or ii probably because io is more difficult from the point of view of predicate
calculus than the two other possibilities but it's just a hypothesis okay if we have enriched
training in this case then in fact we have a remarkable logical behavior yeah where we just
by using in fact the outputs of two neurons we can basically
answer to the questions that are asked of course it's i mean it needs
quite high generalization power from the neural network but it's not bad at all
there is also a very nice relation between the weights on the i mean the the lattice weights
and the logic in fact what is interesting is that at the last layers in fact the weights
it is as if weights were performing the proof sorry because here we can see that
um for example sorry maybe it's yes if you're considering uh uh for example the histogram
of deductive power of the weights applied to quantized activities if you are considering
all possible triplets of neurons at this layer always uh in this case it's six objective functions
so it's three colors then you can see that the weights are i mean i mean i distributed
basically in almost everywhere let's say but if you just select the uh triplets
that are i mean the interesting triplets then the weights in fact become much more i mean
have distributions that is much more narrow and so it it's it's really because in this case
they are in fact they basically in fact they basically perform the the proof from the
for example the the last hidden layer to the output layer
also we had uh oh sorry maybe i will have to skip it because i have not much time
okay so now let's go to the to the part four which is in fact something related to
the notion of semantic information if we want to define what is semantic formation
then we need to understand how semantics appears for example if you use a neural network
okay first of all the semantic category that we will consider
is i mean is a quite general category so first of all the artificial intelligence is connected
to the real world the one that we are perceiving and so in this case languages have to be let's say
as general as possible uh they cannot be just the languages that uh i used the currently for
example in mathematics they have to be richer than that and as it has been suggested in uh
Lambeket Scott in fact a good caricature of semantics will be of course the interpretation
of the language of in the complete category of course topos is a good example but here we aim
at being more general so and what we propose is by closed monoidal category and uh in fact
it is a category such that for any triple of objects x y and a they will exist two
let's say exponents exponential objects one on the left and one right
and natural bijections such that those equivalents will be satisfied
so what does it mean in terms of of language it means that if for example x y etc are the
meanings of of something so the the the the this tensor product would be the composition
and the exponents on the the exponentials on the left and on the right will be respectively
the conditioning of a by respectively the presupposition of x or the post supposition of y
and the rows in this category would be associations of meaning evocations etc of course if those two
exponential if they can commute then we get the classical case of of topos is so in this case
theories will be collection of objects and rows such that if let's say a is belongs to
the theory t and if this row from a to b is in a then b has to be in a and two actions of this
monoid a so the one on the right and one on the left given by the exponentials will be named
conditioning and these condition x will be essential to define the notion of semantic information
that will be defined in in more details by daniel in the next talk so let's let's say let's see now
what what we call data sets in fact we will see that it's not in machine learning they are not
really data sets that are much more than that so in order to see that let's consider the case of
supervised learning so we have input data so xin which are let's say elements of a big set of
possible data so they are basically all possible data that can be seen at some point by the neural
network right and then you have at the output basically some theories t out belonging to
a set of theory capital theta all right so in the classical
settings of machine learning a neural network is seen as a parameterized set of functions
so fw so it's parameterized by the the weights w from x and which associate to any data xin
a theory t out for example you have data and you have to say if
it is a cat is true or false so for for this a very simple example so they have been in a
universal universal approximation theorem by Sybenko in 1989 that says that continuous maps from
a compact subset k of a numerical set space sorry rd so basically in this case it will be the input
data to another numerical space can be approached uniformly on any compact subsets by a standard
neuronal map is fixed nonlinearity of sigmoidal types so the sigmoids are
it's an example of activation function okay so basically this shows that the neural network
works well for interpolation right but the problem is that it has to be a compact subset
so now what happens outside the compact subset because the problem is that in theory even with
low probability you can in fact some new completely new thing input can happen and in this case you
have absolutely no guarantee that you will find the right theory corresponding to these data
okay so what about extrapolation in fact it is related to what is called generalization
in machine learning which is the possibility for neural network to be able to extrapolate
let's say but in order to do that just a theorem of analysis is not enough at all
we need something else we need to capture the essence so the structures of the data with respect
to the goal which is in fact related to the task or the question which is asked to the network
and in this case what we want is that a small set of data of data size zero
which will be the set of data used for training the network can be considered as
representative for the learning problem that means that just training of this dataset is sufficient
to know basically what will happen over the whole possible datasets exact right
and in this case the approach of deep learning is in fact what I presented to you in the
second part which is to construct an architecture that would be expressed using the growth and
site of the architecture of the neural network a stack which will be
considered by using vibrations in groups group weights or modular categories and the language
story which will be a vibration of layers of neurons which will be able to extract
the structure from a minimal sampling size zero and in fact we will have the relations between
data and theories through now properties so that we need invariance by the action of a group
or something more general group weights or modular categories okay so this is in fact
uh we just introduced the action of in fact something much more general than the action
of a group on the set or of group weight on the set which is in fact the action of a category g
on another category v so this category g will act on this other category v when in fact a
contravariant form from g to v is given and in this case because for example if a group acts on
the set we need to consider elements so we need to define elements in the category v so which
will be just a phi which will be morphism from u to v which will be considered as an element of the
object v so now the definition of the action uh suppose that we that g so the first category acts
so this functor f from g to v and we have v equals f of a so then in fact the orbit of phi
under the slice category g over a will be the functor from this left slice category to the
right slice category and it will associate to any morphism this element of f of a prime in v
and to any arrow uh from a second to a prime over a this corresponding morphism from u
to f of prime to u to f of a second and so in this case the theory of topos
stats and languages will extend the notion of actions of categories and the morphism
to the action of fiber the category f to a fiber category v and in fact from group
equivalents which in fact is represented in the structural properties of cnn for example
will go to category equivalents okay and so in particular so what we have is that now
given a shift of category from c to cat for example stack on group ways or in some other
categories that we consider as a structure of invariance and another shift that we consider
as a structure of information flow for example possible theories or information spaces that
Daniel will define afterwards given an object q of c an action f on m will be a family of
contraverent functors such that we have this nice commutation relation this is a vast
generalization of group equivalents and it will allow us to consider much much more general
structure structure sorry on neural networks in order to take into account so many many aspects
structural aspects and the fact that we we will be able to generous much more much better than
what is currently done so okay sorry it's a little bit late but
okay we have done an hypothesis of invariance enlargement that means that in this between
the input side and in fact and the output there exists a kind of layer which we call
a maximal invariance layer that contains basically the the full possibility of of
invariance of the problem all right so okay and in this case uh okay i'm sorry because
it's a little bit late i have to be very very very quick so the correspondence from the input
side to the output of theories the theta will be said to be justified if there is a language
which is external or coming from some supervisor wider than the language of the output so the
language of the question but course and broader than the language of the input basically which is
just a very simple theories where you have just many many objects for example the pixels of the
image by but basically no morphisms and which are the languages respectively adapted to the
questions of the output and the encoding of the input and this correspondence will factorized by
the language lx through a collection of expressions of this type the following aspects a b c of xi
in the language of input expressed by sentences in the language of x of x in the language lx sorry
will characterize the proposition in the language of okay okay yeah and so in this case
we consider something in order to be able then to propose a kind of theorem of semantic coding
which is this is exact category so where you have two semantic
sorry where you have sorry fiber categories at the input at the output
two semantic sheaves of language respectively at the input and output given by the fiber category
and now we will assume that on f this central category which in fact will be the
will be the place where you will have the maximum invariance
we will have this language a and that can provide the justification from for the mapping sigma
from the input to the output right and then to prove that every justified problem can be
realized by triple c f and a we must realize this f and a by a stack and languages
over a site c that is given by a neural network architecture okay and the invariance under f
will be isomorphic to the maximum enlargement of the stack so this is for now
just an hypothesis i mean a kind of conjecture that we hope to be true
okay okay this is just the first notion of semantic information that has been introduced by
carnapp and barilel in 1952 in the in this case what they had was in fact elementary propositions
in this case in this example for example they had three subjects abc and then these subjects were
individuals were persons that could be either that have two different attributes m for male
f for female y for young and o for older all right and then elementary propositions were
just something in which you say that a is a young and male or and b is young and female
and c is old and male for example right and then the combinatorics in this case of course
is the ore of all these of all possible parts of those propositions elementary propositions
all right but what is interesting is that in this case what could be information in fact
by considering some shapes in
some spaces and shapes i will show in which way for example we can see that there exists a galore
group g of the language that is generated by the permutation of the n subjects in this case three
the permutations of the values of each attribute and the permutation of the attributes that have
the same number of possible values in this case the group of subjects permutation is the
the symmetric group s3 the transposition of values will be schema a a 1 a 2 i mean this
transposition and for same for the gender transposition and then you have four exchanges
of attributes defined by this permutation by this permutation sigma k and k3 and tau right
and this group is generated by the group generated by sigma sigma a and sigma j in fact
will be of order eight it is simply the the adrian group d4 of all the isometries of the square
with these vertices and the stabilizer of the vertex will be the cyclic groups c2 of type either
sigma or tau and the stabilizer of an h will be of type sigma a or sigma g and it will be denoted
this one so in this case the galore group of the language will be g which is the the product of
of s3 and d4 that means that in the language here l will be a sheaf over the category g which plays
the role of the fiber f and c as only one object u0 in this case okay so you have four types of
orbits and what is interesting is that in fact all those types in fact can generate a new proposition
in the this big language for example for type one uh can be translated into all the subjects
of the same attributes and this is why et cetera for the other types this is why we need to consider
now information measures semantic information measures that are not only a scalar quantities
as it is a case in channel information measures but these those need to have value in uh uh uh
in the space right which is in this case uh in fact uh okay
so maybe i have to stop sorry okay so thank you very much uh
