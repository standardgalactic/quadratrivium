My lectures are going to be an introduction to the renormalization group from a mathematical
perspective. This is a vast topic whose history spans at least 50 to 70 years.
Inevitably, I'm going to have to make compromises. I'll try to do my best to make these lectures
both accessible to those who have not had any contact with this subject at all and to say
something interesting for those who know a lot about the subject. As you can see, there's going
to be some tension between these things. I'm not going to give a big historical perspective,
but I need to say a few words, I think. Let me begin with that. This is lecture one.
I'm going to try to say what the renormalization group is or what the term means.
There's various places one could start, but I think the easiest one is to start with what's
called Kardanov's block spin picture. This picture is a kind of idealized
version of what the renormalization group is supposed to be. This was introduced by Kardanov
in the 1960s, I guess. Let me first say what kind of problems the renormalization group is
concerned with. It's concerned with problems where, let's say, they can apply to different
problems, but let's keep problems of statistical physics in mind. The kind of problems that
applies to those where correlations or dependence of a long distance remains important. This is
different from situations, say, the hydrodynamic one, where sort of interparticle fluctuations
are known to essentially decoupled from the sort of macroscopic equations of fluid dynamics,
and a simple or perhaps the simplest or most concrete example one can think of would be an
easing model at a critical point. Let me draw a box which is idealization for physical space.
We consider two points, say X and Y, and the situations where the renormalization group is
relevant is when there are strong correlations between these two points. What does this mean in
the case of an easing model? When the easing model, say, we have a microscopic collection of so-called
spins at, say, sites of, in this case, a two-dimensional lattice. One can measure how
correlations, for example, by considering what are called the correlation functions. In the case
of the easing model, one can take the expectation with respect to the easing model of the product of
two spins at sites X and Y. It turns out that at its critical temperature, these behave like a power
law. That's a prototypical situation where the renormalization group picture is relevant.
There's a strong influence between these two points. Now, what does Cardanoff's block spin picture
about? Well, it's the simple idea that one might imagine the whole space as divided into blocks,
say, like this, and instead of looking at the interaction of individual spins in this case,
the idea is that one takes some kind of average of spins over a block. That's the so-called block
spin. There'd be different ways to do this. The heuristic picture is that, well, in terms of,
so each block has an effective spin. The heuristic picture, or the ad hoc assumption
of this block spin picture, is that each block more or less behaves like an easing spin again.
This is consistent. The block spin procedure is going from the microscopic spins to the
block spins. You could iterate this procedure. That's what's called the renormalization group.
It's an idealized picture because if you try to write down any reasonable way to go from
microscopic spins to block spins, you'll soon realize that you get a huge mess. The microscopic
easing model looks extremely simple to write down. Once you try to write down a formulation,
how to go to block spins, and how these interact, you'll see that they interact in a very complicated
way. This procedure, as nice as the picture is, it's infeasible calculation. This is the block
spin picture. There's a second historical motivation for the renormalization group.
That doesn't have to do with statistical physics, but it goes back to quantum field theory.
Quantum field theory, so this is, I guess, in the 50s. The names that are typically cited in this
context are Stuckelberg and Peter Mann and Gelman-Low. It has to do with the famous, or
perhaps I should say infamous, infinities or divergencies that were found to appear when
trying to make sense of, say, quantum electrodynamics. It was realized that the effect of mass or
charge of an electron that one measures doesn't coincide with the microscopic
guess you put into the description of the model, and in fact that the discrepancy between these
was by an infinite amount. This goes back earlier to the beginnings of quantum mechanics, I guess,
but it was realized in the 50s that there's some arbitrariness in the choice of these sort of
effective measurable quantities, which in some sense depend on scale and the microscopic
divergent parameters you put into the model. I will not say much about this at this point,
but let me just record divergencies and scale dependence of coupling constants
in quantum field theory. I think the appropriate list of years to cite here would be the 50s.
It turns out that these two problems have the same root, or they can be regarded as two different
aspects of basically the same question. I think this was really understood conceptually
deeply by Kenneth Wilson, and maybe I think in the late 60s and early 70s, and there are also
many contributions of others, but if one person has to be single out, it would be Wilson in this
context, but maybe a few other names to mention would be maybe Michael Fisher and Wagner and
and others. And let me to lighten these lectures a little bit. Let me let me put a picture of
this. I think this is the picture from the Nobel Prize website they used for Wilson
when he was awarded the Nobel Prize for these discoveries. I like the following quote
how he this is in one of the earlier papers on the subject.
I like the quote how he described the block spin picture, this one. In short,
the carton of block spin picture or block picture, although absorbed, would be the basis for
generalizations which are not absorbed. And in some sense, what he and others understood is a
precise computational way how to understand this seemingly in completely intractable block spin
picture. It's a computational and also conceptually.
These are perhaps really two different aspects of this subject. One is how to actually understand
this picture in a computational way so that you can do something. The other one is the broader
conceptual picture that came along with it. So it's a computational and conceptual
picture approach. So this is sort of let me leave it at this for maybe the
historical perspective on the renormalization group. At this point,
there's a few words I should say which is that the kind of ideas I've presented here,
they're extremely general. You might imagine this kind of approach of sort of this block
spin approach or something that sort of should capture the same features would apply to a vast
range of problems. It turns out it's not so easy to apply it to a vast range of problems,
but I think there's little doubt about it that many of the problems that we don't understand
well would fall into this category. The class of problems where it's quite well developed and
reasonably well understood even though there's much to be done is those which I will call spin
models or spin systems which include the ones that are perhaps historical first ones, the ones
that are shown on this page. So spin models include say the easing model, but I'll also
interpret this term in a sufficiently general way that these models of quantum field theory
are examples of this. So let me briefly tell you what I'm planning to do in these four lectures.
So in the first lecture which is today, I'll discuss
the class of spin models and in particular I'll discuss the kind of models and regimes where
the renormalization group approach is relevant and that can happen as in the example of the easing
model I showed at the critical temperature, but it can also happen in phases of low temperature
or high temperature depending on the model and I will give some examples of this. So that's
part of that's the first half and then I will discuss and introduce sort of the main
protagonist in the study of the renormalization group which is
what is called the renormalized potential.
And I will focus in this lecture on sort of structural aspects of it
and hopefully I will convince you that the renormalized potential has a beautiful structure
and understanding it can be a well I will hopefully convince you that understanding
the renormalized potential is sort of a worthy goal and the remainder of the lectures will be
focused on how to do this. Now how is this done? Well the major source of inspiration
and also a guideline of how to do this is based on perturbation theory and
I have to say a little bit about this. So I'll say how perturbation theory is done
and as simply as I can without going into a lot of details but enough details so that I can convey
what it does and also and perhaps more importantly what its problems are and
I would say there are three problems with it
and I'll try to explain this. Roughly speaking the three problems can be described as that the
first one is that perturbation theory is the one of the infinities if you want perturbation theory
has to be renormalized. The second one is what is known as the infamous large field problem
and the third one is what I would maybe call the infinite volume problem
and I'll try to explain what what what these difficulties are so maybe difficulties is
better than problems but anyway and I'll then show you how one of these problems goes away
namely the maybe most serious of these but technically most serious of these the so-called
large field problem when working with fermionic or grasman variables and this does apply to a
number of interesting problems and I'll explain how this problem goes away there and I think this
is instructive also more generally. In the third lecture my plan is to
introduce the what is called the finite range approach
to the renormalization group.
This is one
of
this is one of different of different approaches to essentially deal with
another of the problems that I'll highlight my second lecture which is that of the infinite
volume problem and so I will explain this in the third lecture or at least that's the plan
and likely I'll not the third lecture will not be enough so this will probably also occupy
quite a well a significant part or at least part of the fourth lecture but
but I make sure that there will be time and then this will be the most technical part of these
lectures and but in the fourth lecture there will hopefully also be some time to give a to
discuss a few softer aspects towards the end and I will try to do this in terms of an example
where I can illustrate various further aspects such as symmetry and I'll do this in the example of
the zero state pots model.
Okay I have I put up the notes I'm writing here as well as
further references for a lot of things that I'll mention but not discuss in detail on on this
following website
and I'll also try to compile a more comprehensive bibliography than
those references that I can discuss in these lectures and in person
and okay so so much for the introduction so the next item my agenda is to begin with the
class of models that I will focus on which are the spin models and and the regime regimes of
their parameters where the renormalization group is relevant.
Okay so what do I understand as a spin system?
Well we fix an underlying graph or lattice which we'll always think of as a subset of zd
for technical reasons I'll always assume that lambda is finite but it will always be understood
that lambda is essentially infinite as well in the sense that we're interested in understanding
what happens in the limit as lambda goes to infinity and alternatively we can also consider
the case where lambda is the subset of a rescaled lattice epsilon zd and in this case it will be
understood that we're looking at the limit at everything uniform in epsilon as epsilon goes to
zero so that that's the situation of quantum field theory the first one is more the situation
of statistical mechanics but they live on the conceptually the same setting
then we have spins I usually denote these by phi x but sometimes sigma and other variables will
appear as well so there's one for each side in lambda and these could be in the simplest instance
you can think of these as say easing variables plus minus one or real variables but they could also
be vector valued its components could be as I said discrete continuous
and or anticommuting and I'll say more about this later
so this is what I'd be calling a spin and then there is some kind of a statistical average
associated to a spin system and I'll use the term expectation and
roughly speaking these these systems take the form that the expectation of some observable f
say f is a function for example on r to the lambda to r so on function of spin configurations
so there's a statistical average associated which is roughly speaking takes the following form
we're integrating over f or phi with respect to a certain weight and the weight usually has two parts
and I'll write this like this
where the first term you can think of as the
in this case a discretized Dirichlet energy where I'm summing over neighboring sites in the
lattice so there's a so lambda is some kind of lattice and we're summing x till the y
means we're summing over neighboring sites x and y and the first term is roughly this Dirichlet energy
and the second term is usually a local in the sense that
it's a sum over spin sites of some local potential vx or phi only at x
so that's roughly the class of models of spin systems and there are various decorations and
variations of this and and this is a class of models for which that as I mentioned both
historically the renormalization group approach was was developed and and also where really
its understanding is most complete and so I already mentioned that the renormalization group approach
would be relevant for such models at the critical temperature
let me say a little bit more precisely so for example if you're looking at an easing model
well that that'd be defined saying in some parameter in terms of some parameter beta greater
than zero which is called the inverse temperature and then this in this case the statistical average
would be given
by summing over all spin configurations sigma which in plus minus one to the lambda
and then
taking the Dirichlet energy term which in this case is just the exponential of beta times
the number of pairs of disagreeing spins that's that's exactly what this reduces to
in this case there's no potential term that potential term is effectively replaced by the
constraints that the spins take plus minus one so in this case
well so spin configuration may look like this
okay and well in this case we know very well how how how these models behave at least
qualitatively that there's a critical value of beta
such that for beta below this value configurations
will look this ordered for beta above this temperature or above this inverse temperature
they will look ordered so roughly speaking most spins have the same sign the ordered phase
and there's roughly where they they look sort of like independent coin flips in the disordered
phase and then there's the critical temperature in between
and this is where the renormalization group picture is is relevant and for these models
where in some cases we know in other cases we expect that spin spin correlations decay according
to a power law okay so now it turns out that you can also look at models that are from a physical
perspective essentially equivalent to an easing model but that are of interest for various reasons
not only its relations to field theory which is the continuous versions of an easing model and
that's called the Ginsburg-Landau 5.4 model
and in this case there are two parameters one g is going to be fixed and then there's another
parameter which will take the role of inverse or temperature which are denoted by nu it takes real
values and then the expectation of the 5.4 model is given in terms of these two parameters
swallows in this case spin configurations are real valued so integrating over r to the lambda
there's again the same
Dirichlet energy term except I'm not putting a parameter beta in front of it and that can
always since the spins are continuous you can always scale it out and put it into the
following two coupling constants and
there's a term g phi to the 4 and there's another term nu phi squared
so this this is the 5.4 model and it in many ways behaves exactly like an easing model
so mostly interested in the case where nu is negative so that the potential looks like a
double well and in this case what happens if nu is sufficiently negative so that it looks like a
deep double double well again configurations are ordered if nu is bigger than nu c the disordered
and again at the critical value
we expect the power law and
the power law which and here here's one of the striking features
that in some sense the renormalization group picture predicts is that
the power law is identical to the one of the easing model
with the same exponent
now I guess this is again a good opportunity to lighten the discussion a bit by putting a picture
and let me say that the pictures I'm putting are somewhat arbitrary those who get included
and don't get included depends a bit on luck but in this case it's well it's
it's a good opportunity to
oh let me try to move it down a bit well it wants to go up anyway let's leave it there
so these are Christoph Gewetzky who sadly passed away recently
but and Antikoupe Einen and I understand Christoph Gewetzky spent a significant
amount of time at IHES so it seems like a good opportunity to emphasize his
his contributions to the subject which are significant and so Gewetzky and Koupe Einen
where I think the first or one of the first two were able to make this sort of this Wilson
picture precise for these kind of critical five four models and later for many other
models as well and in that sense their work plays an important historical role in the subject
let me maybe before going to the next class of models of interest I should maybe say that
well I've already mentioned that this primary alpha or this exponent alpha we expect here would be
the same for the easing of the five four model in some sense this is heuristically explained by
the renormalization group get there and some to a certain extent later on one would say this is
then the same universality class which is that of z2 symmetry
there are many other interesting universality classes
obtained roughly speaking by replacing the group z2 by other groups
so for example there are the potz models where z2 gets replaced by the symmetric group
sn or the on models
or classical heisenberg models and others and this leads naturally to the next example I want to
consider which are the relevance of the renormalization group picture not only
for critical models but also for and one might say perhaps even more interesting
the for systems at low temperature
so for low temperature one might have the prototypical and mind
is a classical heisenberg model
so it's defined exactly as like the easing model except that the spins don't take values plus
minus one but they take values in a sphere it's defined exactly like the easing model except
so every for the heisenberg or n model they take values in the sphere of dimension
n minus one
which we imagine as embedded in rn and then the
the measure is defined in exactly the same way
so here this measure product over sigma x is the har measure on
rotationally invariant measure
so these classical heisenberg models at least in dimension so dimension two is a whole different
story I'm not going to get into here but in dimension three and higher they are again
like the easing model this ordered
for high temperatures so beta small and ordered
for beta large
now it turns out this ordered phase is much more subtle and interesting than the ordered
phase of an easing model though so if you think of an heisenberg on this case I guess for the
purpose of illustration I'm going to draw one where the sphere is just a circle in this case
the model is usually called the xy model or the rotator model a phenomenon that can happen is
well the the spins which are now these errors they can rotate slowly
and this is in contrast to to the easing spins which always have to flip by a finite amount of
time these these can rotate slowly and as a consequence
okay I guess this picture is good enough as a consequence what happens is that if you look at
the spin spin correlation say at for x and y at large distances and beta large enough so that
in the low temperature phase
but to first order they behave in a similar way as the easing spins in the sense that there's
what is called long-range orders so they align over long distances but then the interesting part
is the the next order correction where they have a power law behavior
and that brings this class of problems into the class of problems for which the renormalization
group method is relevant and that's sometimes it's called the spin wave
or goldstone picture
and so the renormalization group has in fact been applied to these kinds of problems and again let
me put a picture this is Balaban it has been applied to these problems but I think it's
fair to say that this a simpler solution would still be desirable these there's a sequence of
number of papers I think it's about 10 extremely which are very complicated and the better
understanding what's what remains desirable but and I think it's physically this is a very
interesting problem you may are in some sense it's less rich than the critical problem but
in other ways it's it's maybe more physically relevant in the sense that there's a number of
other problems of important physical significance quantum versions of these models
superconductivity etc which have a similar picture behind behind them
okay and then since I've discussed the critical temperature and low temperature let me also
say a few words about what can happen at high temperatures
so high temperatures so here again let me just mention a critical or a prototypical model in
in the spirit of the previous ones that are as simple to define as possible and in this case the
the simplest model to define would be the discrete Gaussian model in two dimensions
and again it's it's defined exactly like the other models were the only difference that spins
uh now take values in all integers not just in plus minus so it's like an easing model
except the spins are taking values in all integers
and there's a little caveat here which I'm not going to discuss let's let's fix one of the spins
to be zero because otherwise this integral or the sum is not convergent but I'm not gonna
it's it's a technical aspect and this case well if at low temperature so beta large
it behaves much like a low temperature easing model in this case we would not describe the
configuration as ordered though one could but it would typically rather be described as flat or
localized and one way to quantify this would be to say say that the variance of spin differences
remains bounded and this is in contrast to what happens at high temperatures which in this case
is the interesting or the both regimes are interesting but in this case it's the regime
where the renormalization group is relevant in this case we would say the the the model is in a
rough phase or delocalized so in this case spins remain unbounded and they they they
were spin configuration spin correlations are unbounded they behave roughly logarithmically
so physically this model is a toy model for a crystal interface and this transition between rough
and roughly localized this is the roughening transition
again one can write down
as also in the Heisenberg model but there I didn't write it down again one can
write down a Ginsburg Landau version which is kind of the 5-4 version of the discrete Gaussian
model in this case this is called the Seingordan model and it's again there's a second parameter
in this case which controls the the strength of of the softening of the interaction and
for all physical purposes we think of this model as is completely equivalent to the first first one
and again I've scaled as is customary here to
for a continuous model such that the first term doesn't have a coupling constant in it
okay so this
these models are also of interest for other reasons it turns out that they're equivalent to
a two-dimensional Coulomb gas and in three dimensions they
turn out to be also related to a billion gauge theory but I'm not going to go into this further
I just wanted to give examples of
of relevance of the renormalization group in rather different areas of the phase diagram
the above problems they all discuss
what one would call the infrared problem
so this is the problem of the statistical mechanical problem of what happens at long distances
but as I mentioned this was in some sense I think half of the class of problems that
inspired the renormalization group the other half are the problems that come from quantum field
theory and which here I would call continuum limits
so in this case we're looking say at lambda epsilon which is a scaled lattice
and as before epsilon we're thinking of epsilon goes to zero and then in many cases one can
define versions of the models that I introduced before but on on this rescaled lattice so formally
of course you can just define the same models on the rescaled lattice by just rescaling the lattice
it turns out that if you want to take the limit epsilon to zero you have to be a bit careful
and roughly speaking this is what it is so let me look at the simplest of these models at least
the simplest to define which would be the phi4 model so it's like the lattice phi4 model I
introduced in this case while we're looking at spin configurations on the rescaled lattice so
they are to the lambda epsilon and we'd like to define them on the rescaled lattice and well
what one should do is replace the sum by a Riemann sum so that it becomes an integral
and then we'd like to define them say in the same way as before
except you should also endeloplastia and also rescale it to a rescaled laplacian so that
as epsilon goes to zero this this will effectively become the the continuum laplacian so what you
do is you have to rescale by epsilon to the minus two that that would be the laplacian
and then there's the same phi4 potential as before
denoting the coupling constant by lambda instead of g for continuum
we can also put a mass term gotta leave some space here
and I guess I'm using mu instead of nu for continuum and then that's in the exponential
and then we're integrating f of phi d phi now so this would be just the naive adaption of
the of the lattice definition I gave before except I changed the sum to Riemann sum
and now the goal is slightly different we're interested in taking epsilon to zero and defining
a model on or a measure on continuous configurations turns out that if you do it
the way I defined it you will only get a what one would call a trivial measure or you'll get a
white noise or something like that and it turns out to define a measure that is interesting
non-gaussian you need to put an additional term here I'll denote by a epsilon of lambda
and that is what's called a divergent counter term
and it turns out that if you choose this a epsilon of lambda in a specific way
it turns out there's a simple explicit choice you can make in dimensions two and three
you will obtain a non-gaussian measure and
we also know that in dimensions four you cannot do this and in fact this is a recent result of
Hugo and Eisenman but in dimensions two and three which I'll focus on here you can do this
and what does it have to do with the renormalization group or with the picture of strong correlations
well we can again look at spin-spin correlations and now it turns out at short distances they
behave like a power law
and okay so in that sense they fall into the same category of systems where there are strong
correlations now not that long distances better short distances and you can look at versions
of the other models I discussed if you look at versions of the Heisenberg model that is called
the non-linear sigma model and it's particularly interesting in dimension two
and you could go on
it turns out that these models are very so it's a different
it's sort of a different regime of these models but they turn out to be very closely related to
in the 5.4 model the critical case of or near critical case of the Ising or lattice 5.4 models
the non-linear sigma model would be more closely related to low temperature properties of the Heisenberg model
and okay so this was a brief overview of models just in this relatively tractable and simple
class of spin systems where strong correlations appear in various regions of the phase diagram
and where as I'll try to explain in the remaining lectures the renormalization group picture is
for that reason relevant at this point let me just mention that I mean there's various other problems
that you can look at you can look at dynamics deterministic or stochastic
where these ideas are important you can look at more combinatorial models
such as self-watering walks the dimer configurations etc
you can look at maybe quantum statistical physics models
etc a lot of and others a lot of these problems have the feature that even though
if you see it the first time the class of lattice spin models looks like a somewhat restrictive
class of models if you allow a little bit of flexibility in the interpretation of these
models is actually extremely rich and broad class of model that includes a rather a vast range of
phenomena and different types of models but there are also models that and in fact many
other kinds of models which at first seem unrelated can be sort of reduced to models
like these and that's for example the case for these examples of self-watering walks or dimers
others but there are also other models that don't look exactly like this and where it be desirable
but not so much well there's been some progress on such models but not so much is understood
I would say I like so maybe I can before taking a five-minute break conclude with this
other quote I extracted from one of Wilson's papers it turns out he has a he is a he is a
number of very nice papers with if you're interested in a historical why is this
anyway it doesn't want to move where it should be but anyway we can leave it here
he has a number of nice papers where he discusses the historical aspects
and the philosophy behind this approach I linked some to the website so here's one quote I like
this was in 1975 where he wrote it's a present an approach of last resort to be used only when
all other approaches have been tried and discarded the reason is it's rather difficult to formulate
the renormalization group for new problems etc and then he concludes that in it will probably
require several years of stagnation and elementary particle theory before theorists will accept the
inevitability of the renormalization group is approached despite its difficulties this was in
1975 I think it's it's fair to say that by now it's it's unthinkable to think of much of modern
theoretical physics without the renormalization group approach and it may require several more
years of stagnation in other areas as well for this but anyway let me leave this quote as it is and
maybe this would be good time to take a short break and then after the break I will
begin with more of the mechanics which is the definition of the renormalized potential and
some basics and background on Gaussian integrals which are required to understand this okay great
maybe to to leave time for the people on site to to have coffee maybe we make a 10 minute break
and we resume at five yeah that sounds good
um are we allowed to ask questions at this point yes of course I mean you're allowed to ask questions
during the whole thing not have to wait okay thank you yes please do interrupt me at any point
and um yeah so just a small question when you were introducing a lot of these
examples you had a distinction between the ordered phase the disordered phase and the critical phase
just to affirm this is always with respect to equilibrium states correct yes I I've only focused
on equilibrium here and I will only focus on that during these lectures yes okay but then yes there's
there's a very I mean the picture gets much much richer if you add dynamics into it as well
but this is all equilibrium okay so actually that was my next question about the dynamics
because sometimes if you have stochastic dynamics you might leave equilibrium so you won't be
considering that situation in your talks I won't be considering that situation it's a situation that
I think is very interesting but in fact um it's not very well understood mathematically
there okay can't do too much there I see and and in particular do we understand how to
define order and disorder in this context probably not yet right um as like some kind of perturbation
in the space the space of states or something like this well I am not sure okay yeah okay thank you
so in in this second half of the first lecture I'm gonna introduce some of the
well the one of the main objects in the study of the renormalization group which is the
renormalized potential and to do this I need to discuss a few aspects of Gaussian integrals
so this is maybe 2.1
so first of all in the way I wrote down these these models previously
um there was always a Gaussian factor um the nearest neighbor coupling and
some kind of interaction which was either enforced by a constraint or by a potential
and so it turns out that much of the renormalization group perspective is is based on
um a detailed understanding of the of the Gaussian part um as as a reference and
so the Gaussian part is just what's known as the free field um so the expectation if we drop all
interaction uh just be a free field um and in particular I'd say it's the Gaussian measure
so it's completely determined in terms of its covariance and its covariance is
the two-point function spin-spin correlation function which in this case maybe actually
to make things a little bit more well defined I'm going to add another
quadratic term so-called mass term and then
um the two-point function is simply the the inverse of the Laplacian or the Green's function
and um so of course um I think I
I'll not have to explain but let me still summarize this how the Green's function behaves
and um say um I always like to think about the finite um
about a finite set lambda and usually I it'll be nice to take a torus so periodic boundary conditions
and then we could ask well and then if you look at the say the Green's function
um in the infinite volume limit
and as the mass goes to zero
well we know how it behaves and this limit is is like x to the d minus two and dimensions three
and higher and it's it's infinite in dimensions to a lower and you can ask more precisely what
happens in dimension two and if you subtract the divergent term which is just the diagonal term
um
and take the same limit
well then it behaves uh
logarithmically in dimension two and linearly in dimension one
and uh while all of these spin models I defined um the first part of the lecture looked like some
kind of perturbations of a free field um it turns out they're rather singular perturbations
and a key difficulty is that uh well the Gaussian field they're at least formally perturbations off
is a field that has these correlations I just wrote down so they're very strong correlations
and it turns out the perturbations don't look very small if you do this and that's what I'm
going to try to explain um and but it should be plausible from this uh that it may be useful to
decompose these singularities or divergencies of the free field and look at what happens
at one scale um at a time so you can decompose you can imagine decomposing
x to the minus d o minus two in various ways and similarly for the log etc we want to do this at
the level of of the Gaussian reference measure so let me just uh set up some notation for a
Gaussian measure so I'm going to use p subscript c for the Gaussian measure with covariance c
so it's the density is maybe determinant two pi c to the minus one half and then the density is
minus one half phi c inverse phi and e superscript c will denote the corresponding expectation
so this is the integral of f of phi p c phi d phi um now there are another factors
well that if you have uh two Gaussian measures and you convolve them you obtain another Gaussian
measure and the new Gaussian measure you obtain has the covariance which is the sum of the two
covariances so if c is the sum of c one plus c two where c c one and c two are all
positive definite not bigger than two bigger than zero
um then
p c is the convolution of p c one and p c two or equivalently you can do the expectation
with respect to the Gaussian measure of covariance c by doing first the one with respect to c one
and then the one with respect to c two say phi one plus phi two now there's a little uh I'm
sweeping uh subtlety under the rug here since I usually talk about positive definite meaning
positive semi definite the formula as I wrote down only makes sense if it's positive definite but
that is easy to take care of and I'm not going to do that here
um
so this basic identity plays an important role you can also do it continuously
so if you can write the covariance c as an integral of other covariances c t dot
where the c t dots are now negative now then well the convolution property
becomes the heat equation so it's the fact that if you have the Gaussian measure p with
covariance c t it satisfies a heat equation where
it looks like this where this Laplacian c t dot is a very high dimensional one
it's the sum over x y and lambda c dot t x y so that's the covariance the x y element
the covariance matrix and then there's a second derivative in direction phi x and phi y
and so this fact is just the straightforward generalization of the fact that if you have a
Gaussian density it satisfies a standard heat equation if you put a covariance in that varies
in time this is the heat equation you get and correspondingly you can also have an analog
of the iterated expectation property which we may formulate now in the way that
phi can in distribution be viewed as a stochastic integral
where these dw's are a Brownian motion on r2 the lambda and so this this phi
has distribution pc of the Gaussian measure so this these properties of decomposing Gaussian
measures are in some sense the starting point for the quantitative understanding of the
renormalization group Wilson spirit we have a Gaussian reference measure say the free field
and we can decompose it using using this property if we can write its covariance which
is the green's function as a sum of covariances or as an integral and it turns out there's
different ways to do this and there's not one best way to do this
so question is how to decompose
the free field so oops let me write down a few different ways you can use
one of the most canonical ones would be what I would call the heat kernel decomposition
you can write you can of course write the the green's function as an integral over heat kernels
so that is one way there's there's other ways another one which I call the pauli-villars decomposition
is you can write well the green's function as an integral over a green's functions with
by varying the mass parameter so we take the integral 0 to infinity of d dt of
this Laplace plus m squared plus 1 over t dt so explicitly this is
one
you can do other decomposition you can do a block spin decomposition
I'm not going to write down what this is precisely this is one so all of these decompositions I
wrote down are continuous the block spin one isn't it's sort of a it's a particular way you
can decompose the the gaussian measure that is motivated by by the card enough block spin picture
and in some sense closely related to it and that was used for example in the work of
Gevetsky and Kupiainen I cited earlier so this is the block spin decomposition
you can use a certain decomposition which I will be talking about more later on
which is called a finite range decomposition
in which case we decompose the inverse of the Laplacian
in terms of these matrices ct dot
with the so that the ct dots have a finite range property ct dot x y is zero
if the distance between x and y is bigger than t so note that the heat kernel decomposes so the
heat kernels or the greens functions in the polyvelocity compositions they all have approximate
versions of of this locality property but not exact so for example for the heat kernel decomposition
you essentially have a gaussian tail but it turns out you can make make this exactly zero that has
technical advantages you can also use what I guess it would be called a momentum-based
decomposition yeah and maybe I'm a bit lost I don't really understand why you you need to
introduce such a decomposition so all these are are are the composition of these resolvents but
it's not clear to me why yes I will explain I will explain this more so in some sense let me
so we have this free field and it has these correlations which are very singular right it
has these singular correlations and we're interested in studying perturbations of this
and it turns out that if you try to study and that this is what I'll explain this
concretely so I think things will become clearer then if you try to understand use this full gaussian
measure as a reference measure and look at perturbations of this what you'll obtain is
going to be extremely singular you'll get divergent coefficients and etc so these are the
infamous divergencies of quantum field theory in some sense the renormalization group pictures so
these decompositions are sort of a proxy for Kardanov's block spin picture rather than looking
at the whole system at once we want to look at what happens at a single length scale so say
what happens if you go from microscopic points to like blocks and then from blocks to bigger blocks
and so on and one way to do that would be to go from one length scale to another you can do this
continuously as I as here or in the discrete steps but as I'll explain one way to achieve it
is to start from a decomposition of the free field does that make a little bit sense
so yeah so that's the motivation thank you please do ask at any point because it's
it's better to sort out questions as we go
so another way to do it is one one might call a momentum space decomposition which
has a kind of a finite range property in Fourier space
and so these are different ways you can decompose the reference free field
what is it turns out they all have different advantages unfortunately so for example the
heat kernel decomposition is has many nice properties it has very fast decay etc but it also
has a nice positivity property for example the heat kernel is point-wise positive right
the polyvillage decomposition has well it doesn't have such great decay if you compare
this to a heat kernel it has a rather slow tail and for that reason it's it's not
usually particularly good decomposition but it has other good properties which is that well
determine the decomposition is well is a local operator or it's the inverse of a local operators
and so in that sense it often preserves the property of say fair magnet the fair magnetic
property of spin systems and other so it's it's low it has a locality property which makes it
has nice properties the finite range or the block spin picture or decomposition has a nice
property in the sense that has a comes with an exact reduction of dimension you go from single
points to blocks and so somehow the dimension of the problem gets reduced as you go on i haven't
explained what it is but to take my word for this the finite range decomposition has a nice
compact support in real space property and the last one has a nice compact support in Fourier
space property now it turns out and it may sound a little bit silly if you haven't seen this before
but a lot of the technical difficulty of the renormalization group is related to the fact
that these all have nice properties but you can't have all the nice properties at once
so what i should emphasize is these decompositions you can't do anything you need to do something
so that the term that appear in the integral or in the sum are positive definite and that that
poses a constraint a serious constraint of what you can do so sort of a philosophically technical
difficulty
different decompositions
different advantages
and we don't know very well how to harness all of them
um in fact there is a sorry there's there's in fact a toy model you can define it in a
hawk model you can define by just requiring that all five good properties hold simultaneously
and that model is called the hierarchical model i think it i'm not going to be talking about the
hierarchical model these lectures but it's if you if you look at the renormalization group
literature this is this is an example that that you'll encounter again and again and i think
one can characterize it uniquely to have all five good properties unfortunately if you if you go
away from the hierarchical model you will lose some of these good properties and you have to work
around the others you have you can keep some but you have to work around the ones you lose
okay so so we've looked briefly at how to decompose the free field now we can
uh look at um the renormalized potential
so we're interested in
understanding the measure with expectation i know by bracket i'm putting a subscript zero now
which is of the form say Gaussian density with covariance c
and then some potential which i'm not going to call v zero
we've seen that so the covariance is the reference covariance is a free field covariance
and we we've seen we can decompose it and what we can also define is we can define a
regularized version where we use the decomposition up to some varying scale parameter t so instead
of integrating up to infinity i'm just integrating up to t so that i'm going to call covariance
decomposition and then we can define and this is sort of well one of the
protagonists in the subject it's called the renormalized potential defining it through
its exponentials vt as defined by just convolving
the Gaussian measure with covariance ct with respect to the exponential of the original potential
or in other words we're taking a Gaussian expectation with covariance ct
and then we're looking at e to the minus v naught of phi plus zeta
where zeta denotes the field under under this Gaussian expectation and phi is fixed
and so this this object is called the renormalized potential
so definition is very simple
so here are another few facts well first of all
using the decomposition property of the Gaussian or the convolution property for
Gaussian measures well we can we know that e to the vt of phi well it can be obtained
by starting not with a potential at scale zero v zero but instead by v s and then
convolving instead with the Gaussian measure with covariance ct minus cs so
this is like this
or continuously e to the minus vt satisfies this heat equation
so let me write recall again the notation and write out
sorry yes I think the potential in the first line should be v s sorry
the renormalized potential in the first line should be v s instead of v zero
in the first line oh thank you yes yes
okay so so the exponential of the renormalized potential has has a structurally very simple
structure of the heat equation the if you take the logarithm of us you can also check it
satisfies a nice equation and it's called the pulchinsky equation
it's like a jacobi type equation
so dv dt
satisfies
the same jacobi type equation let me just write out what exactly it means just to get the notation
so the first term is the second derivative of vt with respect to phi x and phi y the second term is
and I guess at this point there's another opportunity to put a picture let me just put a picture of
of pulchinsky here
so this is a structurally very nice equation and I try to convince you in this lecture that
it's it's worth understanding how to study this vt and in some sense that's much of the problem how to
do this now once you know let's suppose we know what this vt is yes
so are you saying that on the finite lattice all this formal manipulations are actually good
yes everything's on a finite lattice it makes perfect sense the definitions are all
completely I mean it's a finite it's there's no problem defining any of this right because
we're on a finite lattice the problem is how to get estimates that that that do not depend on the
fact that the lattice is finite put it that way but it's a very nice equation if we're on a finite
lattice so why why is it useful to study this renormalized potential well you can introduce
another object which is the renormalized measure
well we would so our initial expectation was had a subscript zero here I'm now going to put a
subscript t and this is defined like the original measure except we replace the potential by vt
and we replace the Gaussian part by the Gaussian part that we haven't integrated or
convolved into the potential so that that would be instead of c infinity inverse we put c infinity
minus ct inverse here and then we put vt of phi instead of v0 of phi
so that that one could call the renormalized measure and whatever appears in the exponent
is also called you can give another name ht of phi that's sometimes called the wilson action
um maybe I'll move this next slide
and um well what do we do with this so what do we do with this renormalized measure or with
the wilson action assuming we can find it um in principle we can find it whenever we're on a
finite lattice but suppose we can find it efficiently or quantitatively well um well
ultimately what we'd like to do we'd like to compute for example a correlation function
other expectations with respect to the original measure so we want to look at observables
um so given an observable
say f which is a function of spins say like this
question is can we find ft renormalized version such that the expectation of the observable
f with respect to the full measure is equal to the expectation of ft with respect to the
renormalized measure and the answer is once you know the renormalized potential yes we can
maybe it's another fact or exercise if we suppose
that um
ft satisfies the following linearized version of the pulchinsky equation so lt is the linearization
um
well then you can check that the expectation of ft with respect to the renormalized potential
doesn't depend on t and since there's only one line let me let me just do it
want to check the derivative is zero so what is the derivative well up to a constant i didn't
write above but um if we write it above what we have is a well a gaussian measure with covariance
c infinity minus ct and then we have the renormalized potential e to the minus vt
and then we have the time evolving or scale evolving observable ft
i want to check this this derivative is zero well why isn't that well the derivative can act on
well either the first term or the second one if it acts on the first term well it's a it's a
gaussian measure with a t dependent covariance so it satisfies the heat equation but actually
be careful there's a minus in front of the term with a t so it's a heat equation backwards in
time if you want so if you take the derivative of this it's minus
ct of um pc infinity minus ct um if you look at the other term and take the t derivative
well it turns out the definition of ft was just arranged in such a way that this satisfies
uh forward in time heat equation
um so the first term gives a backwards in time heat equation the other one a forward
in time heat equation but you can always integrate the laplacian by parts so you get the sum of two
terms which are equal with a minus sign in between so it's zero um and you can also write an explicit
formula for ft which is that ft is equal so ft of phi is equal to e to the plus vt of phi
just writing this down to illustrate that in principle structurally all of this structure
is actually very nice and and simple and as you can see from the equation for ft once you know
the renormalized potential it's a linear equation and in fact if you for example use the renormalization
group in the spirit i will explain later uh to core to study correlation this kind of procedure
is what one can do it's it's it's often not written in the differential form here where
somehow it becomes a bit cleaner but it's done in discrete steps for reason i will explain
but um in principle once you have the renormalized potential uh you you have uh you can study
and have sufficient knowledge of it you can you can study what you want more or less
so this is so anyway the upshot is there's a nice structure um
so here at this stage you're deviating a little bit from the physics literature as far as i can see
right so yeah i mean that you can so i've i've in preparation i've done some reading and uh
there i also found some physics references which which do essentially uh which look at
essentially the same um but but yeah i mean um um the central point to the center point to you
but to say well you just split phi into some part which you integrate out and then you
yes so yeah you yeah i mean that that's what i'm that's what i'm doing here right i just wanted to
give some um um motivation or some perspective of why a good understanding of the renormalized
potential is very useful and actually take it back i think what you're doing is is just a smart way
of expressing what we do with physics anyway so sorry i take it back it's yeah i think it's i think
it's the same yeah i think it's the same so anyway there's there's a beautiful structure here i think
and i hope i i mean um and it's very simple but the question is how do we understand it
maybe as a nice exercise let me mention so i've written an equation for the effective potential
the renormalized potential which is the pochinsky equation but you can also derive equations for
all sort of other objects appearing here which are also somewhat nice maybe the pochinsky equation
my view is the nicest of these but you can write an equation for the wilson action uh ht
in fact this i think uh well this appears in one of the earlier papers of wilson
for the renormalized measure etc
so i've already mentioned the upshot it's a beautiful structure
um how do we analyze it
and before explaining some of this there's a point i need to discuss which i have swept under the
rock so far which is the topic of rescaling um so maybe it's a remark
as i if we go back to the cardonov picture i explained in the at the very beginning i um i
sort of motivated that when you start from points you re-block them into blocks and you think of
an effective model defined on blocks and then you iterate this and somehow the blocks become the new
proxies for for the initial points so you'd like to think of the point the blocks as as the new
point so in some sense you might want to rescale the blocks back to points etc in what i explained
here this decomposition of the gaussian does provide a way that to approximately implement
this this block spin picture in the sense that we can decompose this gaussian measure according
to one of these ways look at the renormalized potential and it's and in some sense this renormalized
potential is whatever the the effective model at the level of the next blocks is except i haven't
used i haven't insisted on on on on really going from points to blocks i've just
insisted on or i've replaced this with a substitute of of changing what the gaussian
reference measure is but you can think about it a little bit and you realize that it's more
or less the same thing except it's it's done in a sort of analytically nicer way than while
this block spin procedure is intuitively very appealing it's analytically not very nice to work
with and and this is maybe a nicer way but philosophically the same the aspect i didn't
discover or it didn't discuss is the rescaling aspect and so let me just say a few words about this
so the covariance is
ct dot so ct dot is for example a heat kernel right so they're approximately scale invariant so if
you're on the continuum they're really scale invariant we often like to start with the lattice so
then they only become scale invariant somehow as the limit as t goes to infinity you do some scaling
so the covariances ct are approximately
scale invariant
so if say ct dot is the heat kernel
then well ct dot x y is is roughly speaking
something like this
roughly speaking heat kernel so you you can rescale so let's say i'm rescaling this parameter t
which i sometimes call time but really it's better called scale say by parameter l and you scale x and
y also by l i'm rescaling scale by l square t r because i'm using the standard heat kernel
parameterization and then space has to rescale rescaled by l and it's also good to rescale the
measure we're integrating the heat kernel over which is dt which has to be rescaled by l squared
well then this is the same as l to the d minus 2 ct dot x y dt up to a lattice effect etc
so the c dot which appeared in the pochinsky equation or
elsewhere is is approximately scale invariant and so it would be natural
to add a rescaling procedure to the equations
so
so there are different philosophies about this the entry i mean for
different philosophies how to do this i think you can either do add the rescaling explicitly and
for example when wilson writes down his equations that's that's how it's done and at some point
you need to look at rescaling but the equations become nicer if you don't do it explicitly say
as in pochinsky equation there's no explicit rescaling and one just imagines this potential
you obtained vt ultimately everything should be maybe composed with a scaling map
but you don't write it down explicitly i'm going to take this this point of view here
and
and well then once you take this point of view it makes sense to ask about
what i call renormalization group fixed points or stationary solutions to the pochinsky equation
up to rescaling that's a asymptotically stationary solution
and conceptually this is a this is a i think really important notion that the renormalization
group idea of thinking about the problem
introduces so renormalization group fixed points should be let's say asymptotically
stationary solutions
let's say to the pochinsky equation or any of the other equivalent equations
up to rescaling
and the picture or i guess conjecture or
is that for example if you look at an easing model in
particular three dimensions
um
okay let me
discuss that in a second is that well you can have different fixed points
one is the fixed point
which would be just the free field which in the pochinsky equation just corresponds to v equal
zero surely that is a fixed point and a stationary solution to the free field to the pochinsky
equation even up to rescaling if you rescale zero but then and it and the sort of the picture is that
in and dimensions above four this is essentially the only interesting one there's also what is
called the high temperature fixed point but let me not discuss this much here
so the free field for for critical phenomena is supposed to be the only one if you on dimensions
above four but in dimensions below four we expect there to be another one which is called the
wilson-fischer fixed point and it appears exactly at dimension four so it appears in dimensions less
than four and the critical behavior of the three-dimensional easing model is supposed to be
described by this wilson-fischer fixed point and unfortunately i'd love to but unfortunately i will
not say much about this in these lectures and especially not dimension three but a concept
that has been very instructive is to look at what happens is as that i mentioned varies
not in terms of integers but continuously well to mathematicians this is a concept that may sound
a little bit not sure frightening is the right word but it's not the conventional thing to do and
in fact there's different ways of trying to interpret what that's supposed to mean
one could one way to interpret it is for example replacing the Laplacian by fractional power that
that has much of the effect of a fractional dimension even if i would say not the complete one
but okay let me not go into this and and only mention that if you do this in some way or the other
the distance between these two fixed points is supposed to be of order
four minus d the positive part so if the dimension is four both fixed points merge into one
so about four dimensions there's only the free field fixed point left in four dimensions there's
it exactly for dimension there's a bifurcation point where the two fixed points come together
and you see you can see this in various properties of the four-dimensional model which is the critical
dimension for these models in terms of so-called logarithmic corrections and if you go below
four dimensions well you you see the new fixed point appearing and in particular it's supposed
to be there in three dimensions and so this is another opportunity to insert a picture let's see
where he is i've already so i mentioned this is called the wilson fischer fixed point i've already
had a picture of wilson this is michael fischer um um who who studied this picture in with wilson
and um to be honest well to be clear this is i mean for dimension four close dimension four
dimension close to four slightly below four there are certain things one one can do mathematically
but i i think in general understanding the robustness of this picture and the beauty with
that is remains mostly elusive um okay um so much for that question yes yeah so i guess the wilson
fischer fixed point represents something like the critical the behavior at the point of phase
transition yes description uh yes so that's the i guess the point of view from statistical physics
what does this represent from the point of view of q of t i in the uv uv limit
well it it's um for the uv limit the one you see is this the free field fixed point so it's it's not
really um it's the the the the free field fixed point is the one that is in dimensions two and three
that is stable for the uv um yeah i mean you you can yeah
yeah okay yes
um so what i what yes this is notion of distance you mentioned well well i mean okay so this is
something i will not explain in these lectures i just wanted to mention it because i think
i mean it's important and for the to get some i well to discuss the idea of of this
renormalization group picture but what what you can do in perturbation theory which is the topic
i'm going to discuss uh beginning with the next lecture um you can write down
these well approximate versions of the so-called flow equations how the effective
potential changes as you change scale um and what you see is if you uh if you do this and
dimension four minus epsilon in a suitable way and uh for example by replacing the Laplacian
by a fractional power you can write down these equations say for five four type models and what
you see is that um uh in four dimensions uh at least this perturbative way you you can only
see the the free field fixed point but if you go to dimension four minus epsilon uh you see
there is a new fixed point appearing in these equations so this is essentially at this point
it's a level it's a computation at a level of a single variable it's a single coupling constant
where you can uh write down in this approximate equation for and you see the parameter epsilon
appears and um in some sense the location of this fixed point the Gaussian fixed point is at zero
and and this set up and and the non-Gaussian one will be at a coupling constant which is close to
zero of what epsilon um i'm not sure if that was a convincing answer but uh i'm afraid this is uh
as much about this as i can say in these these lectures or as i'm going to say um because i
want to focus on other aspects that um um um um this is an interesting aspect and it's one aspect
i could one could focus on but there's there's many other things to discuss in somewhat conceptually
simpler contexts where the uh uh technical machinery ingredients um uh are very much the
same but uh i focus on a simpler setting um okay um so next time what i'm going to do is
i'm going to explain how perturbation theory works because that's sort of the main guideline
how to get understanding of the renormalized potential um so i think this i mean the i think
this might be a good point to stop for today and resume there next time this is a logical
division and so maybe maybe let me do that thank you for your attention
