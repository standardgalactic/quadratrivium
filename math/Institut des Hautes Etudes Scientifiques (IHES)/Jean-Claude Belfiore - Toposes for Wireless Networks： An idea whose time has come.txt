Okay, thank you very much, Mouan.
Yeah, exactly.
There is, in fact, I hesitated regarding the question mark
because at the beginning, I was not sure at all.
I mean, there are many aspects of toposis probably
that could be right now very interesting for what we are doing,
but then there are some other aspects.
For now, I have no idea of what they could bring
to our activity at Huawei and essentially in wireless networks.
Okay, so in fact, this talk is essentially a way of trying to identify
the topics covered by these wireless networks,
the topics in which toposis can bring some value.
And in fact, at the beginning, I had very few of them
and then by thinking, by looking at the literature,
many other ones came to my mind and I think that it's just the beginning
and probably in the near future, we will be able to identify
even some other topics very interesting and that could bring some solutions
to some of our problems.
Okay, so let's start with the problematics that we have in wireless networks
and that could be impacted by toposis.
Okay, so first, what is wireless network?
What is the wireless communication in general?
In fact, it is something that uses a lot of tools
that are very diverse. For example, information theories,
so Shannon theory essentially,
signal processing because in these wireless communication networks,
in fact, we have to transmit bits and the problems are to transmit bits.
These bits have to be converted into signals and then, of course, at the
receiver, these signals have to be processed in a way
that we can, for example, minimize the irreprobability
or some other measure. We have also optimization.
For example, I mean, there are many points where optimization
can be useful and I can give you an example.
Suppose that you have, in fact, in wireless networks, in fact,
the wireless network is a cellular network and
the problem is that each cell interferes with the neighboring cells, right?
And the problem, of course, is that if
everybody starts to talk loudly, the problem is that
you will impact the signal-to-noise ratio of the neighbor
that will start to speak even louder, etc., etc., and so, of course, it will
diverge. And that's why there are some procedures
which are called power control in order to avoid these problems
and we have to find the optimal solution of power control,
so it's an optimization problem that can be centralized or
decentralized. We have also problem related to data fusion.
For example, if we have sensors, we want to use them in order to localize
terminals or in order to do something else.
And then we have graphs. So graphs can be used to study the cellular network.
It can be used also for channel coding. I will give you some examples afterwards.
And finally, maybe I forgot some of them. Sorry for that.
And finally, at least in my list, there is channel modeling,
which is very important because this is the basis of the wireless network.
And in order, for example, to be able to transmit or to
receive efficiently the signals that have been transmitted,
in fact, we need to know the behavior of the channel
and we need to model the channel in some sense.
Okay, so you see there are quite a lot of technical areas impacted by wireless.
So now, okay, if I just copy the definition of the grotendictopus,
what can be the relation between this and this?
This is not obvious at all. And this was the problem that
I was faced with that problem. So which of these items can be impacted by
this? In fact, it's not easy at all to answer directly.
But just using the definition, we can first try
to find some relations between some of these topics.
Maybe all of them, you will see. The first one, for example,
Daniel has shown in his talk the relation that we can find between
information theory and the topos. For the other ones, there can be some
relations. And in fact, I will start by finding
some relations between these points and sheeps, okay?
And moreover, there is not a constraint we have.
The problem is that we need to implement algorithms.
We need to perform computations, all right?
And so we need to use computational and algorithmic
mathematics. And that means that, for example,
the sites or the topological spaces that we will have,
that we can use will be quite restricted. They cannot be...
I mean, we need to find the ones onto which we can compute some interesting
quantities, like comologies, etc. And also, we need to find,
let's say, some... We need to find also
sheeps that give rise to computations that make sense for us.
For example, what we can do the best is linear algebra, in terms of computation.
And that's why we will mainly focus on these restrictions.
Okay.
So, first of all, I needed to analyze why we need...
We could need the sheeps or more deeply toposes.
In fact, this property of sheeps is the one which we need the more.
That means the how to... I mean, from local to global,
we have local data how to reconstruct the global picture.
And in fact, that's why the computations that we
will mainly be faced to will be the computation of
cohomology groups, etc., etc. And also, sometimes,
to derive some criteria for setting to zero some cohomology groups in order to
remove the obstructions. Okay.
And so, okay. For example, our local data can be local
probabilities. They can be LLR. So, this is log
likelihood ratio. So, it is related to probabilities.
For example, for coding, for channel coding,
they can be local interferences if we consider
a cellular network and we want to go from
the local interferences picture to something more global.
And so, in this case, we need from there to derive the right sheeps.
And so, for the last stage, I have to confess that I haven't too much idea for
now. Okay. Let's start with one of these
applications, which is, in fact, the channel. It is a basis. The channel is the
basis for us. Okay. So,
first topic is, I would call it learning the channel.
I can tell you why. Suppose that we have,
for example, this is a cellular network with
here a cell with its base station. So, you see
the antennas and a mobile terminal that communicated
with this base station. I consider in this example the
uplink. And so, we have, let's say that in general, if we use a
simple model, we can say that what we receive in
on all antennas, so this is why there is this word,
MIMO, which means multiple input, multiple output. So, multiple antennas
at the transmitter, multiple antennas at the receiver.
So, in fact, it is this model, which is the simplest and the most commonly used.
So, what we receive at all the antennas is modeled as a vector.
Why? And it is some matrix H times what is transmitted by the terminal,
which is also a vector, and plus some noise.
See? This noise component, in fact, can incorporate also
interferences coming from the neighboring cells.
So, as you can see here, the channel is characterized by this matrix H.
All right? The problem is that we have to recover the
signals that have been transmitted here, so which are embedded in the vector X.
And of course, if we don't know X, we don't know X,
and if we don't know H, then we are in trouble.
So, what? No, no, no, it can, it's a rectangular matrix in general, yeah.
So, the idea is that, of course, we have to know in some sense
the H matrix. So, what we do in this kind of network
is that we transmit what we call pilot sequences, which is in fact
a sequence of non-symbols, and onto which we will project
the coefficients of this channel matrix so that we can estimate
these coefficients, all right? And at the end,
so, our new problem is that we have just to
detect what has been transmitted in X when we know,
in some sense, or when we have estimated the channel coefficients
which are embedded in this matrix H. Okay.
So, this is what happens in the multi-user case, so you see
you have exactly the same kind of expression,
but then we have the sum of the two components coming from the two
terminals. So, now for 5G, 5G in fact is using what is called
massive MIMO, which means that at the base station
we have a lot of antennas, many, many antennas.
It can be 64, 128, okay? And what is called CSI, so it is a channel
state information, so it is the knowledge of the channel
or the knowledge of the H matrices, lives in this case in a high-dimensional
vector space because of massive MIMO.
So, the problem is that in order to estimate all these coefficients
we need to send a lot of pilots, all right?
And at the end, the problem is that if we send pilots then we have
less space for sending data. And that's why one possibility
could be this, because in fact the CSI lives in a very high-dimensional vector
space. In fact, what is sure is that for
given cell, let's say that the set of all
possible CSI doesn't fill the whole space, but rather lives
in a mini-fold, which will be of dimension,
which can be, if we have many, many antennas, it can be much less
than the whole space, the whole original space, all right?
And so, the idea should be here to go from this high-dimensional vector that
requires a lot of pilots and so which will,
let's say, prevent the transmission of high-rate
datas. Instead of using the whole space, just try to use
this mini-fold of lower dimension, all right?
In this case, it will reduce signaling and also reduce the feedback. So, the
feedback can be necessary, for example, if you want to
transmit then, for example, in downlink from the
base station to the terminals, the base station has to know
the channel which goes from the base station to the terminals
and which is accessible only at the terminals, for example,
I mean, in some cases. Let's say, let's say, not to general.
And in this case, learning this mini-fold, knowing what could be this
mini-fold could be very viable for us, okay?
Then, another problem, I mean, another application
related to the channel is fingerprinting. So, there are some people
in our lab who have implemented some fingerprinting approach
in order to have the localization of the terminals
by using machine learning. The idea is to use the CSI,
so the channel knowledge at the base station, in order to determine the
localization of the terminal, okay? So, what is the idea?
The idea is that, in fact, by using supervised machine learning,
we train a neural network. It can be extreme learning,
I mean, there are many possibilities, in order to learn, given,
let's say, function phi, which defines a mini-fold here as well,
and this function, in fact, is a function of the CSI
and of the position of the terminal, okay? So, we train a neural network
in order to learn this function, at least partly,
and then, once this stage is finished, we can find the
localization as a function of the CSI, all right?
The problem is that, in this case, as well as in the other case,
what happens is that the mini-fold we have to learn,
in general, is learned with noise, it can be noisy observation,
we can have small variations of the channel,
because the problem is that the channel behavior depends also on the
obstacles that you can find on the path of the
waves, and these obstacles may be fixed, or moving,
like cars, or human beings, I mean, yeah?
So, this is another, let's say, example where we need to learn
some mini-fold, which can be impaired by noise,
by some cluttering, by, you know, some small changes of the channel,
all right? So, and for these two cases,
there, in fact, we can propose to use persistent homology
to solve, at least partly, these problems, okay?
Another problem, so what is quite amazing is that this morning I have
discussed with Danielle about these belief propagation on graphs,
and I didn't know that there were some solutions,
I mean, based on the computation of homologies, and
yeah, so that's good news for us. Okay, especially we have this problem
of making belief propagation work on graphs,
essentially in channel coding, okay? So, for example, in 5G, so in the
fifth generation that we are developing right now,
of wireless networks, wireless cellular networks,
in fact, the channel codes that have been
adopted for 5G, I mean, for what has been done right now, but
maybe there will not be so much changes in the next releases.
There are what are called LDPC codes for, in order to encode
data bits, and polar codes in order to encode
control bits, control channels, so data is the information that
is sent, and the control is the bits that are useful
in order to make the network work, okay?
So, and these two families of codes are quite different.
LDPC codes, in fact, are decodable by using a belief propagation
over a graph, which is called a tannograph,
and polar codes are decodable using another kind of algorithm, which is called
serial cancellation, and apparently it's not related to
belief propagation, not yet, let's see.
So, what we know about the decoding of LDPC codes is that
the tannograph of LDPC codes, on which we have to run
belief propagation, is loopy. We have cycles,
and so, in this case, we know that belief propagation doesn't work.
I mean, we cannot make it work well, or it's something that
is very painful, very complex, and so, okay?
Oh, sorry, yeah, yeah, yeah, sure, sorry for that.
So, you have a graph, yes, and you, the idea is that you have
some probabilities on vertices of this graph, right?
And the idea is that, for example, you have some constraints.
Some vertices can correspond to some constraints,
okay, that, in fact, will change the problem.
You start with unconstraints probabilities, and then you take into
account these constraints, right? And you have, the idea
is to do what is called also message passing on the graph,
on the edges of the graph, so that you take into
account locally all these constraints, right?
And at the end, the idea is to have a global understanding.
That means, at the end, let's say, the optimal thing
that you have to do is to, let's say, to take into account,
let's say, all these constraints, right?
And if you have a tree, then it's quite easy to do it, right?
But if you have cycles, then we get some issues.
And it's very hard to have the optimal solution in this case, right?
So, and, but for LDPC codes, in fact,
the method we use to decode them is to use
belief propagation as if it were a tree, because what we know is that
when the length of the LDPC codes goes to infinity,
then the ternograph tends to be a tree.
But, of course, we are not using infinite length codes
and the idea is that we use belief propagation as if
it were a tree, but, so it's so optimal,
but we design the code in such a way that the gap to optimality
is kept small, all right? Okay.
So, in this case, let's say that we don't
really need to run belief propagations on the loopy graph.
But there is another case which is polar codes.
So, very quickly, so polar codes is based on what is called the kernel.
So, suppose that you have, so the simplest kernel, which is the original one,
is called the Arikans kernel. So, Arikans is the inventor of
polar codes. And, in fact, it works in this way.
U1 and U2 are two bits, all right? And then
X2 equals U2, X1 equals U1 plus U2, okay? Then X1 and X2
are sent into the given channel that can introduce
errors, erasures, or can add Gaussian noise, for example.
And so, in fact, X1 and X2 will see
exactly the same channel, right? And the output of the channel will be
Y1 and Y2. But then the idea is that, in this case,
what U1 and U2, which are the information bits,
what these bits will see, I mean, which channels
will they see? Because, of course, X1 and X2 will see the same channel,
but U1 and U2 will not see the same channel.
And so, here is how it works. So, okay, this is matrix form.
And let's do a capacity analysis of what happens here.
Okay, suppose that I of X and Y is the mutual information between X and Y,
so it can be an index, because it's exactly the same channel we have here.
So, if we consider this vector channel now, we have the
mutual information is the sum, because it's these two channels are independent.
And so, this is I of X1, X2, and the vector Y, which is Y1, Y2.
Okay? Then I replace, in fact,
this, in fact, what I want to know is what happens
if you, instead of having X and X1, X2, you have U1, U2.
So, in fact, because U1 is X1, sorry, because in this case,
you can recover U1 by doing the addition of X1 and X2.
And then, once you have recovered U1, you can
say that U2 is just, can be recovered by the observation Y
and the knowledge of U1, then you get this kind of equality, right?
And what, in fact, what happens is that
this first term is smaller than the symmetric mutual information, right?
Which is the capacity in this case, if we consider symmetric channels.
And this will be larger, okay? And now, polar codes are formed by
considering the end time, sorry, the
Kronecker power of this kernel, of the Arikan kernel.
Okay? And in this case, what happens is that
for a given ratio of the bits that we have, the first term
will tend to zero and for the rest, it will tend to one.
And the ratio of bits for which the mutual information
here tends to one will exactly be the capacity of the channel if n goes to infinity.
All right? So, and this phenomenon is called the polarization, the channel
polarization. And this is why these codes are called
polar codes. All right? But then the problem is that
this Arikan kernel is polarizing but too slowly.
So, too slowly with respect to the length of the polar code.
So, the idea is that we want to find some better polarizing kernels.
Of course, their size will be bigger than
two, which is the case for the Arikan kernel. It will be of size L,
for given L. The problem is that if we have a general kernel, so a general
kernel will be, let's say, a square matrix of size L
and which is invertible in the finite field of size two.
Okay? And, okay, let's forget it. So, this and what we know is that kernels
with better scaling exponent exist if the length is
more than eight. And there is another exponent, which is the error exponent.
And these two exponents, in fact, characterize the way
the channel polarizes with respect to the length of the code.
And, for example, their kernels, okay, for example,
this kernel of length 16 is known to be the best
kernel in length 16 in terms of error exponent.
Okay? So, now, what we have to do is that if we want to
find the equations of decoding of a big kernel,
then, in fact, for each bit that is decoded,
it corresponds to a tanner graph, and this tanner graph may have
cycles. This is, for example, for length eight
and for decoding the third bit. This is the corresponding graph,
as you can see, there are cycles. And this is a very simple one.
If you have, for example, the kernel that I showed you,
the EBCH kernel of length 16 for decoding, for example, bits in the middle,
then the tanner graph will be really,
will be terrible. It's something that is very, very hard to decode.
Okay? So, and how to decode it is by running belief propagation
on this kind of graph, on the tanner graph. Okay? And
we cannot do the same as for LDPC codes. We cannot change the code
because this is not a code. This is the kernel that is fixed
and so we have to find a way. And what I was thinking even before
talking with Daniel was that belief propagation
is exactly this, local to global. So there
there should be some cohomology and so, and I was happy to learn that it's true.
So I will be happy to read what you have done on this topic.
All right. Now, okay, now we have identified a couple of problems.
I have also identified some publication
which applies sheets on the domains, on the areas that I have listed at the
beginning of my talk. Right? And I can show you
some, a couple of examples. First one,
sorry, before starting, we have in fact, as I told you,
to use topological spaces on which we can compute
things like homology groups. And in this case,
one good choice is to use simplicial complexes as topological space.
So for those who don't know what it is, so
what is a case simplex? Let's say, so this is the concrete point of view of
simplicial complex. And so this is, for example, zero
simplex is vertex, one simplex is edge, a triangle for two simplex, the
triadron for three simplex, etc. Okay? And a simplicial complex is a finite
set of simplices that satisfy these two properties. Okay?
So, okay, now in order to apply these,
to apply this, for example, to be able to compute
homology on these simplex complexes in a way that will be
efficient for our applications. In fact, this is a very restricted way
of defining pre-sheaf on this complex, but it is the one which
will be useful for us. Okay, so in fact, we define our
pre-sheaf on the category of vector spaces, or it can be modules.
And so we define, of course, restriction, which we call
this way, f of a, a, b, and a, b are two faces.
And of course, we have consistency of the composition.
And we have also to define a morphism between
pre-sheafs or sheafs. And so this is, it assigns for each face
a linear map which satisfies this equality. Okay?
And then we define a quantity which is, so, b, a,
and this quantity equals zero if a is not a face of b.
Otherwise, it equals plus or minus one, depending on the orientation.
And this quantity will be useful to compute
with what follows. And essentially, the co-boundaries,
etc. So this is our co-boundary, which is defined this way.
And so this is the the former sum over
all case-implex. And you see this appears. So this is the restriction map.
And sigma of a, sorry. Okay, let me go on.
So this, we have the classical, we have the classical
relations between the the the kernel of
decay and the so the cycles and the co-boundaries.
And because of that, of course, we have
we have a bk which is a subgroup of zk. And in this case,
the homology group, the case homology group,
will be defined in this way as the quotient group.
Okay. And in this case, we get a sequence
of linear maps from and of course this
in fact the idea is that only the elements of
in fact the idea of this homology group is that only
the elements of zk that are not already consistent
are worth mentioning. Okay. And now let's apply it to sampling theory.
So this example comes from this reference from Michael Robinson
and he proposes a shift interpretation of the sampling theorem.
Okay. So in fact, he starts with a shift of vector spaces
on an abstract simplicial complex. Okay. And then
he uses a shift morphism between two shifts.
So f which is a shift on on this simplicial complex
and s is in fact what is called a sampling
sheaf and which correspond to the the the the
the operation of of sampling and it is associated
to the to a sampling morphism from f to s
and then he defines an ambiguity sheaf a
in which simply the the stock in fact a of small a where small a is a face
is just the kernel of this map. All right.
And this is the sheaf theoretic sampling theorem that he derives.
It is that the global sections of f so of are identical with the global
section of s which means that we can reconstruct in fact
we can reconstruct the
the we can we can reconstruct the this
just from this if and only if in fact the case homology homology group
homology group related to the ambiguity sheaf is
zero for k equals zero and k equals one.
So and from there it can recover the classical Shannon Hartley sampling
theorem of of band limited the functions for example
or some other results coming from graph theory or
from a quantum graph if I remember well. Okay. So
in fact we can have in signal processing
we can by using sheafs we can generalize the results
that we had at the origin.
Second example network coding and it has been developed in this paper from
Greece and Iraq and in fact so very quickly they have
they have been the sheaf homologies in which case
H0 is equivalent to the information flows that we have in the network
and many practical problems may be solved by using
these homologies such as the mass flow bounds network
robustness and some other ones. Right.
And the final example persistent homology which I think will be very
valuable in our case if we want to learn the channel
to denoise what we measure so to denoise it
to remove the small variations in order to keep
only what is persistent and so it's still
simplicity complex. Okay so in this case
now we define an elementary key chain
in this way and of course we have two different orientations
and which are here represented by the sign
plus or minus and this is a key chain so in the case of persistent homology
what is used is essentially coefficients in z but it can be
an abelian group and yeah and then so we have these
we define these the boundary operator in this way
so so now of course we are in the reverse
direction compared to homology so we go from k to k-1
and the same way as for homology so we have
the k-th homology group now which is the quotient group of the kernel
so the cycles by the boundaries and we have also
betty numbers which are useful in persistent homology
which are defined as being the rank of these homology groups
okay so now coming from there what is persistent homology
so suppose we have a cloud of points in a space
all right and in our case it will be the measurements that we will do
and from there we need to find structures we need to remove noise we need
to remove the small variations the clutters etc and we need to find
some structure inside this cloud of points okay
so how to do it so what would be theoretically the most acceptable
would be to build a chess complex in this way so you draw some balls
around of given radius around the the points
and then you form a simplex s if there is at least
one common point in each ball of s all right the problem is that
computationally it's very hard i mean it's something
for which it will be hard to compute the homology groups
so instead of this what we do in practice is
rather to consider the rips complexes and in this case we form a simplex
if there is at least one common point in the balls which are considered now
pairwise okay so for example this one is a two simplex for the rips topology
but not for the chess one okay so and then we considered
a nested sequence of topological spaces
x0 x1 etc and this is done just by considering very
in our case varying radiuses ready from the smallest one
to the biggest one and in this case
we need for sorry we obtain for each of them
homology groups okay and then in fact what happens
is that we want to identify when a homology class
is born and when it dies all right and so in this case for example this one
is born here and dies here okay and the birth and death of the
homology classes will be very important because with this we can construct
barcodes and sorry barcodes and in this case
long bars will have a long life and it will mean that
long bars will be the persistent features
all right so for example if we use the rips complex
so this is so our cloud of points you see
they are more or less in in a torus i mean on the torus
or donut and when so this is the complexes we obtain
when the radius increases okay and for the same thing these are the barcodes
so these are the so on the x axis you have its parameterized by epsilon
so this is the homology groups a0 h1 and h2 as you can see
this one is persistent this one is persistent yeah
and so
sorry how long does it have to be uh this uh i don't know exactly
excellent question but uh it depends probably on the application
that yeah because uh it depends mainly on the application
okay and i found uh
something related to the topos of persistence
and that means that in fact it's a kind of generalization of
the topos of sets in which instead of having just
sets we have a sets indexed by time yeah so
these are given by this barcodes that
i have shown to you and it corresponds to time index sets
and with the on these time index sets in fact we can construct
a persistence hiding algebra of intervals ordered by inclusions
and the the sheaves of this algebra p encode barcodes
all right and sorry this is a very simple example
for example here so i time zero so these zero correspond to the time and
this one to the dimension of the homology
so here for example in this case as you can see
h0 of zero is essentially i mean xy quotient by zero so it is
as morphic to z plus z and the h1 of zero is just zero because you have only
two vertices so two points then at time one we have this
and we can compute h0 of one which is this and
in fact it has a morphic to z and each one
of one can be computed and we can see that it is isomorphic
to z as well etc we can compute the homology group of all these guys
and so by doing gluing in fact we can glue we may glue the point-wise
homology groups together consistently and
in order to extract global information so it's of course it's a toy example
and the global is formations this one for t
in in the interval zero one we have h0 equals to this
changing to h0 equals to z when t goes from one to five
and for the first homology group we have for
for t equals one to two we have h1 equals z changing to h1 equals zero when
t is in fact the union of these two intervals
and so this is a very simple example of one
topos so which is related to persistent homology
and which may be useful for us and that concludes my talk thank you very
much
in fact there are algorithms for computing of course they yeah
maybe they have to be simplified because in general it's very heavy
and but the idea now for example okay there are some ideas
now of combining persistent homology in fact the topological analysis of data
with the neural networks and for now i have just seen one paper which is not
very i mean it's not very
very easy to understand what they are doing so and that's it
but this will be next step because apparently
using topological data analysis plus machine learning seems to be
very efficient
are there questions
maybe one one
do this metal give an hint on what you told before that's a
this small dimensional yeah yes the idea yes the idea is to try to
characterize these many faults in fact thanks to the
homologi group
usually it's not really exactly the homologi group you come from
the other spectrum yeah yeah yeah yeah
this is not really psychological yes yes yes
anyone comment that i mean it was very interesting the the fact that
some people revisit the sampling theorem of shadow
topos my question is that did you see some some results on compressed
sensing in topos which could be also linked to the fact that maybe you
want to from a couple of samples try to find a global
representation of your information no i haven't seen you right now
but i haven't searched really in a to the direction so
okay if there are no other questions we'll take a 15 minutes break
and then we'll have a short discussion like half an hour
so come back at four and we'll do from four to four thirty
