Here we are
Is this all discussion about what is probability, what is randomness?
But I want to discuss it from some way different angle, so I start first, indicate kind of
club I don't go to, and what I'm not going to talk is philosophy and psychology of randomness,
because there is lots of, partly they made the mathematician, partly by philosophers,
discussing what is random and what is not random, which is, has no, is not focused on
any specific questions.
And here on the country I want to discuss some very concrete questions, both in mathematics
and outside mathematics in applications.
And so, but still first I start with few remarks on history, but these are the subjects which
we shall discuss.
So these are three blacks which I will not discuss, where probability, concept of randomness,
usually from my point of view, is just meaningless, and people discuss it and say random, like
history is random, evolution is random, it's just words, there is absolutely nothing but
an emotional weight attached to them from my point of view.
And because there is no specific mathematical model behind them.
So my point is, whatever you speak, random or whatever, you have to have in mind mathematical
model, very different from the traditional one, but it might be mathematics.
Any speculation, any speculative discussion without mathematics is, in my view, nonsensical.
It may be preparation for mathematics, but either you have experiments or you have mathematics.
Everything else is, I don't know what it is.
And so, the subject where, so, two points, three points, where mathematics is prominent
and which I will discuss, one is statistical mechanics, and very similar, but in a way,
simply in a way, more subtle thing is formal genetics, which are not so kind of common.
Then how probability applies outside of probability per se per square.
Probability is a subject I know very little about, but I know a little bit how it works
in other domains, like in geometry and in geometry and in, in, and in combinatorics.
And then there are applications where you have to change concepts, concept of, concept
of probability, apparently, and these are molecular evolution as opposed to classical
theory of evolution.
They kind of have similar words and they kind of apply to the same subject, one is philosophy
and another is science, and molecular evolution by now became true science.
Then there is statistical analysis of natural languages and learning mechanisms, specifically
how you learn languages and mathematics.
I'm not concerned about how you learn, you know, something trivial, which is a lot of
study, both mathematics at the end by psychologists, but this kind of subject, you know, anyway,
but then how you learn languages and also how you learn mathematics.
The answer must be mathematical.
It's not just words, it must be some specific algorithms, you have to indicate at least
direction how to build specific algorithms made on statistical analysis of the data.
And for that, of course, you have to understand mathematically what, what is a natural language.
And by the way, okay, we come to that in a second.
Now this is a kind of preparation, and now there are two questions which go along, what
is entropy and is there non-Shenin kind of information?
Because people speak about information in biology, right, these kind of flow of information
in cell, as they people say, there are flow of energy and flow of information, and this
really guides biologists, but mathematically we don't know what it is.
And it's very different from entropy of Shannon.
Of course, for that, you have to analyze more carefully what is entropy of Shannon.
And so what is entropy of Boissel?
And this we shall discuss in the beginning.
And now we look in a few words of history, and strangely enough, unlike many other domains
in science, it starts not with science, not with mathematics, but with gambling.
And it was, it can be traced as long.
And actually, these kind of cubes were found in Persia, I was that old, so 5,000 years.
And people have already understood, I think, more or less, what we understand now.
However, in modern time, this was kind of inscribed by Cardano, no, these people usually
for some people speak about Pascal, but he was one of many, and I don't think he was.
So Galileo, there is some writing of Galileo, which we'll publish later on, which indicates
he understood perfectly well everything that was done by either by Pascal or written later
by Huggins, but not probably the law of large numbers in full, in full generality.
But if he had already Cardano, he understood much more than you can believe, and actually
saying many things you would never imagine.
In particular, he was justifying psychology of gambling.
He was a gambler, and he was an incredible character, and he was a great gambler, and
he explained what's so good about gambling, psychologically, not only mathematics.
But this is a minor point, and then there's another point which is more scientific, and
which is not often realized, was coming, was coming, was coming, kind of, was so crucial
for probability, and so it's brown in motion, it's again amusing history, and so can you
guess who said and when was that said?
Democracy.
Democracy.
Democracy.
Democracy.
No, nobody's closer to the truth, yeah, than whatever we can say.
However, you know, this was, as you know, sometime ascribed, as if his name was brown,
who suddenly did not understand it.
And amazingly, that's Tito Kretzos who said it, who
understood it.
And he understood, in fact, many other things.
And what is, of course, interesting about him, he
was not, he used a picture, that he understood many other
things, but he was not really a scientist.
It was common knowledge at that time.
However, how could it be?
Because people argued about Brown if he could see actually
Brown in motion.
He, of course, was not the first to observe it, and his
contribution was kind of minuscule.
His name was, she easily remember, Brown in motion,
only because his name is him.
He was a very good microscopist.
He was a biologist, and he studied cell, and made some
discovery about cell.
But this was his hobby, and he observed, and well, everything
was written about him as wrong.
He was not doing what people say he was doing, and he has
that tangential relation to Brown in motion.
But what's essential about Brown in motion, that this was
the source of actual accepting atomic theory in the modern
time, and because it allowed analyzing that was needed for
measuring, for computing, with sufficient degree of
precision, the Avogadro number.
And who done that?
You know who was the man responsible for that?
And it was, you never heard of the name, of course, about
these names, yeah?
But these actually are ascribed to Einstein, and this is his
most cited paper.
It's not a relativity.
That was his major contribution to science,
according to citations.
Because it's cited everywhere, how you determine.
And then there were experiments made here, I keep
forgetting this French great experimentalist, who was a few
years later, made experiments using this equation,
Einstein-Smolkowski equation for computing Avogadro number.
But it's kind of ticking.
Nowadays, it's done quite differently, not like that.
So, but again, interesting, the mathematics were done
before Einstein 25 years later, and then usually
mathematicians called it to be in the process, though it was
50 years after it was actually done by Thiele.
The name of Bachelor also, you probably know the history.
He was forgotten when he came back, but he was not the first.
Apparently, it was Thiele.
Now on the internet, you can find many interesting things.
This I just find on the web.
Maybe there are all the sources, but this is all that I found.
And then, of course, these words, by the way, how these
Lucretians could do that?
Apparently, some people doubt if Brown, or people who, I
forget his name, who looked before him six years ago,
how they had enough ability in their microscope to see it.
Yeah, because it's on the boundary of what you can see
in a microscope, because you have to see particles of the size
about one micron.
And on them, you see a fact of exactly when many atoms
hit them simultaneously.
And that marginally can be seen in the microscope
with 1,000 microscope.
How did Lucretius come to this idea?
Because he couldn't see that.
But what could he see?
He saw, of course, particles of dust in the sunlight.
How did it move?
And he had nothing to do with Brownian motion.
It's just convection and the turbulence of the air.
But, however, he made this.
And that's very typical.
By the way, this is exactly like Darwin.
What he was saying was kind of right, but on the base of
Che Nonsense.
And for that, he's considered a great scientist, if you see.
Specifically, if he explains something, everything is wrong.
However, it's like Che Lucretius.
In principle, it's still right.
However, he is not.
Lucretius doesn't give him this famous Darwin.
So he understood evolution already, as well as Darwin
did, on the base, of course, of that.
That's another interesting point.
But he shall come, because his son was brought up.
What the hell is this?
Right.
That he articulated this.
And one point, which is certainly opposite to what we can
say from the other great thinkers, that it's not numbers
which are kind of essential.
Because in the 20th century, the end part of the
thematic was dominated by Rothending, and Rothending
certainly wouldn't accept numbers above two.
I think two was the greatest number he would ever
accept as a number.
But of course, the numbers are great, and it was OK.
And then a big step in conceptualization of
probability, you know what was that, was Bufon.
And again, historically, it's amazing, because Bufon, again,
as far as evolution, theory concerned, I think, was
understood much better.
Or at least, he was headed as earlier than Darwin.
But then, he's kind of stupidly ejected, starting from
Darwin himself.
And because I was recently looking at the history of
that, and it is completely distorted.
In particular, a problem with Bufon, he was saying, all
the right thing, but then immediately saying, no, no,
I don't believe it.
It doesn't really be the scripture.
And they say, oh, no, no, he didn't believe in
revolution.
He didn't believe into that.
He perfectly had a very clear idea, and not as detailed as
Darwin, because some geology was not quite ready.
And for him, it was more conjectural.
And because mathematician, he probably saw the flaws of
naive kind of selection theory, which Darwin didn't.
Darwin actually saw them.
But he still believed it's true, and he was right.
And then, the point of application of probability in
physics, and matching mathematics, depends on symmetry.
It doesn't depend on kind of conception of chance, whatever.
And that, I will explain in particular examples later on.
So mathematically, introducing probability is
modifying the symmetries.
You start like with permutation group.
You have equal points.
And then, you're linearized in the full linear group, or
orthogonal group.
And then, mathematics become more interesting, and you can
use it more efficiently.
And one of the big discoveries in probability in the recent
years was this is a Schramm-Leuven evolution equation
when it contains two crucial kind of ingredient of
characteristic of probability.
One is high symmetry.
And secondly, what you can see, the random objects are not
just random and specified sense.
They're parametrized by independent variables.
And in this particular case, they're parametrized by
Brownian processes, which are kind of, in a way, maximum
independently possible for
compatibility discontinuation.
And so the Bufan, again, he was throwing this needle, and
actually, again, there is a folklore story, he was
actually experimenting with that, he was throwing a
baguette on the floor somewhere and see how
they were positioned.
And that was a crucial point, the probability, because
before that, it was as a discrete.
And there were computation kind of kindergarten
computation done by great people, like Galileo or Pascal.
But they were kind of kindergarten, in a way,
because they were not continuity.
It was just counting numbers.
And here, it was integral formula.
It was hard measure.
It entered.
And then this point of view was taken over in an abstract
context by Kolmogorov, and he said, aha, that's the only
probability which exists.
So any probability can be modeled in the following way.
So I want to give an overall scheme what probability is in
order to move away from it.
And so that's the logic of classical probability.
You can say some measure space, universal measure space.
I say square, interval is too small, it can be too much.
Events modeled by measurable subsets, whatever measurable
mean.
Nowadays, we know measurable is a questionable concept,
because it depends on axiomatic.
So it is measurable in any way, that's subsets.
And its area is probability.
However, if you look from, again, perspective of algebraic
geometry, she is a fault of that, because it's exactly how
Andre Weil was formalizing algebraic geometry.
And we know it was dismissed in a decade, less than that,
by a growth and decay approach.
And the point is, in algebraic geometry, it was
coming universal field.
You introduce universal field absolutely at Hock, having
nothing to do with object you deal with.
And it's kind of convenient the main to do it.
And again, my experience, when I was learning it, I took it
also so nice in nature.
And when you do what growth and decay does, it looks absurd.
But it's exactly because growth and decay was really
seeing better than students, and then Andre Weil.
It was really the right way.
And this probability, what Kolmogorov done, in my view,
is like this naive application of set theory.
By the time it was done, it was 78 years ago, it was OK.
Nowadays, it looks still very naive.
And you need to change it.
And in decay, I don't know how.
It's easy to criticize, it's hard to make right thing.
You see faults, it doesn't mean you know how to correct them.
So I indicate possibility of correction, but certainly
they hardly will be of the level of growth and decay
approach.
And then, of course, there is quite a different problem.
So one, just within mathematics, and just even in the
context of statistical mechanics, if you look carefully,
you don't honestly use probability the way it described
by measure 30 by Kolmogorov.
It just doesn't work.
You always make some little turns here and there, and just
pretend you use it.
And then you start making computation.
And then how you make computation doesn't depend on
your background up to your point.
But from some moment on, it may depend.
And then, in other domains, I mentioned them like languages.
And then there is this statement by Chomsky.
He was a very powerful character.
And he shaped, up to a large extent, modern beyond
languages.
And mathematicians sometimes say that a language is just a
measure on the set of words.
And this, again, I think, is a super naive point of view.
In many respects, it's wrong.
It's wrong mathematically, because language
doesn't make a set.
Actually, Chomsky and this guy also say a very stupid
thing.
For example, they say, there are infinitely, or you can
make infinitely many sentences in a language.
And that, again, is a completely absurd mixture.
And this is a sense of point I really
discuss of this.
At some point, you mix model and you mix the object.
Because the total amount of sentences we ever can make for
the time of the universe, maybe 10 to the 20.
It's much less, of course.
10 to 15, maybe, or 10 to 16.
We don't know time.
The universe is very small.
Time is very short.
Infinity is not infinity.
It's a very small number.
And it's essential for these structures, to understand them.
This kind of structure, this also applies to what I was
mentioning before, biology, like molecular evolution.
The scale is essential.
You have to, numbers are essential, but they are
finite number.
And all your means must be adapted to them.
In physics, sometimes, you can pretend numbers are infinite.
And for good reason, because the groups are
seemingly very high, not because the
numbers are idealized.
Because the point why you can say is meaningless.
So there are many ways to justify it.
And this is one is that probability is very small.
When you have a certain sentence, its probability will
become 10 to the minus 20, or minus 15.
And it's a very small number.
We have a sentence of the length of a dozen words.
Probability of this actually appearing will be 10 minus 12.
And this is a small number.
But in physics, you have much smaller number in the stitch
mechanical particles.
You have 10 to the 26 particles.
And each of them may be in the same two states.
So you have 2 to the power 10 to the 26.
This 1 divided by this number.
So maybe I'll write it down.
It's a very small number.
We should say how this probability is 1 divided by 2,
at least 2, to the power 10 to the power 26.
And this probability of a particular state
of a system of particles in this room,
it will be probably maybe 29.
So the state of this room, particular state,
has this probability.
Of course, it's nonsense.
But however, you can use statistical mechanics.
And interesting point is, though this number makes no sense.
You have this state.
I don't know what it is.
It is this state.
And the probability makes no sense.
But the quality of this probability still makes sense.
And that's point in mathematics.
Objects, numbers, and I mean English, it's OK.
Their relation must be correct.
You have to know how to play it with them.
And then you go on and on and on.
But there's a starting point.
There is symmetry in between the particles.
Because all particles are essentially identical.
Therefore, you can speak about this identity
without knowing what the subjects are.
And this will be a kind of essential part
in many applications of probability,
what I will be discussing at the end of my lectures.
So these are two new ingredients which
come to probability from 20th century, which
we're not available to call Magorov.
And one of them is categorithoretic language.
And secondly, non-standard analysis.
And actually, both of them are very close to what
Bolshev was saying and thinking.
If you read Bolshev and translate it to modern language,
this is what you see.
People were translating him, of course,
in the 19th century and the beginning of the 20th century
to the language available to them.
And sometimes saying, well, he was saying not quite right.
You can't do that mathematically.
It doesn't fit up to anything.
Because mathematics was not ready.
That kind of, now in mathematics, is better adapted.
And there may be something else.
In particular, the concept of the entropy
is a definition of Boltzmann, which had in mind, I guess,
of entropy in what you can find in elementary physics textbooks.
Entropy is log of the number of states.
And just decide for that.
It's not writing formula, right?
So when you write the formula, mathematicians
make kind of double mistakes.
So you write the entropy is minus some pi log pi,
and kind of very proud, giving the definition.
In my view, this is a psychological phenomenon.
The ordinary people who have never heard of the entropy.
And you just say, huh, that's the formula.
Why?
Because I'm smart, I know the formula.
No, it's a bullshit.
It's not a definition of entropy.
It was computational formula invented by Boltzmann.
And it's extremely useful formula, but it's not definition.
Definition is it's the number, log of the number of states.
And how to go from this to that, you
cannot do it unless, from my understanding,
unless you pass to more sophisticated language.
And for this, and I explain it today,
you have to take a categorical point of view.
And in physics, it's exactly kind of a categorical point of view.
It's everywhere present in physical reasoning,
in naive physical reasoning.
But it was translated automatically.
Archive mathematical language, it
become kind of fossilized, it become kind of stereotype.
The formula is a great formula.
But it's actually, you know who wrote this formula first?
It was not Boltzmann, it was Max Planck.
There was nice exchange between Max Planck and Boltzmann.
Max Planck wrote this formula.
And Boltzmann suggested discreteness of the energy.
Quantization of energy was the idea of Boltzmann.
Yeah, it's interesting, because Boltzmann
was obsessed with the idea of quantizing the world,
of having discrete atoms.
And also, he believed energy was discrete.
But he, he, he's, this is, actually you can find this
even on Wikipedia, it's the kind of standard knowledge
nowadays, where you have access to knowledge
which you didn't have before.
And so, alternatives, as we shall discuss, are the following.
So one is growth and dig type of describing entropy.
In this, I can be a part of mathematics
and understand what I'm going to explain today.
Secondly, along that, you define probability spaces
also in the spirit of growth and dig.
Because you see, the problem is that every time
in a class, a traditional application of probability
to turbulence, whatever, you say, ah, these are where,
over you, there's some probability space.
And you do all this probability space,
which is rather nonsensical.
Exactly as Weil was doing his algebraic geometry.
You read this field, and you take points
on this universal field.
However, nowadays, the concept of functor of points,
it's a function.
You take domain, and depending on domain,
you have your points.
And the same in probability.
Depending on domain, you have your point.
And this means you can see the functor
from some simple category to category of sets.
And once you do that, measure 13, my view
become extremely transparent.
There's nothing to prove.
You just always back integral, tata, tata,
become tautology.
Because categorical language immediately tells you
what you have to check.
And checking is usually trivial.
But it gives you structure extremely nice and simple.
And I will explain that.
And then another point is, which I understand less,
is large deviation.
What is the right setting for large deviation?
How it goes, why, and understand analysis and geometry.
Some of you can't explain.
Some of you don't quite understand,
both in classical and quantum of von Neumann entropy.
Because one, we shall see understanding this formula.
Well, there are two ways to think of this,
two ways to come with this formula.
And one of them immediately brings in von Neumann entropy.
And this suggests different, again, linearization.
And again, from my point of view, that's
because it solves some mathematical questions,
though it looks absurd physically.
When you replace measures by something
linear, like some homology or other co-homology.
And another subject, when you apply that,
try to apply classical kind of probability to something
unruly, such as languages, learning,
or even molecular biology.
You see that just you cannot assign numbers as probabilities.
Something else.
There are kind of numbers, but not quite numbers.
And that's the last, whatever.
So what I did now is cut and paste, very easy.
I make this spacing from some of my articles,
which you can find on my website.
And that's the names, where there are more details.
And now I want to go to entropy before making.
So again, I like quotation, because they give some perspective.
Because whenever you want to say something,
you check and you see it already has been said.
And of course, as you know, one of the point of modern science
is that you cannot understand nature by pure thought.
And I understand exactly arguing with this point,
that reality can be grasped by pure thought.
And up to a point, he was right, but then he
happened to be wrong, because quantum mechanics
didn't work.
And then, again, quantum mechanics
aren't clear what reality is.
And that's, again, a very interesting point.
And so how to make, you can make mathematical model
of reality, but what the hell is reality.
And then the last great man who said something,
was Alexander Rothenrich, said, in order to understand reality,
you have to understand what zero is, so to speak.
And let's try to go from zero level
and try to understand entropy.
So first, this is a physical language.
We want to translate to the Rothenrich kind of language
what is entropy.
And so just say what physicists say.
So they say, we have a system.
Whatever it is, a mathematician will say, oh, it is a set.
All states are at out.
Why set?
Set is just language.
And when we can't, for certain purpose,
it's not very flexible language, but it's still
not the only language.
It doesn't mean that it's a real state.
When I was saying about that, each state
having this probability, of course, they're not real states.
These numbers, even physical real numbers,
they don't have physical meaning.
It's just up to your point.
You play with them, but they're not the real number,
but they're not physical reality.
And actually, there is no such thing
for physical reality.
And so that's how we can, this is a kind of physical preparation
of that.
So you don't have this physical state.
What physicists do, they make experiments.
They have this, I prefer to speak about crystal,
but maybe a continuous system.
And they make some measurements.
And the measurement is described by a protocol
of how measurement is made.
And this, by the way, is typical for all experiments
which usually disappear when mathematicians look at that.
It's a protocol which is being used.
But the additional thing is maybe to make sure
you make a protocol that you don't know the result.
Because mathematically, description
of the result of experiments following a protocol
described in the language of two categories.
And of course, people who do applied mathematics
don't like two categories.
But this piece is automatically used in this language.
That's another point.
That categorical language is the most, or even
two categorical language, by far more primitive and simple
than usual language mathematicians.
The traditional language developed by calculus, whatever,
it's super sophisticated thing.
For example, if you want to study basic property of entropy,
for example, concavity or convexity of this function,
then it depends on knowing what is the derivative of log.
But if you look categorically, the only mean
is if you use the right definition.
It's just the only thing we have to say
that it is kind of natural definition, functorial.
And then all properties, including basic property
of logarithm, would follow.
That's kind of amazing.
You don't have to make computations.
And this, again, in the spirit of Grottendijk,
you don't make computations.
Emphasis of what Max Ver was saying,
you just make computations, and the physics
and quantum mechanics say you make computations
you don't think in philosophical terms.
But I think you have to think in mathematical terms.
And this is what's happening, actually,
in developing the physics.
People study physics, now think as mathematicians.
But on a much higher level than the one which I will
catch you doing.
And so there are this machine, some equipment.
You make some measurement.
And so what you observe, either something,
in a way, something happen or doesn't happen.
Something blinking on your screen or not blinking on,
and just you count how many things happening.
And from that, you want to define entropy
of some incomprehensible thing like a crystal, which
has no states whatsoever in the sense of kind of physical
point of view.
So again, so one thing which you want to avoid saying
is that physical system of so many atoms
may have some space of states having so many states,
even in the discrete sense, whether they are kind of black
or white.
And there is a lot of kind of confusion with that even.
Just one typical discussion, even in this context, now,
quantum, you know this paradox of Schrodinger-Kett.
If Schrodinger-Kett being kind of secretly poisoned,
and you don't know that if he is in the state of being dead
or in the state of being alive.
And this is exactly kind of also confusing,
because there is no such thing as state.
State is a mathematical word to describe in things,
something, and the description is not adequate, even
in very simple situation, not to speak
about quantum mechanics, of course.
However, how to speak about that?
And now let me now explain that.
And this will be simple mathematics.
Maybe I go one step ahead of myself, so one and two.
So this sum, so this one is functorial,
which I'm going to describe.
Functorial description of that, and the sum
is analytic, but analytic in a different spirit.
Because this, I explained it, but of course, this part,
I don't understand well.
I don't know it well.
Not well, I don't know it at all.
It's the phenomenon that many physical, much of physics
on a high sophisticated level, depends
on taking certain particular integrals.
It's kind of you integrate something
because it complicates or you break expression,
and they come out of that.
And by now, there is very developed theory of that integrals.
And that's an instance of that.
So this function is some kind of remarkable integral.
And from a certain point of view,
so this PI numerically are numbers,
such that the sum equals 1, and therefore, it's a function
on a high dimensional simplex.
So if you have this, I belong to index set i.
So I have two euclidean states, r to the power i.
This again, by the way, I use set theoretic language
as a shoot.
And again, I insist on that, because if you don't do it,
you immediately run into MS.
I don't know, for example, what this
means when n is a number.
This notation is actually incorrect.
And the people, you do that, what the hell it means?
What is n?
n is not a number.
Here, n is shot.
Usually, it means this set.
But there is typically a non-numeration by numbers.
You pull traditionally, you put these numbers everywhere,
but there is no numbers.
Set theoretically, you can power one set,
and then power another set.
And that's a perfect definition.
Why I insist on that?
Because this is preserved symmetry.
It's functorial.
And this is not.
It's completely different category.
Here is category of sets of i's.
Here, the best you can say is category of order set.
It's OK, but it's wrong category.
It's exactly wrong in physics.
You have so many particles.
They are not enumerated.
You have this particle.
So so many particles.
It's not even set, truly.
But even if you accept this set, it's not enumerated.
Because the number of enumeration is this number.
This factorial.
It's even bigger than this number.
So you arbitrarily take something from this number.
So introduce a structure by overriding everything you do.
And then it's produced a tremendous mass in a description.
So the point of categorical language
is really much simpler, much shorter.
And I shall explain later on what the advance to that
say in the case of that.
So but what's an energetic point about integrals?
They're called period integrals, sometimes.
So I'm saying that this function is the simplest possible
function in the n-simplex.
In what sense is the simplest?
Of course, it having maximal possible symmetry.
Of course, the maximal possible symmetry
will be called zero function or constant function.
So this we reject.
Now analytically, so given a function on a simplex,
you want to characterize this from the point of your
analytically based derivatives.
But certainly gradient is a gradient.
You can't see much about it.
What you can see first, one kind of some function shows
its features is when taking the Hessian.
So it's a table.
I want to say I wanted to say a matrix.
It's another by the way, quite an embarrassing thing
in mathematics.
What's a matrix?
What is a matrix?
Mathematicians, you're a mathematician.
So give them a mathematical definition with a matrix.
Can anybody give me a definition of a matrix?
It's a function.
It's a function.
Function where?
From the set of n times n elements in the past.
It's a function, but n, no, no.
So it's a function on set.
n is a relevant here, right?
There is no n here.
This is a function on this set.
So of course, it's not a matrix.
And the Hessian is not a matrix.
Hessian is a quadratic form.
Because it doesn't depend on n.
It just says that for any pair of vectors,
you take this derivative and this derivative
and you have a result, and this will be quadratic.
It happens to be written.
But as for matrices, of course, yeah.
What you say is OK.
But it's not quite true, because very often
matrixing like that is unspecified entries.
When you don't say what's in this entry.
You say a matrix with some entries,
you don't know what the entries are.
They are not function in a particular domain.
So that's tricky point.
So in a specific context, you can
say what you mean by a matrix.
But generally, in a matrix, you confuse mathematics
intrinsically with the way you write it on the blackboard.
And everywhere, by the way, in mathematics,
it looks a joke, what I'm saying.
But you cannot, for example, explain that.
That these two implies, do you know that this is unprovable?
It makes no sense in mathematics,
unless you write on the blackboard.
Because you know by blackboard which order they are written.
If you don't have it, if you have no a priori order
in your head, coming either from blackboard
or the temporal order, you cannot make sense of the sentence.
And that, as we shall see maybe in the end,
on my lecture, is essential of trying
to model learning theory, how we learn things.
And as you know, small children don't just
have no order in their head, right?
They have great difficulty in distinguishing them.
And all mathematicians make mistake, reversing inequalities.
You know there is inequality in the right and between sense.
And it's not accidental.
Because this is an artifact of mathematical notations.
And it's not in our head and not internally
in the mathematical structures.
So that's another point.
And again, it may look a job, but when
I come to the end of my lecture, I see that without understanding
that you cannot understand the learning mechanisms.
And as you know, for 60 years, more or less,
there was tremendous failure with so-called artificial
intelligence.
We're making claim after claim that absolute is no progress.
There is a lot of progress, of course, in hardware,
in sophisticated software, but nothing
kind of close to what was expected to be done.
You cannot make any simple intelligent program.
They're all expert programs.
You have exactly say what to do.
And this one, the reason, because we have a very wrong idea
about how we think.
And the first fundamental mistake,
we think that we think.
OK, now, so what's about the Hessian?
So Hessian is a quadratic form.
And then, what will be the simplest quadratic form?
And think about quadratic form as a metric.
It's a Riemannian metric.
What is the simplest Riemannian metric
you can imagine compatible with the simplex?
One of them, of course, will be just the one you see.
But another one, if you think about the simplex
as a part of a sphere, so a spherical simplex.
And then you have a symmetric of constant curvature.
So 0, you cannot have.
It will be constant function.
But this is the next potentially simpler thing.
The fact you can represent metric of that kind
as a Hessian of some function a priori
seems absolutely unlikely.
Because, I mean, this is the number of possibilities
for function, the number of quadratic forms.
Quadratic forms depends on n, n plus 1, n minus 1.
I keep forgetting.
But square number of variables for n variable.
And here, we have only one function.
So a variable likely you can hit some target like that.
However, entropy does that.
So entropy, this is miracle.
And that, by the way, another point, of course,
about mathematics.
When you play mathematics in this context,
it's always dependent on miracles.
It's not illogical science, unlike everything else.
Against any common sense, there shouldn't be such function.
If you ask a priori, is the function separate?
Of course, it shouldn't be there.
How are we there?
And this entropy.
And intrinsic symmetry of entropy
is orthogonal group, or rather, Lie algebra
of infinitesimal motion of the sphere.
And this was automatically transplanted to quantum world.
So this formula, written by Planck,
is contained inside the germ of quantum mechanics, which
rather amazing.
I don't understand why.
Of course, there are some.
Now, there is geometric quantization.
There are times and fees to say they understand it.
We shall see there are lots of questions mathematically.
We don't have answers to.
So there are two aspects of entropy.
And now I make a little break.
And so on the next lecture, I explain
how you define entropy in the style
of growth and dick kind of categorical language
without ever mentioning any numbers.
Almost without mentioning any numbers.
But on the other hand, it will be kind of understandable,
in my view, in principle, to a child.
Unluck, you don't have to differentiate.
You don't have to know what logarithm is.
There's nothing.
You just have to know what's the terms, I suppose,
what water is.
OK, so let's make 10 minutes break.
OK, so I want to take now this categorical language.
And I want to understand this sum.
And so it's about these numbers.
And these numbers are weights of something.
They are masses or masses.
And so I imagine this was not necessary for definition,
but the picture, they are not numbers.
They are drops of water.
And total amount of water is fixed and called one.
It's just called one.
It's just the same amount everywhere.
And then what you can do with this,
kind of just everywhere you can do that,
you can bring these drops together
and have one bigger drop.
And the others may not change.
Or you can simultaneously bring two of them together.
It comes slightly bigger.
And that looks rather innocuous.
However, mathematically what you have,
we have this PI, collection of these atoms, drops,
which have weights denoted like that.
And these are numbers.
But from the mental objects, these drops of water
are not numbers.
However, you can present them by numbers.
And the point I'm making is, because in some other cases,
they are not numbers.
They happen to be numbers here.
And you have morphism in this category
and what this is called reductions.
But there may be a good word.
But maybe before I said it, you may
have questions concerning my first lecture.
I don't think there were questions.
Yeah, it was just general talk.
Now you may have questions.
And so you have the spaces P. So these are probability,
finite probability spaces.
And this what they are, but again,
categorically, up to some moment,
you don't care what they are.
What you know is that there are errors between these objects.
So there are objects that are finite probability spaces.
Actually, they don't have to be finite.
They may be countable, but that's too complicated.
And there are these errors between them.
And they are exactly this process.
So each of them given by set of these PIs.
And so what you do, some of them can merge together.
And the weight you assign when they merge, there are some.
So there is object with weights, and then there is errors.
So numbers at the tivity is already encoded here.
So the point of category theory, you already
is in the rule of this.
In this arrow, you have this.
You have this arithmetic operation
expressing language of these arrows.
And this, of course, category was good about them.
That's universal language.
And universality is kind of the source of all signs
in mathematics included, including probability theory.
Now, of course, they are very special category.
One can say, huh, it just means that this space P
is kind of greater than Q. That Q is a reduction of P.
And instead of saying there are some more,
if you remember what his name, you can write like that.
So what is advantage of categories?
There are many of them, but one of them kind of evident.
And partly has something to do with this problem,
how we distinguish this sign from this sign, in your mind.
Indistinguishable, yeah?
So there is such a conception as relative entropy.
And relative entropy applied to the pair of spaces,
satisfying this equation.
So say, well, you read this order, you read this arrow.
And then you have to write something like that.
Entropy of P, comma, Q. Some people, P, H,
should have a letter, I write entropy, yeah?
Because you need that one letter of only
when you start doing complicated computations.
But we shall never make any computation.
Everything come by itself.
So I can have this notation, which is to remember.
And so you have here three symbols, P, comma, Q.
And besides, you never know how you can tell this from this.
Actually, when this relative entropy,
I mean, myself, I always lost.
Who is who, right?
Unless you write this arrow.
However, if you use this notation,
you just say entropy of F. And you have only one symbol.
Moreover, you know, categorically,
when you have something defined on objects,
how automatically it passes to morphisms.
And this typical category, you don't have to think.
You don't have to define this relative category.
It comes to you by the language of category theory.
So once you have idea of absolute entropy,
automatically you have relative entropy
with all its properties.
Sometimes you have to prove it.
That's another thing about categories.
Not that they give you the proofs,
but they tell you what to prove.
And then you may not prove it.
And then, notationally, so you reduce three symbols to one.
And that's not so bad.
So your paper may become three times shorter.
Because it carries lots of junk, usually notationally.
And this happens for most mathematical exposition,
for some reason, hard to explain.
Most of what we write in the paper
is junk, completely material.
It's just brought there arbitrarily
because you don't know how to say it well.
We cannot express our ideas well.
The best we have, category theory language,
not perfect, but still certainly much better than set theory,
infinitely better than analytical English, of course.
Which is just too horrible.
Not really understandable in some very very specific
and unclear what kind of sense.
That's again the issue we shall come up later on.
What is learning and what is communication
of mathematics?
How it happens?
And it has led little to do with logic, of course.
But some should do with probability properly understood.
OK.
So after this little kind of propaganda,
so you have this category, call this P.
I don't know how to go.
This is the category of where objects, probability space,
and morphisms are deduction.
What it corresponds physically, by the way.
Immediately, very nice to have this various pictures in mind.
One is when you have the drops of water
and bring them together.
Another, of course, more, the virtual deduction
has the following kind of point of view.
So you have this final probability
to spray some physical machine.
It's a physical system.
I don't know what system is, but the system
exactly to avoid saying it's a set or whatever.
It's something you observe and you see some flashes of light
coming there.
And you count them.
And there are, inside, there are, finally, many windows.
I think, divided into windows.
These windows indexed by some set I.
And there are frequencies of something happening there.
And you normalize them, and you have this bunch of numbers.
But then you can put some filter and just attach to this.
Not filter, but some other thing,
which will depends only on what happens here.
And then you have another number of windows,
maybe bigger, maybe smaller.
I don't know.
Actually, the one which is bigger,
they will not blink at all.
But you have kind of small thing,
and they're blinking here determined by those.
And that's how you think about this deduction.
And later on, we shall see how with that,
you can define infinite measure systems
and how you can define basic concept of probability theory
and actually automatically prove them, sometimes.
Once you find them, usually proving proves very easy.
OK.
So that's your category.
And now I want to say what is entropy.
Now, a category has nothing, a priority, nothing
to do with these numbers.
It's just abstract thing.
It's just, again, not so obvious what category is
and what is the, what definitions tells you.
Because you can say, huh, it's kind of a graph.
So there are points, and there are these arrows.
And so it's some particular graph with arrows.
But then there is, of course, extra point
that you have a rule of composition of arrows.
There are certain distinguished triangles.
And these distinguished triangles
say this arrow, composition of these arrows.
And also, there are distinguished arrows,
distinguished loops, which are called identity morphisms.
And again, it may look stupid.
Why not just to say anything?
But it's crucial for having right structures.
If you describe something that doesn't quite fit in this setting,
something you do is wrong.
Of course, sometimes category 30 is insufficient.
But mostly, you are still doing something wrong.
It's amazingly, amazingly adequate language
for mathematics and unclear why.
It works so well.
So, but the point is, it's not a graph.
It's not like that.
It's something you do this, what you do with this,
very different from what you do with graphs.
Sometimes you do something similar to what you do with graphs.
It's somewhat different.
And it's hard to say exactly what it is.
So from that point of view, you cannot give definition
of categories.
And there's another general principle.
If you try to define some general concept in mathematics,
you usually say something stupid.
Because they are undefined.
It seems to me.
I never saw any mean for definition
of function of category, even, of a set.
There are kind of limited definition.
But what is the language, whatever.
You cannot define it.
But you can live without it.
And that's, again, in this point,
I just said several times I was bringing it.
When you say about function, the function variable,
then for each x, you have y, and these are real numbers.
And this is a, it's nonsensical.
It's kind of a, it's the only function of this definition
to bully the audience.
We never heard of that.
Because the audience, everybody knows,
everybody understands that these are two functions.
Here is zero, here is one.
But you say, I know it's one function.
You can say it, but it's stupid.
It's two different functions.
And so we don't know what functions.
And this kind of, maybe, if you give a big audience,
you admit you don't know it, well, so a professor
is supposed to know it.
But I think it must be realized that science,
and mathematics, in particular, it's not so much of how much
you know, but how much you don't know.
Science is different from a person who doesn't know anything,
understand nothing.
And the common people understand so well.
That's the point.
You actually find them at this point.
You always live in the state of full non-understanding
of anything you meet.
And that's OK.
This is how we can make the next step.
Otherwise, you're blocked.
If you understand it, you don't move.
It's a non-equilibrium state.
Anyway, have this category.
And now we want to define entropy.
And it's supposed to be a number, but it shouldn't be a
number.
If you're in category, there are wide numbers.
And so there is the following general construction
applicable to any kind of category.
You, the growth index, can call growth index group, or
rather growth index semi-group.
And the most simple, the simplest thing you can do, it's
kind of describing a group where this morphine f server
generators, but I take classes of them.
And the basic relation is that if composition of these
equals h, then it implies that f plus g equals h.
So you can see that groups generated by symbols
corresponding to arrows.
And you say to read this, we read this single relation.
And now I say some words.
And when I decipher them, first I explain how they fit
physics and how they fit mathematics.
And so what happened from that?
And now the point is that, because what I say, what I
will say now will be not literally true when I explain
that, but it's true in spirit, I'm saying is that if you
apply growth index semi-group, say the growth index
semi-group of our category of finite measure spaces, then
this is canonical isomorphic.
And this might be corrected.
Yeah, it might be.
Make more precise.
To the multiplicative semi-group of positive numbers
greater or equal to 1.
And this is a theorem.
And then why numbers enter?
When you take log of this number, you have entropy.
And this log of this, so you take, so it applies already
to morphs, not necessarily to objects.
When you have an object, you have this distinguished
element when all drops come together, this particular
arrow.
So when you apply it to object, you apply it to this arrow.
So it's immediately defined for arrows, not only for
objects.
So immediately defined real entropy.
It's much easier than absolute in this context.
And then there's a theorem saying that this semi-group,
it's a billion semi-group, it's canonical isomorphic to the
group of real numbers greater than 1.
And you take log.
And log is justified by the second property of this.
Because formulas become kind of miraculously well.
If you don't understand why, it might be like that.
The deep mathematical reason for me is completely unclear.
And that's the theorem.
And you know who proved the theorem, and when?
Who proved the theorem?
I never heard of this guy.
And it was Jacob Bernoulli.
And this is called the law of large numbers.
So if you properly interpret the law of large numbers, you
arrive at this conclusion except one point.
Except what point?
It must be topological growth in the group.
Now the geometry of analysis enters.
The categories you work with are topological categories.
Besides arrows, there is a concept of two spaces or two
morphisms being close.
And this need explanation.
So you have to use the right topology.
You have to use the right topology.
And the way you use kind of, I cannot tell, either the weakest
or the strongest topology, where it makes sense.
The one which is the hardest to get.
I never know where the weakest are hardest.
Yeah, this exactly is the problem with the science.
I just never know.
And then from that, many kind of things follow that.
So what is behind it?
And so to get some feeling of topology, I have to conceive
an example which, for me, is kind of the source of an
understanding of that.
So to get some pictures, some ideas of this category and of
that, let me look at one particular class of examples
when you have spaces p of the type pi equals to pj.
Well, all n's are equal.
And so what happens to our category?
And so what is kind of the point of the theorem?
Now, so what will be arrows here?
So this will be just a bunch of numbers.
And say you have I, now I use a height in this.
There was notation already, and I don't want to invent you.
You have n numbers, and then you go to another one, to a q,
when you have m numbers, all equal.
So what will be such an error?
It means that number n will be composed as m times another
integer.
So if you restrict this category to atoms of equal weight,
what you have, just the multiplicative of the
composition of numbers, of integers.
So this category, on one hand, you see I was adding numbers.
On the other hand, here you see there is multiplication built
in, so the whole arithmetic is in this category, which is
kind of rather powerful.
It says that you don't lose what you have.
It's always there.
And so, and then this kind of, and then if you look at this,
what this kind of semi-group will be here,
it will be just all multiplicative rational numbers,
greater than one.
Because here's the integers, and then you normalize them
by the common denominator, and then what you have is just
whole rational numbers.
But if you do it here, if you do it for, in general,
you have kind of huge, kind of uncountable,
some horrible space if you don't put topology in.
Because if you put here different irrational numbers,
which are linearly independent, all in growth in the group,
they will be different.
They will know, there's no relation between them.
They have to somehow bring them to glue thing together
also by topology.
And now I want to bring a geometric example, which
clarifies what happens, because this kind of,
so far, abstract.
Maybe just, first of all, maybe before doing that,
I explain what the law of large numbers has to do.
Which was proven by Jacob Bernoulli in 170 something.
And he actually tried to prove it for about 20 years
before he proved it.
And I don't know, actually, what was his visioning, yeah?
It has already been conjectured by Cardano, who
conjectures the law of large numbers.
I don't know how Bernoulli proven it.
I think today's standard proof using Pythagorean theorem.
Of course, you know, it follows from Pythagorean theorem,
like many other things.
One of the greatest, Pythagorean theorem,
kind of one of the greatest theorem.
And law of large numbers also not so bad in the direct
corollary of Pythagorean theorem.
And of course, you know why it's Pythagorean theorem, right?
OK, maybe I'll prove it to this Pythagorean theorem,
of course, follows from that.
But plus modern notation, of course.
And so how you get it?
So what is the, what is here?
And then you also will understand
what is topology involved.
So if you live in this category of finite measure spaces,
we observe, at least, the following cooperation.
Two spaces can multiply.
They can take product of two spaces.
And this is very simple.
If this was built out of Pi's and this out of QJ,
this will be built of Pi times QJ.
And because the point is your number,
total sum 1, it's kind of perfect product.
And here you kind of, you can think about this, of course,
as a matrices.
You have Pi's here, QI there, and you take this product here
and there.
And then once you can do that, you
can write, which is kind of wrong, P to the power n.
Meaning P multiplied by P multiplied by P n times.
It's wrong because it might be set and they put number.
But very often on the cardinality of set metals.
So in truth, you write here number rather than cardinality
of this number.
And again, we said, analyzing it deeply,
you see you can't write the number.
And it is, you need this set because transformation,
operation with sets implies certain operation of these powers.
And I actually arrived at this kind of way of thinking
rather recently when I was solving very specific questions
when you were completely lost if you put the number.
But I write the number.
That was the list.
So meaning you multiply it by itself many times.
You see what's wrong with that because I use notation on the blackboard.
I write on the blackboard.
If I have no right, I cannot write in the sequence.
And that's what's wrong with that.
We want to have our mathematical description of mathematics
free of a blackboard.
And it's impossible.
No, not joke about it.
It's impossible.
Literally it's impossible.
We make some convention all the time.
And you say how I can verify mathematical in computers.
But this depends, of course, kind of rigid structure of computers.
And sometimes I fail.
And it's a kind of great miracle by mathematics still there.
Excuse me.
We don't need induction.
Huh?
We don't need induction.
I don't know what induction is.
For me, all this logic, I don't know what induction is.
For me, it's just empty words.
I'm sorry.
For me, induction is empty words.
It's empty words, I mean induction.
Because again, it depends how you write things on the blackboard.
It's a language.
Language depends how you write it down.
I don't know what it is.
We come back to languages, yeah?
I'm saying all traditional description of mathematics,
in my view, are greatly faulty.
And that's the reason why there is no model of mathematics
rather than a computer.
Why there is no model of understanding languages.
Because we have absolutely wrong perception,
traditionally built in the development of numerical mathematics.
And logic distorted our perception of ourselves
and of mathematics and of languages.
It's fully distorted.
Like at the same as speaking about sun going around the earth
and making all this language describing it.
It's just wrong.
The sun doesn't go around the world.
It's kind of a sheet.
You can describe it.
You can do lots of this.
It's just wrong.
It's a wrong language, wrong description.
And again, it's easy to say it's wrong.
It's hard to say what's right.
You can philosophically say it.
There is no reason to believe who turns around whom.
Unless you see other planets.
When you have many planets you say,
it's more likely we rotate around the sun.
But unfortunately, here we don't have much planets to guide us.
But of course, incorrectness or rather inadequacy
of the standard description of a prescription
by mathematics, I think, is rather apparent.
And that's, so we have no induction, what I'm saying.
I'm going to get it, I don't know what induction is.
Induction in mathematical logic is, again,
a greatly questionable thing.
Like all this mathematical logic.
You know, it's an interesting thing.
You may think mathematical logic is very kind of
a very careful discipline where you never make mistakes.
And in mathematical text, great mathematicians
never made mistakes.
And you see this one.
However, if you look at one of the founders of
mathematical logic, for example,
when he wrote his main book
and the pope incended to Russell,
Russell immediately said,
he's actually self-contradictory.
And Russell wrote his text.
And then Gödel was reading Russell
and saying every line was a mistake.
Mistake over mistake over mistake.
And the thing with Gödel says also,
it's wrong over wrong in a different way.
Logic is absolutely an rigorous sense.
It's absolutely just pure fantasy, mathematical logic.
Of course, as mathematical theory,
like model theory, set theory, it's okay.
Then mathematical theory,
then mathematical we accept them.
But when logic says, I'm foundation of mathematics,
I'm logical, I'm correct, it's nonsense.
The history of that shows it just always was wrong.
Because it's not the reason of logic.
It's illusion in mathematics.
It's something else, I would say.
It's something else, which is, of course,
naively used logic, but it's a great thing.
You shouldn't use it as set theory.
As naive set theory, it's a beautiful language.
When you go next levels within itself,
it's a good science.
As a language, it loses ground
and it's taken over by categories,
which for some reason is better.
But anyway, what you have, you have this product.
And so now, if you have originally, say, to the power,
so it's p to the power n,
and my notation a little bit may be confusing,
p to the power n, and say originally,
this space was considering p i
and i was writing 1 to jk.
And then it becomes big space,
which has k to the n.
Elements is a huge space,
and each probability of each event
becomes absolutely small.
On the other hand, it's exactly with what
you have to deal in physics.
We observe events.
After events, you have this blinking light
and blinks hundreds of times.
So a probability of any configuration of blinking
becomes something like 10 to the power minus 100,
the incredibly small numbers.
So we cannot explicitly make computations.
For that reason, you need formula.
And for that reason, kind of without thinking,
of course, Boyce will have his formula.
These immediately invent formulas,
which are good for accounting,
for making definite results.
In mathematics, we don't have to do it
prematurely.
So what about this space?
The point is,
as I mentioned before,
there are very special states
where all n's are equal,
and they're kind of represented just by numbers.
And their categories correspond
to multiplication table.
So just multiplication table.
And by the law of large numbers,
this space, p to the n,
in some correct sense,
converges to this called a homogeneous space.
It's asymptotically homogeneous.
So when n is large,
all n's become approximately equal.
And this is exactly the law of large numbers.
And if you decipher approximately,
this will be the right topology
where the definition which I gave
become correct.
In what sense approximately?
And you have to just remember
what the law of large numbers tells you.
In a second, I will explain this to you.
And the moment you have it,
this definition has the following power.
That it tells you
not only this abstract statement,
really this isomorphism,
but it says that if you consider inside
of this big category, this very small category,
where all objects are just
atoms of equal weight
and morphine between them just
product of numbers,
then this category dances inside.
Therefore, any property
which continues,
with respect to the topology
I want to describe,
most of them are obviously continuous,
if it's true for constant,
for these kind of simple sets,
this becomes just maps
between sets,
where all these weights disappear.
And if something is true there,
it's also true, it's also true
for general category.
So many theorems become immediately apparent
because of this density property.
It's not only,
slightly more than that, Bernoulli's theorem,
if you look at the logic of that,
tells you not only that space is being approximated,
but if you have a
put here, this morphism
becomes approximated
by just
multiplication of numbers.
And so, this I want to explain in some example
how we can derive
from that rather non-trivial
properties of
geometric objects.
And this for me was someone
surprising, I just realized the power of this
entropy. Before, it's just
a formula, you see, I remember
knowing this formula, you never could
swallow it, it's just a formula.
From a genre, a formula,
it's just a formula.
Completely meaningless, you know.
And then, interesting enough, by the way,
this is a problem of modern, of course, computers,
I needed, at some moment,
some particular inequality about entropy.
It's kind of trivially following from other ones
and I couldn't find it in the literature.
I took all textbooks on entropy
and none of them was doing that
because all the repeating, only one of those
was written on one of the first, I think, books
or articles by Rocklin.
Pacing, copying, pacing, pacing,
nobody ever of the authors tried to think
what entropy was. They believed
these right definitions just make copies.
And this happens, by the way,
very often in science. You make copies,
this way I'm saying history is useful
because you can realize what
we have is just mistaken for
perception.
Now, so what's the example I want to consider
and this is close to my heart.
Geometry is called isoparametric
inequality.
So I'm saying that
isoparametric inequality
follows from functionality of the entropy.
This is from factorial definition.
Just from definition, from the law of large numbers, essentially.
So what is isoparametric
inequality? I say in three space.
So in three space, it says we have a domain
and say omega.
Then volume of omega
is greater or equal to area of its
boundary.
And you have to put the right
exponent. Here you have
to take say
I think in this form, up to
a constant.
Universal constant depending on dimension,
here dimension three, is up constant.
But for the moment, I'm not
very much concerned with the constant.
Though I just realized there is
a problem with correct constant.
It's sometimes very important
to have the right constant.
So this inequality. I'm just saying
it if you
kind of
apply this logic
of
this being almost
constant, then it would
this statement which I said
that
then it would
imply this isoparametric inequality.
Which is kind of amazing because it looks
geometric inequality. You think you have to do
something. But amazingly enough
you need to do very little geometry
not geometry inside. The law of large
numbers takes care of geometry, which I think
is quite amazing.
So this is
kind of
kind of non-trivial
inequality. You know, yes it's even
already in dimension three
it all proves
require some idea, right? Because the
problem is in dimension two, of course
it's all obvious. If you have a
short curve, you just
count, integrate this area anyway
and solve it, of course, area less
than square of the length, right?
Because you integrate twice. Of course
sharpening quality is more, it requires
more effort.
But in dimension,
in high dimension what may happen, you may have
these domains, you know.
It's very long narrow fingers
which
inside very little area, and who knows
they may carry lots of volume. This doesn't
happen. But this we have to prove.
Right? So there is something
to prove.
This concerns
what I'm speaking, non-sharpening quality.
And this is what is relevant
for most of analysis.
So-called sobering inequality.
All the sobering inequality is a trivial
corollary of this inequality.
But the way there is one inequality
which doesn't fall immediately from them
is called log sobering inequality.
Interestingly enough, we shall prove
this entropy will be this log sobering.
Even stronger, up to a constant.
The constant is another issue.
By the way,
nowadays there is no
kind of
good proof, good
meaning, yes, by
formulas of the isoperemetic inequality
of sharpening quality.
Amazingly enough, such proof exists
in dimension two. It exists
in dimension four, and that's it.
There is a formula
which is apparent.
This inequality with a sharp constant
is apparent in dimension two and four,
not even in dimension three.
Not to speak about other dimensions.
And it's hard
well, it's hard
to correctly formulate that such
is impossible for higher dimensions, because
there is some. What do you mean by formulas?
And the formula is very
simple in the case of two and four.
But if not sharpening
quality, the proof which I give you
will be not by formulas, but we will
follow from this functionality.
And so
let's
prove it.
First, the point
is we reformulate it in very
general terms. So it will be amenable
to what we do.
And then it will be the following thing.
I have a measure space, X,
X, X, X, X.
In the examples
in question X is the real line.
However, in the course of the proof, I will have
to change it. So it might be just
measure space.
And later on
I explained why I don't like measure space.
On the other hand, that's all good.
You see measure space is trouble with measure space,
there is no such thing in the measure space.
Contradiction in terms.
Space with measure abstract measures are not sets.
The objective category
is not a category of sets.
And usually all expositions which you find
in textbooks on measure 30 are all wrong.
Mistakes were mistakes, mistakes were mistakes.
They are not rigorous. They say how we
leave something up to one point, measure 0, boom.
And this is not set rigorously.
We want to set rigorously, we need the whole
body of that millifrancule theory
which includes sets bigger than continuum,
which never you do. And of course it's also
pretty certain, usually full of mistakes.
And nobody has real numbers also.
Nobody ever wrote down
rigorous, rigorous foundation
of real numbers.
All known expositions have
faults, there are gaps.
We still believe it's okay, but
you know, about rigor you might be
careful. Okay,
but so, but anyway I say
given that, and then there is a subset
there,
and then you project it, so put it one,
two, three. They don't have to be the same
space, they are different. And it's
notationally much easier. And you have
three reductions of that.
You have omega 1, 2
in x1 times x2
and similarly omega
1, 3
in x1 times
x3
and omega
2, 3
in x2, 3.
And this was a very good setting
for
entropy.
So you have,
and this is one way
to think about entropy
in physical context is as follows.
Very close to what we consider.
That we have a space,
big space, x
matches space, which is a product
of xi.
Maybe finite to infinity.
And then we have some
subset omega.
And then we project it.
Observe, I cannot
here, if I write x to the n, I cannot
even write it down. Which people
sometimes do in combinatorics, it takes
pages just to write it down. And you
never can read it after that.
It's very, very essential
that your space is a product
of not one, two, three,
but any set and then takes subset
and otherwise you have double indices, people indices
have a horrible mess because,
but again it sets third rotation.
They're still much more
primitive than category third indices.
So, so you have
projections.
And something is
about the measures of this projection, how
they relate to measure of this set.
So,
in a second we see that
it's entropy which matters.
Actually, it's about
entropy.
And so the theorem says
if I take measures
of this, I put a kind of
bracket so it would not be confusing.
I take omega one, two, meaning
measure of that times
omega two, three
times omega one,
one, three
and two, three.
And this is greater or equal
than
omega squared.
To read this
in equality.
And this is a
first, I shall prove it
in the next play, but it has to be
entropy and we change in equality
this will be the end of the next day.
So this is called Lumi's Whitney
theorem.
It is
on one hand it doesn't give you the sharp
constant, but it is
in a way stronger
than isopermatic in equality.
Because
if you apply it to the Euclidean space
coming back to the Euclidean space
when you have omega in R3
and you have projection
to three planes.
There are three coordinate projections to
R2 one, two
R2 one,
three and R2
two, three.
Domains here, domain here, domain here
and you say,
this volume squared
bounded by product of these areas.
And this is stronger
because
by geometric arithmetic mean
this give you
this quantity is smaller
with a proper normalization
the sum of this.
Therefore you bound
the volume by the sum of this projection.
But when you consider
anything with domain and project it
here, of course this projection
smaller than area, area goes down.
And so up to factor of three
you see that this volume is less
than area, of course
you have to properly normalize.
Have a right imagination.
It's again, typically
you don't have to write the formula.
You say in principle there is an equality
and automatically you write the formula.
And because
formulas are shorter people write formulas
and then they have to decipher them back.
Right? And so this is
a stronger modular
geometric arithmetic mean.
For certain configuration it's much
better than isoparametric inequality.
And so all you use at this projection
of geometry, what you use
at this projection area
goes down and this area
is a smaller image of the projection.
Right? So
area of the boundary is greater
than area of the projection.
Which is kind of, kind of, kind of obvious.
It's not deep
geometry but still this is the only
geometric gradient in this argument. And the rest
is this formal abstract theorem.
And this is a Lumi-Sweetian theorem.
And this is of course true for any dimension.
Just three, because the first case
when it is, when it is
meaningless, interesting.
Right? So how do you prove that?
So just say two words
and then
I'll come back to that
next time.
Of course you can realize
it's kind of a combinatorial problem.
You can imagine this being a finite set.
This being just finite.
It is subset and you have this
process. So it's a combinatorial theorem
anyway but it doesn't help.
I say measure, I could say equally
just finite set.
And this
means cardinality.
How do you prove that?
So
you have this projection
you project it here
and you use, kind of,
naturally what it's called for being a theorem
whatever. I don't know. You just
evaluate this by integrating
this height. You intersect
these vertical lines. You have this domain.
You integrate it.
And so you have some
equality. But the trouble is
this kind of thing is variable.
So it's
implicitly this function involved.
How these things varies. But imagine
it would be constant.
Then it would be very nice. You know
the total volume equals
this times this.
This height. So if you
write it to all three projections
you immediately have your inequality.
It just becomes A plus B equals C
or something. I will
reproduce it next time. But if you
do it yourself you see immediately, if you assume
that you set, you have this property
this projection on all three
directions. All these
heights
when they non-zero are equal.
See some may be zero. This
a priori subject because you have
inequality rather than equality. And
inequality means it sounds like a secret
zero. They don't appear kind of in the picture.
If you write this inequality
you immediately arrive at this inequality.
So the trouble is of course they are far from
being equal. They are all
like of the domain. You project it
and of course all this
intersection are different.
But then I'm saying aha.
And now we apply the law of large numbers.
But we apply it not to your set omega.
But we apply it put here number
n, number n, number
n, number n. The number
is an infinitely large number.
Here again it's very convenient to speak in the
language of non-standard analysis. So they
have very very big number. Pretend
they kind of, and then everything which is small
compared to this disappears.
Non-standard analysis of course is just a language
but it doesn't have
much depth but still very convenient.
So and then
what I said when n has become very large
I can imagine everything become constant.
In particular this function
of this projection
when you go to the limit
it describes kind of property of our morphism
which I didn't say. All
set, all errors
become
everything become constant.
So the picture it uses essentially
to the one which I described
here. And where it's obvious.
And so I have this theorem.
Again I will explain it in detail
I will explain it in detail next time.
And this immediately
also if you think about that. This is a good example
yes you can think for yourself before
before listening to what the detail explanation.
That first
why it is obvious when all this
projection are equal.
And if they only equal up to epsilon
there is epsilon error and goes to
infinity error goes to zero and have result.
And this one point again depth
conceptual point is
you don't have to stick to one space.
You can allow anything here.
Only this
combinatorics of this product
how it's organized relevant. It's not intrinsic geometry
of the Euclidean space.
However interestingly now
the corresponding sharpening quality
the corresponding sharpening quality
in Euclidean
space is unknown.
So I shall discuss a little bit.
We don't know the similar quality
which will be fully symmetric.
Because this
extreme configuration
is a cubia or rectangular
solid.
Probably it's actually a cubia.
It's not a ball. And how to make
argument when the ball will be
extreme. And there are partial results
and the proof is rather sophisticated.
And it's unknown.
Fully it's unknown.
Though there are very close to
related results we shall know.
So certain
reformulation of that also quite powerful
is true for the Euclidean
space.
Okay, that's
for today. So next time I'll
repeat more or less what I started
about this category and explain
this in detail the formulas
which I suppressed so far.
Thank you.
