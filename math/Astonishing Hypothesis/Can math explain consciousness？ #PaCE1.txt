At the end point of scientific inquiry, we often find mathematics.
Over the past decades, neuroscientists have developed mathematical techniques that aim
to measure consciousness.
And these mathematical measures, applied to brain data, seem to do a pretty good job
of telling us whether an active brain is conscious, comatose, or anesthetized.
Why does all of this matter?
You probably heard of the Google programmer, who thought that an AI he interacted with
was conscious.
This news was mostly ridiculed.
After all, something acting like a conscious being does not mean something is actually
conscious.
How do we know if an AI, well let's say, a human being is conscious?
Not being able to rely on behavior to determine the existence of consciousness has many ethical
implications.
We do not know to what degree certain animals are conscious.
This means we also do not know when consciousness arises during human development.
We can never be entirely sure whether someone or something is conscious.
This is especially bad if someone gives us reason to believe that they are unconscious,
while they are not.
Some patients have been shown to remain conscious during anesthesia, for example.
Other patients were found to be conscious long after being diagnosed as comatose.
What it takes to answer these questions scientifically is a theory of consciousness that can mathematically
translate brain activity into a measure of consciousness.
Of course, there are many theories about consciousness, but there is only one theory to date that
provides mathematical equations that allow us to determine if someone is conscious.
Using this math, neuroscientists have been able to compute if patients were anesthetized
or in dream to sleep just using their brain activity.
And here is the kicker.
According to this theory, we should be able to use brain activity to tell what someone
is conscious of.
In other words, we can use math to peek into your conscious experience by applying a small
set of equations to neural data measured from your brain.
This theory is called Integrated Information Theory, or IIT for short.
IIT was originally put forward by neuroscientist Julia Tononi more than 15 years ago.
IIT has since been championed by other leading neuroscientists with an interest in consciousness,
such as the director of the Allen Institute, Christoph Koch, and many others.
The equations underlying IIT fit on a single page, as shown here, but IIT is even simpler
than it may seem.
As much of mathematics, IIT starts with a small set of axioms.
Axioms are statements that seem reasonable to accept without proof.
In IIT, these axioms define consciousness, that is, these axioms collectively describe
what all of our conscious experiences, ranging from seeing the color red to toothaches and
falling in love, have in common.
IIT builds on its axioms using postulates.
Postulates translate from the axiomatic properties of consciousness to mathematical characteristics
of a physical system.
In other words, IIT tells us what a system must be like to produce consciousness.
Let us first define what IIT means by system.
Well, a system is just a set of things that interact.
Interaction is key.
Science can only assume the existence of things that interact with other things.
Thus, interaction is the basis of existence.
IIT starts with the axiom, that consciousness exists, and then postulates that a system that
is conscious needs to consist of interactions.
Let us mathematically formalize what we just did.
Each system consists of two or more elements.
An element can be represented as a discrete variable x sub i.
In order to allow for interactions, elements are defined by three properties.
They can have at least two states.
They receive inputs from other elements that can change these states.
They have outputs that depend on these states.
We are taking elements x sub i to sub n from system x sub t, where t denotes the present
moment in time.
To keep things intuitive, let us assume that our four elements here are brain cells or
neurons.
As you might know, neurons can either be in an on or off state.
We will indicate these states as unfilled or filled disks.
As all disks are unfilled right now, all four neurons are off at the current time point
t.
Now we said that in order for consciousness to exist, there need to be causal interactions.
These are visualized here by white lines.
What defines a causal interaction?
Well, remember that each element must have an output that depends on its current state.
These outputs become inputs to other elements.
These inputs define whether or not elements change in state.
Also using outputs, single elements, or a combination of elements acting in concert
can change the state of other elements.
And that is a causal interaction.
If a subgroup of the system, such as a single element or a combination error, influences
the overall system state, we can call that a causal mechanism.
Let us denote mechanisms with y.
And to keep things simple again, let us assume that each element of a model is also a mechanism.
Of course, if we consider the brain, we can think of neurons as elements that are also
mechanisms.
But the mathematics of IIT is more general, as the same concept applies to other brain
structures or even AI.
Now, IIT's general idea should determine whether a system integrates information.
Integrated information is quantified as how much a mechanism in a particular state makes
a difference to the system as a whole.
One way this can be done is by cutting a mechanism out of a system.
If such a cut makes a difference, we can quantify how big that difference is.
How is that done in practice?
Remember, each element has a state, receives inputs, and produces an output.
In our little system, each neuron receives three inputs.
The inputs each neuron receives depend on the state of the other neurons that send their
outputs to this neuron.
So in our case, each neuron receives a zero or a one from three other neurons depending
on their on or off state.
Real-life neurons perform simple computations, such as summing up the inputs.
If the sum is larger than a certain threshold, neurons change their state from off to on.
So depending on its inputs, a neuron may or may not change its state.
And this state becomes its output.
In our model system, the output is split so that each of the other neurons receives
an input.
But these are all just copies of the same output.
This means we can describe all possible interactions of this neuron by creating a table.
This table lists all possible combinations of three binary inputs and resulting states.
The resulting state equals that neuron's output.
As we said, real-life neurons sum up inputs.
If the sum is large enough, they turn on.
We can replicate this process in our system by assuming the neuron's threshold to be
three.
In other words, we define a neuron's state to be on, or one, if and only if all three
of its inputs are one.
Put differently, our model neuron's output is always zero, except when all three of
its inputs are one.
An input-output relationship, such as this, is also called an AND gate.
You can probably see why.
The output of this gate is only one if its input one, and two, and three are all one.
Now let us assume our neuron is much more sensitive and has a much lower threshold for
its summed inputs.
Let's say it is set to one.
In this case, we end up with a very different table.
This kind of table resembles a different logic gate called OR.
You can see why.
This gate is one if input one, or two, or three, are one.
Such logic gates are at the heart of computers.
However, keep in mind that actual neurons are much more complicated than that.
So here's our model system again, except now we mark for each neuron what kind of input-output
table or matrix we have defined for them.
Now looking at the single moment in time is not very helpful, since nothing is changing.
And since we are after causal interactions, it is change that we are interested in.
So instead of freezing the system at a single moment, let us look at how the system evolves
over time.
T zero is the current moment, minus one is the immediate past, and T plus one is one
step in the future.
The connections between neurons will be shown between one slice of time and the next one.
In order to calculate the amount of integrated information of a system, we first need to
evaluate the information generated by each single mechanism.
This is done by computing how much each mechanism affects the system as a whole.
This influence is quantifiable, since the current state of a mechanism constrains which
system states can occur.
And IIT goes a step further.
By knowing that a mechanism is in its current state, we also obtained information about
the past, since only a subset of all possible system states can lead up to the mechanism
being in its current state.
This means that a mechanism does not just constrain future possibilities, but in some
sense also constrains the past.
Let's start with the latter.
Suppose we don't know anything about current or past system states.
What would the probability of each possible past state be?
IIT calls this the unconstrained probability or PUC.
For our four neurons, there are 16 possible past states.
We can symbolize these states using zeros and ones for off and on states of these neurons
respectively.
If we do that, we get a table where each row represents a possible past state.
Since each state is equally likely, our distribution PUC is uniform.
All possible past states might have happened with a probability of 1 over 16.
But remember, this unconstrained probability is due to the fact that we lack any information
about mechanism and system states in the present.
Now let's suppose that we know that mechanism X1 is currently in an on state.
Since mechanism X1 only turns on when all of its inputs were on, we can derive a very
different probability distribution of past states.
We now know that there can only have been two past states.
Both states were equally likely.
This means that the conditional probability of each of these states was 0.5.
The resulting distribution is called the cause repertoire of mechanism X1.
This example shows how one can gain information about a system's past by knowing the present.
But how do we quantify that information?
IIT does that by calculating the distance between the unconstrained and the conditional
probability distributions.
The result is called the cause information.
Now that we have seen how we can quantify information about the past, that is gained
by considering a given mechanism in its current state, we can apply the same principle to
future states.
In other words, we can derive an unconstrained probability of future states as well as the
conditional probabilities of future states given the current state of mechanism X1.
The latter distribution is called the effect repertoire of mechanism X1.
Then we calculate the distance between these two distributions and the resulting number
is the effect information of mechanism X1.
Let's recap.
We have seen how IIT quantifies information within a causes system that is contingent
upon or constrained by a mechanism in its current state.
The result of this process are two numbers, the cause information and the effect information.
Of course, both these numbers were specific to a single mechanism in a specific state.
In this case, we looked at mechanism X1 presently being on.
If it were off, both the cause information and the effect information might change accordingly.
And what about the other mechanisms?
Well, if we know the present state of the system, we can simply repeat everything we
have done so far for each of them.
Each time we obtain the cause information and the effect information given that mechanism's
current state.
The final step is to decide whether the cause information or the effect information are
more important to the system.
Since IIT aims to characterize the capacity of a system, it sees the minimum of the two
as a bottleneck.
This bottleneck can be thought of as the weakest link that breaks a chain.
Thus, whatever is the minimum between the cause information and effect information is
carried forward as the cause effect information of the mechanism being in its current state.
Okay, this explains how IIT quantifies information, but how do we quantify integration?
As it turns out, we can use the same algorithm of comparing probability distributions for
this quantification as well.
Specifically, we eliminate each connection between a mechanism and the rest of the system
and see if that makes any difference.
If the system's probability distributions do not change after we eliminate a connection,
we can deduce that the connection was reducible and there was no integration.
In practice, this is done by a process called statistical noising or marginalizing.
This just means replacing the inputs provided by a connection with a random noise distribution.
Then we compute the distance between the partitioned and unpartitioned probability distributions.
Since we have done that, for all connections, we determine which elimination resulted in
the smallest change.
The partition that yielded the smallest distance is called the minimum information partition
or MIP, and the result of that computation is the amount of integrated information or
small phi that is generated by the mechanism.
Now we can compute both the past integrated information as well as the future integrated
information as long as both are larger than zero, we can take the minimum to derive the
cause-effect integrated information for a mechanism.
Of course, this computation only provided the integrated information on the level of
mechanisms, such as individual neurons.
How do we get from here to the whole brain and consciousness?
Well, we can use the same algorithm of replacing connections with noise, not just for single
mechanisms, but also whole subsets of a system.
So there you are.
There's a lot more to say, of course, but this is the gist of it.
You now understand that IIT provides a simple piece of math that links brain and mind.
By computing the integrated information of a neural system, we can finally start to work
on finding out more about consciousness.
