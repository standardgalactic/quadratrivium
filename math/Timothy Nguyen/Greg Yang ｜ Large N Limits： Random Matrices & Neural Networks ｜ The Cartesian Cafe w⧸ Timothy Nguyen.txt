All right, welcome everyone to the Cartesian Cafe.
We're very lucky to have Greg Yang with us here today.
Greg Yang is a mathematician and AI researcher
at Microsoft Research, who for the past several years
has done some incredibly original theoretical work
in the understanding of large artificial neural networks.
Greg received his bachelor's in mathematics
from Harvard University in 2018.
And while there won the Hoops Prize
for Best Undergraduate Thesis,
he also received an honorable mention
for the Morgan Prize for Outstanding Research
in Mathematics by an undergraduate student in 2018,
and was an invited speaker
at the International Congress of Chinese Mathematicians
in 2019.
Welcome, Greg, how are you doing today?
Thanks, Tim, this is a great opportunity to be here.
Yeah, great, I'm very happy to have you here.
So we were lucky to catch up earlier in the year
over ramen in SF when you were visiting the Bay Area,
and we had quite some interesting conversations.
Why don't you explain to our audience
how you got into math and some of the hiatuses
you had along the way?
Yeah, I wanted to dig into that.
Sure, yeah, so I mean, I think like growing up in general,
like I just did a lot of math.
So like when I was, so before I was 12, I was in China,
so I was more in China, and I grew up there for some time.
And then like during, when I went to elementary school there,
you know, like my mom would buy me these like
Olympic mathematics books and just like,
just give it to me to read,
and just like saw problems on my own.
And when I came over to the States,
I also participated in math competitions.
So I kind of like, you know,
I kind of had a history with math even before college.
And then when I went to Harvard,
I took the Math 55 class my freshman year,
which like some of the audience might know as like,
kind of this infamous class, the freshman math class,
like it's like the hardest or something,
like the hardest in the United States or something.
He has his own Wikipedia page that like,
I guess I got his fame from like people like Bill Gates
taking it and saying, okay, Bill Gates is like,
I took it and realized I'm not that good at math.
And so that was pretty fun.
And yeah, so I did like math at Harvard
and I did my freshman and sophomore year.
And then, yeah, after that,
I actually decided to take a leave of absence from Harvard.
And at the time it was because I wanted to become a DJ
and like a producer of like electronic music.
So that's, so there's a parallel history here,
which were like kind of in high school,
I got into like producing music.
So I mean, at the time I was more doing like rock music
and stuff, but like as I got to college,
I got more, you know, more contact with electronic music.
And so over time I just like make my own tracks.
And so at that point in time,
I just want to try something new, you know, like Harvard
is kind of like, or it's not a Harvard thing,
but just like, you know, like you're just kind of
in a hamster wheel of life for like, you know,
for the past sort of 18, 20 years.
And it feels like it's kind of boring, right?
Like let's try something new, like something unexpected,
something surprising.
And I mean, I was really getting to the music at the time too.
So, yeah, so I just decided to take time off.
And this is of course also like leveraging like Harvard's
kind of really generous policy on taking leave of absence.
I'm not sure how long like this policy dates back to,
but like I wouldn't be surprised if like,
this is also in place when Bill Gates,
you know, when Mark Zuckerberg took some time off.
But like essentially you can like take any amount of time off
and come back anytime you want.
Like, so this is really nice, right?
Because you don't have to worry about,
like you just like safety net in some sense.
They can try something like really risky
and it doesn't work out, just come back to school
and finish it up, you know?
So, yeah, so I just decided I'm gonna take some time off
and like make some, you know, like sick beats, you know,
and try to play some big shows, stuff like that.
So yeah, so like around like 2012 or so, yeah, I just,
yeah, I just went home for a bit also,
like stayed in Boston area for a bit and worked on music.
So, yeah, so this is like kind of like my music career,
I guess, at that time.
And, but the unintentional side effect of all of this
is, you know, I mentioned this like hamster wheel of life,
right?
You kind of give you a break from this hamster wheel
and I kind of really start to like understand myself more.
I think you just like, you know, if you're in that hamster
wheel, it's kind of you're always like, you know,
like pedaling and you don't have time to stop and think.
But like with that break, like the unintentional side effect,
with this positive side effect is that I really had time
to understand myself, like, you know, like what I really
am interested in, what I like and like how my brain
and my body works, stuff like that.
So like, I read a lot of, you know, a bunch of books,
like, you know, like quantum, I mean, quantum mechanics,
physics, you know, mathematics, whatever,
like a bunch of random things.
And so like, you know, in summary, like this, this period,
when I was about 20, like kind of gave me,
I think it really changed like who I am.
Like in the sense that, you know, I think the me today
is essentially the same person as, you know,
that person after I took my leave of absence,
but that person is like a delta jump away from like the person
a year before, you know?
So like in terms of like myself identity,
like that period was like actually quite crucial.
Actually, just a quick question.
Did you have, I don't know, a lack of clarity going
into Harvard or did you, did that emerge
once you matriculated to Harvard?
I wouldn't say it's lack of clarity.
I think it just, I was just like everybody else, you know,
like just, I mean, like, I don't know, I'm sure.
Like most people when they go into college,
they don't really know what they're doing.
They know like they're like good at doing things,
which is why they got into Harvard
or other, you know, Stanford, these kinds of schools.
But I doubt that people have a very clear vision
on what they want to do other than maybe the pre-med people
who are like spending like the next 10
or whatever 10 plus years of their life on medicine.
But yeah, I mean, I don't think I was like, you know,
more like unclear than other people,
but it's just that like that period really gave me
kind of a, yeah, this clarity and like this focus,
this purpose of life.
Yeah, so I haven't gotten to this,
but like essentially during that period,
I realized a couple of things.
But one thing was that like I really wanted to make AGI happen
or a strong AI.
Like-
And this is 2012 when you thought had these ideas.
Okay, that's pretty early to have those thoughts.
So that was like when like, I guess like deep learning
was starting to blow up,
but I wasn't aware of deep learning at the time.
So yeah, so at the time, like a lot of like these kind
of futuristic things really appeal to me.
Like making strong AI is one thing.
Like making something that is like much,
much smarter than yourself.
That sounds like such an attractive idea
that like I just decided, okay, like this is like
either this happens or I try, you know?
And yeah, I mean, like among other things,
something other things that I found really interesting
was like, you know, like is it possible
to make humans immortal?
I mean, I want to live for a long time
and see like how the future of it goes.
But, you know, that sounds like not something I can like,
you know, like I'm not like a big, you know, X factor
in this kind of line of research.
Cause there's like a lot of like experimentation
with like, you know, biomedical materials
and like there's a lot of red tape regarding this.
You know, I don't have like, it doesn't seem
like it needs a lot of math either.
Okay, I didn't actually realize that your thoughts were so,
were so like kind of all over the place in some sense
because, because my kind of my impression of people
going to Harvard and taking the hardest math track
is that, you know, they were stellar math students
when they were already applying to Harvard
and they just keep going in that line.
And so to what extent were you not,
even though I guess you were good at math when you applied
or actually did you even know you wanted to study math
when you applied to Harvard?
I would say your characterization is kind of
by and large, not correct.
In a sense, if you like the absolute numbers,
like they are definitely like, I think there's some kind
of psychological bias because, you know, when you look
at like people who wins the fuels medal or whatever,
like they would have gone through the track,
but they're only like a small number out of, you know,
the total number of people who go on.
Like, you know, could take math courses at Harvard
and be a math major.
By and large, I think people like majoring
or concentrating in math, which is like a harder terminology.
By and large, they take other fields.
Like finance is a popular one.
Tech is a popular one.
Like only a very small amount of people want
and have the capability to do like a PhD
and like, you know, follow the math career.
Because I mean, to be completely fair,
it's like a very hard career.
Oh, sure, sure. No, no, sorry.
Maybe I don't know if we're talking past each other.
I met at the high school to undergrad level,
the people who are, you know, Harvard is an elite institution
with an elite math program.
And I just, my impression is that if you're gonna go
that route, then it seems like you had some certainty
about your interest in math.
But in your case, because you ended up taking some time off
and had all these thoughts about artificial intelligence
and aging, it just strikes me as a bit unusual, that's all.
So I was just trying to figure out if you were already
like had all those interests before going to Harvard
or did you go to Harvard and like then had your like,
let me rethink a little bit.
Yeah, yeah.
Yeah, I think going into Harvard, I was like open to everything.
Like I wasn't like dead set on doing math for life
or anything like that.
I mean, I knew math is a powerful tool, but it's not like,
yeah, it wasn't, you know,
it definitely wasn't this dead set
on being a professional mathematician.
I don't feel like, and I don't feel like, you know,
most other people are like in the math department
or also, you know, kind of like what you were thinking.
I feel like there's sorry more like me.
I mean, especially, okay, maybe just to clarify, right?
Like for Harvard, when you enter Harvard,
at least at the time, I think it's still the same right now,
but when you enter Harvard, you don't choose a major actually.
So they let you explore for like a year and a half
before they actually decide a major.
So in that sense, like when you get,
you might be accepted based on, you know,
your math capabilities, but like you're not forced
to choose, you know, a path until you explore
for like sufficient amount of time.
That makes sense.
Yeah, yeah.
So, yeah, so I feel like, you know,
the people I interacted with, you know,
they were all good at math and like some people are definitely,
you are thinking about like the professional mathematician route,
but most people are thinking about all kinds
of different opportunities in life, you know?
I see.
So actually, if I recall, actually you had two,
two like hiatuses, right?
You had one to become or to go further into your music career
and then you had another one for math.
Let's wrap up the first one.
So the first one, you wanted to go into your music career.
How did that, where did that end up?
Yeah, I mean, so, yeah.
So I essentially have produced a bunch of tracks
and then like I said, I read more and more
about like artificial intelligence.
I just like started spending more time on that.
And like, so I just ended up like kind of, you know,
like slowly decreasing the amount of time I spend on music.
I put you something, some nice tracks, so, you know,
but are they publicly available?
They are, but I need to find it.
Okay, all right, maybe we could post it later.
Okay, great.
Yeah, but yeah, it's not like I play like Coachella
or anything like it's just like a very,
very tiny, tiny career in music.
But yeah, so during that time,
yeah, I realized I want to make AGI happen.
And then there are two other realizations
that are kind of synergistic with this.
The other thing is that, like, you know,
before I knew I was good at math,
but like after taking a break,
I really like realized that I like fucking love it.
Like, so you can, you can listen to this out if you want.
No, you're not the first person to cast on.
Okay, okay, okay.
But yeah, I fucking love math.
Like it's really something, I think, you know,
in psychology, they talk about intrinsic
and extrinsic motivation or reward.
And this is like a case where I think,
like you're in a hamster wheel for so long,
the extrinsic reward overpower your intrinsic motivation
for a long time, but like taking kind of a break
and just like, you know, like,
and don't worry about what anybody else think
or like what the society demands of you.
It really brings out this intrinsic motivation.
So I realized I really love this shit.
And so this is the second realization after the AGI thing.
And then the third thing that ties all these together
is that I also recognize that like math
is really the fundamental language
for essentially anything in science
and very likely you like to make good advances
in anything, including in AI,
like you need a very solid foundation in math.
And I mean, I learned very quickly as well.
So like I just like essentially decided to like read everything
in math and like kind of know all the,
at least I know all the basics of the major branches of math.
So this is during your second hi-diss or break?
The first.
All the first.
Oh, okay.
This already started pretty early on like around 2012-ish.
So I decided to just like read everything
from the beginning in the sense of like,
I'm gonna start with like set theory.
And then just like read the set theory
and understand like all the basics,
like this one's lemma, whatever.
And then like go up from there.
I mean, there's like basics like linear algebra
then like abstract algebra, so on and so forth,
geometry, you know, analysis.
It kind of just builds your way up, you know,
like all the way to like, you know,
category theory, I don't know, harmonic analysis, whatever,
you know, there's like all the kind of main branches
of math we know about today.
It's just like to gain like a good foundation
so that I don't have blind spots, so to speak.
It's kind of excessive in some sense because like, you know,
when you want to do research in any particular field,
you don't need all these other, you know,
extraneous knowledge, perhaps.
But I mean, like in my view, there's a lot of synergies
between all of these different branches of math
and like letting you see everything that humanity knows.
I mean, which I don't claim to be,
but like I try to go toward that goal.
It really lets you see things from an equals point of view,
you know?
Sure.
Yeah, I think, I mean, like as a comparison, you know,
I think it's not inaccurate to say that like, for example,
a lot of people in like half the people,
what I say, in machine learning,
they, you know, they would be kind of afraid
or like kind of, you know, like stop in their tracks
if they see like complicated math,
they'll kind of be turned away in some sense, you know?
Like, and that kind of prevents you from doing things
that you could do if you have more knowledge.
So this is kind of just saying like,
just as in post-talk evaluation, my choices,
like I normally don't have this problem, right?
When I do research, like if I see complicated math,
I just like look at it and I mean,
I will be able to understand it, you know,
much better than if I don't have the background.
Sure, okay.
So you had some self-study,
you went back to Harvard
and then you took some time off again.
Can you explain this?
Yeah, yeah, yeah.
Right, so I kind of went back to Harvard for a semester
just because I want to like kind of see my friends,
you know, I made at Harvard before they finish off.
So this is like last semester
if I would have continued without break.
And then, yeah, then took some time off again
to just like really, you know,
like drill through this goal of like reading
all the foundations.
So like that, so yeah,
so first time I took like a year and a half off
and went back for a semester
and then the second time I took two years off
and this is like just purely,
there's no music in this case,
it's just purely like kind of reading
and learning as much as possible, as fast as possible.
Yeah, and that's interesting
because you're at Harvard,
you're at one of the most elite institutions.
Clearly you have much to learn
from your top notch professors and peers.
And nevertheless, you took two years off for self-study.
Can you explain your motivations for doing that?
Yeah, right, I mean, like the way I see it,
which I think is not like that extraordinary
is that for like really foundational stuff,
there are really good textbooks that are like battle tested
over years and years of teaching by, you know,
by professors and like people reading them
that like probably is like, you know,
much on average, much better quality than like, you know,
some professors instruction on a particular day, right?
And I mean, so there are different factors to this, right?
Like one, like if you can read very fast,
you can like read books much faster
than listening to, you know, verbal instructions.
Yeah, second, I mean, like there's also like a,
instructor, you know, is a person,
there's like the variance with respect
to the teaching quality, even if they're very good.
And I mean, and to be honest, right?
I'm sure like a lot of people have this experience,
like when I take math courses,
I kind of just, a lot of times I just skip the lectures
and just read the book anyway,
like it doesn't make a difference, really.
So in some sense, you're kind of naturally
an auto-didact essentially, I mean, you're,
I mean, there are many reasons why one could be an auto-didact,
but basically you don't have a problem learning on your own.
I have this quality as well.
And somehow when you have the motivation,
your, I mean, your bottleneck is just you go, you know,
basically you can go at your own rate
and in a way that a teacher who has to cater
to an entire classroom can't kind of customize to,
you can customize to yourself, the teacher cannot, right?
I mean, maybe that's the point.
Okay, yeah, yeah.
I mean, this is especially true for like, you know,
all the knowledge that we already know for many years.
It's not like we're,
I'm not learning any like super cutting edge stuff, you know?
Like for that, you know, like if you want to learn
the most advanced stuff in, you know, like, you know,
like higher tuples theory today, okay, like probably, you know,
you need to like talk to Jacob Lurie or something.
But, you know, but if you want to know just like things
we've known for 50 years, I mean, you know,
there's no particular reason you can just read books.
Okay, fair enough.
Okay.
Anyways, so you took your two years off, came back
and I guess you were probably, well,
ahead of your peers, presumably,
and then you, yeah, what happened then?
So what happened after what?
So you took two years off, now you're this math buff,
I guess, by lifting your math weights for two years,
you come back and then what happens?
Yeah, right.
So, yeah, I mean, so, I don't understand, like at the time,
I wasn't, you know, there was no like ambitious plan,
you know, like for the next 10 years or something.
We're just like, I knew this is probably valuable.
So let me just spend time reading math
and then like whatever happens next, you know,
whatever you would just happen, you know,
I'll just see where it goes.
But the way it worked out in the end is that,
so I came back to school and I was assigned a,
so I was in the math department at the time,
I declared my major to be math.
And so I was assigned like some random,
randomly assigned like a academic advisor,
which for undergraduates means they sign some papers
and like they don't talk to you for the rest of it, you know?
But I got assigned to, you know, Shintong Yao,
so he must be very familiar with Yao from
like Cloudy Yao Manifolds.
And yeah, so from this, you know, I didn't expect too much,
I had no expectation of, you know, talking to him too much,
but somehow like we did get to talking
and over like multiple interactions,
he learned about like some of the papers I wrote
when I was in my time off.
So in particular, I wrote like this one paper on
like some surprising connection between
algebraic typology and like computational learning theory.
So I mean, in a gist without going into too much detail,
like the VC dimension, which is like a very fundamental notion
of like harness a complexity of a class,
like in terms of learning, like how hard is it to learn
like a hidden function, this VC dimension is captured
by like the homology of some natural space associated
to the function class.
Okay, so that's like a once in a summary.
But like going forward, like kind of back to the story,
so he really liked that paper, you know,
he didn't ever explain why, but he just like started inviting me
to like parties and like events and stuff like that.
And I just, I guess, okay, he must like my papers, I guess,
you know, and like he like, you know, arranges for me
to meet like visiting researchers or whatever.
So yeah, so we became really good friends
and yeah, so then like, you know, like the end of the semester
when I was trying to, yeah, when I was trying to see
like what I should do next, like we had a conversation
and you know, I told him at the same time,
I was interviewing at Google and then he was like,
oh, no, no, no, you should go to Microsoft.
And the next day I get like this email from Harry Shum
who was like the head of our research,
he's like one of the, you know, leadership below the CEO.
And then he just like, you know, asked like asked me
to chat with Jennifer Chase,
who was the head of the New England lab in MSR,
in Microsoft Research.
And then like we had a conversation, me and Jennifer,
and then she also put me in touch with Mike Friedman,
who you know, is another like a big,
he was a metalist who worked on point correct conjecture,
but he's been working at Microsoft on quantum computing
for some time on topological quantum computing.
And so when we chatted, I think we really resonated
because like, I think in my heart, I'm like kind of a topologist
and like definitely the paper I was, you know,
the yow liked was like on this topology
and the computational learning theory.
And of course, Mike himself is, you know,
somebody who won like wide recognition
for his topological work.
And then now he's working on like the intersection
of, you know, topology and quantum computing.
So I think we really resonated.
And at the end of the conversation,
like he just kind of went back to Harry
and just be like, I'll just hire this guy.
Don't ask me questions.
Yeah.
Yeah, so then like I got a like kind of call
like a couple of days later from Harry
and Harry was like, you know, like,
I got two fuels metal is telling me to hire you, right?
If I, if I don't hire you, then fuck me, right?
So that's essentially how I got into MSR
in like around 2017-ish.
Very nice, very nice.
Okay, great.
Well, I think that's a great story
and that takes us to what you've been doing since then.
So why don't we just get straight to it?
So today we're going to be talking about your body of work
known as tensor programs.
It right now spans five very technical papers.
The way I wanted to organize this conversation,
well, first of all, I invited you on selfishly essentially
because I wanted to understand your work better.
But of course, in order to make this maximally worthwhile
and enlightening, I thought it'd be good
to put it in a greater context
and we'll see if this succeeds or not.
But basically, you know, let's just take a step back.
You're, so you're an AI researcher
and right now there's an AI revolution going on.
Right as we talk, you know, chat GPT's been the rage
and before that stable diffusion and many other things
and people are very aware now of the power of AI.
And part of it is that models are getting larger.
And part, what, what your theory does is to try to have a theoretical
grounding and analysis of what happens when neural networks
get larger in a very specific sense.
And a lot of your work builds upon studying random matrices
and nonlinear functions of it.
And so I think the way I wanted to organize this discussion
is to kind of think about your body of work in the context
of this concept of letting objects get larger.
So you have random objects that are, that are, have a certain size.
And as that size gets larger, things can simplify.
And so that's, that's kind of like the big idea
that I think we can sort of nest your, your work in.
And I think that's, that will be helpful
because people will have seen things like the central limit theorem.
And then things like random matrix theory
and then we'll eventually get to your work.
How does that, how does that sound?
Yeah, that sounds great.
Okay, great.
So I, I spoke in a bit.
Let me, let me actually kind of repeat that in writing.
So there's, there's a capital N, which I think will, which I will
let go to infinity.
And in this limit, this large N limit, let's say things simplify.
Okay, that's like the overall concept of what we're going to be talking about.
And the way we're going to pursue this route is by the following.
So first, we're going to review this, we're going to have a warm-up.
Where we'll, uh, re, review this large N limit concept in the context of the
law of large numbers and the central limit theorem.
Okay.
And, uh, here we think of N as the number of samples.
So these are classical results about what happens when you sample, uh, repeatedly,
uh, from a distribution and what happens when you take certain averages or, or
scaling limits thereof.
Then we're going to go over random matrix theory.
So we're going to go over random matrix theory, we're going to go over random
matrix theory, right, which is basically an N by N version of this.
You're going to have a random matrix that is the entries are random.
It's N by N and you're let N get large.
And then, uh, basically there are some very nice results coming, uh, uh, that arise
from looking at the eigenvalue distribution of these, uh, random matrices.
Right.
So that's, that's what's going to happen there.
Then we're going to look at tensor programs.
Oops, tensor programs, right?
And correct me if I'm wrong, but as I was thinking about this, um, I think the
tensor primes could be thought of as some kind of nonlinear compositional central
limit there.
Does that, does that sound kind of accurate to you?
Or is that, uh, is that an unfair kind of, uh, summary?
I'll say it's kind of like a nonlinear compositional, uh, large numbers.
Ah, okay.
Okay.
Sure.
Okay.
We, we, yeah, or law of large numbers, let's say.
Yeah.
Okay.
Um, and then what we're going to do is we're going to use your tensor programs,
um, to, uh, uh, well, we're going to get new proofs of random matrix theory results.
So that will validate, uh, sort of the value of the tensor programs.
And I think of course the real in some sense value of the tensor programs is what
it has for, uh, neural network training, right?
So implications for neural network theory.
Right.
So here we have an end by end matrix here.
You could think of N as the, the size or the width, the neural network.
Yeah.
And the common unifying theme is that, uh, in, in, in these different settings,
you have a random object, right?
You have some random variable.
You have a random matrix.
You have a neural network with some random weights.
And as the size goes to infinity, the analysis becomes tractable.
Right.
Yeah.
I realized actually before I, I, I kind of maybe took over a little bit.
I, I, I said all this stuff, but I should have let you describe what
tensor programs are.
Can you, do you want to say like your TLDR of what tensor programs are?
Cause I just sort of put this outline without actually letting you speak about it.
So what are, what are tensor programs and how did, how did you, what, what,
how did you come about this, this concept?
Um, yeah.
So, uh, maybe in a sentence, I would say like, oh, so there are two
parts, okay.
Maybe it's maybe not one sentence, but two sentences or something.
There are two parts.
Uh, the first part is kind of like this, um, the set of rules for expressing
computation, uh, which essentially, you know, matrix of location and edge wise,
like non-linearity functions or essentially it's just as like roughly speaking
on anything you can write in PyTorch or TensorFlow, at least like deep learning
computational frameworks, uh, can be sort of quote unquote, compiled down to this
kind of low, lower level language.
So this is the first part.
It's kind of like the formalization of the rules of the computation.
It's kind of like a, you know, like a Turing machine, you know, like just
kind of Turing machine formalizes the rules of computation for computation, you
know, that this is like formalizing the rules for computation or deep learning.
Um, and the second part is like this whole large number or essential limit aspect
of this, which is that when you like randomly sample the matrices in such
programs, like you can obtain like really general insights about what happens
when the size of the, uh, the matrices go to infinity in such a program.
So like it concretely in like for, you can, you can implement random
matrix theory inside of this framework.
And in this case, like the size of matrices are just the size of
matrices in random matrix theory.
Uh, you can also implement like all of, you know, deep learning inside this
language and in that case, like the size of matrices in the program correspond
to like the width of the neural networks.
Um, but, but it's like a very, you know, flexible language just like in the
Turing machines are very flexible in terms of expressing computation.
Um, okay.
So that's like the rough gist of, uh, what his programs are.
Uh, I can also talk briefly about like the history.
Um, but yeah, roughly speaking, uh, like when I, yeah, when I, when I
joined a Microsoft in 2017, like I was already exploring like, uh, the
behavior of random neural networks, uh, like, so, like so-called the
initialization.
So what that means is that like the neural network just have random
ways and you haven't done really anything with it.
Um, and then just try to understand his behavior.
So, you know, there's like a very well known behavior of such networks from
like 20, 30 years ago, uh, known as this, uh, neural network
Gaussian process correspondence.
So in other words, like, you know, a random neural network in the sense
of like the ways are randomly sampled, like can be, uh, can be shown
to be, uh, a Gaussian process in the limit as the width of the network or
number of neurons per layer, which is the width, this width goes to infinity.
Um, so this, so this is like a well known result, uh, from a while ago for
like, uh, you know, one hidden layer neural network.
Um, and then like when I was starting to do research, like there were
essentially some more, like, you know, activities in, in that area, in
terms of like deep networks.
And, uh, at the time, like most of the works were done by, like, you
know, like physicists, essentially.
So they were kind of very flippant about, you know, we're like, not rigorous
about like the ways they manipulated some of the equations.
And I think as a mathematician, I was kind of, uh, like not super satisfied
with that.
So my motivation at time was to, uh, you know, like to understand, like, can
I convince myself that this is all good, you know, like in, in any
circumstances, like the assumptions they made, like in terms of these, like
physics, like calculations are okay.
And so like driven by this motivation, I kind of like realized, oh, like all
these different things you do for these are different architectures that
people were writing about actually can be distilled into like this simple
form, like this low-level language, like, which is essentially like tens of
programs that like essentially has two instructions.
One is matrix multiplication and, uh, like in these entry-wise nonlinearities.
So it's essentially, as long as you can express your computation in these
two forms, like you can express, like, you know, Resnet in this form, you
can express transformers in this form, as long as you can try to express them
in this form, you can like kind of using kind of mechanical calculation to
calculate the, the behavior of the infinitely large neural network.
So that's how it came about.
Just like driven by motivation to understand the behavior, large neural
work, especially, you know, like kind of to, to, to convince myself that like
all the physics, the physics way of doing things is actually correct in some circumstances.
Okay, great.
Um, all right.
Why don't we actually just dive in?
Cause we've, uh, we've, uh, wax poetic for long enough.
Also, great.
Why don't you explain us the law of large numbers?
Yeah.
Yeah.
Okay.
So, so, I mean, the, the most trivial case of the law, large numbers.
Oh, let me just maybe write down this is a law of large numbers.
Oh, wait.
Um, right.
So, so the, the most trivial case, okay, maybe, maybe before, you know,
talking about exactly what it is, you know, like in, uh, lost
situations, you, you add up a lot of things, right?
And in general, you, you like sometimes you want to understand how, how does
this behave when you add up a lot of things together?
So, I mean, the most trivial case of this is like, you know, okay.
You like, you know, you, you suppose you buy one orange every day and you buy it
for like a hundred days straight.
Okay.
How many oranges are you going to have?
Right.
Obviously, you know, just, you just one plus one plus, you know, so on so forth.
Right.
And it's like a hundred of these.
And this is like a hundred.
So, I mean, in general, like the, the sum, you know, like the sum, if you have, uh,
instead of a hundred, you have, uh, end of these.
Then like, you know, you have, you know, the, the total is N.
So, in other words, if you have, you know, if you divide this by N, you just
get one, which is like kind of what you're adding every day.
I mean, this is entirely trivial, right?
Like, you know, if you're in kindergarten, you know, this, so like the law
of orange remember is just like really in essence, just like saying that it was
the slightly more complicated case where you like, you know, like, suppose you
put the coin every day and if, uh, you know, if the coin is positive, the heads,
you buy two oranges or his tails, he buys zero oranges.
Right.
So like now, you know, instead of buying like very certainly one orange every day,
you, it's just, it's just a classic thing.
It's like a random thing.
And you're buying on average, you're buying one orange every day.
But so now you can ask, okay, like, if I have like this, um, I'll just call
this X for the number of orange.
Um, on, on, uh, on day one and day two, so on, so forth.
So like this is like the orange number, number of oranges.
But on day a hundred, for example, and I do the same thing and divide by N.
Then law of orange remember says you can ignore all the noise.
Right.
Like it's, it's the same.
The answer essentially will be the same when N is large as if you're just buying
one orange every day where one is equal to the, you know, expected number of orange
you buy every day.
Right.
So this, this will be one.
But if again, like the condition here is that like, you know, um, like two oranges.
Two oranges or zero oranges with probably one half each.
Mm hmm.
Right.
Well, I guess you didn't really mean equals to one.
I mean, it's equals to one with some, what's, what's something else.
So like, uh, or maybe I'll, I'll, I'll say this, uh, or okay, I'll just say this, uh,
as n goes to infinity or in here, this is like not a hundred, but N.
Right.
So, so, so again, like, in other words, law of orange remember says, if you're
averaging things, if you're adding like a hundred things in divide by, by dividing
by a hundred, then like when, when this hundred is instead of, it's not hundred,
but like a thousand or 10,000, then like, you can forget about all the noise, you
know, in each, each, uh, you know, like random variable here, you can just look
at the mean and that's the only thing that matters.
Right.
So, um, like,
Yeah, or maybe it's not so much, uh, maybe, uh, maybe being a little pedantic,
but it's not so much noise as variance, right?
Cause it's like, there's zero oranges and two oranges.
I, I don't know, uh, uh, I'm not sure what to think of as noise, but the point
is that the, uh, if you average over average over those two, two with respect
to the underlying probability measure, which we said was a half, uh, for each
case, then it's the one orange in expectation.
What, what, what's, what's, um, uh, the thing, the thing is that there's
fluctuations, right?
Like there are, like there is, there is, there is one run in which you actually
buy two, all two oranges every day.
And there is another run in which you end up buying no oranges every day, right?
But those are relatively rare, right?
And it becomes more rare as n goes in infinity, right?
So there's basically the fluctuations, the variance goes to zero and gets large.
Yeah, that's right.
Yeah, yeah.
So like if you want to formalize this, the rigorous version of law, large numbers
is that leaf, like, you know, X one, X two, and so on and so forth, um, are all,
uh, all like, I'll say ID for, in case people don't know, uh, independent.
And identically, uh, distributed.
Yeah, we'll assume our readers know what that is, but, uh, basically it's, you
can think of the X size as, as independent coin flips where the coin has the same
distribution every time you flip the coin, right?
Yeah.
Yeah.
That's a quintessential example.
Yeah, they're all ID, uh, then.
Um, you know, like, um, I'll just write this out, then like, you know, taking the,
the sum of the initial sequences and then dividing or like averaging them out,
uh, this will converge, uh, to the expectation of like any one of these,
um, as n goes to infinity.
And if you are more pedantic, more, more pedantic, you can like say, like,
what exactly is this notion of convergence?
Like there are many of these, like, you know, like almost sure that's the
strong law of wash number.
If you, if you say it's almost sure.
Um, yeah.
I mean, that's, that's pretty much it.
Right.
Like this body has some intrinsic feeling for this kind of thing.
Like when you average on large number of things, you get the, the mean.
Sounds good.
Yeah.
Yeah.
We were, it's, uh, it would be, uh, too far field for us to go into, uh, the
different notions of convergence, but, but intuitively it means that with,
with, uh, you know, uh, near certainty or, uh, you know, or, or certainty,
that it's kind of caps encapsulate it with the English language, but with
certainty, when you flip your coin, when you sample all these variables,
ID and average them with certainty, you will get the expected value.
It's sort of like thinking like if you pick a random real number with, uh,
with probability one, you're going to get an irrational number rather than a
rational number.
You could kind of think of it that way.
Like, like, I guess technically you could get a rational number, but it
just, it's just never going to happen in some, in some formal sense.
Again, the English language is too coarse to capture the notion of measure zero,
but you can think of it that way.
Yeah.
Yeah.
Well, yeah, I think you can also be like more quantitative and you can say that
like essentially for any finite, but large N, like the probability that this
average, this empirical average is like very far away from this
expectation is small.
The probability event is small.
Exactly.
The count is like a calculus Epsilon Delta notion of, of making this rigorous.
Yeah.
Okay.
But anyways, okay, great.
So this is the law of large numbers.
Uh, did you want to say anything else about it?
Or should we go on to, to the central limit theorem and how, uh, how it's a
different kind of large, maybe I'll just say that like, um, but when, eventually
when we go to tensor programs, like the, the key, the master theorem, which is
the name of the kind of the one theorem that you need to know, uh, in that theory
is formulated in this way, like kind of like when you take average of certain
things that are actually not ID and like, in fact, they're going to be in
general correlated.
It tells you what the, like the limit of this average is.
And so it's, it's formulated in a way that's most akin to the law of large
numbers, your average, a bunch of things that are correlated.
Like, how do you think about this average?
Because now you have correlation, like, you know, how do I, should I assume
the correlations is just like, you know, zero or like, does it, you know, cause
the limit to differ?
So this is like the, one of the, the most important and the consequences
of tensor programs tells you when like correlations arise from this kind of
like matrix multiplication, entry wise non-linearities, like, how should you
think about this?
Yeah, just, I mean, just to make this more clear.
So in the, in the, okay, let's not get too deep right now, but I just
wanted to unpack what you said and then let's just move on and it'll
make more sense to come back.
But basically the key thing in the law of large numbers is this ID assumption.
And in the tensor programs, we're going to replace that with, well, they're
not ID, but they're going to be sort of the XIs will be non-trivial functions
of the previous ones, right?
So that's what we meant earlier by non-linear compositional, you know,
central limit theorem or law of large numbers.
Basically the XIs can be non-linear functions.
Of each other.
Sure.
Yeah.
And so, so basically that's, that's where the heavy lifting comes about, right?
Yeah.
That's right.
Okay, right.
And the XIs are basically the hidden, hidden units in a neural network.
That's, that's, that's the point.
Yeah.
Okay, great.
Yeah, yeah, I'm actually great that we, we, we covered it this way because
then already here we can kind of get a feel for what tensor programs is about.
That's right.
Yeah.
Great.
Great.
Yeah.
Okay.
Um, all right.
Are we ready to go to the central limit there?
Yeah, let's do it.
So central limit theorem is a different, uh, theorem from the law of large numbers,
uh, but it's, you know, very intimately related.
So, um, you know, so in the, in the, in the, you know, the, the scenario we had
before we had these random variables, you know, like X1, X2 and so on.
They're all ID.
Um, but, you know, assume, assume, you know, the, the mean is zero.
So if, you know, mean of, uh, expectation of X1 is zero, then like, you know, the
law of large number says that, um, that this, this mean over endings, you know,
like the limits and goes to infinity, this will go to zero.
This mean goes to zero, right?
Because we have assumed the expectations zero.
So this scenario, you know, is, uh, this scenario that, you know, when you average,
you know, uh, these things, these random variables with zero, uh, expectation,
then you get the zero in the limit for large number of samples.
So this is kind of, you know, like, uh, consequence of the law of large numbers,
but it's also somewhat not interesting when things go to zero.
So the central limit essentially tells you, like, what is it, what is the right
skating here to obtain a non-trivial result and what it is the non-trivial result.
So, so central limit theorem says the following that you said that if instead
of, uh, dividing by n, you divide by square root n instead, then, you know,
taking the limit, uh, gives you a Gaussian distribution, um, right?
I'll just say, like, uh, sigma square here where, uh, expectation x one square
equals sigma square.
This is the variance of x one, right?
Okay.
Great.
Yeah.
So, um, yeah.
So, so what does, you know, this, uh, imply?
So the one way to think about this, uh, in connection with a lot of large
numbers is that, uh, you can roughly understand a sum of, uh, n, i, d things
with zero mean, uh, and variance sigma squared as roughly equal to n, uh, well,
actually, sorry, like, let's say, let's say, um, let me just say this, like,
expect, let's say expectation of x one equals mu, which may be non-zero.
And then expectation of x one squared equals sigma squared.
So this is a more general scenario than the one we assume just now.
Um, but back to the situation, right?
Like if you're adding n things, uh, each thing being an independent copy
of its random variable with mean view and variance sigma, then you expect
the sum should look something like, uh, n times mu plus square root of n
times, uh, sigma, uh, times, uh, times a Gaussian, like, uh, like a Gaussian.
So it's kind of like, uh, asymptotic expansion, you know, if you're familiar
with that, for example, from physics, how you should think about a sum, right?
Like the dominant term is the mean, the scales linearly with a number of
samples and then the sub dominant, the sub leading term, the next lead sub
leading term is, uh, proportional to like a Gaussian with scale square root of n,
where n is a number of samples, right?
And they're like in some, some low order terms as well, low order terms.
But, um, but like the top two terms are the most famous ones.
So, you know, like this, so this is, this term is the law of large numbers.
And then this term is the central limit there.
Mm hmm.
Mm hmm.
I see.
Um, yeah, maybe, maybe just what, uh, is it, what, what, yeah.
No, no, no, no, it's fine.
No, uh, maybe I was going to say one useful fact to recall, uh, readers of, of, of, uh,
who've gotten this far already know it, but it's good just to recall it, which is
that if you have, uh, fact, if X and Y are independent, then the variance of X
plus Y is the variance of X plus the variance of Y.
And then also the variance of a scalar times X is C squared variance of X.
So sort of that sort of, sort of justifies your, your approximation here in some sense.
Of course, the first term is just the linearity of expectations.
And then the second term essentially follows from these facts I just wrote, right?
Because if you, some n independent things, then, uh, the variance will be n times the
variance of the individual, uh, random variable.
And then that's how you get the second term because the square root of n changes the
variance of, uh, yeah, the square root of n times sigma changes the variance of the
normal distribution by the square of that, which is n times sigma square square.
Yeah.
So, so yeah, yeah.
Right.
So here I think I want to point out like one basic intuition, um, the, the, that I
think we'll come back, uh, later.
Uh, I mean, this is, this is all like a very, very basic intuition that, you know,
like, like, you know, it's, it's not like, uh, the key intuition behind this
appearance, but I want to point this out.
It's just like a very basic thing that, you know, I think benefits everybody if, uh,
they, they can understand it, which is just that like when you add like random
things together, uh, when the, the fluctuation fluctuations are independent,
like, uh, they cannot, in some sense, they cannot conspire, right?
Like to, to push in the same direction because the fluctuation essentially has
no information between like, you know, different, uh, different instances of
fluctuation, like the fluctuation X one doesn't know anything about fluctuation
on the X end, right?
Um, as such, they cannot like conspire to push in the same direction.
And so there's a lot of, which is why that when you add them up instead of
skating like n, like when we add n things together, instead of skating like
n, which is the case, if they can conspire to kind of push in the same
direction, like they don't know about each other.
So they can only like kind of, you can only expect them to push away from zero
by something smaller than n.
And like the, the, the, you know, the basic fact of like how the variances
add essentially imply that the smaller number, less than n is something
like square root of n.
Yep.
Right.
Exactly.
It's like a very basic intuition that I think everybody should really
understand.
And like, yeah.
So basically like law, law, remember it says, okay, the, the mean is the only
thing that can conspire to push in the same direction.
And the essential limit says, if you cannot conspire to push in the same
direction, you get square root of n behavior.
Exactly.
Exactly.
And that follows from the fact that I just wrote, because when you add
independent things, you get growth like n, but if you add n of the same thing,
which is what the second thing I wrote variants of C times X, let X be n,
then you get n square, you see, so you get another factor of n.
So that's the second case is the conspiracy case.
The first case is the independent case.
So here, I mean, I do want to point out, right?
Like when, whether the mean is zero or not, zero gives you like kind of
two behaviors that the two kind of like canonical behaviors, right?
Like when the mean is non-zero, like the, in some sense, the correct
way of skating the sum of n things is to divide by n, right?
Then you get like a non-zero and non-infinite object, which is the
expectation of the random variable.
But when the mean is zero, like the right way of skating it to get an
untreated thing is to divide by square root n, right?
And this, like this, this, uh, kind of like, uh, change in the right
skating behavior, depending on the characteristic of the random variable.
So this kind of, uh, you know, like thought process will come back later when
we talk about, you know, how do, like, how does disciplines apply to like
in training large neural networks?
Because in, in that case, like, there are also different, you know, ways
of skating your neural networks, kind of like n or square root n here,
depending on the different scenarios you're in.
And that could be correct or not correct in different scenarios.
And, uh, like one of the main big contributions of disciplines is to
kind of clarify and to, in fact, derive the correct, uh, skating, um, in
general, so that you have like, you know, nice payoffs when you train large
neural networks, that kind of, like, from a more empirical perspective,
you'll be really hard to guess the right skating.
Yeah, indeed, indeed.
So Greg, so I guess, um, as we were discussing, uh, before, um, it's
instructed to go over a proof or a sketch of a proof of the central limit
there because these ideas will show up again later in particular in the
random matrix theory, uh, subject, right?
Okay, so let's, let's do that.
Uh, so again, like there are many different proofs of this, but, um, one
way to go about this is to first use the fact that, um, like, you know, two
distributions, I'd say, like, um, P and Q, uh, are equal, uh, if and only if,
um, their moments are equal, where, like, by moments, I mean, you know,
like, uh, expectation of, you know, X to the, um, the, like, K power where X is
drawn from P or Q.
So like, you know, if P is, then it's a P th moments and Q, then it's a P,
Q's moments for, for different Ks.
So these are the moments, right?
So, so if this is true for all K, greater than equal to one, then
the distribution is equal, like under some assumptions, under some, like, you
know, regularity assumptions.
Sure.
I think, maybe you're going to already say it, but I think the point here is
that the space of polynomials is dense in the space of continuous functions.
And so if you could show for all the moments, you can show it for any
continuous function and, and, and intuitively if, if a probability
distribution integrates against a continuous function, uh, if two
probability distributions agree on all continuous functions when you integrate
against them, then they have to be the same.
I think that's fairly intuitive, right?
Yep.
Yeah.
So, so essentially then like to back to CLT, right?
Um, the point is that if you can show that we just, you know, suffice to show
that, um, oops.
Cool.
Now, when you add, um, these n things and divide by a square root of n, uh, and
then you take the K tower and then you take the expectation, then this is
going to converge to the expectation of X drawn from a Gaussian.
Mhm.
Also, I'm going to assume that the variance is one from now on.
That's sure because you can always scale it to be so.
Sure.
Um, yeah.
So, so, so this, so we have reduced the problem to, to showing a specific, you
know, convergence of, you know, expectations of this powers of this sum.
Okay.
So this is the first step.
We have reduced our problem to specific, uh, to showing specific, um,
convergence of deterministic, uh, quantities.
Okay.
So then the second step is to actually, you know, compute these quantities, these,
uh, powers, uh, and take expectations and see, like, what, what do you expect them
to be?
Okay.
So, you know, like when you, so in general, right?
When you take a sum, I'm going to use, um, index, uh, alpha and beta, uh, here.
So when you take a sum of, uh, x, alpha, um, and then you take this k power, right?
We can essentially expand this as the sum of x, alpha one to x, alpha k over all
like sequences, alpha one to alpha k, right?
Where, um, each of them range from one to n, right?
Um, right.
And, you know, well, and then when we take expectation, because of linearity of
expectation, you just, uh, you just get, uh, you can push, you can push this inside.
This expectation, right?
Um, great.
So now, uh, again, the goal is to kind of understand how this, uh, the sum behave
as n becomes large while a k stays fixed.
Okay.
So, so just remember this psychologically, like the, the number of indices are
fixed in our scenario, but, uh, their range can become larger and larger.
Okay.
So now the next step, we're going to use this to observe that because we have assumed,
um, because expectation of x, alpha is equal to zero.
Um, these, like these kind of, uh, expectation products of x, alpha's, they're
going to be zero if any x, alpha appears only once in the, in the product, right?
So for example, um, like, you know, x one, x two squared x three to third power,
this is going to be zero, but like x one squared x two squared is like, in
general, not going to be zero, right?
Like, I'll just say like this, right?
And the point is the problem is this guy has a power one, right?
And you're using the fact that by independence, independence means that
the, uh, expectation of the product is the product of the expectations, right?
So, so the point is, is, is, uh, yeah, you're just, actually, this is true for,
uh, yeah.
Okay.
Yeah.
So let me just complete this.
Yeah.
And then this goes to assumption.
Yeah, exactly.
Exactly.
Yeah.
So basically disjoint indices in the, in, in general, you can factorize, uh, over
the disjoint indices, essentially just, yeah, yeah, yeah.
Okay.
Yeah.
Great.
So, so what this implies is that in this big sum here, um, we only need to worry
about, uh, the summands where each alpha appears at least twice.
Okay.
Um, okay.
So, so let me maybe just write this down.
This is, I think, yeah, I see where this is going now.
Okay.
Uh, I, I haven't, I haven't, uh,
yeah, I went, I didn't study this proof in preparation for our talk, but I,
I could see, I think I see where this is going.
You can get a little nice surprise.
Okay.
All right.
I don't want to steal the show, but okay.
I, I, I, I, uh, appear greater than or equal to two times.
Okay.
Okay.
Now, so this is our like first step of, you know, just eliminating some of the
fluffs we don't have to worry about.
And then the next step is, um, to understand like what is the, like the
component of the sum that will dominate when n becomes large.
Okay.
So, so for example, you know, like in these sums, you can have this scenario
where, you know, you have X one squared X two squared and so on and so forth.
X, um, K over two squared, for example, you can have, you can have this kind
of behavior, or you can have like X one to the cube X to the, to the, to the
cube, you know, so on and so forth, or X cube to the K over three, something
like this, or we can have a mixture of these, you know, we can have like X
two, uh, X one squared X two cube, you know, so on and so forth.
Um, and, uh, the point here is that when you compute, uh, when you calculate
the number of terms of these kinds, like, so for example, this kind is
when like every exponent is two and this kind is every X one is three and so on
and so forth, when you like look at how many such terms there are, then you
will, you'll notice that like this, this kind where every exponent has exactly
is exactly two, the number of such terms will dominate the total number of
terms as n becomes large.
Yep.
Yeah.
Um, so in other words, you can kind of forget about all these
other kinds of terms as n goes to infinity because they just contribute less
and less, uh, as n becomes large.
So we can say that I like, this is, um, kind of, uh, asymptotically equal to
the case where, uh, each alpha, uh, I appear equals to two times.
All right.
So, so we're, okay, let me, let me write it another way because in this
case is simple.
Um, something like it's alpha one squared to alpha K squared over all alpha
one to alpha K.
Okay.
Let's just, let, yeah, let's just back up a little bit.
I think the easiest way to see what you're saying is, you know, look, just,
just treat this as a counting problem, right?
And so in the first term, you have the fewest constraints in the sense
that here, when you, when, when you have the lowest possible non-trivial
power, which is two, cause we, we, we don't want powers of one, right?
In this case, the number of such terms is basically n, uh, uh, yeah, you have
to choose K over two elements from, uh, capital N, right?
Because once you've chosen those, uh, K over two elements from a set of N
elements, in this case, indices, you've already constrained them to all have
power two.
So the number of such terms is just this N choose K over two, whereas as you
just wrote, if it was all three, then it would N choose K over three, which is
a smaller power of N, right?
And by the same logic, if you had higher powers, they basically kind of have
each, each power is like kind of, you could think of it as like another,
another co-dimension in some sense in the space of all possibilities, right?
And so, so there's a leading term and everything has kind of positive
co-dimension, right?
And so, so only, only the lowest powers, non-trivial powers survive.
Yeah.
That's right.
Yep.
Yep.
Yeah.
Great.
So now I'm going to kind of just stay on this page.
I'm going to erase all of these.
Sure.
Um, okay.
So now our mission is to calculate what exactly is the sum.
So first of all, like, you know, by symmetry, uh, you know, like,
you can evaluate this expectation and the expectation is just, uh, by independence,
right?
Is you can just factor, uh, pushing the expectation into each square of X,
alphas, and because the, the variance of each, uh, X is assumed to be one,
this is just one, right?
And so the correct combatorics here is, um, uh, N truce.
So it's like a, okay.
So the, the succinct way of writing this is, um, N truce, uh, the multi, so this
is the multinomial coefficient where you essentially choose pairs, um, where
there are, uh, K over two of these, uh, uh, things.
And then times, so this is essentially counting, uh, yeah, like this is kind
of counting like how many ways things can coincide.
And then there's another, uh, one of N truce, uh, K over two.
So what we just showed was the expectation of X one plus N divided by
square root of N to the K power equals, you said it was K minus one double
factorial.
Yeah.
So like the way I use double factorial is a bit different, but, but anyway,
you can just, you know, what does it mean in just this?
All the, all the, all, yeah.
Yeah, exactly.
Yeah.
Okay.
Okay.
And the point is that this is also the same thing as expectation of X squared
where X distributed as a, so this is for K, K even, right?
Yeah.
And for a K on zero.
Yeah.
Of course.
Yeah.
Yep.
Okay.
Uh, great.
And so this, this shows that, uh, Yeah.
So yeah, if you want to get technical, this is in distribution.
Yeah.
Yeah.
Sure.
Yeah.
We're not that fancy.
Yeah.
Great.
It's white and green.
Yeah.
Yeah.
Okay.
Uh, okay.
Great.
So.
So, okay.
So where do we, where do we go from here?
All right.
So, so we can talk about, you know, like something more fancy now, which is, uh,
instead of looking at like a sequence of random scalar random variables and
you're taking average, now we can look at like a random matrix.
So, uh, you know, like if you, if you really read into random matrix, you can
think of it as, you know, like using fancy words, like a non-commutative
version of low, large numbers, essential limit theorem.
Um, again, this is getting maybe a bit too fancy, but like, uh, this is meant
in the sense that random matrix, like matrices, uh, under multiplication,
they are non-commutative versus, um, in this case, uh, when you, when you
look at like, you know, x1, x2, xn, you can, you can, when you look at it
from a random matrix angle, we can think of it as just like a diagonal matrix
where the diagonal entries are x1 to xn.
Um, but in any case, I, let's not worry about this.
Uh, uh, let me, let's just like kind of set the stage for what, what do we care
about in this random matrix setting.
And, um, and then like the deniology between the random matrix stuff and the
CLT, for example, will become more clear as we try to apply kind of the same
moment methods and the same ideas there.
And then we will, from there, we'll also see, you know, like the classical
methods of, you know, uh, understanding these things will look very much
like what we did for essential limit.
Um, but we use intensive program is like a very different way of thinking
about things and, uh, and then from there, we can like, you know, uh, use
as a jumping point to see like how this different way of thinking about random
matrices, uh, translate really well to neural networks.
So when we go, uh, to random matrix theory, um, the, the kind of the, the, the
thing we care about here in out of this to the essential limit scenario is that,
um, we want to understand like kind of the eigenvalue distribution, uh, offer
like a large random matrix.
Um, and the setup is typically something like this.
So, um, yeah.
So if you have a, you know, I'm going to draw this picture, a matrix as m by n.
So, yeah, so like a very common setting is when, uh, this matrix, let's
call, uh, let's call this a, a is a symmetric.
And, um, and then a's entries, let's say I just like ID, ID Gaussian, uh, subject
to this constraint.
So essentially what this means is that like, uh, on the diagonal, they're all ID,
but like, you know, otherwise, like on the upper diagonal, upper triangular portion
is ID, but like the lower triangular portion is exactly the flip of that.
So, so that's what I mean by ID subject to this symmetry constraint.
Um, like, let's say zero one or something like that.
And the question, uh, like classical question asked in random matrix theory is,
uh, what is the, how does the eigenvalue, um, distribution?
Look like, uh, for a as, um, and then ghosting affinity.
So this is the, like one of the key questions.
And just to know that, um, because like we have some, the matrix is symmetric by,
you know, the typical linear algebra terms, uh, the eigenvalues are real.
So we're looking at it like, you know, for finite matrices, we're essentially
going to look at like a histogram of, um, eigenvalues on the real line.
So this is like, say like negative one to one or something.
And, you know, so like, um, the eigenvalues, you're going to, you're
going to rescale the eigenvalues, right?
Maybe you should say that.
Yeah, yeah.
So, yeah, maybe, maybe let me not like be very specific about the scale here.
Okay.
Okay.
Yeah.
I just say, so essentially we're going to look at, you know, like, um, on the real
line, uh, like, you know, a histogram for, you know, any, like, you know, finite
matrix, you can always kind of look at the distribution like a histogram.
And, um, when the, the matrix becomes, you know, infinitely large, you have
like infinite number of, um, eigenvalues and that forms like a continuous
distribution, right?
Yeah.
I guess when you say histogram, basically, I mean,
uh, uh, with, with, uh, you know, it's, I say it's with measure one, you're
going to have matrices with distinct eigenvalues.
What you're doing is actually binning.
And then in those bins, you can have multiplicity, right?
That's right.
Yeah.
I'm, I'm binning it here, but like, so like, if you don't bin, you're just
going to look at like a bunch of deltas or something.
Exactly.
Exactly.
So, okay.
So yeah, just to be clear, the reason why you have a histogram is
because you've been, but it's okay.
Okay.
That's just, I just wanted to clarify that, but okay.
Okay.
Let's keep going.
Yep.
Right.
Oops.
Um, great.
Okay.
So, so how do we answer this question?
Right.
Like, you know, major C's is like a much more complicated than a sequence numbers.
Um, but it turns out like you can use some of the same tricks that we used
previously just now for central limit theorem.
So with movement method, the point here is that, um, when you look at the
distribution, um, let's say, um, P of A, this is defined to be the distribution.
Of, uh, eigenvalues of A, then, um, again, by the usual linear algebra theorems,
you have that, um, the, like, uh, X to the K where X is drawn from PA.
Um, uh, yeah, this is like for, for any, like, you know, like deterministic
even matrix doesn't have to be random, like we said.
Um, is equal to the trace, um, of, uh, A to the K power divided by, uh, N, where N
is the size of matrix again, like the picture is this N by N, A is N by N.
Uh, sorry.
Don't you have to, uh, rescale A somehow so that you actually get a limiting distribution?
Oh, yeah, yeah.
So this is just for finite N, finite, finite matrices, the finite N, N is finite right
now.
I'm not, I'm not going to continue yet.
This is just like, this is true, like for any finite.
Okay.
Okay.
So yeah.
So let me just say, oh, I see.
Sorry.
So P of A is the, P of A is the, um, empirical measure on the eigenvalues.
Is that right?
Yeah.
Okay.
So I think let's just write that out because it's, uh, you know, we're going very deep here.
So, so, so sort of the, so another way of writing a P of A is the so-called empirical measure.
I guess for the eigenvalues.
P of A, which basically means it's the measure such that you have one over N and you have
a direct delta for each, uh, eigenvalue.
Yeah.
So you need a one over N to that.
It's a probability measure.
And then you just, uh, you have a, you know, you count one for each eigenvalue.
Yeah.
Yeah.
That's right.
Okay.
Good, good, good, good.
Okay.
Okay.
So, so the, yeah.
So the point here is that like you can manipulate some like gross characteristic in the sense of
the moments of the eigenvalues by taking powers of the matrix A, uh, and then taking the trace.
Okay.
So this allows you like this, this trace of, you know, powers of A directly gives you the
moment of the empirical distribution of the eigenvalues.
Okay.
So, so this is how we're going to like use moment method through the trace of the powers of the
matrix in the, in the, like non matrix case, you had the central limit there.
And that says that distribution of the sum of one trip things with zero mean and independent
distribution, like becomes like a Gaussian, right?
And when the number of things is large, uh, in this case, we're asking, okay, what does
the eigenvalue distribution look like for a large matrix?
And, uh, the answer is not a Gaussian distribution.
Uh, you know, like I see you maybe would guess if you're coming off from the central limit
perspective, but actually the limit is a, uh, a semi-circle distribution.
So let me kind of just write this down.
So like CLT, right, says, um, that, um,
this, uh, yeah.
So CLT says, you know, this will converge to like the distribution of, um, this scale
sum or conversion distribution like this, which is like a, a Gaussian with zero mean
and, you know, some variants.
And then like this, this thing, uh, called a semi-circle law.
Says that, um, if you have like this in a random matrix and by N, and then you take
the eigenvalues, I'll just call it, do it, call it like this.
Then when N is large, the shape is a semi-circle or this is zero.
Okay.
So, so like the, the limiting distribution looks different, right?
Sorry, but here, here now you better, uh, divide by some power of N to get your limiting.
Yeah.
But, but I didn't put a scale here.
I just say semi-circle shaped, right?
Which is still true.
I just said the scale is kind of going to Fenty.
If you don't, yeah, sure.
Sure.
Okay.
Okay.
Fair enough.
Okay.
Yeah.
I mean, like, you know, I don't, the point here, I want to make it just so the shape is
different.
Okay.
Okay.
Um, but never let the last, like even if the shape is different, uh, you can still
use the moment method to get what you want.
Um, and I'll just briefly sketch how this would go, the argument.
Okay.
So just like in the central limit case to prove the semi-circle law, you want to show
that, um, for this, uh, for this, uh, random matrix, random A as before, you want to show
that the expectation now, like taking over A of the trace of A to the K, um, divided
by N, um, and, uh, also I'm going to scale this additionally, uh, by, uh, N to the K
over two.
So, so I'm going to show that like this, the skilled version of the trace of the power
of K, uh, will converge to, uh, the, um, the century, you know, this, where X is drawn
from the semi-circle.
Mm-hmm.
Okay.
Okay.
And, um, just like before in the CLT case, like this, this trace of A K, you can actually
expand it as a sum.
Like before it was very simple, we just expanding, uh, a power of a sum of scalar
things.
Now we're expanding powers of matrix and the trace of that.
Uh, but you can still, uh, expand this and essentially you're just going to get
something like this, uh, alpha one.
So you're going to get a product of entries of A, uh, where you're going to have
something like A of alpha one, alpha two times A of alpha two, alpha three, so on
support, and then A of, um, alpha K, alpha one.
All right.
So like this thing loose back.
Yeah.
Yep.
Yep.
Um, great.
And then, uh, the, you, you want to take expectation of this.
All right.
So, so now it looks, you know, some somewhat familiar from as before.
Uh, and the task now is to figure out like, okay, what is the dominant term here?
Uh, and as before you can like, you know, see that because, you know, A's entries
have zero mean, like if, you know, any of the A, alpha one, alpha two appear once
exactly, then the whole term is going to vanish.
So the only things that are interesting are terms, products where, uh, each A, alpha
one, alpha two appears two or more times.
And then just as before, uh, the dominant terms are exactly those where each A,
alpha one, and alpha two appears exactly twice.
Right.
And, uh, whereas before there was like a, some like easy combinatorial counting,
which, you know, we can still get wrong if we're not very experienced.
But, but, but here we're the combinatorial, uh, description becomes, uh, graphical
in the sense that like you can think of like the, the, the sequences, alpha one,
alpha two, and so on and so forth, uh, as like, um, choices of vertices on a graph.
So, so the graph is a fully connected graph with N vertices.
And then like, you know, each alpha I is a choice.
It comes from, comes, uh, corresponds to one of these vertices and each A, alpha
one, alpha two corresponds to an edge between, you know, alpha one, alpha two.
This is alpha one, alpha two.
And the fact that like, you know, the, the, in this product, like, the last
terms, alpha K, alpha one, it looks back means that like essentially you're
looking at the structures you're looking at are like paths, uh, through the graph
that like eventually, you know, looks back, or in other words, you're looking
at cycles, uh, in this graph.
And, you know, because we observed that like any A, alpha one, alpha two has
to be pure, like exactly twice for, for the most dominant terms.
Like you're also looking at graphs that kind of like, or cycles that, you know,
go through each edge exactly twice.
Right.
So, so maybe using a different color here.
You know, for example, like the, the, the, the, the edges, the cycles you really
care about, for example, could be something like this.
Uh, okay.
So it looks back like this and maybe it goes here, comes back here and then
looks back and looks back.
Right.
So this is like a particular instance of a cycle where each edge appears exactly
twice.
And, um, from there, it's, you know, accounting problem of like, how many are
there of these kinds of cycles?
And it turns out it reduces to something that are well known from
combatorics called Cata numbers, Cata numbers, like from the, like the region
is saying, um, and, uh, yeah.
And from there, like essentially you're pretty much done because the Cata
numbers are precisely the moments of the semi-circle distribution.
Oh, I see.
No, okay.
Exactly the moments of the Gaussian distribution.
I see.
I didn't actually, let's just take a step back the, the, the, the right-hand
side of this, right?
Semi-circle law means that the probability distribution is, is proportional
to what one minus X squared, I assume, right?
Uh, yeah.
Right.
Okay.
So actually you can, uh, so if I worked hard enough, I could, I could, uh, I
could perform the moments of that distribution by hand and like just doing
some, some, some substitutions.
Pretty straightforward.
Is that right?
Uh, yeah.
Yeah.
Okay.
You can reduce it to like some recurrence equation.
And then that's like the recurrence equation of Cata numbers.
Ah, okay.
Okay.
Cool.
I see.
Okay.
Cause I mean, there are many definitions of the Catalan number.
I, I've never seen it defined in terms of moments of the semi-circle law.
I usually think of it as a more combinatorial, uh, definition, but okay.
I mean, uh, okay.
Great.
So, so the right-hand side, this essentially gives you a proof of that fact as well,
like this construction because you can, sure, anything that satisfies the
recurrence of the Catalan numbers are the Catalan numbers.
And okay.
So you just show that the right-hand side satisfies that.
Okay.
Great.
Okay.
Great.
Yeah.
I'm, I'm, I'm satisfied.
Okay.
So, okay.
So, okay.
So the point of, of, of, of this is that for both the central limit theorem and the
semi-circle law for random matrices, the moment method, uh, is tractable enough that
you can do it by hand, uh, essentially.
Yeah.
Yeah.
So, uh, let me also just kind of, uh, fill in some details we missed here, uh,
which is that like how, what's the right skating for the matrix to get the
founded, uh, distribution in the end.
So like in particular, what I mean is like, what is the scale?
What is the scale of this matrix?
Like what is the entry?
So earlier we were saying like the entries are standard options for the variance one,
but, uh, but, uh, the correct skating to get, you know, a, if you want something
like this, like, uh, think negative one to one, I forgot the exact
sale, it might be negative two, but, but let's say negative one to one, then you,
you need, uh, the entries, the entries should be something like Gaussian or zero
mean or variance one over N.
So in other words, the, the typical size of each entry is one over screw to N.
The standard deviation is one over screw to N, the variance one over N.
So that turns out to be the right skating for you to get like kind of like a finite
non-zero, uh, distribution as a limit of the eigenvalues.
Yeah.
And that, that already came from, from, uh, looking at what you wrote here,
because basically you see that if you absorb a square root of N into the
denominator to eight, then that, that gives you this, uh, variance of one over N.
Yeah.
Yeah.
Yeah.
Okay.
So, so in a sense, you know, it's, it's kind of like analogous to central limit
where, you know, there's a square root N here and essentially here where
multiplying the mid, the matrix by square root N on bottom in the bottom.
Okay.
Great.
All right, Greg.
So at long last, let's get to your tensor programs work.
Why don't you, um, tell us what it's all about and yeah, let's see.
Right.
Um, so, um, maybe let me, um, start by, start from the random matrix angle and,
um, let me motivate this by kind of continuing from this, uh, moment method
discussion, right?
So earlier, um, maybe let me, let me use a different color.
We use a white color.
So earlier we're talking about how to compute it, the moments of the
eigenvalue distribution, uh, it suffices to compute, um, the K power, uh, of a
matrix and its trace, right?
And, um, so the, the point, uh, that I'm going to make right now is that this
operation is this quantity that you want to calculate can be written as, uh, kind
of a series of like vector matrix multiplications.
Okay.
And the way it works is like this.
And so, so the point of this, you know, going forward is that this set of
computation can be attracted into a more general framework of tensor programs.
And then I'll talk about that framework and then I'll kind of specialize
back to this case at the end.
So, okay.
So first of all, let me just express this trace, uh, as this, uh, this
expectation, uh, which is very trivial, but it's very useful.
Where, um, V is like a standard Gaussian.
Okay.
So, so what I'm doing here is like here, I just assume A is at some, any
deterministic matrix and to compute, you know, the trace of A to the K or any,
any other matrix instead of A to the K is fine too.
Like the trace is just the same as the expectation of V transpose
times this matrix times V.
Right.
So like this is, this is a scalar.
Like, because we're hitting this matrix by vectors on both sides.
So you get a scalar back.
And the point is that when you take expectations, essentially, like the only
contributions are going to be from the diagonal of this A to K.
So, so that gives you the trace.
Okay.
All right.
So this is an entirely trivial trick, but what this allows you to do,
is to write this now as like a series of matrix multiplication.
So, so I can define, you know, like say, um, I'm going to do this.
So I'm going to write, um, let me call this, um, I say like X, I'll call this, uh, X
one equals a K times V and then X, sorry.
A times V X two equals eight times X one and so on and so forth.
Right.
So then X K, I'm going to define this to be eight times X K minus one.
Right.
So, so in effect, uh, the, um, you know, X K is essentially a K times V.
Right.
I guess I don't need quotes here.
So, um, a K times V.
Right.
And, and the trace of a K is just equal to expectation over V of X K, uh,
inner product with the right.
Okay.
So also just to note here that, you know, I'm using this notation where the
superscript is a index, not a power.
So like sometimes depending on where you come from, like this can trip you up
with this notation, but, uh, in like more physics, physics C areas.
This is fairly common.
Um, okay.
So yeah.
So, so this is a power and then this is an index just to be clear.
Yep.
Okay.
So, so again, everything I have done here is completely trivial.
I just say re-roll to trace as some expectation and then re-roll this
particular expectation as like a series of measures of locations and then some
inner product at the end.
Okay.
So the significance of this is that you can reason like iteratively about each
of these matrices locations in a principal way that allows you to calculate
this final inner product expectation very easily.
And this is kind of the, the core inside between behind tensile programs, which
is that like, like when you do matrix multiplications and non-linearities,
entry-wise non-linearities, you can kind of keep track of, uh, searching
information about how the vectors obtained from matrix multiplication were
obtained from entry-wise non-linearity, how they evolve with the operations in
the limit when the, uh, matrix size or the vector size become large.
Okay.
So in this particular case, the gist is that, so what's the gist, right?
Gist is that, um, every, um, x, i have, um, roughly, uh, i, d entries, um, as and
goes to infinity.
So like roughly i, d is not, I have not defined it.
It just like, I just want to appeal to intuition that like is, is kind of,
become the entries, uh, become roughly independent and identically distributed as
and at the, I guess we're using large n here.
It's the size of the vectors and the matrices called infinity.
Um, and, uh, okay.
So this is assuming, this is assuming like, you know, um, for example, a has like
Gaussian one risk, uh, entries.
Yeah.
And we're dropping the assumption that a is symmetric now, right?
A, all the entries are independent.
Uh, no, like, no, I mean, for, for this scenario, like a can be, it can be
symmetric, but I'll get to like, kind of, uh, the, the, the more general case
where a is not symmetric, but yeah, you can move, like, you know, a has normal
entries up to where like, sure.
Okay.
Okay.
I mean, in the case of neural networks, a won't be symmetric, but okay.
For now, like, yeah, it's symmetric.
Yeah.
I mean, I just, because I just want to stay, uh, in this, uh, this setting
of, uh, uh, symmetric matrices for symmetric or low.
Yeah.
I was going to say, cause if the, if all the entries were independent, uh,
gaussians, then I think it would be automatic that the entries are
identically distributed, but not independent, right?
Uh, yeah.
By symmetry, you will be identically distributed.
Um, but yeah, like you don't know whether it's independent or not.
Yeah.
Sorry.
For the non-symmetric case, it's, uh, identically distributed.
I'm trying to think if it's symmetric, is it also identically distributed?
I'm not sure.
Uh, for symmetric.
Yeah, I think so.
I mean, just if you permute the entries, like you don't, you shouldn't change
any distribution.
Ah, wait.
You just like, you have like this, you know, yeah, yeah, yeah, yeah, yeah, yeah.
Okay.
Okay.
Yeah.
Yeah.
Yeah.
Okay.
Right.
So, um, right.
So, so that's the first point that like the entries have a roughly ID entry.
Uh, I have the ID, uh, the, sorry, the vectors X, I have roughly ID entries
as n goes to infinity.
And the second point is that like we can track the distributions of X, I's entries
through some calculus or some calculus that I'll maybe briefly like cover next.
But, um, yeah, roughly speaking, maybe let me kind of like try to jump ahead a little
bit and come back and fill in the middle.
The, essentially the eventual conclusion is that you can associate, um, associate
a random variable, uh, Z X, I to each X, I such that like X, X, I's entries look
like, um, ID samples from C X, I.
I see.
Yeah.
And, and there's like a, there's some, some, there exists a set of rules for computing.
Um, Z X, I from Z X, I minus one to Z X one.
Mm hmm.
Yeah.
We're, and I guess Z, let's say Z X zero where X zero is defined to be just V.
Yep.
Right.
If you recall from the previous slide, X one is eight times V.
So X zero naturally can be defined to be V.
So, and V, V was the, uh, Gaussian with, with, uh, unit variance, right?
Yeah.
Yeah.
Yeah.
The, the uniform isotropic standard option.
Yep.
Okay.
And so, so the punchline here.
Is that like the thing we care about in the end was this expectation of inner product
of X K and V, right?
So we're, let me, maybe let me rewrite this given this definition as X zero.
Then like based on this intuition, right?
This is, uh, also I, like, we actually need to scale this by like whatever.
And so because we had the intuition that, you know, each, each entry of X K and X zero
are identically distributed from like the Z X, uh, is the SK and the Z X zero.
Um, this is going to be pretty much the same as, you know, this, um, well, okay.
So yeah, let, let me just write this out, maybe.
So I'm just going to write this, this is, this is equal.
Right.
And then our intuition is that like each of, for each alpha, these two things are coming
from the same or from respected distributions ID samples.
So this should look like.
Yeah.
In case it wasn't obvious, alpha is the, is the coordinate subscripts are
coordinate indices, superscripts are, are like, yeah, like, yeah.
Superscripts correspond to different vectors.
Subscripts correspond to the components of those vectors.
That's right.
Yeah.
And yeah.
So this roughly, it like corresponds to, um, like by a large number of kind of
intuition, right?
If you believe that, you know, X K alpha comes from Z X K and then X zero comes
from the X zero, then this should look like the, the expectation, the product of
these two random variables, um, when X is large.
So, so what we expect is this as an infinity.
Let's just back up actually.
So it seems like this way of bookkeeping is refinements of the moment method in the
sense that the moment method, moment method doesn't, at least the way we discuss
it, doesn't reuse any computation for every K you have to compute the moment
from scratch, so to speak.
Here you're doing it step by step or in an neural network setting layer by layer.
And, and, and so you, you kind of are inductively kind of keeping track of the,
of the distributions, right?
Yeah.
Yeah.
Yeah.
So, so you can think of like that.
I mean, like, so, uh, I mean, so like two different people, moment method might
mean something different, but in the way that we, we kind of phrase it, moment method
is just like overarching technique for like, you know, trying to like show
that two distributions are equal if their moments match.
And like the traditional way of using this moment method is trying to like kind
of expand out some some, and then I try to like see like, what is the leading
term of the sum, right?
And in, in that case, like, you know, for example, in the random matrix case,
we had to count like cycles that contains each edge exactly twice.
So that's like a combinatorial problem that April, you know, you kind of,
you like, you have to redo it for every power K, right?
Exactly.
Um, now, I mean, like, but, but if you really unpack all the proofs, like kind
of like one way, because eventually you do get to cat and numbers and cat
and numbers are recurrently defined.
So in some sense, like if you keep unwinding the proof, you have some kind
of recurrence somewhere in the end, but it's like very kind of like implicit,
you know, whereas here is very explicit, like, exactly, exactly.
Once you have, you know, this computation written down in this expression, you
can just feed it to a computer and like essentially the computers can, I just
calculate what the Z's are and you know, everything is like automatic at that
um, whereas for like more traditional methods, it's kind of a bit more at talk.
Like, you know, there's some combinatorial problems that, you know, you
might be able to do explicitly, there are others that you can't.
The, the master theorem in this case says that like for maybe let's say like for
any, um, like, I'll just kind of say smooth enough, which is really, in fact,
it's like a very mild condition here, but any smooth enough say, um, V, which
just goes from R, K to R, let's say, um, then, um, the following, uh, empirical average.
Uh, let me, let me do plus one because I want to put in, um, X zero, which is V.
X one and so on and so forth to X, K.
All right.
So, so this, this thing is a scalar.
This is a scalar.
I'm, I'm adding, you know, n scalars together and divide by n.
So this will converge.
And you also want to the individual components too, I believe.
Oh yeah.
Yeah.
That's right.
Yeah.
Yeah.
So this will converge to expectation of the Z X zero to Z X.
Okay.
All right.
So, so remember, these are the random variable I have associated with each
vector whose construction I did not tell you about, but I just told you that
there is a construction like this.
Um, and you know, in general, of course, like these, these guys are
correlated with each other.
They're not in general independent random variables.
Um, and, uh, uh, like, yeah.
So, so this, this kind of generally follows from the structure of the program.
There are programs where like these random variables can be independent, but
in general, like in general programs, uh, computations, like these, uh, these, uh,
random variables are not independent, which reflects the fact that, you know,
there are correlations between X zero and X K or X one X K and so on and so forth.
Uh, when you kind of like say, when you take inner product, right?
When you summing the product of entries, like that corresponds to like, you know,
taking, taking the expectation, the correlation or the expectation of
product with the two random variables, right?
And, and these things are usually non-vanishing.
Okay.
So, so yeah, so this is called a master theorem.
Like this is a specific instance, you know, for the computation that we are doing
here, uh, for in the interest of the semi-circle law and the moment method.
Um, but of course, you know, like the, uh, the, the, the reason like this is really
powerful, uh, is because like when you have non-linearities, uh, for example, in
the case of neural network, this becomes like much, much more, you can calculate
much, much more interesting things, right?
That rather than just, you know, got linear random matrix ensemble.
Yeah.
Sure.
Let's just take a step back because this is going to take a while for people to
parse and I mean now there's more, I can see what's going on because I thought
about it for a while, but let's take a step back.
So the, how do I say, um, so, um, okay.
So, so we've only defined these X zero through X K's in the case where you're
just applying a matrix A over and over again.
Of course, uh, after this, you're going to tell us how, um, what a more general
relationship is between these X eyes.
There's, there's a more general class of transformations that can take you between
these X's, but for the time being, the, the point is the following.
So you have these sequence of vectors of size N.
Okay.
And for any finite N, right, the entries are not ID.
They're I, for each X I, the components are, uh, identically distributed, but
they're not independent, but they're, they're, they get more and more independent.
N goes to infinity.
And the, the reason why that statement is how that statement is, is, is encapsulated
by the master theorem is the fact that if you take, um, if you average over all
the samples, which is what this thing here is doing on the left hand side, right?
You're taking, uh, how to say, actually, I kind of cheated a bit.
So if you, if you go down the components, which is what this alpha is, right?
Then if the, these components in alpha were truly independent, then that is
corresponding to taking ID samples of a common distribution, which is to say that
you're approximately averaging over the underlying probability distribution, right?
And that, that's what's the right hand side.
So what I'm trying to say is that like, here's what I'm trying to say.
Like, uh, um, for example, the expectation of F of X or X is distributed as a unit
Gaussian, right?
That's the same thing where it's approximated by this sum where.
X alpha are IID samples, right?
And it's the IID samples that tell you that the, the, uh, empirical average
converges to the expectation in the, in the other extreme case where all the
X alphas conspire to be correlated in the most degenerate case, if they're
actually all identical, so they're all perfectly correlated.
As soon as you know one X alpha, you know all the rest, then this sum just
collapses to the evaluation of F on a single point, which is very far away from
being the average over the distribution, right?
So all I'm trying to say is that the right hand side, I'll just look at K
equals zero, right?
And K equals general K, just generalize what I'm saying.
The fact that there's an expectation on the right hand side is the statement
that the left hand side, the components are becoming more and more independent as
N goes to infinity, right?
Yeah.
Yeah.
Exactly.
So in other words, like what the master's term says, essentially just a
recapitulation of what I was saying earlier, that like the Z XIs are kind
of like encapsulates the distribution, uh, under which like the entries of X, X,
I, uh, look ID.
So like as the master's term just says that like the entries of these, uh, all
these vectors look ID when N is large, and we can like see this by kind
of like, we're like, the way we can see this is that through the lens of a
large number, they look like ID, right?
Like if like the X alpha are all actually ID over alpha, then this is like a
trivial consequence of large numbers.
Yeah.
Of course, in our situation, they're not actually ID.
They're correlated in some like somewhat complex ways.
And the master's term says that as long as we're only, you know, like kind of
doing things like a lot of large numbers where we average things, then it looks
just like, you know, ID things.
You can just pretend there are things that can get back to you.
Yeah.
Or it's another way of saying it.
The correlations, you know, vanish essentially as N gets large in this sense.
Yeah.
Yeah.
That's right.
Yeah.
Yeah.
Okay.
So that's great.
So, so, and then it's kind of an exercise, I guess, to, how do I say, uh,
actually, I don't know, what's the next step?
Do you want to revisit the RMT proof from this master's there?
Or do you want to just go to non-linearities now?
I can just mention that, like, you know, like if you, if I instantiate the things
I skipped over, then the expectation that you calculate, you know, using the Z random
variables are the same as the ones you would calculate from more conventional
means.
So, so that will give you a proof of the semi-circle law.
Okay.
And then move on.
Okay.
So maybe actually it seems like you could just stay in words.
So basically, um, let me, let me just erase what I wrote.
So basically it sounds like a corollary can compute moments of, uh, of, uh, let's
say from RMT via formula for computing disease, right?
Because the left-hand side, based on what we just talked about in the previous
slide, can, is, is kind of the, uh, RMT moment you want to compute, right?
Just by, by a change of variables.
Cause we had the, right.
So in this case here, Phi is a function of K plus one variables.
So in this case, it's actually a function only of the first variable and the
last variable.
It can just be independent of all the intermediate ones, right?
That's legitimate.
So you choose the Phi such that it's the inner product of X zero and X K.
And that gives you the RMT moment that you care about.
And then you just now have to understand what the right-hand side is saying.
And, and part of the tensor program theory is that there's a way of computing
the Z's and you just compute that expectation.
And then you get the, the, the answer you expect, right?
Let's, let me just say that.
So right-hand side, uh, tensor program theory.
And so, and, and, and, and then you get the answer you expect, right?
Yeah.
Yeah.
Great.
Great.
Yeah.
Okay.
Um, great.
So, so, yeah, so just to, to make it really clear, this is how the tensor
program master theorem, uh, re derives the random matrix theory, uh, uh, moment
computations, um, the more, uh, the, the, the, well, we've only illustrated in the
case where you obtained the X's from repeated application of a matrix.
Can you now state it, the tensor program, what a tensor program is in a more general
setup so that, uh, we can see how it applies neural networks.
Right.
Right.
I can write down like a more general formulation, but, uh, roughly speaking,
right, uh, what a tensor program is like the most basic version is that you have
some initial objects.
Um, so the initial objects includes like, you know, some, some vectors in our end.
So, so, for example, in our previous example, like the V, uh, is, um, is the initial, uh,
vector calculation, um, so like you should expect, um, expect that, um, like, um,
let me call these, uh, let's say X, expect X to be sampled from like a standard Gaussian
kind of thing, each entry will look like, by definition, these vectors will have ID
entries.
Okay.
Um, and then you also have matrices, say like, I'll call them W.
Yeah, this is the capital M, I guess, to be consistent with notation.
So, uh, these you should expect to, um, be have like, to have, uh, something like, you
know, this kind of, uh, entries with the variance one over N, meaning zero.
Okay.
So, so this is the most basic version.
There's like more advanced version that has a bit more data, but essentially given these
two, uh, these objects, you can compute, we can, you can like create new vectors via,
uh, two instructions.
One is, you know, you can do matrix multiplication.
So, you know, just if you have W and if you have X, where X can be either initial vector
or a vector that we have generated using these instructions, then you can form, you
know, W times X, right?
Which is also a vector in RN, RN.
Okay.
So this is one instruction for generating a new vector.
Another instruction is non-LIN.
So here, suppose you have a bunch of vectors, which again, either are initial vectors or
vectors you've generated previously using one of these two instructions, uh, say like X1
to XK, right?
And then, then you can generate fee of X1 to XK, where, where fee, so here where I'm
using kind of like a deep learning notation where, where fee has the signature RK to R.
And then the meaning of this is that if we define Y to be this, then the meaning of this
is that this notation is that Y, each entry Y, each entry of this, this, uh, expression is
defined that to be a fee of X alpha, X1 alpha to XK alpha, right?
I'm applying fee entry-wise to the vectors.
Yeah.
So, so, so people in machine learning, yeah, K is typically one, because you apply it just to
a single layer, but you're allowing a non-linearity to allow it to act across multiple layers,
essentially, but always coordinate-wise.
Yeah.
Yeah.
Yeah.
So, yeah.
So like, you know, when people say like entry-wise non-linearity, most people think of like
rally or hyperbolic tangent, something like this, but yeah, you can have more general
things.
And in fact, like people do use them, but you can think of like, if you know about
batch normalization, this is kind of like a, you know, multivariate non-linearity in
some sense, where like X1, XK are like the, the, um, activations from different batches.
Like one to K are the batch index, for example.
Uh, you can think of a batch normalization this way.
So, so it's just to say that, like, uh, people do use kind of more, these more general
types of entry-wise non-linearity.
Okay.
Okay.
Okay.
So, yeah.
So here, again, we have some initial objects, vectors, matrices, and then we can generate
new vectors using one of these two instructions.
And, uh, and then that's, that's pretty much it for TensorFlow.
Just like these two, essentially these two instructions.
And, you know, what we had before, if you go to go back to the previous slide about master
theorem, what we have here, like applies exactly as before where like X0 to XK are all
of the vectors in the program.
Like X0, for example, can be initial, uh, and then X1 to XK can be generated.
Like, which is exactly the case in example for the same server that we had just now.
But, uh, yeah.
So the master, but the master theorem holds exactly as written, um, when X0 to XK are
all the vectors in any program.
And again, like we have omitted the exact way that we have constructed the ZX highs.
Uh, and of course the dial is like kind of the key behind any actual calculation you
will do, but like the spirit of this is that, you know, when you multiply, uh, uh, matrix
by vector, the resulting vector from this memo, like we'll have, you know, some correlation
between the entries typically, right?
But the master theorem says that like you can always reason about them as if they have
ID entries as the size plus infinity.
Right.
So, so that's kind of the, the spirit of the theorem.
Now, again, like the, the key thing you really need to elucidate when you actually need to
do computation is the construction of Zs.
Um, and like, I mean, also related to like, how do you like, uh, compute the new Zs when
you have a new, uh, vector from the old ones?
I see.
Okay.
Okay.
So we just described, uh, a much more general tensor program where we allow these non-linearities.
And the master theorem, if we go, if we go back one slide applies to this result, although
actually, I guess we, we have some redundant notation.
There's the fi for the evaluation.
And then there's the fi for the non-linearity.
Let's call this a different letter for the, uh, what do we want to do?
Or, I mean, like it's, yeah, I mean, yeah, like, uh, I mean, fi, if fi is just any, you know.
Yeah, you're right.
Okay.
Fine.
Yeah.
There's, there's, yeah, yeah, yeah, yeah.
Fi is just a generic symbol.
Okay.
So there's, there's the fi of the master theorem, which is your choice of a non-linearity by
which to evaluate things.
And then there's the fi of slide 15, which is the, uh, non, the, a symbol for a non-linear map
that you could apply at any step when you want to apply one of these, these rules.
The point I was trying to make is that, um, once we have these rules for generating
sequences of vectors, then the master theorem on the previous slide, uh, applies and, and
you get the same, um, uh, asymptotic IID this, right?
Okay.
So I think to round out our discussion, because we've been talking for a while now, why don't
we, um, specialize to the case of a feed forward neural network, uh, which is a particular
kind of tensor program.
And then we can, um, uh, end with one of your more recent applications of tensor programs,
which are the so-called ABC parameterizations and their tensor programs will be used to show
that there's a family of very interesting limits of neural networks and, and, and, and
particularly the dynamics of the neural networks.
That sound like a plan?
Yeah.
Yeah.
Sounds good.
Uh, yeah.
Before that, let me just briefly remark that, you know, like there's like, uh,
low discrepancy between, uh, what I've written here for tensor programs versus like the
semicircular law application where we assume the matrix would be symmetric.
Uh, I just want to like briefly know that, like, this is easy to take into account, uh,
where you can like, when a is symmetric, um, you can just write it as, you know, like
w plus w transpose where like w is not symmetric, right?
So you can always symmetrize.
And then like the, you have to take into account the symmetrization when you express a program,
but everything's your work.
Okay.
Great.
So yeah.
And for those who are going to look at your paper, your first paper had tensor programs with
just these w matrices and then your followup work included transposes, right?
Uh, because w transpose is not independent of w.
And so you had to extend your framework to that.
But okay, those are kind of the details of your papers.
But, but yeah, just just a road.
Yeah.
Yeah.
So in particular, like really here, you know, I can do either w times x or w transpose times,
sorry, times x where the w can be reused.
Sure.
Right.
And this is where the power comes from because in a lot of computations, especially involved
near networks and also in the semi circle case, you're going to keep reusing the same
matrix over and over again.
That creates a lot of correlation between, you know, between the vectors and like keeping
track of these correlation is like a very complex task.
You know, don't know how to approach it.
But like master theorem, you know, like I, I omitted a lot of details, but the theorem
essentially allow you to do it in a mechanical way, which makes like this really complicated
mess very clean.
Yeah.
But just a little check in your first paper, I think every time you applied a w, it had to be
a fresh new IID one.
It was later that you allowed reuse.
Is that right?
No, in the first paper, you're already allowed reuse, but just that you can only, you cannot
do transpose.
Okay.
I want to see the w or w transpose like consistently.
You cannot use both w and w transpose.
That's, that's, that's, that's the first paper and makes things a bit cleaner.
And in the sense of like how you phrase the result and also like for the Gaussian processes
applications, like that's usually only how you need it.
When you add transpose, there's a lot of complications with, but over a little more
powerful, I'll do more things.
Okay.
Okay.
Just wanted to clarify.
Okay.
So let's, okay.
So let's now maybe fast forward to neural networks and ABC.
Yeah.
Yeah.
So, so first of all, I just want to kind of write down what a neural network is and just
make it clear that like this framework indeed express the kind of computations and deep learning
people would care about.
So usually in, in deep learning, you know, you have a, a, a, a neural network function,
which I'll call F.
And typically I use XI for the input of the neural network.
Like, you know, different people have different voices, but such a, such a mathematician.
Yeah.
It's, it's a bit on call in deep learning, but it is what it is, you know, like, like
people should be happy to expand their horizon of the letters.
Okay.
But so, so, you know, like a typical neural network is just a composition of linear and
entry-wise nonlinear functions.
So in this case, like a three, a simple three layer neural network can be written as, you
know, v times, or I'll just say v transpose times phi w times phi of u times xi.
If xi is in rd, then u is in r, say n times d, and then w is, I'll just write the shape
style n times n.
And then v is, like for a scalar output function, v is just one times n.
Sorry.
No, n times one, because I'm doing transpose.
I see.
You write it like that because you want, you want the large dimension.
And to be on the inside.
On the left side, yeah.
Yeah.
Yeah.
Okay.
Okay.
So, and just recall for the mathematicians, like fees applied entry-wise.
Yeah.
Okay.
So let's, so I just want to at least essentially make the point that, you know, like this is,
this new neural commutation can be expressed as a program.
And just for simplicity, I'm going to assume like d, in fact, is equal to one.
The general case follows pretty straightforwardly, but this ladies, like for the first,
for the first intro to this, it helps to simplify this.
So then in this case, xi is just a scalar.
And let's just back up.
So this is, this is what you'd call a three layer feed forward neural network.
Or also called a multi, or sorry, a MLP, a multi-layer perceptron.
Right?
Yeah.
Yeah.
Okay.
So let me just, let me just write that just to, just so people have the lingo down.
So this is a.
Sure.
Feed forward or MLP, neural network.
Yeah.
All right.
Go ahead.
Yeah.
So, yeah.
So, so to express like the forward iteration of this, this function, we can do the following.
So like, first of all, you know, like in a program, you need to specify where the initial objects.
So the initial objects are the vectors u and v.
I mean, in our case, like they're literally in our N, I guess we use big N here.
And then there's initial matrix w, which is N times N.
Okay.
So, and then.
Sorry.
Vectors.
Oh, you mean use, use xi, you mean?
Or.
Oh, no.
So like, it doesn't matter.
It's going to be a constant.
So in this computation, xi is a constant.
I see.
Sorry.
Sorry.
Your distinguishing mean vectors and matrices have to have our N by N, whereas vectors are N by
something that doesn't depend on N, like N by a constant.
N by D and N by one.
That's why you and V are vectors.
Yeah.
Yeah.
You can think of like that.
Yeah.
Okay.
Like it doesn't, it doesn't matter a whole lot here because like, because xi is going to be,
like, xi is not random.
Like we're going to like fix xi in some sense.
Like they're just going to be part of the.
Sure.
So like.
I'm just saying, I'm just saying you and V are matrices as well.
It's just that you distinguish between vectors and matrices because a matrix for you in a
tensor program is something that's N by N.
That's all.
Oh, yeah, yeah, yeah.
That's right.
Yeah.
Yeah.
Matrix is like two dimensions going to infinity.
Yes.
One dimension going to infinity.
Yes.
Okay.
That's it.
That's all I was trying to say.
Yeah.
Okay.
Okay.
Good.
Good.
Good.
Yeah.
Okay.
So, uh, and then, you know, to compute, let's call, you know, like the result of u times
i to be h1, apply phi, you get x1, apply w, you get h2, apply phi, you get x2, and then
you apply a times v, you, um, get the output of the network.
Okay.
So I'm just going to essentially, uh, express everything up to x2, like the contraction by
v, uh, you can, yeah, you can do it multiple ways.
You can, like, express it in a slightly more general tensor program, or you can, like, in
this case, you can just express it as, like, part of the master theorem, because, like,
you can, like, think of v contracting with x2 as, you know, like a sum of, you know,
the entries, uh, which can be expressed as kind of, like, in the statement of the master theorem.
So I'm just going to, again, I'm just going to express everything out to x2.
Okay.
So, so, you know, changing color back to white.
So, no, h1 is equal to, um, like, you can, you can say, like, I would just say, like, on phi1
of, uh, of u, right, where phi1, it just defined to be u times xi, so just, you know,
multiply each entry by xi, right.
And then x1 is equal to, of phi2, uh, of h1, where phi2 is literally just phi, like, from the,
from the definition of the, the neural network.
Again, like, you know, phi is applied entry-wise, right.
And so these are all, like, non-linear operations, even though, like, h1 was actually,
the phi1 was actually linear, but just, like, non-linearly in the sense of.
Why are you calling phi1, phi2, uh, what's the subscripts on the phi?
Oh, like, they're, they're the, the fees, uh, the, the, the non-linearities applied in the
instruction of non-linear.
So remember, we have two instructions for generating new vectors.
I'll go back to, like, page 15, right?
Like, there's a, there's a fee here on page 15.
You can see where I'm scribbling.
Yeah, but why are you calling phi sub 1 and phi sub 2?
Well, they're just, they're just different non-linearities.
I'm defining, I'm putting them in the form of the, the non-linear instruction, the operation,
which requires.
Oh, I see what you're saying.
Okay, okay.
Ah, okay.
Okay, fee one is just contraction with psi and fee two is, is just an application of phi.
Yeah.
Okay, okay.
Sorry.
Got it.
Okay.
Okay.
So, um, continuing on to define h2.
Now h2 is w times x1.
And this is the first time we're using the math more instruction because, you know, w,
as we specified, is a matrix.
And, uh, you know, finally x2 is equal to fee of h2, you know,
which is just like a, it's a non-linear operation again.
Okay.
So, so, you know, it's, again, it's very obvious that, uh, you know, like things that are,
that typically occur in your networks can be expressed this way.
So, you know, for people who are more versed in deep learning, like, you know,
familiar with, like, more advanced architectures, like convolutional networks,
residual networks, batch formalization, transformers, self-attention,
all of these things can be expressed this way.
It just gets a bit more complicated.
But, you know, like, you can look at the papers where there's like explicit examples
of these written down.
But again, like the summary and the gist here is that I want to give an example of why
in your network computations can be encapsulated in this general framework,
this language, right?
And again, like the punchline is that once you can express all these things in this language,
then, like, you can apply this master theorem on page 14.
And, uh, again, I omitted certain construction details about, like, what the Zs are,
but you can kind of reason iteratively through the program to understand the behavior when
the width of the network becomes large, right?
Okay, so now we can get into some more advanced topics in regards to, you know, like,
why does the knowledge of, you know, how large networks behave, like,
give you a lot of, you know, power in practice, right?
Why do we care about this?
Especially in the age of, like, GBT, BERT, you know, as new networks get larger and
larger, they just get so much better beyond people's expectations.
Let me just make one comment, actually, based on what you just said.
You said, basically, what you just did here was express this MLP as a tensor program.
And if you just follow through the master theorem and maybe with one or more steps
because of this V transpose, I think what you're going to get is that F is a Gaussian process,
right? And this is how you can, you're going to get the NNGP if you follow your nose
through the master program. Is that right?
Yes, yeah, so, yeah, so like,
I just wanted to mention that we don't have to go too deep.
Right, right, right.
Yeah, so roughly speaking,
yeah, if you scale this last V in the correct way, then, yeah, like roughly speaking,
like the resulting distribution of F where the randomness comes from the sampling of V, W and U
will become asymptotically a Gaussian process.
Yep.
And, yeah, and this is, so like, you know, again, this fact has a very long history
dating back to the 1990s with referential for simple neural networks.
And, you know, like kind of around 2017, there's some more works involving deeper neural networks.
But like kind of every time you want to, you know, let's say like a new architecture comes out,
yeah, to kind of do a lot of things manually and spend a lot of time to see whether like,
oh, this actually holds also in this case or not.
And so it's like a very arduous process to actually try to manually prove all these cases.
Actually, if I recall, at least for MLPs, it's maybe even for convolutional networks.
It's not that arduous in the sense that if you look at those proofs, what they implicitly assumed
is that if you let the layers go to infinity one by one from input to output,
then you're basically applying the central limit theorem successively and things aren't too hard.
What's difficult is if you let all the layers go to infinity at the same rate.
That's the difficult part and that's where your work fills in that gap essentially, right?
Yeah, and also like when they're like kind of, for example, they're like,
when you look at RNNs, recurrent neural networks, then if weights are tied, right, like cross time
steps, it doesn't make any sense to let each layer go to infinity at the same time because
they're the same matrix, you know? Yeah, I see, I see.
So like when like we're doing a lot of way sharing, this is not a feasible thing to do.
Okay. Yeah, so and also just to remark that like
like kind of going further than this, right? So far, we're just talking about like,
how do you express, you know, like for different architectures, can you express,
right, the architecture in this kind of computation? But really the most powerful thing,
the much more powerful thing to do is to actually, which we'll do in a second, is to kind of like
look at the entire computation graph training. So not just expressing like a single four pass
of the architecture, but unrolling gradient ascent into a very big computation graph.
And then like understanding the behavior of the network after training.
So this is really like one of the most powerful things you can do with TensorFlow
because you can just unroll anything, any kind of iterative computation you care about in this
format. And when you want to do this, when you want to unroll, for example, gradient ascent,
then you must encounter, you know, like the sharing ways because you need to use W
in the four pass and W transpose in the back or passing to iterate this many, many times,
right? Over the course of gradient ascent. And like this kind of way sharing pattern in
this computation essentially prevents you from trying to do any kind of like taking one layer
at a time to infinity, right? Because you're sharing so much weight within the computation.
Yeah, fair enough. Fair enough. Okay. All right, sorry. That aside took us maybe a little bit
far. But okay, let's go back. So where did you want to go next?
Yeah, so what kind of like talk a bit about like, you know, how this foundation lets us
understand the different large width behaviors of neural networks and like what the significance is
in the age of large neural networks. I want to kind of, you know, make analogy with things in
mathematics, you know, in case like mathematicians are watching this video. But, you know, like
traditionally, you know, I think a neural network is thought to, you know, have like a single behavior
when you take the width infinity. And usually that's something associated with like a kernel or like
a Gaussian process, you know, these kind of things. And but it turns out this is not true,
right? Like there's no like these, there's no a single one behavior for large networks. In fact,
there's like a very large space of possibilities for how our large neural networks behave, depending
on how you take certain hyper parameters, you know, how you scale those hyper parameters as the
width becomes large. So, you know, this is kind of similar to certain behaviors we know from
mathematics. As an example, you know, if you're more from like an algebraic background, then you
must know that, you know, while like the rational numbers has a very natural completion in the real
numbers, there are in fact like different completions of rational numbers into other fields, like the
periodic fields for P being any prime number. And a very powerful result in this area is the
classification of all, you know, fields that are completions of the rationales, and which turn out
to be, yeah, essentially the reals plus all the periodic fields. And from there, we have very
powerful, you know, like localization results on how we can like, you know, equate the solvability
of certain equations over rationales to the simultaneous solvability of the equation over
the other reals and the P addicts, right, all the possible completions. And this is like a very
powerful basis from which like Langland's program now extends. And so just like this, when we look at,
you know, like infinite with neural networks, there are different ways of taking those limits,
just like how there are different ways to complete the rational numbers. And in the sense, like to
understand the behavior of a finite neural work is like roughly equivalent to understanding
the behavior of different limits of infinite with neural networks. And like from this perspective,
becomes very natural to understand like what are all the possible infinite with limits of these
large with networks, right. And so this is like a very different perspective from the more traditional
perspective that when we take with infinity, like, some kind of kernel behavior will happen.
And like, you know, if, and because the current behavior is bad, you know, we don't like the
infinite with neural work. But in fact, like there are different behaviors and as we'll see,
like this, in fact, one behavior we really, really like and gives us some really powerful
technology in this context of, you know, like huge neural networks like GBD.
Yeah. Yeah. Yeah. Yeah. So,
right. So now let me kind of set the stage of like, how do we view the space of different limits,
right. For each of these parameter tensors, there are two numbers that you need to specify at the
very least to specify a training procedure. One is the, I'll call it the initialization
variance. So, so supposing that you sample these things with ID, Gaussian entries with
zero mean, then what is the variance that you should specify for those things. And then the
second thing is a learning array. So, yeah, so for those of the viewers who don't,
or haven't seen, you know, gradient descent before, like the, you know, the gradient just
quickly, I guess, you know, gradient descent essentially is an iterative algorithm, which
just says like two, two, if I want, if I want, you know, some loss function, so some like
function of the neural network F to, to become smaller and smaller, then like you should update
the parameters like so, then you should get updated to you minus some, this is the learning rate eta.
Maybe, okay, let me be explicit here. Let me just, just write learning rate instead of eta.
Learning rate times, okay, let me write it in the gradient format.
The gradient of the loss with respect to you and likewise for the other things.
Right. And then like this thing, you know, there's a lot of literature for this,
you know, how do you set this learning rate in the convex literature. It's a very old topic,
but you know, like there's much less understanding of, of this, this learning rate in a non-convex
setting where you're optimizing your networks. But in any case, like this, this neural network,
this parameter learning rate is what we need to set, right, for, for these, each of the UWB.
Okay. So, right. Okay. So, in particular, when we talk about like large N limits, right,
where N here, in this case is, oh, I shouldn't write here, I guess.
So, let's, let's, let's, let's back up. So, what we have is for,
let's actually, let me just introduce it in notation, which is quite standard, just,
it'll be a little easier to talk about. So, let theta be the set of all parameters. In this case,
theta is UWB, all parameters, like neural network parameters,
right. And so, we have a random function, a random neural network function, which depends on this
random parameter theta. And for each loss function, L, so L here is what's called the loss function,
you're trying to minimize your loss, right. That gives you gradient descent dynamics. In other
words, you can run this gradient descent step by step. And that gives you a discrete sequence of
neural networks given by just updating sequentially under this gradient update rule.
And the question you're trying to ask, so we haven't seen the question, so question,
what kind of scaling limits exist for the dynamics of F? By dynamics of F, I mean,
it's not just that the limiting F exists at initialization, but the entire gradient trajectory
must also exist, right? Yeah, that's right. Okay, great. Okay, great. And like the different
behavior of these scaling limits depend on how you scale this initialization variance in the
learning rate, right, for each of UWM, UW, UWMP, right. So, for example, like one way you can
scale this is say, like, I want the initialization variance of V to be, maybe let me, let me erase
this, abbreviate it so things are more like, so learning rate. So, for example, I can say,
let's have variance scale, like, I guess we should use big N here,
big N to negative one here, and then big N, negative one here, big N to negative one here,
right, and then learning rate can be like just one, right. And so here just to clarify,
like, I just mean like how you should scale these things, I don't mean exactly setting these two
such values, but just like, you know, like the initialization variance should have when you
double N, right, that's all I mean, right, there can be constants in front of it.
So, when you specify such a scaling relation in powers of N for these hyper parameters for UWMV,
like, any such choice specifies one way to take the limit, right, when you let N go to infinity,
and that gives you one set of behavior. Yeah, so you will turn out, for example,
yeah, in this case, like, when you specify, like, these hyper parameters, the new network
will actually blow up to infinity, like, after one step. And, okay, so, like, in general, right,
like, in general, if we, or just elaborate further by one step, you'd be basically, even if
the limit exists at time zero, the limit does not exist at time equals one.
Yeah, yeah, yeah, yeah. Yeah, so, yeah, so in general, right, like, you have,
in this particular case, there are six numbers seem to specify.
So, like, I'll just call this,
like, using this notation from
abc franchisation, I'm going to call it negative B here,
and then negative, oh, sorry, negative C here.
So, there are six numbers, like, the B and C for each V, W, and U, okay, and, yeah, each way,
like, specifying each number give you a set of behavior when you take N to infinity.
And, you know, kind of to connect to all the previous things we talked about,
like, the reason that we can't understand all of these diverse set of behaviors when you take
N to infinity is because we have this tensor program machinery where we can, for any set of B and C,
we can express that computation in a tensor program and take the limit against, like, automatic
because you have the master term, and you can just automatically calculate what is it,
what is the behavior, right, in the infinite N limit.
Okay. So, now, like, the question is, what does this landscape of limit look like, right? So,
again, like, kind of using analogy with pediatrics, right, like, we know that the completions of the
rational consists of, like, P being all primes, pediatrics, and the real number, the real field,
like, what is the corresponding picture for neural networks, right, when we vary the set of
B and C, right? So, like, kind of the B, the B's and C's here are kind of, like, analogous to,
you know, like, the P in pediatrics, right, where P ranges over primes in that case, but here B and C
can be any real number. And, like, roughly speaking, you can, like, in this particular case, like,
there's, like, it's a six dimensional space, and you can actually, like, partition the space and
kind of classify what the, like, the partition of space looks like. Okay. So, this is the picture
right here. So, it's a six dimensional space, but obviously we cannot draw six dimensions. So, instead,
I'm going to just give you, kind of, like, you know, like, project this to a two dimensional thing
using, like, a nonlinear projection. I'm going to distort something. I'm thinking I'm going to take
some quotients. But I just want to, like, carry across, like, the, the most important features of
this. But so, this is, like, the six dimensional space. This is, like, the space of the B, U, B, U,
B, W, B, and the C, U, C, W, and CV. So, this is the R6 space. We draw into the dimensions.
Okay. So, the point for, first of all, it's not like most of the space, if you, like, throw a random
dot in R6, like, according to the Lebesgue measure, whatever that means, you're going to get, like,
some pretty uninteresting limits. So, this is, like, this background. This is, like, the measure,
the full measure background, where essentially you have two behaviors, two uninteresting behaviors.
One is that, like, the training blows up in the limit where, where the training gets stuck
at initialization. So, in other words, like, when you do gradient descent, it doesn't change the
function at all. Okay. So, so these are, you know, neither of these are interesting, right? Like,
you get, you don't get any interesting functions. So, if it blows up, you get,
not, not well-defined function. If it gets stuck in initialization, you haven't learned nothing
from the data, right? So, like, very, very uninteresting. And then it turns out that it's, like,
like, a co-dimension one, or, I guess, yeah, at least co-dimension one. I forgot exactly how,
what's the co-dimension, but there's a space in the middle of this, the C of uninteresting things,
where you actually get, you know, like, non-trivial behavior. And so, I kind of drew a quadrilateral
here. I mean, generally, it's kind of like a higher-dimensional, but the, the stadium feature
here is that you have, like, all the, all of the points in the interior, along with the points in
the lower boundary here. So, maybe let me, let me use a different color here. So, like, the lower
boundary here, plus, like, the interior here, they're going to be in what's called the Carnot Regime.
So, what that means is that the neural network
will evolve or will have very simple behavior in the limit, in the sense that,
like, the function f at time t in the grand descent evolution, you know, using, using the
grand descent algorithm we talked about in the last slide, it's going to evolve something like
f t equals f t minus one minus the learning rate times a, a kernel k times f t, t minus one.
Okay. So, so this is, okay. So, also, like, this is, like, the simplest case when you use
the square loss, where this is, like, literally a linear equation
where k is, you know, it's like a linear, you know, operator on f.
Let me just write this down, linear operator. So, so this k will, will kind of change when you,
as you traverse through this space that I kind of colored in here. Like, you know, so, so the
point is that, you know, if you look at the limit determined by this set of b and c, and then another
limit determined by this set, and another limit determined by this set, in general, they all
satisfy a linear equation of this form, assuming a square loss. But the kernels can be different.
The k can vary between them, but they all satisfy this kind of linear equation.
Do we have explicit characterization to that? So, for the n, t, k, it's basically the,
the, so, so n, t, k is here. This is, like, the n, t, k limit. It's actually, like, a vertex
in this polyhedron. And, yeah, for, for k, like, so for any of these, you can kind of calculate
exactly what the kernel is based on, you know, what the b and c's are. Yeah, it's a recursive
calculation based on the program structure, based on the architecture of the neural network.
Oh, I see, I see. Okay, because the, the n, t, k has this very special form of being the
gradient of the function times itself, but it also has a recursive structure. And you're saying
the most general kernel is just defined by that recursive structure. It might not be so simple
as, you know, grad f dot grad f, something like that. I mean, like, so, like, because the n,
t, k has a very simple characterization in terms of the function itself. But in general,
it has to be something more recursive. I mean, in general, it does look something like grad f
dot grad f. But it just, like, when, depends on what the b and c's are, like, you actually will
kind of zero all part of the grad, for example. Okay. Okay. Yeah. Sure. Okay. Anyways, just wanted to
understand. Okay. So, so, so from a mathematical perspective, this kernel regime is very nice
because a prairie, like, we have no idea how the non-convex, you know, evolution or the function
goes under gradient descent, right? Like, this is kind of all the convex optimized optimization
theorist, they kind of like all vex over this problem. But now if we take this limit, this
becomes a convex problem. So it's a linear problem even. And like, you can obtain a lot of results
on optimization trajectory of such functions in the, in the limit in these kernel limits.
So like, mathematically looks great. But, and this is like a very, very, very big, but
the problem with this is that there's no,
does not exhibit the behavior we know as feature learning.
Okay. So, so what that means is that if you look at, you know, go back a slide to slide 17,
and you look at, you know, the equation for f of the line,
like feature learning can be, at least like the, the lack of feature learning roughly means that
if, you know, x two of, you know, input is like equal to x two, uncalled this,
let me, let me write this down. So if x two apply to input at initialization time, so without
training, without seeing the data is, is equal to x two of xi after training.
If this is, so we say that like, there's no feature learned for input xi. And when we say
like, there's no feature learning, it means like, this is true for any input xi.
So I think, I think the more proper way of saying, because x two is going to become,
it's a vector of n coordinates and n is going to infinity. I think the point is the entry-wise,
the, the change in coordinates is little of, of one, essentially, it's going to zero, right?
Yeah, yeah. So yeah, roughly. So like, that's like a more pedantic way of saying this.
Yeah. So like, essentially, right, each, each entry, if like the change in the, in the entries
of x is much smaller than the entries themselves at the beginning, then, then essentially, there's
no features learned. Okay. And I think this is something that, that people might have a hard
time understanding because the, how to say it, the function f can still change even though there's
no feature learning because the vectors are getting bigger. So even though their, their coordinates,
the, the coordinates with themselves changing ever so less and less,
you know, you could, you can have an overall effect just because when you multiply by a large
weight matrix, large being an n by n, where n is large, all those small diffs can add up to an o of
one diff at the very end of the function, right? So it's sort of like, there's two different diffs
going on. There's whether the function changes, the f of theta of xi versus whether the individual
neurons or components change. And even though the neuronal changes are going to zero, as that goes
to infinity, the function overall still changes just because there are many neurons, right? Yeah.
Right. Like I said, that like most of the space, like, like in, in this island in the middle of
the picture, most of the, the, you know, the limits, right, correspond to these kernel limits,
which mathematically is very nice because you, you have simplified the behavior of complex
neural networks, complex finite neural networks, but it has a very tragic behavior of not being
able to learn features. And this is, this is like, like really, I think from a mathematical,
mathematical perspective, it's hard to appreciate how important this is in your settings, but like
all the things that we love, uh, from, you know, the empirical success of evening today,
you know, like ImageNet, Bird, GPT, like all the success comes from the fact that
these neural networks can learn really, really good representations of like pictures or language,
right? Like this, like this X2 in the previous slide is kind of like, it's on the neural networks
internal representation of like a picture of a cat, for example, or like where if the xi is a
picture of a cat, where, you know, xi can be like a sentence, right? And like it's like an internal
representation of the semantics of sentence. And like if, if, uh, you do not have future learning,
then like all of this representation is like nonsense. Like it doesn't even know about like
the data at all. It just like something that will be the same as if you just blindly choose a random
representation. So, so this is a really bad, bad, um, bad behavior, right? No future learning.
So like, if you just look at the picture like this, it sounds, it sounds like, okay,
this looks like infinite with neural networks are just bad, right? Because
anything you pick here probably does not do future learning. But if you squint very closely,
in fact, like the, the last part that I haven't talked about, which is like the upper border of
this, um, polyhedron, this actually does exhibit future learning in the sense that like the, in
the previous slide, like the x2, um, uh, does not, uh, does not like get stuck at the, at this
value and initialization, but it does evolve. And in fact, you can like squint even more closely
and like you'll see that the upper vertex here, like it's kind of maximally
future learning. I'll just say that. Okay. The very like, in a sense that can be precisely
formulated, but roughly speaking, it's like saying that anything that can learn features,
like all the parameters that can learn features will learn features. Like if anything that can
move away from initialization, you'll move away from initialization.
I see. I, my guess, just thinking about this, um,
how do I say, you wrote it as a point because basically, uh, okay. So the way you drew this
picture, the kernel regime is consistent of the interior of this polytope and some of the faces.
And since this is a high dimensional polytope, then you're going to have many co-dimension faces
and higher co-dimension means more features are being learned.
And, uh, the point at which you've used up all your co-dimension so that you're at a single
point, all your, all your layers are learning, right? So that's why you single point, right?
Yeah. Okay. Good. Yeah. Okay. Great. Uh, okay. That makes sense. Um, I think we did get to
everything you wanted to say. I just wanted to rewind it because you made this kind of
comment that I, uh, wanted to push back, but I didn't want to interrupt you. You said,
uh, a lot of things in machine learning are, are quite poetic, philosophical or, or subjective.
And I think this is one of them. You said that, uh, no feature learning is, is bad,
but I wanted to push back just a little bit because I think that might be a little bit
too strong of a statement because there are regimes in which the anti-K does well or even
better than neural networks, especially low data regime, right? So I, I, I want to qualify
bad a little bit. Is that that there's something fundamentally bad about no feature learning?
It's just that in practice, uh, we have gotten the gains that we've seen from very large neural
networks because they do feature learning. But on the other hand, we know that kernel methods
do well when kernel methods do well too. It's just that kernel methods don't scale well. So
it's, you know, if you, if, if, if the kernel computation wasn't a bottleneck, it's unclear
what would happen if you fit a kernel to, you know, millions and billions of images
or pieces of text, right? It's just that we can't do that computation. So I, I,
I want to just maybe, uh, kind of push back in that sense.
Yeah. So, uh, definitely, you know, when you, um, yeah, like in low data regimes where like
essentially you need to specify some kind of, um, inductive bias. Yeah. I mean, kernel is just like
one way to specify this bias and it could be, it could work better than your networks. Um,
yeah. I mean, so like, again, like when car, if kernel methods work better than your networks,
and like it's potentially also true that even without feature learning, a current
kernel limit will also work better than a finite neural network and whatever that means. I mean,
yeah. So, uh, yeah, though I, I, I kind of like don't really feel like, uh, even if you can scale
the kernel methods, you will do better than, you know, like deep learning, um, like in, in the
large data regime, even if you can compute it, like that's kind of my feeling. Like, I mean,
we have very concrete, uh, results on this. I don't see far 10, like, you know,
how many go out like meta learning kind of things. But anyway, like, yeah.
I mean, there's a whole, there's a whole cottage industry of showing how kernels and neural networks
differ. Uh, I'm not up to date on that literature. Yeah. Yeah. Yeah. Um, okay. So maybe like, I'll,
I'll kind of finish up by kind of talking about like how this maximal feature learning limit
retranslates to really good, great gains in terms of like empirical performance in neural
networks or like allowing us to do things that we couldn't do before. Okay. So, so maybe let, let me
kind of come back to, uh, some, some problems with empirical deep learning. Right. So, uh, today,
like in like the, the best, the best neural networks are essentially the largest neural
networks training on a lot of data. And like the advantage is so apparent at larger neural
networks are better that essentially like to train the best model, you want to throw all your
resources at training that model. Right. So like, you know, like, for example, like when companies
I open at the mine, Google, like they, when they are confident that this is like, like we should
scale up this method, they just throw all their compute at this training is one model. And
um, when you do so, you have essentially one shot to make it succeed. Right. Like a lot of
things can break when you do this to kind of put all your X in the same basket. Like just,
X can break for like very mysterious reasons because that you kind of almost by definition,
you have never done experiment in this scale because you're literally throwing all your
compute at it. Right. Whereas like during the experimental phase, you're going to use smaller
amount of compute. So when problems occur at that large stage, it's very costly in terms of,
you know, like the computation, the GPU hours, the energy, and also like the man, the, the,
the man hours in terms like trying to fix these problems. And in particular, like a lot of these
problems come from like, for example, batch voices, hyper parameters, which will like kind of just
make networks diverge during training for, for, for no apparent reason.
And like the, the way people do it now is, you know, for example, things like learning rate and
these initialization scales, they just kind of randomly guess, like roughly speaking,
it's like something similar to what they've used for smaller models, but that actually leads to a
lot of problems. Like with these large models, just things will break or you just like do worse
than your smaller models, right? In which case you spend a lot of money for nothing.
So it's important to like, like kind of predict, like if you extrapolate your compute to
extraordinary amounts, like what is the right hyper parameter or what are the right hyper
parameters to use for that, you know, like a large model training run. And it turns out that like
this on, on the page 18, like this maximal feature learning limit, like, which essentially is a
choice of the B's and C's, which themselves are like recipes for how to scale your hyper parameters
as your model size change. Like this turns out to have the very like useful property that as you
scale your neural network larger and larger, we can follow the recipe given by these B's and C's.
Then if you're, you start with a set of hyper parameters, which are optimal, like the set of
learning rates and initialization, which are optimal for a small model, and you scale those
hyper parameters according to, you know, the B's and C's. So for example, if the B's are one, so
like it means like an inverse, then you want to scale those so that like you have number when you
double the width, right? So if you scale them like so, then you're guaranteed that like things will
stay approximately optimal as n goes to infinity, right? So no matter how big you scale your model,
you have some kind of guarantee that you're never going to be far away from the optimal
hyper parameters you could use if you tuned all the hyper parameters directly on the large model.
Sure. I mean, the point is this, like having a limit is a stability result. It's saying that as
you let n get large, something is converging to something. And this maximal update.
Well, again, I didn't say maximal update here, but yeah, okay. Yeah, it is the maximal update.
Yeah, this is called the maximal update parameterization.
Aka mu p, right?
Yeah.
Yeah, this is your fifth and most recent paper. Yeah. So yeah, this maximal feature learning,
maximal update is such that because of the stability guarantees of this convergence
result, you're able to transfer hyper parameters in a theoretically grounded way.
Yeah, yeah, yeah. And like it's like this, this is like the unique one that can do this
in the general case. I mean, which is kind of, if you think about it, it's obvious because
like the optimal hyper parameters has, there's only one way to scale it, right? If you've got,
you know how the hyper parameters scale. And this is like one thing that can do it,
then no other way of scaling can do it.
So actually, if you tried to use any of these other green ones, why can't you transfer hyper
parameters? So I'm just saying here, I'm just saying that if anything can do it,
if there exists an awful way of scaling hyper parameters, then no other way of scaling it
is optimal, right? Kind of by definition because there's only one way to do it.
Sorry, I'm not following. Like all these green points have infinite width limits.
They have some feature learning. Can you also transfer?
No, like they don't, right? Because, okay, I mean, so like the concrete answer, for example,
is something like this. Like if in the, any green point other than the mu p point,
essentially differs from mu p in the sense that like some like learning rate is goes to zero,
effectively goes to zero as the width goes to infinity. So if like that hyper parameter,
like, right, like the awful hyper parameter for that learning rate is probably like not actually
zero in the limit. So like to compensate for this, like in that parameterization,
your nominal learning rate would go to infinity, right? In that parameterization. I'm gonna say
for the fact that it goes to zero in that parameterization.
I see. I see. I see. Basically, okay, you, you, I think the point is that you bought,
you want both the features learning to converge and the hyper parameter converge. If one goes to
zero, the other one has to go to infinity. So, so you don't want that situation. Yeah.
So like really the thing, the simple thing I want to say is that if you know that the
optimal hyper parameters scale one way, right? And like you then you change parameterization
so that like, like, you know, fixed hyper parameter goes to like infinity or zero,
then obviously that's not the right way of doing it. Like, does that make sense? I'm just saying,
like, I can only be one way to scale. If you know that, like, like, you know,
you know, there is, you know, you know, a particular scaling is correct,
like it preserves optimality, then no other scaling can do the same thing, right?
It's like, it's a uniqueness property. Sure. Sure. Sure. Yeah. Yeah. Yeah. Okay. Okay.
Yeah.
Okay, we're just getting to the optimality is unique, right? Yeah. Yeah.
If you know this thing is optimal, I just think something really dumb, which is that like, if
you, if you try to do like the law of large numbers with n squared instead of n, well,
your learning rate, which is could be thought of as your coefficient in front of that
quotient could could blow up by being n so that you have n over n squared equals one over n.
But that's like, that doesn't count. That's cheating, right? So you want, yeah. Okay. Yeah.
Okay. Yeah. Okay. I think we understand each other. Okay. Anyways, I think this is probably
a good place to stop. We've talked for quite a while now. Yeah, this was a lot of fun, Greg.
Thank you so much for your time. Very, very deep and beautiful mathematics.
Part of the reason why I wanted you to be on my podcast is that I think you're honestly,
I think you're, I mean, I think your work certainly is, is recognized by, by people,
but I think it deserves to be known more. I don't know how many people in the in the pure math
community have come across it because of course you're an AI researcher in industry, you don't
even have a PhD. So I guess maybe, maybe, you know, academics haven't picked you up. I don't know.
But I hope this gets developed further and, and you know, it will be appreciated by both
mathematicians and machine learning practitioners. Yeah, for sure. For sure. Yeah. I appreciate
this opportunity to talk it out with you. Great. Thank you. Yeah. All right. Yeah. Thanks, man.
