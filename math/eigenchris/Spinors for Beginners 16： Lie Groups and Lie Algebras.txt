We're starting a new section of this video series, where we talk about Lie groups and Lie algebras, and how they relate to spinners.
In particle physics, we classify particles according to their fundamental properties, such as mass and electric charge.
One important fundamental property is the particle's spin, which is either an integer or a half-integer.
All known fundamental particles have spins of either zero, one-half, or one.
For spin zero, we have the Higgs boson.
For spin one-half, we have the matter-type particles like quarks, electrons, and neutrinos.
And for spin one, we have force-carrying particles like photons, gluons, and the WNZ bosons.
The value of a particle's spin determines how we describe it in a mathematical equation.
Spin zero particles are represented by scalars.
Spin one-half particles are represented by spinners, and spin one particles are represented by vectors.
The spin value is sort of like the rank of the tensor used to represent the particle.
Scalars are rank zero tensors. Vectors are rank one tensors.
And spinners, loosely speaking, are kind of like rank one-half tensors,
since they rotate half as much as vectors do when transformed.
It's also possible to combine fundamental particles into composite particles,
which can have higher spins like three halves.
These particles are represented by higher-dimensional objects that are tensor products of spinners and vectors.
The hypothetical graviton is spin two,
and this is related to the fact that the metric tensor from general relativity is a rank two tensor.
So what does this have to do with Lie algebras and Lie groups?
Well, it's important to know how a particle will behave
when we do a physical transformation like a rotation or boost.
The various different particle types will transform with different types of matrices.
For example, in 3D, vectors will rotate in the XY plane using this matrix,
and spinners will rotate in the XY plane using this matrix.
Scalars can be thought of as rotating with a one-by-one matrix containing the number one.
This basically means that scalars don't change their value in different rotated coordinate systems.
We say these matrices belong to Lie groups.
In mathematics, a group is a set of elements that can be combined together
and obey certain properties, such as having an identity element and inverse elements that undo transformations.
Since we can combine two rotations into another rotation,
rotate by zero degrees to get the identity matrix,
and take the inverse of rotation matrices to undo rotations,
sets of rotation matrices form a group.
A Lie group is a group that's continuous instead of discrete.
Reflection transformations are not a Lie group because they involve discrete jumps,
but rotations are a Lie group because we can smoothly move from a small rotation to a large rotation.
Every Lie group has a corresponding Lie algebra,
which is a special set of matrices that we can exponentiate to get Lie group matrices.
Understanding these Lie algebras can tell us a lot about how to transform various kinds of particles,
and all the different types of quantum operators and quantum mechanics belong to various Lie algebras.
So let's start off by trying to understand how to take the exponent of a matrix.
This 3x3 matrix here gives us a rotation in the xy plane by an angle theta.
This matrix is a member of the Lie group SO3,
the special orthogonal 3x3 matrices, which do rotations in 3D.
Orthogonal means the matrix's inverse is its transpose,
and special means the matrix determinant is plus one.
Now it turns out that this rotation matrix is equal to the exponential of theta
times this special 3x3 matrix here.
The matrix in the exponent is called a generator, and is a member of the Lie algebra SO3,
which is written with lowercase letters in a fractor font by convention.
So how do we make sense of this?
What does it mean to take the exponential of a matrix?
The answer lies in the Taylor series of e to the x.
Normally this is used when x is an ordinary number,
but we can replace x with a matrix M to give us this definition of a matrix's exponential.
While this gives us an infinite series, in our example, the matrix powers will form a repeating pattern.
So we actually only need to evaluate the first few powers.
So let's evaluate the first few powers of the matrix M and see what we get.
For M to the zero, we're going to say that any matrix to the power of zero gives the identity matrix,
similar to how any number to the power of zero gives one.
M to the one is just the matrix M itself.
To calculate M squared, we just do M times M.
And we get a matrix with the entries negative one, zero, zero, negative one in the top left corner,
and all the other entries are zero.
M to the three just gives M times M squared.
And after multiplying this out, we find the result is negative M.
M to the four is M to the three times M, which is just negative M times M, or negative M squared.
And it turns out that M to the five, after using some of the identities we've discovered, is just M.
So this is where the cycle starts repeating in the Taylor series.
So we take M to the zero to be the identity matrix, and the next four powers of M just repeat in a cycle forever.
So let's take the first few terms in our infinite series and plug in these matrix powers.
Let's focus on adding together the entries in the top left corner of the matrix.
We see this gives us the Taylor series for cosine of theta.
Looking at some of the other entries, we get more Taylor series for negative sine theta, sine theta, and cosine theta.
And the bottom right corner of the matrix is just one because of the one in the identity matrix at the start of the series.
So after using a few tricks with the Taylor series, we can indeed see that this matrix exponential gives us a three by three rotation matrix in the XY plane.
So we know that when we exponentiate this generator matrix times an angle, we get a rotation matrix in the XY plane.
But how did I know to use this generator matrix specifically?
It turns out that there's a straightforward recipe to go from a Lie group matrix to its Lie algebra generator.
Let's say that we have e to the s theta, where s is just a number.
If we take the derivative with respect to theta, a factor of s comes down in front by chain rule.
Next, if we set theta to zero, the exponential goes to one, and we're left with just s.
So to isolate s, we use two steps.
Take the derivative, then set the parameter to zero.
This recipe works in the case of a matrix M2.
Taking the derivative with respect to theta brings a copy of M down in front.
You can try proving this by taking the derivative of the exponential series if you like.
Next, setting theta to zero means e to the zero matrix, which becomes the identity matrix.
So we're left with just the matrix M, which is our generator matrix.
So the recipe for getting our generator matrix M is to take our rotation matrix R in the Lie group,
take its derivative, and then set the parameter to zero.
Let's try this out on our XY rotation matrix.
Taking the derivative with respect to theta, sines become cosines, and cosines become negative sines.
And in the lower right corner, the derivative of the constant one is zero.
Then we set theta to zero.
Sine of zero gives zero, and cosine of zero gives one.
So we end up with the generator matrix I showed earlier in the video as expected.
You can pause the video and show that these rotation matrices in the YZ and ZX planes give these generator matrices.
From now on, I'm going to denote the generator matrices as GXY, GYZ, and GZX,
which generate the rotations in the XY, YZ, and ZX planes respectively.
This is getting a little off-topic for this video, but it turns out you can also take the exponential of a derivative operator using the Taylor series definition.
If we multiply this derivative by a parameter A and exponentiate it,
the resulting operator is a machine that translates a function by some amount A.
You can pause the video if you want to read the full proof of this.
So just as these 3x3 matrices generate rotation matrices on 3D vectors when exponentiated,
this derivative operator generates a translation operator on functions when exponentiated.
This is related to why the momentum operator in quantum mechanics contains a derivative,
because momentum is the generator of translations.
And it uses the complex I to ensure the operator is Hermitian.
I won't say any more about that in this video, but I've left some links in the description to videos that go into more detail.
Now, let's get back to our SO3 generator matrices.
It's worth looking at these generator matrices and seeing if they have anything in common.
It turns out that the generator matrices of SO3 are all traceless,
so the sum of their diagonal elements is zero and anti-symmetric, so their transpose equals their negative.
We can go ahead and prove this.
To prove the traceless property, I'm going to start with this theorem,
which says that for a matrix A, the determinant of E to the A is equal to E to the trace of A.
This is easy to prove if A is a diagonal matrix,
where its eigenvalues are along the diagonal.
In this case, E to the A just means taking the exponential of the individual diagonal elements.
And the determinant of this matrix would just be the product of the diagonal elements.
On the other hand, the trace of A is just the sum of the diagonal elements.
And if we exponentiate this sum, we see it is equal to the determinant we calculated before,
if we use basic exponent rules.
Now, this proof only works for diagonal matrices.
But since eigenvalues, the determinant, and the trace of a matrix are unchanged by a change of coordinates,
this proof also works for any matrix that is diagonalizable.
Our SO3 generator matrices are diagonalizable,
and so the corresponding rotation matrices are also diagonalizable,
provided we allow for complex matrix entries.
So the theorem we discussed earlier also works for our rotation matrices and their generators.
As an exercise, you can try to figure out why the rotation matrix and its generator
transform using the same coordinate change matrix C.
As a hint, remember to look at the Taylor series definition of matrix exponentiation.
So using this theorem, and knowing the determinant of an SO3 matrix is plus one,
we can write the rotation matrix in exponential form,
use the theorem to rewrite it as the exponential of the trace,
and since this exponential equals one for all theta,
we must conclude that the trace of the generator M is zero.
So we've proven that SO3 generator matrices are traceless.
Now let's prove SO3 generator matrices are anti-symmetric.
We know that for an SO3 matrix R, its inverse equals its transpose.
So R times R transpose equals the identity matrix.
Now let's take the derivative of both sides.
On the left side, we use product rule to get a sum of two products.
And on the right, the identity matrix is a constant.
So its derivative is the zero matrix.
Now we can rewrite the matrices as exponentials of the generators.
It turns out that the transpose of an exponential is just the exponential of the transpose.
So we can move this transpose operation to the generator.
Also, recall that the derivative of an exponential just gives us a factor of the generator out in front
because of chain rule.
Now this formula must be true for all values of theta.
So let's sub in theta equals zero.
This causes all the exponentials to go to the identity matrix.
This makes sense because a rotation by zero degrees is the identity.
We're left with the generator plus its transpose equaling the zero matrix.
Or in other words, the transpose of the generator equals the negative generator.
So this proves that the generators of SO3 matrices are anti-symmetric.
Now I want to give you a warning about proofs that involve matrix exponentials.
If you wanted to try proving that SO3 generators are anti-symmetric,
you might be tempted to just sub in these matrix exponentials
and use exponent rules to add the exponents together.
And use this to conclude that the generator matrices are anti-symmetric.
But this proof does not work for an important reason.
When exponentiating matrices, we cannot use the exponent rules we use for numbers,
where we turn a product of exponentials into the exponential of a sum.
The reason for this is, matrix multiplication does not commute in general.
And because of this, the two exponential formulas are not equal when we expand them as an infinite series.
The two formulas are only equal if the generators commute.
You can google the Baker-Campbell-Hausdorff formula if you want to learn more about this.
In the case of an anti-symmetric matrix, it does commute with its transpose,
because we can move the minus sign from one matrix to the other.
So the rule for turning a product of exponents into the exponents of a sum does work.
But we need to prove the two matrices commute before we can use this rule.
So we've proven that all SO3 generator matrices are traceless and anti-symmetric.
Now, I said earlier that these generator matrices are members of a Lie algebra.
Specifically, the Lie algebra SO3.
But what does that mean exactly?
Remember, an algebra is basically a vector space where addition, subtraction, and multiplication between vectors is defined.
We can loosely think of these generator matrices as being like vectors.
Just as we can add and subtract vectors to get more vectors,
we can also add and subtract generator matrices to get more generator matrices.
When we add two traceless anti-symmetric matrices together, the result is also traceless and anti-symmetric.
So we know the result of adding two SO3 generators is another SO3 generator.
The same is true for subtracting generators.
But in an algebra, we also need to be able to multiply vectors together.
You might assume the multiplication rule between generators is just matrix multiplication.
But this isn't quite correct.
The product of two anti-symmetric matrices is not always anti-symmetric.
So the result of multiplying two SO3 generators is not always another SO3 generator.
That is, the exponential of this product doesn't always give us a rotation matrix in SO3.
We can check this explicitly by multiplying the xy generator with the yz generator.
We get this matrix here.
This matrix actually squares to zero.
So when we exponentiate it, only the first couple terms in the series are non-zero.
And we get this, which is not a rotation matrix.
However, let's try something else.
I'll multiply the generators xy and yz,
and then subtract their product in the opposite order, yz times xy.
If we work through the matrix multiplications and subtract,
we get this matrix, which is the zx generator matrix.
So it turns out that when we multiply two generators in either order and subtract the result,
we get another generator.
This operation on matrices is called the matrix commutator.
And in the context of Lie algebras, it is often called the Lie bracket.
This is what we will use as our vector multiplication rule in Lie algebras,
since it's a way of combining two generators to give us a third generator.
If we work out all the commutators for our rotation generators,
we get these commutation relations here.
This set of generators of the Lie group SO3,
along with the rules for addition, subtraction, and multiplication with the Lie bracket,
form the Lie algebra SO3.
Again, Lie algebras are conventionally written using lower case letters with a fractor font.
In some more abstract books, you might see the Lie bracket defined like this,
using the alternating and Jacobi properties instead.
But the matrix commutator satisfies both of these properties,
so the matrix commutator is a valid Lie bracket.
To discuss these SO3 commutation relations further,
I'm going to rewrite our SO3 generators as G1, G2, and G3.
Now, in a general Lie algebra, when we have a set of basis generators,
the Lie bracket of two basis generators doesn't always give us back another basis generator.
However, the Lie bracket result will always give us a linear combination of basis generators.
The coefficients in this linear combination, denoted F with indices i, j, k,
are called the structure coefficients for this Lie algebra.
If we look specifically at the Lie algebra SO3,
we can get the structure coefficients by looking at the first, second, and third Lie brackets,
and then noting that the commutator is anti-symmetric.
So we get a sign change when we swap the lower two indices of the coefficients.
All other coefficients are zero.
This is sometimes summarized using the anti-symmetric symbol epsilon,
also called the Levy-Civita symbol.
Now, how do we know these commutators will always give us a linear combination of generators?
To prove this, I need to talk about how a Lie algebra is like a tangent space
at the identity element of its corresponding Lie group.
Remember, a Lie group is a group that's continuous,
so it forms a continuous curved space, which is also called a manifold.
Each point in this space would be a different SO3 rotation matrix.
Let's imagine that this curve in our Lie group space
is a path containing all the rotation matrices in the XY plane that we get by increasing theta.
The point on this path where theta equals zero would be the identity matrix,
since a rotation by zero degrees is the identity.
When we take the derivative with respect to theta,
it's like we're looking at the tangent vectors along this curve.
And when we set theta to zero,
this means we're looking at the tangent vector specifically located at the identity element.
So the derivative at theta equals zero is a tangent vector located at the identity matrix.
So we can think of our generator matrix as being a tangent vector at the origin or identity of our Lie group.
The set of all generators is like a flat space that's tangent to the Lie group at the identity.
Keep in mind, SO3 is actually a three-dimensional space with three basis elements,
but I'm drawing it as a 2D tangent plane just because that's easier to visualize.
Now, I'm going to use this idea of paths in the curved Lie group space
to prove some properties about general Lie algebras.
So instead of dealing with Lie group members that are rotations in SO3, parameterized by angles,
I'm going to deal with general members of a general Lie group, Big G,
whose generators live in the Lie algebra, small g.
I've taken these proofs from the textbook Naive Lie Theory by John Stillwell.
First, let's prove the sum of two tangent vectors in a general Lie algebra will give another tangent vector.
Let's take group members A of s and B of s in the Lie group Big G.
These describe two paths in the Lie group parameterized by s.
When s equals zero, both of these give the identity transformation,
and their derivatives at s equals zero give their generators, which I'll write as small a and small b.
If we multiply the group members A of s and B of s, we get a new group member c of s.
Let's take the derivative of both sides and set s to zero.
On the right side, we use the product rule.
When we set s to zero, B of s and A of s just become the identity element.
And when we look at the derivatives at s equals zero, we get the generators A plus B.
So we see the sum of tangent vectors small a plus small b gives another tangent vector,
which is the vector tangent to the curve c of s at the identity.
I'm going to make a small aside here to explain the difference between a generator and a tangent vector.
I've already explained that with matrix exponentials,
the product of two matrix exponentials is not always equal to the exponential of the sum.
However, if we expand these two formulas out as Taylor series,
it turns out that the first order terms in both series are equal,
even though the later terms in the series are not equal.
This means that both of these formulas have the same tangent vector at the identity.
It's important to keep in mind that there are an infinite number of possible curves
that go through the identity that have the same tangent vector at the identity.
However, when we're treating this tangent vector as a generator and exponentiating it,
it only generates one unique curve in the Lie group,
which is the unique curve we get when we exponentiate the tangent vector
multiplied by a curve parameter.
Now, let's prove the scalar multiple of a tangent vector gives another tangent vector.
Let's take the path a of s in the group and multiply the parameter s by some constant r
to give us a of r times s.
Now, let's take the derivative and set s to zero.
Using chain rule, we get a vector of r out in front,
and the derivative at s equals zero becomes the generator small a.
So, we see that the scalar multiple of tangent vector small a gives another tangent vector,
which is tangent to the curve a of r times s.
So, we've shown that we can add tangent vectors together and scale them to get more tangent vectors.
This is the basic definition of a vector space.
But to prove that the set of tangent vectors forms a Lie algebra,
we need to prove the commutator of tangent vectors gives another tangent vector.
To prove this, we'll look at a path in the group big g with three legs.
a of s, then b of t, then a inverse of s,
which takes us along the path a of s but in the opposite direction.
We'll call this path d of s and t, and it has two parameters.
This a, b, a inverse formula is called a conjugation.
If we take a constant s and take the derivative with respect to t,
the derivative only affects b of t, since a of s doesn't depend on t.
Then if we set t equals zero, we get the Lie algebra generator small b.
Now, let's take the derivative with respect to s,
which gives us a product rule on the right hand side.
If we set s equal to zero, both a of s and a inverse of s go to the identity element.
The derivative of a at zero becomes the generator small a,
and the derivative of a inverse at zero is just the generator that points in the opposite direction,
which is negative small a.
So we get small a small b minus small b small a,
which is the commutator of small a and small b.
Now, what exactly is this object on the left hand side?
Where we've taken the derivative with respect to two different parameters.
For constant s, d of s and t is just a path in the Lie group.
That depends on t.
And when we take the derivative with respect to t and set t to zero,
this becomes a tangent vector in the tangent space located at t equals zero.
Allowing s to vary, every value of s gives us a different tangent vector in the tangent space.
So we get a continuous path of tangent vectors parameterized by s.
If we then take the derivative with respect to s,
we get tangent vectors along this path.
And setting s to zero gives us the specific tangent vector at the identity.
Now, since tangent vectors in a vector space also belong to that same vector space,
this tangent vector at s equals zero also belongs to the tangent space of our original path in the Lie group at t equals zero.
So all of that shows that the commutator of two tangent vectors gives another tangent vector.
So the tangent space of a Lie group at the identity does indeed form a Lie algebra.
So let's review what we've discovered so far about the Lie group s03.
We found that each of the three rotation matrices has a generator matrix.
And we can exponentiate the generator times the angle to get the rotation matrix.
The two-step process for getting these generators is to take the derivative of the rotation matrix
and then set the parameter to zero.
Using the algebraic properties of matrices,
we found that the generators of the s03 Lie group are the set of traceless anti-symmetric matrices.
We also proved that the commutator of generators always gives a member in the tangent space of generators at the identity,
according to these rules here.
Now let's repeat this process for the Lorentz group s03.
This is the set of spacetime transformations that keep the spacetime interval
with one plus sign and three minus signs unchanged.
The special s means the matrix determinant is plus one, and so spatial reflections are forbidden.
The plus means the top left element of the matrix is positive, and so time reflections are forbidden.
This group includes three rotation and three Lorentz-boost matrices.
We can get the generators for these matrices the usual way
by taking the derivative and setting the parameter to zero.
The rotation generators denoted with J's are traceless and anti-symmetric, as we saw before.
The boost generators denoted with K's are traceless and symmetric.
These six generators form a basis for the Lie algebra so plus one three.
We can also work out the commutators for these matrices to get these commutation relations here.
This is the Lie bracket rule in the s03 Lie algebra.
Note that the commutator of two rotation generators gives a rotation generator.
The commutator of two boost generators also gives a rotation generator.
And the commutator of a rotation generator and a boost generator gives a boost generator.
This shows us that the s03 Lie algebra lives inside the s03 plus one three Lie algebra.
So in this video we've looked at the Lie groups s03 and s03
and discovered their respective Lie algebras.
But these Lie group matrices only operate on vectors.
So three matrices operate on 3d space vectors and so plus one three matrices operate on 4d space time vectors.
Since vectors are rank one tensors, we say these matrices are part of the spin one representation.
This helps us understand how the spin one particles behave when we do rotations and boosts.
But what about other particles like the spin one half particles?
This will be covered in the next video where I cover the spin one half representation
which is one of the multiple different representations of the s03 and s03 Lie algebras.
If you've ever seen the complex number i represented as a matrix that squares to the negative identity
you might know that there are multiple different matrices of all different sizes that satisfy this property.
The same is true for our Lie algebra generators.
We've seen the 3 by 3 s03 generator matrices and the 4 by 4 s03 generator matrices.
These are from the spin one representation because they transform vectors.
However, we can also find 2 by 2 matrices that follow the same Lie algebra rules in both cases.
These are from the spin one half representation because they transform spinners.
I'll go through all of this in the next video.
Also, one final note, depending on the textbook you read,
you might see that the generators I've shown for rotations and boosts look a little different.
Depending on whether they follow the math convention or the physics convention.
In this video, I follow the math convention
where the generators of 3D rotation matrices are traceless and anti-symmetric.
In the physics convention, it's often the case that the generators are multiplied by negative i
where i is the imaginary unit.
This makes them traceless and Hermitian matrices
where Hermitian means a matrix equals its complex conjugate transpose.
To compensate for this extra factor of minus i,
there needs to be an additional factor of i with the opposite sign used in the exponential to cancel it out.
Physicists use this convention because observables in quantum mechanics
must be represented by Hermitian operators,
which end up being the same thing as Lie algebra generators.
This Hermitian property is required so that observables like momentum and spin
always give real numbers instead of complex numbers.
In the case of rotations, the generators will be Hermitian in the physics convention
instead of anti-symmetric, as with the math convention.
Pure mathematicians don't require this Hermitian condition,
and so they don't bother with these extra factors of i in the generators or the exponential formula.
