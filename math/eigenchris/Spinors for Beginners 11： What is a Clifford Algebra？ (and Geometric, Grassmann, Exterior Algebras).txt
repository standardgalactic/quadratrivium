In this video, we're going to introduce Clifford Algebra's, also called Geometric Algebra's,
for understanding spinners. So far in this series, we've looked at spinners in three dimensions,
called Pauli spinners, and spinners in four-dimensional spacetime, called Vile spinners.
We came across these Pauli spinners and Vile spinners almost by accident, by rewriting 3D and
4D vectors as 2x2 matrices, and finding that we could factor them into a pair of spinners.
Clifford Algebras are useful because they stop making spinners look like accidents.
Clifford Algebras give us a consistent recipe for building spinners in any dimension,
like 3, 4, 7, or 20 dimensions. This recipe says that spinners are members of minimal left ideals
in Clifford Algebras. This recipe probably doesn't make much sense now, but it should
make sense over the course of the next few videos. Clifford Algebras also unite many types
of mathematical objects together. In physics, we deal with various objects like scalers,
vectors, bivectors, linear maps, and spinners. All of these objects live together side by side
in Clifford Algebras. Doing physics with Clifford Algebras feels a bit like doing physics with
the batteries included. Oftentimes, everything we need to solve a problem in physics already lives
inside a Clifford Algebra, and we don't need to invent any new mathematical objects to solve the
problem. For example, Clifford Algebras allow us to unite the four Maxwell equations of electricity
and magnetism together into a single equation. The YouTuber Suji Lakmo has some fantastic
introduction videos for Clifford Algebras, and if you're looking for some more advanced videos
on Clifford Algebras, you can look at the Bivector YouTube channel. For Clifford Algebra content on
spinners specifically, I'll be referring to this thesis by Crystal and Mackenzie in later videos.
I've linked all of these sources in the description. So, what exactly are Clifford Algebras?
They can be thought of as a generalization of the complex numbers, quaternions, and the sigma
matrices. All of these algebraic systems involve symbols that square to either negative one or
positive one, and also involve symbols where swapping the order of multiplication gives us
a negative sign. This is also called the anti-commutative property. Loosely speaking,
Clifford Algebras are algebraic structures where we can square symbols to plus one or minus one,
and also swap their order to get a negative sign. A good first step for learning Clifford Algebras
is to begin by understanding Grassman Algebras, also called exterior Algebras. I realize I'm
throwing a lot of names out right now, but Grassman Algebras can be thought of as a stepping
stone to learning Clifford Algebras, so we're going to start with learning Grassman Algebras.
The key idea in Grassman Algebras is the wedge product. If we have a vector u and another vector
v, their wedge product, u wedge v, is a plane segment formed by u and v, with an orientation
that follows the arrows. If we swap the order of these vectors and look at v wedge u,
we get the same plane segment, but with the opposite orientation. These oriented plane
segments are called bi-vectors. These bi-vectors are a useful way for representing rotations
and angular momentum. In introductory physics classes, angular momentum is usually represented
using a vector, which you get by using the awkward right hand rule, where you curl the
fingers of your right hand in the direction of the rotation and stick your thumb out in the
direction of the angular momentum vector. Not only is this awkward, but it can lead to strange
geometric results. For example, if we reflect a rotating object in a mirror, the angular momentum
vector points in the opposite direction. This is a very strange behavior for a vector,
and indicates we might be doing something wrong. However, when we represent rotations and angular
momentum using bi-vectors instead, the direction of rotation is immediately obvious, and when we
reflect the rotation in a mirror, the bi-vector changes in a predictable way, reversing the
orientation in a way that makes sense. When we have a vector u, its opposite minus u is a parallel
vector of the same length, but pointing in the opposite direction. When we add u and negative
u together, we get zero. Similarly, when we have a bi-vector u wedge v, its opposite v wedge u is
a bi-vector of the same size, but oriented in the opposite direction. When we add u wedge v and v
wedge u together, we get zero. For this reason, we can also write v wedge u as negative u wedge v,
because it's the same bi-vector as u wedge v, but with the opposite orientation.
This gives us our first important property of the wedge product. When we swap the order of the
vectors in the wedge product, we get a negative sign in front. This means that the wedge product is
anti-commutative. When it comes to vectors, a vector's magnitude is equal to the length of its
arrow. If a vector has length zero, we call it the zero vector. For bi-vectors, a bi-vector's
magnitude is equal to the area of its plane segment. If a bi-vector has zero area, we call it the zero
bi-vector. Notice that if we take the wedge product of two vectors that are parallel, we end up with
an area of zero, so we get the zero bi-vector. In particular, the wedge product of a vector with
itself always gives zero. When we have the wedge product used over a sum of vectors, we can distribute
the wedge product over the sum, similar to the way we distribute multiplication over brackets
in high school algebra. This has an easy visual interpretation. If we add u1 and u2,
and then take the wedge product with v, this is the same thing as taking u1 wedge v and u2 wedge v
and adding them together into a single oriented plane. This gives us the distributive properties
of the wedge product. Also, when we multiply u wedge v by a scalar, like 3, we can either
multiply the u by 3 or multiply the v by 3. Mathematically, the results are the same.
Now, these results might look different to you visually, however, as long as the areas of the
two plane segments are the same and the orientations are the same, the bi-vectors are considered equal.
It doesn't matter if we draw the areas as squares, rectangles, parallelograms, circles,
or some other shape, and the angle or rotation of the shape also doesn't matter.
For example, any bi-vector drawn as a parallelogram can be redrawn as a rectangle.
The portion of the wedge product involving parallel vectors goes to zero, so we only need
the orthogonal portion of the vectors. Also, the property of the wedge product being anticommutative
and the property of a vector wedged with itself equaling zero are actually equivalent properties,
and can be derived from each other. If we take the anticommutative property and set v equals u,
we get something that equals the negative of itself, which must be zero. So that proves
the equivalence in one direction. If we instead start with this wedge product of a sum, u plus v,
with itself, and assume anything wedged with itself is zero, we can distribute to get four terms,
two of which go to zero, and we're left with the anticommutative property. This shows the
equivalence in the other direction. Now, we've talked about the abstract properties of bi-vectors,
but I'd also like to talk about bi-vector components. Let's say we're working in 2d
space with basis vectors ex and ey, and we're looking at the bi-vector u wedge v. What are this
bi-vector's components? We can write u and v as linear combinations of the basis vectors,
then distribute over the wedge product to get four terms, first, outer, inner, last.
The ex wedge ex and ey wedge ey terms go to zero, and we can rewrite ey wedge ex as negative ex
wedge ey, using the anticommutivity of the wedge product. So we find that u wedge v has only one
component with the basis bi-vector ex wedge ey, and the component is ux vy minus uy vx,
which is the formula for the area of a parallelogram using vector components.
There is only one basis bi-vector here because in the xy plane, there is only one possible pair of
axes. In 3d space, the formula for u wedge v has three components, because there are three possible
pairs of axes, yz, zx, and xy. As an exercise, you can try proving that these are the components.
These end up being the same components that result from the cross product of u and v.
If we continue using the wedge product, we can build higher dimensional multi-vectors,
like tri-vectors, quad-vectors, and so on. However, we can't keep going up forever.
In three dimensions, for example, the tri-vector is the highest we can go. If we include an extra
wedge product, the result will always go to zero, because we'll always have a vector wedged with
itself somewhere. Also, take note that in three dimensions, there is only one tri-vector, possibly
with a scaling constant in front, since we can always swap the basis vectors into the xyz order
by introducing minus signs as needed. In space with a given dimension, we can count the number of
basis multi-vectors of each type just by counting the number of possible basis vector combinations.
As a result, the multi-vectors in a given Grassman algebra form a sort of diamond shape,
with low-grade multi-vectors at the bottom and high-grade multi-vectors at the top.
Every multi-vector has exactly two possible orientations, given by a plus or a minus sign.
We reverse the orientation of a vector just by flipping the arrow's direction.
We can build a bi-vector using a set of vectors, all oriented so that the tip of one touches the
tail of the next, forming the boundary of a plane segment. The orientation of this bi-vector just
follows the orientation of the vectors along the boundary. We can reverse the orientation of the
bi-vector, introducing a negative sign, just by reversing the orientation of the vectors along
the boundary. This logic applies for higher dimensions as well. We can build a tri-vector
using a set of bi-vectors that form the faces of a cube. The rule is, each bi-vector must be
oriented opposite to its neighbors, wherever they share an edge. If we want to reverse the
orientation of a tri-vector, we just reverse the orientations of all its bi-vector faces.
The approach of using the boundary to define the orientation of a multi-vector
applies for multi-vectors of all dimensions. So that's Grassman Algebras, which are algebraic
structures over vector spaces that use the wedge product to build multi-vectors.
Now, what's a Clifford Algebra? Loosely speaking, it's an algebra where symbols square to either
plus one or minus one, and flipping the order of symbols introduces a negative sign, similar to what
we saw with Grassman Algebras. This multiplication rule is called the Clifford Product, or Geometric
Product, which is denoted by writing symbols next to each other like this. We'll soon see that this
leads us to the complex numbers, the quaternions, the sigma matrices, and their generalizations.
Instead of starting with a formal definition of Clifford Algebras, I'm going to go through a few
examples, and then I'll give a definition at the end. So let's look at some examples of Clifford
Algebras, which are Algebras with symbols that square to plus one or minus one.
Let's say we have a symbol i that squares to minus one. One way to approach this is to interpret
this as a matrix problem, saying that the number one is actually the identity matrix, and the symbol
i is actually a matrix like this that squares to the negative identity. The expression a plus i b
would then give you a matrix like this. Another way to approach this is to simply treat i as its
own symbol, with i squared equals negative one as the definition. This is the more traditional
approach, and it's how we normally define complex numbers. What we have here is the Clifford Algebra
CL01, because we have zero symbols that square to positive one, and a single symbol that squares to
negative one. So the complex numbers, also called CL01, are a first example of a Clifford Algebra.
So we have two options for looking at Clifford Algebras. We can either look at them using matrices,
or we can look at them using abstract symbols. Now we normally take the second approach of
abstract symbols as the more pure or correct definition of the complex numbers, and we prefer
it over the matrix approach. The reason is there are an infinite number of matrices of all different
sizes that square to the negative identity matrix. We say that these are all matrix representations
of the abstract symbol i, but we don't actually need any of these matrix representations to do
math with complex numbers. It's often simpler to just take the symbol i as its own object and
forget about matrices. As an exercise, you can try multiplying the expression a plus i b times c
plus id in both matrix form and symbolic form, and show that they give equivalent results.
Here's another example of a Clifford Algebra. Let's say that we have a symbol j which squares to
plus one. Again, we can interpret this as a matrix problem, saying one is the identity matrix,
and j is a matrix like this that squares to the identity. So the expression a plus j b would
correspond to this matrix here. But once again, there are an infinite number of matrices of all
sizes that square to the identity. A simpler approach is to just treat j as its own symbol
with j squared equals plus one as the definition. This is the Clifford Algebra cl10, because it has
a single symbol that squares to plus one, and zero symbols that square to minus one. This is also
called the split complex numbers. Here's another example. We have three symbols, sigma x, sigma y,
and sigma z that all square to positive one. But since we now have more than one symbol,
we now also need to decide how these symbols will multiply with each other. Let's take some
inspiration from Grassman Algebras. Let's say that when we multiply two of these Clifford Algebra
symbols together, we can flip their order if we include a minus sign. If we approach this as a
matrix problem, we get the sigma matrices or Pauli matrices that we spent a lot of time talking about
in videos six to ten. But the Clifford Algebra style approach is to forget about the matrices
and just treat these three sigmas as their own symbols that square to positive one
and anti-commute with each other. Actually, if you were to go back and watch video number six,
which covers sigma matrices in detail, you'll notice that almost everything in the video
can be figured out without writing down any matrices, and just using these squaring and
anti-commutative properties of the sigmas. Again, the sigmas only approach is seen as
simpler because we can get all the same results without worrying about matrices. This gives us
the Clifford Algebra CL30 because we have three symbols that square to plus one and zero symbols
that square to minus one, and the three symbols also anti-commute with each other. This is also
called the algebra of physical space. Pairs of different sigmas give us the by vectors in CL30.
And it turns out these are completely equivalent to the Quaternion imaginary units i, j, k.
The reasons i, j, and k anti-commute with each other is rooted in the fact that the sigma pair
by vectors also anti-commute with each other. We say that the Quaternions are the even grade
subalgebra of CL30 because they only involve the grade zero scalar and the grade two by vectors.
Here is one final example with four symbols. Gamma zero, gamma one, gamma two, and gamma three.
Gamma zero squares to positive one and the other three gammas square to negative one,
and all the gammas anti-commute with each other. Treating this as a matrix problem, we can get
these matrices, which are called the Dirac matrices or gamma matrices. These will pop up if you've
ever studied the Dirac equation for particle physics or quantum field theory. And again,
there are multiple solutions. This set of matrices is called the chiral basis or vial basis.
There's also this alternative solution called the mass basis or Dirac basis. But as usual,
I'm going to say it's better to just treat the gammas as their own symbols that obey these
that obey these squaring and anti-commutative properties and take those as the definition.
This gives us the Clifford algebra CL13 because we have one symbol that squares to plus one
and three symbols that square to minus one. This is also called the spacetime algebra.
Suji Lakmo has a great video showing how this can be used to formulate special relativity,
linked in the description. So generally speaking, we say that the Clifford algebra CLPQ
is an algebra with p symbols that square to plus one and q symbols that square to minus one.
And the symbols all anti-commute with each other. Sometimes if q equals zero and there are no
symbols that square to minus one, we can omit the q in our notation and just write CLP for a
Clifford algebra with p symbols that square to plus one and anti-commute. I should mention that
this anti-commutation relation is sometimes written like this with curly braces and the dot
product or metric. If the two symbols are different, these two terms in the sum cancel
and we get zero, since the dot product of orthogonal vectors is zero. If the two symbols
are the same, we get twice the vectors squared length. As a quick aside, sometimes in Clifford
algebras, we also include symbols that square to zero. If we have a symbol epsilon that squares to
zero, we can again interpret this as a matrix problem, where zero is the zero matrix and epsilon
is a matrix that squares to zero, like this. But it's more common to treat epsilon as its own
symbol with epsilon squared equals zero as the definition. These are called the dual numbers.
In Clifford algebra notation, we call this CL001, where the first two numbers tell us how many
symbols square to plus one and minus one, and the third number tells us how many symbols square to
zero. These won't come up very often in this video series, but it's useful to know about.
So let's compare the wedge product from Grassman algebras to the Clifford product in Clifford
algebras. Now with Grassman algebras, I was often talking about vectors. In Clifford algebras,
I was loosely talking about symbols, but we're going to interpret these symbols as basis vectors
in Clifford algebras. For example, with the Sigma matrices CL30, we can think of the three symbols
as forming a 3D orthonormal basis. This is how we've been treating the Sigma matrices in previous
videos. With Grassman algebras, the wedge product of a vector with itself always gives zero,
because the resulting plane segment has an area of zero. With Clifford algebras, taking the Clifford
product of a vector with itself gives that vectors squared magnitude. Unit vectors in our
orthonormal basis square to plus one or minus one, because they have a length of one. Likewise,
a vector of length five will square to plus or minus 25. If you're confused about why a vector
would square to negative one, this ends up being useful in special relativity when we need to tell
the difference between time-like vectors and space-like vectors. With Grassman algebras,
any time we have a wedge product of two vectors, we can always flip their order if we include a
negative sign, which is the anti-commutative property. For Clifford algebras, the anti-commutative
property applies only for orthogonal vectors. For the sigmas, sigma x, sigma y, and sigma z
are all orthogonal, so they all anti-commute with each other. But a given sigma will not
anti-commute with an arbitrary vector v, which is not orthogonal to it. Let's look at the Clifford
product of two vectors in CL20, where the ex and ey basis vectors both square to plus one.
We can write vectors u and v in terms of this basis. We can then write the Clifford product
u times v using distributive properties. First out, inner, last, and get four terms.
The terms with ex squared and ey squared both go to plus one, so we get a sum of scalars.
Since ex and ey are orthogonal, we can flip ey times ex to get negative ex times ey,
using anti-commutativity. We can then group these two terms together.
So, the Clifford product of two vectors is the sum of a scalar and a bivector.
The scalar part looks like the dot product of u and v, and the bivector part looks like the wedge
product of u and v. So, loosely speaking, the Clifford product can be thought of as a combination
of the dot product and the wedge product. When we take the Clifford product of two orthogonal
vectors u and v, the dot product goes to zero, and we're left with just the wedge product,
so we get to use the anti-commutivity of the wedge product in this special case.
When we take the Clifford product of two parallel vectors u and v, the wedge product part goes to
zero, and we get u dot v, which also equals v dot u due to the commutivity of the dot product.
And remember, the dot product of a vector with itself gives that vector's squared length.
Before I finish the video, I'm going to go over some more abstract definitions of
Grassman Algebras and Clifford Algebras that involve tensor algebras and quotients.
This is a little more advanced, and you probably don't need to worry about it if you're a beginner
with Clifford Algebras. I'm just including this section for completeness for those who want to
know about it. I've already talked about the tensor product in video number eight. As a basic
review, the tensor product is like an abstract generalization of the outer product of a column
vector and a row vector. It obeys distributive properties over addition, just like array
multiplication, and distributivity works in both directions. And when multiplying by a scalar,
we can either give the scalar to the left-hand side or the right-hand side.
We can create tensor products of two, three, four, or any number of vectors.
Now I'll introduce the tensor algebra. Let's say that we have a two-dimensional vector space v
with basis vectors e1 and e2. The tensor algebra of v, denoted t of v, includes scalars, all the
vectors in v, all the tensor products of two vectors in v, all the tensor products of three
vectors in v, and so on up to infinity. As we increase the number of tensor products,
the number of possible terms increases exponentially. Individual vectors live in the vector space v.
Vector pairs live in v tensor v, also written v squared. Vector triples live in v tensor v tensor v,
or v cubed, and so on. Scalars live in the set of scalars s, which is sometimes written as v to
the power of zero. In tensor algebras, we can add together arbitrary elements from this infinite
collection. So we can write expressions like two plus three e1 minus five e2 tensor e1 tensor e2.
Now let's try modifying our tensor algebra with a special rule. We're going to say that anytime
we see a vector tensored with itself, it will go to zero. This leaves scalars unchanged, and also
leaves individual vectors unchanged. But with tensor products of two vectors, e1 tensor e1
and e2 tensor e2 now get sent to zero. But as we proved earlier, the property of a vector product
with itself equaling zero automatically implies that we can swap the order of a product if we
introduce a negative sign. So using this new rule, e1 tensor e2 equals negative e2 tensor e1.
For the tensor product of three vectors, it turns out every single term goes to zero,
since all terms contain at least one pair of identical basis vectors. This applies for any
larger tensor products as well. So using this new rule, we've eliminated a huge number of infinite
tensor products, and we're left with only four. What we're left with is exactly the rules for
the Grassman algebra. So abstractly, the Grassman algebra is what we get when we start with the
tensor algebra and apply the special rule that any vector tensored with itself goes to zero.
More formally, we say that the Grassman algebra is what we get when we take the quotient of the
tensor algebra by the ideal generated by v tensor v. We can start over and do this again,
but with a slightly modified rule, where we take any vector tensored with itself,
and instead of setting it to zero, we set it to that vector's squared length.
Using this rule, scalars and individual vectors are unaffected. e1 tensor e1 and e2 tensor e2
are now equal to the scalar one. This property also results in the anti-commutative property
if two vectors are orthogonal. This is because orthogonal vectors obey a Pythagoras formula,
which allows us to cancel the terms from both sides of this equation.
And so because e1 and e2 are orthogonal, we get that e2 tensor e1 equals negative e1 tensor e2.
For the terms with three vectors in the product, any identical vectors get sent to the scalar plus one,
so we're left with just individual vectors, which we already have in our algebra.
In fact, any more complicated combination of vectors will always give us either scalars,
vectors, or bivectors. So again, we've removed all the higher infinite number of terms,
and we're left with only four terms. This is exactly the Clifford algebra CL20,
where vectors square to their squared magnitudes and orthogonal vectors anti-commute.
So, abstractly, the Clifford algebra is what we get when we start with the tensor algebra
and apply the special rule that any vector tensored with itself goes to its squared length.
More formally, we say that the Clifford algebra is what we get when we take the quotient of the
tensor algebra by the ideal generated by v tensor v minus the squared magnitude of v.
You might also see this magnitude formula written with a g. This is called the metric tensor,
and it's just a function that gives vector lengths.
So, to conclude, a Clifford algebra, also called a geometric algebra,
is what we get when we take a vector space and allow multiplication between vectors using the
Clifford product, also called the geometric product. The Clifford product of a vector with
itself gives that vector's squared magnitude, and the Clifford product of two orthogonal
vectors is anti-commutative, so we can swap the order if we include a negative sign.
In the next video, we're going to see how we can use Clifford algebras to build spin groups
in any dimension. These are groups that double cover the rotation group in a given dimension,
and do rotations and Lorentz boosts using double-sided transformations.
