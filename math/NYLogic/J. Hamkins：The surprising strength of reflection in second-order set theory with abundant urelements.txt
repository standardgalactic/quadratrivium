Alright, hi everyone, welcome once again to the CUNY Setteries seminar, and it's a great
pleasure as always to have my former PhD supervisor Joel Hamkins in a way back with us at CUNY.
Alright Joel, go right, let's see, do I have the topic of your talk?
Yes, Joel is going to speak about the surprising strand of reflection in second order setteries
with abundant ore elements.
That's a mouthful.
Alright Joel, take it away.
Okay, great.
Well, it's really fantastic to be back here in New York and speaking to all of you and
seeing so many friends here and so on.
So I want to apologize, as I said, because I'm just going to be going through the draft
of the paper instead of having proper slides because I don't have any slides yet, prepare.
So I hope this is okay and please interrupt with questions at any time if you have questions
and I'm happy to entertain any kind of questions.
So this is joint work with Boca Yao who's here, I noticed, and he is a PhD student of
mine at the University of Notre Dame.
So what I want to do is, let's see, can I do it like this if I go like this and then make
it bigger?
Is that sort of visible for everyone?
This is best.
This is best.
Okay, so, and as I said, I put the draft in the chat, so if you joined after a few minutes,
maybe someone else can download and then upload that copy again, that's fine.
That's just a draft paper.
This is current work in progress, but I think it'll be ready pretty soon because it's mostly
finished.
Right, so I'm going to talk about set theory with ore elements.
So this is a kind of traditional way of doing set theory where you start with some already
existing primitive objects or atoms or ore elements and you build your set theory on
top of those ore element atoms.
So the picture maybe is like this.
It's a variation of the cumulative hierarchy where the atoms are all given initially at
the bottom and then we form the sets of atoms and sets of sets of atoms and so on.
And so of course, along with that cumulative hierarchy, the pure sets, the ones that don't
involve any atoms at all are also getting constructed, but then the sets outside of
the pure sets have some atoms in their hereditary membership.
Okay, so the notation, what happened?
Oh my gosh, let's see, I'm sorry, somehow it went to the end.
Okay, I apologize, oh, I'm sorry to be so lame about this, okay.
So the notation I want to use is V of A means the whole universe of all sets using the class
A of atoms and I reserve V to refer only to the pure sets, the sets that don't involve
any atoms.
And then the structure that we're going to talk about is the class of all sets and the
membership relation and then we have this predicate that picks out the atoms.
It's true of all and only the atoms.
There's a kind of connection, the atoms are something like the empty set in that they
are epsilon minimal, they don't have any members, but they're not sets.
So we distinguish the empty set from the atoms because the empty set is a set.
In particular, it's a set of atoms, it just doesn't have any elements.
So the empty set is not an atom and this predicate can therefore be defined if you just take the
empty set as a parameter, it's the only epsilon minimal object that's a set.
OK, so let's go on then.
What?
My gosh.
Oh, I see.
When I'm in this mode, it really wants to go many pages at once.
OK, so what are the axioms of this irrelevant set theory extensionality?
You have to change it a little bit because we want to say any two sets with all the same
members are identical, but the elements, of course, don't have any members and so they
all have the same members as each other, but they're not identical.
But OK, we just weaken extensionality to be talking only about the sets and not about
the atoms.
OK, the other axioms are just the sort of ordinary once foundation pairing and union power set
infinity and separation and collection and the axiom of choice and this is all stated
in the language with the element predicate.
One thing to notice is that I'm using collection and separation instead of just replacement
and in CFC in pure set theory, it doesn't matter whether you use collection and separation
or replacement because they're equivalent, but in the irrelevant context, this is no
longer true.
You cannot prove collection from replacement and so we included explicitly on the list
of axioms.
OK, so there's some sensitivity to what the exact axioms are because many things that
are equivalent in the sort of ordinary ZF or CFC context turn out not to be equivalent
anymore and so you have to really watch out that you're in the right theory when you're
dealing with elements.
OK, so let's talk about the elements support of a set.
This is just the set of atoms that appear in the hereditary membership hierarchy of that
set or in other words, the set of atoms that appear in the transitive closure of the set.
OK, it's also called the kernel.
And so for any class of elements, we can form the class V of B, which is just the sets that
whose support is contained in B. So the pure sets are is V of the empty set because those
are the sets that don't have any elements in their transitive closure.
But the entire universe is the union of all the V join W for every set of elements.
OK, because every every set will have a set of elements that are in its transitive closure.
And so this union is constituting everything, even when there are a class of elements.
If you have a subclass of elements, you can make the sub universe V bracket B, which is
the transitive class containing all pure sets whose I'm sorry, it contains all the pure
sets and also it's easy to see that it's closed under pairing union power set and so on.
And so in fact, it will be a ZFCU model if V of A is OK.
So the rank, my gosh, I'm really sorry with this page.
When I just do page down, I guess it's because I'm zoomed in and the scrolling doesn't work properly.
So I apologize for this completely lame presentation here.
OK, what are we doing?
So we did our element and support now the rank hierarchy.
So in ZFC, of course, we can define the V alpha rank hierarchy.
And if you try to do the same thing with elements, you get a problem
because if there's a proper class of elements, then these are not sets.
And so this is not a set like iteration.
And in fact, it's even worse than just that they're proper classes, but that in general,
you can't undertake class recursions like this unless you have the ETR principle.
It goes beyond the sort of ordinary recursion principles.
And so that that way of trying to define rank just is not going to work in the element context.
But nevertheless, you can still define rank if you just define it by the recursion on on sets individually.
The rank of a set is the supremum of the ranks plus one of its members.
And that is a set like recursion set, like we'll find a recursion that defines
the rank of any set, even when there are elements.
And then we can define the class V alpha of a is this the the sets whose rank is less than alpha.
But in general, that won't be a set if a is a proper class.
Now, one of the main things to observe about my gosh, this scrolling is really driving me nuts.
OK, I guess I just can't scroll that way.
Oh, gosh, I apologize.
I'm never going to do this again.
I should always have the beamer slide.
OK, good God.
Oh, it's terrible.
Maybe I have to zoom out to prevent this.
OK, so OK, now where are we?
OK, so in ZFC in ZF, transitive sets are rigid and the whole universe is definitively rigid.
You can't have two transitive classes being isomorphic unless they're identical.
Because you can just prove by epsilon induction that the isomorphism is the identity map.
That's completely fine in ZF context.
But in ZFU, that is breaking down completely because any equineumorosity of elements
lifts to an isomorphism of the corresponding transitive classes that are built over them.
Because you can just define recursively, it's sort of like the Mstowski collapse.
I mean, when you define pi of y is the set of pi of x so that x is in y,
it's already defined on the atoms and then this is extending it to the whole class
and it's going to be an isomorphism of those two universes.
And so in particular, when you have at least two atoms,
then there will be non-trivial permutations of the atoms
and those permutations all lift to definable automorphisms of the universe.
And so that makes a very strong anti-rigidity phenomenon in Erlman's set theory,
which has a lot of consequences that you have to pay attention to.
So let's look at some of those consequences.
Okay, so one such consequence is this collection issue, for example.
Suppose that, suppose we have infinitely many atoms
and let w be the class of finitely supported sets.
So these are sets that have only finitely many atoms in their transitive class.
So that's a super transitive class and it's closed under pairing union power set and so on.
And so you can easily see that most of the axioms are going to hold in w
and you get replacement for the following reason.
If you have a set in w, so it has finite support,
and every element of that set has a witness with a certain property,
a unique witness with a certain property,
then that witness has to come with the same support
because if it was using some other atom,
then there would be many automorphic copies of that witness
and so it would violate uniqueness.
And so because of the uniqueness of the witness,
the replacement set must come from the same support
and therefore the replacing set will also have finite support and so it will be in w.
So it's a kind of lesson that when you have a lot of automorphisms of the universe,
then replacement becomes very easy because it's too hard to have a unique witness
because there's all these automorphic copies of the thing.
And so it's a hint that replacement is maybe not really what you want
when you're in a highly non-rigid universe.
So but meanwhile, this class w violates collection because for every n,
there is a set of atoms of size n,
but you cannot collect those because the collecting set would have infinitely many atoms in it,
but w is the class of finitely supported sets and so only the finite support sets are allowed.
So we don't have collection, but we do have replacement.
OK, so I view this situation as extremely similar to the analogous failures of collection
that have been described in other work that I did with Victoria and Tom Johnstone
and also goes back to Zarach who showed that collection is not following from replacement
when you drop power set.
So in the theories, EFC minus, a very similar thing happens.
And the main list, yes.
I have a quick question about this phenomenon that you just described.
It seems to be crucial there that basically the atoms don't carry any structure, right?
Like if you add a variation of this where you basically build the universe
on top of a collection of atoms that have some structure
that are, for example, linearly ordered or something like that.
Yes, exactly right.
So if we introduced a well order to the atoms and we insisted that the automorphisms respect
that well order, then it would become rigid again and it wouldn't work.
That's exactly right.
So if we were in a language where the atoms were well ordered, then we would have the set
of finitely supported sets wouldn't satisfy replacement anymore
because it would break that argument.
So the way I think about it is, OK, the lesson of the ZFC minus case is
that ZFC minus when axiomatized with replacement instead of collection is the wrong theory
because everything goes wrong in that theory.
Even though you have the axiom of choice, you can't prove omega one is regular
and there's a whole list of things that go wrong here, ever expanding list.
And it's a kind of strange situation because when you look in the literature,
people do axiomatize ZFC minus using replacement.
They say, oh, yes, ZFC minus by ZFC minus, I mean, and they list the axioms
and they list replacement and not collection.
But then they go on and they use consequences of the collection version of it
that are not consequences of the replacement version of it.
And so basically, I think it's just a mistake that they made.
They didn't have the right theory because they really wanted the theory
in which those other things were following.
And so in almost every case that I know of, I think in every case that I know of the errors
that are appearing in the literature because of those axiomatizations are completely corrected
if you just change collection because they achieve collection in the models
that they're looking at and they're using the consequences of the collection version.
And so the right axiomatization of ZFC minus is using collection and not replacement.
And my view is very similar in the ZFU case.
So I want to count collection as one of the fundamental axioms of ZFU
because if you don't include it, then a lot of things go wrong in a very similar way.
OK, but not everyone agrees with that.
And you can definitely find axiomatizations of ZFU in the literature
that use replacement and not collection.
OK, so you have to watch out, I guess.
So there's other things that go wrong, but let me just press on here.
OK, oh my gosh.
When I get to the bottom of the page, it wants the next page.
OK, here we go.
So there's another thing that goes wrong and that is in ZFCU,
you cannot prove the omega one dependent choice scheme.
In other words, there can be a class relation with the following property.
Every, well, let me just give the example, I guess.
If you use the countably supported sets instead of the finitely supported sets.
So suppose you have uncountably many atoms and W bar is the class of countably supported sets.
Sets whose transit of closure has only countably many atoms.
Then you can prove collection in that model because if you have a set with countable support
and everybody in that set has a witness with a certain property, maybe many witnesses,
then in fact, you can fix any countably many more countably infinitely many more atoms.
And you can always find a witness in that new set because they're all automorphic to each other.
And so whatever witness is to be found can be moved so that its support is contained in the original set of atoms,
plus those countably many new ones.
And and therefore you can find the collecting set with countable support.
OK, but this model is quite strange because if you think about the the class of all elements is a proper class
because there were uncountably many and so that's not countably supported.
But every set of atoms is countable.
OK, and that's a strange combination because you have this proper class, but every subset of it is countable.
So in particular, and you have choice.
So you can start enumerating if you try to build an omega one sequence,
the collection of alpha sequences of ordinals is a proper class tree and it has no terminal nodes.
And it's countably closed because any countable sequence you can take the union and that's going to be just a longer countable ordinal sequence of elements.
So it's a countably closed proper class tree with no terminal nodes, but there's no branch.
There's no cofinal branch through that tree.
So that's a failure of what of what is called the omega one dependent choice scheme.
It's different from omega one DC on sets because this is a proper class.
And because we have choice, we do have the set version of DC.
What we're lacking is this class choice class version of dependent choice.
OK, so it's another way in which the element set there is a little bit strange and that things that you think should follow don't.
So you have to be careful.
OK, so let me now.
Explain the connection between Earlman set theory and CFC, the how to interpret the one in the other. OK, so I want to interpret Earlman set theory inside ZF.
So, so let's suppose that we have a set theory universe of say ZFC and I have any class a maybe a is the class of all ordinals, for example.
OK, and now I'm going to define a certain model that I call V bracket a or that we call V bracket a.
And this is going to be an element model in which there will be a many elements.
OK, and so the first thing we do is we make a copy of a.
So every every element of a has a copy, which is a pair starting with zero. OK, this is going to be these are going to be the atoms of V bracket a.
So we put all those into the bracket a now we close under the following operation.
If I have a subset of V bracket a, then I make this thing, which is a pair whose first element is one, and that's going to represent that subset as an element in V bracket a.
OK, so that I've put in this object into V bracket a and it's representing the set whose elements are the actual elements of why.
OK, so I define this membership relation X as an element is a bar element of Y bar.
Just in case it's one of the actual elements of the set Y appearing in the second coordinate.
So everything in V bracket a is a pair. It either if it starts with a zero, it's an atom, and if it starts with a one, it's a set.
And I can tell the membership relation on those guys in the way that I just define.
And it's easy to see it's set like and well founded in V.
So I've defined this class structure, the V bracket a structure with the epsilon bar relation.
And I claim it's a model of ZFCU with a many elements.
OK, so first of all, we get extensionality for the sets because two sets are going to have the same epsilon bar members just in case the set that they came from is the same.
And so we're going to get extensionality and foundation holds because epsilon bar is well founded and oh my gosh, what happened?
Let's see. Right. OK, here we are.
So and and it's easy to see that you get union pairing power set and so on.
It's something like forcing. It's it's a lot like forcing because enforcing when you want to show that that those axioms are forced,
you take a name for a set and then you build the name for the union set or the power set and so on.
You just build the name by hand.
And in fact, we can do the same thing here.
If I have two elements of V bracket a, then then this is the object in V bracket a that will represent the set that has those two objects as as elements.
OK, because the the epsilon bar elements of this set are precisely you and me and only those.
OK, so now we get we can see how V is copied into the pure sets of V bracket a because just let you check be this set whose members are the checks of the elements of you.
This is just like enforcing.
This is the pure set version of you as copied into V bracket a.
And and so it's easy to see that the original universe is just isomorphic to the the class of pure set of these checknames, which are exactly the pure sets of V bracket a.
And now we can expand the language a little bit by introducing what we call the Earlman enumeration predicate a vector.
It's not a it's not a unary predicate, but a binary predicate.
And it associates the the copy of little a in V bracket a with the element that is indexed by a.
So a bar was the zero comma a and a check was the pure set that's representing a inside V bracket a and a vector is that enumeration.
So it's a function that is mapping the class a or the copy of the class a inside V bracket a to the elements.
It's enumerating your elements by that class of pure sets.
OK, and now that my gosh.
OK, can I have a can I have a question about the construction of V bracket a.
So you said that the or elements of the V bracket a are the ordered pairs right of zero and a ordered pairs in V bracket a.
OK, yeah, so because I had a question about how they can be or elements if ordered pairs are simply, you know, defined by a different building.
OK, building the universe of elements.
So we've said, well, we need to distinguish what will become the sets of V bracket a from the elements of V bracket a.
And so we just take these ordered pairs.
If it starts with a zero, then that's what we're going to say is an early element and it won't have any bar epsilon elements.
So the bracket a doesn't see that they are some ordered pairs.
It only sees the elements.
OK, exactly.
They have internal structure in V, but that's not visible in V bracket a using the epsilon bar relation.
Thank you.
OK, so the main observation to make is that the original center of the universe is by interpretable with this V bracket a structure.
And so I define this structure.
It's a sub it's a certain class structure definable in V.
So that's giving me an interpretation.
It's a it's a by interpretation because V can see how it is copied into the pure sets of V bracket a by the check map that associates every object with its check.
So V can see its copy in there and V bracket a using the a enumeration, their element enumeration predicate can see how it arises from its pure sets.
It can see the isomorphism.
It can see its copy inside the pure sets as implemented by the V bracket a interpretation.
So that's a by interpretation.
And then one can observe a little more, namely every set in V bracket a is equinumerous with a pure set because why bar is going to be bijective with why check, which is a pure set because V bracket a can build that isomorphism can build that bijection.
Okay, so I want to just dwell on this issue a little bit. Oh my gosh, I really have a trouble with that. I don't know what it is.
Okay, so when
Good God, what is going on. Okay, so when every set is equinumerous with a pure set.
Then this is a situation that is philosophically interesting in my opinion because
If every set is equinumerous with a pure set, then every structure, every mathematical structure is isomorphic to a structure in the pure set realm, because you can just carry the structure over to the pure set to which it's bijective.
And so that situation is exactly when philosophically you don't need any elements. I mean, from a foundational point of view, if the reason you introduce the elements was to do was to have a better foundation of mathematics then if you're a structuralist.
That situation is exactly when you don't need the elements because you could have just done it in the pure sets. And I think historically this is has a lot to do with why the standard theory became irrelevant free because people realize
that we don't need the elements to represent any isomorphism type of a mathematical structure that we want to talk about. And the theory is just much cleaner and better without the elements and and it's foundationally from a structuralist point of view, just as good.
And that's why we use CFC today instead of CFC you, I think. Okay, but one man's modus ponens is another's modus Tolens right and so the idea is, if you think that elements are important foundationally to represent mathematical objects and structure then you should probably think that there should be
sets, you know, involving the elements that are not equinumerous with any pure set, because that's why you introduce the elements, right. Okay, but now the puzzle is right.
So these weird primitive mathematical objects that lead to these sets that can't be put into bijection with any pure set I mean the traditional candidates for elements were numbers, first of all, and maybe there was resistance to thinking of numbers as sets, and so people wanted to say
the natural numbers I'm going to take them as early elements and build the set theory on top of that or maybe geometric points was another natural candidate of things that weren't fundamentally sets and so we treat them as somehow irreducible primitive mathematical objects of some kind and then take sets of points and so on.
And in both of those cases, they are equinumerous with pure sets, because we have the finite ordinals and we have the, you know, real numbers as implemented in CFC and and coordinates and so on and so we can seem to handle both of those kinds of primitive objects without needing the early elements and so.
So if you want to do or elements set theory and you view it as indispensable for the foundations, then I would want to hear more about what are these kinds of elements that lead to these sets that can't be put into bijection with a pure set.
So if you want to control just to bring this up again it seems again like it would be very natural from that perspective to have where elements that do carry a structure because like if what would be the point of adding an amorphous blob of items.
So if you think of things like natural numbers or whatever things that that would be hard from some perspective to to reflect by sets. So it seems like the, it would be the natural thing to do to have some collection of atoms with some complicated structure where you think this would be hard to replicate
right. Right. Exactly. But that's what I'm saying what what is this class that you're talking about I mean that this traditional or element candidates aren't like that.
Is what I'm trying to.
I understand that but also just another point like you constructed this V double brackets a inside a model of set of C so it's not all that surprising that in a way that everything.
It's a totally different construction to handle that other kind of of her element situation. Yeah, it wouldn't be interpreted like this. Yeah, exactly.
If I understand correctly that the interpretation or the by interpretation depends crucially on having this correspondence between the little a check and the little a bar that you're essentially saying.
Yeah, are enumerated in terms of pure sets a priori and then from that you get the everything else. I'm going to address that because there's a theorem coming up here that exactly is about that point and that is.
Okay, so first of all, let me just sort of reiterate.
All of these theories are by interpretable because of this construction if you specify a specific number of atoms to have any specific enumeration, then you're going to get by interpretability with with ZFC.
Okay, that's what the construction provides.
In particular, it follows that ZFC you is not a tight theory so a tight theory ZFC is tight, which means that no to extensions of it, if if they're by interpretable then they're identical. So that's definitely not true for, oh my gosh.
That's definitely not true for ZFC you, because here are the by specifying different numbers of early elements we have by interpretable theories that are not identical.
But if we don't specify exactly how many early elements there are and this gets to Andreas's point.
Then, for example, in corollary six here. No, let's see what is it. No, I want to go a little further.
This one, theorem seven, if I only say ZFC you plus say infinitely many atoms or ZFC you plus a proper class of atoms, then this is not by interpretable with ZFC.
And so this is exactly addressing Andreas this point showing that in fact it's necessary if you want to have a by interpretation, you have to, you have to provide the enumeration with a specific class, because if you don't if you just have a theory like this then it won't be by interpretable
The reason is that the early element universe is has too many automorphisms and that's not possible to have a by interpretation of such a universe with a universe that's definitively rigid like ZFC.
So, so basically the proof says if you had a by interpretation like this, then go to a model that had a proper class then there's going to be automorphisms fixing any desired said and moving and moving things outside of that support of that set.
I'm going to prevent you from actually having a by interpretation with ZFC because there won't be any such automorphism of the of the ZFC model that you get.
Okay, so let's press on and talk about reflection a little bit I realized that everything is taking much longer than I thought, because of my problems with scrolling yeah okay.
So reflection theorem, I assume most of you know the levy monarchy reflection theorem in ZFC says if you have a formula in set theory, then there's some ordinal lambda so that it's absolute between the lambda and V, and in fact you can get a proper class club of such
lambda for which all the sigma and formulas are absolute in that way.
In the early limit context, then we shouldn't generally expect to reflect truths from V of a to the lambda of a because the lambda of a would include every set of atoms, those have very low rank, but they might be bigger than lambda if there's a proper class of
than V lambda of a will have sets that are bigger than lambda and so the fact that every set is bijective with an ordinal won't be true here ever.
If a is a proper class so we don't expect to reflect from the V of a to V lambda of a, rather what we should do is reflect from V of a to some transitive set.
One statement in the early limit context says that for every set P and every formula that there's a transitive set V containing that P, so that he is absolute between that transitive set and V of a so we're reflecting to a transitive set rather than specifically to a rank
initial segment.
If there's just a set of elements, then you can just prove the reflection theorem in the normal way.
And even that's true if there are say or many atoms you can lead them in a little bit at a time and get this sort of tower of this continuous tower of the universe and that's enough to prove the usual proof of the reflection theorem.
What we should do is discuss reflection theorem, just in the general context of ZFC you where we don't always have this tower, this continuous tower we can't build such a continuous tower, there might be too many elements.
Okay, so first is, if you have, if every set of elements is well orderable, I'm not assuming AC I'm just assuming the sets of elements are well orderable.
You can set a set you so that every set of elements that's disjoint from you can be duplicated to another disjoint set of elements disjoint from both of them.
And that's not too difficult to see you just pick a you so that the sets of elements in the complement have as small a cardinality as possible.
And then you're always going to be able to duplicate such a set because if you couldn't then you could make a better you is the basic idea.
This is, you have you for anything disjoint you can find another one disjoint of the same size.
That's duplication. Now, duplication implies homogeneity, namely,
any two disjoint sets, any two sets of elements that are disjoint from you and the same size then there's an automorphism of the entire universe that swaps them.
And, and that's just because well if they were disjoint then we can make a permutation that swaps them and extend to the universe.
If they overlap we just find another copy that's disjoint from both of them, and then first go to the other copy and then to the other one so even if they overlap it's no problem.
Okay, so now using using this homogeneity property we can prove the reflection theorem, the reflection principle, the first order reflection principle.
So first we can prove the Omega DC scheme. So if you have a class relation and everybody's related to somebody.
Then we want to find an Omega threading of it. And the way you do that is you you build a sequence of Cardinals.
So the first one is big enough so that for some class of atoms it has some elements in the domain of the relation.
And then you look at everybody in that class and you, you look at the wise that they are related to, and you look at the sizes of the supports of those wise, and the ranks of the sets of those wise and then you get some lambda n plus one that's bigger than all those.
And then because of homogeneity, a fixed set of elements of that size is always going to have a copy of any of the witnesses why.
And so we can, we can specify lambda n plus one as a cardinal that's big enough so that you can find wise in a fixed set of elements of that size of rank less than lambda n plus one.
So if you iterate that Omega many times and take the Supreme, then you can find your transitive set V, which is going to be closed under our witnesses.
So every X in V is going to have a Y in V, so that X are why.
But now using the axiom of choice, we can well order this, this set V. And then we can find the Omega threading by just picking the least one.
So now, once we have Omega DC, we can prove the reflection theorem in the usual way, because you're just closing under skull and witnesses which is our relation.
So, so what you really want to do is find a transitive set which is closed under the skull and witnesses for the X's that are in that transitive set.
And then that is enough to make fee absolute between V of a and that transitive set V.
Okay.
Can I ask a question about the proof of Omega DC. So, as you build as you're iterating this process you're building these larger lambdos.
Yes.
And at each stage you're also enlarging the set of elements.
No, no, because they're all automatic. It doesn't matter which one.
Okay, that's, that's the key. Okay.
And so, they all have the same ordinals for the ranks and everything and that's important in the proof.
Thanks.
Okay, so that the surprising thing about this proof is that we use the axiom of choice to prove reflection which is somehow strange because that's not at all how you do it in CF.
And you can just prove the levy monarchy reflection theorem with no choice needed but here we're using the extra choice in order to prove reflection and so does reflection first order reflection hold in CF you and we don't know the answer to that.
But we can, we can prove a number of cases of it but let me, I want to just skip ahead.
Now, and let's get to the class theories.
So, just to review quickly. So, Gertau Bernays set theory and Kelly more set theory are two sorted theories we have the first order part of sets.
And the second order part of all classes and the difference between GBC and KM is that in, in GBC, okay, the sets form a model of CFC, including allowing class parameters in the collection and separation schemes.
We only have class comprehension for first order definition so a fee has only first order quantifiers, then in Gertau Bernays where we're allowed to form this class.
But in Kelly Morris we allow second order quantifiers in this fee so second order class comprehension.
And both of them also have the global choice principle which is equivalent to saying either that there's a global choice function.
There's a global well ordering of the universe, or that there's a global well ordering of the university and order type or and those are all three equivalent in the pure set context, but that's no longer going to be true in the early element context.
So we're going to use the, in the early element context we're going to use the, the well order the class well order principle every class has a well order.
But it's not necessarily order type of word.
Okay, so, right.
So let's also talk about this interpretation of Kelly more set theory in a first order theory. So it's due to Merrick also Cameron who's here gave a very nice account of this in his dissertation.
So we need the class choice principle, which is the principle that says if you have a, if you have a class and for every element of that class there's a class with a property then you can find a two dimensional thing whose sections have the desired property.
If you combine them with the class choice principle then this is by interpretable with ZFC minus with the largest cardinal which is inaccessible so that this is what's called the unrolling construction.
The unrolling construction is, you take the classes in the original Kelly Morse model, and you, you think of them as, as representing a set in this ZFC minus model which is going to be taller than the ordinals so.
So the typical situation. Okay, let's see. Right.
I first do the easy direction. If we have a model of ZFC minus with the largest cardinal capital which is inaccessible then we can just take the V Kappa of that model together with all of its subsets.
And that is going to be a model of Kelly Morris with the class choice principle. So the hard part is the other direction, really.
So, so we have a model of km plus CC, then we define what's called the unrolling a membership code is an extension all an extension of class directed graph relation with a unique maximal node.
Basically, it's something like the transitive closure of a set with the epsilon relation it's like this except that it's a proper class objects so we can for example make such membership codes that are corresponding to the ordinals or the ordinals plus one or the ordinal squared, or much bigger
that would exist beyond or if we had continued the cumulative hierarchy beyond or okay so using the classes, we take these membership codes and we want to say that two of them are equivalent.
It's isomorphic as directed graphs, and we want to say that one of them is an element of the other one is representing an element of the other one if it's isomorphic to the restriction of the larger one to one of the predecessors of the maximal node.
So the maximal node is like the node whose elements are representing the elements of that set and then their elements and so on all the way down.
It's well-founded. That's a proper class. It's well founded. And so the elements of the of the set that's coded by e are precisely represented by these wise which are predecessors of the maximal node.
So one can prove using KM, you can prove that the, the, the set W of equivalence classes by these equivalence relations gives you a model of CFC minus.
So that's the instruction and part of what I like that Cameron had done in his dissertation is to find exactly what the second order principles we need weaker than KM going down to GB and ETR is a big part of that.
Okay, I guess some of that also appears in in earlier work.
So, okay, so let's now do the same thing but with her elements. So we want to have GBC you and KM you.
So we're going to have the first order part and the membership relation and the element predicate, and the collection of classes. Okay, so we're going to have a model of CFC you here, and then the classes for GBC you we're going to allow first order
comprehension and for KM you we're going to have a second order comprehension.
Oh gosh, let's see what is going on here.
Let's see.
So, okay, we did that already. And now,
So, oh, it's just I click on it and it goes ahead. Okay. So, right, every model of GBC is by interpretable with the corresponding model of GBC you using exactly the analogous construction to the bracket construction that I described earlier for the CFC case so you use the ordered
pairs again and you take all the classes are just all the old classes that happen to be subset subsets of the class of these pairs.
And that exactly works it gives you a by interpretation of GBC with GBC you plus a many elements and again it's this Andreas point that we're using explicitly the element enumeration predicate here not just saying that there's a lot of
elements, but specifically tying them to an enumeration with the pure sets, and that gives you a by interpretation.
And then also it works with KM and KM plus CC, we get it in the element context as well.
Okay, so that shows you that these theories are all by interpretable. We need parameters to give us the class parameters to give us the element enumeration.
So you can have GBC plus or many atoms or V many atoms or whatever and KM plus CC all of these collections are by interpretable.
So, now I want to come to this axiom the abundant or element axiom, which.
Let me just prove the following. So suppose we have a model of GBC you and kappa is an inaccessible cardinal, then we can cut down to the hereditary less than kappa sets.
Okay, these are the sets of hereditary size lesson kappa, but using all the elements.
Okay.
So, but we only keep as sets the small ones but the classes will have all of the subclasses of this model.
And the claim is that that this is also a model of KM you, if, if the original one is modeling KM you, then the hereditary class also models KM you.
And that's pretty easy to see because the classes, anything that's second order definable here is going to be second order definable in the original one and so you're going to have the class existing there and so it will exist here because we're
taking all the classes here that existed there that are subclasses of this new domain.
And what this is on is the nature of this hereditary model. So kappa was an inaccessible cardinal.
And the ordinals are of course are exactly kappa, but the class of elements is now much bigger because it used to be a proper class in the original model or or many even, which is much, much bigger than kappa.
So we have ordered many elements to start with. Now we've got this KM model, where the number of elements is extremely big in comparison with the number of ordinals, they're no longer equineumerates.
But yet every class is well orderable so we have global choice in that sense, but not every class can be put into order type or because it's just much bigger than or because the order of this model is kappa, which is tiny in comparison
to the order type of the atoms which was the ordinals of the original universe V.
Okay, so, so let's observe something about this model. So we kappa was inaccessible we had ordered many elements in the original V but we cut down to the hereditary sets.
Let's say a class is small. If it's not equineumers with all the elements.
It's equineumers with a set in V. And so in particular its power set is also small, you know, and, and so if I have a class a small class of elements.
Then I can, if, let's see, if I is, let's see.
If V is a small class of elements, then I can find another small class which is corresponding to the power class of it, and I can have a set D that lists all possible subsets of B subclasses of B as the sections.
And that will also be small.
So the small classes of our elements have their power classes also small.
And similarly, we're going to get a kind of regularity property, namely, if I have a small class, let's see, if I have a small class of, if I is small, and I have a class on the product space so that every section is small, then D itself is small,
and that's corresponding to the, to the fact that ordered is definitively regular in the original universe.
And so it's something like the, the cardinality of the class of elements is something like an inaccessible cardinal is how we're thinking about it.
The abundant atom axiom is the assertion that the class of atoms has those properties, namely it's strictly bigger than or and every small class of our elements has a small power class and every small index class of small classes is small.
So it's saying that the universe is very wide with respect to atoms in comparison with its tight height.
So if you have GBC you with or many or atoms, then when you do that hereditary cut off, you get the abundant atom axiom. That's a kind of typical way that the abundant atom axiom might hold.
And then we get this by interpretation.
The KM plus the abundant atom axiom.
Then when you do the unrolling construction you're going to get to inaccessible cardinals this Kappa comes from the ordinals of the KM model and the lambda comes from the, the order type of the elements of the abundant atom axiom model.
And then there's like this. If you start down here with the KM you plus CC plus abundant atoms axiom, and you do the unrolling, you're going to end up with w which is going to be as DFC minus model with two inaccessible cardinals, where Kappa is the original height of the ordinals, and
the order type of the atoms in the well ordering that was provided the minimal well ordering.
And then, of course, such a model w is by interpretable with its or element coding by W bracket a. So we can have lambda many atoms here. And also we can cut down to the to the KM model, which is just chopping off at lambda now instead of
Kappa and that's going to be a KM plus CC model in which Kappa is still super compact. And this one is by interpretable with its or element one. So all five of these models, not the V but all the other ones are going to be by interpretable with each other.
That's the basic picture of the by interpretation that we're going to use later on.
Okay, so now what time is it here.
Oh my gosh. Okay, so not so much. So now I want to move on to second order reflection.
So,
so second order reflection is the principle saying in the in the context of GBC or KM, or also we're going to do it with her elements.
So the second order formula that's true in the second order model of set theory, then there should be some V lambda where it's true at that level, interpreting the second order quantifiers is ranging over the subsets of the lambda.
So this is different from second order reflection that appears in some of Vika's work, also with Cy Friedman, where they say second order reflection is is reflecting from the whole universe to a coded class of classes in the universe.
It's a different principle. We're reflecting to a transitive set to a V lambda here.
And so one has to pay attention there's different kinds of reflection.
So that the second order reflection principle has large cardinal strength in, in the KM context. So, for example, if you, if you just reflect GBC itself which is a finitely axiomatizable statement that's true in a KM model, say, if you reflect that,
then you're going to get a V Kappa which thinks GBC is true, but when equipped with all of its subsets but that means that the second order quantifiers are interpreted correctly and so it would be have to be a model of, of second order ZFC, which by Zermelo's
means that Kappa would have to be inaccessible. So we're going to already get lots of inaccessibles coming from second order reflection just by reflecting GBC itself.
And in fact, we get more than that.
You can get a stationary proper class of pi one n indescribable cardinals, just by pushing a little harder on that idea.
And for an upper bound, if Kappa is a measurable cardinal, then you can produce V Kappa will be a model of KM plus CC plus the second order reflection principle. So let me just explain that a little bit.
So if it is measurable and fee of X holds in V Kappa, so a second order fee, then just apply the measurability embedding with critical point Kappa.
And so fee of J of X holds in the J Kappa rank initial segment of M by elementarity, but, but V Kappa and V Kappa plus one, both exist in M.
And furthermore, J is elementary from V Kappa to V to V J Kappa of M, which is M J Kappa. And so M can see that fee J Kappa reflects down to this structure which exists in them.
And so, excuse me.
So for M thinks that fee reflected down to an inaccessible cardinal below and so therefore by elementarity V must think that fee reflects from V Kappa to something smaller.
So that shows that reflection holds, if Kappa is measurable and you can do better. So Omega Airdoche is enough, which brings it into L. Okay.
I don't have curiosity that I noticed.
If you have a weak second order set here so this is GB without global choice but just with the set version of the axiom of choice so every second be well ordered.
Then if you add second order reflection to that you get full KM plus CC so second order reflection erases the distinction between Girdle Bernays and Kelly Morris they become equivalent.
And you don't even need Girdle Bernays with global choice you only need it with set choice.
Right. Okay, so now I want to consider this following new large cardinal notion. Kappa is second order reflective.
Every second order sentence that's true in some structure containing Kappa in the domain in a language of size less than Kappa is also true in some first order elementary substructure of size less than Kappa whose intersection with Kappa is an ordinal.
So, any big structure has a tiny elementary substructure to which the second order statement reflects.
So every lambda reflective if it's true when that original m has size lambda.
And we can say it's reflective for pi 11 or pi one and assertions if we get the reflection for that level of complexity.
And now the interesting connection is that this notion interleaves with super compactness.
And the reason why this super compact cardinal is second order lambda reflective. And the reason is that if campus lambda super compact and you have a structure M of size lambda, then you can just take the super compactness embedding.
And the basic fact is that J image M is first of all isomorphic to M, and therefore will satisfy fee.
And so is an elementary substructure of J of M.
And so from the point of view of the image of the super compactness map.
And thinks J of M has this elementary substructure J image M to which fee reflects.
And so therefore, if Kappa is lambda super compact, then you're going to get the lambda reflection property.
And then the super interesting part is that we get the other direction, actually, if we have to the lambda lesson Kappa reflection, just even for pi 11 is enough.
Then Kappa must be lambda super compact this theorem is related to the Magador characterization of super compact instead of a refinement of Magador characterization of super compact is how I think about it.
So if you have a reflective cardinal at the level of to the lambda lesson Kappa, then look at age lambda plus let me just assume lambda equals lambda lesson Kappa.
So this structure has all has pick up a lambda in it and it has all the subsets of pick up a lambda in it, and it has all the functions from pick up a lambda to pick up a lambda or to lambda.
The question of whether there's a normal fine measure on pick up a lambda is a second order assertion about this structure because you're saying, can I pick out the measure one sets.
So that it's normal fine it's normal and fine but being normal and fine in this structure, our first order assertions about the predicate, because you already have all the subsets and all the functions all you have to do is say,
this property that for every alpha, the set of Sigma that contain alpha is measure one but that's just a first order assertion over that structure about what the measure one sets are.
Okay, and so now, so therefore if Kappa isn't lambda super compact, then that second order property would reflect.
So let's take a first order elementary substructure of size lesson Kappa whose intersection with Kappa is an L is an ordinal.
And then this is this tiny thing we can let SB it's intersection with lambda which is an element of pick up a lambda, and we can use that as a seed to generate a filter.
And then we can argue that this filter is a normal fine measure over M, as far as M is concerned. And so that would contradict the reflection of the fact that we were reflecting the absence of lambda super compactness.
Okay, so therefore Kappa must really have been lambda super compact in the first place.
Okay, so what that shows is that being second order reflective is the same thing as being super compact, even when you're restricted just to pi 11 assertions.
So there's this tight connection between super compactness and second order reflection.
Okay, so now.
In fact, we get a level by level version. So being lambda reflective for pi 11 is equivalent to being nearly lambda super compact, using a notion that was introduced by Jason Shanker in his dissertation.
A few years ago so nearly super compact is like a weak compactness version of super compactness where you have a normal fine filter that measures up to lambda many subsets of P Kappa lambda and lambda many functions from P Kappa lambda to lambda.
And, okay.
So now, let's move to second order reflection with her elements.
If you start with a model.
So this is really the main observation that appeared in yows, the my co author yows dissertation and his paper that was just published.
So as he started with a Kappa which was Kappa plus super compact and he built a model of KMU with second order reflection and more than or many elements.
And, and actually he asked me, because he was using this very strong assumption of Kappa plus super compact Cardinal Kappa in order to make that happen.
And I felt like it was out of sync with the level of the strength of second order reflection about this indescribability and so on because the super compact hypothesis was considerably stronger and so he asked me, well, do we really need this.
And I looked at it and I said well it looks like you're really using it in a fundamental way and so that's what led to this work then we're now we're going to reverse it and show that in fact super compactness is needed if you push on it a little harder.
The main observation was that if you have a lambda super compact Cardinal and you make that V bracket lambda. So that's going to have lambda many or elements, and then you cut down to the hereditary structure.
So this is the H cap of a of that model.
Then you're going to get second order reflection holding there.
And the proof of that is basically, this is a model of size lambda.
And because Kappa is lambda super compact we get reflection for free in that situation because J image H Kappa bracket lambda is going to be a model in which is elementary in J of H Kappa of a, and, and so it's exactly the same as the other proof.
So, so let me show you now. So in fact, the first observation now to make after that is that you don't actually need a lambda super compact but being nearly lambda super compact is good enough.
If all you want is pi 11 reflection.
So I see this should say, this should say pi 11 here, I think we're not getting full second order. I'm sorry there's a mistake in the paper here should should be pi 11.
Okay.
So let's now move to the main theorem finally. So what I claim is that the following theories are by interpretable.
If you have Gertau Bernays plus the axiom of choice for sets, plus the abundant atom axiom plus second order reflection, actually this theory is identical to the next theory.
For the reasons that we discussed earlier K mu plus CC plus abundant atoms plus second order reflection.
The second thing is now we're making it by interpretable with the pure set theory KM plus CC plus a super compact cardinal plus second order reflection we could drop the CC and so on because second order reflection is implying CC and so we could put GBC plus super compact plus second order reflection.
And the proof is using the same interpretation that we had before it's the same picture.
So if you have this model with abundant atoms so that's very wide in comparison with this height kappa.
Then if you do the unrolling you get this model w of CFC minus where that the kappa is becomes inaccessible and lambda that the order type of the early elements becomes also inaccessible, but then the second order reflection principle is going to give us that kappa is super compact in this
model up to lambda. And the reason is that structures of size lambda in this model are coming from proper classes here which reflect to transitive sets which are of size less than kappa.
And so we we fulfill the fact that kappa is second order reflective, which is equivalent to super compact.
So, so here we have an actual super compact cardinal up here we have that it's super compact up to lambda and it's lambda reflective.
So, let me just mention also the pie one one variation if you drop from second order reflection to pie one one.
You also get the corresponding picture, we're now at the top instead of having a lambda reflective cardinal you have a nearly lambda super compact cardinal, which is this shanker notion, but the interpretations are the same in this case.
So,
I guess that's it that's all I wanted to say so thank you very much.
Thanks very much Joel.
That was really, really super interesting.
Questions for Joel.
Okay, quick question.
So you look at these don't like structure put on the early elements is when you have this enumeration of them. But I think as Gunter touched on in a question earlier during your talk.
We think of this as coming from ordinary mathematics and so that would have, you know, if you took like the real numbers you'd have the certain structure on that.
So, did you and Bokeh look at what happens in the context where you start with kind of ordinary mathematical structures.
They would be, they would be equinomers with a pure set, because they're really not all into that okay. So, and in this situation when it's your set then you're going to be in this by interpretable situation.
What did this affect some of your results about automorphisms because your automorphism to have to respect the structure on the elements.
Or maybe that was just showing up with when you have.
Always when you don't have the, if you're just looking at it as a class of elements. So if you, once you fix a well ordering or something of the elements, then you don't have the homogeneity anymore because the automorphisms won't respect that.
I haven't looked, we haven't looked specifically at like having the real numbers as elements and thinking about homogeneity there.
I mean you wouldn't want just the order right you would want. Yeah you want whatever the full structure is the ordered field but that's rigid and so you won't have any homogeneity there because it once you put the ordered field structure on the elements then it's a rigid structure I mean the real
the complete ordered field is rigid and so you can't have any automorphisms right of it.
So it would, it would ruin the homogeneity. And so, I don't know if that answers your question or not so all the homogeneity arguments are in the case when you don't have that structure.
Yeah, but I think it does I guess it sounds like there's not much interesting set theoretics going on when you have this extra structure.
Let me make a comment about about one word in Tamron's question.
I think he mentioned you have only this enumeration what if you had something like the structure of the real numbers.
My inclination to view the other way around that enumeration is an awful lot of structure much more than you would get from the real numbers because you can interpret the real numbers in pure sets and now you've got these atoms labeled by pure sets.
That enumeration of the atoms is a very powerful structure. Well as some of the results show it propagates upward gets your isomorphic copies of everything and so on.
And it includes things like the real ordered field and practically any other structure that mathematicians would normally look at.
I think that's right. So, but I mean, just to emphasize the main theorem doesn't have that enumeration in it right so we're in the situation in KMU plus abundant atoms.
The atoms are not even bijective with any pure class right so it's impossible. I mean that's exactly what the abundant Adam axiom.
More than more than or many atoms. And so therefore, you can't have any such bijection when you have the abundant Adam axiom so that the interesting situation is.
I mean, in a sense there's another way of thinking about the abundant Adam axiom.
Do you ever want to entertain the idea that the center of the universe was like that with so many atoms that it was wider than it is tall because the fact that there were abundantly many atoms is just proof that you didn't finish building the set that you didn't
finish building the ordinals you should have gone taller and the by interpretations with the unrolling construction are exactly telling you how you might have continued, and you probably should have.
And so, once you have the, if you have global choice in the sense that every class can be well ordered including the class of all elements, not that we're fixing such an enumeration.
And if you realize that there is such a class.
Then, if that's, if that's not in bijection with the ordinals then you're in a situation where you would probably should have built the ordinals taller right and so why didn't you do that.
But if you are in this extreme case of the abundant Adam axiom, then, because it's by interpretable with KM plus or many atoms, then there's a sense in which.
Maybe you should have built the universe tall enough to cover the atoms and then you would have just or many and so.
There's a situation when we when we do have an order numeration of the atoms.
So, what maybe a motivation to have, you know, more atoms than they are set, so to speak, be something like, maybe you just want to build a model that has, for example, maybe a memorable cardinal or something
like that, but you just throw in the top of any atoms and the predicate for for the ultra filter.
And then you build just a few steps until you reach a model where you have, you know, I'm just thinking about ideas for how you could make a model where the atoms have a structure like maybe they carry some, some normal ultra filter.
You can reach a model where you can talk about this without going all the way up to a memorable cardinal or something like that.
I see that would be interesting. Maybe y'all, if he's still here, can remark on what is the reason to look at the theory of KM you with more than or many elements what, because you were studying that before you ever met me I think and so what was the motivation.
The original motivation was to just to see if second order reflection implies this principle of limitation of size, because we can easily show that second order reflection implies all these second order choice principles including the well ordering principle.
So I got this question, well, because in the war and separate context is second order choice principle they come apart. So I got this question whether second order reflection can also give us this strongest second order choice principle, namely,
limitation of size.
And then I figured out while assuming we have a lot of cardinals the answer is no.
And of course to get a model of that we need for more than four many atoms. So that was the original motivation.
So, so in a sense that's
you wanted to look at it because you wanted to prove that it originally maybe that it didn't happen but then in fact it did and you proved it by this, this nice large cardinal construction I mean, because of course, if I'm understanding you correctly.
You're saying that maybe one would have hoped originally that perhaps second order reflection would imply limitation.
In which case your elements would have to be in bijection with or the fact that it doesn't is what you prove by producing the model which required the large cardinal strength.
I think maybe ultimately one can still look at the theory as a bit strange from a foundation of math point of view because when you have so many atoms then you should have built the cumulative universe taller right I mean, especially if it's well orderable.
It seems like you're committing yourself to these bigger ordinals so why don't you have them as actual ordinals instead of only as the coded versions of the ordinals with the classes of elements.
That sounds very similar to what I guess is the original motivation for axioms like replacement or collection.
The theory has this natural model V sub omega plus omega, but it's got all these well ordered things in it that are way taller than the universe. And, as you said, therefore you should keep going.
Right. Yeah, I think that's right. Yeah, that's a good point.
I mean, of course, no model of safety is ever going to be closed under that completely because once you have ordered whatever it is then you can define an or a well ordering of type ord plus one or ord plus ord ord squared and so on and so you can ever be closed.
So the well known principle. Yeah, once you have ord. Therefore, you don't have ord.
I guess.
I'm upset to you it's not a class steers. So that's.
I mean the principle I guess that you were just referring to though is something like every small class, every small well ordered class should be instantiated with a set.
And that would imply the negation of the abundant or element axiom right so if you, if you had some class of atoms that was well orderable and wasn't objective with the whole class of atoms then you should have it as a set.
So you couldn't ever have more than ord many elements in that case.
I had a question.
If you had, if you started with I don't know seven or elements in the universe or maybe more, and then you forced to another universe as it changes the.
So I understand how to. So if we have only seven, then we can buy interpret that with the piercets and we can force over the piercet model like normal forcing and then reinterpret again afterwards.
If you had a set number of elements that would always work but in general forcing over ZF you, I haven't quite thought about it enough yet and actually yeah I was working on this now I think forcing over set theory with elements I think it's an interesting topic.
At least when the model is by interpretable with its piercets, then basically there's nothing to say because it's just going to amount to forcing over the piercets which we already understand quite well, and then reinterpreting in the extension.
So everyone wants maybe to look at forcing over models of elements that theory with way more than seven elements you want some weird set of elements that isn't objective with any piercet.
So in particular, not well orderable in order type less than ordered and.
And, and in that case I don't understand how forcing works very well yet.
I just want to make sure like when you said, like if you had a if you had a universe with two elements and one with seven are those did you say that those are isomorphic or.
But they're by interpretable.
Because they're both by interpretable with the piercets because when you have the piercets you can pretend that you added these are elements as many as you want any set number of them.
The V bracket a construction gives you. So if you had a set number you can do it you can undo that and go back to the piercets, and then add any other number that you want and all of those are in a sense the same.
They're not isomorphic models that they don't have the same theory, because they think there's different numbers of elements.
And since their equivalent theory semantically equivalent because they're by interpretable which is, you know, one of the tightest ways of saying that two models are fundamentally the same is to say that they're by interpretable.
Okay, thank you.
Yeah, forcing over models with lots of elements.
I would get all surprised if this is in the work of for Pankha and his school, or Pankha Hayek book on semi sets. The problem is digging it out of there.
And a long time since I looked at that book.
I can press I wrote a review of it.
That was a prehistoric times.
I think that every single idea in set theory, including those that haven't been published elsewhere is already included in that book.
That's very new.
But it's well hidden.
So, Joe, the letters in the book a little bit to get.
Just.
I really like this version of second order reflection that you have. And my question is, are there any interesting questions about the structure of KM with this principle that you know about reflection.
Yeah, is everything understood or is there something there to work on because the equity consistency isn't exact. There's these bounds, I guess, so maybe one would want to say something more precise about it.
I mean, there's Omega Erosion there's indescribables coming below and my understanding of it is that we don't have an exact large cardinal equivalence there.
So that's one thing, I guess.
Yeah, I'm not sure maybe y'all know something else.
Yeah, that's KM and second order reflection.
Yeah, that's also all I know, I guess. Yeah, but with KMU plus second order reflection. Yeah.
So the question of the exact strengths of KMU plus RP to plus more than or many atoms is still open.
So we know that it's bi interpreter with CFC miners, plus a Kappa plus nearly super compact, but we don't know the strengths of that theory. Yeah.
In, in Jason's dissertation he proved that if Kappa is Kappa plus nearly super compact, then ADL of our holds. So it's, it's definitely strong. If you're in a CFC context, but the bi interpretation that we're getting with KMU plus plus second order reflection
is with that CFC minus plus the largest cardinal Kappa is nearly Kappa plus super compact.
And I just, I don't know how strong that is and I was talking with gunter about it I was hoping that gunter would just tell me oh yeah it is still implies ADL of our but
certainly that's maybe not quite as easy, but maybe there's still hope to get strength out of that. I mean, it seems very likely that it should be strong. Maybe we can just prove at least it's implying the consistency of a measurable or something.
Any more questions.
So thanks very much again great talk and a great discussion afterwards and good to have you back at least.
Thank you very much and I apologize again for my completely lame slide presentation and I think I'm never going to do that again.
I mean slides are better sorry Joe.
Do my taxes.
I thought it was kind of like it was kind of like sitting down at a table and going over the pre print.
Okay, even though the flipping your head.
Table with a pre print the strong winds maybe.
I'm sorry what.
Table with a pre print the strong winds just blowing the pages everywhere.
Yeah, that's what happened. It's kind of gusty in here.
Bye.
