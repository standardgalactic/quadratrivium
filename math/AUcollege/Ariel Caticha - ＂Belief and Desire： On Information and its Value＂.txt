Let me first start by thanking Amos for going through the trouble of organizing these conferences.
What we have seen today already is a very, very good indication of the status of studies
of information at the very beginning of the 21st century.
How many different types of information have we confronted with today?
This reminds me more or less of the status of the concept of energy sometime in the beginning
of the 19th century where people were not even totally convinced that the warmth of living bodies
had anything to do with kinetic energy.
The term potential energy had been invented but not yet the term kinetic energy
and they didn't know that these things were supposed to be conserved.
We are today in a, I would say we're in a similar state of confusion about the definition of many of these terms.
We do not agree.
Confusion is not necessarily valid.
We're probably using the term information to solve different problems here.
But it is good to be aware that we're using these terms with very, very different meanings
for the purpose of solving very, very different problems.
So thanks to Amos for putting us all together so that at least we become aware that this is actually going on.
My subject is belief and desire.
Pretty sexy for a physics talk.
Let me see.
Yes.
Let me start talking about belief.
If there was anything that's supposed to be sexy in the very beginning, it's going to disappear very, very quickly
because I'm talking about beliefs of rational beings.
I'm not concerned with individual beliefs or the subjective beliefs of regular people.
I'm concerned with the beliefs of ideal rational agents.
What we're trying to do here is sort of develop a calculus for what it is we ought to believe
rather than a description of what it is that actual human beings believe.
The analogy that I would like to propose is something along the lines of,
suppose we are given two huge numbers and we are supposed to multiply them.
We follow certain rules that are supposed to guide us as to what we ought to believe,
the product of those two huge numbers.
A priori, I have absolutely no belief what the result should be of this multiplication.
I'm totally confused.
So I follow certain rules.
I have a calculus and by the end I can do accounting and finance.
My goal is not to develop a fuzzy arithmetic that would accurately describe what high school students actually would obtain
when they do the calculations.
That's not it.
Very good.
So the question that I will try to address here concerning belief is what is information.
And as I said, there are many, many different approaches to this.
One approach is that information is epistemic.
Information is whatever is conveyed by an informative answer.
Another that it is sort of like a probabilistic, whatever Shannon came up with.
Another that is this algorithmic, and that's what normally studied in the Cormagore complexity.
And we've heard already quite a few about all of these.
The epistemic type, which is the one I'm interested in,
is the kind of information that we adopt in everyday usage.
We ask a question because we want an answer.
Shannon information is of real value when doing communication theory, when doing physics, when doing econometrics.
Algorithmic complexity is a concept that was essentially developed from the field of computer science
and helps describe complexity.
So these things are very different usages.
One is concerned with meaning.
It matters a whole lot what information that you receive means.
Another one that's not concerned at all with meaning.
It's only an amount of information, a number of bits.
Now, algorithmic complexity is more complex.
It's concerned with amount.
And perhaps also with meaning, it's well-arguable.
It's being developed.
So what is the goal that I am here concerned with?
The goal is to update from old beliefs to new beliefs when new information becomes available.
This is the kind of calculus that I would like to develop.
How you update from old beliefs to new beliefs when information becomes available.
The old beliefs, and this is where I'm dealing with ideal rationalizations,
the beliefs are going to be described by a probability distribution.
The old beliefs are described by Q, the prior.
The new beliefs are described by P, the posterior.
And the question that I want to address, the first part of this talk,
is what on earth is information?
What is that?
Now, the answer I'm looking for by design, this is the kind of answer I want, okay,
is that we seek an epistemic concept of information
defined directly in terms of the effects that this information would have
on the beliefs of rational agents.
This is interesting, it moved.
The ellipse is supposed to be on rational, so rational.
This is crucial, okay? This is absolutely crucial.
Now, I know that in the audience there are lots of people who fool around with economics.
The term rational here is used in a different sense than it is normally used in economics.
This is not rational in the sense of people who are trying to play games
or anything of this sort.
I would say this is rational in the sense of people or agents
who are trying to reason systematically.
Very good.
In trying to nail down this notion of information, let me draw an analogy from physics.
Now, I know what some of you might be thinking out there.
These guys are physicists, everything is going to be in terms of physics, right?
The only tool you have is a hammer, everything looks like a nail.
And the answer is, you got it, okay?
That's it, yes, that's it.
And I have a reason for this.
The reason I have, which is a very heretical one in physics,
is that when we do physics we're not describing ultimate nature and ultimate reality.
When we do physics, what we're doing is we're processing information
to make predictions about reality, to control reality, to explain reality.
So physics is quite conceivably one of the better examples
of how we can deal with information successfully.
So that's why the analogy from physics is important.
It's because an example where we know what we're doing.
And so here goes.
This is something that you probably learned in Physics 101.
It's like the most amazing instance of genius, it's Newton.
If you have an initial state of motion described by its momentum, right,
that's part of the genius understanding what you mean,
how you describe mathematically a state of motion.
And then something happens and you have a final state of motion.
Well, what happened in this red blob there?
Well, what happened there, you have no idea, but you give it a name.
You call it a force.
Force is what force does.
It changes states of motion.
Force is whatever induces a change in motion.
So you define forces as being the rate of change in momentum.
In pulse is actually the change in momentum.
So this is a very nice thing because you might not have known what force was.
It's really interesting to look at accounts in the Middle Ages
of what people thought force could be.
And it's only when you actually come up with a real pragmatic definition
for the concept of force, namely force is what force does.
It changes motion.
If I know how to measure motion, I have an automatic definition of force.
Then you can use that.
I'm going to do exactly the same thing with inference.
And so if you have a description of your old beliefs and something happens
and you get new beliefs, whatever happened there,
I'm going to call that information.
Information is what information does.
It changes your mind.
Or at the very least, the mind of this idealized, ideal, rational agent, okay?
So information is what induces changes in rational beliefs.
The word rational here is important too, right?
If you're not being very rational, you can receive information
and you can care less.
It happens all the time.
Ah, yes.
This little bird here, the red QM, is to remind me that
it is not just that I'm going from physics to inference.
For me, if this is going to be useful at all,
I should be able to go back and say,
if you have a theory of physics which is described in terms of probabilities
and you want to figure out how those probabilities change,
you are going to do it in terms of information about what are the allowed changes.
Quantum mechanics is a theory that's precisely of this type.
So if you have a good idea of what information is and how it is processed,
you can come up with a lot of quantum mechanics, which is pretty cool.
So what is information?
Information is what induces changes in rational beliefs.
It is what constrains beliefs.
If you're rational, then not everything goes.
You're not allowed to think whatever you want.
There are constraints on what you're allowed to think
and if you still want to claim you're being rational.
So information is constraints.
Information is the constraints on probabilities.
The great advantage of doing this is that I started with a kind of wishy-washy notion of beliefs.
You say it's sort of like Tachyphilia and all of that.
But that becomes very structured once you say that beliefs are represented by probabilities.
Same way here.
Information is whatever changes beliefs.
It seems wishy-washy, but the moment you say information is the constraints on probabilities,
then this is something that's amenable to mathematical manipulation
and it becomes very structured and rigid.
So epistemic information does not measure amount of information.
It's not Shannon. It's completely different.
Epistemic information has no measure of amount.
That's not the way we would characterize it.
It's just the constraints. That's all.
Now the question that follows from all of this is how do you select the posterior,
the final probability distribution from among those that actually do satisfy the constraints?
It's a matter of picking one.
And the idea is to rank them.
The strategy is take all of this probability, I have a prior.
I want to pick a posterior from a family that satisfies the constraints.
How do I pick one? Which one do I pick?
So the strategy is you rank these probability distributions according to preference.
And in a second I'll tell you what is to be preferred, the criteria for preference.
But the idea is that you invent, this is how we designed the method,
to invent a quantity, a real number associated to each probability distribution
in such a way that you have a transitive ranking.
If you prefer probability P1 to P2 and P2 to P3, then you prefer P1 to P3.
So to each one I'm going to give a real number,
and I say well the one that has the highest number is the one that's to be preferred.
So this is part of the method, how it's going to be,
it's being designed in order to fulfill a certain function.
You associate to each probability distribution a real number
that helps rank these probability distributions.
That real number is called entropy.
Entropy is therefore a real number associated to a whole function
designed to be maximized.
So it's not like the world comes with entropy floating out there.
We invent these tools, and we invent them for a purpose,
and we design them, this is very much like an engineering talk.
We design them with certain design specifications
to make sure that the tool is working as we want.
Very good.
So the idea by design, the whole method becomes something like
you select the posterior that maximizes this entropy,
it's relative entropy, the entropy of P relative to the prior Q,
subject to the constraints.
So what are those design specifications that I wanted to include in this ranking?
How do they come up with this function S?
First thing, I would like this function S
to provide me with a general method for reasoning.
I would like to have a method that is useful in the real world.
I do not know much about a problem, I want to have techniques
that are of general applicability.
So I'm going to design this method so that it applies everywhere.
To be useful, the method must be of general applicability.
So what we want to capture in this function S
is what all of these methods of inference might have in common.
The differences are going to be assigned to different problems,
different constraints, different contingencies of each inference problem.
We look at the commonality here.
This statement, it comes in small print and there is a reason for that.
What I'm telling you here is that there is no distinction between probabilities
as they're supposed to be used in clinical trials
and probabilities as they're supposed to be used in, say, economics.
And in particular, there are no difference between the probabilities
that you're supposed to be using classical physics and in quantum physics.
So this is where things start pushing in directions
that are not normally accepted out there in the big world.
From this approach, there are no quantum probabilities.
It's probability theory, end of story.
This is a universality design feature.
And what makes it, what allows me to fix what the function,
this entropy function is going to look like,
is a principle of minimal updating,
which is motivated by the fact that whatever information
I managed to process yesterday and the week before,
whatever I learned in the past is useful.
And I don't want to throw away all that valuable effort
that has been put into creating the prior.
I want to keep as much of the prior as possible, right?
What my grandmother told me yesterday is valuable.
I don't want to throw it away very easily.
Therefore, I will only try to update, oops, not yet.
I will only try to update what I need to in order to be in agreement
with the new information, the new constraints,
but keep as much as possible of what was learned earlier.
One of the advantages of this approach is that
what I'm going to constantly specify in trying to come up with this entropy form
is not how I'm supposed to update.
And the reason is that if I have something and I have to change,
I have to be extremely specific about the possible ways to change it, right?
And that maybe that leaves room for lots of subjectivity.
What I'm going to do is exactly the opposite.
I'm going to tell you what not to change in the prior.
When I update certain features, I don't want to change.
The reason that is interesting and extremely useful is that
while there are many, many ways to change something,
there is only one way to stay the same.
So what I'm actually going for here is an updating process
that is maximally objective, that has an incredible amount of inertia.
I will only change my mind provided I have real reasons to do so.
Very good.
So I'm going to go over the specific design specifications.
I'm going to be three, and I'm going to go very quickly over here.
What you're supposed to capture from this is essentially that there are three.
That's it, okay?
Because this comes in the form of slogans.
Locality.
The first design specification is that local informer is an updating process
that is maximally objective, that has an incredible amount of inertia.
I will only change my mind provided I have real reasons to do so.
Very good.
The specific design specifications are going to be three,
and I'm going to go very quickly over here.
What you're supposed to capture from this is essentially that there are three.
That's it, okay?
Because this comes in the form of slogans.
Locality.
The first design specification is that local information has local effects.
What that means is the following.
If I'm doing inferences about a variable x, here's x,
and I give you information about this axis,
then the probabilities of those axes conditional on being in that region
are not going to change.
All right?
I'm not going to change conditional probabilities in a region that I didn't tell you anything about.
I'm going to keep it the same.
What that means, well, if information does not refer to a particular domain D,
then the prior qx conditional on D and the conditional is important.
It's not updated.
So here is my rule.
I'm not going to change it.
There's only one way not to change it, okay?
That's it.
This is an amazing rule,
because it is this rule that makes sure that the entropic methods of inference
are completely consistent with Bayesian methods of inference.
The method of entropic inference here is being designed in such a way
as to incorporate Bayesian data analysis in those particular cases
where the information comes in the form of data.
The entropic method is a little bit more general
because it allows for possibilities of updating
when the information does not come in the form of data.
It comes in the form of my grandmother told me something
or the expected value of something.
But Bayes' rule is included as a special case.
The second important design specification is much easier to understand,
which is that the coordinates that you use in a certain problem do not matter.
It amounts to saying more or less that if you're going to carry out an inference in English
and you carry it in French, you better agree or else there is a problem, okay?
It doesn't matter what kind of language you use.
You can use Cartesian coordinates, you can use circular spherical coordinates,
confocal paraboloidal coordinates, whatever you want.
It may be more difficult to do the calculation in one coordinate system or another,
but it shouldn't make any difference as to the results.
There is a third design feature included in the whole scheme,
which is like really important, which is considerations of independence.
The reason this is important and the slogan that goes with this is that
not everything matters, and you'll see how this comes about,
but if you're trying to do science of any kind,
if your methods of science are going to be useful at all,
it must be possible to talk about a particular system of interest
without having to include the rest of the universe.
If in order to talk about something, you have to talk about everything,
your science doesn't go anywhere.
As a feature here that I want to have my method to include that I wanted to satisfy
is that it must be possible to recognize that there are other parts of the universe
that are completely independent of my system.
Something out there doesn't matter,
and the formal mathematical way of actually stating it doesn't matter
is that when two systems are believed to be independent
and we get information about one,
it should not matter whether the other is included in the analysis or not.
I must be able to talk about this system here and Andromeda,
or just this system here.
It shouldn't make any difference whether I include Andromeda in the picture or not.
Very good.
There are three constraints.
We're done.
The ranking of universal applicability that implements this desire feature
of minimal updating, give value to prior previous information
is given by relative entropy, and that's the criterion.
There is a criterion, that's the crucial thing.
So let me summarize this first part of the talk.
Oh yeah, fine print.
Andromeda's entropies may be useful for other purposes, right?
Not for updating.
There are such things at Sallis Entropies,
there are rainy entropies, and those are amazingly useful constructions
that are very, very useful for other purposes, not for updating.
They violate this design feature of independence.
So if you're going to maximize an entropy at Sallis Entropy,
because you are interested in making inferences about this system,
and you decide to include Andromeda in the picture,
well, guess what?
It changes your conclusions.
So you have to be very careful about what you do then.
Anyway, so here's the summary.
This is a space of all probability distributions.
So this is a large space.
One point in this space, it has infinite dimensions, right?
One point in this space is the prior, that's the prior.
If you were just talking about Gaussians,
then the prior would actually be defined,
univariate Gaussians, one-dimensional Gaussians.
The prior would be defined by, say, the mean and the variance, right?
So that point would be a point in a two-dimensional plot.
Very good.
Now we get our constraint.
Our constraint is, my grandmother tells me that I shouldn't believe Q anymore,
that's not the right probability distribution.
I should believe something in the blue region.
So the question becomes, which of these points in the blue region do I pick?
The procedure is, rank them according to entropy.
The maximum is going to be here at the prior, right?
Which tells me, if I had to pick the one with maximum entropy and absolutely no information,
I would not change my mind, which is like a reasonable feature of rationality.
If you were very careful about picking your prior and you get no information,
why should you change your mind?
You already thought about it.
Anyway, but you do have a constraint,
and so what you do is you proceed to pick the probability distribution in the blue region
that has maximum entropy, which is that one.
And that's it.
This is what the method is all about.
We can do statistical mechanics.
We can do thermodynamics.
We can do quantum mechanics with these things.
We can do an incredible amount of damage in science.
With Amos, we're actually trying to do some economic modeling too,
and hopefully that, and it looks good.
It's going to work, yeah.
There is another question that you can ask here,
which is like really interesting, which is the following.
Are you really convinced that that real point there is the answer
and that all neighboring distributions here that have very close entropy
are supposed to be thrown away?
Couldn't you have fluctuations a little bit uncertainty there?
Yep.
And the answer is that you can ask for,
what is the probability that you should believe something that lies not there
but in that real volume element?
And the answer is the probability that the choice should have been there
is actually according to the exponential of the entropy.
So if you have an exponential decay here, that is much, much lower,
but there is a possibility of fluctuations here.
I love this formula.
That was the Einstein formula for thermodynamic fluctuations
that he came up with in 1905 when he first worked out the theory of Brownian motion
and proved that atoms exist.
So it's very interesting how we end up beginning of the 21st century,
we're inventing these theories, inference, trying to do physics,
and then we start discovering all the laws that were invented in the past
from completely different intuitions.
So that's why physics is inference too, even when people didn't know about it.
Very good.
What next?
Oh yes, yes.
As I said before, this sort of formalism includes Bayes,
which includes the regular Maxent method of James and Shannon,
and includes large deviations theories and special cases.
Very good. Desire now.
The question is, processing information and updating
require an incredible amount of work and labor.
Thinking is difficult.
We'd rather not think.
Rather, why collect information and why go through the process of processing information?
Why not just stay in bed all day?
Answer, better beliefs lead to better rational decisions.
So the real reason why we update is that we had a prior,
but we know, this is what I ought to believe on the basis of past information,
but let's face it, I mean, it's incomplete information.
What happens if we get better, more constraints, more information?
Can I have more trust toward the beliefs?
Yes, and better beliefs lead to better rational decisions.
So let me tell you a little bit about decision theory.
And from now on, I'm going to be saying things that are completely standard.
Nothing new here, okay?
Except for one thing.
What I want to show here is the following.
If I said that the notion of information that I am,
the notion of entropy that I am pushing here has no interpretation,
that it doesn't measure amount of anything,
then how can I actually go ahead and try to quantify amount of,
a value of information and amount?
Where does the amount come from?
It's not entropy.
That's not it.
So how do I assign a value to information?
What I would like to say is that the tools for assigning value to information
are already implicit in anyone who's doing decision theory.
And the idea is that some states of the world are more desirable than others.
We want to have them be, come about.
We want to work towards making them come about.
So to each X, and X is a state of the world,
we associate a measure of desirability,
we associate a U of X,
and this desirability or value is described by a utility function.
Now you see that there is nothing new here.
What I'm doing is extremely simple,
but it also comes with all the difficulties that are inherent in decision theory,
namely, how do you come up with a utility function?
Different people are going to have different utility functions.
It's not even obvious what my own utility function is, what my values are.
It's not obvious. Those are problems to be solved.
Those problems are carried over completely from decision theory
into any theory that tries to assign values.
The same information is going to have different values for different people.
That's what's going on already here.
So the idea is that different actions,
we must decide on an action,
and different actions lead to different states of the world.
So the rational decision is to choose the action that maximizes utility.
Rational Asians might behave this way.
Human beings do not normally do this,
but there is a complication.
The complication is that the consequences of our actions are uncertain.
When we perform an action, we do not really know what it's going to lead to,
what state of the world will result,
and this uncertainty is described by a probability.
It's the probability of X, the state of the world,
given that uncertain action, alpha, was performed.
So then what we have is we do not really know what the utility is.
What we may be able to compute is the expected utility.
We take the utility function ux,
we average over it with the function here,
the probability that describes our uncertainty about which is the actual X,
and this gives us an expected value for the utility,
for the action alpha, sorry,
expected value for the utility when the action taken was alpha,
and the state of prior probability was given by Q.
The rational decision is choose the action that maximizes expected utility,
and so the alpha that is most rational to choose in this situation
is maximized over alpha, that uq.
Standard decision theory.
I'm going for the very, very simplest form of decision theory.
You can really complicate this issue by saying what if your utility depends on this,
what if it depends on that, you can really complicate it,
but what I want to point out is that I'm carrying whatever decision theory you prefer
into this formalism where entropy is a tool for ranking probability distributions
and doing updating, you can just transform that automatically.
Very good, so the complication is that we do not know
what consequences result from actions,
and the rational decision is maximized expected utility.
But now we can answer the question, why should we update?
Why bother?
And the answer is, if we receive information, we can update,
and if you update, you have a new expected utility.
The rational decision is to make the better decision
of maximizing this updated expected utility.
Very good, so now we can talk about the value of information.
The only thing here that has value or represents value is utility.
That's all there is to it.
So value is measured by utility.
Different rational agents who have different utilities will assign different values.
Yes.
So here we plot, expected utility as a function of alpha, of the action.
If our degrees of belief were measured by that prior probability distribution,
we have this green curve, right?
It's the utility expected value under Q, the prior.
And you can see what the optimal decision is.
It is the one that maximizes the green curve.
But now you acquire information.
When you acquire information, the expected values will change, of course.
And now you see two important things here in this new curve.
First, the optimal decision lies somewhere else.
The optimal decision is not alpha Q, it's alpha P.
Second, you're already, so there.
Now the actual utility, expected utility, can go up or down or whatever,
and that's not the point I'm trying to make here.
It's just that the best action under the new state of information is different.
The second important piece of information is that your original estimate of utility was wrong.
It's not at the top of the, let me see, where's my arrow here?
It's not there at the top, you were wrong.
It's a much, the blue curve is a much better assessment of utility than the green curve.
The real value for utility when you take the action of a Q is the one at the bottom.
And there is a change in that utility.
So the value of having acquired the information that leads you from prior Q to posterior P
is the difference of those two utilities, expected utilities.
So there, that's the value of information.
And it comes with all the subjectivity and uncertainties and everything that characterizes utility functions.
Good, I'm done, conclusions.
For those of you who have attended previous conferences of this type,
you must appreciate what an unusual event this is.
I am finishing within my allotted time.
Ah, this is, this is, this is...
Anyway, the two conclusions on belief.
Information is what information does, it affects your beliefs.
On desire, information has value.
It leads to better decisions that allow you to achieve what you want.
To put a little bit more flesh into this, to make it sound a little bit less wishy-washy,
conclusions, epistemic information is the constraints.
This is something that can be mathematically implemented.
The criterion for updating is you keep as much as of what you knew before, before updating it.
You only update what's really necessary.
Previous information is revaluable.
The tool for updating is entropy.
You do not need to interpret it in any way in terms of heat or in terms of disorder
and not in terms of amount of information either.
In this approach, none of those things are necessary, which is wonderful
because if I keep my mouth shut, I can't make a mistake.
So I won't tell you what entropy is, it's just a tool for updating.
Maxent-based large deviations, I didn't really prove any of this, but it's nevertheless true.
There are special cases of this, which, by the way, is really, really important.
Many of you might know that one of the real, real problems with quantum mechanics
is that there are two modes of evolution for the wave function.
One is unitary time evolution when you have the Schrodinger equation, right?
And the other is when you perform a measurement, there is a wave function collapse, right?
And this is one of the things that was bad, bad, bad physics from the very, very beginning.
People are nuts over this, right?
Anyway, if you figure out that information is being processed differently,
but it's nevertheless processed information according to entropic means,
when the particles are evolving continuously,
and this is different from when you actually put a big measurement device there
and you hammer the particle into submission and you collapse the wave function, right?
Two very different situations it appears, but it's just processing information.
One is given by entropy methods and the other is the special case of Bayes.
So there, the solution to the quantum measurement problem is the recognition
that Bayes theorem applies to quantum mechanics.
Pretty nice.
So this is important.
Finally, the value of information is the change it induces in expected utility.
Thank you.
So I have two questions.
Maybe I'll just ask one.
One of them is, so in this final definition, the idea that the value of information
is the change in utility, I can imagine mathematical and or physical systems
where when I provide new information, the utility distribution jumps around like crazy, right?
And so that I have a system that's jittery in some sense
and that a very small change in this information is providing me these changes,
apparent changes in utility based on my updated posterior,
but that's not necessarily meaningful because I just have a kind of system that's very jittery
and the next time I get a new piece of information, it'll recenter my...
I'm not sure that you're thinking of utility the way that people who have to make decisions are thinking about utility.
Typically, a person who has to make decisions will have an option, a menu of choices
and each one of these has its own expected utility.
Nothing need to be shaken or maybe it is, but the idea is there is a utility associated with one of each one of those actions
and the weight will change, shift around when I update.
So the situation you describe, hopefully as it looks, may not be quite what people are interested in when they want to make decisions.
Okay.
Hi, Eric. It's great talk.
And so what's your view about in terms of the truth
because when come to a probability function p and q?
Yeah.
Which are often we don't know, we couldn't find out what is the truth, probability function p and q at specific time.
So that's the truth.
I'm sorry, I guess that's kind of problem.
So go back.
Truth.
Truth. Going back to your old relative, the earlier slides on the relative entropy and with the p function and the q function.
Probably as the true probability.
Probabilities, just like an entropy, probabilities are tools for reasoning.
They serve to describe the degree to which I ought to believe in the truth of a certain proposition.
Another way to describe it is probability describes the degree of implication in which a implies b or something like that.
So there are not probabilities are not true.
There are not reflect states of the world.
There are tools for reasoning with incomplete information.
There are tools.
They're not.
And so if my prior reasoning led me to believe in a certain q and then I acquire new information and say,
okay, now I change my mind, I go to some other probability I update.
But none of those are states of the world.
We're not talking, that's why I say physics is not about reality.
Physics is more about what we can say about reality.
Physics is way more like economics.
Economists are honest enough to recognize the limitations of their modeling.
Physicists have not yet reached that level of wisdom.
Okay, yeah, I got my points here.
Okay, yeah, a quick one.
Yeah, a quick one about Slice 19, but I can do it also without the slides.
So you walk into a party and you want to know who Mary is.
Okay, and someone...
No, no, no, let me finish, let me finish.
Okay, you want to know what X is and someone tells you X is what X does.
Now that is fine, but it's a criterion to identify X does not tell you what X is.
Are you happy with that?
It's an open question, it's not a criticism.
So you may be told Mary is the one who is actually cutting a cake.
You don't know who Mary is, but you can identify Mary and you go there and shake the hand.
So hello Mary, fine.
So is that the difference between criterion for X successful versus definition of X?
I think you're objecting to the information is what information does, right?
I'm not objecting, I'm just asking you whether you are happy with the criterion base rather than the definition.
I'm actually retreating from saying what information is.
I'm telling you, you don't need to know more than if I have a constraint, this is the way you update.
Forget the word information here now.
Forget the word information.
I have a prior.
My grandmother tells me, ah, forget that prior, you better give me one of these probability distributions here.
I update.
I never use the word information, right?
So information is a little word that we append at the very end to make this process a little bit more palatable and more efficient, right?
But I never needed to tell you.
Much the same way that when you do statistical mechanics or thermodynamics,
I may be interested in figuring out if two systems are going to be in thermal equilibrium, whether heat is going to flow or not, right?
And I may do my calculations and then there is a particular parameter there that appears in my calculations that I call T.
And the question is, what does T do?
Well, I don't know, but if T is big and this T is low, then heat flows that way.
And if it's other way, it's the other way around.
And so at some point, I'm going to say, oh, this T appears all the time and then I give you a name.
Temperature.
So that's the way I'm going with information.
I'll give you a name to a particular concept that keeps appearing all the time, namely constraints.
Yeah?
It seems very much to be describing this kind of activity of a basic and wonderful program.
In this case, information is a situation in the project.
It depends on what you're at all.
Whatever the situation that you have tonight.
Your best point about the problem is information.
Okay, I have a few of some causes that we do talk to you by now.
Turns out that I can double find, so find a way to value information to actually value a change that I'm actually using in this place.
That's to the extent that you place, you describe your preferences in the form of, if I prefer X1 to X2, I assign a bigger U, X1.
Then it's the same thing, you take care automatically.
It's automatic.
It's...
I think I have a pretty reserve of time for the next issue.
Thank you.
Thank you.
