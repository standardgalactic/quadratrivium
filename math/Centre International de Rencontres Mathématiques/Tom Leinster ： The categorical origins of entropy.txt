"...
Celenniod di'r cerddiniach,
oeddech i ni ar dan yi'n bach,
a'i'r cerddiniach yma yn d Bildion.
Gydd custom yn benedig er mwyn oeddynd,
sydd yn yumyn ar yr ywynhwr,
ac yma'r myryw buddies.
Felly dim gwn i fy門 â'i chliad ac Eryf yng Nghymru.
..stad raising all出 cloak myth합니다.
Felly another thing I find exciting is that we didn't have to submit ..
..a title or abstracts beforehand.
Erdaglef y ffriddym wedi gweld ..
..a wahanol yr adrodd.
Rwy'n strifeld gan flynyddoedd – ..y gyrionedd yn eu abrasio ar enlygen.
Fy"?n ddannol i'n dod....
..wysgfa yrddangos diwrnod ywe.
I'm going to give this talk, but I kind of want to give this talk as well, and I was asking myself yesterday which one should it be.
So here's a talk I'm not going to give.
So the talk I'm not going to give is about maximum entropy on metric spaces,
and what I'm not going to tell you is that if you take a metric space, a finite metric space,
then there's a sensible notion of the entropy of any distribution on that space.
So the entropy of order Q for any real parameter Q, and this is sensitive to the distance between points.
And for example, in the extreme case that all the points are completely separated, the distances are all infinity,
this just reduces to the random entropy of order Q.
So in particular, it's the Shannon entropy when Q equals 1.
And there's a maximisation problem.
So you fix your space and you ask what the maximum entropy is of all possible distributions on that space.
And in principle, the answer depends on Q, but actually it's a non-trivial theorem that it doesn't,
that it's independent of the order Q that you're talking about.
And of course, I mean, this would be rather trivial if it was always a uniform distribution,
but it's not in general the uniform distribution.
So the maximising distribution is independent of Q, and moreover, the maximum entropy itself is independent of Q.
And so, and in fact, almost always this maximising distribution is unique.
So, I mean, one of the upshots of this is that on almost every finite metric space,
there is a canonical maximum entropy distribution.
There's a canonical distribution on almost every finite metric space, and it's not a uniform distribution.
And so, I think about this a lot during yesterday's talks because there are all these talks
involving the normal distribution as the maximum entropy distribution,
things like the isoparametric problem in that respect.
It's probably true that these results extend to compact metric spaces.
This is work in progress by my collaborator, Marcus.
But in any case, it certainly is true for finite metric spaces and there's a reference there.
So that's a clickable link if you download it from my website.
So anyway, that's the talk I'm not going to give.
And the reason why I'm not going to give it is that it belongs to yesterday.
It belongs to the theme of Monday, session one.
And so this is session two.
This is more of a topology session.
So I thought I should give the other talk.
So now I'll give my talk.
Excuse me, I have a question for the talk that you are not going to...
Yeah?
So what sense is the entropy of all the Q sensitive to the matrix?
Well, I haven't given the definition, but...
And I'd be happy to...
Sorry?
Are you referring to the sun?
No, I'm referring to something else.
I'd be happy to tell you about that afterwards.
Okay, thank you.
Right.
So...
As I said, there's this amazing array of knowledge in this room.
And one thing that category theory is for is kind of mathematical anthropology.
So it's looking at mathematicians and other scientists and just noticing what they do.
So let's think about where entropy comes up within science,
in particular mathematical sciences.
And of course it comes up everywhere.
There's entropy in thermodynamics.
There's entropy in communications engineering.
And this is just some kind of random references.
Entropy and information theory in statistics and in inference, in machine learning.
And in macroecology, in the measurement of biodiversity, in biochemistry and chemistry,
in the robustness of water distribution networks,
in the study of complexity, as we just saw in the last talk.
And then more mathematically in ergodic theory, dynamical systems, algebraic dynamics,
combinatorial dynamics, topological dynamics, and of course information geometry.
And so, you know, as we're all very well aware, entropy comes up everywhere.
But in this conference information pack that you may have had,
I noticed it says that entropy has a sense of humour.
And one humorous thing about it is the world of algebra and topology.
So an algebraist or a topologist may never ever think about entropy at all.
Right?
If you're an algebraist or a topologist,
you can go for your whole life without ever saying, writing,
or even hearing more or less the word entropy.
So that's an algebraist or maybe a topologist there.
And on none of these pieces of paper is the word entropy even written.
Right? He spends his whole life not thinking about entropy.
And this is entirely possible in algebra and topology.
It's completely normal, in fact.
And so the point of this talk is that although that is sociologically the case,
there is a sense in which, a precise sense in which,
entropy is inevitable in algebra and topology.
So even if you're not interested in thermodynamics or information theory, etc. etc.
Purely from the algebra and topology point of view,
I'm going to argue that entropy is inevitable.
You have to accept it as a natural thing.
And the argument is a categorical argument.
So this is a mixed audience,
and so I'm going to say some general stuff about what category theory is and does.
So what category theory does is it attempts to rise above
the mathematical landscape and look downwards to spot patterns
that you can't see from ground level.
And another way of saying that is that it takes vague analogies
and it makes them precise.
And in doing so, you often bring other things into...
You see that other new things are analogous to the old things
that you didn't realise before.
And so that's what we're going to do here.
We're going to show that Shannon entropy appears in that sense.
And so I will need some category theory in this talk.
I'm going to rely in an essential way on the previous insights
and concepts of category theory,
but because this is a mixed audience,
I'm going to try to minimise what we need.
So starting about halfway through the talk,
we will need the definition of category
and a functor and natural transformation and a little category.
So those will start to be needed about halfway through
and then right at the end, we'll need a little bit more.
But for the most part, I'll try and keep the knowledge to a minimum
so that this is as broadly comprehensible as possible.
In the most direct possible terms, this is what this talk is about.
There is a categorical machine.
You put into this machine, on the one hand, the simplices,
the topological simplices, which are, of course, a central object of topology.
And on the other hand, you put in the real line,
which is obviously also a central concept.
You turn the handle on this categorical machine
and what comes out is Shannon entropy.
So that is a sense in which it's inevitable.
There's this very general construction that I'll tell you about.
And that when you give it these very canonical inputs,
this is what comes out as the output.
So that's the sense in which, you know, if you're,
even if all you're interested in is algebra and topology,
you kind of have to accept entropy as an inevitable concept.
So most of the time I'll be spending,
I'll be telling you what this machine is.
And so here are the words.
You look at the internal algebras in the categorical algebra for an operad.
So I have to tell you what all that means, right?
And I will.
So, here's a plan.
We start off with operads and their algebras.
These operads are kind of very widely studied concepts.
Then I'll tell you what internal algebras are.
And then there'll be the main theorem.
And then hopefully I'll have time to tell you about a corollary,
which is kind of understandable with no categorical jargon whatsoever.
So here we go.
Operads and their algebras.
So what's an operad?
It's first of all a sequence of sets.
And there is more to this sentence to come.
But you should think of an element of on
as a kind of abstract operation that has n inputs and one output.
So here's an element of o3.
You think of going down the page.
So three inputs at the top and one input at the bottom.
So it's got some extra structure.
And the main bit of extra structure is composition.
So what's going on here?
We've got an operation theta with n inputs.
An operation of phi1 with k1 inputs.
That's a superscript and not a power here.
An operation phin with kn inputs and so on in between.
And when you put them all together,
the number of inputs is the number of inputs along the top.
So it's composition like that.
So the various phi's feed down into the theta at the bottom.
So that's the main bit of structure.
There's also an operation with one input that functions as the identity.
And these things are subject to associativity and unit axioms.
And the effect of the axioms is that if you have any tree of operations,
maybe with ten things going up here and seven going up there,
any shape you like, there's a unique way to compose it down into a single operation.
So that's what an operad is.
They originally rose in two places at the same time on the one hand
in category theory and linguistics,
with Jim Landbeck on the other hand in algebraic topology
with Boardman, Fogarton and May.
So here are some examples.
The very simplest one, it's kind of boring but I'll talk about it a lot,
is the terminal operad.
So that's just, ON is just a single point.
So there's a single operation with each number of inputs.
So you don't need to write the labels at all.
And I don't need to tell you what the composition is
because there's only one possible choice.
Okay, second example.
Suppose you have a monoid.
So what's a monoid?
A monoid is a semi-group with an identity
or to put it another way, it's a group without inverses.
So if you have a monoid, you can turn it into an operad.
Again, it's kind of trivial.
It's only got operations with one input.
There are no operations with zero inputs or two inputs or three inputs.
The operations with one input are the elements of your monoid
and the composition of course is by multiplication in the monoid.
A more substantial example is the operad of polynomials
over your favourite field.
So there, an element of pn is a polynomial in n variables.
And to compose these things,
well the point is that if you've got a polynomial
and in each variable you put another,
you substitute in another polynomial,
then the result is also a polynomial.
So for example, if you take phi1 to be this polynomial,
phi2 to be that polynomial,
and you substitute them into this polynomial theta,
you get a polynomial out again, like this.
The only thing is that you have to re-index
in order to do things sensibly.
So phi2 here, I've shifted all the indices up by three,
so they're numbered in sequence.
Okay, so that's a fairly typical example of an operad.
But I haven't yet said the one that's really going to matter for us,
which is the operad of simplices.
So the operad of simplices has, as it's n or e things,
it's operations with n inputs,
are the probability distributions on n elements.
Of course the set of such forms a simplex of dimension n-1.
So well certainly we have got this sequence of sets.
Delta 0, delta 1, da da da.
But I need to tell you how to compose them.
Probability distribution can be composed in this way,
so that's the general formula,
but it's probably easiest to explain it with an example.
So suppose I flip a fair coin.
If the result is heads, I roll a fair die.
And if the result is tails, then I draw fairly from a pack of cards.
So the outcome of this composite process is,
there are 58 possible outcomes.
Either it's a number between one and six,
or it's the face of a card.
And the probability distribution that comes out is this,
it's no longer uniform.
So the probability that the end result is a four on the die is one over twelve.
The probability that the end result is the two of hearts is one over one over four.
So that's a distribution on 58 elements.
Okay, so this is a very natural operad
embodying probability distributions in that composition.
Or if you're geometrically minded,
you call it the operad of simplices.
What time did I start? Was it 10.40?
Okay.
So operads have algebras.
These are not algebras in the sense of algebras over a ring.
I mean it's not a completely distant concept,
but it's a different concept.
So you should think of it like this.
Just as groups have actions and rings have modules,
operads have algebras.
So let's take an operad.
So an action of the operad is a way of interpreting its operations,
like something with three inputs and one output,
as actual operations on an actual set.
So an O algebra consists of a set together with a map like this
for each element theta of the operad.
So if theta is a thing that I invited you to think of as having n inputs,
then it's an operation with n variables here, n arguments.
And it has to satisfy axioms that look like the usual axioms for an action.
So there's one axiom involved in composition.
There's one involving units.
And so the simplest example is when O is this terminal algebra
where there's just one operation with n inputs for each n.
And then what we've got is just a single function from A to the n to A
for each value of n satisfying these axioms.
And it turns out that's just a monoid.
That's just the n-fold multiplication on A.
That's what this thing is.
So in this case an O algebra is just a monoid.
I used the word monoid before in a different context,
which is if we start with, if we fix a particular monoid m,
then an algebra for this operad om that I defined for
is simply a set equipped with an action by m.
So again, you see the link between algebra and actions.
OK.
So now we've got the operad of simplices.
And this is the one we really care about.
I'm not going to give a general description of what an algebra for that is,
but I'm going to point to a particularly significant example,
which is if we take a convex set.
So we take a convex subset of any Euclidean space.
And this is an algebra for the operad of simplices in a very natural way.
Because what do we have to do?
For each probability distribution on n elements,
we have to define a map from A to the end to A.
And because A is convex, there's this very natural way of doing it,
which is you simply take the convex combination corresponding to P.
So we've now got three kind of subtly different ways of thinking about delta n.
It's the set of probability distributions on n elements,
or it's the elements, the points of an n minus 1 simplex,
or it's the possible convex combinations of n things.
OK.
So far, it seems like not very much has happened,
but we need some more conceptual stuff.
So I mean groups, think about group actions.
Groups can act on lots of things.
Groups don't have to act on sets.
They can act on topological spaces or vector spaces or whatever.
And similarly, operads can act on many things.
And well, I kind of put it in an abstract way here,
which you may or may not like,
which is that you can talk about O algebras in any category of the finite products
and not just the category of sets.
I'm going to say it more explicitly.
Sorry, let me just say it differently.
So in particular, you can talk about O algebras in cats.
So cat is the category of categories.
And that's called a categorical O algebra.
So let me say this explicitly.
A categorical O algebra is a category with an action by O.
So instead of having a set A and a function like this for each theta,
a category bold A and a functaw like this for each theta,
satisfying the usual kind of axioms.
So it's a category acted on by our operad.
And again, standard examples of operads give sensible things here.
So if we take the terminal operad 1 again,
a categorical algebra for this is a strict monoidal category.
So it's a category with some kind of tensor product on it.
A categorical algebra for this operad O of M is a category acted on by M.
OK.
And now I want to talk about categorical algebras for delta,
for the operad of simplities.
And I have to explain something to you first.
So some of you will be familiar with this and some of you not.
What is a category with only one object?
So a category with only one object.
Where's the chalk?
That's here.
A category with only one object.
Look, there's the object.
It just consists of a bunch of arrows like this,
which are allowed to compose with each other.
So the fact that we're drawing them as arrows doesn't really matter.
It's just a bunch of things that you can multiply together associatively.
So a category with only one object is the same thing as a monoid.
The elements of the monoid are the maps in the category.
The multiplication in the monoid is composition in the category.
So if I take a linear subspace of Euclidean space,
I shouldn't really have written this plus and zero here, never mind.
If you take a linear subspace of Euclidean space,
it is a monoid under addition,
which means you can interpret it as a category with only one object
whose maps are the elements of A.
The composition in the category is addition.
So this is a standard categorical thought that monoids,
for example groups, are special kinds of category with only one object.
This becomes a categorical algebra for the operative simplices
via convex combinations.
That's the formula I showed you at the bottom of the last slide.
The point is that when you take convex combinations according to P,
that is a linear map or an additive map in particular.
We only care about the example where A is the real line.
So there's one more little part of this story.
It's an important slogan of category theory
that whenever you have some objects that you find interesting,
you should ask yourself what the maps are,
what the appropriate notion of map is.
We're talking about categorical algebras,
so we should talk about maps between them.
Let's take a pair of categorical algebras for a particular operad O.
What should we mean by a map from B to A?
B and A are categories with certain extra structures.
For a start, it should be a functor like this.
It's like asking what's an equivariant map between sets acted on by a group.
There should be some compatibility with the action,
which means something about this square.
You could just stop the definition there
and say this square ought to commute,
and that would be some notion of a map between categorical algebras.
But this whole square takes place in the category of categories.
A and B and so on are categories. These things are functors.
So you don't have to ask that it actually commutes.
You could just ask that it commutes up to naturalisomorphism.
Or you could go even further and say
there just has to be a natural transformation in one direction or the other.
That's actually the notion that we need here.
A lax map is a functor like this,
together with a natural transformation in this direction
that satisfies some sensible axioms.
Again, I should emphasise all this language that I'm telling you
is completely standard language.
It's not something that I've made up for the purposes of this talk or anything.
It's applied in all sorts of ways.
Because the level definitions are building on definitions in this talk,
I want to say I'm going to keep telling you what everything is in explicit terms.
So it's a functor g together with maps like this.
So we'll come back to...
I'm going to use these explicit expressions later.
You can kind of ignore them for now, but that's what it is.
The kind of axioms they have to satisfy,
well, again, there's one axiom on composition and one axiom on units.
Right. So that's that.
Because of the particular application I'm telling you about,
where we're talking about the convex combinations of real numbers,
I want to talk about the case where...
Oh, no, sorry, I do apologise.
I got my slides.
That's the end of that section.
Okay, so let me just recap.
So that was... I gave you definitions of operad, algebra and categorical algebra
and lax map between categorical algebras.
Right.
So we're most of the way through the phase of all these definitions.
And the next chunk of that is to do with internal algebras.
And so we're fixing an operad.
We're fixing a categorical algebra for it.
And remember that we've got...
Sorry, not remember, but there is a terminal categorical algebras
that just consists of the one object of the category of one object and one map.
And it's an O algebra in an obvious way, in a unique way.
So an internal algebra is defined to be a lax map from one.
So what's going on here?
The thing to think about is that...
..if you want to talk about points of a topological space...
..so a point of a topological space can be regarded as a continuous map
from the one point space to your space.
Or more generally an element of a set x is the same thing as a map
from the one point set to x.
And so here we're talking about a particular kind of map
one of these lax maps from the terminal O algebra into A.
And so in explicit terms it's this, and again we'll come back to this explicit expression,
and it's got to satisfy these axioms.
But really the best way to explain this is to look at some examples,
and this example will justify the terminology.
If we take the most trivial operad, the terminal operad,
we've already seen that a categorical algebra is a monoidal category,
a category with some kind of multiplication or tensor product.
And if you look at the definition, you see that an internal algebra in it
is simply a monoid in that monoidal category.
So for example if your monoidal category is abelium groups with the tensor product,
an internal algebra is a ring.
So an internal algebra is an internal monoid.
So an algebra for A is a monoid, an internal algebra is an internal monoid.
So the terminology fits well together.
So how do you see that?
Well, what you've got is this thing, in this case there's only one theta for each N,
and the domain here is just the n-fold tensor product of A.
Okay, anyway.
And the second of our running examples, we had some particular monoid M,
and we were talking about this operad O of M that you get out of it,
which has no operations except with one input.
And a categorical algebra for that is a category acted on by this monoid.
And then, I mean, you can kind of contemplate this over a cup of tea,
but when you have a category acted on by a monoid M,
you can think about objects of that category acted on by M,
and there's a sensible notion of it which looks like this.
This won't be important for what we do, but I guess this example is just to illustrate
that for various different operads other than the one that we really care about,
sensible things come out.
So we care particularly about categories that have only one object
once it looked like this, which correspond to monoids.
So using the explicit description that I just gave you,
an internal algebra then looks like this,
satisfying actions on composition and unit.
So this is worth, I mean, the earlier expressions,
the earlier explicit expressions were maybe not super important to
for you to get into your head, but this one kind of is.
So if you have an internal algebra in a one object categorical of algebra,
what it amounts to explicitly is just this stuff here.
So I'll come back to that.
Right. Everything I've said was about sets, everything can be to apologise.
So, you know, the categorical jargon is you say you work internally
to the category of topological spaces, so instead of talking about sets,
you talk about spaces instead of talking about categories,
you talk about topological categories.
But what it means explicitly is that everything I've said,
you just add a continuity condition.
So instead of having just this condition one on composition,
condition two on units that I keep mentioning,
you add condition three as well on continuity.
So that's that.
So now I've given you a whole lot of language,
and either, you know, I'm sure some people in this room,
I know at least one person in this room knows everything I've just said,
I'm sure some people in this room follow just about nothing I said,
but let me bring you back to the overall picture, which is as follows.
I promised you that there is a categorical machine
that when fed the simplices and the real line,
so when given these as input, produces Shannon entropy as output.
I've gone through these standard categorical definitions here,
and all I want to do, or what I want to do next,
is to show you how Shannon entropy comes out, right?
So I've described the machine.
So here's the theorem.
So the inputs, there are two inputs.
On the left we have the operative simplices.
So that's, again, that's just all the collection of all probability distributions
or finite sets together with the way in which we compose
probability distributions for composite probabilistic processes.
On the other hand, we have the real line,
which is, I mean, the real line is a convex set, right,
and which means it's acted on by this operative.
And we just saw that an internal algebra
in this one-object categorical delta algebra, r,
consists of functions like this satisfying certain axioms.
Let me just flick back to that.
There we are. See, an internal algebra in a one-object categorical algebra
consists of a bunch of functions like this satisfying some axioms.
So here, o, n, is the simplex delta run.
So ever since, I mean, for most of this talk,
I haven't mentioned the word entropy at all.
I'm like the algebraist with a messy desk, right?
I haven't said entropy since the beginning of the talk.
But, of course, everyone here knows a famous sequence of functions
of this type, namely Shannon entropy.
And the fact is that Shannon entropy and its scalar multiples
are the only internal algebras like this.
So from this very abstract, very general categorical definition,
given these absolutely standard inputs here,
Shannon entropy just drops out.
So that's the sense in which there is no avoiding Shannon entropy,
even if all you care about is algebra and topology.
So before I explain this any further,
now would be a good point for any questions on the meaning of this theorem.
OK?
So, let me again put the theorem into explicit form.
So when you unwind all the definitions,
when you undo the definitions of operand and algebra
and categorical algebra and internal algebra,
what this theorem says is the following.
It says, take a sequence of functions like this,
then it's a scalar multiple of entropy,
if and only if, it satisfies the following conditions.
So first of all, gamma of some composite distribution
is given by this formula here.
That comes out of this composition condition that I've been labelling one.
Secondly, that's gamma of the unique distribution on the one element set is zero.
And thirdly, that all the gammas are continuous.
You know that when gamma is Shannon entropy,
these are all satisfied.
And obviously, if you scale Shannon entropy over a constant,
they're still satisfied.
And it's more or less a theorem,
this famous old theorem of Fadaeff,
that these three conditions are equivalent
to being a scalar multiple of Shannon entropy.
There's a slight difference.
So Fadaeff himself, in his original work,
didn't seem to have noticed that you don't need the symmetry axiom,
so he imposed the symmetry axiom,
but you don't need it if you write the composition law,
this thing that's sometimes called the chain rule,
in this particular form.
There are a bunch of other equivalent forms of it,
and depending on which form you use,
you may or may not need to impose symmetry axiom,
but if you put it in this reasonably general form,
then you don't need symmetry.
Yeah.
Maybe entropy will not satisfy the first law.
Correct.
Yeah.
And nor will the so-called Salis enthrapeas.
And do you have symmetry in your composition laws?
No.
For the definition of opera?
No.
No, so sometimes, I mean just, you know,
there are rings coming in commutiv and non-commutiv flavours,
and opera rads come in symmetric and non-symmetric flavours,
and so, yeah, often when people talk about opera rads,
they assume a symmetric group action,
but I don't have one here, it's the simpler kind.
I mean, I could have had one,
and because of course it is true that if you permute the arguments of entropy,
then it doesn't change the entropy.
So I could have worked with symmetric opera rads throughout,
but I didn't, and that gives a stronger theorem.
Okay, so that's the result.
Yeah.
Does this allow us to restore a free field?
For every field.
You mean in place of the real line?
Yeah.
That's an interesting question.
Right.
No, I'd be happy to hear about that from you.
Yeah, I don't know anything about that. Thank you.
Okay, so where are we?
There's been a lot of abstract language,
and at the risk of repeating myself many, many times,
the abstract language was not created just for the purposes of giving you a theorem about entropy.
This is language that's used for other purposes to general language.
We started out with operads and algebras,
and then there were categorical algebras and internal algebras in a categorical algebra,
and finally we reached the top and declared that if we take a particular operad
and a particular categorical algebra for it,
namely the operative simplices and the real line,
then the internal algebras are exactly Shannon entropy and its scalar multiples.
So we see the Shannon entropy coming out of it naturally.
But you see that this guy hasn't quite reached the top,
and we can go a little bit higher,
and something interesting is going to happen.
So you'll see what happens.
So this last bit of the talk, this is joint work with John Byers and Tobias Fritz,
where is Tobias? There he is.
So this is from a few years ago.
So what we're going to do is we're going to go a bit higher up and then see what happens.
So the next slide, for one slide only,
I'm going to assume quite a lot of categorical fluency,
but you'll see what happens.
Right, so it's a standard fact that you'll find in, for example,
McLean's introduction to category theory that a monoid in a monoidal category
is the same thing as a lax monoidal functor from the terminal, the terminal category.
And it's also noticed there that a monoid in a monoidal category A
is the same thing as a strict monoidal functor from D,
the category of finite totally-ordered sets with disjoint union is the product.
So another way of saying this is that this category D is the free monoidal category containing a monoid.
So if you start with nothing and you build a monoidal category that contains a monoid
and you put in no more objects, no more maps, no more equations than you need to,
then what you get is totally-ordered sets.
Now all of that, all this stuff about monoidal categories and monoids in them,
we've just seen that that can be phrased in terms of algebras and internal algebras for the terminal operad one.
So we can try and tell this story here for different operads,
and in particular you can try and tell it for delta.
And what happens is that if you take the free categorical delta algebra,
that's like a free monoidal category containing an internal algebra,
that's like containing a monoid, then it's pretty much the category of finite probability spaces.
So let's be precise, by Finprob I mean the category whose objects are finite sets
equipped with a probability measure and whose maps are the measure-preserving maps,
which you can think of as deterministic processes.
So I'm not going to explain why that's true, but it is true, or at least it's nearly true.
But nearly the caveat there is to do with zero probabilities.
So there are some delicacies involving probability zero which kind of,
broadly speaking, anyone with Bayesian blood shouldn't be surprised that there are some delicacies there.
But roughly speaking this is the category of finite probability spaces.
And so the upshot of this is that, by definition, to say that,
oops, pardon me, to say that Finprob is the free what not,
is to say that a functor from Finprob to something else
is an internal delta algebra and something else.
So the internal delta algebra is in the real line.
We know two things about them.
On the one hand, the big theorem that I showed you says that they're the scalar multiples of Shannon entropy.
And on the other hand, we're just observing that they are certain functors like this.
Now an internal delta algebra is kind of an abstract thing,
but a scalar multiple of Shannon entropy is not at all an abstract thing
and a functor is also a pretty concrete thing.
So what that means is that because internal delta algebras are equivalent to these two different things,
those two different things are equivalent.
So I've just said all this particularly abstract stuff.
And now what happens is this, that we kind of fall off the cliff of abstraction
and we land on something very concrete.
And this concrete thing that we land on is an absolutely explicit characterisation of entropy
that has no categorical terminology whatsoever.
So, you know, even if you have never even seen the definition of category,
the following theorem is entirely comprehensible to you.
So, here we go.
Suppose we're interested in functions that assign a real number
to each measure-preserving map between finite probability spaces.
So think of such a map as some kind of deterministic process.
It assigns a real number to each search map, probably a positive real number.
And you think of it as something that measures information loss,
the information lost by the process.
So some sensible axioms that it could satisfy as follows.
If you have a composite process, do one thing and then another,
the information lost by the composite process is the information lost by the first bit,
the first process plus the information lost by the second process.
That's the first axiom that it could sensibly satisfy.
The second one, so I haven't defined this notation,
but you can guess roughly what it means at least.
Suppose you have two different processes.
You can think of them as parallels.
So these have different domains and co-domains, different sources and targets.
And you flip a probability lambda coin.
And if it's heads, you do F, and if it's tails, you do F'.
So that's what this process is here.
Lambda F plus 1 minus lambda F means the process you get by flipping this coin,
then doing either F or F' accordingly.
So the information that's lost by that whole process
is lambda times the information lost by F
plus 1 minus lambda times the information lost by F'.
OK, so if it's 50-50, then it's just the average of the two.
So that's the most non-trivial axiom.
And then, if F's an invertible process, then it should lose no information.
And if you change the process a little bit,
it shouldn't change the information lost very much.
So if all that's true, then L must be simply the difference in entropy
up to a scalar.
And conversely, if L is defined by this, then it satisfies all this.
OK, so that's a characterization of entropy in terms of information loss.
And what I think is particularly charming about this is theorem is that
if you look at these four conditions, they look linear.
Well, I mean, the first two look linear and the second two are kind of trivial.
So there's no hint that something involving p log p is going to be involved.
So there's somehow it comes out of it.
And this theorem is just an explicit, concrete way of rephrasing.
Oops, there he goes.
It's just an explicit, concrete way of rephrasing this stuff here.
Yeah, right.
You have to say what you mean by continuity.
It's up to you. There are different ways of doing it.
You mean do we want more than one object in each isomorphism class?
You can handle it either way.
It's easier to handle it if you just have one of each, I guess.
Excuse me, what is lambda x?
Lambda is a real number?
Lambda is a real number between 0 and 1.
So I'm not defining this.
I'm defining this.
When I say I'm defining it, I mean, I kind of said the definition out loud.
I haven't written it down.
So let me say it again.
So you flipper probability lambda coin.
And if it's heads, you do F.
And if it's tails, you do F prime.
That's the idea.
Your second axing there really looks like something related to delta 1.
You don't have to use delta 2 or delta 3.
That's right, yeah.
OK, so since I'm nearly at the end, maybe I'll just finish
and then we can discuss that more.
It sounds like there are more questions.
So let me summarise what we've done.
There are standard notions that are useful in many, many parts of mathematics
of operas and algebra.
And there's also this concept of an internal algebra that's not as well known,
but it's still a kind of a completely general definition
that wasn't made at all for this purpose.
And so when you apply these definitions to the simplest possible operad,
the terminal operad 1, it gives you the concept of an internal monoid
in a monoidal category, which again is just a kind of a standard thing.
On the other hand, if you apply it to the operads of simplices and the real line,
then it gives the concept of Shannon entropy.
So somehow the concept of monoid is analogous to the concept of Shannon entropy in this sense.
So that was the first theorem.
And the moral of it is that even if all you care about is just kind of very pure
algebra and topology, the concept of entropy just comes out naturally.
It's inevitable.
So that was the main theorem.
And then this corollary was that given any operad,
oh, you can form the free categorical algebra containing an internal algebra.
Again, when you do it to the terminal operad 1,
it gives you the category of finite totally ordered sets,
which is a reasonable enough thing.
When you do it to the operad delta,
it gives you very nearly the category of finite probability spaces.
So these things are analogous in the same sense.
And the moral of that is what the outcome of that is that you get a new and,
or newish, 2011, a newish and completely explicit definition,
a characterization of Shannon entropy that looks like it's very linear,
but somehow Shannon entropy comes out of it.
OK, that's everything I want to say.
Thank you for listening.
APPLAUSE
OK, one more question.
If you take instead of R as your algebra,
do you get any interesting generalisation?
If you take some other vector space,
or R or something, do you get...
I think as far as I can tell, it's not super interesting
in that you just get different multiples of Shannon entropy
and different coordinates.
Unless I'm missing something, I think that's all that happens.
So it's not as interesting as it might be, I'm afraid.
So if you apply your theory to general topological spaces,
the category of topological spaces and probability distributions,
say, rad of measures on topological spaces,
what would be the outcome of your theory?
You could also consider their measure-preserving maps.
So I'm not quite sure what you mean.
So what is it that we're replacing by general topological spaces?
So instead of having a finite set of elementary events,
we consider topological space.
So are we looking at this result here?
OK, so let's take that one.
So you have the general topological spaces
and the final is the measure-preserving maps
on these topological spaces.
I have a feeling that I'm going to ask Tobias
if he's thought about that, because it seems like you might have done it.
So the thing is that you can consider the composition
of the measure-preserving maps and require that the information loss
of the composition is simply the sum of the information losses.
That makes sense for general topological spaces.
Then you can consider this second construction in the same way.
So everything makes sense without the restriction to finitely many events.
So what would be the associated entropic quantity in that context?
That's a good question. I don't know what the answer is.
I'm afraid, yeah.
Are you putting your hand up to us because you know the answer?
Tobias?
For relative entropy.
Are you talking about your work with John Byers?
No, that was only for finite sets as well.
I don't know anything like that, but I'm going to have done it for polar spaces.
Probably for some polar spaces.
Right.
Thank you.
So do you need the existence of a reference measure?
It's about relative entropy, so yes.
Okay, do you need something like a Ha measure?
No, it's just a laboratory.
Some reference measure.
Okay.
Can you consider within your framework the quantum case
and try to recover the fun name on entropy?
So that's certainly not something I've thought about.
You can very easily get similar characterisations of the salisentropy of any order.
I don't know whether you would call that quantum or not.
But if you simply put Qth powers here, where Q is the order of the entropy,
or similarly back here if you stick the Qth powers there,
then of course you get the salisentropy,
but that's the only kind of defamation that I know about.
It may be possible to do what you're suggesting, I just don't know.
I was speaking about the quantum case.
Natural entropy would be more the fun name.
I just don't know, I'm afraid.
Thank you.
The first appearance of categorical approaches to probability setting
is now by Nicolae Centso, who was the last PhD student of Kolomogorov.
In his book on the maximum decision rules, there are several categories defined.
It's 1972.
So for example, he had the category of couples of probability measures on the same space,
and he came up with relative entropy as the most natural invariant for the category.
So he's embedded in your approach.
Fantastic, right. I'll have to get the reference from you. Thank you.
Thank you.
If you replace R by a different field, say Fp,
what kind of function do you get, or do you get a similar result?
Yeah, I don't know.
So is it the finite polylog maybe?
Okay.
I guess I have a kind of sociological question.
I mean, it's very interesting to see this beautiful pure maths brought to a room of people who know about entropy.
So what happens when you go the other way, when you tell the pure mathematicians about entropy?
I mean, as the guy with the photo changes his office now,
is somebody's piece of paper got entropy on them?
Yeah, I don't know. I mean, I have done exactly what you said.
I've given a version of this talk to an audience of topologists.
I didn't show the photo of the guy in his office, but I don't know.
Maybe no one ever gave me a really honest reaction.
Well, as I say, people gave me positive reactions, but I don't trust those as being honest.
So, yeah, I guess there's a difference between being convinced in theory and changing your life.
I don't think it's made anyone go and spend their life studying entropy,
but I have a feeling people have been moderately convinced.
That's not a very interesting answer. I'm sorry.
The question?
If you look to that and you write entropy radiator, then you will find fantastic it.
It's kind of the most sexiest cooling system for touring cars.
The entropy radiator.
The entropy radiator. Look at it.
Okay, okay.
You have a question or not?
No. Thank you, Tom.
Thank you.
