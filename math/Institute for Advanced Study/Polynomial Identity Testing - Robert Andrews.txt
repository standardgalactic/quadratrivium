Okay, so that's a great pleasure to introduce Robert Andrews and he'll talk about polynomial
identity testing.
Thank you for the introduction.
So I'm going to be speaking about polynomial identity testing, this is a problem in computer
science.
It's an important problem, it's one of my favorite problems and I hope that by the end
of this talk you like it as well and think that it's a nice problem to think about.
So to motivate what this problem is about, you know, I work in computational complexity
theory.
What is this area about?
You know, viewed from a thousand feet up, what we're trying to do is we're trying to
understand if I have two different computational resources, A and B, how do these resources
compare to one another?
So for example, if I can solve a problem quickly, can I solve it using a small amount of memory
how do sequential and parallel computation compare?
How do deterministic and randomized computation compare?
And often it's fruitful to study this question by instead looking at a somewhat related question
prime which is if I have some resource A, I have some problem X that I want to solve,
how much of A do I need to solve X?
And if you pick the right choice of problem, it turns out this second question will tell
you a lot about the answer to the first.
And so one thing that I'm interested in and will be the subject of this talk is what's
the role of randomness in computation?
Do we need, does randomness allow us to compute things more efficiently than we could deterministically?
And one interesting lens on this problem or on this question is through the problem of
polynomial identity testing.
So what is this problem?
So I give you some multivariate polynomial over the rational numbers and I ask you to
tell me is this polynomial the zero polynomial?
Now if I give you this polynomial as a list of, you know, manomials and coefficients and
this problem is stupidly easy.
You just check that all the coefficients are zero.
But if I give you the polynomial in a more succinct representation, then the problem
becomes more interesting and also more difficult.
So there's work of value from the 70s that shows, you know, whatever succinct representation
of a polynomial you might consider, you might as well consider the following one where I
give you the determinant of some symbolic matrix.
So here are these AIs, these are rational matrices and I multiply each AI by some fresh
variable Xi and I take their sum.
This is some matrix where every entry is a linear form in the variables and I want to
know is it's determinant the zero polynomial or is it non-zero?
Now how would you solve this?
Well, of course you could expand the determinant, you know, look at every single term, check
that they all cancel, but this is going to take you a ridiculous amount of time.
If there's something more efficient you can do, if you allow yourself the use of randomness.
In fact, the algorithm is so simple here it is, you're going to sample a random point
and evaluate this symbolic determinant at that point.
This sort of representation allows you to evaluate the polynomial easily because I just
plug in values for the X's and then I compute this determinant as some actual number.
Now I look at this random evaluation and I tell you that the polynomial is zero exactly
when I see a zero evaluation.
So why does this work?
Well, if the polynomial is zero you always see zero evaluations, that's easy, and if
the polynomial is not zero then it's very likely that this point I sampled is going
to lie outside the zero set of the polynomial.
Now if you were to sample say a random point from a unit cube then probability one you're
outside the zero set but we're doing this on a computer, we need to discretize things
so I'll just pick a random point from some integer grid and this algorithm as I wrote
it down will make errors about 10% of the time.
Now this is a fast algorithm, it's a simple algorithm but it uses a lot of randomness.
If you want to do this sampling you need to spend about n log n random bits to sample
one of these points.
Can you do better?
Can you find an efficient algorithm that uses less randomness terms?
That's the question that I'm interested in.
Now before I tell you something about reducing the amount of randomness here let me give
you an example application of this problem to really convince you there's something
going on here.
So the application I want to tell you about, it's a bit surprising if you've never seen
it before, it's an application to a problem in graph theory which is perfect matching
on bipartite graphs.
So I give you a bipartite graph so there are some vertices on the left, vertices on the
right and edges going between them and I want to know if there's a perfect matching in
this graph.
I want to find a set of edges that pairs up the vertices on the left with the vertices
on the right.
Now this is a well-studied problem, you learn about it in undergraduate algorithms, it's
rather easy to solve but these are all combinatorial algorithms that have been known for many years.
Let me show you an algebraic algorithm for this problem.
So we'll take this graph and you'll instead write down some symbolic matrix.
So here the ij entry is a fresh variable if there's an edge between the vertices i on
the left, j on the right and it's a zero otherwise.
Now why would you do this?
Well Edmunds observed that this graph G has a perfect matching exactly when the determinant
of this symbolic matrix is not the zero polynomial.
And why is that?
Well every term in the determinant when you expand it out would correspond to some matching
of the vertices in this graph on the left and because all of these terms in the determinant
are on disjoint variables you can never have any cancellations.
So for example this matching on the left corresponds to the monomial where you multiply
these three variables on.
This is great, we solved a problem that we already knew how to solve and we're solving
it in a randomized fashion which is maybe even worse we know deterministic algorithms
for this problem but in fact you can buy something with this.
You can design a parallel algorithm for this problem because what are you doing here?
You're forming some matrix, sampling some random numbers and then computing a determinant
and we know how to compute the determinant efficiently in parallel.
So this gives you a parallel algorithm for bipartite matching where all the other combinatorial
algorithms we know are very inherently sequential.
They proceed in steps where you find one new edge of the matching at a time.
So this seems very interesting and you know it would be a really interesting task to design
a deterministic parallel algorithm for this problem and there's been work done on this
and in some sense we're nearly there.
So that's one application of this problem.
You can solve problems in combinatorics by doing something in algebra which is I thought
this was very nice the first time I solved this.
So now let me tell you about actually reducing the amount of randomness you need to solve
this problem.
So just recall you know we saw an algorithm that solved this problem in the general case
using about n log n bits of randomness.
What would it take to use less randomness to solve this problem?
There's this really nice work by Khavenets and Paliatto from 2004 that in some sense
answers this question.
If you want to find an efficient algorithm for this problem that uses only n to the epsilon
random bits where you can think of epsilon as like a tenth, one hundredth, you know some
small number, you can do this if and only if you can prove some version of p not equal
to np for algebraic computation.
So in one sense our job of de-randomizing this problem is extremely easy.
We just have to prove that p is different from np.
Well what is p equals not np is a problem that's independent of for algebraic.
What is for algebraic computation?
So I want to compute the permanent polynomial.
I understand but is that necessary?
It's not equivalent if p is not np.
So it's a variation on p not equal np.
So it's different.
Under something like generalized Riemann hypothesis this is implied by the standard p not equal
np.
So it's true.
Okay so I mean you can think of you that this is just true but let's try to find a problem.
I'm just trying to understand the distinction.
Yeah yeah so this is something which is slightly weaker than the standard p versus np.
So on the one hand this tells us what to do on the other hand it tells us our job is very
very difficult that if we want to use less randomness to solve this problem we're kind
of stuck we have to solve a very hard problem.
But what's interesting about this is a similar phenomenon occurs even for special cases of
this problem.
So what do I mean by this?
On the left we have some question about de-randomization and on the right we have some question about
computational lower bounds.
And it turns out there's a connection between de-randomization and lower bounds even in
restricted settings.
So I want to tell you about one restricted setting where we actually know something about
this problem.
So the restricted setting I want to tell you about is the following.
So if you were to write down a polynomial as just a sum of monomials I could represent
it as sort of a depth-to-formula.
You have some summation at the top, you have a bunch of products in the middle and every
product corresponds to some monomial.
And you know maybe you can put some coefficients from your field on these edges to indicate
the coefficients of the polynomial, that's fine.
We understand the identity testing problem here very easily.
So let's add one more layer to this computation.
Instead of taking sums of products of the variables I want to look at sums of products
of linear combinations of the variables.
So already this is a more succinct representation of polynomials and this is something we don't
entirely understand yet.
So what can you say about this?
So in fact there was this really wonderful breakthrough work just a few years ago that
proved computational lower bounds for this kind of representation of polynomials.
So we know that if you were to try and write say the determinant as one of these depth-to-formulas
you just cannot efficiently represent the determinant this way.
You have to pay a lot to write the determinant as one of these formulas.
And as a corollary of that you can actually design efficient identity testing algorithms
when I represent a polynomial in this way where these algorithms are only going to use
something like n to the epsilon random bits.
So we actually have some progress here.
There are two algorithms for this.
One was given as a corollary of this lower bound by the Maya, Srinivasan and Tabana.
And then in some work later together with Michael Forbes we designed a different algorithm.
It also uses this lower bound in a crucial way but the algorithm is different and maybe
a little bit more explicit in its description.
So I want to tell you what this algorithm does or at least one step of this algorithm
and give you some idea why do lower bounds help for this devandemization test.
So here's an algorithm that actually surprisingly works.
So if you remember in the general case we sampled a random point and we evaluated at
that random point.
And here I'm going to think of this polynomial instead of a vector of n variables, let's
say it has a matrix of variables, square root n by square root n.
I'm going to sample a point which is some low rank matrix and then I'll evaluate at
that low rank matrix.
And because I'm sampling from a more structured set I can save on the amount of randomness
Now the epsilon here will depend on what exactly I mean by low rank.
Let's not worry about that.
The point is this is some efficient algorithm we're sampling from a more structured set
and we're saving on the randomness but why should this work?
Well let's think about this.
So if this algorithm were to fail what does that mean?
I have some small formula and every time I throw a low rank matrix at it it's bit spec
zero.
So really what it's doing is we have some formula that computes some identity of low
rank matrices.
And if you want you could replace the word low rank with singular and this is essentially
saying you know this formula is something like the determinant.
And in fact that's true even when I have the words low rank here we were able to show that
you can massage whatever this identity is into an honest to goodness representation of the
determinant.
You can get a small formula out of this that would compute the determinant but we already
saw we cannot do this.
And so what this is in some sense saying is you have this lower bound that these formulas
can't compute determinants efficiently so let's try to find a test that would require
them to compute the determinant in order to break this test.
Now these ideas I'm showing you them in some restricted setting but it turns out they're
useful even for the fully general setting of this problem.
So I want to tell you just a little bit about that and in order to do that I need to tell
you about one more problem from computer science but I think it's familiar to everyone.
Matrix multiplication.
So I don't need to define this but you know if I give you two n by n matrices I want you
to multiply them and how many arithmetic operations additions and multiplications do you need to
multiply these two matrices.
If you use the textbook definition you can multiply them using n cubed operations can
you do better.
Why would you want to do better first of all well it turns out this governs the complexity
of many tasks in linear algebra if you want to invert a matrix solve the linear system
of equations optimize a linear program all of these tasks you can do quickly if you can
multiply matrices quickly.
So what do we know about this there's a whole industry of designing matrix multiplication
algorithms started with Strassen in 1969 and in fact the current record is just a few months
old you can multiply n by n matrices using about n to the 2.371 operations and it's a
major open question do n squared operations suffice or do you actually need to pay a little
bit more to multiply matrices this is a major question and using some of the ideas from
the previous slide you can actually show a connection between the complexity of multiplying
matrices and this question of de-randomizing polynomial identity testing so the last thing
I'll tell you is if you could actually prove some kind of lower value for matrix multiplication
if you need n to the 2 plus epsilon operations then you can use this to design an efficient
algorithm for the general case of identity testing that's only going to use something
like n to the 1 minus delta random bits.
Just to conclude randomness computational lower rounds these are intimately connected
there's this wonderful problem of polynomial identity testing and I hope that I mean if
you have ideas about it I'd love to talk about it so that's it thank you.
Is it clear that n squared is sort of a boundary?
Yes.
You need to write n squared.
What is still to depend on, sorry.
There's some linear dependence.
So the implication that you ended with the left-hand side is like there isn't an algorithm
and the right-hand side is there is an algorithm.
Exactly.
Can you say something about the mechanism by which that?
Yes.
Yes, yes, yes.
So it's the same mechanism here where I'm going to give you some candidate algorithm
for this de-randomization test.
If it failed well you should think of that as there's some algorithm that broke my test.
There's some interesting algorithm that broke my test and then I'm going to do a lot of
massaging around of that to turn it into a matrix multiplication algorithm.
I don't know if that helps too.
Is the conversion, can you actually produce the thing with distinct systems?
So if you give me the algorithm that breaks my test, I can very explicitly give you a
matrix multiplication algorithm.
So it's a deterministic reduction.
Yes.
Any other questions?
Yep.
You said that the three-level evaluation of determinant is common to it efficiently.
What exactly do you mean by efficiently?
So if I want to compute an n by n determinant, I cannot write down a polynomial-sized step
three formula, so polynomial and n, that would compute that determinant.
Yeah, the comments, questions, questions, I'll be, perfect, all right, with that let's
thank the speaker.
