So I'm Elizabeth Mechis and I'm doing these talks on concentration phenomena and the compact
classical groups.
So let me just say before I start talking about all of those words and what they mean
that I'm really excited to be here and I was completely thrilled to be asked to come.
I've never been IAS before and I've certainly never gotten a chance to be involved in a
program like this and I think it's really exciting.
So I also kind of wish I'd known about it when I was younger although it was such a
snob that I probably would have just not come which would have been my loss because it seems
like it's a really great program so it's really exciting to be here.
Also I was thinking about what I wanted to talk about and I was trying to think where
I had learned this stuff that I wanted to tell you about and I actually couldn't figure
that out.
So I asked some other people where I learned this stuff.
Conveniently my PhD advisor is visiting the same place I'm visiting right now so I asked
him and he didn't know either.
So which is incidentally Toulouse I should probably tell you that so I am an associate
professor of mathematics at Case Western Reserve University but right now I'm on sabbatical
so I am living in Toulouse which is in the south of France which is definitely one of
the perks of academia as you get to go exciting places like that.
So anyway right now I'm in Toulouse which is a great environment and incidentally my
PhD advisor Percy Diaconis is also there so we've had lots of nice chats this year but
he was not able to help me as to how I learned stuff about compact classical matrix groups.
So I started writing up some notes thinking well I'll just make up some lecture notes
that I can distribute to everybody and they kind of got a little out of hand so there
are about 50 pages at this point so I definitely can't talk about all of it.
I'll email it to all of you so that you have just kind of a record of what we're saying.
So this is a really wide range of backgrounds and I know that so I'm going to try to just
kind of give you a flavor of things so for some of you probably everything I say is going
to be completely clear and easy and others of you it's not intended to be followable
every little detail just try to get kind of the big picture and I did write down a lot more
details in the notes so I'll email you those if you're interested in looking at the details
and then there are the review sessions as well.
So okay so we're going to talk about hard measure on the compact classical matrix groups so what
are those? So what is an orthogonal unitary symplectic matrix? I mean I want to tell you
what's a random matrix from one of these groups but before we do that let me just tell you what
the matrices are themselves partly to make sure we're all on the same page but partly because
I find that you know a lot of times we sort of introduce the formal kind of algebraic definitions
but to me these are very geometric objects and I want to think about them in terms of the geometry
so we have the orthogonal group which I guess I'll try to write this way that's supposed to be an
O not a Q sorry so this is the orthogonal group okay so this is N by N matrices let's call them U
so kind of for historical reasons I'll call them all U because all of these are unitary groups in
some sense although only the middle one I actually call that so we have the orthogonal group which
is N by N matrices U over R such that the following holds so U U transpose is U transpose U is the
identity so the transpose is the inverse okay fine I mean that's a perfectly legitimate definition
okay that's a matrix what does that mean to me so this is the way I like to think about it
is that the columns or the rows so note that this is invariant undertaking transpose okay so I can
think about either the matrix or transpose which I'll do all the time okay so if it's more convenient
to talk about columns more convenient to talk about rows I'll just do that okay because everything
here is invariant under transpose in the matrix okay the columns of U form an orthonormal basis
of Rn okay and that's how I really think about these objects I'm not that's not the only way
to think about them so this is one way is just to think about it in terms of okay it's perfectly
legitimate algebraic condition on the entries fine okay you can stop there this is another way so I
think if this is just encoding all of the possible orthonormal bases of Rn okay I can make the columns
of a matrix I can take you know the standard basis if I want I could make that those the columns of
a matrix and if I put them in order I'd have the identity matrix which definitely satisfies this
okay or if I put them in the wrong order okay then I would have a permutation matrix
which also satisfies this or I could have some other orthonormal basis and I could make those the
columns of my matrix and I'd have some other orthogonal matrix okay so so this is how I usually
think about these things it's not hard to check that this is equivalent to this right all you have
to do is write down what's the ijth entry of this okay the ijth entry of a matrix product right you
take the row from the one the column from the other and you're taking the dot product right
and this guy's being transposed so here I can think of the ijth entry as the dot product that the
i-th and jth rows of you here I can think of the ijth entry as the dot product of the i-th and jth
columns okay just so to say that that matrix is the identity is exactly to say that the dot product
is delta ijth okay so the columns are an orthonormal basis great that's getting into a more
geometric setting um also you might like to think that these are the matrices so these are
exactly the matrices which act as isometries on rn so we have uv dot uw is equal to v dot w
for all v and w and rn and this is also a way to think about orthogonal matrices is they are the
matrices that act as isometries okay so that makes me happier because now this is sounding
much more geometric um and it sounds much more like something I can think about this is just
kind of like okay I can write down a formula but I don't know how to think about it so this is the
orthogonal group the unitary group uh I'll probably forget to write the blackboard bolts but you
know okay so u u of n is the complex version okay so sometimes people call this the real unitary
group um it's analogous to this but over reels these are over complex so these are n by n complex
matrices such that u u star is u star u is the identity and this is the you star is the conjugate
transpose okay so this makes sense living over the complex numbers I don't just take the transpose
I take the conjugate transpose or if you like I take the adjoint of of this guy as a linear map
on cn okay so that's the unitary group and similarly um these the columns or the rows
are an orthonormal basis of cn okay with a complex inner product where I dot two vectors by
multiplying you know the ith component of one with the complex conjugate of the ith component
of the other and then adding them up okay so this is exactly analogous to this and then we have this
kind of thing as well okay so um good so that's the unitary group and then finally I have this
inflected group so in this this part of the world where people who are working on um random matrices
from these groups we kind of like for form sake we talk about this inflected group and there are
people who care a lot about this inflected group um it's just that in this area people usually
just say well yeah and then there's this inflected group and they kind of ignore it um so I don't
yeah I don't know why um so I can define it in a couple of different ways um one way is to say
so these are 2n by 2n matrices over c such that the following weird condition holds um
I have to call it u again uh so I get uj u star is u star j u is j where j is this matrix
so it's 2n by 2n and it's it's blocky okay so here's a zero block and then here's the
n by n identity here's a zero block and here's a minus the identity okay and if this condition
makes you happy that's great and if it doesn't I'm not really going to talk about it that much so
don't worry about it um so actually this object is more natural than this makes it look um would
at least this does not look natural to me um probably due to my upbringing um but so to
some people that looks very natural um to me it's it makes this whole thing sound much more natural
if I say the words um which turn out to be true that what this is is it's the octonian unicharic
group so if you take an n by n matrix and it's a matrix over the um sorry a quaternionic unicharic
group if you take um if you take an n by n matrix with quaternionic entries okay this is the unitary
group that is it satisfies this condition here you use star equals u star u equals the identity
where that time star has to be interpreted as the quaternionic complex conjugate so you change the
signs of the ij k's and you leave the the real part fixed um and then there's this whole business
that you can represent quaternions as two by two matrices ever see and um and if you take a look
at the notes I wrote up you'll see a little footnote saying um you should be able to grind it out and
see that this condition is the same as this but I started too and I got bored and I stopped okay
so apparently they're the same um many people have told me so like I said we don't really care
about this one that much but we just kind of do it for form sake because it's in there um and there
are some people who do serious stuff with that but I'm not one of them so sorry okay so anyway
these are the matrix groups that that I want to be talking about um and hopefully at least not
maybe not from the initial condition but hopefully by these kinds of statements
you're willing to to spot me for the moment that these are sort of natural and interesting
geometric objects okay so I'll I'll talk a little bit more later about you know why we would care
about trying to find a random one um but but for now let me just say a couple more things
about um geometrically just a couple of tiny things uh so one which is is not hard to see at all
is that if um so if u is in I'm going to switch notation sorry about this it's just easier to do
it this way because it's easier to write a script to you than a blackboard bold you um so if u is
in u of n then I should have done it the other word uh then the determinant of u is a modulus
one complex number um if u is orthogonal then well same statement uh determinant of u is
a modulus one real number so it's plus or minus one okay so um and I'm gonna stop talking about
this inflected group now so because of this something that we do sometimes is we talk about
the special versions of the orthogonal and unitary groups so we have s o n this is the special
orthogonal group
okay special because these are the matrices with determinant one so this is u and o of n such
that the determinant of u is equal to one so in this case it's half the group okay half the
group has determinant one half of it has determinant minus one okay sometimes we want to work with
those two so I may as well go introduce notation now so s o minus of n this one doesn't get talked
about as much but it's a useful technical tool um it doesn't really have a name um sometimes people
just call it the negative coset um so this is u in o n such that the determinant of u is equal
to minus one okay and so the orthogonal group is different from for example the unitary group
and it's disconnected it consists of these two separate components the special orthogonal group
which is a group by itself and the negative coset which is not a group okay this isn't closed
under multiplication because if you take two of them and you multiply it together you get
determinant one okay um but it's a coset of this group inside of that group okay so good
so that's the special orthogonal group and it's negative coset and then we have the special
unitary group which is the unitary matrices with determinant equal to one and so this is a very
difficult different situation because in the unitary group you can have any unit modulus
complex number be the determinant so you have the whole circle possibilities for the determinant
and here we're picking out just this one so we don't have this disconnected thing happening here
this this group is connected but this one's not okay i should have said at the beginning um that of
course you are definitely encouraged to stop me and interrupt me and definitely also encouraged to
stop me and slow me down if need be i know i talk really fast um but i've kind of given up on trying
to talk slower so i just hope that if i encourage people to interrupt me a lot that'll kind of
counteract it so please interrupt me if i uh if i say anything weird or or just confusing um so
okay with the definitions these are these are the the kinds of matrices i want to think about good
okay what else do i want to say about them um there are isometries good okay so um we're
going to talk a lot about these groups um so these are that one of the things that i love about
working on on random matrices from these groups is that there are so many different areas of
mathematics that are really important and if you want to really understand um matrices from these
groups and and random matrices from these groups it means that you have to know about representation
theory and you know have to know about romanian geometry and you have to know about you know
euclidean geometry and you have to know about all kinds of things so it's it's really beautiful the
way all of these ideas come together and um so i don't want to talk too much yet we'll get into
some of that more in a couple of days um but for now let me just say that all of these are compact
league groups um so that's fine they're compact league groups i do want to talk a little bit
about their metric structure okay so um we can think of let's first simplicity just talk about
the orthogonal group because i'm a simple minded person and real numbers are easier for me than
complex numbers um so let's think about the orthogonal group so the orthogonal group sits inside
r n squared right i mean i can think of that because it's a box of numbers it's n squared
numbers this isn't these are n by n matrices so i can think of them as sitting inside of
euclidean space okay it's not especially natural for me to think of a box of numbers as a little
arrow but i mean it is a set of n squared numbers so i can think of it inside of euclidean space
okay which is good because i know a lot about the geometry of euclidean space so hopefully it can
tell me about the geometry of this object okay so we can do a little bit better than just kind of
it feels a little unnatural to say oh well you know i can make it into a list of n squared numbers
by you know some ordering i mean yes i can um but usually we don't what we do is say if i made this
into a vector if i made an n by n matrix into a vector um and i took the dot product of two of them
then i would still be doing what we know you do when you take the dot product to be multiplying
corresponding entries and adding them up okay so the dot product of let's say u and v no matter
how i sort of try to realize this it's going to be the sum over all the entries so i and j go from
one to n of u i j v i j okay i take corresponding entries and i multiply them together and i add
them all up okay so this is the trace of u v transpose it's just a formula okay so what we
do is instead of trying to kind of think of this as a long pointy arrow um instead we just say well
o of n sits inside of euclidean matrix space so i'll call it m n of r this is the space of n
by n matrices over r and this space is a euclidean space in that it is endowed with an inner product
and the inner product is this one okay so i can just think of everything in matrixy language
and that's really what i want to be able to do so i have um u v so this is sometimes called the
holbert schmitt holbert schmitt inner product and it's just exactly what i wrote there so it's the
trace of u v transpose so there's no reason to write it again there it is okay so this is when i
talk about this group sitting inside of euclidean space i don't really mean r n squared i mean
m n of r the space of n by n matrices over r endowed with this inner product okay so i can
think of this then as a sub manifold of this particular euclidean space okay and because it's
a sub manifold of this euclidean space then you know i can think about its manifold structure
that it inherits from the euclidean space that it sits inside um so there are actually two ways to
do that so i should maybe i should well let me say there are two ways to do that so two metrics
on o n or u n or whatever um so one of them is what i can get by um just measuring distances
inside of euclidean space okay so let me draw a picture um i should really be talking about
the unitary group just because well it doesn't matter um so so let's let me just draw a picture
and this is a schematic picture or if you like it's a picture of u of one um inside of of euclidean
space i have this sub manifold okay which i will suggestively draw like that okay um now if i have
two points i have two ways to measure distances between them one way is i can measure the straight
line distance because the whole thing is sitting inside of euclidean space okay so i can perfectly
well just see how long this line is okay so that's the Hilbert Schmidt distance okay so one way is
the i can measure the Hilbert Schmidt distance between say a and b they sit inside of euclidean
space how do you measure distance you look at the norm okay of the difference
what's the norm okay it's the dot product of the thing with itself and the square root
so this is the square root of the trace of a minus b times a minus b transpose
good so that's one option okay another option is the so-called geodesic distance
okay and in your in my picture you know what it is it's arc length right
since i'm sitting inside of euclidean space that's one option as i can say i have to look at paths
from one point to another that lie within the sub manifold i'm working on okay so i have to look
at a path from a to b that lies inside of the orthogonal group and i have to measure its length
as a as a curve that sits inside of euclidean space so either way i'm using the euclidean
structure but i'm using it differently okay just like in the circle there are two ways to measure
the difference the distance between these two points either the straight line or the curved path
okay is there a question please seriously stop me if there's a question
no okay so i sort of snuck in there that this is a picture of u of one right so u of one
is one by one matrix c over c okay so numbers complex numbers and they are supposed to satisfy
the condition there u u star equals the identity okay if u is a one by one matrix that just means
u u bar equals the identity which means that the complex number has a modulus one okay so u of one
is the circle and so what about o of one
yeah say it out loud it's two points which two points plus and minus one yeah be brave come on
um yeah that's that's that's why we're all here is because we're going to be like unafraid to
shout things out and i'll make lots of mistakes just to make everybody more comfortable it's it's
tactical um okay so yeah u of one is two points it's plus one and minus one so it's disconnected
we already talked about that um and u of one is the circle okay so okay good so we have this
so i'll just call it the geodesic distance
okay so to compute it i mean it's potentially a little bit complicated right because what you
have to do to compute it is you're computing the length of an arc so you have to in principle
i mean of course we can do better than this but in principle what you have to do is look at all curves
within uh in this case o of n that go from a to b okay and you can measure their arc length as arcs
living inside of euclidean space you know how to do that you want how to do that in calculus and you
have to take tangent vectors stuff like that um and do integrals okay so we can do that and here
it's much easier to think about because here it's just arc length okay so in this picture we've got
this versus that so it's not really that important um which one you work with as you might start to
guess from that picture um they turn out to be about the same so on the unitary group for example
so i have the geodesic distance and the Hilbert Schmidt distance so which one's bigger
yeah the geodesic distance is bigger right why does it have to be bigger
sorry
other way but yeah yeah but yeah that's that's exactly right that's exactly the right issue
right i mean either way we're computing a distance and we can think of it as the end of our path
lengths okay and in the geodesic distance we're requiring the paths to lie within the sub manifold
and the Hilbert Schmidt distance we're not okay they're allowed to be anywhere in the ambient
space so for sure we have that the geodesic distance between a and b is bigger than the Hilbert
Schmidt distance between a and b and i actually make sure i get the constants right uh and the
Hilbert Schmidt distance pi over two
i'll put it over here because i have space
okay so that's the string of inequalities you have so they're really basically the same
so this you might have kind of guessed this from the picture because this is easy to see
if n is equal to one okay so if n is equal to one what you're doing is comparing straight line
distance versus arc line distance uh yeah yeah yes yeah that's what it is um just for historical
reasons we call it Hilbert Schmidt um to sort of remind ourselves that we're working in matrix
c space but yeah it's Euclidean distance that's right um so so what i was saying is that you can
guess from the picture that something like this might hold however you might have guessed that
the constant here depends on the dimension and it doesn't okay so that's an important point
because it means that when we're working with these two notions of distance it really doesn't
matter much which notion of distance we're using even if we're trying to think of really high
dimensional spaces which we're definitely going to be doing okay so good um so there's an there's
an exercise that you guys can talk about in the um the review session this afternoon about uh
approving those inequalities so there's steps okay good so um i think that's almost everything
yeah that's pretty much what i wanted to say about the matrices themselves see i'm just gonna
so okay this is random matrix program so let's get some randomness involved um so matrices so far
we have these nice geometric objects so various people have made the point especially in recent
years that anything worth thinking about is worth thinking about randomly um we're thinking about
a random version and actually i really believe that and and one of the reasons i believe that
is because um and this idea certainly isn't original to me uh one of the ways that it's nice
to try to understand a big complicated object like one of these groups is to think about what
is a typical element of it look like you know what can i say i mean i can make some statements
that are true for all orthogonal matrices all unitary matrices and that helps me get a handle on
what these objects are um like i said in the beginning you know i wrote down this algebraic
condition well that doesn't do much for me but when i start writing down conditions oh this
encodes orthonormal bases then i feel a little bit more better because more better oh my god um so i
feel a little better at that point because i i feel like i know a little bit more what it means um
but you know there's a limit to how much i'm going to be able to say that's true about all
orthogonal matrices on the other hand if i can put a probability measure that's natural and okay
that's philosophy at that point that if i can put a probability measure that i believe is natural
on the space and then i can start answering questions about what a random matrix distributed
according to that measure is like that at least makes me feel i'm a probabilist so i
can't help it but that makes me feel like i understand a little bit better what these
objects are like if i can say well pick one what what's it going to do okay so that's kind of the
philosophy uh there are many other reasons to care um formal reasons to care about what's a random
matrix from these groups but that's kind of it's the reason that i think even if you don't care
about the other motivations you should care just because you want to know you know what are they
usually like okay so what are they usually like um well what what should it mean to take a random
element of this group so this is really different from what yonah was talking about this morning
and what you talked about last week because then it's sort of a little bit you know you were able
to say okay i'm going to take this empty box and i'm going to start filling it with random variables
okay so just you know put in um if you want to make a wigner matrix then you just you know fill in
the top triangle with iid random variables and then you make it her mission and then you're happy
okay so that's pretty natural if somebody says how do you make a random matrix well i make a matrix
and i make the entries random that's that's reasonable okay but that doesn't work as well here
okay and the reason it doesn't work as well here is because we want the matrices to lie in these
groups okay and the condition for lying in this group it's a geometric condition it's a global
condition it's not something that is so naturally expressed in terms of the entries and in fact that
was my original problem with the first definition i gave you it's just a formula in terms of the
entries but it doesn't give me geometric insight as to what these objects are okay so what i want to
be able to do is say what should it mean to pick something at random okay by which i mean in a
natural way from these groups so you know the answer for you of one how do i pick a point at random
in you of one what does that mean sorry choose an angle at random what does that mean that's
exactly right but can you elaborate okay i needed distribution on zero to two pi not just some
distribution though right which one i can't take i can take something else should i take uniform
what does uniform mean
sorry no say that again okay that no that's a really that thank you for saying that because
that's exactly the kind of statement that i wanted to hear so yes we can put a distribution on
angle so we know what that means we put a probability distribution on the interval from zero to two pi
and then we just you know wrap that around and that gives us a distribution on the circle okay so
then the question is which distribution how do i know which one i should take okay i mean probably
most of you kind of believe that there really is a most natural distribution on the interval
but then the question is why i mean how do we know which is the right distribution okay it's
supposed to make every outcome equally likely well that's garbage i mean this is an uncountable
space you can't do that but but still i mean morally that's what it's doing okay and how
do we say it's making every point equally likely what should that mean okay because this is a
continuous space so we can't actually talk about the likelihood of individual points we can talk
about the likelihood of individual tiny intervals okay that makes sense so if it's supposed to make
the the outcome equally likely to be in each of these individual tiny intervals
what's that really saying it's saying that the measure that i want to put on the circle
should be translation invariant okay where by translating i mean rotating i mean translating
in the group okay because if it's rotation invariant then the measure only cares about how
long the arc is it doesn't care about where it is and then if you start making those really really
tiny that gets exactly at the idea that each point is equally likely right each tiny little
interval is equally likely okay so that's what we want so from here we want so uniform on the circle
means translation invariant
and i mean formally that really is true right i mean uniform on the circle is uniform on the
interval from zero to two pi and Lebesgue measure is characterized by being translation invariant
okay so that's really what we want is we want something that's translation invariant okay
so we're working on groups so we want the same thing here we want a probability distribution
on these groups so on un blah blah blah i actually i gave a talk last summer in which i
gave an introduction to these groups and i characterized um i talked about the unitary
group first and then i called the orthogonal group the unitary groups kids sister and this
inflected group was the weird uncle that nobody talks about um because that's that's really kind
we just never talked about this inflected group so okay so anyway oh i didn't you want um we
want a probability distribution on here which is translation invariant
okay and what should that mean at this point that is if u is random
and let me just say g n okay g n is one of o n or u n or whatever and m is fixed
in g n then what do i want i want the distribution of m times u to be the same
as the distribution of u and in the other order too so
okay so that's exactly analogous to over here when i said that i wanted whatever distribution
i put on the circle to be invariant under rotations of the circle okay that's how we
should think over here is i'm transforming the group it's acting on itself by multiplication
either on the left or the right and i want the measure to be invariant under that this
distribution is the same as that or if you like i have a measure that if i measure a subset
okay so if you want to think of it this way if i have a sitting inside of g n um i can define
m a to be m times u for u and a and then i want the probability of the set a to be the same
as the probability of the set m a okay i want to be able to take my subset of the orthogonal group
move it okay translate it by this fixed fixed matrix and have that not change the measure
okay so that's what we want that's what it should mean to take a if you like you can
insert the word uniform that's what it should mean to take a uniform random matrix from one of these
groups it should be that there should be a measure which is invariant under this okay and the good
news is that there was this very clever guy who figured out that that there is one and there's
only one so his name is par so we have a theorem which is due to par which is there is one and only one
translation
invariant probability measure on a compact league group
okay so that's horribly phrased and i hope nobody's going to give me a really hard time about the
the order i wrote things in yeah okay so yeah i so hard proof more than this here i'm only talking
about compact league groups so he proved more and sometimes you have to worry because being
invariant under left translation is not the same potentially as being invariant under right
translation for a compact league group they're equivalent okay so i'm i'm not venturing into the
the lee theory world of so apparently har announced at some point that he was going to do this you
know that we had these examples on on compact league groups where we knew this that there was this one
unique translation of variant measure and that he was going to prove that that that was the case on
some like huge class of groups and apparently people thought he was crazy um but he did it
so good for him and uh and that's very nice so so we have this theorem which i'm not going to say
anything about it all about where it comes from but we have this theorem that says that there is a
unique translation invariant probability measure on these groups that i'm interested in and that's
what i'm going to mean by saying a random unitary matrix a random orthogonal matrix
i mean one that's distributed so we say hard distributed okay or sometimes we say uniform
okay so any questions up until this point or am i talking too fast and you want me to say the last
10 minutes again they are compact league groups i mean i i'm sorry i don't really understand the
question oh thank you um i should have said that um so i'll talk more about that so i was kind of
trying to sweep that under the rug today because we'll talk about it more in a couple days um
so it's a group okay it's a group with matrix multiplication okay so these these objects are
closed i can multiply two matrices together and stay within the group satisfies group axioms
it's a manifold okay so it's a romanian manifold in fact okay so um and as a manifold it's compact
that's it okay thanks thanks for pointing that out yeah if i use terminology that's not familiar
please ask me um because otherwise i'll just merely go along talking about all kinds of
random weird things and so let me know um
no yeah it's the weird uncle right i mean
no yeah i mean honestly like i you know for form sake we kind of prove theorems sometimes but yeah
yeah it's i don't want to talk about it okay um yeah or no wait isn't it
isn't it wait a minute no isn't it compact it's compact yeah no i don't know why i yeah no it's
compact it's yeah it's it's the yeah i mean we should be able to see that it's compact because
it's just the unitary group for the the quaternions so yeah it's compact
yeah no um right i think that's not the same thing um i'm really not that familiar with the
terminology in that corner of the world um so this may not be i don't think it's the same as that
but yeah this one this one's compact and it has a hard measure on it so and a hard probability
measure so good um good okay so it is all right so um good well i've told you what random
orthogonal and unitary and sort of symplectic matrices are um so we're done right
so somebody should be yelling at me at this point and saying well you haven't actually
told us what they are right i mean i've told you that there exists a notion i've said that apparently
this black box tells us this black box named har uh tells us that there is one and only one
translation and variant probability measure on these spaces so i mean great but that doesn't
really do you any good if you want i mean i said that the philosophy or was we want to understand
these groups by knowing what a typical element is like and it's pretty hopeless if i have this like
abstract existence theorem okay so what i want to do for the rest of today is just give you a few
different ways to actually build a random matrix from one of these groups um so good
okay so um so how do you do it so i'll describe three different constructions and all of them turn
out to be hard measure and all of them look very different so i was going to start with the most
abstract but i think that's kind of not a great idea so i think instead i'll just start with my
favorite um so here's my favorite it's very hands-on if you're a mathematician um and you don't
actually really build things you just talk about like how you could in principle and then later
i'll talk about one that you can actually do on a computer um so so here's the first construction
um all right i said that the columns of you are supposed to form an orthonormal base of rn i'm
just going to talk about the orthogonal group just for simplicity but everything that i say is it
works in the unitary group two you just have to put bars places um okay so so the columns of you
are supposed to form an orthonormal basis so i'm supposed to construct a matrix whose columns are
orthonormal in rn all right well so i'm going to do it one at a time okay i'm going to start by pick
u1 to be uniform on the sphere living inside of rn okay so that you're supposed to believe
that i already know how to do and that you i mean okay it's the sphere so you have surface area
measure okay if you like you can think about it constructing it as thickening up Lebesgue
measure and then taking a limit um there are lots of ways to think about it but okay i have surface
area measure it's just arc length measure but the analog and higher dimensions so i pick u to be
uniform in sn minus one it's supposed to be random so this seems pretty natural so this is going to be
my first column okay so now i have to pick a second column all right and i know that it's supposed to
be orthogonal so it's a unit vector it's supposed to be orthogonal to this guy okay so i'm going to
pick it to be sort of as uniform as possible given that restriction it has to be so this guy we pick
u2 uniformly from the orthogonal complement of u1 in sn minus one so what does this notation mean
so this is all of the unit vectors in rn this is all of the vectors in rn which are perpendicular
to u1 okay so this here is a sphere of one smaller dimension okay so if you think about the picture
so imagine first that i am starting off and n is equal to three okay and i'm living in r3 okay so
i've got three dimensional space i've got the sphere okay so the first vector i pick is just a
random point on the sphere so it's a rotational invariant i've got a point okay now i think about
what's perpendicular to that point so i get a plane i'm sorry i'm terrible at drawing three
dimensional pictures um that's why i do high dimensional geometry you don't you just give up
on the pictures um so i've got this point got a plane okay it's perpendicular to it cuts through
the sphere gives me a circle right the circle is in a kind of random direction right because i chose
the normal randomly so i have a circle in a random direction that's okay it's still a circle
so i still know how to pick a uniform point on that circle okay that's you too all right now what
how do i fill in so in the three three by three case i'm building a three by three
matrix the first column is a random point on the sphere okay that's fixed i've got a plane
cuts through the sphere gives me a circle i pick a uniform random point on there okay how do i pick
the last one how do i pick the last column
which is what what if i have i have u1 and u2 and you're right exactly i have to take
something in the orthogonal complement of the span of u1 and u2 and it also has to lie on the
circle on the sphere the original sphere so what are my choices
sorry it i have one little bit of a choice
yeah direction exactly i have a choice i have basically the sign that's the only choice i have
left because at that point i've got a line and it's crossing through the sphere and so it hits it at
two end to little points and i have to pick one okay so for the last column there's not much choice
but there is a little bit of choice okay if i were constructing uniform measure on the special
orthogonal group i would have no choice because then that would determine which of those signs i
needed to take and if i wanted to construct something uniform on the negative coset also
i would have no choice because i'd have to take the other one okay but if i want it uniform in
the whole group then at that point that last last one i only have two choices left so that's how
you you continue and okay you do the same thing in higher dimensions you just have to do it more
times right so here we pick uniformly from here this is the same thing as sn minus two it's a sphere
of one dimension lower we have surface area measure we can choose something according to
the surface area measure there and then we continue so u three i do the same way i pick something
that's orthogonal to both of these lies on the sphere okay so i've got a sphere in one dimension
down and i keep going until i get all the way to the end and the last one i only have two choices
i have two antipodal points left to choose from and so i just choose each of them with equal
probability okay that's my construction so definitely i've built an orthogonal matrix
okay i built it to be orthogonal this is a unit vector this is a unit vector that's perpendicular
to this this is a unit vector that's perpendicular to each of these and so on it's an orthonormal
basis for sure okay the question is do i have harm measure have i constructed harm measure
okay and it's pretty natural to think that i should have because after all i mean i did things in
this sort of very invariant way i did things as symmetrically as possible given the restriction
that i wanted to come up with a an orthogonal matrix
so what do you do when you have a theorem that tells you that there's a unique thing with a
certain property if you want to check that you've built it you check that the thing that you've built
has that property and then you invoke the black box okay say thank you har and you're done okay
so so what we have to do is we have to take m and we have to multiply this matrix
by m and we have to see that the distribution is the same after it's not the same matrix okay we've
shifted it okay but we need to see that its distribution is the same well okay if i multiply
a matrix it's written this way with columns by a fixed matrix on the left that just multiplies the
columns by the matrix okay so if you're not used to this kind of manipulation you can work it out
in the the margin of your notes if you feel like it so okay so this is the thing and i want to know
is this equal okay wow too many decorations so i need it to be equal in distribution
question mark we hope two
okay is the distribution the same okay when i hit it that's what i want all right so i have
to think about the construction what if i do this all right so here u one i picked uniformly
in this sphere and then i multiplied it by m what's the distribution of m u one
yeah you have a hint because hopefully the theorem is true
yeah why
um i can you say that again i don't think i quite got it
yeah but we don't have that yet that's what we're trying to prove okay so we're hoping that that
turned out to be true and that's what i'm trying to check so first i'm checking at least works for
the first column okay so the first column over here is just uniform in the sphere okay first
column over here i pick something uniform in the sphere and then i multiply it by this fixed
orthogonal matrix so then that's a question for us is if i take something uniform in the sphere
and then i multiply by fixed orthogonal matrix what happens what's the distribution of the thing
after the fact you can just guess we hope it's the same yeah it's the same okay so it's just a
linear change of variables right i'm applying a map to our end the map is a linear isometry right
m multiplication by m is a linear isometry of our end so if you like calculus you can just
write out the change of variables and you'll see that the distribution is exactly the same
okay so maybe that wouldn't be a bad thing to talk about this afternoon
in the the review session because we're running short on time right now but okay so it's it's
really just calculus it's a change of variables that says if i make the change of variables y
equals mx and i write down integral formulas for the density sphere you'll see they're the same
okay so this is distributed the same way as this okay that's reassuring okay what about the next
one so the next column is m u2 okay so m u2 what does it do so u2 was picked uniformly
in the orthogonal complement of u1 the spherical part okay then we hit it with m okay so what does
that do it moves the orthogonal complement too okay so this by exactly the same change of variables
this is going to be uniform in the orthogonal complement of m u1 okay which means in other
words the second complement the second column is uniform in the orthogonal complement of the first
good so that's exactly what's true here okay so it's basically a calculus exercise that this
distribution is invariant under multiplication on the left multiplication on the right you would
think about the rows instead but like i said everything is invariant undertaking transpositions
so that's fine so we have constructed hard measure and this is my favorite construction
because it feels so like you know this is what you would do right i mean build me an orthogonal
matrix that's as symmetric and distribution as possible i mean this is this is what you would
do okay so i like this one if you're a riemannian geometer you laugh at me at this point any
riemannian geometers here are you laughing at me are you laughing at me yeah of course you are i
mean because this is totally stupid right it's totally trivial i've riemannian manifold okay i
think i'm not even going to write anything down about this i've riemannian manifold okay i have this
metric that on the riemannian manifold i can actually use either one of them
so that gives me a volume form okay and the metric is invariant under multiplication under
applying this a linear isometry okay if i multiply if i use multiplication by m as a linear isometry
on rn then this these metrics are invariant under that so the volume form that i get by just turning
the crank of riemannian geometry is invariant so so hard measure on in fact on something like this
it's just the volume form okay so if you're a riemannian geometer you can go now because we're
done i mean that's it it's just the volume form but i don't like that construction as as much i
mean it makes it obvious that it's really really natural but on the other hand i still feel like
it's not something i know how to compute with whereas this i know how to compute with because
this is built up from spheres okay and spheres are things that i feel like okay i mean general
abstract manifolds i don't know about that but spheres i can i can do that okay good so we have
a few minutes left so i can tell you about one more construction and this is probably the one
that everybody else would have told you first just so you know so that if you go out in the world
and say i learned how to construct a uh random orthogonal matrix if you want to sound you know
unusual tell them about this one but if you want to say to people what they're expecting to hear
you should say this one um so here's another thing that you can do is fill an n by n matrix
oh i should give this one a number uh so actually this is number three right number two is monian
geometry so there's another black box for today um three is the gaussian approach so you fill an
n by n matrix with iid standard gaussians okay so that's sounding a lot more like the approach
that you take when you talk about building wigner matrices or things like that um uh
wishart matrices any of those things we always talk about just like kind of filling in the
entries with some prescribed distribution so okay i start and i take a gaussian random matrix um so
it's not the goe because it's not symmetric okay i really fill in every single entry of the matrix
with iid standard gaussians so that's fine um this produces a measure which is invariant
under left and right multiplication it's exactly the same argument as the argument for why i erased
it but the linear the change of variables it's the same thing right because here i have a density
which is e to the minus sum xij squared with some constants places but let's ignore those for the
moment okay so this is what my density looks like with respect to the product measure of all of the
entries okay so if i multiply by m i'm just i'm rotating space i'm i'm making a linear
isometry of space um where y is mx and you can see that if i look instead at e to the minus
sum of m xij squared okay this is because m acts as an isometry it's equal to e to the minus
sum of xij squared and the change of variables the jacobian factor is one because it's an isometry
okay so it's just calculus again um that this guy is invariant under multiplication on the left
by um by a random or by a fixed orthogonal matrix okay i can change basis without changing this
measure if you like okay so what's the problem there's a problem it's not orthogonal right so
this is not a hard distributed random orthogonal matrix this is a Gaussian random matrix it's not
i mean with probability one it's not orthogonal okay so this isn't going to do me any good because
i haven't built har measure so i make it orthogonal well what do you do if you have a matrix and it's
living in r and it's just some any old matrix and you want to make it be orthogonal what do you do to
it you do gram schmidt okay we know what to do i mean great you guys know all the answers so i mean
that's what you do is you do gram schmidt so this gives me a matrix so i have a random matrix let's
say g and then the next thing so three continued is do gram schmidt
to g okay and so then what you have to worry about is when you do gram schmidt to g have you messed up
the the invariance under multiplication and the answer is no okay and if you think about it for a
second think about what you do when you do gram schmidt so let's say we have our first column
which is x one okay so we normalize it to make it a unit vector that's clearly okay right i mean
we're not going to have any trouble um that's still going to be invariant under multiplication
by um by m in fact that's what we already checked okay so what about the next one you take x two
i can never remember that computer science notation with the arrow but anyway we replace
x two with x two minus x one dot x two uh x one right with the component in the x one direction
okay i'm i'm not worrying about normalizations because that part i think it's pretty clear
that that's going to be okay um so this is what i do so then if i multiply this i hit this with m
on the left we would replace m x two with m of this so i would get m x two minus x one x two
m x one okay on the other hand if you think about so this is what happens if you do gram schmidt
on the first two columns and then you hit it with m what if you do m first okay then okay so
um so if we first multiply by m then we have m x one
m x two okay and now we do gram schmidt okay then we replace
m x two with m x two minus m x one m x two m x two and then we're happy because m is an isometry
and so this is equal to x one x two okay so they're the same formula okay so what that says
is that multiplication on the left by the fixed matrix m commutes with performing the
gram schmidt process and so then i'm okay because i know that i originally created
something that was invariant under multiplication by the left and now because commuting because
the gram schmidt process commutes with that multiplication i know that the result is still
invariant okay so so that's a completely different looking construction we fill a matrix with iid
gaussians and then we fix it to make it orthogonal we do gram schmidt versus this construction where
we just build it to be orthogonal from the get go but then we have to check and think a little
bit more about why it's invariant or you can just say i know romanian geometry you're wasting your
time okay so i see that i have 30 seconds left in which i was going to try to answer the following
question which has to make an appearance okay so somebody should have asked this much sooner
but i'm sure you are all much too polite to do that i mean you've been talking about different
random matrix models all week and i'm sure that everybody has has talked a lot about what the
motivations are and why you should care about all of these different random matrix models i can't
tell you why you should care about these models in 30 seconds so i'm going to try to convince you
over the course of the week and in particular so the fourth lecture on friday we'll talk about
what i think is a really fun application so this is this is not by any means the origin of this area
but you know you should believe me that since orthogonal matrices encode orthonormal bases of
rn if i understand what it means to talk about a random orthogonal matrix it means i understand
what it what it is to talk about a random orthonormal basis so in particular that allows me to talk in a
very natural way about a random projection or a random subspace i can talk about a random k-dimensional
subspace of rn okay this gives me a way to do that and it turns out that there are very interesting
properties of harm measure on the on the orthogonal group which turn out to translate
into these this huge area of randomized algorithms where people do they they build algorithms to do
something that's computationally infeasible by projecting whatever it is they're working with
the objects they're working with onto random subspaces i mean this is a huge area that's really
important right now in in real applications i mean people do algorithms doing this and it has
the whole reason it works is because of stuff that's true about harm measure on the orthogonal group
so i was going to say a lot more about the motivation but i think i'll just leave it at that and
i'll tell you a lot more about that on friday
