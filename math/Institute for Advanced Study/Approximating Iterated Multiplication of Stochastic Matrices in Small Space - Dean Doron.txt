Thanks, thanks a lot for inviting me. I'm happy to speak.
Yes, so this is going to work with Gil Cohen, Oris Berlo, and Amnon Tashmar, all from Tel Aviv University. Do you hear me okay by the way?
Okay, so if you have any questions, please scream at the screen or something because I can't see you, so hopefully I'll hear you. But yeah, feel free to stop me anytime and ask questions.
I'll try to go slowly, but in case there are any questions whatsoever, feel free to stop me.
Okay, good. So let's begin.
So what's our problem? A very natural one. So we're given A1 up to An, W by W matrices, and we want to compute or today we want to approximate the product A1 up to An.
So, yeah, it's obviously one of the most fundamental primitives, even just for the matrix powering case, where all AIs are the same.
But beyond the algorithmic implications, it's also a very important problem in complexity.
Because indeed we now know that we can get better understanding of space bounded computation through the lens of some canonical and algebraic problems, for example, IMM and other problems.
And what we will be interested in in the space complexity of approximating the product, or even matrix powering, or even A to the N.
Okay, so we have A1 up to An, each Ai is of dimension W.
Okay, so this is what we'll be interested in, and I'll try to motivate why.
First, let's set the ground and define the model of a space bounded computation.
So it's a standard model of a probabilistic space bounded computation, in which we have an input tape, but for those of you who are unfamiliar with this model know that we cannot replace the input with intermediate computation, it's just to read only input.
So we have our work tape, in which we do all our auxiliary computations, and we can use random coins, we can toss random coins, but know that if we want to save them, and to look back at them, we need to save them in our work tape.
So random coins tape is read only.
Okay, and of course, we have our output tape in case we want to compute a function, or just to accept or reject in case we want to decide a language.
And the quantity we're after is the space complexity, which is just the number of steps we used on the work tape.
So if we're talking about languages, we said that the language is in a language is in BP space F.
If there is a Turing machine randomized one that accepts it using space over F of n with with two sided error, say with error one third.
So we're trying to find our, our class BPL the class we're interested in is just BP space of login. Okay, what can we do with just very small amount of work tape of auxiliary memory.
Okay.
For research, what we would really want to do is eliminate a story but of course, we don't need to either accept or reject. We can also, of course, compute a function, for example, IMM is a function and we output it on the output tape space efficiently.
So our goal is to remove the need for random coins. And this is the corresponding deterministic space bounded class, which we call the space F of n, in case F is login, we just call it.
Good.
Is it okay.
Yes.
All right, cool.
So, indeed, the BPL versus L problem asks, can we de-randomize any space bounded algorithm with small space overhead. So can we take a space S randomized algorithm and turn it into a space S prime deterministic one, hopefully where S prime is only O of S.
It actually suffices to study the small memory regime, the login space complexity. And the question is whether BPL is equal to L.
And if you believe circuit lower bounds, quite possible circuit lower bounds actually, BPL is equal L. So we believe that the BPL is equal to L. It follows from circuit lower bounds.
But unlike the time bounded regime, the BPP versus B problem, here we do have explicit results, which I'll mention right now actually.
Okay.
So we had significant progress, as I said, getting explicit results.
So what do we know.
So RL, which is the one-sided version of BPL, is already in the space log squared n, which I'll just call L squared. This just followed from Savage.
And it's not hard to extend Savage and improve the BPL is in L squared.
Okay, so the Seminole Nistan's PRG from the 90s, give us a way to de-randomize BPL, simultaneously in space log squared n and polynomial time, which is the previous result.
It doesn't follow from the previous result.
A few years later, cleverly building on Nistan's work, Sexton Joe proved that BPL is in L to the three halves, and both these results I'll heavily rely on in this talk, but you don't need to know them in advance, so I'll cover what was needed.
Recently, using new machinery from what we call the low-air regime, William Hosa managed to shave off square root log log n, facto, so we now know that BPL is in L to the three halves, the minus the low one.
So we already broke the three halves barrier.
And there were other important milestones. For example, Nistan and Suckerman considered the case where you want to de-randomize algorithms that toss very few coins.
So for example, if you know your algorithm only tosses polylog and random coins, you can fully de-randomize it.
In fact, Lea to Nistan Invictus on game PRG with the similar parameters to Nissan, which turned out to be very versatile and important in future works that consider it some other models.
And also, of course, Ryan goes famous on the recordistic connectivity in low space.
You can also view it as a de-randomization result. And we also have nearly optimal results for more restricted classes of computation, for example, some subclasses of branching programs and other computation models.
Okay, and if you're interested, William has a great survey. I think it's called recent advances in space boundary randomization, I think.
So, yeah, if you're interested, it's a great, it's a great manuscript.
Okay, now let's switch back to the problem of IMM and then later we'll see how they both connect.
So, what about approximating IMM, sorry, of course, computing IMM exactly, not approximate, but computing.
What can you say.
So, you can do repeated squaring and do that in space log n times log nw.
So, when n and w are polynomially related, this means that this problem is in L squared.
Now, I'll mention like a nice fact, which I'm not sure it's so well known, that you can actually get log squared w plus log n, which is better when n is very large, using a Cayley Hamilton based algorithm,
not a repeated squaring based algorithm, but it only works when all the AIs are the same.
It's just a nice fact, you know, it's an easy algorithm, I can give it to you offline if you want.
Okay, but this is computing exactly.
What about approximation?
Can we do better?
So, the game is essentially between the optimal L and L squared, this is our game.
But if you want to be, for those of you who want to be a little bit more precise, it's actually between L and the class called debt, or which is subclass of nc2, which is the surface of L squared.
So, we know how to put it in debt, but roughly the game is between L and L squared.
And for general operators, for general real matrices, nothing better is known than just repeated squaring.
And the question is, okay, let's try restricting our operators.
And I think it's a very, it's a fabulous fact that doing approximation in algebra tells us a lot about space bounded computation in the different models.
For example, if you want to approximate linear algebra for symmetric operators, this is roughly captured by the class BQL, which is the quantum analog of PPL, which I will not talk about today.
If you want to do approximate linear algebra on stochastic operators, this is roughly captured by PPL, the class which we will be interested in today.
And if you want to do double stochastic operators, a recent line of research tells us that it's already in almost L, so it's like log n times dog log n.
And our focus today is on arbitrary stochastic matrices.
So why is it important, even just metric sparring when N and W are related.
It's already complete for PPL, in what sense.
And we can convert any, sorry, any probabilistic space boundary machine into some stochastic operator A, such that the probability to move from any state S to T is just the metric sparring A to the N in ST.
That's easy if you kind of have some familiarity with the model.
And on the other hand, we can approximate random walks by just estimating the probability by taking multiple random walks and the corresponding Markov chain.
So essentially the randomizing PPL in O of S space is like approximating metric sparring deterministically in space O of S.
Okay, this is one of the reasons it's, it's very interesting to study.
Okay, so what's done. Again, we want to approximate IMM for stochastic operators, and we want to distinguish between N, the number of matrices and W that they mentioned of each matrix.
So if you do iterated powering and you truncate whenever necessary, you can do roughly log N log W.
Neeson's generator and also I and W gives us log N times log NW but actually in a more black box manner, which I will mention soon.
So it gives specific algorithm, which we can actually utilize it's very specific and nice properties to improve, to improve on things, I'll show it in a bit.
And Joe gives us log to the three F's and which is the term that dominates when N is roughly W. But in general, if you want to distinguish between N and W, this is log N to the power of three halves, plus square root log N times log W.
Okay.
And, and, sorry, when, when all AIs are the same, and we want to distinguish between N and W, what is the corresponding de-randomization problem.
It's de-randomizing two-sided algorithms that use log W space and bits of randomness.
Okay, this is how we should correspond W and N to the space and number of random bits of the algorithm.
Okay, that makes sense.
Okay.
So, as a result, we solve IMM in space, nearly log N, plus square root log N log W.
So, we managed to shave this three to the half's power.
Okay, from the first term.
So, when W is not so large, let's call it the medium W regime, or looking at from the other direction, when you have many matrices compared to the dimension of each matrix.
We can get the newly optimal O till the log N.
And just in the lens of de-randomization, if we consider a space, a randomized algorithm uses space S, and those are random coins.
The general setting is when R is two to the S, right, we're allowed for the algorithm to run into the S time and it can toss to the S coins.
The Nissen-Suckerman result tells us that for R, which is poly S, we can simulate the algorithm determined in space to O of S.
That's what I mentioned, the result that talks about tossing few coins.
Our result is the other extreme, where the algorithm tosses many coins, like two to the S to the C for some C greater than one.
And if you want to compare ours to sex and Joe, we get space S to the C over two plus one versus S to the three C over two.
Okay, so this is the de-randomization result.
So just to recap before moving on to the techniques, what do we have in our landscape?
So if W is stay constant or very, very small, actually, IMM approximation is almost trivial, you can do it with nearly log N space.
So there, the question is actually a black box challenge of constructing a PRG. And I'll mention it towards the end of my lecture.
But right now, just so that the constant W is not an algorithmic challenge, it's a pseudo-randomness challenge.
For the medium W, let's say two to this square root log N or this regime, this is our result.
When W and N are polynomial related, we have this extra result and we have positive results.
And the extreme large W is the Nissan Sucker on the results, which I mentioned a couple of slides ago.
Okay, so I'm going to move on and talk about the actual algorithm and the components that we're using.
So if there are any questions about the result themselves, let me know.
So we use, let's say three components, two of them are from the 90s. Maybe the third one is also from the 90s, but that's the way to get there.
So we have two classical results.
The first one is the Nissan's generator, but I'm going to view it, not as a PRG, but as a randomized matrix powering algorithm.
Soon, elaborate on it quite a bit.
And we're going to use Sexton Joe's framework, which uses this generator together with a shift and truncate mechanism in order to recycle randomness.
Okay, but you don't need to know any of this because I'll define them later on.
The second one is more recent in the space bounded community is the preconditioned Richardson iteration, which is just gradient descent, essentially, and it's, it's, it's used as an iterative method for solving linear systems in optimization
and time bounded works.
But it is, it has recently appeared in space bounded works and already found many applications.
And there, we use it also in in today's work.
And we use it as a way to obtain high precision approximation of matrix powers and matrix inferences from mild approximations.
Okay, so to kind of get started with the algorithm.
Let's start and actually this will be most of our talk.
In the case where all the eyes are the same because it already has many challenges. I mean, yeah, I guess the main challenge already appears here.
Okay, so it needs some generate though as I said we can view it as a randomized matrix powering algorithm, which is very space efficient one.
But it needs kind of a lot of random beats in the sense that those random beats are what we call offline randomness.
We need repeated access to them we cannot just toss them and forget about them. We need repeated access to them.
So in terms of parameters.
What's the distance algorithm the way we will view it today.
It gets the matrix.
Some seed or some string or some, some random string age.
And then, and it approximates a to the end.
The epsilon will be our accuracy parameter and delta, we call it a confidence parameter.
It tells us that with probability at least one minus delta over the ages.
If you apply an instant with this age with this good age, you will get an excellent approximation of a to the end.
Now, and the two quantities to keep in mind is the space, which is good. It's like logarithmic in the parameters want.
But the length of age.
You should think about it as like log squared.
S is the number of beats.
We will present a matrix in. So it's like s times log n plus log squared, at least.
Okay.
So what do sex and Joe tell us.
You know, to raise a to the power of R and R is to the scroll to log n for scroll to log n times.
So a to the R and then to the R and then to the R, etc.
Swirl to log n times.
This will give you a to the end.
Of course, the question is, okay, you're applying the sun repeatedly, but with the with, but with which ages, which seed do you use.
So of course, you can do independent HIV.
But I mean, you're better yet just applying Nissan with to approximate a to the end.
It makes no sense. It's too costly.
What about staying the same age across all iterations.
Now, a priori, it doesn't need to work.
And it's not like a union bound issue. It's an adaptivity issue.
So, suppose age.
There are many ages, which are good for a, but once you computed Nissan age, your result depends on age.
So it's dimmingly possible that future iterations will fail miserably.
Okay, we have an adaptivity issue issue here.
But you should keep in mind that a good age, like a good output of Nissan should not heavily depend on age, because if you're good.
You're actually close to a fixed value, right, which does not depend on your internal randomness.
And the question is, how can we, there is, of course, there can be some dependencies, and how can we get rid of them.
And the cool mechanism, which is now used in many other works is the shift and truncate.
So, it's a very general paradigm, and it makes a lot of sense.
So, bear with me. Let's, let's discuss it more generally.
We have an algorithm that takes an input x and some randomness y, and it approximates some value f of x to within high accuracy, let's call it epsilon.
And let's say f of x is, as you see 1.5123.
Run the algorithm for like a couple of randomness tricks and get like two good invocations.
One gives you. Okay, so you see here, one with why one and why with why two.
So both of these results are good.
But they also depend on the internal randomness.
So, given the output, I might learn something on why.
But what happens here, if I take my value and truncate it to a higher error say epsilon prime, say epsilon prime corresponds to truncating up for two digits.
What happens now.
Same value right.
We obfuscated the dependence on the seed.
But is it always the case that after we truncate, we no longer depend on the seed.
So is it.
No, thanks.
Right because we can be close to the border right so for example things with this stuff of x.
Here again to successful invocations, and you can truncate and see that one falls from the left of the border one falls from the right of the border, and it's not the same value.
So what you can do is use the random shift.
So what sex and Joe do is they choose some, some is data.
Shift, the return value with high probability over the new randomness zeta.
We're not, we're not close to a to a boundary. And now we can truncate.
So with high probability over zeta, for most wise, we get a value that does not depend on why.
And the value itself is epsilon prime close to f of x.
Okay, so it's simple and powerful.
And again it enables them to eliminate dependencies between consecutive applications of Nissan's algorithm.
Okay.
So what's the, so what's the sex and Joe's algorithm now we're ready to to give an outline.
So we pick a seedful Nissan and square with log n independent shifts.
And for square with log n iterations. Oh, sorry, each, they take each shift to be all log n bits.
And then we compute Nissan.
And we ask it to trace to the to the square with log n power with some one over n squared accuracy.
Shift and truncate, say to one of ran accuracy.
And repeat.
It's a randomized algorithm. So to do randomize it we just average over all the ages and all the age and status.
So to improve upon this algorithm. Let's understand what's the balance. So what's the barriers.
The space complexity comprises to two factors or three factors actually.
The first one is the course of the shift. So we have square with log n shifts, each cost log n bits. This gives you already look to the three have sent.
And this term is both the length of age, the seed for Nissan.
And so the space, it takes to compute Nissan at each iteration.
Okay, so we have quite a lot of factors that gets us to log three, three over two and, and, and we need to take care of them all right.
So, the first bottleneck is the magnitude of each shift is one of our polly and sex and Joe tells us okay let's use all again beats for each shift.
Actually, it's not a real bottleneck, I mean it's bottleneck but it's not hard to see that in contrast to the magnitude of each shift, which needs to have n in it.
The amount of randomness doesn't so it can be independent of n.
And we use roughly log W random beats per shift. In other words, we use roughly one over W, sorry, we use roughly W shifts.
So overall, this requires a seed length of square with log n times log W.
So we already can clear one factor of the space complexity.
But the, the more severe bottleneck is that the Nissan generator is costly. If either of the two things, the, the following two things happen.
If the matrix is represented using many bits of precision, or if we require high accuracy from the Nissan from Nissan's generator.
So, what do sex and Joe do, they work with one of our polly and accuracy already bad, and all log n bits of precision.
Okay, so this gives us the other log one and a half and factor in the space complexity.
And the thing is, we do need this accuracy.
Because if you take a stochastic matrix, which epsilon approximates some other, and you raise it both the power of n, you do get the factor and blow up in the, in the accuracy, unless you
are always around it but not, not in this general scenario. So we do need to maintain accuracy of one over N.
The question is what, what do we do.
And the strategy, which, which seems, which might seem strange at first is before we apply any son.
We purposely decrease the precision of the input by truncating its entries to a precision of only one over W.
And recall that we're in the regime where W is much smaller than N.
So, when we apply the same generator iteratively, we only do it with accuracy with a mild accuracy guarantee of one over W.
Okay.
So what do I mean by pranketing.
I'm going to incorporate it into our algorithm. So, I just want to remind you, these are the standard Nissan parameters.
When we want to raise a to the power of R.
And you see that we have a factor of s times log R, where s is the number of beats taken to represent the matrix.
So I just want to say, what if we truncate to T beats of precision.
Okay, of course, we have an additional error on top of epsilon.
But the seed length, the length of H becomes smaller. It's only T times log R, which, which will be the dominant factor. This will give us the log W times square root log N.
So we're good in terms of parameters, but it shouldn't work right. The accuracy is too bad.
Okay, but just in terms of parameters were fine.
So let's see if there is a way to keep the parameters intact but improve the improve the algorithm and its analysis.
And as I said, I'll just say it again that mild approximation is not enough.
We need somehow to restore the high precision approximation of one over N. This is, yeah, this is one of our N exclamation mark not one over N factorial.
So here enters rituals on iteration to the picture, and we view rituals on iteration as an L reduction procedure.
So here's the, here's a statement.
If we have a the original a and our matrices a one up to a R.
It's such that each AI, mildly approximates a to the I.
So note that it's crucial we get the original matrix and approximation to its powers.
And a very simple space efficient algorithm, I can show you offline it's very simple.
The takes all these bunch of matrices and outputs.
Much better approximation for the earth power.
To the minus K, and the space is double algorithmic in the accuracy.
So if you want our times to the minus K, you need space log K times log R w.
It's very dependent.
And let's see if we can incorporate it into our iterative scheme.
I just want to say a few words about the literature iteration first is that, as I said it's very space efficient.
And in our case, this, the space complexity is only log W plus code log N.
So if you multiply it by square root log N because this is the number of iterations we do.
It will exactly match what we need this log N plus square root log N log W.
Unfortunately, we cannot in general kickstart the process if we have a constant error say now that I did need one of our error in order to kickstart the process.
There are some works in some very specific settings, which do get something better but in general you do need one of our.
And you need access to the original a and to all its intermediate approximation of all its intermediate powers.
Okay, so what do we suggest.
So each mild Nissan invocation.
Let's retrieve the higher precision using the literature iteration.
And although we decrease the precision before using Nissan.
I'm not really losing it, because we, we need to devise it in such a way that will still have access to the untrunkated matrix.
We need to make sure that we're at a space bounded setting, so we never write down entire matrices. We only have very, very limited number of auxiliary bits, so this should always, this should be done space efficiently.
The literature, the literature to iteration combines the untrunkated matrix and my approximation of its powers in order to get the high precision approximation of the earth power.
So in just, just as a very high level outline.
How does our algorithm look like for powering me first.
So, we start with our initial matrix day.
We truncate it.
Okay, to, to decrease the, the number of bits.
Each entry has to log w bits.
We apply Nissan.
But with accuracy one of her w.
And note what we have. We have Nissan but we also have our original a so we can plug them both to throw Richard's on and get some are zero.
We, we use Richardson to accuracy stay one of rain squared.
We have to shift and truncate because we need still we still need to seemingly be coupled to dependency.
And that's a truncate Nissan.
Richard son shift and truncate.
Okay, so we have the truncation to decrease error.
And we have the shift and truncate to remove dependencies.
And so on and so forth.
For scroll plugin times.
So this, so it has this alternating nature where we switch between mild approximation of one over w and high precision approximation of one over N.
And the nice thing about it, which is pretty surprising is that you can actually maintain one of her and accuracy without using log N space per iteration, which is kind of unique in this area of study.
Before I before I move on.
Let's put it on the slide and let me know if it, if it makes sense.
Okay, so now we have different ai's.
So, the first thing to try is actually, well, do the same.
Instead of approximating powers, just approximate products, like chop a one of the way and, like do a tree structure, I'll have a sketch in the slide.
Do the same thing.
And seemingly, again, after scrolled log N iterations, the entire iterated iterated product is approximated.
So here just do the same.
You know, at first for powering all these intermediate computations at each level were the same.
Okay, but now they're distinct. Each node represents some Nissan invocation so do Nissan shift on Kate, and the charts on.
And just do the same.
So we will follow this tree structure, but there are quite a few issues. Let's just briefly discuss them.
The first one, which is the major one, only, I mean, I'll just mentioned them briefly is is working with the shift.
So, remember that our algorithm for matrix powering.
We invested log w beats per shift.
And we had growth log and iterations, and it was fine.
And at each level, right at each level here.
We had the same matrix.
So it makes it made sense to use the same shift.
But now we have about N intermediate matrices.
So we cannot afford independent shifts and we cannot afford the union bounding over all subsequences.
The union bounding needs to be done, something a bit radical.
And the idea is, instead of shifting intermediate matrices, like instead of incorporating shifts within the algorithm.
Just forget it, just if the input matrices, and that's it, and use the same shift rolling with matrices.
So just shift the input.
Okay.
I'm sorry, I didn't hear you. I didn't hear it.
Okay, okay. Okay, sorry, sorry. Okay, got it.
Okay, so we shift once.
And it's only like it's the best you can hope for it's only log n bits overall.
The analysis is obviously more delicate and we need to study what happens when we just shift the input.
We need to keep track of the way the matrices and the overall evolve evolve and actually,
if you think about it for, for some time.
You see that this can't work but you need to do like a little bit extra, and we actually end up introducing a second which is on iteration.
Now for the purpose of handling dependencies, rather than improving the accuracy.
But let's not dwell upon that because, as you soon see there is an even more elegant solution.
So this was the shifts problem which is with the like the major one.
We have more minor things to overcome. For example, we need to improve Nissan's generator, because we need to take the confidence parameter smaller than what we needed it to be in the matrix power algorithm.
And, and there is actually, if you didn't know about it, and you're interested. It's actually nice to know there is to improve confidence parameters in PRG can use a sample, it's called a sample taken we now call it the money sample
or also the money victors on generator.
And but we will employ it here.
And we also need to do better space complexity analysis, but this is minor compared to the shift thing.
So I do want to tell you a different way of doing distinct ai's due to Louis pattern, due to Louis pattern and Ted pine, very recently.
So based on our page exploring algorithm.
They handle the IMM case concurrently and independently using the following beautiful approach, very elegant.
And they proved that most applications of Nissan's algorithm will be good, even without shifting.
And they did, and they proved it by by showing that this is pure G can also fall a related like an error indicator read one spreadsheet program, which is a technique we often use.
The error blew up so they also need to keep using virtual iteration.
So, so the nice thing is the, the observation that multiple shifts are too expensive for IMM, if you want to be sex and Joe.
And they proved it by doing one initial shift and a reduction.
And they said, well, you don't need any shifts, you just do a reduction.
Okay, so questions up until here, where am I on time. Okay, so I have five minutes.
To complete a picture.
There is the black box challenge, which I haven't discussed and just want to put it on the table and saying it's still a challenge.
So the challenge is to construct PRG is for read one spreadsheet programs, which is the non uniform model that correspond to space on the computation.
You won't even define.
Yeah, just of course, I mean, if you know it, then you know that characters for length and with w is what we need to approximate IMM.
But I'll just give what a period she is by saying, which algorithm does it induce for IMM.
So here is a way, a very bad way to compute IMM exactly.
Okay, this is a terrible way to compute IMM exactly.
This is not how you would do it.
If you only wanted to approximate IMM.
The pseudo randomness challenge, the black box challenge asks you to construct a specification of w to the n minus one.
So come up with curly with the set curly D.
The first set of all the walks that you see here.
One D should work for all a, a, all the AIs, all the stochastic matrices.
And this is exactly how you should do the, the approximation algorithm.
Okay, this is the black box challenge for, for IMM.
The quantity we after how much does it cost in terms of space.
Assuming we can generate the efficiently, this is just log the size of the, this is the quantity we're after.
And, as I told you before when W is constant, and we're only looking for any algorithm, then it's easy you can just do it at powering.
If you want to PRG, like this specific algorithm, then only recently, we know how to do it for W equals three.
This is to make a wrangled in tall.
And for W equals four, we have nothing better than Nissan.
Okay, so we call the state of the RPGs fall short, compared to algorithms.
I'd say okay, but I'm interested in algorithms.
But historically, even if you, if you're only interested in algorithms, PRGs often help.
So one open problem is make this text show based constructions.
Holy black box in their few ways to interpret this sentence but in general.
And you can feed it back and have direct applications for BPL.
And I'll just mention that the pattern and pine paper show that for medium W's.
You can get a sex Joe type result, even without shifts and without a reduction.
Okay, the only non black box component is some canonicalization step in essence generator in essence algorithm which I haven't explicitly said what it is.
But it's even if you kind of forget about the shifts and forget about the reduction.
It's still not black box.
So, one concrete open problem from this stock is, is this.
Okay, so yeah, thanks for listening.
So is it easy to describe this richard some aggression.
Yeah, yeah, sure. If you want, if you just want the algorithm I can give it to you in a minute. I'm not sure you want to.
So yeah, seems like magical.
Yeah, yeah, yeah, but okay so let me show it to you and then do I have like a couple of minutes or do you want to do it off. I mean, okay so let me share again.
Yeah, so I'll show you Richard some forum.
Oh, sorry.
Let's let's unhide.
Okay, so I'll show you how to do to do Richard some iteration for reducing the errors in the matrix powering.
So say you take some stochastic matrix a.
And I know I minus day is not invertible but but forget about it as soon that it is.
And just, you know, it doesn't even matter. So, you see, put a put a matrix a embedded in the off diagonal.
So here we have this path tensile with with a.
Okay.
And it's not hard to see that if you take the inverse of I minus this.
You get on the left and on the bottom left most place, you have like the largest power.
Now, of course, we don't have that right want to approximate it.
We want to start from a mild approximation, say epsilon zero and get an approximation say epsilon zero to the power of K.
It's a very simple thing. So, here you see okay right.
And what's the algorithm, just do the sum where L is the original matrix where.
So this is so L is just I minus this.
So it's I'm so it's I on the diagonal and minus a on the off diagonal.
L inverse tilde is just you replace each through power ring with an approximation of the power ring.
So if you see the top matrix, just replace each through power with the approximation of the power and compute this with some.
And like if I had 15 more minutes, I can, I will be able to prove it and also kind of show you how how it's actually gone and got in this end.
And you can actually also have an example.
No, sorry, but.
Yeah, yeah, so here is what I said that you replace the true powers with the approximations.
So, so yeah, it's just just basically a degree K polynomial in the approximations and in the tool matrix.
This is just it. It's a load degree polynomial of degree K. And the input to this polynomial is your mild approximations and the original matrix.
That's it.
It's a degree K very explicit degree K polynomial.
What is the scale of the algorithm that you mentioned that.
I don't know.
Yeah, sure.
No, no, it's in Hebrew we say I came prepared.
Let me share again.
So this is for computing powering. Exactly.
So you compute the characteristic polynomial, which takes log square W space.
You compute X and mod px as a polynomial.
And then just compute this polynomial just evaluate a on this polynomial.
And you can just, I mean, it's polynomial division.
As you can see, and then by Kylie Hamilton, this is a of N.
Yeah, I don't think it was that well known. But as you see it's very straightforward.
Any more questions.
Again.
