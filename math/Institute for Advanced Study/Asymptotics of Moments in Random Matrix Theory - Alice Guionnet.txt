Okay, so just before starting, I wanted to do a short story of why people got interested
into random matrices, and this goes back, I forgot how these boards are tough.
So the story started with V-Short, so John V-Short, so in about 1928, so he was a statistician,
but working apparently in the Department of Agriculture, and he was interested in trying
to analyze data, and so his question was the following, so imagine that you observe some
vector, x, x, so it would be m-dimensional, x1, xm, I don't know, you can imagine it can
be some sort of disease in some species of animals or whatever, and you observe it several
times, xn, alright, and you try to infer if there is correlation between what you observed.
So for instance, if this kind of animals get this kind of disease, more often will they
get another one that you are also measuring and so on and so forth.
And so the way to try to understand correlation is to look at the spectrum of the matrix,
so you look at this matrix x, so big x, which will be x1, xn, so it's m by n matrix, and
what you see is that the dimension, so the correlation between the data that you are
measuring can be understood in the spectrum, so for instance, imagine that you can write
x as lambda times uv transpose, so this means that this is lambda, and then you have v u1
um v, so v is going to be n by n, and so this means that you multiply just each vector
in each row by one, coordinate of u1, and then um, alright, so what you see then is
that, so if you have this case which corresponds to the fact that you have only one eigenvalue
which is non-zero, all your observations will be proportional, so they will be highly correlated.
And more generally what you want to understand is can you find a few eigenvalues which will
be done zero or which will be large and the other will be small, so that somehow you understand
that the data will concentrate onto this vector space, so this will show that this will allow
you to, so imagine that in general you try to find r, so that you can write something
like this, and the rest is small, so then you will try to understand that the data that
you are observing, they are all living in the smaller subset, so they are correlations,
so that's the idea, and in fact so in general considering eigenvalues of non-ermition matrices
is kind of complicated, so what you do is you consider what is called a covariance matrix,
so it's x times its transpose, but you can see that if the fact that x has low dimension
is equivalent to the fact that this guy has a small dimension, so in this case so x transpose
would just belong to u transpose, okay, so if in the case here, and so the question is
how can you study the eigenvalues of this guy and find somehow how many eigenvalues
big enough you have, and what are the dimensions of the data, okay, so more generally find
lambda i plus something small, okay, and so these are the eigenvalues, so it's lambda
i square eigenvalues of x, x star, so you see that somehow you want to understand this,
what are the large eigenvalues of this kind of covariance matrices, so far there was absolutely
no randomness, okay, so now you have matrices, so this matrices can be large because you
can imagine that you have many observations of a very big data, so actually nowadays people
are more and more considering big sets of data, so you have this question with very
large matrices, and so still it's not random, but you can introduce randomness by saying
so what I'm trying to understand is the correlation of the data, so I can try to understand what
kind of eigenvalues I will have if I would imagine that my entries are for instance independent,
okay, because I would like for instance to try to understand if my data is made of independent
events, so, but then how should look the spectrum of a matrix where I have independent data,
right, so this was, so if the spectrum, if the entries is independent, how should look
the eigenvalues, because then you can say well I observe this kind of eigenvalues, it
really looks like something independent or it doesn't, or maybe I can retrieve the correlation
between the data from the observation of this if I assume some kind of form of the data,
some kind of correlation, all right, so this was V-shot question, and then actually there
was not much interest into random matrices till the 50s, so in the 50s it was Wigner,
so in 56 I think, so Wigner is a Nobel Prize in physics, and he was interested in completely
different kind of problem, and the type of problem he was interested in was the quantum
mechanics of particles, and in particular now of what is called heavy nucleus, and what seemed
to happen is that they were observing this nucleus, and somehow the analysis they were
trying to do based on previous studies of simpler systems was not at all matching the
observations of this nucleus, so, and somehow the analysis that they were doing was based
on a simpler one kind of body interaction particle, and what Wigner said is that this
cannot explain what we observe, and this is because what happens in this nucleus is very
complicated, you have lots of particles, and somehow what we cannot approximate that by
simpler system, but what we can do is try to model this physical system by something
very random, okay, and the idea is what he said is that let's model our system by something
as random as possible within what we know, okay, so often very often randomness is used
to model what we don't know, okay, that's a general principle that random variables
just hide somehow what everything we don't know on a model, okay, it's kind of the uncertainty,
so it was the same thing, so Wigner said well in quantum mechanics everything is described
by what is called the Hamiltonian, so the Hamiltonian is something which is an operator,
and it's complicated, but you can imagine that you can eventually approximate it by a
matrix, so approximate by a matrix, so you have to imagine that the Hamiltonian is usually
acting on the continuous space, but here you can approximate it by a discrete thing by
approximating by a matrix, and then the idea is, so this matrix has to be large because
it's approximating an Hamiltonian, and then the idea is to try to model this matrix in
a random fashion within everything you know about this matrix, okay, so for instance what
he said is that for some system what I know is that, so this should be self-adjoint, okay,
so what this means is that, so this is a matrix H ij, ij in between 1 and n, and you know that
H ij is, so the conjugate of xji, so sometimes the entries are just real, in which case you
can forget this conjugate, and if they are complex, so this does a complex conjugate,
alright, so, and maybe actually we don't know anything more about this matrix, so what then
your beginner will say is that let's choose these entries independent, choose entries independently,
and they can be like Gaussian, so I think it was taking Gaussian variables, independent,
and just you have the symmetric constraint, and then, so the idea is just, so now the spectrum
of this matrix is going to be important because in quantum mechanics, the eigenvalues of the
Hamiltonian represents the energies of the system, so this is the thing you can measure,
and so the idea is, so let's try this model, we look at the eigenvalues, and we try to compare
with what we can see in the experiments, and then this will allow us to say whether the
model is fine or not, and it turns out that this model worked for some examples, and then
what another physicist said, so Dyson, what Dyson said was that, well, if ever, anyway,
your model is not working, so the observation of the eigenvalues is not giving you what you want,
it's maybe that there is something that you didn't know, and some symmetry or some relation
between the entries that you didn't know, and you have to try to find it, and you try to change
your model until somehow your observation will match the results given by the matrices. Okay,
so in both cases what you can see that random matrices appear like a big complex array of data,
randomness is coming in both cases from the fact that we ignore more precise information on the
model, so we try to model it by random things within everything we know, okay, and then we try to
match it with observation, okay, and then so in both cases the question is, so how can we
understand the spectrum of matrices when they're very big size, okay, so and so this is the subject
how to study the spectrum of matrices random with size going to infinity
and it's not an easy question, because if you think about it, if you are given a model,
let's say you have a matrix which is a mission with independent entries, so the spectrum is not
an easy function of the entries, so how can you solve this question, and actually so it took
quite a long time to analyze this question, so now in the last 10 or 20 years there was lots of
progress in the understanding of the spectrum of this type of matrices that I'm going to describe,
but we will see that there are really different kind of questions, so the question that I will
address is about the macroscopic kind of properties of the spectrum, and what Catherine will
consider is more what I would call a microscopic, so analyze macroscopic properties
so namely what I want to say with that is that I would like to know that like the collective
behavior, so for instance if I look at some interval AB in R, can I compute the number of
again values which will fall in this interval, okay, and here A and B will be of size one,
okay, can I say, can I say like, can I study the fluctuations, so fluctuations
of this guy, so this is macroscopic because you just try to understand the common behavior,
and what Catherine will consider is a question of which is more local, which is for instance let's
look at the largest again value, so only one, so it's that's why I call it microscopic because
you look at only one, so you really zoom on one again value, and what is the behavior,
so similar question would be if you look at two nearest again values, how far are they,
what is the gap between two again values, so this would be also microscopic, all right,
questions, so I think that Alyssa introduced yesterday one of the central results about
this macroscopic analysis, and the central result is to show what is called Wigner's theorem,
so it was actually proved by Wigner in the 50s, so Wigner considered matrices as I said which
show, so n by n which show a mission, it's n by n matrix, then you have to specify a bit
more about the model, so what is specified was that the expectation of xij is 0, so this is
not really important, but if you want to have a well-defined limit you should also specify
so the covariance, let me say that this is 1, and so for Wigner's theorem you have a technical
condition which is that for all ij you have all moments, so this means that this is integrable
on all lp, so I have a collection of random variables, and so they will be independent,
so what I can say that, so let me write it like this, so if I look at the fact that xij
belongs to some set, let's say aij, and I look at the intersection, so I take i smaller
or equal to j because of the symmetry, so this should be just a product measure, so this
is independent, and I specify some properties on the other, so this should be let's say
I call it mu, so I will assume for simplicity that outside of the diagonals they all have
the same law, and on the diagonal maybe because if this is complex I want to have real entries
on the diagonals, so I take another probability measure, so this defines for me, I will put
it afterwards, they will be the square root of n, don't worry, I mean that's always a
question whether you renormalize your law at this point or after you just renormalize
along the i, that's about the same, so the question of Alisa is that if you look at matrices
like this they will have very big eigenvalues, it will be of order square root of n, and
so we need to divide these eigenvalues by square root of n to make sense of a limit like this
if a and b are of order one, so I could also have here decided that the covariance is one
over n and do something like this, but I think it's clear to put the normalization afterwards,
okay, so here this defines a joint law for the eigenvalues of my matrix,
and so this property is that the integral of x d mu x is 0, the integral of x square d mu x is 1,
and you have all moments which are finite
and the same for nu,
okay, and now I put all this random variable into my big matrix, is it clear?
so that's okay, so maybe I should have just done the real case, but so in the case where
you have complex entries, so if you want this property to be verified, so you see that
if i is equal to j this implies that you should have a real entries on the diagonal,
so in this case you have to choose probability measure mu on the diagonal
and which is on the real line and a probability measure eventually on the complex plane
mu outside the diagonal, so that's why usually and in fact so if you look later at the proof
I don't really need to take all of them equal, if I would have these two properties
and they would be chosen to hold uniformly on ij, here these bonds would be uniform,
then I would get the same limit, okay, so in fact the asymptotic, so that's a big
point in random matrices is that many properties of the spectrum are universal in the sense that
they do not depend on the precise definition of the matrix, so here is the case, I give you
as an idea that you have the same probability measures, but in fact you only really need that
and in fact you need even less, I mean we could also suppress this and we could also suppress
that in some cases, so in fact to get the same limit for this kind of number of eigenvalues in a
set, you need very little, but you need to have enough integrability, you need to have
a finite covariance, this is very important, okay, so then
for what Wigner is saying
for what Wigner is saying is that, so if you look at the number of eigenvalues, so here I
normalize by i, so I fix a and b in r, so the eigenvalues of emission matrices are always real,
so I know that it makes sense to look at them only on the real line, so if I normalize this,
so what he says is that this converges as n goes to infinity towards, so the measure,
the semicircular measure of a, b, where a, b, where sigma is one of x smaller equal to four,
I didn't say yet in which sense, times four minus x square dx divided by two pi,
so here the convergence, so you have to think that this is a random quantity,
and the convergence holds in many ways, so it holds, so in expectation,
so this means that if we take the expectation here, we will have convergence, it's also in probability,
so this means that the probability that this is far from this, this is at some distance
from this is going to zero when it goes to infinity, so we will see these two things,
and it holds also what we call almost surely,
so this means on a set with probability one,
oops sorry, yeah sorry, actually I was thinking I don't remember if it's one over pi or one over
two pi, that's terrible, and then I got confused about the four, I think, yeah, okay, so this is,
this is Wigner's theorem which tells you somehow how the eigenvalues are distributed,
and so what I would like to do now is to prove this theorem when you take convergence in expectation,
is there any question before I start to give you the argument how to prove that?
No?
Yeah, so okay, so what I will prove now,
so the convergence in expectation means that,
so if you take the expectation, so this means the integral under your law,
the number of eigenvalues, so this is just, so this is just some random variable with values in
zero one, because you have at most n variable, so the expectation of this converges as n goes to
infinity towards, so sigma of AB, which is the integral over AB of the square root
minus two two of four minus x square dx, so it's over two pi, okay, so this means,
oops, I forgot the, yeah, so that's the trick, when you don't normalize at the beginning,
you are likely to forget the one over square root on the way, okay, so this is,
so this is the first theorem which was really proven as a matter of convergence
in random matrices, and so the way that actually Wigner proved it, and this is also the way that
we will follow, is by computing moments, so the idea, and so we will think that Alissa will
discuss that, otherwise I will do it next time, so the idea is that if you want to get convergence,
so you can see that this is a special case of another convergence, which is that if you take
f, let's say a bounded continuous, so then,
so you could ask whether if you take f, a bounded continuous function, this is going
to converge to the integral of f of x, sigma dx, okay, so the relation between these two things
is just you take f of x, which is one of x in AB,
right, so this is not bounded continuous, but the idea is that somehow because a bounded
continuous function, or you can approximate this function by nice sequences of bounded
continuous function, in fact if you can prove this for all AB or this for all bounded continuous
function, this will be the same, okay, so we will detail that in further discussion if you want,
and then
and then you can go even further in this kind of reasoning and think, well you just need to
prove this kind of convergence for sufficiently many functions f, and in fact
what is pretty useful in random matrix theory is to prove it for what is called moments,
okay, so you take functions which are not anymore bounded,
but they are moments,
and you want to prove this as n goes to infinity for all k,
so again you have apparently changed a bit the problem on the way, but you see you go from
a function which is just one and zero,
so then you go towards some kind of smooth functions,
and then so I don't know how to draw xk, so it's kind of,
some kind of thing like this when k is even,
and the point is again that this will be enough to prove this convergence for these
moments, because if you you can then get the convergence for all polynomial functions,
and you can approximate any function f which is continuous by polynomial functions,
and then you can go to this kind of step functions,
so
so this is supposed to be a representation, so this is one,
yeah, yeah, so this is just a step function, so this is this case,
then you kind of modify it, so you could imagine you can approximate
any function like this, so and then you can, so there is some lines of mathematics to write,
but I think you can at least understand the idea now, and we can detail that maybe in the exercise
session, so then you can modify it to get continuous function, and then because polynomials
can be used to approximate continuous function on compacts,
then this is enough to prove that, and this is actually what Wigner proved,
so how are you going to prove that, because of course why do you want to change all this
into this question, the point is that your hypothesis concerns the entries of the matrix,
so you want somehow to express things at some point in terms of the entries of the matrix,
and then so there is this nice result of spectral theory which is that, so the trace of
a matrix xk is just the sum of the eigenvalues to the k,
okay, here the trace, so the trace of a matrix A is just the sum of its diagonal elements,
and so this is a use of the spectral theorem that Alitha showed yesterday,
all right, so on y, yes, why are you happy, it's that x to the k, you know to compute the entries
of x to the k in terms of the entries of x, so a k, so if I look at the entry ij of a to the k,
so it's just going to be the sum over ai i1 aikj minus 1, where your sum over i1 ik minus 1,
from 1 to n, so this is a formula for computing moments of matrices,
okay, so if you look at these two things, so this quantity I am trying to estimate,
so what I was trying to estimate,
lambda i to the k,
so I can write it as, oops, sorry, I forgot the square root of n,
I think Alitha was right, should have included at the beginning,
okay, so then this is just 1 over n, so I can put the square root of n outside,
so it's 1 plus k over 2, then the trace of x to the k,
and now I can write everything in terms of the entries,
so I just write the trace on the moment formula, so it's sum over i1 ik, x i1 i2,
x i2 i3, I continue until x ik i1,
so here is a trick, by looking at moments, you could express everything in terms of the entries,
and now you know the law of these gentle entries, so you have some hope to estimate this moment,
all right, and so actually we are not going to prove this directly,
what we are going to prove is that this converge
towards some numbers that Alitha described yesterday, which are the catalon numbers,
by exactly using this formula, so the statement, the Wigner's theorem,
is that for all k, so the expectation of 1 over n, the sum of lambda i over square root n to the k,
so this converges towards, so 0 if k is odd,
okay, so this means somehow you can see, in fact, the point is that the lambda i,
their law, you expect them to go towards some symmetric measure around the origin,
so you expect odd moments to vanish, and otherwise ck over 2, where ck is a catalon number,
so the catalon numbers were introduced by Alitha
yesterday,
So Ck makes 2k factorial divided by k factorial k plus 1
factorial.
So what Alisa will show is that this is, in fact,
so if you take this semicircular law,
this is, in fact, this moment, x2k of the semicircular law.
She will show that today.
But, in fact, this catalogue numbers
of many other descriptions, there
appear a lot in combinatorial problems.
And the way we will see them showing up
is by the fact that they count trees.
So I will explain what this means.
So there are a certain number of combinatorial objects.
So there are not the number of rooted trees with k edges.
But I should specify something.
So I will specify actually everything.
But these cages, which are embedded into the plan,
are embedded, or you can sync drone.
So what does this mean?
So a tree, or maybe I should start by defining graph.
So a graph is just a set of vertices and edges.
So v is like I1, Ik.
And the edges is a collection of I, g, k, Igl for some choices
of these indices.
OK, so, namely, what you are with.
So you are given points for this I1, I2, I3, I4, that's I5.
And you decide that you are going to put some edge
in between some vertices.
So this will be your collections.
So something, for instance, like this.
So a graph is connected.
So if you have a pass from any point to any other,
inside the edges, so for all i, j in v,
there exists I1, Ik, so that i equal I1, I2, Ik equal to j,
where Il, Il plus 1 is an edge.
So this forbids, for instance, to have suddenly
two vertices like this.
And an edge is over there.
And so it is a tree if, so there is no cycle.
It's a connected graph with no cycle.
So a cycle is, if you can go from one point to itself,
by taking edges in the graph, which are different.
So a cycle is, so if you have a path.
So that ij, ij plus 1 are in E, are different.
So typically in this example, I don't have a cycle
because I can never come back.
But if I would have this extra edge,
I could take this series of points and get a cycle.
So now the question is, how are we
going to count the trees?
Because of course, there are lots of symmetries
that you can think about.
And the way that we're going to count the trees
is first we're going to give them a root.
So this is a favorite distinguish edge.
So for instance, this one, which is oriented.
And then the other thing that we want to do
is to specify how we count symmetries.
So for instance, what I want to say is that, so imagine I
have this tree, will I count this tree and this tree
to be the same or will they be different?
So imagine I have a root.
So on what I want to say, I want to say that this is not
the same.
So I don't count the symmetries of this rotation.
And I do that by saying that it is embedded into the plane.
So this means that I specify the way
I actually draw it in the plane.
So this is equivalent.
So this means that some of the plane has some orientation.
And I can give myself an exploration pass
onto this tree.
Well, I will follow the orientation of the plan.
And I will just start from the root.
And I will explore my tree following this exploration pass.
So this is equivalent to say that I am given an exploration
pass on the tree.
And this exploration pass, so it's
a pass which starts from the root.
And it will cover, oh, I have two.
And then I have h, so g1 and so on.
And then I return to the root.
So it's a pass.
So when I speak about a pass, it's
always that each step is an edge of the graph.
And so this exploration pass will have two k-steps.
And we'll visit all vertices.
And we'll return to the first point.
And so it gives me a unique embedding into the plan.
So somehow, when I'm counting the trees, I am counting, in fact,
the number of trees on the number of exploration
pass of these trees.
So now the question is how to relate our, of course, I erased it,
but how do we do that?
How to relate our, of course, I erased it,
but how to relate our quantities with the Catalan numbers.
So remember that so what we want to consider is this guy.
And I want to show that this guy is going to converge
to this Catalan number.
So this enumeration problems of enumerating these trees.
And so my main tool is going to be this spectral formula
that we just described, which is 1 over n plus, so k over 2,
coming from here, and the sum over x i1 i2 x i k i1.
All right.
So the idea here is that somehow, if you want to look,
so you can expand that, so you can take the expectation inside.
And then you have the expectation of x i1 i2 x i k i1.
So this is what I will denote t i1 i k.
So the idea is that I want to estimate that.
So this is what I will denote t i1 i k.
So I want to estimate that.
And I know that all these guys are bounded,
because I know that my entries are in LP.
So they're all finite by some constant,
and depending on n.
And so what I want to understand
is the main contribution of this quantity.
And so what you see that you have to understand,
what are the indices so that this guy is non-zero,
and what is the main contribution?
What are the indices which will contribute to the lending order?
And so how can we get some kind of idea of graphs here?
Well, the idea is that you can associate to i1 i k.
So these indices, you can associate a graph g.
So let me call this i.
So a graph g, which show is given by a set of vertices
and a set of edges.
So what are the vi?
So the vi are just this set of points.
Where here, when you have two i's which are equal,
you just merge these points, or just equal.
Where e are e g's which are equal or merge.
And then you have a set of edges, which is just i1 i2 i k i1.
OK, so being given your indices, you can just.
So for instance, so let's see i1 is equal to 1, i2 is equal to 2,
i3 is equal to 1, i4 is equal to 2, 3.
So let's say something like this.
So I start from i1, then I go to i2, which is different from i1.
So I have this edge, i1, i2.
Then I go to i3, so this is i3, which is equal.
Then I go to i4, which is something different.
And then I go back to i1.
OK, so in this case, I have three different vertices and four edges,
because I have degree 4.
So what we will see tomorrow is that for the main contribution,
so if you want ti i1 i k to be non-zero,
then the main contribution is such that for this graph, for gi,
so you can think about is a tree and e defines an exploration pass.
So it's a rooted tree, where e defines an exploration pass on the tree.
OK, so the idea is that somehow trees are the graphs,
which allow to have the most different indices,
because somehow the main contribution will come when you have as many
i's which are different, because the indices varies from 1 to n.
So you want the main contribution will be given by the indices where you can
choose as many indices different as possible.
Because this has to be non-zero, we will see that this, we cannot take t,
t cannot be non-zero, for instance, when all the indices are different.
When all indices are different, this will be zero.
But we will see that somehow the most number, the biggest number of indices that we can have,
which are different, is 1 plus k over 2.
And then these indices have to be given by this structure of embedded trees.
And this will allow us to get the convergence of the moments.
So I will do that tomorrow. And do you have any questions?
No? OK, so I will go into how to do this analysis tomorrow.
And then we will investigate central limit theorems, which are always kind of,
so how do you fluctuate around this kind of limits?
And we will see some generalizations. So that's the plan of my course.
And this will all use these kind of ideas that when you compute moments,
somehow moments are going to be given by numbers. A bit like moments of Gaussian
are given in terms of an uncrossing partitions, moments of eigenvalues are going to be given by
some kind of number of graphs. And this generalized to many other fittings.
So that's the idea that somehow to estimate this kind of properties of the matrices,
you can express everything into combinatorial problems by going through this kind of formulas.
And by looking at the main contribution of indices. OK, so if there is no question,
thank you for your attention and see you tomorrow.
