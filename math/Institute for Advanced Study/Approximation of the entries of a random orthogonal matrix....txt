I'm interested in random matrices, in particular random orthogonal matrices.
I figured I'd start by giving all the necessary background, state some classical results and
then get into some more recent results and maybe hopefully prove some things.
So first thing let's start out and just recall what we mean by an orthogonal matrix.
So an n by n matrix u over r is orthogonal if u transpose is equal to u transpose u
is the n by n identity matrix.
And then the set of all such matrices, so the set of all bigger, yeah, the set of all
n by n orthogonal matrices over r is the orthogonal group.
And we denote this with a bold o of n to indicate the dimension.
So a couple facts about the orthogonal group that'll come in handy for us.
So the first fact is the columns of a matrix u and the orthogonal group form an orthonormal
basis for r n. Secondly, if we have an orthogonal matrix, it acts as a linear isometry on r
n.
So that is, again, if u is in the orthogonal group, then the inner product of u v with
u w is just the inner product of v with w for all v w and r n.
And lastly, the orthogonal group itself is a compact v group.
So here, that is, it's a group as well as a compact manifold.
Okay, so here's our group of orthogonal matrices.
We'd like to know what does it mean if we take a random element from this group of matrices.
And so here's what we would expect.
So if u is a uniform random element from our group
and we fix another matrix m in the orthogonal group, then we'd like the following equality
and distribution.
If we multiply on the left by m, it has the same distribution as multiplying on the right
by m, and that has the same distribution as the original matrix itself.
And so what this says, the distribution of our random matrix u is a translation invariant
probability measure.
So here, invariant measure, same u.
And so if I have, again, a fixed matrix in the orthogonal group, and for any subset or
the orthogonal group, mu applied to ma should be mu applied to am, should be mu applied
to a.
And now there's only one possible measure that does this.
So this is a theorem.
There is a unique translation invariant probability measure on our orthogonal group.
And we call this heart measure.
Yeah.
So this is what I expect.
This is what I want to happen.
So I'm saying this is what we'd like a uniform random element to do.
Exactly, exactly.
This is what I expect.
And in fact, the measure that does it is heart measure.
Yep.
So when I'm taking a uniform random element, I'm exactly choosing an orthogonal matrix
that's distributed according to heart measure.
And so that's what it means when I'm taking a random element from the group.
And so I'd like to describe what heart measure is.
So we understand that better.
And so I'm going to go through two constructions.
There's many.
So let's construct heart measure.
So the first construction I'll use is a column by column approach.
That is I'm going to start with an empty matrix and fill it in a single column at a time.
The way we do this is you choose your first column, u1, uniformly from the unit sphere
Sn minus 1 contained in Rn.
So the norm of u1 is 1.
Then to choose the second column, choose u2 uniformly according to surface area measure
on the u1 perp intersect the unit sphere.
So this is the set of all x in Rn so that x has norm 1.
And if I take the inner product of x with u1, it should be 0.
That gives me my second column.
I continue this process of choosing my columns uniformly according to surface area measure
from the unit sphere of all vectors orthogonal to each of the previously chosen ones.
And so you can see how at the end of this process, if I continue, I'll end up with a
matrix u that has these columns.
u is orthogonal by construction so that's good.
In order to show I have harm measure, I just need to see that its distribution is translation
and variant.
So I need to check that when I multiply by a matrix m in the orthogonal group, I'm not
changing anything.
So here, again, I'll fix a matrix m in the orthogonal group.
Then mu is just m times this matrix that's been constructed.
And this has columns mu1 up to m, un.
And the way I can think about mu is the first column mu1 is then constructed by choosing
u1 uniformly from the sphere as before, then multiplying by m.
Now by my second fact over here, m is in the orthogonal group so it acts as a linear isometry
so it's going to preserve surface area measure.
And if I look at the inner product of the second column mu2 with mu1, again because
it's a linear isometry, it's just the inner product of u2 with u1.
And that was zero from the previous construction.
So again, the columns are orthogonal.
And you can see if I continued this argument, I would again have a matrix with the same
distribution.
So I have the translation and variance I needed and then the harm measure.
So one last thing to note here is I only handled multiplication on the left.
That's where the compact league group comes in because in a compact league group, left
translation and variance implies right translation and variance and vice versa.
So I just need to check that multiplication on one side worked.
So this construction, what I did is I took an orthogonal matrix and then showed its
distribution with translation and variance.
I'd like to also do a construction where I start with a matrix that I know has a translation
and variance distribution and then orthogonalize it instead.
So I'll need a definition first.
I'm going to define the Frobenius norm.
So if I take a and b n by n matrices over r, then the Hilbert Schmidt inner product,
this is given by take the inner product sub hs and this is equal to the trace of a v transpose.
And so then the induced norm, so then the norm on a.
So the square root of the Hilbert Schmidt inner product of a with itself, this is the
Frobenius norm or sometimes the shot into norm.
So let's do that second construction of our measure.
This is going to be a Gauss-Gram-Schmidt approach.
So the idea here, again I take an empty matrix and this time I'm going to fill it with IID
standard Gaussian random variables.
So I'll fill an n by n matrix x with IID independent identically distributed standard
Gaussian.
And I'll denote these as a set xij.
Then it's pretty fast to check that this matrix has a translation and variance distribution.
So we know the density function for this x.
The density x with respect to the product dxij, this is 1 over 2 pi to the n squared
over 2 x minus the Frobenius norm squared on x over 2.
And so now I'll check that if I multiply by a matrix m, so if I take y to be m times
x, the distribution is unchanged.
So here then the joint density of the entries of y is given by absolute value of the determinant
m inverse over 2 pi to the n squared over 2 x minus the Frobenius norm squared on m
inverse y over 2.
Because m is an orthogonal matrix, the determinant of m inverse is just 1.
And again because it acts as a linear isometry, it doesn't change the Hilbert Schmidt norm
when I multiply by m inverse.
So this is exactly the same as the density of the entries of y itself.
But I don't yet have harm measure because this matrix is not orthogonal.
And so I do what we always do when we want to orthogonalize.
I use the Gram-Schmidt algorithm.
And so all I need to check now is that in implying the Gram-Schmidt algorithm, I'm not
losing this translation and variance that I really need.
So the way you can check that is showing that multiplication by an orthogonal matrix
commutes with applying the Gram-Schmidt algorithm.
So here I'll let the xi denote the columns of x.
First I'll apply Gram-Schmidt and then multiply by m.
So here the order will be Gram-Schmidt, then multiply.
So for instance in the first step, I replace x2, the second column, with x2 minus the inner
product x1, x2 times x1.
And then if I multiply afterwards by m, I end up with a second column, mx2 minus the
inner product x1, x2, mx1, and similarly in all other steps in the Gram-Schmidt algorithm.
Now if I do this in the reverse order, so if instead I multiply by m, then apply Gram-Schmidt.
My matrix now has columns mxi.
So at the same step in the Gram-Schmidt algorithm, I'm replacing the second column of mx, which
is mx2, with mx2 minus the inner product mx1, mx2 times mx1, but again because it acts
as a linear isometry, I can pull those m's, get rid of those inside the inner product.
And so the key was that I ended up with the same exact matrix in the end, and so I haven't
lost the translation and variance of the distribution.
So this gives us an idea of what we mean by uniform random orthogonal matrix.
We mean something distributed with harm measure, which we can instruct in any of these ways.
So the next thing we can ask is how can we approximate the entries of such a hard distributed
orthogonal matrix?
So first of all, the asymptotic distribution of these individual entries of such a matrix,
this is a classical result.
So this is due to Burrell in 1906.
So we'll let x equals x1 up to xn be a uniform random point on the sphere.
Then the probability that root n times the first coordinate of x is less than or equal
to t.
This converges as n goes to infinity to 1 over root 2 pi integral minus infinity t e
to the minus x squared over 2 dx.
So that is the root n times the first coordinate of a random point on the sphere converges
weekly to a Gaussian random variable as we let n go to infinity.
And so as an immediate correlate to this, if for each n we let un be a random orthogonal
matrix, then the sequence of root n times the 1, 1 entry of our random matrices, this
converges weekly to the Gaussian distribution as n goes to infinity.
And so this follows from that first construction of harm measure where we chose the first column
uniformly from the unit sphere.
So the Burrell's result on the sphere gives us the result for the matrix.
So we have the individual entries of our random orthogonal matrix that are essentially Gaussian.
We like to extend this result to higher dimensions.
In order to do so, we need a way of quantifying the distance between two probability measures.
So the notion of distance I'm going to use is called total variation distance.
We let mu and nu be Burrell probability measures on some metric space x.
And the total variation distance between mu and nu, which I'll denote as a norm sub-TV,
this is defined to be two times the supremum over all a contained in x, absolute mu a minus
mu a. The supremum is taken over all Burrell measurable subsets.
So now using this notion of distance, Diachronus and Freeman greatly strengthened Burrell's
result about 80 years later.
So this is due to Diachronus and Freeman.
So again, I'll take x to be a uniform random vector on my normalized sphere, root n, s
n minus 1.
I'll let n be at least 5 and k be between 1 and n minus 4.
I'll also let z be a Gaussian standard Gaussian random vector and rk.
Then the total variation distance between the distribution of the first k coordinates
of my random point on the sphere x and my Gaussian vector z is less than or equal to
2 times k plus 3 or n minus k minus 3.
So what this gives us is we can approximate simultaneously k entries from our point on
the sphere by k iid Gaussian random variables in this notion of total variation distance
so long as k is little o of n.
So in our matrix context, what this says is again so long as k is little o of n, then
the k entries from the same row or column of a random u in the orthogonal group can
simultaneously be approximated.
And again, this is in total variation distance by Gaussians.
And so this led Diachronus to ask the question, all right, how many entries can we really
simultaneously approximate?
This is something Diachronus himself, along with Eaton and the Ritzen, provided a partial
answer to in the square case.
But really, Zhang gave the big result on the square case.
So what Zhang showed was that a pn by qn principle sub matrix of a random orthogonal matrix
is close in variation distance to a Gaussian matrix when the dimensions pn and qn are both
little o of root n.
So I can take a little o of root n by little o of root n, sub matrix and approximate it
simultaneously.
And he showed that this was sharp in the sense that if x and y are greater than zero
and pn is asymptotically x root n, qn asymptotically y root n, then the total variation distance
does not go to zero.
So in fact, this is the maximal square sub matrix that we can do the approximation for.
So putting this result on the square matrix being little o of root n by little o of root
n together with the original Diachronus-Riemann result, a little o of n entries from a single
row or column indicates that I can take a sub matrix that has dimensions whose product
is little o of n, and I should be able to do this always.
So this is the new result.
And this is due to myself under my advisor, Elizabeth Mechis.
So here we'll let zn be a pn by qn upper left block of a random matrix u uniformly distributed
on the orthogonal group. So that is distributed according to our measure. We'll let xn be
a matrix of pn qn iid standard Gaussian. Then as long as we just know the product of
the dimensions is little o of n, we let n go to infinity. The total variation distance
between the joint distribution of the entries of root n, zn, and the joint distribution
of the entries of xn is tens to zero. So this is a nice result in that it recovers the
John's result, it recovers the Diachronus-Riemann result, and all of the intermediate cases.
So notice here that if q over p tends to some eta, that is finite, then we're back to John's
case. In that case, pn and qn must both be little o of root n. And so there's not a whole
lot of new information there, we can essentially recreate his proof. The interesting case is
going to be when qn over pn tends to zero, so pn is going to infinity, it could be almost
like n, and qn is staying bounded. So here, what you end up needing is you're going to
be looking at covariances of traces of powers of wish-heart matrices, which I'll do in
a second. And so when you were doing it in John's case, you only needed to go up to powers
one or two. And so those are easy to kind of compute by hand, but as soon as you let
the powers of these matrices grow with n, they get very difficult to do. So one of the
main things that had to be done was figuring out what those covariances looked like. So
I thought maybe this would be the most interesting part to look at, would be the proof of this
little lemma here. So here, the lemma will need, so again I'm assuming q over p is tending
to zero. Then the covariance of the trace of x transpose x to the h, and the trace x
transpose x to the k. This is equal to 2 hk p h plus k minus 1 q, plus p q to the h plus
k minus 1, plus an error term, delta hk, where this error term is bounded by a constant
times p h plus k minus 2 q squared, 2 to the k k over root h. So really the important
part for us was we needed these leaning terms to cancel with other things. And we needed
this bounded part, the error term, to be small enough that when we plugged it into the rest
of the work, the total variation distance went to zero as we needed. So I thought I
would do a little bit of the proof here since it goes into graph theory. So the idea here,
we can take the trace of x transpose x to the h and rewrite it as a sum i1 up to i h
between 1 and p, the sum j1 up to jh between 1 and q of xi1 j1, xi1 j2, so on up to xi h
jh xi h j1. So I'm writing the traces as product of these Gaussian random variables.
And now I'll rewrite this as a sum over g of xg, where g is going to be an s-graph.
So g is an s-graph. The way we define an s-graph, we plot the ik on a top line and the jk on
a bottom line. Then we draw h up edges from jk to ik, and h down edges from ik to jk
plus 1. So the each edge in the s-graph corresponds to one of the random variables, and so the
product of all the random variables corresponds to an s-graph. And so I can count up the number
of s-graphs instead of counting this. Let's do a picture to get a feel for this.
So I can plot the i's. They don't have to be distinct, so maybe i2 overlaps with i4 this.
I'll plot the j's on the bottom, maybe j1, j2 overlap, j3, and j4 overlap. Then I draw
up edges from jk to ik. So here, j4 to i4, and down edges from ik to jk plus 1. So i1
goes to i2, i2 goes to j3, i3 to j4, and i4 to j1. Good. So now if I go back to looking
at this covariance that I'm interested in, this is equal to the expected value of the
product minus the product of the expected values, right? So here, minus the expected
value, expected value trace x transpose x to the k. And so I can think of this as being
the sum over all s-graphs, g, and k of the expected value of the graph I get by the union
minus the expected value of my first graph times the expected value of the second graph.
And so what we need to do is we need to figure out how the difference in the expected values
can change and count up the number of graphs per class. So we end up with two dominating
terms. The first, so recall what the moments of a Gaussian are. So if I have the expected
values z to the m, this is m minus 1, skip factorial, if m is even, and 0 otherwise.
So the first thing we can do is any of these products where I have a variable appearing
to an odd power, those are going to have expected value 0. So I can go ahead and throw away
all the graphs with edges of odd multiplicity. And so my first main term is going to be the
case where I have double edges in both of my individual graphs. So the expected value
of xg is equal to the expected value of xk is equal to 1. And then in the union, if
I take two graphs that have only double edges and overlap them, I'll have one graph or one
edge of multiplicity 4. So the picture here may be my g looks like this with all double
edges. And my k looks like that also with all double edges. And so the union of the
two graphs, I have to choose some edge in both to overlap. And so I get something like
this with my g union. And so this will have expected value 3 since I have the one edge
of multiplicity 4. And the difference gives me 2. So the last thing I need to do is count
up the number of ways I can construct graphs that look like this. So we have two h edges
to begin with. If I have only double edges, I have h distinct edges. Then I choose which
vertices are going to be on top and how many on bottom. So if I take r0 up to h minus 1,
I can choose r plus 1 bottom vertices. So the j's. And I'll have h minus r top vertices.
If I have h distinct edges, I have h plus 1 distinct vertices total. Then we count up
the number of isomorphism classes. So here we have two s graphs are isomorphic. If one can be
converted to the other by permuting 1 through p on the top line and 1 through q on the bottom line.
So that is two s graphs are isomorphic. If I'm just changing the labeling really of the vertices.
So the way I count up the number of isomorphism classes, I count up the number of ways I can lay
down the edges. And so to do this, I count all the possible ways that you can draw up
and down edges. When you have two h or h plus 1 vertices, that looks like h choose r, h
minus 1 choose r and subtract off the ones that don't give you an s graph. So here I
subtract off the bad ones. This gives me 1 over r plus 1. h choose r, h minus 1 choose
r, number of isomorphism classes, and then the number of graphs per class. So this is
given by the number of ways I can label those edges. Or I can think of it as the number
of ways I can label the vertices. So I can label the first top vertex with a p. Then
I have p minus 1 choices for the next top vertex, and so on. p minus h plus r plus 1.
Same thing on the bottom, I have q choices for the first bottom vertex up to q minus
r plus 1, minus 1. And so this is less than or equal to p h minus r, q dr plus 1. And
then finally, this constructs one of my graph. You construct the other graph, so the k graph
in exactly the same way. Only now you must have at least one overlapping vertex, so you
have one less new vertex to play with. And also you need to choose the number of ways
in which they can overlap. Choosing that edge of multiplicity 4. And I have 2 h k ways in
which they could pick the overlapping, just h k. Ways I can pick the overlap, h from the
choice in the first graph and k of choices of edges in the second graph. So putting this
all together, I get a term 2 h k, sum r 0 to h minus 1, sum s to 0 to k minus 1, and
then all of the things from my first one, and so on. So for the k graph, I get k minus
s minus 1, the one less vertex, k choose s, k minus 1, choose s. So that takes care of
the first case. Second case, I'll do much more fast just to explain what the second
possibility is. So the other possibility in counting up these graphs would be if we have
both of our individual graphs have expected value 0. So here x g and x k are both 0. But
when we take the union, it has non-zero expected value. And so the way this could happen would
be if I had some cycle of single edges or edges of odd multiplicity lying in the one
graph that exactly overlaps with the same cycle inside my other graph. So that when
I combine the two, if I lay them over each other exactly, everything has double edges,
and then I'm all good. The expected value is no longer 0. And we can have other stuff
going on. That's fine too. So in this case, you do the same kind of process, counting
up all the number of possibilities you can do this. Then you extend two graphs with six
single edges and so on and so on and so on so that you get a couple more of the big sums
like this. And so then the last thing you can finally do with this is this holds always
for any p and q we like as long as the product is little o of n. Then under that special
condition q over p tending to 0, you can start to bound these sums and bound the difference
in the expected value in that case. And that's how you end up with that really nice formula
I had over there for the covariances. Good. Yeah. All right. So maybe the last thing
I wanted to mention was just a result. So the original question asked by Diakonis didn't
put any stipulation on having to take a sub block of all the entries together. There was
no such thing in that original question. And so these proofs all rely heavily on the geometry.
But if we allow ourselves to weaken the notion of distance, then you can actually approximate
any little o of n entries you like from the random matrix. So this is just one last result
here. So this is due to Chatterjee and Meckis. So here if we take a1 up to ak, n by n matrices
over r, such that the trace aij, or aij transpose is root n delta ij, we let u be a random orthogonal
matrix, and x be the random vector we get by taking trace a1u up to trace aku. So this
is in rk. And again, we let z be a standard Gaussian random vector in rk. Then the l1
torvage distance, which is topologically weaker between my x and z, is less than or equal
to root 2k divided by n minus 1. And so in particular, if we take the ak to be some set
of root n times eij, where this is the matrix with a 1 and the ij entry in zeros everywhere
else, then this gives us the result that in this weaker notion of distance, we can choose
any little o of n entries out of the matrix we like. And so this answers the original
question by an coordinate freeway. All right. Question?
