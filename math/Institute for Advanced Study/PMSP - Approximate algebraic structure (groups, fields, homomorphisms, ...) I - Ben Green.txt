‫ieni.
‫אתהaval supuesto את איזה?
‫ Coming home.
‫ durability oh, yes, thank you.
‫נגר ראש.
‫конתש ninguna ע chyba.
moment second term last three options so you shouldn't know that the next
Jason building is Anthony Hall and heChash some important things there are some
computers if you want to read email or make us internet although there aren't as
In these two rooms in the first basement floor there are restrooms there as well as here in the lobby
will be a huge crowd going to lunch every day.
So first of all they take only cash and try to have it ready so that the floor is reasonable.
On Tuesday tomorrow there is a staff picnic of the whole institute so they are closing the dining hall at once
so we'll break earlier around 12 and get there earlier, what else?
Wireless, so you can read this.
Okay, so let's start the first talk by Ben Green, the first tutorial, so here's Ben.
Right, so I'm going to deviate very slightly from the title that I provided and I'm basically going to talk about the subject of additive combinatorics
and from a viewpoint that's motivated by pseudo-randomness.
So it occurred to me that there'll probably be at least a few people here who don't know what this term pseudo-randomness means
so maybe I'll start off just by saying a few words about that.
The idea of pseudo-randomness is, one way of looking at it, is that you've got some definite object
and you want to somehow say it looks like a random object, it doesn't have any particular structure.
So an example that everyone is probably familiar with is the decimal expansion of pi.
The digits are supposed to look random, well what does it mean to look random?
Well, let's think of a sensible test for what a random decimal expansion should look like.
It should at the very least have the right proportion of each digit.
So the frequency of each digit is the same, so each digit occurs one tenth of the time.
I mean, conjecturally, none of this is actually proved for pi.
And the same should perhaps be true of each pair of digits, so 1, 4, 4, 1, etc.
And for each triple and so on.
So what I've described there is the concept of what's called a normal number.
A normal number is a real number such that in any base, like base 10,
the proportion of each digit is exactly the same and the proportion of each pair of digits is exactly the same and so on.
And this notion of normal number is one type of notion of pseudo-randomness.
So if a number is normal, then that's one sense in which you can say it looks like a random number perhaps.
A random number is normal and so normalness is one kind of instance of pseudo-randomness.
So I shouldn't be talking about numbers and things anymore.
I'm going to be talking about sets of integers.
So additive combinatorics.
Well, I suppose a large part of the subject is motivated by the study of sets of integers.
So let's say sets A are finite sets of integers.
So I'm going to talk about various questions that arise in additive combinatorics to do with,
well, we'll be looking for things like arithmetic progressions inside sets of integers like this.
And other things of that sort.
So I want to talk first about the simplest definition of what it means for a set of integers to be pseudo-random.
So let's first of all think a little bit about what a random set of integers would look like.
What do I mean by a random subset of the first n integers?
Well, you might just toss a coin, toss coin number one,
and if it comes up heads, include that element, include one in your set A.
And then toss coin number two independently, if that comes up heads, include that in the set and so on.
So you'll get some set that looks, I don't know, like this.
So actually, to make the discussion a little bit easier,
I'm going to very slightly change the context in which I'm working.
And instead of talking about subsets of the first n integers,
I'm going to work inside the cyclic group Z mod n Z.
So I'll fairly frequently switch between sets of residues mod n and the first n integers,
often without commenting too much more on it.
So let me take a random set here.
So if you choose sets randomly, it's sort of clear and not hard to prove as well,
that you'll get, you know, in each half of Z mod n Z,
you'll get sort of the same proportion of elements in each subinterval of length n over 10.
You'll get, you know, roughly one tenth of the elements of A and so on.
And roughly speaking, a pseudo-random subset of Z mod n Z is a set which has this property.
A is pseudo-random.
If the intersection of A with each interval for I has size roughly,
well, what it should be.
So the size of that interval I over n times the size of A.
And also I want the same to be true for every dilate of A.
So equivalently, if the same is true for every dilated interval,
lambda I, which is a set of all lambda X for X in the interval.
So again, a set A is going to be thought of as pseudo-random.
If it's basically sits evenly in every interval and in every dilated interval,
or in other words, arithmetic progression.
And at the moment, the motivation for this definition is that a truly random set will have this property.
It will be nicely distributed everywhere.
So that's not a definition because, well, there are at least two things that need to be clarified.
First of all, I've used the word roughly here. What does that mean?
And secondly, I mean a single point is an interval and it can't possibly be the case
that the intersection of A with a single point always has size roughly size of A over n.
I mean, that's the intersection of A with a single point is either empty or the point.
So that's nonsense.
So I'm not going to try and formalize the definition of a pseudo-random set in this language,
but rather I'm going to introduce a slightly different language which I'll use quite a bit
and that's the language of the discrete Fourier transform.
on z modulo nz.
So this is not too hard to define.
So suppose that f from z mod nz to the complexes is a function.
Then for every point r in z mod nz, define the Fourier transform f hat r in this way.
So it's going to be the average of f of x twisted by the character e to the 2 pi i rx over n.
And a couple of points about this.
So expectation is my favorite notation for average.
This just means average.
And it's extremely tedious to keep writing e to the 2 pi i.
So we'll often write this as e of rx over n.
That's the standard e of theta is e to the 2 pi i theta.
So what does the Fourier transform mean somehow?
Well let me show you the Fourier transform of 2,
sorry, I've defined the Fourier transform of a function.
I can easily define the Fourier transform of a set in particular.
We can apply this definition with f equal to the characteristic function of a set a.
So this is 1 sub a of x is 1,
if x lies in a and 0, if x doesn't lie in a.
So at the moment it's just a definition.
Now I want to show you what it does by looking at two particular sets a.
So my first set is going to be just a very random looking set.
Or a pseudo random set in this definition.
I'm going to make sure it's nicely distributed in every interval and in every dilated interval.
And the second example will be as far from that as I can possibly manage, which is this.
So my set is just all bunched up on one side of z modulo nz.
So this is supposed to be highly pseudo random.
And this is supposed to be highly not pseudo random.
Now let's look at the Fourier transforms of these two sets.
So the first one, 1 sub a hat r is the average of x, 1 sub a x, e of rx over n.
And let's just for simplicity take r equals 1, let's just have a look at that.
So what is that sum on the right doing?
For each point of the set a it's weighting it by this unit complex number e to the 2 pi i x over n.
And what that does is it just adds up these vectors as x ranges over a.
But there are sort of vectors pointing in every direction equally many in each direction somehow.
So there's a huge amount of cancellation here.
In fact if a is a truly random set obtained by selecting elements by tossing a coin,
then you're going to get square root cancellation in this sum.
1 sub a hat 1 will be about 1 over the square root of n if a is truly random.
But the situation on the right is completely different.
Here I've got, I'm just adding up these vectors that all point in the same direction.
And there's almost no cancellation, I get a huge resolvent vector.
So here 1 sub a hat 1, well it's almost as big as it can possibly be.
Certainly, well it'll be bigger than let's say a half.
Sorry, not a half.
In this example it may be some constant.
So on the left I have a pseudo random set and I get a lot of cancellation.
So this makes the Fourier transform small and on the right I don't get cancellation and the Fourier transform is big.
And it turns out that there's a completely, a close correspondence between what I said here
about sets being equally distributed on intervals and progressions
and the Fourier transform being small or large.
And for my purposes today and I think for most purposes the right way to define
what a pseudo random set is, is in terms of the Fourier transform.
So I shall do that now.
So definition, this is a slightly temporary definition in some sense
but it's at least a precise definition unlike anything else I've said so far.
So let a be a subset of z modulo nz with size alpha times n.
Then I say that a is epsilon pseudo random.
If all of its Fourier coefficients are small.
So if one sub a hat r is less than or equal to epsilon for all r not equal to zero
but equivalently if f hat r is always small for all r where this f
you get by taking the characteristic function of a and subtracting off its average value.
So that's sort of just adjusted for what happens at r equals zero.
And it's a fact which I shan't prove that apart from the dependencies on this parameter,
I mean this is a precise definition involving an epsilon.
But this is basically equivalent to what I said here.
That a is epsilon pseudo random if and only if it has basically the correct intersection with every interval that's sufficiently large
and with every arithmetic progression that's sufficiently large.
It's quite a fun exercise, not a particularly trivial exercise but quite a fun exercise to sort of explore how this definition connects to the previous one.
But all I'm going to do here is just claim that I've motivated it using these two examples.
So what is the point of this definition?
Well I want to illustrate several points in connection with this and I'll list what they are.
So the first point is that if you have a pseudo random set, the smaller epsilon is the more pseudo random your set is,
you can say a lot about it.
So that's the first point.
Well that's all well and good but I mean you can say a lot about random sets as well.
But you'd be very lucky to have a truly random set.
So we can say a lot about these sets and then the remarkable thing is that we can also say a lot about sets that are not pseudo random.
Well I've already hinted at that.
I mean a set that's not pseudo random is going to be biased a little bit on an interval.
But we'll see lots more variants of this principle.
So they have windows of structure.
And then I also want to talk about a quite general principle which is that every set.
So at the moment well sets are either pseudo random or they're not.
But it turns out that every single set can be partitioned into a highly structured piece plus a pseudo random piece.
So highly structured plus pseudo random.
And I want to show you some examples of how you can use this decomposition to prove statements about every set of integers.
So any set can be split into a very structured bit and a pseudo random bit.
And often you can analyse the two bits separately and analyse how they interact.
So I want to explore a little bit these three points in the context of the definition of pseudo randomness that I've given.
But the beef of my lectures is not going to be quite about this definition of pseudo random.
But about a whole hierarchy of more subtle definitions of pseudo random.
So it's possible for a set to be pseudo random according to this definition.
But for it's still not to be possible for us to say a lot about it.
In particular, and this is a key example that I want to discuss in a little bit.
If you have a pseudo random set according to this definition, it's not possible to say very much about how many arithmetic progressions have length longer than three it contains.
So I want to talk about this hierarchy of notions of pseudo random and about each of these three points in that context.
And their things become a lot more complicated.
And in particular this point two turns out to be quite deep.
So for these more subtle definitions of pseudo random saying that if you're not pseudo random then you have a window of structure is a much deeper question and that's a question I want to focus on in particular.
But for now let me just carry on with this definition and I want to show you a sketch for you.
A proof of what's called Roth's theorem on three term arithmetic progressions.
So this is a theorem of Klaus Roth from 1953.
And it's the first case of what's called Samaradi's theorem, which I'll be mentioning a bit later.
So suppose that alpha is greater than zero.
And that n is sufficiently big in terms of alpha.
And then I'm going to take a subset of one up to n of size alpha n, be a set of size alpha n.
And then the claim is that this set contains three elements in arithmetic progression.
So that's x, x plus d, x plus 2d.
And what I'm going to sketch is a sort of modern formulation using this language of pseudo randomness of Roth's original proof of this statement.
So first of all, instead of working with the integers up to n, let me work inside the integers mod n.
So there is some sort of slightly tedious but not especially enlightening technical devices that let you transfer from one setting to the other.
So I'm just going to pretend now that I have a set a inside z modulo nz.
And then I'll divide into two cases.
Case one is that A is pseudo random.
And this is for some parameter epsilon that I'm going to choose.
Case two, well, is that A is not pseudo random.
So I'll analyze the two cases separately.
The first case is probably the easiest, the easiest instance of the first point that I made over here that we can say a lot about pseudo random sets.
What I'm going to show in the first case is that if A is pseudo random, then it contains not just one three-term arithmetic progression, but many of them, and in fact, the number you'd expect.
So here it turns out that we claim that A contains roughly alpha cubed n squared three-term arithmetic progressions.
Why is that the number that you would, quotes, expect?
Well, the number of choices of x and d in the definition of progression is n squared, n choices for x and n choices for d.
And for each of them, you've got three elements that you want to hit, and somehow each is going to come up with probability alpha.
So here's the proof. The idea is to use a little bit of discrete Fourier analysis.
And the particular fact that we'll use is the inversion formula, so the inversion formula, and that states that, I mean, this could be any function f, but I'm just going to do it for the characteristic function of A.
It's basically the inverse Fourier transform, suitably interpreted. It's the Fourier transform of the Fourier transform.
This is a very easy fact to prove, and this is a nice thing about the discrete Fourier transform.
If you study Fourier analysis on, say, the real numbers or on the circle, there are complicated issues of convergence.
There's no convergence issues here. It's a finite sum.
To prove this, just substitute in the definition of the Fourier transform 1 hat AR, and you'll immediately, well, you'll immediately see a cancellation that occurs, and this drops out.
So easy exercise.
So now let me, now that I've got this formula, let me work out the number of three-zone progressions in A.
So it's the sum over X and D of 1 sub AX, 1 sub AX plus D, 1 sub AX plus 2D, and then if I substitute in the inversion formula at each step,
So it's the sum over X and D, sum over RS and T, 1 hat AR, E of RX over N, 1 hat AS, E of SX plus D over N,
and then the remaining term, 1 sub A hat T, E of TX plus 2D over N.
And then if I take the summation over X and D inside, I get a lot of cancellation.
So swap the summation over X and D, and I get sum RST, and then so the sum over X is of E of R plus S plus T, X over N,
and then times sum over D, and now I've got SD plus T times 2D.
And the amazing thing is that if either R plus S plus T is non-zero or S plus 2D is non-zero, then those sums cancel just by orthogonality or summing a GP or whatever you like.
So what remains are the terms where R plus S plus T equals S plus 2T equals 0.
So the remaining terms have S equals minus 2R and T equals R.
And the upshot of that calculation is the following very nice formula.
So the number of three-term APs in A is exactly sum over R, 1 sub A hat R squared times 1 sub A hat minus 2R.
And then I forgot a normalization factor because when R plus S plus T equals S plus 2T equals 0, I pick up N for both of those two sums.
Okay, so there's a nice formula for the number of three-term APs in A.
So what I'm going to do is just split off the term R equals 0, and that gives me this expected number of terms, of three-term progressions, and then all of the other terms are going to be small just using the pseudo-randomness.
So this is alpha cubed N squared plus the sum over R not equal to 0, 1 sub A hat R squared, 1 sub A hat minus 2R.
So let me just have a look at that error term.
That error term represents the difference between the number of three-term progressions in A and the number that you'd expect in a random set of the same size.
So the error term in modulus is at most the supremum over R not equal to 0, 1 hat A minus 2R times the sum over R, 1 sub A hat R squared.
Now let's assume that N is odd, so that 2R is not 0 either, and then this is at most epsilon by the pseudo-randomness times this sum.
And the sum is, sorry, I wanted an extra N squared factor up there.
So the sum is by Parsivall's identity, which I'll draw attention to in a moment.
The sum of the squares of the Fourier coefficients is the same as the sum of the squares of the function, so this is Parsivall's identity.
So this states that, well, F hat in L2 equals F in L2, if you like.
Again, it's a fact about the discrete Fourier transform that's an easy exercise to verify.
And this thing here is just, well, it's just the density of A.
So if I choose epsilon, the parameter in my pseudo-randomness, to be much smaller than alpha squared, then that error term will be totally dominated by the main term.
Does there is a difference between the sum and the expectation by a factor of N?
Yes.
So we can just now put a factor of N, when the sum over where...
Yeah, what, between here and here?
Here.
Yeah, the point is that the L2 norm is two different L2 norms, one on the group and one on the dual side of the group.
And okay, so let me address that point properly.
So Parsivall's identity is the following statement.
F hat in L2 squared is the sum over R F hat R squared, and F in L2 squared is expected value over X of F of X squared.
And Parsivall's identity is assertion that these two are equal.
Why have I used sum upstairs and expectation downstairs?
Because really I've got the group Z mod NZ, and then the Fourier transform takes...
The domain of the Fourier transform is the dual of that group.
And so these two things correspond to dual measures on the group.
But you don't need to worry about that.
This is the statement of Parsivall's identity.
So to return to the point, if epsilon is really small...
So if epsilon is much smaller than alpha squared, then A contains roughly alpha cubed N squared, three-term progressions.
So that was case one of Roth's theorem.
Case two was the situation where A is not pseudo-random.
What am I going to do then?
So I shall sketch this very briefly, because I want to move on to other things.
Suppose A is not pseudo-random.
Well, then I can... It has a large Fourier coefficient.
For some R.
And if I dilate this set, then I can take R equals one.
So replacing A by, I guess, R inverse A, a suitable dilate of A.
We can assume that one sub A hat one equals, just by definition,
The average of the characteristic function of A times e to the 2pi i x over N,
that this is at least epsilon.
Now I showed you an example of a set which had this property,
and it was an extremely non-random-looking set.
It was just where all of A was concentrated in an interval like this.
And it turns out, as I've already hinted, actually,
that this is more or less what has to happen in this setting.
So this implies that A has a weird intersection with some interval.
So it's not roughly equal to what you'd expect it to be for some interval i.
Some reasonably long.
It's actually not very hard to prove that. I'll leave that as an exercise.
So here's the idea. This e to the 2pi i x over N,
if you chop up z mod nz into intervals,
that's going to be roughly constant on those intervals.
So you can compute this Fourier transform by partial summation.
You just sort of chop up into intervals.
On each one, the thing's roughly constant.
If A has the right intersection with each of those intervals,
then this Fourier transform will have a lot of cancellation.
So just as in the picture.
So I'll leave the proof as an exercise.
But it's sort of reasonable.
If A does not cancel along these vectors that go around like this,
then the only reason for that is that A has to be biased in one direction
compared to the other direction.
And if you do this carefully,
if A has either too big or too small density on some interval,
but it can't have too small density everywhere,
because it would have to make it up by having very large density elsewhere.
So I can actually make sure that A has substantially bigger
intersection with some interval than it should.
So in fact, A intersect i over i
is at least something like alpha plus some constant times epsilon.
And this is for some reasonably long interval i.
So let me recap.
What have I done?
I've taken a set A,
and I want to show that this set has three term arithmetic progressions.
I've divided into two cases.
First case is that the set is pseudo-random, and then I'm done.
The second case is that it's not pseudo-random,
and then I've selected a little interval
on which the density of A is bigger than it was before.
So it's substantially bigger than alpha.
Now I haven't quite got the same set A anymore.
I've got a dilate of it,
but dilation preserves three term progressions.
So I'm going to just focus in on this new bit of A
and repeat the process.
So now I replace A by A prime
equal to A intersect i.
Alpha by alpha primed is the density of this A intersect i on i,
which is at least alpha plus constant times epsilon,
and then finally N by N primed,
which is the size of i, and then repeat.
Now if I choose epsilon to be...
I mean, epsilon had to be smaller than alpha squared,
but let's make sure it's not too much smaller than alpha squared.
So if I choose epsilon to be alpha squared over 10, say,
this process can't go on forever.
So if I apply this process, I've increased the density of A
and the density of A can't be bigger than one.
So the process must stop after...
well, if you think about it for a little bit,
let's make some constant over alpha steps,
since density must always be less than or equal to one.
And that is...
That's the end of the proof.
Yeah, I was a little bit vague about the size of N primed.
So there are some technicalities that I've suppressed.
One thing that can happen...
Now, I dilated this set A, and then I took an interval.
Now, this is where the difference between...
This is where the difference between one up to N
and Z modulo NZ is important.
So suppose I take a dilated arithmetic progression modulo N,
so it's something like this.
It wraps around on itself.
When I unwrap and go back to one up to N,
it will no longer be an arithmetic progression.
It'll be some weird sort of slightly grid-like structure.
But it turns out that although progressions
don't survive this dilation and unwrapping,
you can always at least find bits of progression
on the dilated and unwrapped bit.
And if you're clever about it,
you can make sure that this has length
at least something like square root N.
So that's important because you want to know
that you've increased the density of your set A,
but you want to make sure it's not on a tiny,
vanishingly small interval.
So as I said, that's an important technicality
that I've suppressed there.
Okay, so let me just very briefly reiterate how it went.
If you're pseudo-random, then you can say everything
you want to about the set A.
If not, then you locate a small piece of structure,
which in this instance is a little window
on where the density of A is bigger,
and you deal with that bit of structure.
So that is an example of how pseudo-randomness is used.
Now, what I really want to talk about,
as I said, is these hierarchies of pseudo-randomness.
So it's very reasonable to ask,
does the same argument work to prove a statement
about four-term progressions?
Can we prove Roth's theorem for four-term
progressions in the same way?
So in other words, can I show that if I have a subset
of one up to N of density alpha,
then it contains a four-term progression
provided only that N is sufficiently large
in terms of alpha?
Well, let's have a look at the proof.
In two cases, does case one work?
So is it the case?
Is it true that if A is epsilon pseudo-random
and A has size alpha N,
then A contains roughly alpha to the 4N squared
4-term progressions?
And this is, I think, where the theory starts
to get really interesting because the answer is no.
The notion of pseudo-randomness that I introduced,
although it lets you say a lot quotes about the set A,
doesn't let you say this much about it.
And I want to give two examples of this.
So the answer is no.
So the first example is I'm going to take A
to be the set of all N less than big N
for which the fractional part of N squared times
the square root of 2 is at most alpha over 2.
There isn't anything important about the square root of 2,
any suitably irrational number would do.
And this means fractional part.
So fractional part of N number X,
you just reduce the number modulo 1
and find that reduced part that lies
between minus a half and a half.
So that's the definition of fractional part.
So what a strange set to consider.
And let me tell you some things about that set.
So fact 1 is that A has size density roughly alpha.
Well, that's reasonable.
If I look at the fractional parts of N squared root 2,
they really ought to be uniformly distributed on the unit circle.
And so the proportion of N for which that fractional part lies
between minus alpha over 2 and alpha over 2 should be alpha.
But the proof is not so easy.
It uses a technique of vile.
In particular vile's inequality on exponential sums.
So that's definitely a not particularly easy exercise.
Essentially the fact that N squared root 2,
mod 1, is uniformly distributed.
Second fact is that this set A is actually highly pseudo-random.
So I think even if you want epsilon to be as small as 1
over the square root of N, this is going to be epsilon pseudo-random.
And the proof is sort of the same.
I mean, again you need to use some kind of vile's inequality to check that.
And again, you know, it seems kind of reasonable.
Why should this set that's defined in terms of a quadratic function
correlate with one of these linear exponentials?
It would be slightly surprising if it did.
But it turns out that that set has many four-term arithmetic progressions.
However, A has too many four-term APs.
So why is that?
So too many means much bigger than alpha to the 4N squared.
Well, here is the reason.
So indeed, if N squared root 2 and N plus D squared root 2
and N plus 2D squared root 2 are all less than alpha over 2,
i.e., if N plus D and N plus 2D lie in A.
So I suppose I've got the first three terms of a four-term progression inside A.
Then automatically, the fractional part of N plus 3D squared root 2,
well, it has size at most 7 alpha over 2.
And the reason for that is the following identity.
This identity, N squared minus 3 times N plus D squared plus 3 times N plus 2D squared
minus N plus 3D squared equals 0.
So you can see that if I've constrained the fractional parts of N squared root 2,
N squared root 2, and N plus 2D squared root 2,
then I've got some information about the final term.
And what does that mean?
It doesn't mean that N plus 3D is automatically in A,
but it certainly means it's very likely to be in A.
So I'll put this in inverted commas.
3D lies in A with probability 1-7.
And actually, it turns out that that's precisely true.
It does lie in A with exactly probability 1-7.
So how many 4-10 progressions are there in A?
There are roughly alpha cubed N squared 3-10 progressions in A.
And that's because A is pseudo-random.
And every time I've got a 3-10 progression,
I've got a chance 1-7 of getting a 4-10 progression as well.
Alpha is smaller than 1-7. This will be much bigger.
So if alpha is, I don't know, like 1 over 100 or something small,
this will be much bigger than it should be.
So this quadratically defined set is an example of a pseudo-random set
that has the wrong number of 4-10 progressions.
Now, it somehow has the wrong number of 4-10 progressions in a good way
in that it has more than it should.
But if you're a bit cleverer about things,
you can actually construct an example
that has less progressions than it should.
But the basic idea is that pseudo-randomness is not good enough
to control 4-term progressions.
Now, I want to introduce a more disturbing example
of the same phenomenon.
So I'll just show you this on the same.
So I'm now going to look,
instead of a genuine quadratic function
at a slightly weird object,
I'll look at the set of all things for which
the fractional part of n root 2
times the fractional part of n root 3.
So there's two fractional parts there.
Fractional part n root 2 times fractional part n root 3
is less than delta over 2.
And it turns out that the same phenomenon occurs here.
So this is an example of a generalized or bracket quadratic.
So the first two statements are true.
Again, this is a set with size roughly,
I meant alpha here.
It's a set with size roughly alpha n,
but now the proof is quite a bit more difficult.
You have to come up with some variance of vials
in equality for these bracket quadratics.
I may come back to that later.
That can be done.
And it's also pseudo-random.
You'll need the same ingredient there.
So why, how does this identity work?
Again, this set has too many 4-term progressions.
And the reason is slightly different.
This is because the function,
this weird function psi of x,
which is x root 2
times the fractional part of x root 3.
It's not a quadratic function,
but it's locally quadratic.
In the following sense,
well, it's locally quadratic
because x root 3 is locally linear.
So,
provided I don't want linearity everywhere,
so if x and y both satisfy
fractional part,
fractional part is less than, let's say,
one-tenth in modulus,
then the fractional part of x plus y root 3
is the sum
of the fractional part of x root 3
and the fractional part of y root 3.
And that's very simple to see.
It's clearly correct modulo 1,
and it's also got the right size.
It's banded in modulus by one-fifth.
So this is not a genuinely linear function
because there are these carry issues.
If the fractional part of x root 3 was like a third,
and the fractional part of y root 3
was about a third as well,
then the fractional part of x plus y root 3
is not the sum of the two fractional parts
because that's two-thirds,
which is not in the right range.
But you've got enough quadraticness going on
that you still can play the same game
with this identity.
So psi of n minus 3 times psi of n plus 2D
plus 3 times psi of n n plus 2D
minus psi of n plus 3D is 0.
Not always now,
but provided at least the n, n plus d,
and n plus 2D satisfy this little constraint
about root 3.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
אין אין פונמונו שעשי,
כשאם אני רוצה להסתכל מיני פרקרסיה,
First of all, pseudo-randomness is not enough, which is a shame.
Moreover, I've got to take account of at least two examples.
One of which is reasonably natural, this quadratic function,
and the second of which was a bit more disturbing.
These bracket quadratic things do not at first sight seem very natural.
אני חושב שבאמת, אני חושב שזה אינטוארטיבלי רזנבל,
ואולי אפשר, although not extremely easy to prove,
That this thing is not a genuine quadratic in disguise.
So that really is a new, it's not the same as example one.
My aim now then is to understand these quadratic phenomena.
This will be done using some things called the Gower's Norms.
But the first thing to do is, well, we've uncovered these quadratic
and these generalized quadratic examples.
How did we, when I was showing that a pseudo random set has the right number
of three term progressions, how did I do it?
I used Fourier analysis, so I used the inversion formula of Fourier analysis
and possible's identity.
How could I possibly model that with these kind of weird examples in mind?
Do I know some kind of variant of Fourier analysis that uses quadratics
and bracket quadratics?
Well, I really wish I did.
The answer is, I mean, I don't.
Finding such a thing is actually somehow a key open question in the subject.
So the first thing I'm going to do is go back to normal pseudo randomness
and try and handle as much of it as I can
without ever saying the word Fourier transform.
I won't be able to absolutely never say the word Fourier transform,
but I want to suppress the Fourier transform as much as possible.
So this is back to pseudo randomness.
Or what we might now call linear pseudo randomness.
Because we're aware that there are more exotic sort of quadratic things out there.
So fact is that there's an alternative characterization of linear pseudo randomness.
So the following are equivalent.
One, that the set A is linearly pseudo random.
Well, maybe I'll list three facts that the last two are closely related.
Actually, no, I'll just list two.
The second fact, so the definition of pseudo randomness
is the definition of pseudo randomness is the Fourier transform.
The second fact is not.
So this thing called the balanced function of A.
Let me keep it like that.
It satisfies the following.
So it's average over what are called additive quadruples.
Is it most epsilon prime?
So one and two are equivalent,
but you might have to, in going from one to two and back again,
you might change epsilon to an epsilon, a square root epsilon or something like that.
Actually, these modular signs are superfluous.
Turns out the thing's always positive.
So how do I prove that?
Well, it turns out that this expression at the bottom here,
has an expansion in terms of the Fourier transform.
It's actually the same thing as the L4 norm of the Fourier transform of F,
which is the same thing as the sum over r not equal to zero
of the Fourier transform of A to the power 4.
How would you prove that?
Well, you could prove it using the inversion formula.
You could prove it, in fact, once you've written it down,
you could prove it without using the inversion formula,
just substitute the definition of F hat.
Or you could be a little bit more conceptual about it,
and I haven't mentioned the notion of convolution yet,
but this thing that I've written down here
is actually just the L2 norm of the convolution of F with itself squared.
Which, by Parsivall's identity,
is the L2 norm of the Fourier transform of that.
And hat Fourier transform takes convolution into product.
And that's just the same thing.
So as I say, that's somehow the conceptual way to see it,
but you could just verify it as a formula.
And then the proof that these two are equivalent
is almost exactly the same as the argument I used in Ross theorem.
So the proof is then straightforward.
I'll make a few remarks about it.
So if A is pseudo-random,
then this sum is small.
To do that, just take out,
take out a factor of 1 sub A hat r squared
and use Parsivall's identity,
exactly as I did in Ross theorem.
The other way round, if this L4 norm is small,
then each term is small, just trivially.
And that turns out to be enough
to conclude that A is pseudo-random.
Incidentally,
when I said
that if this is small, then each term is small,
I have, of course, relied on the positivity of every term there.
In the formula for three-term arithmetic progressions,
which was 1 hat A r squared
times 1 hat A minus 2 r,
there was no positivity.
And so that argument wouldn't work.
So although pseudo-random implies
the right number of three-term progressions,
the converse is not true.
So there's something special about this expression
that gives you this positivity.
And before I break for coffee,
I just want to tell you that this thing here
is called the Gower's U2 norm of F.
...
Actually, it's better to write it
in a slightly different way.
So this is just a re-parameterization
of the same thing.
...
So I'm summing F over these kind of additive quadruples.
That's defined to equal
the U2 norm of the function F
or to the power 4.
So now I've written this.
This is an important expression.
You can probably guess what the definition
of the U3 norm and the U4 norm and so on are.
And I'll take a break now,
and after that I will define those properly
and tell you about how they can be used
to study these four-term progressions and things.
...
