‫ Meteorite jGLP...
‫שפה מל twists.
‫או כן.
‫אני חשיב סדר졌י מלática מה שם New Superman.
‫כיinnn.
‫ב plenty.
‫ת internally,
‫מל участppen כוכבים
אמה על פינת גורבה?
זה הן
lege blank
אלוהנ rasp
operating
votre
miał
סיים
מה Juwe.
המסכלBL
מקוק
יהיה
deeds start earlier or you late
Layton
So everything is
East really simple in knife and face very very beautiful this theory is the beautiful theory and
So Dharma
it's of course very important mathematics I'm going to concentrate on
applications that are somewhere in
the border of computer science
מאחד submarins fault 됐קר Regarding bush and Creators but of course
there are, there are many important applications
with the only representation theory
of finite boiling for infinite and compact groups
and so on there very important
in lots of dark of gradient sus
I'm not going to talk about it.
I'm just going to talk
mainly about applications that have to do
with graph expansion, not only graph expansion
although some other things but
I mainly want to focus on application
‫היאורי 2-אקספансיינג של גרפט, ‫בפרקולר, של קלי גרפט.
‫ואלה שאני רוצה לתת לך,
‫הזכות שהגרפטים של גרפטים ‫בין כל הגרופטים,
‫שכן שהגרפטים של גרפטים ‫בכל מיני גרפטים
‫הרעים להסתכל על כל הגרופטים.
‫אני חושב על הגרפטים ‫באולי.
‫זה הrei Color paper ‫הואresentי שאלה
‫ TT SMART Mность,
‫או שמוחים טרקטית ואידיביידים לכם.
‫לכן היא תלכה איזה ‫הגרפטה ה пам indicated
‫זה ס replaces dual dermel
‫שמציג Tamil
‫הרעיה איזה מ relaxation
‫הכם הואnormוס ‫ levers
קרונאומנסים ושפסקוינט-פאפרסים.
אני צריך לעשות את מיטריסט-ממליבקציה,
פסט-מיטריסט-ממליבקציה,
ובגרופו-פרסנטציה,
אני רוצה להסליח את הבאסיקה הזאת.
אני אוכל שאתה,
ואני אוכל שאתה,
ובכלל, אני לא יודע אם אני יכול להתחיל,
אבל כי ראסל פרומיסטי לסלום.
אני רוצה להסתכל על שפאפרסים
THAT HAVE TO DO WITH EXPANSION IN NON- are simple groups
Really nice application of potential application of the representation theory for
League groups and that's papers, a series of papers of Malmali and so on that have to do with
proving lower bounds, we're proving that the permanent is cannot be reduced to the
determinant in polynomial size. Okay, so let me just start and just stop me,
Rasser is not the only one that can slow me down, anybody can.
So the basic notion that we will study to begin with will be group action.
So we will have g will be a final group and I'll always have n to the north,
the size of the group and the first notion is the notion of group actions.
What does it mean that g acts on some set x and it's a very simple notion and it's
very familiar, what it means is that every element of g, every group element,
let me call it omega, because I'll use x for elements of x.
What does it mean that g acts on some set omega?
It just means that every element x here is associated with a permutation pi x
on omega, in such a way, so that's a permutation,
in such a way that it respects group multiplication,
so what we want to have is pi x, pi y equals pi xy.
So let's start to see examples.
The most basic example and it's very important, we'll return to it,
is when g acts on itself, so omega equals g.
So we start with examples.
The set is g itself and the group acts by left multiplication.
So what happens is pi x of y is simply x.
Okay, and that's obviously respects the group multiplication by definition.
There are many other actions of groups on questions that are already here.
For example, if you have any subgroup h in g,
then the group acts on the cosets of h mod g.
So you can take omega to be the cosets and then pi x of,
now a coset is yh is xyh.
You move the cosets, you permute the cosets.
We may need it later.
Let's go to specific groups.
You can take a specific group like Sn and then, of course,
it can act on itself, but it can act on other things.
For example, it acts on just the elements 1 through n, just permuting them.
So even though it's a big group, it acts on a small set,
it just permutes the elements of n.
It acts on pairs.
That's basically what we have when we talk about symmetries of graphs
or isomorphisms of graphs.
We permute the vertices of a graph and we get another graph.
It's a permutation of the edges of this graph.
So it acts on pairs, on triples, that's on, and lots of other groups.
And the fourth one, which I always like to give just for those who've never seen,
these are sort of very natural actions and very familiar
and you could invent them yourself.
There is an action that when maybe in the first instance you see it
and you didn't see it before, it's not obvious that it's an action.
You think about a minute and it is obvious,
but let me anyway give this example just to make the point
that actions of groups can be complicated objects.
This is a group.
So here it is, this group.
Here G is this group.
So what is this group?
This is a group of 2x2 matrices of determinant 1.
So this is A, B, C, D.
There are all these elements A, B, C, D, R, F, P.
And the determinant A, D, B, C is 1.
That's a group.
And this group acts on the following set.
The set omega would be just F, P union infinity.
So it's P plus 1 element.
And the way it acts is as follows.
The permutation associated with such a group element A, B, C, D is A, B, C, D.
The way it acts on some element alpha here,
or let's call it omega, so that omega be an element here.
And omega is by taking it to another element
by this what's called fractional linear transformation,
or maybe it's transformation.
And you can see that you added the infinity
because you'll be dividing by zero,
and you have to take care of the meaning of infinity over infinity and so on.
But anyway, it's a permutation of this.
Every element gives you a permutation,
in fact, they respect the group multiplication.
So that's an action.
And given this example and lots and lots of other examples
you can imagine and invent,
you can ask, well, what are all representations?
What are all actions? How do they look like?
What are all group actions?
Even if you fix a group,
can you understand the set of all actions of a given group?
And the short answer is yes,
and I'll give you the longer one,
but the first question sort of to ask
is something I like saying,
and I think some people like it,
and some people don't like it,
but anyway I'll say it.
Group actions are something very nonlinear.
I mean, the group is, well,
there are not linear operations.
And if you want to understand something in mathematics,
that's the thing that I try.
Maybe controversial,
you have to make it linear because otherwise you're stuck.
So you have to make it linear.
So how do you make group actions linear?
Well, there is one trick that works.
This one universal trick that's used all over mathematics
is that if you have an action on a set,
if you have some functions on a set,
then you immediately,
or you have something operation on a set,
then you immediately look at the same operation,
you extend it linearly to functions on the set.
So if G acts on omega,
and you have an F in some field,
then immediately G acts on the vector space,
F to the omega.
By this I mean just the vectors of elements of length omega,
this is just a set of functions from omega to F.
What is the action?
You just permute the coordinates.
So since you have this,
everything is clear,
don't hesitate to stop me.
For this set of lectures, yes.
It's not necessary,
you can talk about infinite, but yeah.
So since you have now something linear,
so G acts on this vector space,
so we said we have,
so here pi x,
pi y also on this set is pi xy,
but this is a linear set,
so we might as well think of these acting elements,
these transformations,
these linear transformations,
namely matrix sets,
so we might as well think of these guys as elements
in M,
what is the size,
so let's suppose it's size omega,
size D.
So these are elements,
they are permutations,
but they are elements.
Okay, they are linear transformations.
So now we are dealing,
the actions we are studying is,
a set of linear transformations
that behave like the group G.
In other words,
we have a definition,
so this mapping that takes x to pi x,
we'll call it rho,
and rho is a group representation,
which is the right way
we want to think about actions,
is a group representation,
simply if rho from G to,
is a group of morphisms.
Okay,
so that just means that,
rho of x,
rho of y,
rho of xy,
which is exactly what we had.
Okay,
so we have this family of actions,
all this family of,
by now we want to think about these,
all group representations.
What are all group representations?
That's what we want to understand.
And,
in what sense do we want to understand it?
Well, I don't know in what sense,
whether it was clear
or whether it was even the way
people started thinking about it,
but here's a way to understand it.
The way we want to understand the integers
under multiplication,
is that you try to build them up
from, you know, the prime constituent.
So we know that,
we know that every element,
every integer, I don't know,
A is the prime,
is the product of P i to the E i,
P i is our primes,
P i is our integers.
We know that there is always such a decomposition,
so that's an integer.
There is always such a decomposition,
we know that it's unique.
We know what are the prime elements,
although it cannot be divisible by any other integer,
except themselves in one.
So we understand it,
at least in this very simple sense.
We have the uniquely composition
to element that are irreducible,
cannot be reduced further,
and then, you know,
that you find each one of them
an integral number of times
inside your original number.
So as it turns out,
this is true just in this very high level.
The same statement is true
for group representation.
So we'll just start developing it,
every representation,
it doesn't matter how complicated it is
or what you see in there,
can be decomposed into a, you know,
a number of irreducible representations,
and, you know,
they are well defined
what are the irreducible representations,
and each one of them will be found
in that representation,
exactly some integer number of times.
Okay, so let's see how do we go about
building the irreducible representations.
So you can take any such representation.
So let's spell this out.
So what is this representation?
It's a mapping.
So we fix the field.
By the way, most of that,
I mean,
lots of the theorems I'll talk about
will be true only for the complex numbers.
In fact, most of them will be true
when, so, let me just write it.
Characteristic of the field
that does not divide the size of the group,
we'll see where it's needed
and many things will be true even more generally,
and there are theories
that take care of the,
the so-called Broward theory,
that takes care of the case that, you know,
if it doesn't satisfy this condition.
But we won't get there.
We'll do that, just a simple theory.
This is called the,
I said the simple theory,
but it's called the semi-simple case.
There must be something simpler.
Okay, so anyway,
we fix F, we fix G,
and we are given some representation
and maybe very complicated
and we want to simplify it.
So what is our representation?
So we have our group elements,
you know, there are n elements,
n elements,
and each one of them,
Rho assigns to it some matrix,
this Rho of X,
and, you know,
they satisfy this multiplication property
and now we want to simplify them.
So how can we simplify them?
Well, there are matrices
that act on this vector space,
on F to the omega,
on F to the D, basically.
So, right, it acts right.
These matrices,
so F to the omega,
basically I'll think about it as F to the D.
So they all act on this vector space F to the D
and one thing we can try to do to them
is somehow to, you know, diagonalize them
or make them, you know,
make them smaller.
And the way to do this is as follows.
So they,
let's all let me just say.
F to the D.
So we want to make them all smaller.
So they are not simple
or they are reducible.
These matrices are simultaneously reducible
if you have the situation
that's for some subset of V,
which is not V
and which is not the zero,
some non-trivial subspace of the whole space,
you have the situation that Rho of X,
W is contained in W.
In this case,
you found an invariant subspace.
So in this case,
so if this holds for every X in G,
then we have that W is an invariant subspace.
And you can see that in this case,
if you have this,
then in fact you have the span
of all these Rho of X, W
is exactly W,
because if this is the identity element,
then Rho,
I should have said,
it's sort of obvious
that if we have this property,
then we have the product property here.
If it's a group homomorphism in particular,
we have that Rho of the identity
is the identity matrix
and Rho of X inverse is Rho,
sort of obvious.
So the span of all these,
you know,
the way the movements of W under these matrices
is W itself
because we have the identity matrix in there.
And so W is an invariant subspace
and you could hope that you could change bases
so that, you know,
you'll change bases
so you'll separate out the action
of all these matrices on W
and the action of all these matrices
on the complement of W.
And it works,
although it doesn't work immediately,
you have to think a little bit
or to apply Maschke's theorem,
which is that if you are in this situation,
W is an invariant subspace,
I should say G invariant,
but we are fixing this,
invariant
and non-trivial,
like here,
then there exists a U,
that's the orthogonal complement of,
so over finite fields there may be several complements
and you may want to choose it carefully
and you can choose it carefully
in such a way that U plus W
is everything, is V,
so that it is an orthogonal complement,
but moreover,
and U is also invariant.
So not only if W is invariant,
but it's orthogonal complement is invariant.
Why is it good?
It's good because now you can do a change of basis
and start diagonalizing all these matrices.
So what this means,
it's a terrible silence,
so is it too clear or too...
No, I'm talking about the people who...
I haven't found a reason to slow you down.
Good, okay.
So we found that we can,
if we have these invariant subspaces,
we are not looking at the matrices as right,
we should move to this basis
and change it so that...
What this says is that the matrices,
all of them together are separately on W
and separately on U.
So what it means is that we can change basis,
there is a unitary base change.
I don't know how to call it.
It says Z,
but if we look,
if we now look at these matrices here,
let me continue here.
There is an inner product.
You don't need it at this point.
You don't need it at this point.
So let me...
I won't prove it even though it's just one line.
To prove this that there is an orthogonal complement,
which is also invariant,
you start with any orthogonal complement
and then you take the projection from V to W
with respect to that other orthogonal complement
and you just conjugate it with all group elements.
So you sort of average out so that maybe it's not clear
or it doesn't matter.
If you want to know later, I'll explain.
It's a one-line proof.
But if you do it at the right time,
then you will get your name on it.
So there is a unitary change of basis
so that if we look at all this,
so here we had rho,
but if we now look at Z rho, Z inverse,
the same matrices,
what they look like,
initially they were arbitrary matrices,
but now they split.
They now look like this.
And this is, let's say, the W part
and this is the U part.
So by this I mean that these are zeros.
So since the rho split into the action on U
and action on W,
we might as well write rho as tau plus sigma,
the direct sum,
the direct sum of two representations.
I mean, because when you do matrix multiplication,
we know that rho x, rho y is rho of xy.
This is also true in this basis,
which means that when you multiply these matrices,
what is in this left top corner multiplies
and what's the bottom right corner multiplies.
So we have tau and rho.
So this, we can call this guy tau,
tau of one, tau of two.
And this is sigma of two, sigma of one,
sigma of n, tau of n.
And of course these are also representations, right?
So tau and sigma are representations in the same sense.
Okay, good.
So that's a method to simplify representation
just like dividing by some number is a method
to simplify an integer.
And we can go on and on and on and on until we get stuck.
So what happens when we are stuck?
When we are stuck,
we eventually had some other unitary.
So after some basis change,
we got the representation down.
So we eventually wrote rho as the sum of rho i.
i goes from one to some t.
And each of these rho i's are irreducible.
What does it mean irreducible?
We cannot do this process to them anymore.
So we got stuck.
So that means that they have no invariant subspaces.
Some of them may be the same ones.
Some of them may be different.
That's what we'll try to understand next.
So you can do this process to every representation.
So we know you can get prime elements.
It's not at all clear that if you do it in one order,
you won't get something different
than if you do it in another order.
But it will not happen.
We'll prove it.
So that's where we are.
Every representation can be decomposed
into irreducible representations.
So now we want to understand what are these representations.
Questions?
Is it true that there is always one invariant substance
in which all coordinates are equal?
No.
No.
Actually, good, good, good, good.
I remember something I wanted to do just for Russell
and because he reminded me almost for God.
I want to do an example.
So let me do an example.
I hope I remember it right.
Just before the example,
does this direct sum between the representation
is defined with respect to z?
z doesn't.
So if you take any representation,
forget these invariant subspaces.
Take any representation and do a base change.
It will still be a representation.
That essentially is the same as the previous one.
There will be a representation.
You have to do this decomposition,
so every time you use that, it's the basis on a different z.
Right, yes.
So after a change of basis,
what you will get is that the original representation,
all these matrices,
are just block diagonal.
It's 1, 2, up to t.
They will have some dimension.
Yeah, yeah, yeah, you recurve.
No, of course you recurve.
You continue doing it.
Yeah, that's right.
They will have some dimensions, but yeah.
That means...
Is that a t-me?
No, no, no, it's not obvious that it's a t-me,
but we'll see that it's a t-me.
So is it a t-me?
It stops at some point, yes, it always stops.
Okay, so now let me just do one example
so that you see how this decomposition works,
and you'll see that it's not always
that you have this invariant space,
which is all ones which you are used to.
Most of our examples do have it, but not all.
So let's look at S3,
and look at its action
on the first three elements.
So let's write down the...
I hope I'll have room here.
Here are the elements of S3.
I want to just do this picture in the particular case,
so we have the identity, we have one, two,
we have one, three, we have two, three,
we have one, two, three, one, three, two.
What is... how does the matrix look like?
First of all, there are three by three, right?
Which board?
So what are these action matrices?
They are permutation matrices,
so this is one goes to two, two goes to one,
one goes to three, three goes to one,
two goes to three, three, one goes to two,
three, one goes to three,
three goes to one,
one goes to three, three goes to two.
Right?
Okay, so that's a set of matrices.
Anybody sees an invariant subspace?
מדור?
Yeah.
So we see immediately that all these matrices,
so rho of x, when we multiply by 111,
we get a constant times 111.
We get 111.
So that's an invariant subspace,
it's not empty, it's not everything, so it's non-trivial.
So we want to find a good basis change.
So what do you think is a good basis for the space
so that, what is the right complement?
What basis should I choose for the complement?
You decide the field already?
Yes, yes.
That's what you want to say?
Would the sum be zero?
Yeah, so no, no.
You're right that you need two vectors
for both of them, you need the sum to be zero.
But you want the basis for this space,
for which when you act with these matrices,
it will be preserved.
Okay?
No, complex, complex.
So what Olga suggested is the right,
so the other two vectors we should take
are these vectors,
so these three vectors, of course,
generate the space,
and let me not do the,
well, maybe I'll do one example.
Let's see why is it a good space.
Let's look what happens when you...
Okay, let's do off one, two.
Let's apply it here.
So we just reach the two coordinates.
Omega to the three is one.
What?
Omega to the three is one.
Yeah, omega is the third root of unity, thanks.
Omega cubed is one.
Yeah, that's the third root of unity.
So what happens when we apply
rho of one, two to this vector?
These two are not orthogonal.
Which two are not orthogonal?
This and this?
Why not?
They are?
They have to be.
Yeah, yeah, you multiply like you multiply complex vectors.
I'll keep forgetting to conjugate things,
but yeah, when you do inner products
between this and this, you get zero,
right, because it's complex inner product.
Yeah, so when you multiply,
so what does it mean?
This is a transposition, one, two, three.
So it's the two elements.
So this is omega, one omega squared.
And this is really good,
because this is a shift of this guy, right?
That's, I think, this, I think, is w, yeah.
Omega times one omega squared, omega.
Okay, so what will happen
after we do the change of basis?
So here we have the usual basis,
you know, E1, E2, E3.
We want to move to this basis,
and after we move to this basis,
what will happen?
Well, what will happen?
Let's draw them.
Maybe you can tell me.
We've got the two representations,
you know, one of dimension one
and one of dimension two.
Okay, so one will be of dimension one.
What will be written in these coordinates?
So this is the representation
that corresponds to action on this,
on this subspace.
The identity.
So if we, if this is our row,
and we write row equal sigma plus tau,
then we have a sigma is the identity representation.
It sends everything to the,
to every permutation to the constant one.
And what will be this guy?
So you can start figuring it out.
So the identity element,
it will remain the identity matrix.
But let's see, we looked at one, two,
and we saw that what happens is the first guy
moves to the second guy multiplied by omega,
so you can check.
I'm not going to do this.
This will be omega,
and this will be omega squared.
And I think that when you do one, three,
this will be the opposite,
omega squared and omega.
And when you do the last two, you get one, one.
Okay?
So that's, that's the portion.
I mean, I, I write it in blocks,
but obviously they don't talk to each other.
So there's sigma,
which is a one-dimensional representation
that is the identity representation.
And the tau,
which is a two-dimensional representation,
you can figure out what should be here.
Probably when we shift by one,
we shift down by one,
then we multiply by omega squared.
So this is omega squared,
omega, and this is omega, omega squared.
I may have made a mistake,
but you can check that the two-by-two matrices here,
first of all, act just like the symmetry group S3.
They just multiply exactly like they should.
And another thing you should see,
Madour, is that this two-by-two representation
has no fixed point.
There is no invariant surface.
It's irreducible.
So as it turns out,
if you look at S3,
you decompose it here to,
you got two representation.
Forget, okay, let me not mention it.
We'll get to it in a second.
But anyway, here's a non-trivial representation.
That's not one-dimension.
Okay.
But you could have picked any other basis.
Yes, yes.
So then the only thing that would have happened
is that you may have gotten,
this you would get anyway, no matter what you do.
And this you may have gotten this one under some basis change.
It turns out, I mean,
in this case, it's not a problem to prove,
but we'll prove it in general.
So why was I supposed to know to pick that?
No, you didn't,
but you could apply the one-line proof
Damascus theorem, and some basis would pop up.
You didn't.
Especially good, I guess,
when Maressa was asking about these basis.
It's invariant.
No, but if I pick any basis,
the space isn't there yet.
No, no, no, no.
What do you mean?
So take another basis,
1 minus 1, 0,
and 1, 0 minus 1.
Right.
Okay, good.
So let's apply,
let's apply rho of 1, 2.
Sorry.
So let's apply,
so 1, 0 minus 1.
Okay.
So let's apply,
so these are the two you pick,
and let's apply rho of 1, 2 to this.
Yeah.
So you get 0, 1 minus 1.
0, 1 minus 1.
Which is the sum of the two bases.
The sum of the two bases.
So in this case, maybe any would do.
Any basis for...
Oh, sorry.
Okay.
Is it the same subspace?
If not over a...
If we were working over some finite field,
we would have a problem.
We would have...
Yeah, over the...
So it's not any subspace,
but once you pick the subspace,
any basis is there?
Yeah, yeah.
The subs...
I'm sorry.
You are right.
The subspace.
Yeah, the basis doesn't matter.
The subspace,
well, yeah, it will be invariant.
Sorry.
Here the subspace is uniquely defined,
the orthogonal complement,
and there is no problem with it.
It is a subspace for which the sum of coordinates is 0.
I'm sorry.
I take it back.
Maybe the nice thing about these bases
is that all matrices are either diagonal
or they're opposite diagonal?
It doesn't matter.
It's an example that you...
Just to see this process in action.
It's...
Some people like examples and others don't.
Okay, good.
So...
Any more questions?
So I want to...
Yeah, so I was hoping to blend enough applications
in the middle of this, you know,
mini course on representation theory,
but I can't do even the first one
before doing, you know,
doing the full prime decomposition.
I mean, it's not that I could.
I could do it without any of...
But anyway,
it will be more appropriate to do it
after we have decomposed...
We found the prime decomposition
of all representations.
So let me do it.
So...
So there's two questions we asked.
What are all the primes?
Are they finite, infinite?
Certainly they are infinite number of representations.
So even for a given fixed group,
infinite number of representations.
So we want to know what are the primes,
are they infinite,
and we want to know whether the decomposition
is unique or not, right?
And it turns out that the answers to all questions
come from just one simple lemma.
It's as simple to prove as Maschke's theorem,
but it's called a lemma,
and this is Schur's lemma.
And everything follows from it.
It's sort of amazing.
So you take two irreducible representations.
So basically the message of Schur's lemma
is that the space of representations
is extremely rigid.
There is no room for much.
And I hope it will be clear.
So we take two irreducible representations.
So rho and power irreducible.
Okay?
They may be of different dimensions.
So in general, I'll use d rho to...
Right?
I'll use this notation
when I need the dimension of representation.
So there are two irreducible representations.
And suppose, later we'll see where these things come up,
but suppose we are in the following situation.
We have some linear transformation,
some matrix A,
and it's of the appropriate dimensions,
such that...
Okay?
This is the same.
This is an assumption.
Okay?
This is the same.
I could have written it.
Sometimes it's more convenient.
So when you sort of pre-multiply A by rho of x
and post-multiply it by tau of x,
or the inverse,
then you always get A.
So it's always fixed.
It looks like a very special condition.
We'll see later that we can generate it easily,
but let's just assume that there is such matrix A.
In this case,
and these are irreducible representations,
in this case,
we completely understand this matrix.
What does it mean?
We have either...
Oops.
Either...
Okay, sorry.
I forgot to make one definition.
So given what we've done,
this basis change,
let me...
Let me just say here...
Let's draw...
The two representations are the same.
If there is a unitary...
z such that rho is z.
Okay, in particular...
What?
What does it mean to be unitary?
It's finite field.
It's finite field.
It's an orthogonal matrix.
A matrix who's...
Does it matter that it's unitary, not inverted?
Yeah, it doesn't matter.
It's invertible.
You know what?
Invertible.
It's unitary, it may matter later.
Yeah, invertible.
So...
At this point, it's invertible.
Okay, so...
So if tau is not
the same as rho,
the only way this can happen
is that it...
There is nothing...
This cannot happen.
The only case it happens is A equals 0.
Well, okay, yeah.
There's...
Yeah, either doesn't...
I meant there are two...
You know, this theorem has two cases.
One case is that tau is different than rho and...
And then the other, let's write tau equals rho.
Okay?
tau equals...
It's invertible out of 0.
Yeah, and then A is invertible.
No, no, but that's not all.
It's much more rigid than this,
is what I'm going to say is absolutely crucial.
Of course, you're right.
It's invertible out of 0.
And something stronger than both
is A equals lambda i.
For some lambda.
It's what they...
It's called the homo 30.
It's just a constant times identity matrix.
What?
A is identity and tau is the inverse of rho.
No, they are equal because here there is an inverse.
Okay?
For the second part, you need the field to be algebraically closed.
For the second part, you need the field to be algebraically closed.
That's true.
I'll say...
Yeah, so this...
So, when you are working with...
Maybe that's a general comment.
There'll be lots of...
Yeah, it's good to have some representation theories in the audience.
Who can speak Hebrew also.
When you are working...
So, most of...
As I said, most of what I've said,
just if you want it to be absolutely true,
although I'm sure there'll be mistakes there also,
then think about the complex numbers.
But when you are working over finite fields,
to do this, you often have to go to the algebraic closure of the fields.
So, you always should think that you are working over an algebraically closed field,
and the characteristic of the field does not divide the group for this theory to work.
So, let's just see the proof and see where this comes.
And it's just basically the definition.
This is justifiably called a lemma,
but it's a foundation of everything.
And why is it so?
So, I'll just state it.
I mean, you can just stare at it and check.
It's the following.
So, you look at this equation,
and you see immediately that the kernel of A is tau invariant.
Everything that makes zero has to remain there,
so that the zero will be on the right-hand side.
And similarly, the image of A has to be row invariant by the same reason.
And so, I mean, you see immediately that A has to be a square matrix if you want it.
Well, we know that there are no non-trivial invariant subspaces to these representations
because they are irreducible,
so each of these has to be either everything or zero.
So, the only way it will be non-zero is if it's invertible, right?
And if it's invertible, already we know that tau and row are the same representation.
They are equivalent representations.
And now, in the case they are the same representation, how do we see this?
Well, A has some, so A is an invertible representation.
If we are over a field of characteristic zero, then it has some eigenvector.
And now you look at the, so suppose,
so now we assume that A is invertible.
So we are basically done with the first case.
And A is invertible, and lambda is some eigenvalue.
Then you look at the kernel of A minus lambda I,
and it's also, you check, this is also an invariant subspace.
And the only way this would be an invariant subspace is if,
so this invariant is that, well, now row equals tau and it's all invariant.
The only way this can happen is that, okay?
Okay, so.
Could you slow down and just go through that?
Yeah.
So why is the terminal of A shall invariant?
So let's look just at the kernel.
So suppose you have some V, so that A V equals zero.
Okay, so you have some vector in the kernel, so A V equals zero.
So V is in kernel A.
That means that A V equals zero and so row of X for every X.
But this has to be the same as A tau of X V.
So this has to be zero.
How can this be zero?
If tau of X V is in the kernel of A for every X.
So it's invariant.
And then.
And the same on the other side.
Let's not do it.
Okay, so the only way, so each of these, therefore, has to be the whole space
of the two different dimensions.
Well, of course, if it was rectangular.
Zero.
What?
Either the whole space or.
Oh, zero.
So zero is a case.
Zero is a case.
Zero is a case we take care of.
So.
So look at the image.
Is the image zero or is it there?
If the dimensions are different, then.
Okay.
So if the kernel is the whole space and the A is zero.
Yes.
So the kernel has to be zero, so A has to be zero.
That's right.
And also the image has to be everything for the same reason.
Otherwise it was zero.
Right.
So that means that they both have to be the same.
So rows and columns have to be the same dimension.
And A has to be invertible.
That's the last option.
Right?
So, but if there, if it is invertible, then you see that.
Then you see that the row of X is A tau of X, A inverse.
And then they are equivalent.
Okay, then they are equivalent.
So we are down to the equivalent case.
So let's assume they are equal.
And then you look instead of the kernel of A,
you look at the kernel of A minus lambda I.
That's also an invariant.
How do you go from a full one to equal?
No, you just, I mean, might as well change basis.
You don't go.
You just say they are equal.
But if you change the, one is the change of basis of the other.
That doesn't mean there's a change of basis where they're equal.
I didn't.
Yeah, you, you change.
So it's, it's, it's just raffle.
It just, this is a second.
I didn't say that when they are equivalent, then it's,
I just said that if they are equal, just if they are equal,
if now you take it to be.
I think if they're equivalent but not equal,
then A would be something similar.
Yeah.
If they're equivalent, I make them equal and then I claim this.
That's the only claim.
That if they are equal, then.
If they are not equal, the state don't exist.
If it's a clear, then it's just unique half the scale.
Yeah.
But, but rather forget, I didn't say, this just says
that if they are equal, then you have this.
But then there's a missing case.
It's not, yeah, yeah, fine.
Let's say there is a missing case.
Okay.
Yeah.
The only way A is not zero if they are equivalent.
Okay?
First of all, that we established.
And if it's not zero, it's invertible and they are equivalent.
And now I want to look what happens if they are equal.
If they are equal, we have this.
Okay?
That will be sufficient for our purposes.
Okay.
So why does it give everything?
So I just want to explain this.
I'm going much slower than I planned, of course.
I always, always do.
So, yeah.
Another way to say that A has to be constant.
So if it's zero and zero, it's not some other constant.
Yeah?
Yeah, but I mean, the zero case may be rectangular.
The constant case has to be square.
It's not.
So why is this, where is the utility of this?
Where is the magic in this lemma?
I slipped.
So, Olga, you, sorry.
More questions.
You can ask more questions.
I just want to make sure that you listen to me when I continue.
So why is it such a great lemma?
Because, you know, you have to find the right, the right A to apply.
Well, where do you find such an A?
They are not, you know, they are not obviously around,
but in fact, you can take any matrix B.
Take B to be any matrix that you like.
Okay?
And just write A to be the sum of X B of X inverse.
Okay?
Just define it.
Okay?
And of course, A satisfies this condition.
So if we call this condition star.
Right?
Because, I mean, when you pre-multiply it by a particular,
let me call it Y, so it's not confusing to there.
Y, Y, Y.
And you pre- and post-multiply it by row of X and tau of X inverse,
you get the same sum.
Okay?
So this is the standard conjugation trick.
Okay, so A satisfies star.
So we can apply, we can take any B we like, do this trick,
and then it satisfies.
Why is this useful?
It's useful because we can now establish the orthogonality relations
of the entries of the representations of this.
Okay, sorry.
There's one more thing I need to do.
But anyway, this is how we will apply it.
So actually, let me just do it here.
It doesn't matter.
So what B should we take?
Well, we take a generic B, or if you want,
just span all generic Bs by just a different, you know, by some basis.
So let's just take B to be something which is one in the JK coordinate.
And zero otherwise.
And we'll do it to all J and K.
Okay?
And now, suppose we look at AIL.
So we look at A at some entry of A.
A is defined this way from B.
Okay?
So let me just try to say what I'm, maybe, before I do it,
I'll explain what I'm doing.
We have these two representations of the group G.
One is rho, and one is tau.
And maybe they are different.
Okay?
What I want to show is that when you look at,
so it's a vector of matrices, and that's another vector of matrices.
But here, this vector of matrices,
you can think of it as if this is D by D.
These are really D squared vectors.
You take this, this is one vector, and there's another vector,
another vector, D squared vectors.
And if this is D prime, then you have also here vectors.
Take a particular coordinate and look what the value is for all X.
It's a vector, you know, whose length is,
it's just a function from the group to the field.
And what I want to show you is that all these vectors,
that are not obviously,
well, essentially all these pairs of vectors are old or not.
Okay?
Except the ones, except when you take a vector with itself.
That's what is going to happen.
So that's what we want to show.
Okay?
So you take any two irreducible representations,
you look at any entry of one,
and you look at any entry of the other,
or even of the same, and we'll see.
And, you know, you take the inner product and you get a zero.
And this will follow from Schuyl's lemma.
So it's clear what I want to do.
It's clear what these vectors are.
So I'm going to look at rho ij.
So rho ij is a function.
And I'm going to look at tau, maybe rho i,
I'm not sure which, how did I pick the indices,
whether I picked them right or not.
You see, I think, rho ij and tau kl.
And I guess now I'll work over the complex numbers.
I want to show that they are orthogonal.
Okay?
So that's, that's what we are doing.
We'll see when they are the same.
We'll see, but we'll just take two of these vectors
and try to understand what their inner product is.
So, sorry, what are the vectors are?
You look at the ij place in each of the rows,
in each of the row x.
Yes, exactly.
You look at the ij place.
So this is, you look at one coordinate in all these matrices.
And that's a vector of length n.
Yes.
And there are two vectors of length n.
And we want to show that basically all inner products are zero.
Okay?
So how does it work?
So we are here.
So I pick B to be the matrix that in the jk coordinate is one.
And I look at this sum.
That's the matrix A.
And what do we know about A?
Let's first assume that rho is different than tau.
Okay?
And we'll later look at what happens when rho equals tau.
But then, you know, when rho equals tau A, you know, everything is zero.
A is zero.
A satisfies tau.
So it must be the zero matrix.
But what has happened here?
What is happening here?
Here we get, when we look at this product with this particular B, we get exactly this inner
product that's written there.
So this is basically rho ij inner product with tau kx.
And it's zero.
I, now, you know, you have to conjugate and you have to take inverses.
Basically, that's what you get.
So all of them are orthogonal if tau is different than rho.
And, moreover, even if tau equals rho, even if tau equals rho, the only case we get non-zero
here is on the diagonal, right?
And so we know that, you know, in this case, rho ij and rho kl is zero unless ij is the same
as kl.
i equals k and j equals l.
Unless it's the same.
Because of the transpose and the inverse.
It's, you know, I say, if you do the same vector with itself, you get non-zero.
And if you, in fact, we know exactly what it is.
So let me write it down.
I'm not going to do this calculation because it just will be confusing with the, it's one
line calculation that you'd better do at home.
Yeah, you have to pick the right, but you understand the trick.
That's the important trick.
Sure, Slema tells you that this matrix is either the zero matrix or it's almost completely
the zero matrix except the diagonal.
And then you get all these orthogonality relations.
And moreover, what you get in this case, unless, you know, they are equal and in this
case rho ij, rho ij is going to be exactly one over the degree of rho.
And that's another calculation you can do because you know, you know this structure.
So you know, basically, you know the trace.
But anyway, that's a small calculation.
So the point is, the point is that when we have irreducible representation, now there's
not so much freedom.
You can see that, you can see that, you can already see that there can be an infinite
number of irreducible representations.
Why?
What?
Yes.
So if, you know, if rho 1, rho 2, rho t were all, you know, maybe they are, you know, were
all irreducible representations, sometimes people write it short and irreps.
If they are all irreducible representations, that their dimensions are d1, d2, dT, then we
are already manufactured.
How many pairwise orthogonal vectors?
How many?
All right?
That we already proved, so we already know that there has to be a finite number of irreducible
representations.
In fact, it will turn out that this is, there is equality here, so let's do this.
But is this clear?
Is this also true in the finite future?
Yes?
Yes.
Yes, but it's true.
Yes.
It doesn't follow from the argument.
Because from every representation of dimension d, we got d2 pairwise orthogonal
vectors, and they are also orthogonal to anything else of any other irreducible
representation.
Right?
So that's it.
There's no more room for, so there can be only finite number of primes.
So we know that they are finite, so, you know, we just have to find them, and now we are
going to find them.
Okay, so far?
Yeah?
Did you assume that they are equivalent, or this was thought for, and for that they are
equal?
Yeah, so they are, yeah, they are non, pairwise non-equivalent.
So wow.
Wow.
Okay, you know.
I don't distinguish equivalent ones.
Okay?
Yeah, you don't.
It's just, like when you work over the integer, it's a good analogy.
You work over the integers, you don't distinguish p and minus p.
You work over positive and negative numbers, and you factor.
That's the same.
Okay, it's the same.
It's just a group of symmetries.
It's a little richer, but, you know, it's all invertible transformations.
Okay, so now we want to find all of them.
And you'll find all of them in exactly the place you would expect, maybe.
And that's in the regular representations.
In the regular.
Okay, what's the regular representation of G?
It's the one we started with.
So it's, I'll call it R, because it, our effect is this matrix that we before
hadn't called pi of x.
It's just a matrix that in position y and z, you have one, if and only if,
I guess, x, y.
So,
inverse is x, y.
So, y, z, inverse is x, I guess, yeah.
Okay, so just the matrix that corresponds to multiplication,
to left multiplication of the group.
Okay, so that's ours.
So what we did before, we took an arbitrary representation and started
decomposing it.
We can do it, in particular, to this representation.
Right?
So let's assume that we decompose it.
From what you just said, this is much too big,
much too high dimensional to be irreducible.
That's right.
Every irreducible has to be approximately a period of time.
That's right.
It's much too big to be irreducible, so it will be reducible,
and you start to reduce it.
You also know that it's, for example, the vector all 1s is for this representation.
So you start reducing it, and you get,
so we said that these were all irreducible representations.
We saw that they were finite.
They are all of them, all in equivalence irreducible.
And you write, so that means that r will be the direct sum of,
well, like with the prime decomposition.
Let me write it additively, so m1, rho1 plus m2, rho2.
And this is just short-hand to writing rho1, rho1, rho1m times.
Right?
Okay, so we're going to just prove that,
so that's the composition we made.
Maybe it's not unique.
We'll show that it's unique, but it's one such representation,
such decomposition of the regular representation,
this representation, to a sum, a direct sum of irreducible representations.
Here are all irreducible representations.
Now we want to deduce something.
And what we can deduce from what we just did from,
well, I'll do a little more.
We need just a tiny bit more.
At least I want to get to the Fourier transform, but,
okay, which we are all almost there.
So we decompose the regular representation,
and now the claim is that all,
in fact, I'll just write it for this.
So mi equals di for all i.
So remember, di's are the dimensions of these representations.
So in particular, they are positive, right?
They are integers.
So positive integers.
So all of the irreducible representations appear in the regular representation.
In fact, each one will appear exactly with its dimension, number of times.
And then we'll get, yeah, this will imply that the sum, yeah.
Right.
So how, okay.
No, let me delay this comment too later.
So let's prove this.
Everybody is happy?
Yeah?
No matter what G acts on there.
No, we look at G acts on G,
the regular representation G acts on G.
Yeah, but when the previous example S3 did not act on S3.
Yeah, we took some,
it was some other representation that we decomposed.
But they still the same, right?
These will appear at both cases?
So whatever representation you take,
the irreducible components will be among the ones that appear when you decompose.
Yeah, they contain, they contain all the prime.
They contain them, in fact, in this, exactly this number.
Okay?
And this again is really simple.
And for this, we just need to do one more thing and look,
this is to look at the traces of the representation.
So trace is a very important operation.
So we'll prove it in a second.
This is, well, maybe I should erase my plan and therefore,
you won't see all I promised.
Let's do it next time.
So, if we have, if raw is the representation,
then we can look at Cairo,
which is the, you know, the character of,
it's called the character of raw.
And the way it's defined, it's very simple,
is if we have Cairo of an element X,
it's simply the trace of the matrix of X.
Okay?
What can we say about these characters?
Well, anybody wants to say something about the characters
of irreducible representation?
Let's say that we have raw and tau irreducible.
What can we say about?
What?
They're orthogonal.
They're orthogonal.
I mean, this is, so if, for example,
tau is not the same as low,
then this is zero.
Because what is it?
It's just, you know,
trace is just the sum of the diagonal elements.
Inner product is linear,
so you just do it entry by entry for,
you know, each of the diagonal elements.
So this is zero.
But just by the orthogonality of these vectors,
raw ij, and we get that this is zero.
And moreover, another thing that we get from this part,
from the situation that it is the same vector,
and we look at the diagonal vector,
the other thing that we get is that chi-rho.
So let me, let me define,
I didn't define inner product, but I should define it.
So if we have two functions on f and g from g2,
let's say c, let me define fg,
just one over n,
c of x,
and maybe we need to take the conjugate.
So I just take the average,
that's one convenient way to define inner product,
in which case we get t1.
So they are really orthogonal.
They are really orthogonal.
There aren't so many of them, right?
I mean, maybe there aren't so many,
there may be less than n of them.
This number t,
just because the sum of the di squared is n,
so if some of them are bigger than one,
then there may be less,
t may be less than n.
In fact,
what is the case that t equals n?
Maybe it's a good thing, too.
So we know already the quality here.
So t equals n, if and only if g is a billion.
So why is this?
Yes, matrices all commute,
because they all commute,
there is a way to diagonalize all of them simultaneously
by invertible transformation,
and then you get all of them are one.
And that's the only case,
because if the group is not a billion,
then they don't simultaneously commute.
Okay, so the number of characters here,
and we'll get to this maybe in a little bit,
there may be a few of them,
maybe fewer than one of them,
but they are all orthogonal.
So why is it relevant?
So let's look at this representation,
and we know that
we have this formula,
and we can write this formula also for the traces,
like we can take the trace of this matrix,
the trace of this matrix on.
We can just take traces within our operation.
So we know that
Chi R, the trace of the regular representation,
is
M1 Chi,
let me call Chi I to be Chi OI.
Okay, so we wrote,
in particular we have this,
decomposition of the character
of the regular representation.
Now, somehow we know,
we know something about,
so maybe you should tell me,
what is Chi R of X?
What is the trace of the matrix,
which is a regular representation
of the element X in G?
Well, what is it in the case that X is 1?
It's an identity matrix, right?
It's always, yeah,
and if X is not 1,
it's 0, because X,
multiplication by X moves every element.
Okay, so
if we
now look at,
so let's look at,
now it's, where should I move?
Let's move here.
Okay.
Remember, we want to prove this theorem ourselves.
You already see the proof, right?
Yes.
Well, I take an inner product with kaya.
Yeah.
So, like you said,
kaya,
kai,
well, it's just this sum m i,
m j,
kai j,
kai i,
which is biotogonality,
just m i,
right?
Yes.
So on the one hand we get,
but what is this?
This is just 1 over n,
the sum over all X,
kai r of X,
kai i of X,
right?
But kai r of X
is non-zero only once,
right?
It's just,
we just have to look at the identity element,
and the identity element,
so kai r,
kai r of 1,
kai r of 1,
kai i of 1,
what is this?
This is n,
what is this?
What?
Di.
Di,
this is the identity element.
Di,
and this,
well,
over n, I guess.
So m i equals di.
If you want to conjugate kai r of X,
it's any way of doing this.
Yeah, it could.
Yeah.
Okay.
So all the,
all the elements,
you know,
all the irreducible representation,
the care when you factor the,
the regular representation.
Okay.
So,
I think I'm close to an application.
Well, let me just,
Is this also true in the final future?
Yes.
Yes.
No, but yeah,
it doesn't,
it's,
whenever the characteristic of F,
you do everything in the algebraic closure,
despite the fact that you work over the algebraic closure
or over the complex numbers,
all the entries of all these matrices
in the irreducible representation
will always be algebraic.
They will also always live in a finite extension
of your base field,
just because,
you know, general nonsense.
So,
So,
No, no,
you'll get,
you'll get that,
no,
you will get exactly,
you'll get equality.
Yes.
Yeah.
You'll get equality.
So,
so,
okay, good.
So we know,
we know this,
and now I want to
write the Fourier transform
and maybe do one application.
Oh, we have half an hour.
No, that's not bad.
We didn't take a break.
Okay.
So everything is clear so far.
So we proved that,
by the way,
this proves uniqueness also,
not just,
you know, this,
you know,
if there were two,
two ways to write down the
regular representation
as some of irreducibles,
we'll just get a contradiction
when we take inner product
with the different chi-eyes.
So we got
uniqueness,
we got that the regular representation
contains every,
every possible irreducible representation
and the number of times it occurs
is exactly di,
if the dimension was di.
So
this will be,
so towards the Fourier transform
of Fourier analysis
on non-Abelian groups.
What we get is the following.
So we took the regular representation,
that was
N
and we decomposed it.
So this,
we decomposed it,
it means we changed basis
and we got
etc.
We got this decomposition
and this was
row,
this was row one
or two
or T
and the dimensions were d1.
In fact,
yeah,
I didn't write it down correctly.
If we have any representation
here of dimension di,
it will occur
if row one
has size d1,
then there'll be
d1 copies of row one.
And then we'll continue.
Okay?
So that's the picture.
Every representation of some dimension
occurs this dimension number of times.
This works perfectly
to give us the sum of squares
is N,
so which we already did.
But
another thing it does is,
okay,
so I want to talk about the group algebra
and let me do it here.
We have transformed
and the group algebra
goes hand in hand,
so that's what
we'll do now.
So suppose we have
okay,
just,
well, okay, fine.
A group algebra.
A group algebra
is denoted
in this notation
and it just
points out that
vectors,
so basically functions
from G to F,
they fall not only
a vector space,
but there is multiplication
that you can endow them with.
How can you endow them
these functions
with multiplication?
Well,
you have multiplication of the group elements
and you just extend linearly,
right?
So in other words,
if we have F and G
two functions,
we write their product
which is denoted
by convolution,
simply as,
so that's what happens
when you extend linearly
the multiplication of group elements
to multiplication of functions.
This at point X
will be the sum of all Y
F of
X, Y,
Z, Y, U.
That's just a standard convolution.
So
you have
a vector space
or you have a set
that has addition and multiplication
on it.
It's an algebra.
So that's why it's called
the group algebra.
These are the
elements in it,
just these functions
with addition and multiplication.
What are the
Okay,
I want to ask this.
What's the dimension
of this algebra?
How many generators does it have?
It's an algebra.
Yeah,
how many elements,
you know,
if I
take a set of
elements,
so N, yeah,
take a set of elements
or if you add them
and multiply them,
you'll get all these guys.
So
the dimension is N.
The dimension of this algebra.
So,
and when we look at
this,
no,
you'll get,
by adding them with coefficients
in F,
you can get any function.
Yeah.
Yeah.
You couldn't
give you a lot of multiplication
when you couldn't get
everything.
No,
I don't have multiplication
in F,
multiplication just in this sense.
Multiplication less.
So,
so really important
message is that you should,
we should think not only of
function,
we should think not only
of elements in the group
as matrix says,
like these ones,
like the regular representation.
We can think this way also
of every,
every function.
So we,
what we have done
with the regular representation,
we represented only
elements in the group,
but we can represent
any elements in the
algebra of the group.
And
the way you do it
is that this R of F
in coordinates
X and Y
is just
in coordinates
X and Y.
You write here F of X, Y.
Okay,
why is this,
first of all I claim
that this is consistent
completely with the
definition in the case
that F was just
one on one group element
and zero on,
on the rest.
But
if you think about this,
what,
what is this matrix?
This matrix just,
let's look at the first column,
the first column Y
is,
let's say
we put the identity
here and here.
The first column
is just F.
Okay?
And then the other columns
are just permutations
of F.
Right?
So there are
permutations of F.
And
because,
well,
because of the definition
I guess,
what you get
is that,
well,
what do I want to write?
So suppose I want to
look at the regular
presentation of F.
What do you think
it would be?
I want to,
to think of this multiplication
as,
as we should,
as the, you know,
convolution.
To me at least,
it's always confusing
and you have to remember
the indices and so on.
What I,
what I'm saying is that,
you know,
functions should not be
thought of as functions.
It should be thought of
their,
the matrices that represent them
in exactly this way.
So it's a redundant form.
Yes?
It's just,
so what happens
for group element
extends linearly
and that's,
that's true for every two functions.
And this is really great
because that's,
well,
you have
all functions represented
as matrices.
You have the algebra
represents,
represented as a normal
algebra that you are familiar with
and matrices
that you know how to add
and multiply.
You don't have to think
of strange
convolution.
I don't know if it's,
anyway.
Yeah,
this is the regular
representation.
I'm talking now about
the regular representation.
When we talk about
the group algebra,
we represent it
using the regular representation.
It's a,
and now
will come something
that is already explained,
but let me
just say
what it is.
This algebra is n-dimensional,
so that it's
spent by n matrices,
but it has n-square
degrees of freedom.
So in the normal way
we represent,
we represent it,
we are doing something redundant.
However,
when we move to the,
you know,
when we change basis
and we get
it in this form,
in this form,
right?
We see really
all the,
you know,
we see the algebra
in its sort of
naked form
or a beautiful form.
Namely,
so the same
change of basis
that moves R
to the sum
of irreducible representation
does the same
to all these matrices,
just their linear
combinations
of the others.
So it does the same
to all of them.
So,
so let's call this
change of basis.
This Z,
let's call it
the Fourier transform.
So we know
that there was a linear transformation
taking the regular representation
to the,
this diagonal form
of irreducible representations.
It does the same
to the group algebra.
So this is a Fourier transform.
So,
I want to write it
let's write it,
let's give it a name,
so f hat
of rho.
So if rho is an irreducible representation,
so f hat of rho
is the coefficient of,
the Fourier coefficient of,
let me just say again,
we had here a matrix.
So here was f
and the other permutation of f.
This was r of f.
We take it
via this
change of basis Z,
which is,
we call the Fourier transform
for a good reason
because it's exactly
the generalized,
so in the case of a billion group,
it is the Fourier transform.
It's what diagonalizes
the matrix.
Yes, r of f is the regular,
r of f is the regular representation
of the function f.
It is.
I didn't change the group.
The group is g.
The group is g.
I look at all functions on g.
Yeah, I'm not,
I'm not moving to a representation
of f of g.
I'm just saying that
I extend the representation,
what I call the regular representation
of g,
linearly to functions on g.
Okay, I do it in this way.
That's the definition.
This is the definition.
So the matrix corresponding to a function,
it just put the function
and its permutations on the column.
r is defined here.
No, but it's not just,
yeah, it is defined here,
but notice that r of x
is r of 1x.
That's it, right?
So it's just an extension.
It's just an extension
of the definition of r
to point functions,
to now it's defined over, you know.
It's linear.
It's linear.
Yes, it's a sum,
the weighted sum.
Yes, exactly.
So if we had the matrix,
then we get from it
these block matrices
and what we call the Fourier transform
of f is the values,
these matrix values
in these places.
Okay?
So we just get what we call,
so r of f,
it just,
this again extends linearly
because this is a linear transformation,
so it's just f of x.
4 of x,
let me rewrite it
if I do inner product
as an average.
So it's just extending linearly, right?
This, for every x,
for every,
f is a linear combination of x
with this value f of x.
So now the value in this little matrix
will be f hat of rho
if this corresponds.
Is it the same value
in all the different matrices
that correspond to this?
Yeah, for every, that's right.
For every,
if we have a repetition,
if we have the same
rho repeating several times,
we'll get the same matrix several times.
Okay?
No, you couldn't,
but because you're supposed to have
n degrees of freedom.
Yeah, that's exactly.
If you use different values
for the same repetition.
No, no, no, no, I don't.
I don't, no, notice again,
maybe this should have,
I should have stressed it.
All right, all right.
Every representation occurs,
every representation,
rho one of dimension d one
occurs d one times.
This, this works to make sure
that the sum of square zero is n.
But in terms of degrees of freedom,
you take just one of them.
This is d one squared
and then d two squared and then,
and indeed that's,
that's the way to see that the,
well, actually there is a theorem
that I should state somewhere now.
I start getting a bit messy on the board.
It's not clear that it continues
reasonably.
So let me state this before we proceed.
So that's the Wetterburn decomposition.
It just says that f of g,
again we are in the semi-simple case,
is, so we again have rho one up to rho t
are the reducibles.
Of dimension v one up to dt
is the direct sum of m di of f.
Well, I guess f is algebraically closed in this.
Okay, so we have this decomposition.
We take rho one once,
we take rho two, rho two once and so on.
Each of them is, you know,
using this extending the regular representation
to functions and then diagonalizing it
according to the Fourier transform.
Then we get little matrices for every f.
These are the little matrices, this guy.
For every representation,
we can look at all possible functions
and see what value we get there.
What the Wetterburn decomposition tells us
is we can get all possible matrices.
All possible matrices.
f of g, this matrix algebra,
decomposes into the direct sum
of full matrix algebra.
So in every such square,
when you run over all functions,
when you run over all functions f on the group,
you get all possible matrices.
And this completely explains
the dimension of the algebra.
This algebra is dimension n,
but these are n by n matrices,
so they are obviously special.
In fact, we see that there is redundancy here.
If we have a vector here,
then it's just permuted in the other columns.
But after this change of basis,
we see completely where the n degrees
of freedoms are coming from.
There are t full matrix algebras.
They are all dimension di.
These n degrees of freedoms are coming
from exactly d1 squared
for the first representation,
d2 squared for the second one.
They are full matrix algebras.
So questions.
Yes, it's less than n.
So that's very good.
If you remember,
when you look at every matrix,
when you look at the different coordinates of the matrix,
the vectors that they represent
as you move over the whole group,
this rho ij,
are an orthogonal system of n,
of exactly n orthogonal vectors.
So that's good.
They decompose the whole space of functions.
But when you look at them at matrices,
you have fewer,
indeed, you have fewer than,
in general, you have fewer than n.
Each of them is a matrix.
So totally you have all the information,
but it's represented differently.
That's right.
Okay, so yeah,
I'm just wondering
if I can do for you one application.
Yeah, fix the base.
You're right.
Yeah, that's right.
It's not canonically defined.
For each irreducible representation,
it's never canonically defined.
So you pick one basis for each of them.
That's right.
Yes.
So...
Yes, in fact,
I wanted to point this out.
That's very good,
but I was going to point it out
just immediately after I do the characters.
But anyway,
it's a great question even now
to ask you
what is the missing representation of S3.
So S3, yeah, the sign, right?
Yeah.
So the missing one,
the missing representation
is what sends an element.
It's a one-dimensional representation
which sends a permutation to its sign.
So it's either 1 or minus 1.
And so it's one-dimensional.
We have 1 squared plus 1 squared
plus 2 squared is 6.
So you can actually...
Knowing these facts,
in fact,
the orthogonality of characters,
we can...
Well, finally,
okay, maybe I should say
computational complexity.
If I give you a group
represented by a truth table,
just the multiplication table of a group,
it's not completely obvious.
In fact, it's not obvious at all
how to compute the irreducible representations.
So irreducible representations
are known explicitly for very few groups.
But if you have your strange group
that you concocted,
then maybe it's not obvious
what its representations are.
But there is a polynomial-time algorithm
due to Baba and Ronier
to find all the irreducible representations
of a given group.
And so polynomial-time is polynomial
in the size of the group itself,
the truth table of the group.
You find any variant subspace?
You find any variant subspace
so that you can do it, right?
That's not the way you do it.
That's not the way...
Let me not get into this.
Maybe I can tell you about this,
about induced representations
and so on some other time.
I just want to try to show something,
and I'm not sure
I may have everything I need for it,
and maybe not.
I just want to show an application.
So let's see what we have.
I think with this Fourier transform,
maybe I'll need the inverse Fourier transform,
but let me just do an application.
Maybe I'll have to do it again next time,
but...
So I want...
This application is sometimes called
the Gower streak,
and sometimes it's called...
Well, okay.
Some people call it the Gower streak.
What is the Gower streak?
The Gower streak has to do with estimating
the rate of convergence of a random walk
on a group,
and, in fact, an inhomogeneous random walk.
So you have two random variables,
let's say two distributions,
S, P, and Q distributions
on a group G,
and what you want to estimate
is how quickly do you convert to uniformity.
What you want to estimate
is what happens when you take P
and then you take Q,
so you take a random step in P,
and then you think of them as matrices.
How fast do you convert to uniformity
if you know how close to uniform P is
and how close to uniform Q is?
You take...
So you pick an element,
you pick an element at random from P,
and you pick an element at random from Q,
and you multiply them.
Okay, and that's the distribution.
Okay, so...
That's L2 norm.
Thanks.
There are results on other norms, but...
So there's an obvious...
There's an obvious bound that holds for every group
that you can calculate,
and that's square root N here,
and what Gower's noticed...
Sorry, that's not Gower's, but this is...
So this result is, as stated,
is the result of Babai,
Nikolov,
and Pbert.
Is that you can write here,
you can shave a square root N factor.
So non-Abelian groups,
the convergence of such works is faster,
where M is M of G
is the minimal dI,
where dRaw,
Raw irreducible,
and Raw is not a trivial one.
So there is always, in all groups,
we have the trivial representation
that sends all elements to the identity.
If you forget it,
then look at the minimal dimension of a representation,
and that dimension is something
that you can put here.
Now, if you don't have any intuition about these things,
it may look...
Maybe why should you care?
This turns out to be...
If I really talk about this application,
I will need more time,
but this is absolutely critical,
because in several works,
because, well, I guess Gauss' motivation
was for arithmetic combinatorics,
well, I'll just tell you about it.
I will prove it next time, I think.
So let me just tell you a few things.
What?
It can be tight in the case,
suddenly in the case of M equals 1,
it can be tight,
and yeah, yeah, it's tight.
I mean, I don't know if for every group it's tight,
but for lots of groups it's tight,
for SS2P it's tight, and so on.
So...
Is this different if Q is, like,
into the k or something?
If Q is...
No, no, it's interesting also
to take the same distribution for both.
It's not...
It's just more general.
If Q is into the k, then...
Yeah, yeah, yeah.
Yeah, that's right,
and you can iterate this,
and you get an iterative one.
It just...
In many...
Or in some...
Pulse of expansion,
when you do a proof of expansion
of some random work on a group
or a caligraph
on some group,
what happens,
at least with the methods that we currently know,
which are arithmetic combinatorics
for...
I erased already,
but results like Gambo-Tzarnak
on several groups
and Gambo-Tzarnak and Schu,
and so on,
is what you can prove
using some methods
and some properties of the group,
is that you can converge
to a distribution
that's almost the size of the group.
That's, you know,
the tools work in such a way
that you can sort of...
Not easily,
but with different methods,
but easily...
Let me write it easily.
Prove
that...
Or get to a situation
where P minus U
is something like...
This I have to know,
it's something like...
Square, let's say.
It's something like
1 over N
to the 1 minus epsilon.
You just...
It's not uniform.
It just occupies about,
you know,
the main entropy is...
You occupy
N to the 0.9 of the space.
You have expanded
to occupy...
What?
P minus U squared
would be that.
You have to be...
Let's say you are uniform
on N to the 1 minus epsilon of the...
Then
that's going to be much bigger, isn't it?
No.
Because you're going to have...
You know, say you have
N over 2 elements,
where you have probability 0.
No, it's...
In general, if you have...
If you have, you know,
L to norm 1 over S,
then you are uniform on some S element.
You can think that you are
uniform over S element.
Right?
L to...
It's P minus U, right?
Yes, think of...
If you want...
This is this...
That's 1 over N.
It's...
Yeah.
Okay.
So...
I'm just telling you,
you can get to such a situation.
P squared is like the sum of the PI squared.
PI squared, yeah.
This is just...
What is P squared?
P is some probability distribution.
Some probability distribution on the group.
So, I'm not telling you
where it's coming from,
but I'm just telling you
that there are certain situations
where you can get
to a probability distribution
over the whole group,
which is not uniform.
It's not even close to uniform.
It's just, you know,
supported on N to the point 9
of all the N objects,
all the N group elements.
But then,
if the group is such
that this M is bigger than
N to the point 2,
is bigger than, you know,
N to the 2 epsilon,
then in just one more step,
you'll convolve these two distribution
with each other once,
and you are already uniform.
And that's...
Something that doesn't work
for a billion groups
and shouldn't work
for a billion groups,
and it's critical
for all these proofs like,
so I mentioned,
Burgen and Gambo
proving that in SL2P,
if you take two random elements,
they are expanding.
Let me say one more point
because that's basically the end.
Just one more point
about more generally how
the decomposition
of the group algebra
is related to expansion.
This...
I'll elaborate about it next time,
but let me just
spend five minutes on it now.
Since we are talking about distributions,
I just want to point out
that you know you are friends
of elements of this group algebra
when you represent them
in matrix form.
So let's assume again
that we have some distribution
on a group,
and the most typical distribution
we like to work with
is this distribution of...
We have some S inside G,
a generating set,
and we look at the scaling graph
of G with respect to S,
so we connect elements
if we can move
from one to the other
using elements in S.
And the probability evolution matrix,
the normalized adjacency matrix
of this graph
is a member of the group algebra.
So I claim that
the element of the group algebra
corresponding to S
is just 1 over S,
some X in S.
X,
which just means
that when we write the matrix,
the transition matrix
of this graph,
the normalized transition matrix
of this graph,
so S here is
the size of S.
If you write
the probability transition matrix
of this graph,
what we get is
Y in position,
I guess,
YZ.
We have S,
we have 1 over S,
so we have 1 over S
if YZ inverse is in S.
So this is a markup process,
the normalized adjacency matrix
of this caligraph.
And what do we want to know
about this caligraph?
Typically,
we want to know
that it's expanding.
What does it mean
that it's expanding?
We want to know
that except for
the top eigenvalue,
which is obviously 1,
all the other eigenvalues
are small.
How can we
know the other eigenvalues?
I mean,
in general,
it's a problem
and also in the case
of caligraphs
and groups,
it's a problem,
but we have this
look,
this matrix
is a member
of this group,
of the group algebra.
So this,
this is an F of G,
right?
It's a matrix
exactly of this form.
You have the
the entries
determined just
as a function
of YZ inverse,
right?
It's basically,
yeah,
this is the F,
but this is
the R of F of,
and now
after we diagonalize it,
the eigenvalues
don't change,
so we can look
at the eigenvalues
in the diagonalized form,
and then we can,
you know,
we can and we should.
You know,
so we just
simplified,
we move to,
here will be the one,
and here we have,
we have these representations,
and we look
for the eigenvalues
of this matrix
after the linear transformation here.
This helps
in many ways,
more ways
you know,
I'll explain,
but one particular way
that's related
to what I said here
about the,
where was it,
about this,
the way it helps here,
and that will be the end,
is that
every representation
occurs
the number of times
as its dimension,
right?
So what does it tell us
about the eigenvalues?
Let's assume
that all,
all representations
have dimension
at least m.
What does it say
about the eigenvalues here?
Each one of them,
each eigenvalue
that appears,
appears with multiplicity m,
and this multiplicity
of appearance
of the eigenvalue
is the source
of this m
in this,
in this result.
It's the source
of this kind of,
it's a trick
that appeared
many years ago
in a paper
of Sarnac and Schu.
They were the first
to point out the power
of this multiplicity
in analyzing random walks
and eigenvalues,
and they gave
an elementary proof
of a theorem of Selbert
using just,
I mean,
almost only this,
a weaker bound,
but this was a proof
that had
a heavy number
of theory in it
and a much simpler proof
was given
using this
eigenvalue multiplicity.
So,
ah, no, no.
The eigenvectors
are not the characters.
We'll talk
about the characters
next time.
Yeah.
So now questions, yeah.
So that's it.
There is a general theorem.
There is this bound
that every connected graph
has a certain eigenvalue gap,
and if you know
that all the eigenvalues
of multiplicities
at least m
then the gap is bigger.
Yes.
Yes.
It's something like that.
Exactly.
Yeah.
Basically,
if you do the trace method,
so you try to estimate
the eigenvalue by,
you know,
looking,
the second eigenvalue
by just looking
at the trace,
looking at the sum
of power
of the eigenvalue,
if you know
that
the one closest to one
appears many times,
then it couldn't be
too close to one
if this,
there is bound
on the number of walks.
Yeah.
So you gain by this.
That's exactly
where the source of,
but anyway,
I'll show you this proof.
It's five minutes,
but I think I need
another five minutes
of background,
and that will probably be ten.
And anyway,
we are not in a hurry,
so we will continue
next time.
Probably I'll do
two more times.
Thanks.
תודה.
