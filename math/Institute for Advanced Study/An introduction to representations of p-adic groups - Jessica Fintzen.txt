Thank you very much, thank you very much for the invitation to give this talk here and
thanks everyone for coming.
It's really great to see such a nice audience.
Always makes me happy to see this room build a diverse room.
So I was asked to give a long talk and just learned that we have 90 minutes.
So that means you can ask tons of questions.
So don't hesitate to ask if anything is unclear.
I was also asked to give a colloquium talk and so just a few words.
I know that this audience is very diverse in many different senses, in particular we
have undergrads and advanced graduate students and postdocs here.
So since this is a colloquium talk, I really encourage all the undergrads to stick with
it.
Try to listen until the end.
So for such talks, it might happen that you get lost at some point and then you keep listening
and then you understand things again.
So it's not a talk where each slide builds up on the previous one, but you will be able
to understand future slides even if you miss a previous slide.
For the advanced grad students, this means the first few slides might be super boring
for you.
So there are two options.
You can either sleep and manage to wake up on time.
The other option is you will likely in your life have to give a colloquium talk yourself
sooner or later, in particular when you give job talks.
So the other thing you can do is just listen to it and think about how would you give a
colloquium talk if you were supposed to give it to a colloquium talk?
What do you think works well in my talk?
What do you think doesn't work well?
And how can you give a better talk than I give it to you?
And you can also tell me afterwards.
I know how you can tell me that.
All right.
So that's enough about three stuff.
Now I want to tell you about representations of periodic groups.
And so this is very much related to actually the two talks we see, in particular Charlotte's
talk series, but also Anna's talk series, and Charlotte's more obvious than Anna's.
We learned in Charlotte's talk already what representations of groups are.
So the key thing we need to understand what is the periodic group.
And I want to tell you what does periodic mean.
So what is P?
P is prime number.
Which is your favorite prime number.
And then we can define a periodic absolute value as follows.
We take an integer, write it as P to the s times r, with r and p co-prime.
And then the periodic absolute value of that integer is 1 divided by P to the raise to
the s power.
So that means the periodic absolute value measures how often an integer is divisible
by P.
So if an integer is very often divisible by P, then the periodic absolute value is time.
And so this allows us now to define what the periodic integers are.
So the different definitions as the periodic integers, which is denoted by z sub p and
z p, is the completion of the integers as you know, then by this new absolute value,
this periodic absolute value.
And so that means that the periodic integer is just of this form here.
We can write it as a0 plus a1 times p plus a2 times p squared and so on, where these
coefficients are a0, a1, and so on, are some integers between 0 and p minus 1.
And this, well, if you stop it anywhere here, if you stop here and just calculate, add these
numbers up, we get an integer that we can go on to infinity because this will converge
because higher powers of p have smaller periodic absolute value.
So that means the new thing that we're adding to the integers is that we allow the exponent
here to go on to infinity and keep going.
And then we can define the periodic numbers in two ways.
We can say either it's just the fraction field of the periodic integers and we write it as
q p, or as the notation indicates, we can also start with the rational numbers q and
complete those with respect to the periodic absolute value.
So that's the formal definition.
Maybe it's a bit confusing if you see it the first time, like how does this object look
like?
I want to give you an example.
So here are the three other integers.
Here's a picture of how they look like.
I said they are power series in three, and so let's see, what is it?
So we could just look just at a0, a0 can be either 0, 1, or 2.
And the distance between say 0 and 1 is just 1 because the difference is not divisible
by 3.
So the periodic absolute value is 1 third to the 0.
So that's 1.
It's getting more interesting when we add a second digit.
So let's add a second digit.
So this number here, it's 1 plus 0 times 3.
So that's just 1.
Well, this 1, 1 stands for 1 plus 3 times plus 1 times 3, so 1 plus 3 is 4.
And so the difference between 1 and 4 is 1 is divisible by 3.
The periodic absolute value of the difference, that's 1 third.
So that means if two of these power series have the first digit, the same, and only the
second differ, then the difference is 1 third.
And that means we get these smaller triangles, it's here shown as triangles, which means
that all these integers here, that's 1, that's 4, and that's, I can't, 7, all these three
have distance 1 third from each other.
But anything in this triangle has distance 1 from 0, for example, or also from 3 or from
6.
Of course, the difference is not divisible by 3.
And with the second step, now we can keep going.
We can play that again and add a third digit, wherever you like to add a third digit, and
then you see the difference here will be 1 over 3 to the second power, so 1, 1 third squared,
so 1 ninth, and you get all these smaller triangles.
But again, everything in here has distance 1 from everything in here.
And now you can keep going, but I can't draw this anymore.
And you can imagine what happens, you just get a fractal.
So that's what the periodic integers look like, they look like a fractal.
So even though they arise as a completion of z, the integers, it looks very different
than the real numbers, and I want to contrast these two now.
So the real numbers, you all hopefully remember that these are just the completions of the
rational numbers by the regular absolute value.
And I introduced this real periodic absolute value that gave us then the periodic numbers.
And real numbers are this line, usually when you draw it, it's a line, while the periodic
numbers, I hope I convinced you, it's this fractal.
And so even though they arise in a very similar way, they're very different.
The real numbers, they are connected, it's just the line, while these periodic numbers,
they are totally disconnected.
You can see that in the picture very well.
And here's another crucial difference that takes a moment to realize.
So the real numbers, they have only one compact subgroup and addition, which is the trivial
one.
Because as soon as some real numbers say five is in your subgroup, then 10, five plus five
has to be in your subgroup, 15 has to be in your subgroup, 20, 25, and so on.
So it will never be complicated.
On the other hand, for these periodic numbers, there are infinitely many compact subgroups
and addition.
So for example, Zp, this picture here that we've seen before, that's compact, it fits
in this ball here.
It's something compact, it doesn't go up to infinity.
The same was p times Zp, so that's the smaller thing here, that's close to an addition, it's
a subgroup, and it's also something compact.
The same was p squared times Zp is also compact, to the 15s times Zp or one over p times Zp.
So there are a chance of compact open subgroups, there are infinitely many, and they even form
a basis of neighborhoods of the identity, because they're getting smaller and smaller
and smaller.
So they form a basis of open neighborhoods of the identity.
So that means it's very different from the real numbers.
And these compact open subgroups, they play an important role in the representation theory
that we don't see.
So now the next question is, I said, these are the periodic numbers, and I said, oh,
it's going to be on my title slide, I want to study periodic groups.
So what are periodic groups?
So first I would like to remind you there are some objects called real league groups.
So for colloquium talks, you usually assume that people have some basic understandings
of what's at the level of qualifying exams, if you're an undergraduate, you might not
have encountered necessarily league groups yet.
So what you really should think of is GLN over R. So the N by N matrices that are invertible
over the real numbers, that's a real league group.
They can also build nice subgroups of that.
So really the groups I want to look at are nice subgroups inside GLN of R. So for example,
the group SLN, all the matrices of determinant one, that's a nice subgroup.
Subgroups that come from nice equations.
Two other examples are SLN, I've written the formula over there.
So these are just all the matrices of determinant one, that set aside this very explicit formula
that's the transpose of the matrix times the matrices, the identity matrix.
Another one is the symplectic group, which has a similar property, but instead of the
identity matrix, you put in here a matrix that I call J, J, like Jessica, well, it's
actually called J, which has minus ones on the lower undiagonal and ones on the upper.
And that's called the symplectic group, resource symplectic for, so this defines as a symplectic.
And so that's what I mean by the league groups.
It just mean groups of this shape, nice subgroups of GLN.
And so what about periodic groups?
Well, just for the same and replace the real numbers by the periodic numbers.
So over there I already wrote K for the field rather than actually specifying if I need
the real or periodic numbers.
So you see all these formulas on the side, they work also for the periodic numbers.
We can just look at invertible matrices over QP, or those of determinant one, or those
that preserved in that product, that's the first example, or those that preserved a symplectic
form the second and second black.
And that's what I mean by periodic group.
For the dots here, so for those who are more familiar with the real league groups, they're
also these exceptional groups.
There's a classification, I think in diagrams that tell us something about these groups
and the same thing works for periodic groups.
So you are also welcome to think of the most crazy example you've come up with, something
like E8 or so, for people who have heard these words before, so they're in planning names
like E6, C7, E8, G2, F4.
So there are some exceptional groups, but they are all also nice, close subgroups inside
GLN.
It's just that the formulas to describe them are more complicated.
I can't just write it that easily.
So that's the periodic groups you want to study.
And I want to introduce the sister fields to that.
So first, let me recall what the periodic numbers are.
So I said earlier integers are power series, and I said that the periodic numbers were
the fraction field or your complete Q.
And in terms of this expression, it just means you allow finitely many negative experiments
here.
So you're allowed to go to A minus N times P to the minus N, where A minus N is again
something between 0 and P minus 1.
And so a first question if you learn about the periodic numbers that you might wonder
is, what is the characteristic of this field?
And the answer is the characteristic is 0, or what can the characteristic be?
It's some integers, some prime number, there's this 0 in our field, but I think the canonical
guesses here are either 0 or P, and P itself appears here as a non-zero number.
So the characteristic is not P, P is different from 0, and also all the other integers, the
other prime numbers that you can come up with, they are non-zero in here.
So the characteristic is indeed 0, none of the prime numbers is 0 inside this field.
But there's a close friend to that, and that's what I call the Laurent series over a finite
field with P elements, and that looks essentially the same.
The difference is just instead of P, I use now a dummy variable that I call T.
So that means I take a Laurent series, which means power series in T, where I allow finitely
many negative exponents as T added to it, and one of the coefficients now, again the
coefficients are numbers between 0 and P minus 1, in other words, I take them in the finite
field with P elements so that I can add and I deploy these, and it all makes sense.
And they look very similar, and a lot of things work very similar for these two fields.
So the main difference is now the characteristic is actually P, because now P appears in this
power series as an A0 here, and P inside this finite field with P elements is equal to 0.
That's the finite field with P elements that is the field where P is the same as 0.
So that's the main difference between these two fields, the characteristic.
So if you haven't seen periodic numbers before, you might choose to either think about these
periodic numbers or think about the Laurent series.
Some people who come from Alanisus say P for Laurent series because they know them where
the coefficients are complex numbers, and now you just replace the complex numbers by
a finite field with P.
And so when I talk about periodic groups, I said before, take one of these groups and
plug in the periodic numbers.
Now I also want to allow you to plug in the Laurent series over a finite field.
So that's what I mean by a periodic group.
So there's one of these groups here.
Any questions so far?
I guess given the fact that we don't have a definition, which is why it also makes me
feeling a bit uncomfortable, because what you have here at P is the character of P, but
QP is mixed character, so why are those folks allowed to be a periodic group?
Why are they supposed periodic groups?
Because I like to abuse notation and call them periodic groups, rather than call them
periodic groups and groups over normal series or a finite field with P elements.
That's not a satisfying answer.
It's some people call both periodic groups, some call only the first one, periodic groups
that really come from periodic numbers.
I just want to make my life easier.
If you want to have a really precise definition, just give it to you.
I mean the F points of a reductive group where F is a non-archimedean local field.
So that would be the formal definition.
Since this is a colloquium talk, I'm not going into these details, but rather say these are
periodic groups so that we all understand what's actually happening here.
For the symplectic group over the periodic numbers, does J change?
No, J is always this J.
Because this J makes sense over all P's.
If you stick to it, it's actually not the same as SO, but let me not worry about the
case where minus one is equal to one.
Excellent question.
All right.
I don't see more raised hands.
These are the periodic groups, and now I want to study the representations of these periodic
groups.
So from now on, as fully noted, either QP or the normal series over a finite field, or
if you want to be fancy a non-archimedean local field, for those who have heard these
words before, but it's really these fields that I'm considering.
And G is a periodic group.
Like one of these groups we have seen before, I just make my life easy by abbreviating these
fields by now.
And here's the definition of smooth representation that I want to consider.
I start with the definition of a representation that Charlotte gave on the first day.
We recall that we said that a representation was a group homomorphism from our group G
into the space of automorphisms of complex vector space V.
So that's what we started this lecture series with.
Charlotte introduced that.
The interesting thing is for us, usually V is infinite dimensional.
So we now work with infinite dimensional vector spaces, while Charlotte worked with
finite dimensional vector spaces, usually.
And since these are no infinite dimensional vector spaces in the groups themselves are
infinite, like these groups GLN over the periodic number, it's something infinite.
I want to get more structure.
And I told you that there is this topology on the periodic field.
So we have the fractal that appeared, which also gives a topology to the group.
And so I want to take this into consideration when defining smooth representation.
So I call representation smooth.
If, in addition, the last line reports, I want that the stabilizer of every vector is open.
So I take any vector I want and look at all the elements that fix this vector.
And I want this to be open.
So that means every vector is fixed by something large.
It's fixed by one of these red triangles we saw earlier, but now at the group left.
Sorry, not red triangles, red circles.
So that means each vector is fixed by something large, but still the group is infinite.
So still there are infinitely many elements that act non-trivial.
And we get an infinite dimensional vector space.
And that's actually a nice category that we can study.
And here's the long-term goal.
What we want to do is we want to understand the category of all these smooth representations.
We want to understand all these smooth representations, the building blocks,
and also how we can map between these representations.
Map between these representations are just maps that preserve the action.
And so how do we do this?
We know that there are certain building blocks.
They are called super-custodial representations.
So that's always a scary word.
I don't know, maybe the super makes it scary.
And the idea of this talk is that I want to explain what that is.
And the short answer is just building blocks.
So it's something very simple.
And the first problem is we want to construct all these building blocks.
Let me just move the long-term goal and the problem one up.
And let me just say a bit why do we want to do this?
So when you give colloquium talks, it's always important to automate why do you do things
because people usually outside your area don't know why you care.
I myself just care because it's cool to study these things.
But it's also helpful to write down these representations very explicitly
to do all kinds of things in the representation theory of PRD groups.
So what questions can you ask?
You can, for example, ask, let's take a subgroup.
Let's restrict our representation to a subgroup.
What happens?
What representations occur?
If you want to answer that question, well, you need to be able to write down
these representations somehow.
Otherwise, if you don't know what they are, you can't really say what happens when you restrict them.
But there are also all kinds of interesting things like transfers of representations
from one group to another.
If you want to understand that, you also want to write down these representations
as explicitly as possible.
And also, who doesn't like to write down things explicitly?
There are also applications.
If you want to write down an explicit local language correspondence.
So we see that a bit in Anakalyani's talk, the idea of the Langlands correspondence.
So the idea is that we start with representations of a PRD group.
So Anakalyani talks about the global version more.
We talk more about the local version.
So that means we start with a representation of our PRD group.
And we want to attach to it something from Nambathiri, something from the word of Nambathiri,
which is a Galway representation, roughly.
But if one wants to do this explicitly, well, you need to know how to write down the representations
of stagras in order to explicitly attach something in the other world to it.
There are also applications to automorphic forms.
These are generalization of modular forms, something we have seen in Anakalyani's talk.
So you might want to understand what are these automorphic forms?
What are these modular forms?
Are there nice congruences between different things?
I just refer here to a joint paper with submission in which we use results about these representation
of PRD groups to construct congruences between automorphic forms.
So that then enters the global world that we see in Nambathiri's lecture series.
And well, the way it enters is through the adults, where we see that if we, in the global world,
we look at all the periodic numbers for all the P's together.
And if we specialize to one P, then we are in the slow curve.
There are also many more applications.
I've heard about applications in mercymetry, conjectures, and so on.
And I encourage you to just find more applications yourself.
So here's the way to answer.
We can do this.
I will undermine that subject.
And for now, I want to assume that we can do this.
And later in the talk, I will explain to you a bit more what I mean is we can do that.
But for now, let's just assume we're done with that part.
Let's assume we know all the supercasual representations.
Let's black box this.
Then how do we get the rest out of that?
Well, before doing more abstract things, let me give you a very explicit example.
You might be crying already internally that you want to see a representation.
So here's one, which is actually an analogous to what we've seen in Shadow Chance and talk already.
And so I take my group to the SA2 over the periodic numbers.
And inside there, I developed by B, because it's often called Burrell subgroup for Burrell, who used to be here.
The upper triangular matrices inside this group SA2.
So they have determinant 1 and upper triangle.
You've seen the same over a finite field in China.
And now I need to give you a vector space first and then I need to give you this action on it.
So here's my vector space.
I take all the functions from the group to the complex numbers that satisfy the following property.
F of BG is equal to F of G for B, any element in my Burrell subgroup, the upper triangle matrices and any element G in the group.
So that's just the first line.
If I stop there, this is the same definition as BFC and for finite groups in Charlotte's talk,
except while I write my B on this side and Charlotte wrote it on that side and had a character in there.
So there's some difference in conventions.
I just don't like to write inverses.
Charlotte seems to prefer to write inverses.
If you write on the other side, you have to write a bunch of inverses.
But now since we work with these periodic groups that have this topology and all these fun things on it and no longer as a finite group,
I want to take this into account and ask the function to be locally constant.
So let me show you a bit more explicitly what this means in this setting.
So this formula asking F of BG to be F of G can be also said, but we just look at functions on the quotient G mod B.
So all the functions where on B nothing new happens.
And what is this quotient here?
So the quotient between SA2 and B is essentially just whatever entry is here.
If you actually sit down and do the exercise, you will find out it's actually whatever entry sits here, but divided by this entry here,
which means, in other words, it's called P1 of QP.
So the projective line over QP, which just means it's QP together with infinity.
Infinity comes from the fact that if we take here number and divide it by zero, we get infinity.
So really this quotient is QP, which you have seen before, and infinity and QP itself contains like ZP, which is this picture here.
And now the function should be locally constant.
So that means it should be constant and one of these, one of these circuits, like one of the very small circuits.
That's where it needs to be constant. So that was locally constant.
So it's something really explicit.
You can sit down, write what it is, understand what it is, but still it's something infinite dimension.
Because this is just C and QP goes on to infinity.
I just not drawn this because it's fine at my side.
So that's vector space.
It's an infinite dimensional vector space, but it's something rather concrete.
And now I need to give you the action.
So I need to give you this map from G into the space of automorphisms of the vector space, which satisfies this nice smoothness condition.
What does it mean to give a map from G to the automorphism of the vector space?
It means for every element G in my group, I need to give you a map from the vector space itself.
And here's the map.
I take one function.
I mean, that's how I write that function in here.
And I send it to the same function except I write translated by G.
So that's my map.
You've seen this in Charlotte's talk where the G n was on this side because she had the condition on the other side.
So that these two convention will exist.
And that's smooth representation.
So you can check the smoothness is satisfied with just follows from the condition locally constant here.
So that's why I put in the condition that it's local.
So here you go.
You have seen your first example of a smooth representation of a periodic group if you haven't seen these before.
And it's very similar to what we've seen in finite groups.
Now I want to generalize this and want to get from just this one example to a huge number of examples that are almost everything.
And I do this via the notion of a parabolic subgroup.
So parabolic subgroups already secretly showed up, but I don't think they really find yet.
So here's what a parabolic subgroup is.
Let's do it for GLN.
You can do it for all the other groups in the same way.
It's just a subgroup of this shape.
So what does it mean?
You choose the number and sizes of blocks.
So I've chosen two by two, one by one, three by three.
That was my choice.
And now I take all the matrices that have any entries you like in the blocks and above the blocks and that's my parabolic subgroup.
I can choose any block size I want and then take the block upper triangular matrices.
To be very precise, instead of just choosing number and sizes of blocks, I should also choose an element small g in my periodic groups inside GLN, for example, and then conjugate that.
So I fix my g and then take all the elements that is after a conjugation by g or g inverse, have this shape after conjugation by g inverse.
But they have this shape.
That's what a parabolic subgroup is.
And why are these parabolic subgroups so important?
Well, they have a really nice structure and here it goes.
It's called a levy decomposition and says that if you have our parabolic, I call it P for parabolic.
That's upper triangular.
I can write this as a semi direct product of two things.
I can take all the matrices that are block triangular.
So zero is above and below the blocks.
I call this M and that's a levy subgroup.
That's the name for it according, named after levy.
The nice thing about this M is this is again a really nice group.
It's again a periodic group.
For example, here, it's a product of GL2 times GL1 times GL3.
So it's really nice to do it with symplectic and orthogonal group, but also product of some GLNs and orthogonal symplectic groups.
And so that means here we can already construct representations and that's what we use.
And then there's this other thing here that's called you.
It's called you for unipotent radical.
That's just the name for it.
These are unipotent matrices.
What are unipotent matrices of these shapes?
So matrices and fonts on the diagonal that are upper triangular.
And in this case, they are block upper triangle.
And I can write every matrix in here as such a product.
And that then allows me to do something called parabolic induction.
So I take a periodic group G and take a parabolic subgroup as it does.
And now I take sigma representation of M, a smooth representation of M.
So a map from M to the automorphism of an infinite dimensional usually, infinite dimensional vector space.
Sometimes it's also finite dimensional.
And then I form the parabolic induction.
So here in gray, it's what we did before and now it's the generalization here.
So what is it?
It's functions from G to this vector space.
So previously we just had C, one dimensional vector space.
Now it's this vector space of the representation that satisfy the following properties.
We want that if I write F of M, U, G, where M is an M, U and U and G is any element in G elements.
It sounds weird to say M and M, small M, capital M, small U, and capital M.
So the property is that this M and U pulls out via this representation sigma.
So that means it's sigma of M times F of G.
Does this make sense?
Let's see.
So F of G is an element of the vector space.
So this is an element of the vector space and sigma of M, sigma is the map from M to the automorphism of the vector space.
So that means really what it is, is this is an element of the vector space and this is an automorphism of it.
So it acts on this element of the vector space to give us a new element in the vector space.
So that makes sense to me.
And this U just drops out.
We forget about the U.
And that's the same as before where we just had sigma, the trivial representation.
So the B just dropped out and nothing happened.
Secretly, you can write here multiplication by one or the trivial action more precisely.
And so that's the power body conduction.
That's the vector space.
What is the action?
The action is the same as before, just this right translation.
And the notation that one uses often is also to write this as in from P to G of the representation of the space.
And this thing, that's what charits are also used together with our other notation.
That's more coming from the elasticity.
And so this power body conduction is a nice way to start with a representation of a smaller group and build a representation of a bigger group.
So now you can create a lot of representations.
Any questions at this point?
Yes.
Is there an intuitive reason for why U drops out or should we just forget about it?
Oh, that's an excellent question.
Is there an intuitive reason?
Yeah, because the representations of U are very different.
So the...
It's an excellent question.
I'm wondering how to best answer.
So I said I want to talk about periodic groups.
And periodic groups are really groups of this shape and not of that shape.
Because the representation theory of groups of this group is very different.
And so I'm really interested in the representation theory of groups of that style and to take representation from that group and build new representations out of it.
Maybe one answer is that you don't have to, and that's remarkable that you can get them all without taking them.
That's a good point.
Yeah, it's just remarkable that it works, was the comment.
You would expect that you have to take this into consideration to get everything and remarkably somehow you don't.
That's some magic that happens there, I guess.
And so in particular, so maybe now I can say that I was asked earlier, what periodic groups do I really mean?
I said this word reductive groups.
And reductive group just means that the unipotent radical is trivial.
So the unipotent radical is the maximal connected normal subgroup of this shape.
So that is conjugated to upper triangular matrices.
This one's on the diagonal.
And the groups we consider are those where there is no normal subgroup of that shape.
So that's why we closed it out to get a nice period.
So that's how we kind of construct a lot of representations.
Now how I started off this building blocks.
What are they?
Now I can define them for you.
So let me go once more back.
So this is how to construct a lot of representations.
But what are the representations that are missing?
These are the building blocks.
And that's the definition here.
So from now on, I will drop the word smooth because I can't write smooth all the time.
It's just getting too much too long.
So all representation are smooth.
And an irreducible smooth representation is called supercustital.
These ones that I'm really interested in.
If the following happens, if it does not embed into a proper parabolic induction.
So that means you cannot find a parabolic subgroup that is smaller than G,
like one of the ones from the previous page,
such that the representation embeds into this parabolic.
Which means in other words, these are the ones that we need to construct somehow different.
And here's a nice fact that explains why I call them building blocks.
If you take an arbitrary irreducible smooth representation of your periodic group,
then there exists a parabolic subgroup.
And a representation of the levy M that is supercustital,
such that your representation is started with embeds into this parabolic induction.
Now you might be a bit confused because I said the supercustulons,
there's a do not embed.
But the difference is that here I am now equality.
So every one moment, every representation can be embedded into the parabolic induction
of some parabolic subgroup of a supercustular representation.
And the supercustular ones are the ones where I have to choose P to be equal to G.
So the trivial parabolic induction right now can happen.
Does smoothness matter here?
Is this true for all irreducible representations?
Or is it irreducible representations or irreducible smooth representation?
I mean, everything is smooth here.
I don't even think about non-smooth representation.
Things are getting crazy.
From now on, everything is smooth.
So that means the supercustular representation, they are really the building blocks.
And they allow us not to understand all representations because everything else embeds in them.
And that allows us to even do this at a category 11.
So I write by Beboff G, the category of smooth representations.
If you don't know what a category is, don't worry about that word.
Just ignore that word.
What it really means is I just look at all the representations and I look at the morphisms between them.
That's just what the word category is.
And the statement of what Bernstein found out in the 80s, I think, was that you can decompose this category into blocks.
They are called after him.
They are called Bernstein blocks.
And they are indexed by M and sigma.
M is the levy subgroup and sigma supercustular of M.
And so how do I know what to put in there?
Well, take an arbitrary irreducible representation.
You know it embeds in a parabolic induction coming from an M and sigma.
And then you just put it in the block with the M and sigma written down here.
So I should say that M is a levy subgroup and sigma supercustular of M.
There are also non-irreducible representations.
What do you do with them?
Well, you just take all this irreducible ones, put them in the blocks, and then you just take everything you can build out of them and put them in the same block.
So for example, if you take a representation, irreducible representation that's in here,
then you take also the direct sum of the representation with itself and the triple direct sum and put it in there.
There's a little trigger here.
This means that you need to consider things up to equivalence because, for example, if you induce something from M or from a parabolic containing M and sigma,
if you conjugate your parabolic subgroup by some element in the group and then induce, you get isomorphic representations.
So this trigger means up to an equivalence relation.
The equivalence relation is conjugation by elements and also unremifed twist for those who have heard the word before.
So these are characters that are trivial on all the compact cycles.
And that's just the technical detail that we can do.
So that's nice.
So we now know a lot about the structure.
And we started out with, okay, there are some smooth representations.
We have no idea how they look like.
And now we know at least they fit into these blocks.
There's a lot of structure.
So I just moved up the Bernstein decomposition.
And I want to give you an example to actually see what are these blocks.
Let's take my favorite group for talks as a tune.
And what are the possible Levy subgroups?
Well, Levy subgroups means these block triangular matrices.
What block sizes can we have?
We can have two by two blocks.
So that's just M equals G.
Or we can have a one by one block and one, one by one and one by one block.
That means diagonal matrices.
And so as I show the determinant should be equal to one.
It means TT inverse is really what we have.
So let's just look at the two cases.
Let's start with the case where M is equal to G.
In that case, well, sigma is a super costly representation of G.
We need to induce that from G to G, which does nothing.
So that means the only irreducible representation in there is really sigma.
And then we need to build everything we can build out of sigma.
And that's just the direct sum of sigma is itself.
So it's just the direct sum of vector spaces and the action is just the direct sum.
It can also take three times sigma and add them four times infinitely.
Many sigmas can be added to each other.
And that's all that is there.
So that's a nice fact that you can't have some non-trivial extensions.
So it's really, these are all the objects in.
And then you can ask, what are the morphisms?
So there's a nice lemma.
She was lemma that says that the only morphism of sigma from sigma to sigma itself.
So the only vector space morphism that commutes with the action is actually the scalar multiplication.
So the only morphism from sigma to itself is scalar modification.
And then you can deduce the same for, I'm not the same.
You can deduce what it is for these sums as well, that corresponds to matrices.
So that's the first block.
That's a rather easy block somehow.
Well, easy after we don't know yet what sigma is.
So that's still the tricky part.
Here's another example.
Here's the next example.
We said we take the T, I didn't actually explain why I called it T, T stands for torus.
I think we have seen this in previous talks that there are weird different torus that float around here.
It's the diagonal matrices.
And I take the trivial representation.
That's the nice super-casual representation because, well, the torus doesn't have any proper levy subgroups, which means all the representations are super-casual because nothing comes from parabolic induction from something smaller because there is nothing small.
And then the big question, what is this?
So how do we find out what that is?
Well, I said we need to take the parabolic induction from a parabolic containing T, that would be the Borel subgroup, these upper triangle matrices, of this trivial representation.
And then it turns out that nice exercise, if you can actually work it out, that the trivial representation is contained in there.
That's great.
So we should take the trivial representation to be in this block.
But it also turns out that there's a quotient.
And this is called Steinberg representation.
So that's the definition of it.
And it turns out that actually this is a non-trivial extension.
So the Steinberg representation is not a sub-representation of the parabolic induction.
It's only a quotient.
So this is not the direct sum of the two.
So that means inside here, we also have the Steinberg representation.
We have the direct sum, but we also have this non-direct sum, this weird extension, which just means it's complicated.
This is called the principle block.
And it's complicated.
So let me give you the answer of what it is.
It actually turns out that this is modules over something nice, something that's called an affine-hacker algebra.
What is this?
The main thing that you should take away is something explicit.
And here's an explicit formula.
It's an algebra with generators T sub w.
W sub w is what w is.
Small w is an element of the affine-wahl group, wR denoted, which is the group generated by two elements, 0 and s1, with the property that the squares of each element is 1.
How can you think of this very explicitly?
Well, you just take the real line, take 0, take 1, take reflection across 0, take the reflection across 1.
These reflections, they have order 2 indeed, and you take the group generated by these two.
That's the group we look at.
And then for each element in this group, we take T sub w a basis element.
So as a vector space, this algebra is just the complex vector space of basis elements T sub w.
And then the algebra means there's also multiplication on this vector space, and the multiplication is given by following relations.
So I see this the first time.
Don't be scared.
Just accept there is something complicated. If you have seen this before, maybe we recognize it.
If you haven't seen it before, maybe you go in the future to a talk and see it again and then recognize it.
So the relations are, we have to do, there's a length functions on these elements, and you have to distinguish the two cases.
So if the length of s, I, your generated time w is longer than the length of w, then it's what you would want to do.
Then the product of TSI with TW is just T of SIW. That's somehow nice.
The problem is if the length gets shorter, then there's this real relation.
So then this product is T times the thing you really want T sub SI times W.
But then there's this extraneous thing, this P minus one times TW.
So that's just what it is.
Mathematicians are really excited about this. For some reason mathematicians like this, that appears quite a lot of different areas.
So that's really what people study, so people actually understand that, like other mathematicians.
And so that means they also understand this model.
All right, so that's, yeah.
Is there a reason the lengths can't be the same?
That's the next thing fascist, the reason the lengths cannot be the same. Yes, the lengths always get, you always add one or subtract one.
That's just the property of coxida groups. So the length means what is the minimal number of generated as zero and as one such that you can like a W as a product of those.
There are lemmas, I think called exchange lemma, deletion lemma, that tells you if the length is not longer, if there's a short expression, you can find it by deleting an even number of things.
Even what I think that's, well, you can make it shorter by deleting two things. That's the statement of that lemma. So that's why you know it's either longer or shorter.
All right, so that's an example. And now the big question is, how do they look like in general. So that's our second problem. So remember, first problem was, what are these super customers and patients that index everything.
And once we know what the super customers and patients are, the next question is, yeah, what are these blocks, how do they look like.
That's the answer. So the big answer is, and when you see this the first time you want a lot. This is good for. So the big answer is, this thing is the same as the thing was yours at everybody.
Awesome isn't it.
Let me explain a bit what this is. So the G zero is a subgroup of G.
So first of all, M zero is a levy of G zero and Sigma zero is a super castle of M zero. But so what is so nice about is, is that the Sigma zero is what I call and explain later at that zero representation of M zero.
So here's the way explanation. It means Sigma zero corresponds to representation of a finite group, which means it corresponds to some of these representations that we encounter in childhood science talk.
So child's talk basically has been about these secretly about these steps your presentations that's where she might be.
Well, general blocks are very complicated. We reduced everything back to representations of finite groups. So you can forget about these weird Pali groups and actually study finite groups, which is much easier.
It's still very complicated mathematicians have worked on it for a long time as we learn from childhood. It's easier.
And then the other nice thing is that actually we already know what it is. So these blocks corresponding to these zeros everywhere. So if this is a castle is of depth zero, we understand explicitly what it is thanks to work of Morris in 93.
And what I add, I should say that's joint work with Jeff Allen, Manish Misha and Pazuma Hara. And we, it's a slight light, not all to do tomorrow's we had to add something to it, but luckily already in the 90s we knew the structure that this is equivalent as a category to models over some hacker algebra.
So that means I've written out here formula can essentially ignore it. Let me just say, that's a semi direct product of a group algebra. So that's just something rather easy except there's a small two cross cycle in it, and an up and out hacker algebra.
This time we have seen before.
The similar answer important more important is it's something was exclusive generators and relations that certainly really understand something people have studied something explicit rather than this weird representations of piety groups that we don't know about there.
So that's the, that's the big answer.
Why do we care. I said we need to say a bit about why do we care.
Well, there are many applications to the representations of piety groups themselves. For example, in general I expect we can reduce a lot of problems to representation of finding groups because suddenly everything is equivalent to representation of finding good.
So if you do it in a nice way you can reduce a lot of problems to the setting of finding groups with answers are either known or much easier.
There are lots of applications and explicit local language correspondence, which essentially you can reduce it to finding an explicit language correspondence for depth zero of sensations, which again is easier and is broken for us.
But both are working for us to reduce it to depth zero and to use the depth.
We can go beyond and apply it to something that is fancy nowadays, which is categorical local language correspondence. Some of people nowadays like to view everything at higher levels, higher category categories all around.
And so in particular people try to upgrade the language correspondence from something that is just take a representation attach a language parameter to some categories you take the category derived categories of representations and send it to some category of sheaves on some space.
Something weird.
But if you want to do this. Now, since we know that even as a category, even when we consider extensions and morphisms.
All these blocks are equivalent to depth zero block that means also the categorical local language correspondence can be reduced to depth zero.
And there it's actually kind of work in progress so she went to announce already last year that he can prove the categorical local language correspondence in a depth zero case.
She was already there after I'm actually writing and releasing the paper. And once this is done then we can hopefully reduce everything to that.
So that's the broad overview.
Any questions at this.
If you have this algebra, and just in the case of the trivial character.
And we want to write a representation, can you explicitly write a representation as a module over this algebra, like you can explicitly write the algebra but can you also write the representations as modules.
Does this make sense.
Sorry, can you repeat this.
Sorry, your category of representations. It's an it's a category of modules of an explicit algebra.
Yes, can you write down the reference.
I'm not sure exactly what I mean.
Do you want to like if I give you the trigger presentation, can you give me the module. Yes, of course.
So the correspondence between representation of PID groups and modules of a hacker device very explicit. Okay, you can go in both directions very explicitly.
One direction you take appropriate fixed vectors.
So it's very explicit to go between these two worlds.
Yeah.
All right, so let me say a few things you should always acknowledge what people have done before us.
Somehow, people haven't done much before this question surprisingly I mean I said that since the 90s we know what these things are.
I didn't know that everything looked like it should somehow reduce the depth here.
But if you just look at general groups.
I'm not looking at special examples and special examples. People have worked out things like classical groups and gel and our special examples.
In general groups, there are essentially only two results in 1998 wrote, looked at the one extreme case where this levy subgroup is just diagonal matrices, but just the top split toys.
That's the one extreme case.
And in that case Sigma is just the character it's one dimensional so it's actually nice.
And then recently Kazuma was a master student at that time, looked at the case where is equal to G so that's the other extreme. It's the case for as a to that was rather boring because there we just had Sigma Sigma plus Sigma and so on so
in some sense this looks rather boring there are cases where it's a bit more interesting but it's in some sense the easiest.
And now you wonder what is in between. There's a lot in between and well it's about took more than 20 years to figure out how things go.
All right, I want to. So this is the end of the overview now I want to go back to problem one tell you a bit what we know about problem one, and then tell you that how we can approach this.
I said problem one was we want to construct the supercasual presentations.
So what do we know, I want to draw a picture those are seeing me give talks before I've seen this picture before. So maybe I should first say so in the case of G a before picture in the case of GNN this has been known for 25 years or so by now 30 years time flies.
Yeah, something more closely something in the 90s.
Thanks to well it started people started trying to construct these things 50 years ago or more than 50 years ago.
And people first started with the group gel and the end by end virtual matrices that's a good starting point because you can write them down easily.
And more and how they did a lot of work initially and all of people worked on it, and then bushland could put finish the story in the 90s construct all the recent patients.
Then people looked at classical groups. So these are these groups and the orthogonal some practical groups also the unitary groups that's around that maybe 15 years ago.
That's John Stevens, and then in a forms of GLN so that's GM of a division algebra that was done by she's seven Stevens, around the same time.
And what I'm interested in is to do it for all groups together and also these weird exceptional groups and everything.
And so there, it took some time to get started. And in the 90s, more impressive they introduced an important notion which they call depth.
I said we look at smooth representations, which means moves means that every vector is fixed by some open subgroup. That's what I gave us definition.
But so this open subgroup, it might be huge, or it might be tiny.
And so what one has that did is they define a filtration of our group by compact home subgroup that gets smaller and smaller and smaller and smaller like these red circles after all.
And then they said the depth is how far you have to go down in the filtration until you get fixed vectors.
So it has high depth if you need to go down very far in the situation. And that's zero means it has fixed vector under the huge maximum.
Or something slightly smaller.
And so what my professor did that they introduced this invariant that's very helpful to study presentation and then they studied these depth zero representations.
I drawn this direction of prime number, because some results depend on what P is the P of the periodic field.
And so my research study all these steps your representations and Mars with the same roughly around the same time, using a different approach using.
And then the question was, well, how do we go deeper? How do we find it in terms of positive gaps.
In 1998, Jeff Altman is PhD thesis. If you're looking for PhD thesis. Well, this one has been written already constructed some representations of positive depths.
And then you looked at it and found out how to construct many more presentations by generalizing this basket.
And so all these dots, they should represent some representations here.
I guess I put my initial there. That's a longer story short version is that these only people encounter that subject you rather than a construction and prove that these are super customers nation.
And his group relied on some other result in the literature. And it turned out that in this other result, a plus minus one was missing because somehow I think it was at the time where you hand wrote papers gave it to the secretary who typed it for you.
And somehow the plus minus one got out. I don't know when and where it happened. The problem is, if you put the plus minus one back into the paper where it should be, it has to be there.
The purple Jackie you broke down.
So that happens.
Five years ago or so, as some people noticed that like long time after this was written so people were a bit worried and eventually we realize, okay, no problem.
It still provides us with a customer presentation all is fine. I just needed to provide a different second half of the proof the first half is fine.
Second half needed to be changed.
So, because a lot of people work with these representation assuming they're super hospital.
But, but that happens. But now it should be fine.
And so then the question is, well, it's nice to have some representation but really what we want is we want all representations if you want to make statements about all super hospitals are nice.
And so big breakthrough was obtained by a woman and Julie Kim, around seven years or so later, six years later, and Julie Kim showed that if we are in this region of the prime P is very, very large.
And if we work with QP this characteristic zero for you, not the characteristic P here, actually we get everything from use construction.
So if you want to prove something about an arbitrary super customers information, you now have something in your hands, you can use taking use construction right down explicitly how these representations look like, and then prove something with them now that you know how they look like.
So that was a big breakthrough bag.
And then to be one was that a few years ago, math is not my strength. Well, that sounds like around 10 years ago.
We are new together was we don't you get some construction of some representations they said, suppose there is a nice input then they constructed an output which is some representation that are smallest positive depth so they're close to the surface.
They call it the representation if the logic representations.
This is an odd word and my meter state is to me so a people logic comes from oceanography.
The upper zone of the ocean the zone that can be reached by sunlight that's called the epipelagic zone. And so they constructed these representation that are very close to depth zero that are small as positive depth.
And my reader told me things of representation as fish that swim in an ocean so these are all fish in an ocean. And so the upper zone of these yellow epipelagic.
So they have this input and when the input exists they gave construction it's very easy and produce some nice output. I mean, in the sense it's easy to perform not easy to come up.
So always to keep it up in the case that things that look easy are difficult to come up.
And then partially in a joint paper with better manner, also did the general case we produce more input for the construction that in particular results and representations for small crimes that were not previously there.
There's something else. There's something else out here.
You might ask, well, but where is there more, which region is the more rich region do we get everything. And so it turns out, there is a region here where there are a lot of blue dots, the reason my piece just large not very large.
And it turns out we get everything just in this region appears large.
Over this extraneous thing got removed here as well. We don't need to assume let me work with QP. We can also work with the sister fields, the North Sears or a finite field.
So let me state this as a theorem. Suppose the prime P is large. What does it mean? Here's the answer what it means. It means that P doesn't invite the order of the wild group.
The bar group is something attached to each group. Here's a table of names if you have seen these names before I talked about names for PID groups.
The name for GLN is a and minus one.
So that means for GLN the the bar group is actually just the symmetric group on n letters. It corresponds to permutation matrices.
The symmetric group with n letters is n factorial. So we want for GLN that P is larger than n. And the same for these groups, the classical groups want P to be larger than n.
So these are the, the names for these classical groups and but also these exceptional groups, and there we exclude in worst case the prime two, three, five and seven, even though the order is huge to the 14 times, whatever, the only exclude four problems.
And that's actually not that large.
And so another assumption use construction provides us with all the supercastling isn't it.
That's something that I actually wrote while being at the IS. Thank you for hosting me.
This is just also optimal in general I put a little star there because I don't want to go into the details of what I actually mean but essentially, we don't expect anyone to come tomorrow and say, oh we've produced construction gives us everything even for small p.
That's already not possible because of the yellow dots here that I mean.
So that's kind of work in progress with my post of David Schwinn where we provide new supercastling presentation that are arbitrarily deep. So not just those that can be reached by sunlight but also those that are down deep in the dark ocean, where we need, I don't know, flashlights to see them.
So these are the green dots.
So that's kind of work in progress.
My poster David Schwinn has been already giving some talks about it in the process of writing the paper.
So that's the state of the art. That's the answer to what I mean was, we know it, how to construct supercast on almost all cases and a minor assumption. This is the minor assumption here the psl.
All right, so I want to show you now how such a representation looks like to just give you an example unless there are other questions at this.
Could you give an example of the debt filtration.
But I guess an example for that filtration.
Yeah, yes.
Um,
So,
or can people see here.
Let's take a statue over to peace.
There are essentially two, two interesting examples and then you tweak them a bit. So one example is.
So it's at depth zero. Let's say, or you can see I guess this.
So at this.
We have just the matrices with ZP everywhere so that's just the compact set group.
And I need to write determine the first one inside as it to I might just drop that. Hope you can just think of that all the time.
And then a set group. You can just take the congruent filtration so you can like all the matrices that I was the shape one plus PZP on the diagonal PZP off diagonal.
And that's that's true. And you can keep going down. Just take all the matrices congruent to one, the identity matrix modulo P to the F.
That's one nice filtration.
And that's something that is rather well known I think in general people like to work with congruent filtration.
And what my cassette now observe this, actually, you don't need to have jumps everywhere that you don't have to go down on the diagonal and the off diagonal at the same time.
So what they did is they started with a different group and this is actually due to brand teams, they classified compacts up groups and another nice compacts up group is one where the off diagonal element here is slightly smaller.
So we have the same group, except I put a PR ready here so that's a bit smaller. And now the jump now the text doesn't really work.
Let me put the desk to 012. And now the fun thing is that at first, the diagonal gets smaller, but the off diagonal stays the same.
So that's a nice stock group. And this first one has that zero the next one is a depth one half you need to put something in the middle and then we change the, sorry, did I say off that first we chase the diagonal.
And then the next step, we change the off diagonal. So the next step, I'm increasing this P to P squared and add a P here.
And that's the depth one. So in some sense, to go from here to here, I did make everything smaller. And there's something in the middle where I first make the diagonal smaller and then the off diagonal.
And I can you can guess the next step would be again it's the diagonal that needs to get smaller off diagonal stays the same.
And you can keep going.
How do the denominators.
Into the filtration. How do the what the denominator is.
What do you mean by denominators to see.
But yeah, you're going to.
How does it get zero representation for as a QP.
This, this would be as a two ZP representation.
No, this is just the more present. You're asking for.
Yes, yes, yes.
So these are just compact subcourses.
And that then define the depth. So I just see where do I get fixed that goes which of the compact subcourses get lost as fixed.
Sorry that
that top decided.
And that's essentially up to conjugation and up to tweaking by the jumps happens all the things you get for us.
More questions.
So just make sure these green dots are things that use construction does get exactly the green dots are things that you blue is you take a year and the green dots are those that you don't give us.
And these are the ones that I construct as my postage David tree.
And you say no overlap like your instruction.
No, there's no one that the construction that we give recovers everything that JKU gets we generalize JKU is construction.
So what we do is we realize that actually the filtration that more impressive defined is not enough.
We define a final filtration that gives us more representations.
What's the tool by which you know you have everybody.
What is the tool by which I know everything.
So that's very different and it's very interesting Julie Kim had very abstract arguments using potential formula measures, something you need more complicated.
I just create them. I mean, you give me a representation and I just give you the input for the construction. I mean that's maybe not the tool, not exactly the tool it's a bit more complicated.
So, what I do is, I use the bridge teach building to find like basically the first step is if you give me a representation, I need to figure out what the depth is.
So if you look at all the filtration subgroups and secret right at fixed vectors. That's the first step.
And that's only a very small part of the construction of JKU. And then there's a much, maybe I shouldn't go into too much detail here can do this afterwards, but there's a JKU has in addition to the depth he has a lot of more additional input data and I tried to recursively find the input for
the JKU's construction from the representation that you give me that uses for teach the more concentration of fine analysis of the structure of these, these groups.
Is there any connection that you could point to Irish under as well.
Um, I can't. I mean, that any super customers don't really exist there. So what is, so how genre worked with real groups and I said at the beginning there are these really groups and then they're PID groups.
And what is interesting is you learn in three series class.
Same role.
Yeah, so what's interesting, we learned in Charlotte talk that there are these characters that we learned tomorrow, we learned today.
Did you talk about characters. Yes. So we learned today about the characters of representations and one can also define characters of piety groups.
And it turns out that actually the character formulas, if you define, if you interpret things appropriately others magically the same.
But I don't understand where this magic is coming from yet. So that's harsh and has left its principle that there should be an analogy between the two things, but we don't understand yet why.
So just to clear the definition of depth is that what the, when you get the fixed, when you get fixed vectors.
More precisely, it's the next, I mean, yes.
Exactly. So you see, you see how far you have to go to find big things like this.
All right, let's, let's go through an example so that you actually see super customer presentations when you leave this.
I've seen that way.
Yeah, that's just a recap of ZP as power series QP is finally many negative exponents.
And now we have, well, we, I started with the group G being as a to an insight there I take a compact subgroup as a to over ZP.
So that means I want to give you a map from this group for the space of on the morphisms of vector space that's what a representation is that's what I need to give to you.
So how can we possibly do that.
Well, here's something nice we can quotient out the next filtration subgroup that's conveniently already written here.
And so one nice thing is these are actually all normal subgroups inside each other.
So this quotient makes really a lot of sense.
We get a group.
If you do this, you can try to do the exercise.
You end up with matrices over the finite field of determinant one, essentially because the difference between ZP and PZP is just this zero is compulsion.
It's just this a zero here, which is a number between zero and P minus one.
So in these entries, the difference is something in a finite.
And now it's much easier.
Now we have a finite group.
So we are back to childhood.
So we can construct representations of finite groups.
So that's just as a to over finite field.
So in general, it's a finite group of detail.
And I just take an irreducible hospital representation of that.
What does customer mean as before it means it doesn't arise from a compact from a parabolic induction.
What does this more concretely mean?
In Charlie Chan's talk, you actually wrote down all the possible representations of as a to or at least counted them in some way.
And the customer presentations are those of dimension P minus one or P minus one over two that you saw in her talk.
So it's something very explicit.
You can look at your notes can actually work that out.
What precisely in the setting customer actually means that there is no vector in your representation that's fixed.
And I'm trying to make use of this one's on the diagonal.
So you can look at the representation and just check that.
So it's something.
We proved that today.
Yeah.
You proved it.
I gave you.
I did.
Yeah, I should have come to your class whether they know your notes.
Perfect.
Excellent.
So you already know this.
So great.
So here's something you know.
And now we get something new that you didn't know before.
So now we have a representation of a compact group.
And now we want to build a representation of G.
So we know we do the thing that we love to do is we endures.
And what do I take here I take something where I put a small C here that means compact induction that's maps from G to the spectra space that satisfied the standard relation.
This time I take my K inside this capital K this compact group.
And compact induction now means that I asked my friends to be compactly supported, then automatically locally.
And the action is again just this right translation.
And that's a depth zero presentation.
Ta-da.
Here is a super customer representation.
So it's not trivial.
We have to prove that that requires some work.
But here is a super customer presentation.
So now we have seen what are all the other depth zero presentations.
It's always the same idea that we start with a maximum compact set group here, particularly nice one.
And then we quotient out what's called the property.
Unipotent radical and normal, a huge normal subgroup.
And then the quotient is a finite group of lifetime.
And then you take an irreducible customer representation of that.
And then you compactly induce.
These are the depth zero super customers and patient and full generality.
If you replace.
So to by general G.
And K by general nice maximum complex.
And then you learn from charlotte's talk how to do that.
I guess you looked at the case as I do, but you can do this fortune.
So these are the depth zero of sensation to which we reduce everything.
All right, let me move the example down and let me tell you how to get everything else.
Here is a formula conjecture that's a theory in many cases.
So all the super customer presentation that we constructed so far that all this picture that I've shown you is all the dots.
They are all constructed in the same way.
They are all constructed via this compact induction.
So these are all functions where we take a compact one center open subgroup in G.
This mod center is just if there's a center we need to include it.
Just ignore that.
And then take some representation of K that is finite dimensional.
So here we are back to the final dimension of work much nicer than infinite dimensional work.
And then we just compactly induced. So we take all these functions that transform via this representation on K on the left, and that are compactly supported mostly.
It's a formula conjecture that everything arises in this way we don't know yet but everything we know all the representation we know out.
Right. So the last part is now.
So this is now the end of the super customer story.
The last part is that I want to give you a better hint of how do we understand these burn steam blocks.
So that's what I said was a second problem.
Are there any questions about this first problem?
Yes, I have a question.
So could you use this to actually construct a say like a depth one representation.
Can you use that to describe the depth one listening excellent question. No, you can't because unfortunately or fortunately, if you do this construction you always have a fixed vector under this group.
This group here is trivially, which means you have something just below depth.
So this first group that matters that zero screw. So that's always your presentation when you want to construct that's one representation.
You will have to take in something that is trivial on just the piece below one so it has to be trivial on here but not trivia at the depth one piece.
Right.
And that's more complicated to construct.
So, so basically what you're saying is like another fruit place this leafy like this is like a part which is from that zero to that she was the best one.
Exactly. That's what you can do.
And it's more complicated than the depth one part in general it will be a complicated group that contains this one is somehow build out of some depth zero piece of another group and that group.
And so that's a bit more complicated but in general the rough idea is you take always a depth zero piece of some subgroup.
You have smaller groups for the rest of it and they're essentially you just have a character so you, you combine having a character of higher depth together with the depth zero representation and general.
And the construction is then more complicated which is why I'm not talking about it here.
I have recently written expository notes for an idea summer school that are my website if anyone is interested in seeing more details. That's a good source.
Thank you for your questions for this one.
So then we dive into the last part, which is to understand the problem to how do we, how do we understand these bunch of blocks.
So that PB large to simply find my life.
So the claim is, and that's based on work of Julie jk you and Julie Kim they constructed these things. And together with my last improve.
It says the following so for all these burgeoning blocks take one of the blocks you want to choose M and Sigma essentially, there also exists a nice compact set group and representation on it.
The second is that, well, this is not the compact induction anymore but some of the compact induction encodes the block and here's the answer of how it does it.
The answer is that such that for every irreducible presentation by the following boards. This pie really is inside this burgeoning block, if and only if it is a quotient of the compact induction.
So the compact induction is no more an irreducible representation it's not super custom, but it encodes all the representations by just looking what's in the question.
Another way to face this is that's called for being as reciprocity is to say that it's really difficult to check if something is a quotient of a compact induction because this compact induction is huge.
Much easier is to check the following and take your presentation restricted to the contact subgroup K and then check if it contains a finite representation.
So really, what does it tell you it tells you if you want to know if something is contained in this block, you just take your presentation restricted to a compact subgroup there it's nice and easy and you can see it doesn't contain the finite dimension representation or not.
So that's the corollary of that result. So the corollary is well that's work of Bushland and could go.
And that stated that if you have a cake and the ball that satisfies this property, then you know that this block is equivalent to modules over and again this equivalence is as I was asked earlier something you can write down very explicitly.
So in a sense, it's just you take an appropriate isotropic component. So an appropriate sex space where K x y and y.
And what is this weird thing. So this is something we understand rather well and here's the official definition.
So this is what I call the hacker algebra. And here's the definition that should now look similar to what you've seen about the induction. Now it's not from G not to the vector space but till they end the morphin of this vector space, just as a complex sector space
is just n by n matrices if this is n dimensional, which now has the property that f of k one g k two where k one and k two are inside my compact subgroup and G is in my group G satisfies the property that on both sides they pull out by a row so we have this formula where f of G is an
endomorphism of the vector spaces n by n matrices if this is dimensional and both these are also an amorphous of this vector space because that's what row of an element k is it's an endomorphism of the vector space.
So these are just three n by n matrices that we can multiply. So that's what that formula is. So the difference from before is before we had here vector and here endomorphism now all of these are morphisms and they pull out on both sides.
And that's something very explicit again you can write this down it's an algebra you can write down a multiplication on there it's just some integral called convolution.
And that's what it is so that means we now have a way to find out what this is why are these structures of the second.
The result of Morris that I alluded to earlier, and also work in progress to actually get this exactly on this on the nose, which says then, if this m sigma this m sigma here, the sigma is a depth zero if the sigma is a depth zero representation of m, then we
can understand how the hacker algebra looks like the semi direct product of two things this after hacker algebra like these things that I have generated indexed by up and right groups and these weird relations, and the group algebra that's twisted.
So that's what we knew already in the 90s. And let me say what I'm going to do now is, I combine these theorems in one so let's actually see what is a theorem stay.
The theorems say that there exists this complex subgroup and the representation well, such that we have this equivalence here. So that's what I know combined up there is that there always exists this K and both that's that we have.
And some more is not said we understand the hacker algebra in the depth here.
So this is the theorem and progress I'm working on with my collaborators. We show that for every and sigma so now I'm trying to make the statement I said before precise for every m sigma that exists at depth zero pair.
m zero, which is a levy subgroup of some subgroup G zero and a depth zero representation sigma zero of it. And for this pair for G zero containing m zero the levy and sigma zero that exists also compact open subgroup K zero and rosier.
And like the one above K and K and go for G such that we have an isomorphism of a guy.
So we produce an isomorphism of the hacker algebras on the nose of these guys that I've shown you on the previous page.
The proof is complicated. The proof relies in particular on some technical result that I've proved with such a kind of learn spice recently which let me just say a few words about this.
So I said this proof of JKU unfortunately broke down because the plus minus one was missing.
So a lot of things in the literature always needed to be corrected by plus minus one. And so what our idea was to put plus minus one back into the equation to twist everything by plus minus one so that it disappears everywhere, and then everything looks much cleaner.
And that's what we did. It sounds easier than it was.
And that's a key ingredient without that it doesn't work. And so in particular, a corollary of that is now that we get an equivalent of the burgeoning box because the burgeoning blocks are just modules.
Okay, let me just say one more anecdote at the end side started giving talks about this result recently and then after the talks people often came to me and said, I don't believe your result.
Not really what you want to hear after you give the talk.
And so, well, there were several concerns people had. So one concern was people said, Oh, that's way too easy. These complicated tech algebra, they can't look as easy as this.
But I don't know what to tell them like, it is the case. I mean, I've proved it and what we have proved it. So this concern, I can't really do anything other than tell them why that's what it is.
The other concern is much more helpful people came to me and said, here's a counter exam.
And so that had more than once but we studied all the counter examples and none of the counter examples was actually a contact examples all the contact examples we're just missing this twist.
If you don't put in the twist you can find tons of contact examples this doesn't work you really need to include this twist by a class.
So maybe that's also why it took so long to actually get this result so mathematicians from more than 20 years we're actually thinking that should hold.
Well then some people said it shouldn't hold their context on it so there was actually it wasn't clear if it's true or not.
So it also requires courage to just keep going and believe in things.
Let me just finish by an overview of what we have done.
We have studied representation of the added groups. We saw that we know the building blocks and almost all cases, and we now understand that category decomposes and blocks, each block is equivalent to that zero block, and they are equivalent to nice models.
Let me stop here and ask for more questions.
Yeah, let's start with you.
Okay.
So the question is how do we, one could think about why a levy subgroup and a subgroup of G, is there a way to sort of understand it?
I don't know.
It's a good question.
So the question is how do we, how do.
So I need to think a bit more about, and the whole theory is called type theory. These are types, but it's a good reason to believe they exist.
I don't know other than, well, I know how to prove it.
You can feel free to say, tell me the answer later, but if you take G to be GL2 and M to be GL2, then you take a positive depth supercuspital.
Can you tell us easily what like the G zero and the M zero and the sigma zero are?
So I take G equals M, I take a supercuspital of positive depth.
If you assume that it's honest positive depth, so that it's not just by a positive depth character, then the G zero will be an isotopic Taurus.
So,
I hope you can grab these words before it might look like something like A, A, B, like matrices of that shape and that over, but that's one example instead of P you could also use something else there and something that becomes isomorphic to diagonal matrices of a field extension.
This case wants to join the square root of P.
Okay, so that would be your G zero and that case.
I don't know how like related this is, but there was like a slide a while ago where you have like this, I think it was like the first slide you started to think about algebra and how like the referential is not a few days.
Oh yeah.
You mentioned that like, this is like, like a lot of people care of like, how about this, can you say a little bit about that?
Yes, I'm not sure I'm getting about that.
Yes.
Well, in general, there's a whole story about hacker algebras where you take more general affine line bits of these are groups that are generated by appropriate reflections.
And then you have similar relations for all the, I mean, the relations look like that.
It just shows up in a lot of areas of mathematics. Some are people who study algebras, they like these hacker algebras, they try to study what the representations are.
It's similar to something that has a nice structure and that pops up naturally in various different situations.
Not sure how. What was exactly the question why mathematicians care?
Or like it pops up in my research.
It pops up in your research.
I just wanted to know like, what are the areas it pops up.
Well, in your response.
We can try to bring some afterwards a bit more together.
I meet regularly people who say they care about it. Now I just met another one, which is an excellent.
I have a question about this slide actually. Thank you for asking.
The equivalence class, you said it's about twisted with a random factor. Yes.
So should I think that all the twists of the trivial are in the same, like, yeah, yes, but then they don't have extensions.
Or do they have extension?
I'm not able to answer questions about extension.
You're right. So everything that if you take an unverified character here and induce that will be also content in that.
Well, so there must be somehow extensions otherwise it wouldn't be in that block.
I mean, these are really the blocks. So these are the minimal blocks. You can't divide them any further.
Okay, so there must be somehow some extensions.
Yeah.
So these are really the smallest possible blocks.
In other words, it's a definitional block.
One more question.
Thank you.
