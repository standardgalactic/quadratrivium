I'm going to talk about complexity science from a perspective that's different from what
you've heard so far.
I'm going to talk about the concept of the maximum information entropy principle and
how it can be used as a foundation for constructing complex systems theory.
And I'm going to illustrate it with the example that I have been working with for the past
decade, which is ecosystems.
So I want to start by highlighting something that's been mentioned earlier, and that is
that there really are prevalent patterns in ecology.
Wherever we look, whether we're looking at insects or trees or birds or even microorganisms
in the human gut, we see certain patterns showing up over and over again.
I'm going to just very quickly describe some of these.
There's the famous species area relationship.
As you sense this bigger areas, you see more species.
But how fast does species richness increase as you increase the sample area?
Well, it turns out it increases with area faster than a logarithm of area, but slower
than a power law.
Why is that?
Let's see if we will explain it.
We will explain it.
If we look at the distribution of abundances of species, let's take a hectare of Amazonian
forest and might have a hundred species of trees.
Some of them will have thousands of individuals and others will be very rare.
They may be represented by only one or two individuals.
What is the distribution of abundances across the species?
It turns out if you plot the logarithm of the number of individuals against the rank
order of the species, with the most abundant species having rank one and the least abundant
species have rank whatever the number of species is, you get over and over again from one ecosystem
to another a hockey stick curve.
It's actually the Fisher log series distribution.
It does a marvelous job describing most species abundance distributions, except I have to
add in systems that are quite disturbed and changing rapidly, like under succession in
the aftermath of fire.
If we look at spatial distributions of individuals within species, we see certain patterns.
I don't have the time to go into this one in detail, but it shows up over and over again
as we look at real data from sensor scene real ecosystems.
If we look at the distribution of body sizes or metabolic rates of individual organisms
across all the individuals in the ecosystem, say within the trees or within the insects
or within the birds, we see a pattern which is roughly a power law with an exponential
tail.
And if we look at the distribution of metabolic rates of individuals versus the abundance
of the species that the individual is in, we see a relationship on log-log plot with
a slope of minus one, meaning metabolic rates of individuals are inversely proportional
to the abundance of the species the individual is in.
There's scatter around some of these patterns, obviously, but these are the general prevailing
patterns that show up.
There's more.
There's a pattern that Neo Martinez showed with the linkage distribution of connecting
nodes in a food web.
And all of these patterns, including the linkage distributions, I'm going to show you can be
explained with the concept of maximum entropy.
So the complex systems dilemma that ecologists face and people in other fields as well is
that there's just a tremendous number of mechanisms.
And it's very difficult to know what to choose.
If we look at ecosystems, there's predation, mutualism, competition, dispersal, speciation,
birth, death, pollination, et cetera, et cetera, nitrogen uptake strategies, disease
resistance, density dependence of population growth rates, sociality, et cetera.
With all of these mechanisms, and this is only a partial list, how is the modeler supposed
to begin?
How do you pick?
You can't put all of these into a model.
You don't have enough knowledge to know how to parameterize them.
And so what we do is, if we're interested, if we really like predation, we put that into
our model and we ignore a lot of other things, or we put dispersal in, or climate sensitivity.
But all of these potentially could have an influence on those patterns.
There's also multiple abiotic factors that influence patterns, variability in the climate,
for example.
There's stochasticity, historical contingency, the ambiguity of system boundaries.
Where do we draw the edge so that we say, ah, we're studying what's inside that system?
Ecosystems don't have edges.
Everything is connected.
There's a difficulty in conducting large-scale controlled experiments.
We can do small-scale climate manipulation experiments or removal experiments, but doing
them on very large scales is very difficult.
There's a difficulty isolating the particular stresses that act on systems.
So if we're going to try to predict what the future will look like, we have difficulty
knowing how to characterize all of the disturbance mechanisms that are operating.
And then finally, ecologists face a very sad problem, which is the increasing degradation
and loss of the very objects of our study.
It would be like if you were a particle physicist and you had to wake up the next morning and
worry that the Higgs particle went extinct.
They don't have to worry about that.
We do.
And we see the plots we study, the ecosystems, the species that we focus on.
We see them disappearing.
All of this makes ecology the study of a complex system.
Basing complex systems models and theory on explicit mechanisms and traits chosen in advance
often results in arbitrary choices of governing mechanisms.
Nice adjustable parameters and models and theory that are difficult to falsify.
Now, I'm not against mechanism, and in fact, what I'm going to show you is that we can
get at the driving mechanisms in ecology, but we're going to come at it backwards.
We're going to start with a statistical information-theoretic-based approach and back out at the end insights
into what mechanisms may be particularly important.
So we're not going to start by building a model that arbitrarily chooses x, y, and z
as the driving mechanisms.
The maximum information entropy concept.
Just what the heck is it?
What are we maximizing when we maximize information entropy?
Well, first, what is information entropy?
It's defined by Shannon, I'm not sure if there's a pointer on this, who's defined by
Shannon in the 40s.
Information entropy is that familiar expression, the sum over n of p of n log p of n.
And it's a measure of the lack of structure or detail in a probability distribution that
describes your state of knowledge of a system.
When we're maximizing information entropy, we're finding the smoothest possible probability
distribution that is compatible with what we call the constraints that arise from your
prior knowledge.
So you start out, when you do science, you always start out knowing certain things and
you want to extend that knowledge predicting new probability distributions or improving
existing ones.
So if you use your prior knowledge as a constraint and maximize information entropy as so defined,
you end up with something interesting.
You end up with the smoothest possible distribution that is compatible with all of your prior
knowledge.
Now, the method of Lagrange multipliers is the mathematical tool, but we won't go into
that.
That's how you do the maximization.
And basically, it's illustrated here.
Here are two functions.
Let's suppose we know two things about a system.
We know the mean and the variance.
And let's suppose both of these distributions have the same mean and variance as what you
know to be true, because you had measured it.
If that's the case, which one would you pick?
You should pick this one.
You should prefer the one with the higher information entropy, because it makes fewer implicit
unwarranted assumptions.
There is, if this and this are compatible with all your knowledge, you don't want to
pick this one, because it's implicitly making some assumptions about other things that you
don't have any reason to believe.
So maximizing information entropy picks the smoothest possible distribution that's compatible
with prior knowledge.
It was all worked out by Ed Jains, a physicist in the 80s, and this is the method we're going
to use.
If we want to sort of look at the architecture of this approach to theory, what we're doing
is we're going to start with some insider information at the macro scale and infer distributions
that describe the micro scale.
So at the macro scale, we have what we call state variables.
In thermodynamics, these would be the pressure and the volume and the temperature, let's
say, of a cylinder of gas.
And knowing those state variables, thermodynamics allows you to predict the distribution of molecular
velocities in that container of gas.
So what we're going to do is start out with some state variables, which we have measurements
for in ecology, and derive probability distributions.
And here are the kinds of things you could use this for, abundances of species.
Speakers of languages, if your state variables describe how many languages there are and
how many people altogether are speaking them, 7.2 billion people speak 6,000 languages on
earth.
What's the distribution of speakers across languages?
Incomes of people, sizes of organisms, spatial distribution, speeds of molecules, linkages
per node, and food webs, occupancy, and a spatial grid, et cetera, et cetera.
That's the basic structure.
There have been many applications of MACS since Jane's early work, improving image resolution
in medicine and forensics, inferring gaps in economic data, such as input-output tables,
deriving the laws of STAT-MEC and thermodynamics, that was Jane's contribution, improving estimation
of climate envelopes for species.
In other words, predicting distributions of how distributions of species are influenced
by the climate parameters in ranges where they are found.
Predicting linkage distributions across nodes and networks.
These are some of the past applications.
And the goal of the maximum entropy theory of ecology, or MEET for short, which I'm going
to very quickly summarize, is to predict the shape of STATIC and DYNAMIC micro-scale
patterns in ecology.
All those patterns I showed you on the second slide.
Species-area relationships, abundance distributions, metabolic rate distributions.
We're going to predict the shapes of those, and we're going to do it with one theory across
all the taxa that we study, plants, arthropods, birds, microorganisms, across spatial scales
from small patches to large biomes, across habitats.
We don't want different theories for different habitats, and we're going to do it with no
adjustable parameters, and without prejudging what the specific mechanisms are that drive
the system.
All order.
And I'm going to, with limited time, you have to go through this very quickly.
The basic idea is we're going to show you just one model of this theory, one model realization
called the ASNI model, because the state variables in this model are area, S for number of species,
N for the total number of individuals, and E for the total metabolic throughput.
And a diagram of what the theory is doing is it's allocating metabolism to individuals,
individuals to species.
We can extend this model, allocate species to genera, genera to families, and so forth,
and work our way up a taxonomic tree, and we'll get to that very briefly later.
So this, you're not going to be able to, I'm not going to be able to go through this with
you in any detail, but Maxend gives the shape of two core functions that are the core of
the theory, a spatial distribution function pi, and what we call the ecological structure
function, which is a function, it's a probability distribution, it's a joint distribution on
abundance and metabolism, and it's conditional on the state variables S, N, and E. And from
those direct outcomes of Maxend flow all the metrics, all the probability distributions
that describe all of those, plus more.
So we can, in principle, now test, and we've done numerous tests, we've looked at about
25 distinct habitats, we've looked altogether at over 100,000 species, and over a billion
individuals.
Here's a list of some of the habitats that we've tested theory with, and there are some
pictures of them.
I'm going to just show you a few tests, I have dozens of slides of tests, but this is
observed abundance versus the Maxend theory predicted abundance, it's a data dump type
test done by Ethan White and his collaborators.
They looked at almost 16,000 communities of plants, mammals, arthropods, and birds.
So it's a huge data set, each community has many, many species and individuals in it.
And you can see the observed versus predicted abundance distribution from the theory matches
up very nicely.
Zhaozhou, at all, did a test of the metabolic distribution prediction.
And except for a deviation, which, if you asked me about it, I think I can tell you
why it might be there at the very upper end, it also does a very good job predicting the
theory, predicting the behavior.
The most surprising prediction of the theory, and the one which, when we saw it, we said,
oh crap, the theory's dead, it can't possibly look this way, nature can't possibly look
this way.
Let me tell you what the prediction was.
Traditionally, people plot species area relationships by plotting the logarithm of the number of
observed species versus the log of the area within which they were observed.
And you get a curve.
Since the curve is very slight and people think it's a power law, it isn't.
But it can resemble one a little bit.
You get a curve.
If you took lots and lots of species area relationships and plotted them all on the same
graph, you would fill space with data points.
There is no rhyme or reason, no coherence in a log-log plot of species versus area.
Maximum entropy theory of ecology predicts that if I take all those species area relationships
and replot them in such a way that I've described, everything should fall neatly on a simple line.
It's called scale collapse.
And I want to show you very quickly how, operationally, what it is.
At any point on that curve, I can define a tangent, z of A. That's the slope on the
log-log plot.
At that same value of area, that data set had a certain number of species and a certain
number of individuals.
The Maxent theory told us if we plot the slope z of A against the log of the abundance at
that area A, n of A divided by s of A, so we're replotting tangent versus number of
species and instead of area, n of A divided by s of A. What the theory says is all data
from all species area relationships should all fall on this solid line.
And when we predicted that, we'd grafted up, we said it's inconceivable that nature behaves
this way.
And when we took all the data we could from, in 2008, we took all the available species
area data, we could get our hands on about 30 different data sets, plotted up all the
tangents on this axis and log of n over s on that axis, and all the data fall nicely
around the predicted line.
Many people claim that species area relationships can be well approximated by a power law with
a slope of a quarter.
If that were the case, all of these data points would lie on that dashed line.
The power law, nobody in their right mind should ever again say species area relationships
are power law, they're just not.
And yet that idea persists.
We can extend the theory, add new state variables like genera or add another resource.
When we do that, we get new predictions that are interesting.
And in particular, when we add genera, we do something, we alter what's called the size
abundance or metabolism abundance relationship, and it looks more and more realistic as we
add more and more higher taxonomic levels.
When we add more resources, we change the abundance distribution prediction and usually
make it worse.
So what happens is when you add a second or a third resource, you predict more and more
very rare species.
It looks like nature is consistent with one resource, namely energy.
Of course, we call it energy, it could be water, it could be nitrogen, it could be anything.
I want to make just two slides about general comments, I've sort of put all my philosophy
off for the end.
Complex systems versus what I call hyper-complex systems.
Complex systems, in the sense that we can apply max-ent theory and learn from it what's
going on, are systems that can be decomposed into a macro and a micro level, or perhaps
three levels, but let's stick with two for simplicity.
In thermodynamics, we have bulk properties such as pressure and volume, and then we have
micro scale properties like the kinetic energies of the molecules.
In economics, we have firms, nations, total wealth or GNP, and then we have individual
incomes at the micro level.
Linguistics, we have the total number of languages, and the total population versus the numbers
of speakers of each language, that would be a micro property.
In ecology, we have bulk properties such as species richness, total abundance, total metabolic
throughput of the system, and then we have all these micro distributions that I've talked
a little bit about with you.
These are the types of systems for which max-ent has a natural application.
If your system breaks down into this macro-micro pattern, you can use max-ent.
However, there are lots of interesting systems, and turbulence is the best example of what
I call a hyper-complex system, because you can't make a macro-micro distinction in turbulence.
Life phenomena occur across all scales, across a continuum.
There isn't a natural discontinuity in turbulent media, and so this would be an example of
a system where we couldn't apply max-ent.
At least right now, we don't know how to do it.
We've organized a workshop at the Santa Fe Institute in the spring to talk about whether
there might be a way to do it.
At least for now, it looks pretty, oops, uh-oh, I'm not sure why.
Can you use the last minute or two for questions?
Okay, yeah, I think I showed my last slide, but I'm not sure.
Anyway, because this went blank.
Okay.
So we have more switching computers, we have time for a few questions, where is the microphone?
And so Sharon, we'll just come up with one second.
Great though.
Yeah, there was a, oh, wait, just one second.
So regarding the complex versus hyper-complex, is it the discretization that is important
or the number of relationships, do you think?
No, the number of relationships can be virtually unlimited.
It's the fact that there are things you want to know about, which are the micro-distributions.
The distributions of incomes across people or speakers across languages or individuals
across species.
And there can be arbitrary many of those things you want to understand at the microscale.
And at the macroscale, there are the coarse-grained things that we know how to, we can go out
and measure them.
And the question is, do the coarse-grained information through the Maxent principle determine
the micro-scale distributions?
And it turns out, in ecology, they do.
I was asking about the complexity, hyper-complexity, and the question of levels.
Okay.
So the question of continuum is that, you said two, and it could be more than two, so you left that open.
Yeah, that's a great question.
I'm hoping at the Santa Fe workshop, we'll maybe be able to answer that.
Right now, I don't know the answer.
I suspect that if there's a discrete number and you have enough computer power to run
the Maxent agenda down the hierarchy, you could do it.
How about machine learning?
Yeah, exactly.
But maybe we'll know the answer later.
Great.
Thank you again.
Okay.
