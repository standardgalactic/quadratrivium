In a lot of textbooks on Lie algebras, they start off with the definition, which is a bit clunky, but here it is.
Lie algebra is a bunch of vectors with an additional operation, where if you have two vectors x and y in the Lie algebra,
there is a square bracket, x, y, that gives you another vector, also in the Lie algebra.
But this square bracket operation is not randomly defined, and it has to satisfy certain properties.
Now sure, that's a definition, but why do we consider these properties in particular?
Why not something else?
It turns out that in understanding the y, we can also intuitively make sense of trace AB equals trace BA for any two matrices.
In the previous overview video, I mentioned that the relationship between Lie group and Lie algebra
is that the Lie algebra is the tangent space of the Lie group at the identity of the group.
This is what I would call the manifold view of this relationship.
However, throughout the video series, we are mostly considering matrices like SON and SUN,
where this notation has been covered in a previous video.
Because these are matrices, there is a more tangible view of this relationship between Lie algebra and Lie groups.
Any point on the Lie algebra can be thought of as a vector from the identity,
and because the Lie algebra is tangent to the Lie group, this vector is just a tangent vector of the Lie group.
This in turn can be thought of as a velocity vector when we traverse along the Lie group.
Equivalently, when going along this curve from the identity for a small period of time epsilon,
we would arrive at a point that corresponds to a small rotation.
This is a rotation because the Lie groups we are considering consist of rotational matrices.
Let's call this small rotation R, then the velocity vector on this manifold
would be the usual rate of change formula, going from I, the identity,
to R, a small rotation.
This velocity vector is something that lies on the Lie algebra,
but right now, it is still too abstract, so let's use the example of a 2D rotation.
Right now, we haven't done anything to the plane, and it represents the identity matrix.
Then, we rotate.
Technically, I should have illustrated a small rotation, but it would not be that visible,
but anyway, we call this rotation R.
How can we make sense of the velocity that is the rate of change of the matrix when we do this rotation?
We focus on the action of these matrices on the points.
Let's say this green dot is the point X after rotation, so that this point is R acting on X.
Tracing back the original position of X, we do the inverse rotation.
But we can also think about this original point as the identity I acting on X.
The initial velocity of this point X, when we do the rotation,
is then written with this rate of change limit formula.
This expression can be massaged with the linearity of limits,
and the matrix in the limit is the tangent vector in the manifold view, which is on the Lie algebra.
Now, call this matrix A, so the velocity of the point X when we rotate is A acting on X.
If we do this to all the points X, we get a vector field, where at every point X,
we attach to it the vector AX.
Does that sound familiar?
These are mentioned before in the previous two videos in the series.
In some sense, this vector field generates rotations,
because if you simply follow this vector field, you rotate every point on the plane.
So, an alternative way of thinking about Lie alters
is that a linear vector field AX, where A is a matrix in the Lie algebra,
will generate a transformation, and in this case a 2D rotation,
which is in the Lie group we consider.
I would call this alternative view the vector field view.
Again, from the previous video, the generated transformation should be of the form E to the TA,
where T is the time you choose to take to follow the vector field.
A Lie algebra element A exponentiates to become an element of the Lie group.
These two views with either the manifold or the vector field
are equally valid and important and will switch between them.
But let's get some ideas on how these Lie algebra matrices A look like.
Let's go back to this 2D rotation again using the vector field view.
If we focus on 1.10 and see what it can be rotated to,
because the origin is fixed, it is constrained to be on the unit circle.
So, the initial velocity vector can only be up or down,
but the magnitude or the speed can be whatever.
This up or down vector is then 0A for some real number A, positive or negative.
For now, let's assume A is positive, so initially this point goes up with speed A,
and we do an anticlockwise rotation.
Now what should be the initial velocity of another point, 0, 1?
It should be towards the left because we are rotating anticlockwise,
and the speed should be the same as the previous point we considered,
so the vector is negative A0.
Even though we have assumed A to be positive and hence an anticlockwise rotation,
if A is negative instead, then the velocity vectors would flip directions,
but the velocity of 0, 1 would still be negative A0.
So the corresponding matrix of the vector field would be 0, negative A, A0,
where the first column describes the vector attached to 1, 0,
and the second column describes the vector attached to 0, 1.
So these matrices are really everything in the lean algebra for 2D rotations.
A bit of notation here in case you want to search online.
2D rotations form a group denoted as SO2,
and the lean algebra of it is denoted with the lower case
with a weird font called the Fracture font.
Anyway, we have established that lower case SO2
is a collection of these anti-symmetric 2x2 matrices.
But what about lower case SO3?
Can we do something similar?
Well, it's not as easy, but let's proceed anyway and see where the difficulty is.
Again, first focus on the point 1, 0, 0 on the x-axis.
This time, whatever the 3D rotation is, it has to stay on the unit sphere.
This in turn means that the initial velocity of this point
could be anything as long as it's tangent to the sphere.
In general, a vector on this tangent plane should be of the form 0ab.
The x-component of the vector is 0
because the tangent plane at this point only extends along the y- and z-directions.
Using a similar argument, if we focus on the y-axis point 0, 1, 0 instead,
then the initial velocity vector could be anywhere on the tangent plane.
So it is of the form c0d.
And this time, it is the y-component of the vector that is 0.
Finally, if we consider 0, 0, 1 instead,
then its initial velocity would take the form ef0.
And this time, the z-component of the vector is 0.
So the corresponding matrix for this vector field would look like this
because like the previous two decays,
the first column describes the velocity of 1, 0, 0.
The second column describes the velocity of 0, 1, 0.
And the third column describes the velocity of 0, 0, 1.
However, it's not like we can have these ABCDEF to be anything.
For example, I don't think this velocity field at shown corresponds to a rotation
because these two points seem to be moving closer together.
So there must be some constraints on these ABCDEF.
You can perhaps come up with some geometric arguments,
but for more generality, we do the following.
We start with the defining relation of rotation matrices.
r transpose r equals identity.
Consider r as a small rotation,
which can be thought of as starting from the identity
and wander off in the direction of a for a time epsilon in the manifold view.
Technically, there is a higher order correction,
but for now we don't care about it.
This orthogonality condition can then be written with r replaced by i plus epsilon a.
Again, we are ignoring higher order terms.
Then we simply expand this product.
But now we get an extra quadratic term, which we ignore.
After all, these calculations are only accurate up to order epsilon,
so we shouldn't keep higher order terms anywhere.
So we are left with this equation,
which implies a transpose equals negative a to make the first order term vanish
because a is the tangent vector at identity,
a is in the Lie algebra.
Our calculation means that if a wants to be in the Lie algebra,
then it better be anti-symmetric,
satisfying a transpose equals negative a.
However, this orthogonality condition is only one of the two for being a rotation matrix.
To be classified as a rotation and not reflection,
it has to have determinant plus one.
Perhaps it is possible that this poses an extra constraint on what a can be.
If we adopt a vector field view again,
then any small rotation or can be represented as an exponential of some matrix a,
because it is generated by a matrix in the Lie algebra.
This means that a determinant condition for a rotation matrix
can be rewritten as the determinant of a matrix exponential.
However, if you still remember the previous video on trace,
this determinant is the exponential of t times the trace of a.
So by taking the usual logarithm on both sides,
we have a being traceless.
So the determinant condition poses a constraint,
which is that a needs to be traceless.
But is it really an extra constraint?
a being anti-symmetric derived from the orthogonality condition
already implies that the diagonal entries need to be zero,
and hence the trace must have been zero anyway.
So the constraint from the determinant is not really an additional one,
and the Lie algebra SON really consists of all these anti-symmetric matrices.
In the case n equals 2,
we have already seen that the Lie algebra consists of all these 2 by 2 anti-symmetric matrices.
Our computation just generalizes this to higher dimensions.
However, I want to emphasize why it is useful to consider the Lie algebra SON.
This anti-symmetric condition is really easy to solve or parameterize.
Even for n equals 4,
it is very easy to write down what an anti-symmetric matrix looks like.
It's just that when you reflect across the diagonal,
you flip the sign.
However, if you go directly to the Lie group,
when you face these equations,
it is very difficult to see what these matrices should look like.
However, our discussion suggests that R can be written as the exponential of A,
where A is in the Lie algebra, and so it's anti-symmetric in this case.
Luckily, we know very well what an anti-symmetric matrix looks like,
and so we can write these rotation matrices very easily as an exponential.
Sure, you still have to compute the exponential,
but at least this gives us a way to easily parameterize the rotation matrices.
In this chapter, we will see two main properties of the Lie algebra that we can deduce so far.
Which highlights why we want to know both views of the Lie algebra.
That's because the first of these two, that the Lie algebra is a vector space,
is a really obvious property if you use the manifold view.
But it's a lot less obvious if you use the vector field view of Lie algebra.
After all, in the manifold view, the Lie algebra is a tangent space.
So if you have two tangent vectors A and B on the tangent space,
then any linear combination of A and B should also lie on this tangent space.
That's what I mean by it being a vector space.
But if we switch to the vector field perspective, it's not as obvious.
Let's focus on the Lie group being rotations, and the vector field by matrix A generates a rotation.
Now suppose another vector field by matrix B also generates a rotation.
What about the vector field from a linear combination of A and B?
It isn't that obvious that the linear combination also generates a rotation.
The way to show this in the vector field view is first, if we scale these vectors,
the rotation that it generates would have different speeds,
but a scaled vector field still generates a rotation.
In other words, if the vector field AX generates a rotation,
then the scaled lambda AX also generates a rotation.
The next step is to suppose that this vector field AX generates a rotation
and another vector field BX that generates a rotation.
We want to show that there's some also generates a rotation.
To do this, let's consider a new rotation that is equivalent to following the vector field BX for a small time epsilon,
then follow the vector field AX for another small time epsilon.
Then we ask the question, what is the vector field that generates this new rotation?
For a general point X, we first follow the vector field BX,
but because we only follow for a small time epsilon,
we end up at a position I plus epsilon BX, which is accurate up to order epsilon.
Next, we want to follow the vector field by A,
but because we have moved to a slightly new position,
the vector attached to this point is A acting on the new position, I plus epsilon BX.
Following this for a small time epsilon,
we end up at I plus epsilon A, I plus epsilon BX,
again, accurate up to order epsilon.
So any point X travel to this new point I plus epsilon A, I plus epsilon BX with this new rotation.
The displacement of this vector is then the new position minus the original position X.
This can be computed exactly, but similar to the previous calculations,
we ignore the higher order terms in epsilon,
and so throughout this endeavor, the displacement is epsilon times A plus BX.
To find the vector field that generates this new rotation, we want to see the velocity.
It should be the displacement, which is what we calculated before,
divided by the time elapsed, which in our case is 2 epsilon.
This gives the velocity to be the average of these two vector fields.
So the answer to what vector field generates the new rotation
is the average of the two.
In other words, if we know that the vector fields BX and AX both generate rotations,
then the average can also generate rotation.
And if we double the speed of this new rotation,
then we can deduce the sum of these two generate rotation.
Coupled with the previous result about scaling the vector field,
we know that if a vector field is a linear combination of vector fields that generate rotation,
then itself would also generate one.
This is a bit more work than the manifold view,
but in any case, we have to deduce the first property of the algebras in these two views,
and now we move on to the next property, which isn't that intimidating, I promise.
Unlike the previous one, this is a lot easier when you adopt the vector field view.
Suppose the vector field AX generates a rotation.
We want to know what is the vector field corresponding to the matrix GAG inverse,
where G itself is a rotation matrix.
From the previous video where we discussed the properties of trace,
we have this new vector field to be just the vector field AX,
but the whole picture is rotated by G.
Now, if the vector field AX generates rotation,
and we simply rotate the whole picture,
the resulting vector field should also generate rotation,
just with a different axis of rotation.
So, we have established that if the vector field AX generates rotation,
and G itself represents a rotation,
then GAG inverse X also generates a rotation.
This is what I would say in the vector field language.
In terms of the lean theory language,
if A is an element of the lean algebra, and G is in the lean group,
then GAG inverse is in the lean algebra.
This is the second property of lean algebra we want to get to.
Let's go to the manifold view to see what we have done in the previous chapter.
Essentially, if A is a tangent vector, that is an element of the lean algebra,
then sandwiching it between G and GAG inverse,
we get another tangent vector.
However, G is itself a rotation,
and we can treat it as a process from the identity 2G.
So, in this manifold view,
we can also think of smoothly transitioning from A to GAG inverse
by smoothly rotating from the identity 2G.
These intermediate vectors are created by the intermediate rotations
between the identity and G.
In fact, because G itself is a rotation,
it is generated by some other vector field BX.
So it can be written as E to the B for some B in the lean algebra.
And these intermediate vectors are then in general A sandwiched between E to the TB
and E to the minus TB, where T smoothly goes from 0 to 1.
T equals 0 corresponds to the original A,
and T equals 1 corresponds to GAG inverse.
Now, saying that this continuous family of vectors is denoted as F of T,
we are interested in the rate of change when you increase T.
Visually, this rate of change would be the velocity vector of the tip.
In particular, at time T equals 0,
F prime of 0 refers to the initial velocity of the tip of the vector
as you move across this curve E to the TB, A to the negative TB.
This initial velocity will, for now, be denoted as ad subscript BA.
Ad stands for adjoint.
And for now, I want you to think of this as some sort of directional derivative.
Originally, you just have the tangent vector A,
and then you embark on this path, which in some sense is generated by B,
because G is generated by B.
So this F prime of 0 is kind of like the derivative of A in the direction of B.
Not quite, but spiritually like that.
And we can see exactly what this derivative is in terms of A and B.
Using the definition of derivative, F prime of 0 can be written like this,
where this is just F of T, and A is the starting point of your journey.
The next step is to expand F of T to first order in T.
Any higher order is negligible because we will divide by T afterwards.
This step is just expanding the exponentials to the first order.
The next step is to further expand, but again, only up to first order in T.
Technically, there is a T squared term,
but when divided by T, it vanishes when taking a limit T tending to 0.
Finally, we compute this to be BA minus AB.
So F prime of 0, which is supposed to be like the derivative of A in the direction of B,
or adds up BA, is equal to BA minus AB.
This manifold view for now is a bit too abstract,
so let's switch to the vector field view and see what's going on.
We focus on 3D rotations because the 2D ones aren't really that interesting.
I will call the matrix corresponding to this vector field as Z,
because it generates a rotation along the Z axis.
The direction is from the right hand rule if you point your thumb towards the positive Z axis.
Similarly, this vector field is YX, generating rotations along the Y axis,
and again here, the direction is from the right hand rule
when the thumb points towards the positive Y axis.
And finally, X generates rotations along the X axis.
We will work through an example, namely the adjoint action of X on Z.
That is, we start with this vector field by Z,
then rotate the whole vector field along the X axis,
because that's the rotation generated by capital X,
and then see the rate of change of the vector field.
To do this, we can observe how the vectors attach to certain reference points change.
In particular, we choose a point on the Z axis and another point on the X axis.
Let's start with the point on the Z axis.
Originally, the vector attached to this point is the zero vector,
because a rotation along the Z axis keeps this point fixed.
However, when we slightly rotate the vector field about the X axis,
this point picks up some small but non-zero vector.
In particular, this small vector is along the X axis direction.
And in this case, it points to the negative X direction to the left.
So, from the original zero vector attached,
to now a small vector towards the negative X direction.
The rate of change of the vector attached at this point
is also pointing in the negative X direction.
For now, we don't care about the quantitative details,
like what exactly is the length of this vector,
whatever the length of the vector,
the rate of change should be the at sub XZ vector at this point.
In other words, the vector field corresponding to at sub XZ at this point
should be a vector pointing in the negative X direction.
We now need another reference point to determine which rotation this at sub XZ generates.
This time, in the original vector field,
the vector attached at this point is towards the positive Y direction.
If we focus on this vector as we rotate along the X axis,
this vector simply rotates.
So, originally, the attached vector is towards the positive Y direction,
then it simply rotates as we do the X axis rotation,
so the rate of change is along this perpendicular direction,
which is in the positive Z direction.
So, the at sub XZ vector field would attach this point
with a vector pointing in the positive Z direction.
Adding to the previous result that at the Z axis point,
the vector points in the negative X direction,
we now have two vectors of this vector field.
Even only with these two vectors,
you can probably see that it corresponds to a rotation along the Y axis.
However, in a little bit more detail,
if we use the right hand rule,
this rotation is about the negative Y axis.
So, this at sub XZ is equal to negative Y.
Technically, we haven't taken into account the lengths of the vectors,
so it could be negative 0.5Y or negative 2Y,
but if we actually do this in more quantitative detail,
this statement is true if we take X, Y and Z
to generate the rotation at unit angular speed along their respective axes.
We can also deduce the other adjoint relationships using similar arguments,
which is very similar to how cross-product works.
In any case, we move on to some properties of this adjoint action,
which will finally answer why we want the Lee bracket operation
to fulfill these three properties when we rigorously define a Lee algebra.
I know very well that we can use this BA-AB thing
and show the properties algebraically,
but I want to show the properties more directly from this derivative idea,
and there is a very good reason for using that, as we will see.
Anyway, the first property is actually relatively simple.
If you have a vector field AX and you rotate the whole vector field
using a rotation that is generated by A itself,
then you still end up with the same vector field AX.
So, the rate of change in the vector field is 0
because the vector field hasn't changed.
So, ads of AA equals 0.
That's the first property.
The second property is a lot trickier,
which is basically that this adjoint action is linear in both arguments.
We deal with the first one, where the original vector field
is a linear combination of other vector fields.
As we discussed before, we want to consider the A vector field rotated,
giving F of T that sandwiches A between E to the TB and E to the negative TB.
If we now suppose that the original A is a linear combination of A1 and A2,
then we can simply separate this F of T into a linear combination of two of these,
corresponding to A1 and A2 respectively.
If we now take the derivative at T equals 0,
then we should have ad sub BA on the left side,
and the right side will be a linear combination of the adjoints.
So, that's one part of the property.
If you want to, you can think about how to show this property in the vector field view.
The other part, which is the harder part,
is what if B itself is a linear combination of two vectors B1 and B2?
Again, I know very well that the algebraic way is faster,
but it doesn't give us much insight.
Instead, we separate this into two special cases,
one with simple scaling, and the other being the sum of B1 and B2.
If we go back to this vector field view,
the adjoint action concerns the rate of change of the vector field.
In particular, if we slow down the rotation of the whole vector field by a factor of lambda,
then the rate of change of the vector field should also slow down by a factor of lambda.
The rotating slower by lambda part corresponds to making the subscript lambda B.
And the fact that the rate of change is made slower corresponds to lambda times ad sub BA.
So this is the first part of this endeavor.
If we scale the vector field by lambda, then the adjoint also scales by lambda.
What about B consisting of a sum of two vector fields?
This one is a bit tricky.
However, because we only care about small time t,
this can be separated into two exponentials.
This step is fairly similar to how we deduced that the sum of rotation generating vector fields also generates rotations.
Anyway, after this step, we realize that in the middle,
we have A sandwiched between e to the tB2 and e to the minus tB2.
So to first order in t, this can be written as A plus t times the adjoint,
because A is just the original vector field or f2 of 0,
where f2 is when you rotate with respect to B2,
and the adjoint is the derivative of f2 at 0.
But now we have another thing sandwiched between two exponentials.
So again, to order t, we can write it in terms of the vector field being sandwiched in the middle
plus t times the adjoint acting on that vector field.
And now, luckily, we have already demonstrated that if it is the original vector field that is a linear combination,
it is a linear combination of the adjoints.
But again, because we only care about terms up to order t,
we don't need the t2 term.
And finally, we can read off f' of 0 here.
A is f of 0, the original vector field, and the sum of these adjoints would be f' of 0.
So after this rather long derivation, we have arrived at adjoint of B1 plus B2
is the sum of the adjoints.
And this is basically the end of this bilinearity property of the adjoints.
This leads us to the third property, that is, anti-symmetry.
This one will be dealt with much more algebraically.
It starts off with the first property that we derived,
where if the two vector fields involved are the same, then the adjoint is 0.
The next step is to utilize the bilinear property that we just derived,
which separates this into four terms.
Then we can use the property that the adjoint of a vector field on itself is 0 again
to see that the cross terms are of opposite signs to each other,
i.e. ad sub AB equals minus ad sub BA.
I have considered trying to understand it from the vector field perspective,
but it's a bit too convoluted.
I encourage you to try this though.
The final property looks quite intimidating and somewhat unusual,
but it is reminiscent of the product rule.
On the left side, we have ad sub A acting on another adjoint.
The first term on the right side has the ad sub A acting on B,
and the second term has the ad sub A acting on C instead.
The reason why it is similar to the product rule is that in the normal product rule for derivatives,
the derivative operator also sort of distributes over F and G respectively.
So this is why I call this property the product rule.
So how do we show this property then?
We start off with a single adjoint.
This gives us a new vector field, so let's just rotate this new vector field with G.
However, it should be the same as simply rotating every vector field inside,
so both B and C are rotated with G on the right side.
Now, as with the usual adjoint action, we suppose G is exponential of another vector field,
and to first order in T, we can write the left side as the original vector field
plus T times the adjoint of A on this vector field.
On the right-hand side, we do a similar thing for each of B and C.
The next step is to use the bilinear property of the adjoint to expand into four terms.
Again, we ignore the T-square term here.
And if we now compare the order T terms, we get a product rule property of the adjoint action.
So these are the four properties of adjoint that we have seen and shown,
but I want to emphasize that these are not some random properties.
They are just to compare with the directional derivatives that you see in multivariable calculus.
Here, we define the directional derivative as the dot product of the direction v and the gradient.
The directional derivative also satisfies this bilinear property,
linear in the function f itself, and linear in the direction vector v.
But the more important similarity here is the product rule property.
I know I've said this similarity a minute ago,
but this is due to the ads of A being distributed across B and C on the right side,
just like how the directional derivative operator is distributed across the two functions f and g.
It shouldn't be too surprising this adjoint thing shares similarity with derivatives.
It is literally defined using derivatives.
The adjoint has an alternative notation,
where we put a square bracket around the vector fields called the lee bracket.
This means that we can rewrite all these properties of the adjoint into properties of the lee brackets.
And particularly for the last one on Jacobi identity, this kind of property seems random at first.
However, if we recognize that it comes from the so-called product rule of the adjoint action,
it seems a lot less random.
So this answers the question of why we want these properties of lee brackets to define a lee algebra.
These properties are in some sense a requirement that this lee bracket is a directional derivative,
a derivative of y in the direction of x.
However, I did say in the beginning of the video that we can understand this property of trace more intuitively as a result.
Here is the explanation.
If we have a particular vector field A x, as said in the previous video on trace,
the divergence of this vector field is trace A.
Now, when we try to apply any linear transformation to this vector field, in this case a rotation G,
the vector field has changed to G A G inverse.
But as said in the trace video, the trace wouldn't change in this case.
So throughout this process, the rate of change of the trace is zero.
If we suppose G itself is generated by B, then restating this into symbols,
we have this limit, where the numerator is the new trace minus the original trace.
And in fact, this is true even if we are not taking the limit because the trace itself hasn't changed.
Using the linearity of the trace operator, we can put these all into the trace.
Again, this is true for all t.
The thing inside is add sub B A plus something of order t.
However, because this is true for all t, we can simply take t equals zero here
and get the trace of add sub B A equals zero.
We have previously established that add sub B A equals B A minus AB.
And so finally, using the linearity of trace again,
trace of AB equals trace of B A.
We haven't gone through any matrix components to arrive at this result.
Now in my original plan of this video series,
this video belongs to a combination of these two, which is why this video is so long.
When I planned this, I did plan two other videos,
but I'm not sure I can give it a unique enough spin.
The video where I originally stated as limitations of XP
is basically an exploration of topology in Lee theory,
which any video talking about topology of spin half would have covered.
And what I originally planned as the last video
was basically representation theory of Lee groups and algebras,
which has been covered by Eigen Chris.
So I'm not sure if I want to continue this series.
And to be honest, the major reason for starting this video series
was to answer some of the big questions that I had,
like how to motivate the definition of Lee algebra with the Jacobi identity
and why intuitively trace of A B equals trace of B A,
both of which were answered in this video.
For the future, I have a few video ideas, but none fully formed.
A video series on differential geometry or general relativity is on my mind
and there are also a few video ideas on probability and statistic stuff
that I haven't blown through or perhaps more excitingly
an overview of what I do in my research.
I don't know and if you have any specific video ideas in mind,
please do comment below.
In any case, thanks for the patrons and don't forget to like, subscribe and comment.
See you next time.
you
