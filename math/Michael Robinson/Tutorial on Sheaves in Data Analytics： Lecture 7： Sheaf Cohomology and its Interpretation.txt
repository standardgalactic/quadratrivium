So the thing I found is what I learned about YouTube is sometimes, in particular, everything
crashes the video, and that just reloads the control, and sometimes you have to remove
the cache.
It's like a two-figure cache.
Oh, no.
Okay.
Interesting.
Were you finding that as well?
You had to reload?
Yeah.
Yeah.
But it just runs twice here.
We're live?
We're live.
All right.
We're live.
Great.
Okay.
So this is a sheaf cohomology.
More or less, finally we'll get some invariants for sheaves, and these will allow us to start
understanding when you've got various kinds of defects or gaps in your data structures
that we've been building.
So now at this point, we've got sheaves built on topological spaces.
We've got all the linear algebraic and homological algebraic tools at our disposal, and we've
We've got the topology and the data put together.
So now we can go to the very top of the diagram
and combine them and start getting out
reasonable topological summaries of your data
and storage sheet.
It's not so much topological, but it
is sheath theoretic invariance for the data
stored in those sheets.
OK.
So really what we want to do is we
want to understand what do the global features of fused data
look like, how do we compute them, how do we assemble them,
and really understand how to find sheath invariance.
Now, the truth of the matter is what we want to do
is we want to build them in much the same way we've
been building homology for topological spaces.
So how did we do that?
We took our abstract simplicial complex or topological space
in that kind of format, and we wrote down
a chain complex in which the chain spaces were
based on the simplices, and the boundary maps
were based on how the attachments worked.
It is essentially the same sort of thing
where the chain spaces are going to be the stalks of the sheath
put together in an appropriate fashion,
and the boundary maps will come from the restriction maps.
So that's the idea.
OK.
So let's talk about this from the level of global sections,
because that's the right place to start.
And this is a very basic idea is that if I
want to have a global section of a sheath, what has to happen?
Well, I have to have data that agree.
It says that if I have data on this vertex
and data on this vertex, and they both map through restriction
map to the edge, did that data agree?
So graphically, here is the stalk of a sheath s
over a vertex v1.
Here's the stalk over a sheath s over vertex 2.
Here's the stalk over the edge.
And what I'm going to do is I'm going
to map through the restriction maps.
Now, to keep the discussion reasonably general,
I'm going to use this notation here.
Script s is for the sheath.
And then this is read a attached to b.
This is a lower dimensional face.
This is a higher dimensional face.
So this is the restriction map that
connects a cell a to b, or a simplex a to b,
b being the higher dimensional one,
a being lower dimensional one.
So in particular, this is then the restriction map from v1
to edge e.
And this is the restriction map from v2 to edge e.
Now, initially, we thought of these things as linear maps.
And that works great when the stalks are all vector spaces,
i.e. these are morphisms in the category of vector spaces.
But these could, in fact, be, now we know,
these could be other kinds of data spaces as well.
Now, it really turns out, though,
that you want the vector space structure in order
to compute cohomology.
Here's why.
See, if I've got a global section,
it means that I specify data at these three data spaces,
picked out an element of here, another element of here,
and another element of here, so that when I map the element
here over v1 through this restriction map,
I get the element that's already here on the edge.
And similarly, when I take the element out of this vertex,
map it through this map, it ends up over here.
Write that down as an equation.
Map through the restriction maps from the vertex 1
to the edge.
There's the data.
I'll say s is a function that's assigning data
from each vertex or each edge, from each simplex,
to the data spaces.
So this, you can think of it as a function on the simplices.
So if I put in a simplex, it gives you out the data element
that you're interested in.
So here we go.
That map from that data through this restriction map
has to be equal to the map coming from,
map acting on the data coming from the other side.
OK.
So this equation, if satisfied, gives me a global section here.
OK.
Now, if I'm in a vector space, or at the very least
in a Boolean group, what I can do
is I can just subtract this from both sides.
If I subtract this from both sides,
I can set it equal to 0, good old algebra.
I could have done it the other way.
I could have subtracted from the other side as well.
But I come up with, in either case,
I'm finding this equation equal to 0.
So I've just rewritten the equation.
Nothing fancier than that.
And it's predicated on the fact that I'm able to do this
because I'm in linear algebra land.
Now, if I take one of these two equations here
and look at it, it's something equal to 0.
Something equal to 0 is good because it
means that I can turn it into a matrix equation, where
it's a block matrix.
This is a function, so it's a matrix unto itself.
Here's another matrix unto itself.
And it's acting on this block vector.
The block vector has components taken right here.
So all I've done is I've factored out
these maps, thinking of them as matrices.
And now I've got this linear algebraic constraint.
So what I've done is I've reduced the question of finding
a global section to finding the kernel of a matrix.
Now, if you think about it from the standpoint
of what we talked about in the previous lecture,
we were finding homology elements
by finding kernels modding by image.
So this is like finding a kernel.
Oh, so this indicates that there's probably
a good way to generalize this into some kind of kernel mod
image construction.
At least that's the suggestion.
And for instance, let me make this concrete on a SHIF example.
So here's the Q-SHIF from several lectures ago,
where we're moving data through time,
through a three element SHIF register.
There's the SHIF.
Here is a section of that SHIF, sort of zoomed in
on the middle.
So I've got a particular element.
And I know that that element is a section because these elements
live in a section because if I map them
through the appropriate matrices,
I get the data on the other faces.
Right.
So let me take this and take it out
and think about it from the standpoint of this linear
algebra that I just mentioned.
So I've got now four restriction maps
that I've color coded them.
Got the blue restriction map on the upper left.
If I write down the appropriate equation
to compute the global sections as a kernel,
that matrix ended up right here.
I've got another matrix, this red one.
This one ended up over here.
I've got the purple one and the orange one.
And what you should notice is that if you're
looking at this particular edge, remember,
vertices that restriction maps go out of edges they go into,
looking at this particular edge, well,
that is taking data that has come from this vertex
and this vertex.
Now, what are those vertices doing?
Well, those vertices are mapping into this edge
via those restriction maps and I want
to check them for consistency.
So if I take this particular data,
this particular element of the stock over this vertex here,
192 is right here.
And then one on the other side, the vertex on the other side,
this stock is element 119 is right here.
So if I now take this vector and multiply it
by this pair of matrices, you'll notice what I'm doing
is I'm subtracting, there's the minus sign attached
to this particular restriction map.
So I'm going to ask the difference between the data
brought from this side and the data brought from that side.
If they agree, I get 0.
And indeed, I get 0.
Now, the same thing happens for this edge,
but now with data coming from this vertex and that vertex
and here they are.
And so what we've done in order to compute
this set of global sections, to verify
that this is a global section, is to verify merely
that this vector lives in the kernel of this matrix.
OK, yeah?
The s's of v1, these are vectors.
So this s here.
So this s is a subvector, so to speak.
It's like a subblock vector.
This is over a particular stock.
So let's track where this guy shows up here.
That's one of these elements on their own.
So this particular guy could be a vector.
In the example we were just looking at,
this is a three-dimensional vector.
If this is a three-dimensional vector
and this is a three-dimensional vector,
this is then their vertical stacking of them.
So then this is a six-dimensional vector.
This then has to be a something by six vector,
or matrix, rather.
Something by six, because this is then something by three
and the same something by three.
But it's a single section.
That's right, I'm looking at a single section.
That's right, I'm looking at a single section
and verifying that, in this particular case,
that this is a section.
By showing that when I construct the matrix,
this matrix out of the various restriction amounts,
sort of block, popped to them in there.
When I block constructed this matrix
and I then block constructed the elements of the stocks
that I'm interested in testing, that I get zero
with that matrix voltage.
Yeah?
So if it doesn't come up to zero,
does it appropriately get the sign bling?
Yes, it does, actually.
It does appropriately get the sign bling.
Let's say I took this and turned this into a three.
If I turn this guy into a three, what happens?
Well, row into column, I'm going to get nothing.
I'm going to get a 1, 0, 0 here, right?
And because that's 1 times 1, 1, 0, 0.
And this is going to give me a 1, negative 1, 0.
So those 1 and negative 1 adds to 0, so that's fine here.
The problem shows up where, actually, I guess
that was a bad example.
This three is fine.
That example is OK.
If instead I change this nine to say an eight or something,
then it will show up right.
Because what will happen is I should
be matching up this nine through here with this nine
right here, which should be a nine matching up.
If this is an eight, then I'll get something non-zero here,
which says, where's the problem?
The problem is at this particular edge.
Problem is at this particular edge, which
is then highlighting this edge, and in particular,
will highlight which slot in this edge.
So yes, it does assign blame.
OK, other questions?
In that column, that's your 2, the third 2, and the very left 1.
They're moving all the way.
Yes, that's right.
Yep, those are indeed irrelevant.
You're right.
Yep.
He said, made the observation that this third 2
is irrelevant to whether or not it's a section or not,
because it simply is never tested.
Because if you look at it, this is a full column of zeros.
There's another full column of zeros right here.
This one, so that corresponds to this five.
That five is never used either.
So this could be anything it wants.
It'll still be a section.
That's right.
Anything else?
More of a Q is looking at the Q itself.
Right.
You can assume what's happening outside your reading.
That's right.
Absolutely.
So the 2 is way off here.
That's right.
So this 2, the reason why it's irrelevant
is because as far as the Q is concerned,
I could have loaded anything onto this side of the Q.
And this side over here, I could have
had anything happening on that side.
Yep.
Very cool.
All right.
So this matrix, then, this matrix
is rather useful for determining global sections.
The way that I paste it together by block assembling the thing
suggests that I can do this more generally.
And here we go.
So what I should do is these involve vertices and edges.
Well, vertices and edges are 0 and 1 dimensional things.
And what I did is, let me back up here, what do I do here?
I took vertices, data on vertices,
and mapped it through here to go from vertices to data on edges.
So I went up in dimension.
Homology went from data on edges and went down to vertices,
or data on two simplices, and went down to edges.
Here's what it looks like we're going up.
So we should build the same kind of chain complex,
but we will go in the opposite direction.
We'll go up in dimension.
So in particular, then Ck, what should C upper k be?
It should be data associated to each of the k simplices.
So it should be all of the stalks over k simplices.
Glom them together, stack those vector spaces together,
take the direct sum of them, if you like.
So stack them together by concatenating bases.
And then k plus 1, that's going to be, again,
the stalks over the k plus 1 simplices.
Stack them all together.
k plus 2, that's all the data over the k plus 2 simplices.
The stalks over the k plus 2 simplices.
Stack them together.
Now, what about these boundary maps,
or the analog of the boundary maps?
These typically people call them co-boundary maps
to remind you that they're going up in dimension.
These co-boundary maps are going to be built
by gluing together a bunch of restriction maps.
We'll see that in a moment.
But regardless, if I do that right,
if I build these maps correctly, then that same proof
that we had wrote down in the last lecture, proving
that we had a chain complex, that same proof will go through.
And that will mean that I have a chain complex,
so immediately it has, as a chain complex,
it has a homology, defined kernel mod image,
as you might expect, to remind ourselves
that the dimension is going up rather than down.
We call it a co-homology, and this
is called the Schief co-homology.
So this is where we're aiming, so that's now
sort of delve into the details.
And let's remind ourselves that there's
some notational reminders so that we don't mix up the indices.
Because mixing up the indices is almost
as endemic to mathematicians as mixing up the signs.
We mess up signs an awful lot.
We also mess up indices.
So there's some reminders.
If you've got a chain space, and it's with a K down below,
that says that we're dealing with homology,
and we will be going down in dimension.
We then call the maps boundary maps.
If you're dealing with co-homology,
the Ks are upstairs, that says we're going up in dimension.
We have co-boundary maps.
And so as a result, we have the difference
between homology and co-homology.
Now, we'll talk a little bit more
about this in terms of how it impacts your, first of all,
is there a co-homology theory?
It doesn't involve sheaves, and the answer is yes, kind of.
And we'll see that in a little bit.
But right now, let's focus on constructing, then,
these co-boundary maps for the sheaf co-homology.
Yeah?
Should we go back two slides?
So on that equation at the bottom,
the way we were defining the quotient,
we were taking a wave basis, which
would bring the dimension down.
That's right.
How is this working where it's bringing the dimension up?
OK, so there are several different things
that are dimensions.
There is the dimension of the simplices
that we're putting data on.
And then there's the dimension of the stalks, which
store the data.
So the question is, how do these interact?
In the case of homology, there's only one dimension.
That's the dimension of the simplices.
Now, we've got dimension of simplices
and the data sitting on top of the simplices.
And they're actually pretty close to being on correlate.
So this is dealing with the k's here
are not dealing with the dimension of the stalks.
They're only dealing with the dimension of the simplices.
And in particular, you'll notice that the indices here
are a little different than they were before.
That rather than this being k and this being
k plus 1 in the case of homology, this is k minus 1.
To say that I'm taking the kernel of this modulo,
the image of this, like we were before.
Now, before, we were taking away some of the bases.
That's right.
And that's why the dimension went down.
Are we here?
We're doing this algebraic operation
is the same operation.
It's the same operation, but the difficulty or the difference
rather is the mental difficulty is
that we're dealing with the dimension of stalks.
This is all happening on the level of stalks.
Oh, OK.
So you're right.
There is some definite subtlety here
in that difference of dimensions.
OK.
All right.
So as I said before, what we want to do
is we want to build these chain spaces out of the stalks.
Primarily because that global section example I just gave you,
what would we do?
We were taking that big long vector, that nine long vector.
They had the three blocks in it.
That was a concatenation of the stalk over vertex, stalk
over vertex, stalk over vertex.
So we took all of those vertex stalks
and concatenated them together.
So if I do that, that's called a direct sum
where I concatenate bases.
It's a way to make new vector spaces by concatenating bases.
So exactly the thing you want to do
is you want to walk over all the k-symplices
and gather up all of their stalks.
This will be at least as long.
Well, it would seem like this would be at least as long
as as many k-symplices as I have,
but there could be some k-symplices
that have zero dimensional stalks.
We've seen this in particular when
we were looking at sampling where we had some places that
were zero because we simply weren't getting
any information there.
OK, now there is one little comment here that I'm making.
If you're working with abstract-symplicial complexes,
that's the end of it.
If you generalize beyond abstract-symplical complexes,
work with CW complexes, you have
to be a little bit more careful.
This does define a reasonable chain space,
but it's not as reasonable for most examples.
Most examples you want to ignore any of the k-symplices
that are missing faces.
In the pie-sheaf code, for instance,
there is a marker asking for whether or not
a face has compact closure.
That's essentially what we're asking for,
is we're asking for everyone that has all of its faces.
Reason being, there might be somewhat undetermined.
OK, so each element of this space is called a k-coachane.
And it really specifies data from all stocks
on all k-symplices.
If you walk over all k-symplices,
you grab data out of each and every one of them.
That's what this thing is.
OK, so how do you build the maps?
Well, you build the maps exactly the same way
that I kind of suggested, which is you grab the restriction
maps and drop them in there.
Where do you drop them in?
Will you drop them in block-wise?
Drop them in block-wise in the corresponding blocks.
So if I'm looking at row i, column j,
that corresponds to a particular cell.
These are block rows.
This corresponds to a particular cell, cell j, cell i.
That's a higher dimensional, k plus 1 dimensional cell.
And the column j, that's an input to this matrix.
That's a k-dimensional cell that corresponds to that block.
And then within that block, you stick the restriction map
that goes from the lower dimensional cell
to the higher dimensional cell.
And as before, in the case of homology on a space,
we've got this index in there.
The index orientation index is keeping track of the fact
that I would like in the end to have a chain complex.
I'd like, in fact, to have that composition of contiguous maps
to end up 0, consecutive maps to end up to be 0.
So you drop in that orientation as well here.
And the reason for that orientation
is just as before.
If you remember what we did when we had the equation
for the global section, we had the two restrictions.
And they moved one of them over to the other side.
And it ended up with a minus sign.
That minus sign is this guy right here.
So we're keeping track of the fact
that data could be coming in from various locations.
And we'd like to subtract it all out
to measure the difference at higher dimensional phases.
So this particular construction yields
then the right co-boundary map that I then
have a chain complex, or a chain complex, which
we'll call the co-chain complex.
So what we've done then is we've assembled this co-chain
complex for a sheaf.
So given a sheaf of vector spaces,
you can build the chain spaces, co-chain spaces.
And you can then build the co-boundary maps,
just as I showed on the previous slide.
And that thing does, in fact, turn out
to be, once you do a little bit of work,
basically the same proof as before with a little bit of tweaking,
but basically the same proof as before,
that says that the thing is a chain complex.
And so it has a homology, which we'll call sheaf co-homology.
And really, what is it doing?
At level k, at dimension k, what it's doing
is it's asking for all of the co-chains
that when I go from level k to k plus 1,
so I go and check them on higher dimensional
simplices.
When I check them, they lie in the kernel.
They're consistent.
They are the sort of things that when
I did that global section check, I picked it up,
moved it over to the other side, and got zero.
It says consistent.
On the other hand, I'm ignoring the ones
that were already present.
So take all of the data that are consistent at simplices
of this dimension, but ignore the ones that
have come from data from lower dimensional simplices.
That's what this is.
This is merely asking for that kind of consistent data that
was non-reductant.
So let's go back to what does this mean?
So h lower 0 is the space of global sections.
Why?
Well, it's specifying data at every zero co-chain,
for every zero co-chain.
What's a zero co-chain?
That's all the data specified on vertices.
Remember, all of the maps, restriction maps,
go out of the vertices.
They go out of the vertices, and therefore,
determine all of the data everywhere else
on your simplicial complex.
And so the kernel, then, of that co-boundary map
is checking for consistency.
So it is literally all of the consistent data
over all of the specified all over the vertices.
So I've got a vertex here and a vertex here, vertex there.
Those where I check consistency, if they come out to be zero
when I check consistency, i.e., they are consistent,
then that's an element of h over 0.
Notice that I don't actually have to worry about the image
because I don't have negative one dimensional simplices.
OK.
Now h upper 1 is a little more subtle,
and that's usually where the action is.
H upper 1 is, in essence, the loops.
In essence, detecting the loops.
However, these are not just loops.
These are data loops.
These are situations where I have data feedback.
Where I have data feedback, these
are situations where I have some kind of loopy belief
propagation, some kind of recipe for disaster,
in some sense, or some kind of resonance condition.
So in particular, and that's a theorem,
if you're looking at wave propagation on graphs,
any time you have a resonance mode,
where you have a wave that starts up,
travels around the loop, comes back,
and comes back exactly in phase with what it started with,
that turns out to be an element of h upper 1.
Those are resonant states.
And really, that's the right intuition
to think about these things.
So they end up often looking very much
like non-collapsible loops.
But now we've got the additional constraint
that the data is somehow consistent.
Now, it turns out that like homology,
and because homology is a functor,
that sheaf co-homology is a functor, which
means that it takes sheaves.
And if I have two sheaves with a sheaf morphism between them,
then in fact, I get a linear map between the sheaf
co-homologies.
This means that co-homology is an accurate, or at least,
it is a precise analogy for what's
going on in the level of sheaves.
This is very valuable for understanding the structure
of sheaves, and understanding how they relate to them.
Hi, before you continue, what would
be the corresponding intuition about h2 in this case?
So it's some kind of higher dimensional loopy
belief propagation.
I don't actually really know.
I don't actually really know.
And it's honestly, what's that?
Data voids.
Data voids.
Yeah, in some sense.
Truth be told, what typically happens,
and part of the reason why this is difficult to say things
generally about this, is because in order
to interpret what h1 is meaning requires
some amount of domain knowledge about what the sheaf is,
what the sheaf is telling me.
The fact that I can compute this is domain agnostic,
but its interpretation depends very much on the domain.
And that's very true for higher dimensional sheaf
co-homology as well, which is it may be present in the data.
I can see it and locate those co-homology classes
and even track down what has given rise to them,
as in what particular assignments of data gave rise.
But their interpretation is not as obvious.
So in fact, I have come across each two
in different contexts, but practice misleading.
First of all, the differential forms.
Oh, sure.
Secondly, I'll break k theory.
Sure.
I mean, I'll break k theory, it's still weird.
Agreed.
Structural rings.
But it's best in that case not to take those intuitions here.
That's right.
Because it's a different case.
It's a different use of homology.
I think you're probably right in saying
that one should be very careful about making
any kind of inferences about that.
And thus far, I've not really seen a lot of higher domain.
Sheaf co-homology of higher than one being something
that I can nail down and say, I know exactly what the difference.
In group homology, you have h1 relations generated.
In h2, you have relations between.
Yeah, so it is something involving multi-way interaction.
It's something involving multi-way interactions.
But it's not entirely clear to me what it is.
OK.
Right.
So one thing that we should nail down
is the relationship between co-homology and homology.
If I have a chain complex of just a space,
that's the top guy with the boundary maps.
I could transpose everything in sight.
Come up with something that goes the other way.
If I transpose a map, it flips the domain in range.
This gives me another space.
And this is the co-chain complex.
With real linear algebra, this thing
is just finding functions on simplices
rather than simplices themselves.
And this, in many cases, carries identical information.
It's encoded a little differently.
But it's more or less identical.
There's something called the universal coefficient
theorem that tells you about that.
If you look at this and say, well,
what are these co-boundary maps doing?
And these now are, I'm saying co-boundary map
because they're going up in dimension.
They're just the transpose of the boundary map
in the case of spaces.
They work kind of like discrete derivatives, which
is something that is kind of interesting in the sense
that they end up being useful in asking for differences
between data along an edge rather than between an edge.
And this is really the intuition
that we use sort of in another way
when we're building up the Schief co-homology.
Because what we're doing really is
we're asking for differences between data.
So if I have data on a vertex and another data
on a vertex, and I'm comparing them on the edge,
essentially what I'm doing is I'm taking the difference,
doing some kind of discrete derivative.
So this actually works on the level of spaces as well.
So there is a co-homology theory for just spaces
that doesn't involve sheets.
And in some sense, what we're dealing with then
is what it's saying is when we're
dealing with Schief co-homology, we're actually
looking at functions from the abstract simplicial complex,
starting from all the vertices, edges, faces, et cetera,
to, well, to what?
To a collection of possible outputs
where the output space can change
as you move around the space.
So it gives you a good idea of what a Schief is.
A Schief really is it's a space in which it is a thing in which
I'm looking at functions on a space.
But the domain can vary, or the range
can vary as I move around.
Sometimes people call this local coefficients.
Functions with local coefficients aren't like sheets.
And this is an explanation for that.
OK.
So let me now give you a very detailed example
of a Schief co-homology computation,
because I think it will be very helpful to see
how that plays out.
OK.
So in particular, let's take a look
at this simplicial complex.
And let's look at what the various data spaces are.
Well, what we've got is we've got this loop structure.
Let me transfer it over to the board.
And in essence, what we have is we've got a news source.
We've got a web source.
We've got a camera.
And we've got a Twitter feed.
These are the data sources.
They are the vertices there.
Additionally, we've got edges connecting them.
So this edge checks whether or not there are clouds.
This edge checks to see whether or not there is rain.
This edge checks to see what the humidity is.
And this checks to see whether or not there is sun or not.
OK.
Well, if you sit down and write down
what the possibilities are for what the data spaces are,
build the sheaf model out, well, over the news,
there is some data that they're giving.
In fact, the question is, what data are they giving you?
I don't actually know what the news feed is,
but this should be the raw data of the news feed in some sense.
This is some ontology, potentially a large ontology.
And so I'm going to categorify it.
So I'm going to take that ontology, which
I'm going to call 01.
And I'm going to look at the vectorification of that,
a categorification of that.
So I've got a real vector space built up
on top of this ontology.
Over here, what do I have?
I've got the web.
Again, I don't exactly know what it is,
but it's got some ontology associated with it.
Again, the Twitter's got some other ontology
associated with it.
I'm calling this one 03.
And what does the camera give me?
Not an ontology.
It gives me a picture, say an n by m picture.
Now, when I start saying those are the data sources,
now I need to start actually doing the multi-int integration
and start pairing them together.
Most of these are binary.
Are there clouds?
Well, that would be a yes or a no.
That would be a binary.
So I should categorify that much the same way
I did the logic circuit.
So it is either yes or no.
It's a real dimensional vector space.
The first basis element is for yes.
Second is for no.
Similarly for sun and similarly for clouds.
Now, what about humidity?
Humidity is a number.
It's 70% humid right now.
I don't know.
So these are supposed to be the maps.
So there are my restriction maps,
and they are whatever they are.
So let me now write down what the chain complex for this
sheaf is.
This is now a multi-int sheaf that
is described in a multimodal sheaf describing
this data collection scenario.
And realize I'm leaving this someone under specify.
And it turns out that it's OK.
Because I don't know what ontology this is.
Could be big.
I don't know what the ontology this is.
Also, it could be big.
But I'm going to say that there is some map that translates
that ontology to a yes, no answer.
So it ingests the ontology that you're hearing.
The words that are being used to describe the weather,
and we're going to pull out whether or not it's raining
or not.
Similarly, I'm going to pull out the words from this website
and turn them into a yes, no answer to whether or not
it's raining.
So here's what the chain, the co-chain complex looks like.
So I've got, first of all, the zero-dimensional faces.
Where are they?
They are right here.
These are the zero-dimensional faces.
And so what I'm going to do is I'm going to form the zero
co-chains by writing them all down.
By direct summing the whole works by just
concatenating bases.
So concatenate the basis on of two.
Concatenate the basis on three.
They are my three ontologies.
As well as the space of pictures.
So that's this whole thing here.
This is C0.
Now, I've got a co-boundary map, D0.
And where does it map?
Well, it's going to map to the co-chain space for the things
that I have not underlined in red.
And let me make sure I get them in the right order here.
And these are, in particular, they are, whether or not
there is sun.
So I'll label these.
That's A. Then whether or not there are clouds.
That's B. Whether or not it is raining.
And what the humidity is.
OK.
Should it be R2 plus R2 plus R2 plus R?
R2.
Yep, yep.
You're right.
Thank you.
Get me honest.
OK, and that's C1.
And are there any two dimensional faces here?
No.
No two dimensional faces to speak of.
OK.
So now what I can do is I can sit down and say, in a block
diagonal form.
Let's pull back to here.
In a block diagonal form, fill out that matrix.
This co-boundary matrix.
Erase what I've got here.
And insert it.
So D0 is.
Now, it's going to be a block diagonal matrix.
Let me list what the blocks are.
This is the sun in question A. This is the clouds question.
The B. This is the rain question C. And this
is the humidity question D. So there they all are.
And now, let me list across the top here.
This is the news.
This is the web.
This is the Twitter.
And then this is the camera.
OK.
And so I end up with this block matrix.
OK.
So this block matrix, I can fill in some of this right away.
This is a 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0.
Those are all 0s because they're simply not connected.
That there is no connection between the news and question.
Wait.
Yeah, there is no connection between the news and the clouds
question.
The news and the clouds connection are not connected.
There's no connection between the news and the humidity
question either.
So these blocks are 0.
Now, on the other hand, what do I have in here?
Well, I have some block.
It's 2 by something.
I don't know what that something is.
This is also 2 by something.
This is 2 by something.
This is 1 by something.
This is 2 by something.
This is a 1 by something.
2 by something.
And a 2 by something.
I don't know what they all are.
But each block that I filled in that is some kind of map.
I don't know exactly what those maps are.
But I can start bounding the rank of this matrix, which
we'll start telling you about the size of the 0th and first
cohomology.
Now, the 0th cohomology is pretty big.
There are lots of possible consistent sections.
In particular, what this is saying, there's lots of possible
ways to describe the weather.
We knew that.
That's uninteresting.
However, if you look at H1, what is the kernel of this map?
Well, it's everything.
What's the dimension of that kernel?
It's 2 plus 2 plus 2 plus 1.
That is 7.
So the dimension of this guy, dimension of the kernel of d1.
The dimension of the kernel of d1 is 7.
But what is the dimension of the image?
Because I need to take the kernel module of the image.
Well, if you look at it, this map here, most of this map,
is probably not going to capture everything.
And because particularly, each one of these blocks
are full-rank blocks, probably, which
is to say the news can answer either yes or no.
It is either sunny or not.
The news can also say it's raining or not.
Those are both conceivable things that could be said.
Similarly, the web could say that it's raining or not.
And it could list possibly in principle
any level of humidity.
So that indicates that each one of these blocks
is a full-rank.
Well, if you start then doing a little bit of algebraic
manipulation, you find out that just the way that we've put
this thing together, this resulting matrix
is not a full-rank.
A little bit of row reduction will show you that.
So this matrix is rank deficient.
What does that mean?
It means that the image of this matrix
is not a full-rank, which means that there
is something in the image or something
in this portion of the matrix that was never actually touched.
Which means, in the end, that each upper one
is not actually trivial.
Which means that what are those things?
Those are, in fact, the potentially locally consistent
states of data that really don't make
a whole heck of a lot of sense.
So this is, in fact, a way that you
can detect that you've got an information gap.
That you could, in fact, there are opportunities
to improve our description of the weather.
How?
By asking for corroboration between various other,
various of these sources in ways that you haven't checked.
So for instance, asking for the news
to not just tell me about whether or not it's sunny
or whether or not it's raining, but also to have them talk
about the humidity and whether or not
they're going to talk about the clouds as well.
Now, they might.
I mean, this is a constructed example.
But the point is that if I can fill in this gap right here,
that will end up eliminating the each upper one, which
is saying that I'm giving a more complete picture.
So you can do these kind of computations extensively
on this sort of multi-integration problem.
You can also do this on some other simple examples as well.
Move ahead.
So we talked about the logic functions.
Actually, before I go talk logic functions,
the question's on this.
Yeah?
That's kind of a stupid question.
But when you say sun, it's either sunny or sunny.
Yeah.
Why isn't that a logic?
It is a logic.
It is a logic.
It is some of a logic.
Why is it R2?
OK, because what I've done is, remember,
I've said in the previous lectures
that we need to categorify this so I can talk about these
functions being linear.
There's no.
That's right.
It's a vector space over true and false.
In essence, you can think of it as kind of like telling
probability.
It may or may not exactly play out that way,
but it kind of has that feel.
What about question marks and dimensionalities of laws?
Right?
Or is it addressed or do you need to?
You don't need to address them.
The analysis you made, all you're asking,
you're not actually calculating the whole homology.
No, I'm not actually calculating.
You're determining whether or not it's non-trivial.
I'm reasoning about whether it's non-trivial.
If you wish to actually calculate it,
then you do need to know those.
So how is that resolved?
So that is resolved by actually looking
at the particular analytics that you're running.
The particular analytic that will take a look at this ontology
and make it a determination about whether or not
this is a yes or no situation.
So that's a particular function that's
going to ingest the ontology that's being used.
So question marks are a function of those unspecified?
Those unspecified analytics.
That's right.
And what N and M are in the image?
That's right.
Yeah?
So with introducing those additional consistencies
give you a higher-dimensional simplex.
So they might.
And therefore shrink the kernel of that, and therefore.
That's exactly what you do.
That's exactly what would happen is
by adding additional consistency checks
with higher-dimensional simplices,
you end up reducing this kernel.
Yep, precisely.
So of course, this is outside the scope of this tutorial.
But in the application case, you wouldn't actually
have all these bits of information at the same time.
That's true.
So there'll be a time element.
And also, you might want to think
that some of them are more acceptable or correct
than others.
Yes.
Yes.
So does anybody look to this?
Or is this sort of work?
This has not been done.
This has not been done.
OK.
So why isn't the image of this rank 7?
It looks like the way you could write it if you had numbers,
and you put in C0.
It does look like you could get the image to be rank 7.
Right.
So in particular, this is one of those circulate matrices
that if you write down reasonable full-ranked blocks
for these things and start doing the reduction,
usually comes out that it's ranked efficient.
I'm not saying it always does, but in most reasonable cases
where each of these blocks turn out to be full-ranked,
that it turns out to be ranked efficient.
Yeah.
So you say in most reasonable cases,
but it's not a mathematical problem.
It's not a mathematical problem.
Yes.
You're right.
Yep.
Yep.
OK.
So we talked a little bit about logic circuits.
If I translate this logic circuit into a sheaf,
I can sheafify and examine its cohomology.
And this does not have a cohomology thus far.
This particular logic circuit is, well,
it's something in particular because this AND function
is not actually linear.
Because it just doesn't satisfy the appropriate linear algebra
requirements on this side.
If I add 0 plus 0, I don't get 1, generally speaking.
So if we categorify the thing, turn it
into an appropriate vector space,
this is now a vector space over each of these elements.
Sort of note that turns out that that turns
into a tensor product.
That's OK.
Doesn't really matter.
That's a four-dimensional vector space then.
And we saw this a little bit early
when we build this kind of construction.
So this is then what I had called a switching sheaf.
Vectorified everything about this.
Here I've got that linear map representing the AND,
and these are the linear maps representing projections.
OK.
Good as far as it goes.
If I build out, say, one that has three inputs,
then I have more inputs.
Now the thing to realize here is that do all of the edges
in this have all of their faces?
Does this edge here have both of its vertices?
No, it only has one vertex.
So when we start talking about cohomology of this thing,
we don't actually end up worrying about anything but this.
It's simple enough.
There are how many sections then are there
determined entirely by this particular space?
This particular space is all of the various possible input
and output combinations that I have.
Turns out there's quite a few of them.
And the question is, if I build a logically equivalent circuit,
do I see something like that same number of sections?
So if I build a logically equivalent circuit,
here's another circuit with a similar flow tree,
where instead of having a single three input gate,
I have two two input gates and I thread one of the inputs
together.
This gives me a three input circuit,
but broken out a little differently.
The data flows sort of top to bottom in this.
And the question is, well, what do I get?
Well, if I go ahead and make use of this cohomology,
it turns out I can actually find something out.
I could discriminate between this circuit
and the previous one because the cohomology is different.
It was eight dimensional before.
Now it's six dimensional.
What happened?
It's different.
It's an invariant.
It's telling me something useful.
What's different?
Well, what's different is actually
that some of the sections have gotten glued together.
Here are all the sections on the input.
Here are all the sections on the output.
Notice there's some overlap there.
And so what that's saying is that saying
that we're representing transitional states
a little differently, a model of uncertainty.
That's kind of interesting.
This doesn't have any higher dimensional cohomology.
But the fact of the matter is, if I have higher dimensional
cohomology, it starts representing storage of data.
I'll give you an example of that very quickly.
Sort of highlight this.
And you can sit down and compute this.
The cohomology over this particular circuit
has nontrivial first cohomology.
Notice this does not have a directed loop in the sense
that it's got one input, one output,
and one internal non-directed loop
going out from this initial input, splitting out,
and going reconvening.
That particular non-directed loop
offers storage data.
It's got a race condition in it.
This particular circuit, if you turn it on,
it generates a small glitch, or turn it off,
and generates a small glitch.
It's a glitch-generating circuit, which basically
means logically it's supposed to always be at the same state.
But if you mess with it, it doesn't.
That's kind of neat.
Here's another circuit.
This is actually a famous circuit.
It is a set, reflet, set, flip-flop.
Having come from the electrical engineering curriculum,
this is what we use to torment first-year electrical engineering
students, because we say, what does this circuit do?
They've never seen a feedback in a circuit before.
They start messing out on truth table, and you think,
here's a truth table of part of it.
Problem is that the output is fed back into the input.
So the state of the circuit depends on its previous state.
It's storing information.
And it's not obvious what it does.
It actually has a set condition.
So if you set this particular choice of inputs, A and B,
it'll store some data for you.
It'll store a 1.
If you do the reset, it'll store a 0.
If you leave some of the inputs alone,
it'll just hold onto that data for you.
And then there's another situation where you hold it,
and it's sort of an uncertain state.
And whichever of the two inputs you release first,
either A or C, it really depends very much
on fractions of a second, like nanosecond stuff,
it'll pick one of the states for you.
That's very interesting.
Turns out this circuit ends up with non-trivial first
co-homology, which tells you that it's storing information.
And in fact, the truth table comes
embedded in the rest of the co-homology of the circuit,
which is kind of cute.
And in particular, the co-homology
includes the same states that you
found in the truth table with some additional transitional
states telling you what happens when you hold it
in that funny, unstable state and then let it go.
So this is an invariant that you can compute right
out of the sheaf model.
And so final bonus in the last few minutes,
so what co-sheaves, we mentioned co-sheaves for a while ago,
don't they have something to do with this?
There's the same kind of construction.
Well, the globality of co-sheaf sections
is kind of bad, because unlike sheaves where global sections
live in H0, global sections of co-sheaves
don't live in a given dimension.
They live in top dimensional simplices.
They don't live in necessarily all the same dimension,
because the top dimensional simplices might be different.
They may not all be of the same dimension.
However, what's interesting is numerical instabilities
in models that are represented by co-sheaves,
like various kinds of finite element models
are modeled by co-sheaves.
Those can show up if you have a certain non-trivial,
now homology glasses dimension goes down.
So I'll give you an example of what this sort of thing
might look like.
Here's an antenna with two of my students testing it out.
And here's a numerical simulation of it that I ran.
And what you can do is you can model this thing
as having a narrow feed channel that opens up
into an open aperture.
And here's the signal traveling down the narrow feed
channels, two complex values.
One for the wave traveling towards the aperture,
and one for the reflected wave back.
And then there are a bunch of integral transforms
describing the rest of the thing.
So this is a co-sheaf model of it.
It's sort of incomplete, because I'm
missing the boundary conditions.
There are the boundary conditions.
This is a co-sheaf model, then, of that numerical and analytics
simulation.
If I try to write down the co-sheaf homology in exactly
the same way we've been doing it, grab all the dimensions,
zero stocks, grab all the dimension, one stocks,
all the dimension, two stocks, put them together,
build a boundary map now.
That's exactly what you do.
Turns out the global sections end up
spread across dimension one and two, which is kind of awkward.
So even though co-sheaves are great for modeling models,
top-down models, the homological study of them
is somewhat stymied by this.
And I actually don't know a heck of a lot beyond that.
And numerical analysts tend to work in situations
where the top dimension is uniform across the entire complex.
So this problem doesn't arise.
So I don't actually know what to say about that beyond that,
except to ask the question to say,
anyone wants to play with this?
I'd be very interested to see what happens.
So next up, we'll talk about computing, homology,
and co-mology, and software.
And then the next and final lecture
will be dealing with symbolizing data.
So thank you.
Thank you.
Thank you.
