This talk describes how you can analyze topological systems built on sheaves using something called the consistency radius.
As always, this is work shared with many other people, so these are the people that are most closely related with the ideas in this slide.
Of course, apologies if I've missed anyone.
The main ideas in this talk are that you should model multi-way relations between data items using finite partial orders instead of something like a graph,
or even something as general as a topological space.
Finite partial orders are somehow a little easier to work with, a little less general, a little more restrictive,
but on the other hand they have sort of a happy medium between generality and practicality.
Once you've gotten your relationships between data, at least you know they're there,
then the idea is to model the consistency between different observations of the data elements using a sheaf,
that's a mathematical object that specifies local data,
and once you have some data in hand and you've got this sheaf model built,
you want to analyze the quality of that data and try to improve it using something called the consistency radius of the data associated to that sheaf.
Basically the idea is we're going to start with the idea that we want to do much of our modeling upfront and then analyze the data once it comes in.
We don't want to spend a lot of time trying to learn our data.
This is not a talk about learning, this is a talk about much more of a model-based approach.
You can build sheaves that model many different things, general systems models,
you can build discretized functions as sheaves, you can build various sorts of differential equations and whatnot.
We're going to focus on just the particular structures in green.
Sheaves that are on partial orders of the Alexandrov topology,
sheaves built on systems of equations, and sheaves that model differential equations.
The way to read this diagram is that the bottom, as you go down below the bottom,
is more fundamental mathematical structures, fewer axioms, and as you go up you add axioms.
The ideas in this talk are essentially captured in two papers listed at the bottom of the slide,
and of course I'm happy to entertain questions about them.
So why partial orders?
Usually when people talk about sheaves in mathematics they talk about sheaves over topological spaces,
in particular over manifold or algebraic varieties.
On the other hand the topological data analysis community tends to work primarily with abstracts and partial complexes,
or cubical complexes.
So those are kind of two ends of the same spectrum.
On one end you have great expressivity, topological spaces are very expressive,
on the other hand they're very large and so they're hard to analyze.
On the other end of the spectrum is things like abstracts and partial complexes,
which are quite general and quite flexible,
but they have the disadvantage of being perhaps not as compact of a representation as you might like.
So the go-between here is every partial order, in fact every pre-order,
has a natural topology called the Alexandrov topology.
Most of the time people think of this as a very limited topology,
especially when you're dealing with a finite partial order,
but it's going to suffice for our purposes.
One of the benefits of this is that pre-sheaves and sheaves are essentially the same thing in this topology,
because the gluing axiom can be insured more or less trivially.
And so the commutativity of a diagram is really the only constraint that you need to worry about when you're constructing a sheaf.
So what does the Alexandrov topology look like?
Let's start with a partial order.
Here's a partial order.
So the elements of my partial order are these blue diamonds,
and the arrows specify going from smaller elements to larger elements.
You can see that the elements along the top are bigger.
In fact, there are two maximal elements,
and there are two minimal elements at the bottom of the slide,
and there's a whole bunch of elements in between.
Some of the elements are comparable, some of them aren't.
The way that you build a topology on this is you need to declare some of these elements
Some of the sets of these elements are open sets.
So what's an open set in the Alexandrov topology?
Well, it's built of unions of the up sets,
all the sets of things going up from a particular element.
So something like this.
So if I mark that element in red and follow up,
go all the way up in the partial order,
that forms an open set.
So that is one open set in my topology, and I can make unions out of them.
It happens that this is a base.
So here's another open set.
It's the union of the two up sets,
in which I'm marking in red the elements that are at the bottom.
Okay, so that is an open set.
Close sets are, of course, the complement.
They're kind of the reverse.
Not quite down sets, but often they are.
Okay, so now, what are we going to do with these?
Well, in a topology, it's all about the unions and the intersections.
Well, if you look at it, the intersections of upsets often,
well, if I take the union of two things, that's fine,
but if I take the intersections, does that also work?
Yes, the union of two upsets,
or rather the intersection of two upsets is indeed an upset itself,
or union thereof.
Here's another three-way intersection.
You can see the union, rather the intersection is present there.
Okay, so what is a sheaf?
Well, a sheaf, on a partial order,
assigns an object or a set to each of the elements,
and that's called the stock over that element.
People who are familiar with sheaf theory, as it is generally put together,
will say, oh, yes, the stock is, of course, some kind of limit construction,
and indeed, these are limit constructions.
Why?
Because in the Alexandruf topology, there is a notion of a star,
the smallest open set that contains a given element.
That's not often the case in other topological spaces,
and it's particularly convenient for working with Alexandruf topologies.
So the stock on any given element is really best thought of as the set being assigned
to the open set, the upset at that element.
So we're going to build a sheaf of vector spaces on a partial order.
Notice I've highlighted the two prepositions of and on.
Sheafs are of something, some kind of data type,
and are on something, some kind of topological space.
So we are going to build a sheaf of vector spaces on a partial order.
I could, as well, have built a sheaf of sets on a partial order.
What's the difference, whether the stocks might not be vector spaces?
They might be just any random set.
Or I might do sheaf of abelian groups, for instance, on a topological space.
So this flexibility exists.
We're going to focus with sheaves of sets.
So, for instance, vector spaces on a partial order.
Okay, but I don't just assign the stocks.
I also assign some maps.
And those maps are called restriction functions,
and they follow the order relation.
You can think of this as going from a larger open set
or a bigger upset to a smaller one.
So I'm telling you how I can restrict my attention to a smaller upset.
And these are in a sheaf of vector spaces.
These are linear maps.
They're the appropriate kind of morphism in that category,
if you know what that means.
If I'm dealing with a sheaf of sets, then these are just functions.
If I'm dealing with some kind of abelian group,
well, they ought to be group homomorphisms.
Okay, and I can't just assign these any way I'd like.
They are subject to a constraint.
And that constraint is that this diagram commutes.
Essentially, composition needs to be path independent.
So if I follow that blue triangle,
there are two different ways to get from the R2 at the bottom to the R3.
One by going directly, or one by taking a detour through the R on the left.
And if you notice, if I compose those two arrows, matrix multiplication,
I get the same thing, whether I go around the left or directly from the right.
And this happens on all the other places in the diagram as well.
So those two other rectangles as well are commutative.
They are path independent when I do composition.
So that thing defines a sheaf on a partial order.
Okay, cool.
So once I have this structure, in some sense I've specified a mathematical object that can house data.
I now want to ask, if this is a schema, for instance, a database schema that can house data,
what happens when I take a bunch of observations or a bunch of instances?
How well do they match up with this schema?
Well, essentially, this is the concept of an assignment.
I will note, sometimes people use the word serration as an homage to J.P.Sare.
As nice of a visual as that is, sometimes that's a bit opaque.
So I'm just calling this an assignment just to make this very straightforward.
It's a selection, or I assign a value from each stock.
So here I've replaced each of the vector spaces with an element from those respective vector spaces.
Now, of course, that's not just the sets that are in play, but there are the maps.
And the beautiful thing about this particular assignment that I've drawn here
is that this assignment is what's called a global section.
It is consistent with the restriction maps.
So if I go from the 2, 3 on the very bottom, and I go up to the minus 1,
well, notice that is indeed the matrix multiplication of the 1 minus 1 matrix with the 2, 3 matrix.
Then you can follow around all over the diagram and find that whenever you get from one matrix element,
or one vector to another vector, I can follow the compositions, of course, anyway.
I like it because it's path-independent.
I should get the data element that is there, the vector that is present there.
So that's a global section.
It is globally consistent.
Now, anyone who's really worked with data will, of course, tell me,
yes, global sections are nice and all, and they're somehow consistent,
but they're also rather rare in practice.
No one ever gets exact values that match up perfectly, do they?
Certainly there's always some kind of noise or errors or places where it doesn't agree with the model.
So what happens if I tweak something?
Some assignments aren't going to be consistent.
So, for instance, I changed two particular values in this assignment.
I changed the minus one on the left to a plus one,
and I changed the 2, 3, 0 to a 2, 3, 1 on the bottom there.
And you will notice along the arrows marked in red that I no longer get a perfect matchup.
It's only partially consistent.
Okay, so this certainly can happen, and we can measure this.
Why?
Because in these vector spaces, I have a way to measure distance.
So if I go ahead and I walk around and say,
how far off are the various different values that are present
from the values that I get by arriving there from somewhere else in my diagram,
I can make some observations.
So, for instance, if I go from the 2, 3 to the plus one there,
I notice that, well, the 2, 3 maps to the minus one.
The distance between 1 and minus one is, of course, 2.
Okay, so in some sense that red edge there, that red arrow, is off by 2.
Similarly, if I go from the 2, 3, 1 on the bottom to the 3, 2,
kind of in the middle of the diagram there,
the distance between what value is placed there, the 3, 2,
and what value I got is square root of 2.
And similarly throughout the diagram.
So the maximum value here across the entire diagram is called the consistency radius.
And just by looking at these three particular restrictions that I checked,
the maximum is at least as big as 2 square root of 14.
Of course, there are lots of other restrictions to check,
but you can certainly imagine iterating over all of these edges and checking it out.
Of course, you don't have to iterate over all possible combinations
because you know the diagram commutes.
Okay, so the consistency radius tells you how far a particular data instance
and assignment is from what the assumed structure,
the consistency built and encoded in the sheaf is.
This is very handy and we can use it in many different places.
But first, let's talk a little bit of some theoretical properties of this.
So the space of assignments really is I assign a value to each stock.
In this particular example, I need to assign 17 real numbers to make that happen.
You can think of it as r17 mapping into each of these guys here.
And of course, if you do it right, as in each one of these is a projection,
each one of these dotted arrows is a projection,
then this bigger diagram commutes as well.
Well, the space of global sections is, of course, a subset of that.
It's those assignments that happen to commute.
And in this particular case here, it happens to be the product of r2 and r3.
So it takes five real numbers to specify a global section.
Not every one of those pairs of a 2 vector and a 3 vector
happen to be a global section because they might not be consistent,
especially when I get to the r2 kind of in the middle of the diagram.
So it's some subspace of that.
But what should happen, and what does happen,
is that those assignments with zero consistency radius
happen to be exactly the global sections.
Great.
Of course, if you're not dealing with metric spaces as we are here,
you might have a little bit of a problem.
But in most cases, metric spaces are quite good to work with,
so you can think of global sections and assignments
with zero consistency radius as being the same thing.
Okay, cool.
So in fact, what this lets you do is this allows you to say,
if I have a sheaf of pseudometric spaces on my poset x,
or the poset with the Alexandra of topology,
then I can build a pseudometric on the space of assignments
just by essentially lifting up to the product.
There are many ways to do this.
This is a particularly convenient and simple one.
Once you've assigned this particular pseudometric,
now you can ask, okay, how far away is a given assignment
from any global section?
And indeed, there's a nice theorem that bounds that.
It says that if I have consistency radius epsilon,
and I know about how much each of the restriction maps
distort things, then in fact I have a lower bound
on how far away you will need to move that assignment
to get to a global section.
So the data fusion problem can be essentially phrased then
as a constrained optimization.
Given an assignment, find me the closest global section
in the pseudometric on assignments.
You will say, well, can't I just do that without doing
all this sheaf theoretic nonsense?
Well, not quite, because you need the space of global sections.
This constraint is where all the action is.
The metric that you're optimizing,
or the cost function you're optimizing in here,
the nearest part, is not interesting.
It's the constraint part.
If you're interested in a lot more details,
you can read the paper noted at the bottom of the slide,
but I'm going to show you sort of the punchline
of that paper through an example.
OK, so I'm going to show you two examples.
The first one is from that paper, a sensors example.
I'm going to show you one that's something of work
in progress about economics.
So the sensors example says, suppose you're tracking
commercial flight.
In particular, this is especially important
when you need to deal with search and rescue.
So this example actually is based off of some guidelines
from the Australians about how to track and find
where an aircraft potentially has crashed.
So you might have several different sensors.
In this particular example, I have several sensors,
two of which are RDF sensors, Radio Direction Finding Sensors.
They give you bearings.
They don't read out real numbers.
They read out something on the circle, an angle.
The air traffic control radar tells you a location and time.
And of course, actually, there's another data source,
which is what the flight plan was supposed to be.
I said, did you follow it?
And of course, this would not be a very interesting example
if something didn't happen.
So if the aircraft deviates from a flight path
and then crashes somewhere, maybe you get a satellite image
as well.
So you get some detections that say,
I think I see some things in the satellite image.
Are they possibly what I'm looking for?
OK, so for this example, I'm generating several different
sensor streams using realistic simulated data.
I'm going to withhold the known crash location for validation
to show you how this process works.
OK, so of course, I need to build a partial order
and build a sheaf over it.
So I'm going to start with the partial order.
I have four physical sensors, the air traffic control radar,
the RDF two sensors, and the satellite image.
I have some reported data in my partial order as well.
So the air traffic control radar tells me the last known position.
The RDF sensors tell me bearings, and they tell me a time
at which they hear a beacon.
And the satellite image doesn't know necessarily when the crash
time occurred, but they can certainly help me estimate
bearings.
OK, so this is the partial order structure.
And I can go ahead and try to build a sheaf model on that.
Of course, it's going to look much like that partial order.
The restriction maps A, B, C, and D compute bearings
from latitude and longitude.
If you want to see formulas, you can read the paper.
And the restriction E estimates the crash location
from the last known position, velocity, and time.
What the request that this diagram can mute and, in fact,
be a sheaf gives you as it gives you the dead reckoning
at the very top, which essentially contains the last known
position and some velocity information.
OK, so given all of this, I now want to start using it.
So let me give you a first example.
I'm going to give you three separate cases.
The first case, nothing particularly bad happened with the data.
There's just some Gaussian white noise thrown into various
sensor data.
And then it's been processed through the appropriate
sensor chains.
So in this case here, if you look at the errors marked on
each of the arrows, you can see that there's a variety
of different errors.
They're in a variety of different units.
Don't let the units bother you.
You can just sort of convert them all into some kind of
common form.
The metric that we built to make the theorem tick just asked
for the largest value.
Of course, there are some unit concerns here, but they don't
appear to matter substantially for this example, particular
because the units of angle are in a pretty narrow band.
OK, so you can see from the diagram that the satellite
object detection and the crash location are pretty much
coincident with the two bearing sight lines.
So that's pretty good.
If we look at the consistency radius associated with this
data, it's 15.7 kilometers.
And if you just grab the last known position and extrapolate
from that outright, you're off by about 16 kilometers.
OK, that's not too bad.
But if you try to say, all right, give in all this data
as an assignment.
It's not perfect.
Go try to find me the closest global section.
And then use that to estimate where the crash is.
Your error drops down rather precipitously by a factor of
8, which is probably super optimistic.
So let's be a little more realistic with that and throw
some errors in there.
In particular, in this second case, let's throw a bias.
Let's take the sight line from the RDF2 sensor
and bias that by two degrees.
That's not at all unreasonable, actually.
Bearings drawn from weak beacons are not particularly accurate.
In this case here, my consistency radius is perhaps
about the same order of magnitude.
The crash site error just from extrapolating the data
is about the same.
Why?
Because I'm not really using the bearings very much.
But the crash site error does not go down to 2.
It's down maybe by a factor of 2 instead of a factor of 8.
OK, what happens if I get a serious error?
If I modify that satellite object detection,
I get a false alarm.
And I think that I see something that looks airplane-like
nowhere where there are airplanes.
Well, in this particular case here, my consistency radius
is 152, which kind of says that's about how much distortion
I've got present.
The error from the actual crash is quite a bit higher
in accordance with the theorem, in fact.
So it's about 200 kilometers off if I try to extrapolate.
If you look on the map, you can see there's a pretty big gap there.
And doing the data fusion, finding the nearest global section,
does drop the error quite a bit.
Still not great.
This is, of course, not something you'd imagine
to be able to recover from necessarily,
unless you were to completely reject the satellite
and say, you know what?
I think the satellite's not working right.
Based on the fact that only that particular restriction map
there marked is the problem.
So I can kind of diagnose problems.
But if I don't, I'm still improving the situation
by doing a data fusion, by finding the nearest global section.
OK.
Well, let me show you now another example.
It's a very different character.
It's very much a work in progress.
And it's with my students, Philip and Jackson.
And the idea is that here's a simple description
of a national economy.
It's described by these two differential equations.
If you look at it for a little bit,
you'll realize it's very similar to the predator-prey model.
This particular model was developed by Goodwin in the 1960s.
It takes two values, the employment rate
and the workers' share of income,
and plays them off each other.
To say, as the employment rate goes up,
the workers' share of the income starts to go up,
but that starts to impact the employment rate.
And so you end up with some cyclic behavior.
This was meant to be a character of the business cycle.
I'm going to show you how to build a sheaf over a partial order
that describes the dynamics of this.
And so what you start with, of course,
is you have to start with the differential equations.
Somehow, I have two differential equations,
equation one and equation two, and I have two variables.
If you write out the dependencies between the equations
and their variables, it looks like this particular kind
of complete bipartite graph between equations and variables.
And I've written it as a partial order,
because it'll come in handy in a moment.
Now, as anyone who deals with differential equations
might know, there's just a little bit under the hood here,
there are two other variables after all,
the functions and their derivatives.
Okay, but the functions and their derivatives
are related by two other equations,
i.e. namely that the derivatives are in fact the derivatives.
So now I have four equations and four variables,
and this is pretty much a complete description of what I need.
And so what I can do is I can start saying that the values
that these variables take over time
should live in some function spaces.
And then in that case, the arrows will be actual functions,
they'll be actual maps, and then this will be a sheaf diagram
over a partial order.
Actually, note commutativity will be trivial
because this partial order has only two levels.
Okay, so here are possibilities for what those spaces might be.
Anyone who does analysis will of course say
that there are many possible function spaces,
depending on exactly the problem.
But this is sufficient for the purposes of discussion.
So the way this works is each time I had an equation,
I write the state space of that equation.
So in the case of equation one or equation two,
recall where they are, they're kind of in the middle,
equation one and equation two are time series of pairs of values.
So they're c1 functions from r to r2,
and the two values being the u and the v,
the employment rate and the worker's share of national income.
You can see then the maps going from those equations
to the respective u and v, they're projections.
Now, you will notice there are two arrows labeled with d by dt.
Of course, those are taking derivatives,
and of course we lose a degree of continuity by doing that.
And there are also the equations implemented
by one and two, the actual equations here,
that compute the u prime and the v prime from the u and the v.
Okay, so this particular model has a nice property
that the global sections of this particular sheaf
are indeed solutions for this differential equation.
And you can start asking questions about can I build other systems
and can I diagnose and debug how systems are built this way.
Well, first of all, there's a nice theorem.
It's not like this is particularly new.
I will admit that from the outset.
But what I am saying is that this is a very constructive,
straightforward thing to do.
Once you have equations and variables,
you can build a sheaf model.
And this opens up lots of doors,
because now you can start asking questions about differential equations
or other systems of equations using the language of sheaf theory
and making use of those tools.
Okay, that's pretty handy.
One of the things that inspired us from the outset
was that this is actually kind of handy for diagnosing what's going on.
If I take each place where the identity maps are present,
you'll notice that the spaces are the same on either end.
It doesn't change the global sections
if I already collapse those two spaces along the identity map.
If I do that, I get something that looks a bit like a butterfly
on the bottom left.
It's a little simpler to write down,
but watch out the thing in the middle.
Now I have to worry about commutativity.
That thing in the middle is actually a subspace.
But bearing that in mind,
this allows you to very quickly look at a model
and understand the interrelations between the variables
but actually be aware that you've actually got
the full and faithful description of the model present.
So that's kind of handy.
Normally when people write variable dependency diagrams,
they don't necessarily expect that the arrows mean
actual functional dependencies,
and you can't recover the entire model without that.
So this gives you a way to do that.
And in fact, our original inspiration was to be able to read some papers
and understand what they were up to,
and sometimes the papers were a little hard to read.
So you could ask, for instance,
if I have this economic model, can I extend it to more countries?
It should be cut and paste,
like I take a country one on the left and country two on the right
and link up some kind of trading relationship there.
We found a paper that claimed to do this,
and this paper was kind of hard for us to read
because none of my students, and not me either,
were not economists.
But after a while, we were able to write down the diagram,
and in fact, this is the diagram lifted from that paper.
They didn't draw it this way. We wrote this diagram.
But this is a good way to understand how your models are built up,
and you can see that your models are very compositive.
You can compose your models very nicely this way.
So this paper, written by Ishiyama in 2001,
does this kind of thing.
It takes two Goodwin models associated to two countries,
builds a pricing model that explains how trading occurs between them,
and allows you to splice those two models together.
Exactly as we had thought.
So that's pretty handy, and now we're starting to go down the process
of starting to bring some data into these models
and really see whether or not this is a good model of economic processes or not.
And of course, that means we need some kind of computational approach for doing this.
So we can quantify data model consistency,
and I think it would be very interesting to see how this works in many fields.
See, this worked for sensors,
and this appears to be very useful, or at least promising, in economic modeling.
And so the question is, where else does this work?
Well, in order to really test this out, you need a computational library.
You need to be able to bring data into this and try it out.
You don't want to be computing global sections by hand.
So although you can put together small examples together by Ad Huck,
a larger process should have a nice consistent software library.
I propose to you that there is such a software library that we've been developing called PySheef.
It's up on this GitHub page. Have a look.
It's still very much work in progress, but I welcome comments and suggestions on that end.
And if you want to do pull requests, go right ahead.
And with that, I'd say hope you enjoyed this video.
Have a look at some of the preprints that describe this in detail.
And if you have any questions, please feel free to contact me.
Thanks a lot.
