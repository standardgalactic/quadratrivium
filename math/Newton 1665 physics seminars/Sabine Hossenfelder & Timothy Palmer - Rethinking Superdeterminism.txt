Okay, I guess we can start. Welcome to today's session of the Newton 1665
seminars. This week we're gonna have a couple of sessions on less beaten
tracks and today we're gonna talk about super determinism. And I think Alessandro would like
to say some introductory words before we start. So please Alessandro.
Okay, so just because this is an unusual problem in high-energy physics, so maybe a brief
introduction is useful. So one possible unusual question is what is the gadget that makes random
numbers in quantum mechanics? So can quantum mechanics be some sort of statistical approximation
to a deterministic theory? The usual answer is don't even try because of barely inequalities,
but super determinism maybe is a way out as we will hear today. And then tomorrow,
TOFT will present a possible idea along these lines. And next week, Gerard will talk about the
Wolfram model. Okay, so Sabina, please share your screen and you can start.
It's okay, the first speaker is Sabina Stumphelder. Yeah, please go ahead.
Okay, well first of all, welcome everybody. As Alessandro just said, this will be a talk about
the foundations of quantum mechanics. And I think I want to start by saying that I've always been
perplexed about the foundations of quantum mechanics, not because of the mathematics,
but because of the way that the research area is organized. You have on the one side,
you have lots of great experiments going on. And then on the other side, you have people talking
about the interpretation of the mathematics. And in the middle between the two, there's a big gap.
So this is where I would expect that to be a phenomenology of the mathematics of the quantum
foundations. And there is basically nothing, not in the way that we have it in particle physics,
where we have theory and experiment and then in the middle between them, we have phenomenology.
So what I'm talking about today is my attempt to fill in something into this gap, where there
should be the phenomenology. So it would be a little bit unfair to say that there's absolutely
no phenomenology. There are the models with spontaneous collapse that I think reasonably
can be said to be phenomenology. And I think this is not entirely unrelated to what I'll
be talking about. So this approach falls into a type of theory that has become known as super
determinism. And so I want to start with saying that, so this is a talk about the foundations
of quantum mechanics, but not about the interpretations of quantum mechanics. And
if there's one message that I want to get across, it's that super deterministic models are not
interpretations of quantum mechanics. They're different theories from which you can derive
quantum mechanics. So they're more fundamental and in certain circumstances, they also make
different predictions. And I wrote a paper about this recently together with Tim, who will be talking
after me. So I want to first give you an idea of why I'm interested in this question.
Quantum mechanics is a very successful theory. And that's great, of course. But fundamentally,
it just cannot be how nature works. And that's not because it is unintuitive or ugly, but because
it is axiomatically inconsistent. And I want to spend a few words on this, because I find that
for the most part, the typical education that students get in physics kind of glosses over
this problem. So in quantum mechanics, we have two equations that play the role of the
dynamical law. This is the Schrodinger equation and the measurement update, which is also sometimes
called the collapse of the wave function. And it's the combination of these two dynamical laws that
leads to the measurement problem. So the measurement problem in brief is that quantum
mechanics is not an ensemble theory. It's supposed to be a theory that, yes, it also holds for
ensembles of particles, but it is also a theory for individual, for single particles.
Now, what quantum mechanics tells you in certain circumstances, except for very special cases,
is that you end up with a state that is with a certain probability in a certain measurement
outcome. It does not tell you, well, the particle is measured in this particular state. It says,
it is in the state with a certain probability. And that's fine if you're speaking about an
ensemble, but it's not fine if you're speaking about a single particle. A particle that's 50%
measured is just not a thing. It does make any sense. And this is why we need this update of
the wave function, because if you have the Schrodinger equation together with Born's rule,
then you get this probabilistic prediction. But then once you measure the particle,
you know that it is with 100% probability in a particular state. And the only way to
accommodate this in your theory is to update the wave function so that now you're with 100%
probability in this particular state. And this is just necessary to correctly describe what we
observe. So this is the measurement problem, you know, that this assumption fundamentally
makes no sense, as I will explain in a second. Let me just point out here that decoherence does not
solve the problem, because the only thing that decoherence does is basically to convert
quantum probability into a classical probability. But it does not make the probabilistic
prediction itself go away. So in more detail, where does this problem come from? Well, the
measurement process in quantum mechanics is not linear. If you have a state, say a photon or
something, and you put it through a beam splitter, so now you have a superposition of the photon going
into two different directions, then the outcome of measuring this state is not the same as the
superposition of the outcomes for every single particle. So if you have a particle going left,
it will go into the detector with, into the left detector with probability one. If you have a
particle going right, it will go into the right detector with a probability one. But if you have
a particle that's in a superposition of left and right, it will either go on the left detector or
on the right detector, it will not do both. So this is clearly a process which is not linear,
and this means that it's incompatible with the Schrodinger equation, it just cannot be derived
from it. But if you think that quantum mechanics is a fundamental theory that describes elementary
particles, and you know that the detector is made up of elementary particles, then the measurement
postulate should be, strictly speaking, unnecessary. The behavior of macroscopic objects in general,
not only detectors, but in general, should be derivable from the rest. But this is not the case.
And now you can say, well, maybe it's just really complicated and no one has been able to do it so
far. But because the one equation is linear, and the other one is not, we know that it isn't
possible. So this is clearly an inconsistency. If you think that quantum mechanics is a fundamental
theory, the other logically possible option is to just give up on reductionism and say, well,
the detector just does something that you cannot derive from the underlying theory.
But I don't know how to make any sense of this formally on paper in some equations. So I do not
think that this makes a lot of sense. So I'm afraid I'm a reductionist all the way down.
The measurement problem has been known for a long time, of course, but it has remained unsolved.
So there are the copemaking approaches and the neocopemaking interpretations like
cubism, they bring back the problem by just renaming it. Instead of talking about detectors,
they may be talking about knowledge that is held by agents and so on. It's the same thing. You're
bringing in concepts that are actually macroscopic that should in principle be derivable from the
underlying theory. So you still have this inconsistency. Many words, there seem to be a lot
of people who think that many words solve the measurement postulate, but you just bring it back
in because you have to, in the many words, you have to tell, so if you want to make a calculation
with the theory, you have to make a statement about relative to what you make the measurement.
So you do not make your measurement with the detector that is smeared out of all possible
branches. You only calculate the probability relative to the detector in your branch and just
formally that's exactly the same as the measurement postulate. There's actually an easier way to see
why many words does not solve the measurement problem. Many words is basically the same as the
normal quantum theory Copenhagen that we have learned except for the measurement postulate,
and then it has a fancy interpretation on top of it. But if you just look at the mathematics, if
many words were able to solve the measurement problem, it would mean that you can derive the
measurement postulate from the Schrodinger equation in many words. But this would just mean that you
can derive the measurement postulate from the Schrodinger equation period. So the many words
interpretation really has nothing to do with that. Yeah, and then there are models with
spontaneous collapse that I already mentioned in the beginning, and they do solve the problem
in the sense that they tell you the state actually does go into a detector eigenstate.
By making little collapses along the way basically. But this only works after you
tell your dynamical law what state the wave function is supposed to collapse into. So you
actually have to put this into the equation already, which is a little bit awkward because it doesn't
seem to be quite local. Yeah, and speaking of locality, then there are pilot wave theories,
and they kind of solve the problem. But they have an explicitly non local ontology. And also,
you know, I suspect that they actually have the same problem about having to specify what state
to collapse into what I have an ongoing argument with pilot wave fence about this. So I don't want
to get into this. Okay, so what is the conclusion to draw from this? Well, I think for me, this just
means that quantum mechanics is not a fundamental theory. It's an emergent theory. And the measurement
postulate is an effective description for a complicated process in an underlying theory,
which we have not yet discovered. This underlying theory is deterministic, but it has to be non-linear,
as we've seen previously, because otherwise you can't get the measurement process out of it. And
it is what goes under the name of a hidden variable theory. This idea is also supported by the obvious
similarity between the classical Leoville equation and the von Neumann Dirac equation.
So now, of course, you can say, well, that's, that's obvious, right? So clearly, if you have
a process that is not deterministic, the first thing you would try is to come up with
an underlying theory that's actually deterministic and try to derive it, right?
So why hasn't anyone looked at this before? Well, of course, people have looked at it. It was pretty
much the first thing that they looked at after the discovery of quantum mechanics. But as we have
learned notably from Bell's theorem is that any theory that is deterministic local and solves the
measurement problem must violate a condition known as statistical independence. And so here's the
key condition of statistical independence. Lambda are the hidden variables that make
the measurement outcome determined and no longer stochastic and A and B are the measurement settings.
So this equation tells you that the hidden variables are not independent on the detector
settings. So violating statistical independence means essentially that the time evolution of the
prepared state depends on the detector settings. And that's what's called super determinism. So
super deterministic theories have a bad reputation. And I now briefly want to tell you why and why I
think that these criticisms are not justified. Yeah, before I do that, I have to briefly define
exactly what I mean by super determinism because not everyone means the same thing. So super deterministic
theory is a hidden variables theory, as I already said, that solves the measurement problem. And it
reproduces quantum mechanics on the average. So it's a psi epistemic theory in the sense that the
wave function itself does not correspond to an element of reality. So it's like I could, you know,
I could take an average of the phases of everybody in this meeting or something and I would get out
a phase, but the phase itself would not correspond to anyone who's actually in the meeting. And so
the wave function basically plays the same role in this kind of theory. It's deterministic.
Interestingly enough, not everybody who works in super determinism also assumes that the theory
is deterministic. I think it makes no sense if it's not deterministic because then why bother.
And it is local in the sense that it does not have any spooky action at a distance. So
any kind of interaction is supposed to travel locally and not jump through space. And then it
follows from this that the theory must violate statistical independence if it's supposed to be
in agreement with observation and be nonlinear because otherwise it could not solve the measurement
problem. Yeah, and needless to say, it follows that it reproduces quantum mechanics and it sounds
kind of silly, but I point this out because it has happened to me quite frequently that people
come back and say, yeah, but what about the so-and-so inequality and the blah, blah, blah bound and
all that kind of stuff. And I'm like, well, yeah, you know, if the theory reproduces quantum
mechanics on the average, then it's also going to reproduce all these bounds. So of course,
you want to know how it actually reproduces quantum mechanics on the average. But once you have that,
all the rest will be fine. Okay, so the literature is full of misconceptions about
what the consequences are that I briefly want to go through. The major misconception is that this is
just unscientific because super deterministic theories are necessarily void of explanatory
power. So that basically they say, well, you can't explain anything with this. But this doesn't make
any sense because super determinism gives rise to quantum mechanics. So in the suitable limit,
it just makes the same predictions. Now you can say, of course, well, if it makes the same
predictions with more assumptions, then that's just useless. So why bother? But of course,
you don't know how many assumptions you actually need until you've looked at your theory. So if it
makes the same predictions with fewer assumptions, it has more explanatory power. And if it requires
more assumptions, then it should give rise to more predictions. So whether or not it does that,
you can only find out by writing down the concrete theory and looking at it just by
saying that the theory is super deterministic, you just don't know if it explains more or less
than quantum mechanics. So it just doesn't follow. You just have not enough information to even make
that statement. Okay, so the second misconception is that it would make science impossible.
If we would allow calling correlations between the prepared state and the detector
and explanation, then science just wouldn't work. Okay, so the first mistake of this argument is
fairly obvious, which is that by assuming that such correlations cannot explain anything,
you've assumed what you want to show. You would actually have to tell me why this doesn't explain
anything. But what's worse than this is the second mistake here is that just because statistical
independence is a useful assumption for classical measurements, and no one denies that,
it explains a lot. Certainly, you cannot infer that it also holds for quantum experiment. That's
just not an allowed inference. It's kind of, it makes about as much sense as saying, well, I have a
lot of soccer balls or something, and look, I throw them in the water and they all float on top.
I conclude from this that stones will also float on top of the water. Clearly, it doesn't make any
sense. And also, you know, I think this argument is just silly because science arguably works just
fine regardless of whether anyone talks about a super determinism. So I think this is just nonsense.
Okay, misconception number three, it's a conspiracy. This is the argument from fine
tuning. It's kind of similar to the first argument. The claim here is implicitly that one needs a lot
of detailed information to obtain a prediction from that theory, which renders the theory
non predictive. Now, the thing is that you just cannot say anything about this without actually
looking at a particular model. If you think about what fine tuning means visually, it means that
you have some final state that depends very sensitively on the initial state. But the only
way that you can make a statement about it is by having a dynamical law that connects the final
state with the initial state. And, you know, in our fairness, no one has any dynamical law for a
theory that is super deterministic. So there's just no statement that you can make about it.
And why do people even come up with this? Well, they have an intuitive idea that rests on a notion
of what states are probable or close to each other. So this argument typically comes in the form,
well, if I would only make a teeny tiny change to this initial state, then everything would go
badly, badly wrong. But to even make such a statement requires that you have a measure
that tells you which states are probable or are close to each other. And again, you just cannot
generically make a statement about this without writing down the theory. Okay, so this is an
appeal to intuition because it relies on this idea of what it means to make a small change.
Yeah, and so here's the misconception number four, it would eradicate free will and there seem to
be a lot of people who are bothered by that. Where does this even come from? What statistical
independence is often referred to as the free choice or the free will assumption in Bell's theorem.
That's because it tells you something about supposedly tells you something about what the
experiment can do with their detector setting. This terminology, I think it's really unfortunate
because it raises the impression that super determinism puts some really outrageous constraint
on human consciousness. I think this really makes no sense because, well, for one thing,
choosing a detector setting does not necessarily require any human to do anything. So that doesn't
have to be any free will in this free will assumption. But maybe more importantly, free will
is hard to make sense of in any theory that is deterministic. It has nothing to do with
super determinism in particular. I would say if you have a theory in which the future is determined,
then free will is a notion that you should give up on. It just doesn't make any sense
to talk about it. Now, of course, there are a lot of philosophers who would disagree with that.
And the way that they like to argue is that there is a certain way that you can define a notion of
free will, which you can do even though the theory is deterministic. And that's fine with me,
really. You can debate this with the philosophers if you want. The point I have here is that this
whole debate does not depend on whether the theory is deterministic or super deterministic.
It also does make a lot of sense to me to obsess about this relation between the detector
setting and the hidden variables because the laws of nature always constrain what we can do.
And again, that's not specific to super determinism. An example that I like to mention is that if you
have two fermions, you can't put them into the same stage. We all know that. But no one goes
about and says, oh, well, that puts a constraint on the free will of the experimenter because
they're not able to put these two fermions into the same state. You know, this would just be ridiculous.
That's just the way how nature is. And it's the same with super determinism. You know,
may just be the way that nature is. Okay, so this is a red herring. It really has nothing to do with
super determinism in particular. Okay, so what do we actually learn from all these bell type
tests that have been done? Well, we learned from this that Bell's inequality is violated.
And we learned from this that at least one of the assumptions of theorem
cannot be realized in nature. But these tests cannot tell us which one it is.
And so by assumption, the super deterministic theories give rise to exactly the same violation
of Bell's inequality. So doing bell type tests over and over again just will not teach us anything
about super determinism. So this was basically my motivation for why one should think about
super determinism. And now this was a lot of words. Now I want to show you at least
two equations or something. There are a few toy models for super determinism for which
you find references in the paper that I mentioned in the beginning. There are basically only three
theories. So the probably best known is two which you will hear about tomorrow, I guess.
Then there is Tim's which you will hear about in a moment. And then there's my little approach
that I briefly want to flesh in the final slides. So I have a background in particle physics. So I
like to conceptually think in the terms of particle physics. And the way that I like to think about
this super determined evolution is by way of a path integral. So normally, if you use the path
integral in quantum field theory, you say that the system takes all possible paths.
And you get a probability distribution over the outcomes. So the modification that I'm proposing,
and I'm working on this with my student, whose name is Sandra Donati, is that the system in the
end takes only one particular path. And that's the path for which the end state optimizes a
particular functional. And the question then comes down to, well, exactly what is the functional
that the path is supposed to optimize. And we know that to solve the measurement problem,
you basically need something that gives you a large contribution for states that are in a
quantifiable sense, very classical. And here is the path integral that we're currently
looking at. In the end, maybe that's not exactly what will work, but that's what we are looking at
today. It uses the same formalism that we have in quantum theory. So we have some
wave function that is an element of the Hilbert space. And we could make this a Fox space,
but for now we just have a Hilbert space. I think the complication is not going to the Fox space.
And then in the path integral, we sum over all paths in configuration space.
Okay, so we do not sum over classical paths. We sum over all the paths in configuration
space that are encoded in these factors in the expansion of the wave function. And then
where we have an exponential of some function that basically plays the role of what is usually
the action. And so that's the question exactly what goes there. This function that we are trying
and now here, it has two factors. The first one, you can see this just by looking at it,
it will be stationary if the wave function fulfills the Schrodinger equation and we can
actually formally show this. And then we have a second factor that roughly speaking measures
the quantumness of the state and the higher the quantumness, the more this path will be suppressed.
So why do I write something funny like quantumness and not actually an equation?
It's not that I don't have an equation, it's that I have too many equations.
What could go there is basically any measure that you can find of entanglement or whatever is
the opposite of classicality. And there are literally dozens of them that people are talking
about in the literature. So we are right now basically looking which one works best. I think
in the end, it will not make a big difference. So what this term basically does is it will quantify
how fast the collapse happens and how it depends on the number of constituents of the system.
But qualitatively, I think it will not make a difference exactly what one puts there.
Yeah, so how could one test this? Well, all super deterministic theories
have some things in common, which is notably that they are, well, they are deterministic in
contrast to quantum mechanics. And this means if you have exactly identical measurement setups,
you will get identical measurement outcomes in contrast to what you would get in quantum
mechanics. Of course, the question is how do you actually know that you have an identical
measurement setup? Again, this is something which strictly speaking, you can only show by looking at
a theory. But I think there are some generic statements that can be said about this, which is
that most of the variation you would see in the measurement outcome probably comes from the degrees
of freedom of the detector itself. So if you can limit the number of degrees of freedom of the
detector, or you can freeze them into as good as you can, of course, you can't entirely freeze them
in because then you can't detect anything with the detector. But you want to limit them as good as
you can. And then you want to measure in a sequence as quickly as possible. Then you should
see an autocorrelation in the time series of the measurement outcome that according to quantum
mechanics should not be there. And this generally requires small code systems in which measurement
can be repeated in rapid sequence. I think that we would learn much, much more from experiments
of this type than doing bell type experiments over and over again. Okay, so this brings me to my
last slide. I do not want to repeat everything that I just told you. I just want to repeat the
major point that I want to get across, which is that super deterministic theories are not
interpretations of quantum mechanics. They are more fundamental theories for which quantum mechanics
derives. Okay, so that brings me to the end of my talk. What do I do now?
Okay, thank you Sabine. No, please leave the slides on for now. There is time for questions. So
for the audience, does anybody have questions?
You can just pick up if you do.
Hello, you are talking about, it's Maria, I don't seem, I couldn't put my video, it's Maria Kupczynski.
At the beginning you told that probability is the probability of the particle. Probability is
property of the random experiment. So you dismiss, you are saying that quantum mechanics,
you talk only on this individual interpretation of quantum mechanics, but usually in every
domain of particle physics, and we are mostly using statistical interpretation of quantum mechanics,
and quantum mechanics predicts some regularities in quantum phenomena and things like this.
You are talking about that you can reproduce each time if you have the same preparation you
should give as the same outcome. So you neglect completely that you have measuring devices,
different PBS and detectors, when you cannot control internal parameters. So its initial
preparation has nothing to do that you obtain the same result. And if you have statistical
interpretation, the collapse is nothing magical, nothing instantaneous. So why you just,
you don't need super determinists. Moreover, you talked in Berlin, you have the statistical
independence, and it should not be statistical independence because in each settings you have
different parameters describing measuring devices, and it was shown that you can explain in
locally causal way correlations and real experiments, and so you can completely dismiss this type of
research. Besides, if you are talking about particle physics, I cannot see how from your
Schrodinger pass integrals you will reproduce standard model and all these things. Thank you.
Okay, this was like five different questions, and as you came to the end I had already forgotten
the first one, but I'll try to do my best. Okay, so as to the statement that probabilities really
only make sense if you're talking about an ensemble, I entirely agree with that. Nevertheless,
we want a theory that actually describes what the single particle does, and if your point
was to say that quantum mechanics is not the right theory for that, then I entirely agree with this.
Now about the point that, you know, the measurement apparatus changes from one
measurement to the next, that's exactly the problem. This is why I say you have to limit
the degrees of freedom. You have to try to freeze them into the extent possible.
You can ask, how do you know that you actually prepare the state the same way? This depends,
of course, on what you think the variables are. As I said, strictly speaking, you can only make
a statement about this if you actually have a model, but what you can do in any case is that
you can make repeated measurements on the same state. So this reduces the number of degrees of
freedom further. Of course, you will never get entirely rid of the variation that comes in from
the measurement apparatus. So the question is, can you reduce this variation to a level so that
you can actually measure the autocorrelation time? You seem to be referring to a certain type of
theory or interpretation that I am not familiar with and so cannot say anything about it.
For what the standard model is concerned, so the reason that I was trying to write this down as a
path integral is that I think it has an obvious way to be generalized to a quantum field theory.
I haven't thought about it all that much because for now it would be fine if we could
make it work with quantum mechanics, but this thing is Lorentz invariant and I think you
can second quantize it. Priority, I don't see the problem with that. In the end, you should be able
to get back the standard model, I think. Okay, so more questions, please try to keep them focused.
I have a question if I may. Yes, tomorrow I will talk about roughly the same subject,
except I'll put in much more equations and from the equations, then you will see that what I
consider super deterministic theories are a small subset of all quantum theories. So most
quantum field theories do not allow you to simplify them into something super deterministic,
but a very special subset does. It only differs from the quantum theories we are using today
at very high energies where we have no experimental data at all. So what's wrong about assuming
that at very high energies something very special happens. If so, these theories would be highly
predictive and non-trivial because they tell you something about the domain of physics where you
have no access today, but perhaps tomorrow. So a question to you then in the very end is
what you call a super deterministic theory, is that a subset of all quantum theories or is
it totally different? Because the latter I think is not necessary. Maybe you can first try to
answer that question. Well, I would just say they're not quantum theories. It's like they're
neither classical nor quantum, but maybe let me try to answer you the question in a little
different way. I think if I understood this correctly, you were making a statement about
the possible interactions that you can have in the theory. Yes, and so this is something that we
just have not thought about. It may possibly be, but since we are only dealing with quantum mechanics
and we have not even written down exactly what the Hamiltonian looked like,
I just don't know. It may very well be that there are only certain types of interactions
that you can write down. But you very well know that only very little is actually
known and understood today. We have the so-called standard model of the subatomic particles,
but much of the standard model is unknown. And in fact, everything that happens at very high
energies is unknown. So it's obvious that if you make a change in theory at very high energies,
nobody will notice any difference except in the area of determinism, a lot changes. If you change
the very high energy behavior of your particles, you will change all the statistical consequences
by their theories. Although whenever you do a scatter experiment, you see the same statistics,
but you don't see the same actual events. They will depend a lot on what happens at extremely
high energies. So there is an enormous amount of space there to modify your theory. And I claim that
if you modify your theories in a very, very special way, there is no effect on the low
energy behavior. So it will just be the standard model again at low energies, but at very high
energies there will be something else. And this should make these theories very predictive and
to my mind also very important. Yeah, I vaguely feel like we've had this discussion before.
I think the difference is that I implicitly assume that effective theory remains valid.
So you can basically forget about all the high energy degrees of freedom. And that may just
be wrong. But yeah, this implicitly goes into what we are trying to do.
So I saw that you wrote only one equation, which I thought was a very vague equation.
What I'm trying to do is try to write down accurate equations that define for you what a
super deterministic theory is and what the quantum theory is and how one can be a subset
of the other. This is not obvious at all, but it turns out to work that way. For instance,
the question you asked about what makes quantum theory produce random results. And the answer is
in a super deterministic theory, random out equals random in. You don't know the initial state,
so don't be surprised not to be able to predict the final state. What's strange about that? That's
totally reasonable in any scientific theory. If you don't know exactly what you're doing,
you'll have error bars around your results about the prediction. Nobody should be surprised about
that. Yes, I entirely agree with that. So for me, the mystery is not so much where does the
randomness come from, but why do you always end up in a detector eigenstate? That's the
quantum theory function. And then exactly where also a super deterministic theory says, well,
if you have a true description of the actual initial state, your description of the final
state will also be true or be accurate. But of course, we don't know what the initial state is,
so don't be surprised and we don't know what the final state is. And don't be surprised when you
measure something, you have some actual information as you didn't have before. So you prefer to look
at another wave function, which happens to be the collapsed one. That's just an improvement
of our understanding, just like it is in any non quantum statistical theory, like the van der Waals
gases. Of course, when you observe an atom in a van der Waals gas, as soon as you deserve it,
you know for sure the probability distribution has that particle cause a delta peak in your
probabilities. That's not a collapsible probability function. It is just a different description of
what we know today. So that's a psi epistemic story, which doesn't wear such size, never need to
be collapsed manually. Nature will do it for you. Yes, I said, as I said in the beginning,
you can have the certitude that the wave function only gives you some probabilistic some
makes only a statement about the information that you have about the system. But then you are
implicitly giving up on reductionism, as I said in the very beginning. Well, because then this
theory is not the theory for the constituents of the system. You know, you somehow have to either
you don't have a theory for the constituents of the system or they're actually incompatible
with each other. You somehow have to. Maybe you can continue this question tomorrow if you're
listening, then I'll try to explain how it goes. We are just doing ordinary science,
you know, working with the uncertainties that modern science is coping with. Of course,
we don't know everything. Nobody should be surprised about that. Nobody should complain about that.
That's the way our field works. We have limited information about the two states we are in. So
limited information going in equals limited information, right, getting out. So that's not
something specific for quantum theory at all. Any deterministic theory and lying it,
you'll have the same property. Okay, I would suggest to pass to the next speaker and then since
we have many questions in the chat and people raised hands, in case at the end we postpone
these questions and we try to address them at the end. So I would thank Sabine for now and
pass to the next speaker. So Sabine, please remove your sharing and team, please prepare
your slides. So perfect. So yeah, it works. So next speaker is Steve Ballmer. So please.
Can you hear me? Yep. Okay, great. Okay, so what I want to do is give a specific example of what
Sabine calls a super deterministic model and it's something I call the invariant set model
of quantum physics. I won't say very much about myself. I have a little bit of time at the end,
I'll say a little bit about myself, but basically I'm in Oxford in the physics department.
Sorry, I need to... Sorry, just one second. Yeah. The talk is based on a couple of papers. One
appeared in the Proceedings to the Royal Society about a month or so, a couple of months ago.
Paper called Discretization of the Blocksphere Fractal Invariant Sets and Bell's Theorem.
And I'm happy to send people either pre-prints or the link to the paper. The other is from an essay
written for the Foundational Questions Institute, which is a bit more, a bit less technical, but
goes into some of the issues about undecidability and uncomputability, which I think is a relevant
issue here. I don't want to repeat what Sabine has said, but I just want to again stress
that what we're not talking here about is an interpretation of quantum mechanics,
but an approach to quantum physics that somehow goes beyond quantum mechanics. And like Sabine,
I feel very strongly that there's no compelling reason to believe that quantum mechanics is a
fundamental theory of physics. Sabine mentioned the close similarity between the Schrodinger
equation in the density matrix form and the Louisville equations. And the Louisville equation for a
chaotic system is linear, not because the chaotic system is linear, but that the linearity expresses
conservation of probability. So my own view is that because of that close similarity, the linearity
and indeterminism of quantum mechanics is unlikely to be a fundamental feature of quantum physics
itself. There's something quite interesting which I'm not going to dwell on, but I think is something
people might want to think about, which is that the continuum seems to play a much more fundamental
role in quantum mechanics than it does in classical mechanics. In classical mechanics, we can very
simply discretize systems which are based on the continuum. And a good example is to do
weather forecasting from the Navier-Stokes equations. We can very easily discretize the Navier-Stokes
equations and produce pretty reasonably good forecasts of the weather. But the continuum
seems to play a much more fundamental role in quantum physics through the complex Hilbert space.
And if you try to discretize that, you violate one of the axioms that Lucy and Hardy formulated a
few years ago as what he calls reasonable axioms for quantum mechanics. The continuity axiom that
you can get from one part of Hilbert space to another by a continuous transformation will
violate that. And so you will not approximate quantum theory. So that's I think quite a deep
concept that we need to think about when we try to formulate more finite-based,
discretized theories of quantum physics. The property, as again Sabina said, property of
non-locality makes unification with GR very problematic. And in my view, it's perhaps the
single most important barrier to the prevention of us formulating a quantum theory of gravity based
on quantum field theory and GR. And I want to show in this theory that by going towards the
thought of a more finite theory of quantum physics, we can actually evade this conclusion that physics
is not locally causal. So I'm going to start off in a place which seems extremely different
from quantum mechanics and in fact is being very much motivated by non-linear dynamical systems
theory, which is to think of the universe as a non-linear dynamical system, but one that evolves
precisely on a particular geometry of chaotic systems, a fractal invariant set geometry. And
the basic notion then is that the laws of physics, a little bit not totally dissimilar to general
relativity theory, but now thinking about state space rather than space time, that the laws of
physics at the most primitive derive from the geometry of this invariant set. So I want to
just kind of focus on that a bit more because I'm not talking that the example in that last figure
was from the Lorentz attractor and that's not exactly the fractal attractor I want to discuss.
So what I'm imagining here on that curly line the left of the diagram is a state space trajectory
and the idea is that if you zoom into it sufficiently with sufficient resolution,
you will actually see that it comprises a helix of trajectories and the number I'm going to denote
by two to the power m. And if you zoomed into one of those helix of trajectories you would see that
two was comprising a helix of two to the power m trajectories. So if you take a cross section
through that, if you continue that process and take a cross section through one of those tubes,
the cross section is on the bottom middle there, you'll see something that as you take more and
more of these fractal iterations looks like a set of disks within disks within disks.
And those of you who might know the mathematics will know that this is a kind of topological
representation of a number system which is incredibly powerful in pure mathematics used
routinely in number theory and so on called the the the the piatic integers. So it's the set of
so-called piatic integers would be homeomorphism homeomorphic to one of these fractal disks.
Here the p is two to the power of m. And we'll come back to that in a little bit later in the
talk. The other thing I wanted to say was the notion of measurement if you like or decoherence.
In Sabina mentioned the many worlds theory where somehow trajectories branch into separate
branches. Here at the coarsest fractal iterate you might think of these trajectories branching.
But in fact if you zoom in and think about these helixes of trajectories that go around,
then in fact these are just exponentially diverging from each other in a in a way very similar to
classical chaotic instability. So there's no branching at a high enough fractal iterate.
And the branching is presumed to sort of go in the way of or the divergence of trajectories
are presumed to lie into discrete clusters. And here I've just shown two clusters which I call
A and not A. And these are presumed to correspond to to measurement eigenstates or the observed
measurement outcomes. And it means that we can label symbolically each of the trajectories in
one of these fractal helixes with a symbolic label A or not A according to whether that
trajectory evolves to the cluster A or the non-linear cluster not A.
Okay. Now the purpose of this paper, the Procroy sock paper is to show that you can use the language
of complex Hilbert vectors and associated tensor products for that matter to describe
probabilistically this notion of a helix of trajectories. Now I'm not going to have time
to explain why and I would just refer people to the paper for the details.
But the idea is the following that you have two parameters theta and phi of a complex Hilbert
vector sort of qubit vector qubit state. The theta parameter is in the amplitude as you
can see there and it's such that the cosine squared of theta over two gives you the frequency,
the number of the trajectories in the helix that evolve to the A cluster. And sine squared theta
over two gives you the frequency of members in the helix that evolve to the not A cluster,
the other cluster. Now a crucial point here is that the number of trajectories in the helix is
finite. So this cosine squared theta over two is a rational number and moreover it's a number which
has a finite binary expansion and this means that the cosine itself is a rational number.
Now the complex phase phi denotes essentially a rotation around that helix and again because
there are a finite number of trajectories around the helix, phi itself can only be a
rational multiple of pi or two pi. So where we differ from quantum mechanics is that these
Hilbert vectors are only defined where the squared amplitudes and the phases are rational numbers.
This is quite a crucially important restriction over standard quantum theory and essentially the
irrational for example the irrational values of phi would correspond to angles which lie in the
gaps, in these fractal gaps in the helix that I talked about, the gap between the trajectories.
Okay I'll explain this, you'll see shortly why this is a super deterministic theory but just
just I'll just go through a couple of more sort of semi-technical slides for the moment.
Now everything in the theory rests on a crucial theorem in number theory called Niven's theorem
which basically says if you have an angle between zero and pi over two
and it's a rational multiple of pi and in particular it has a finite binary
expansion then necessarily the cosine of that angle will be an irrational number.
So as you know for example if phi was pi over four then its cosine is one over the square root of
two which is irrational and that's a generic property of this cosine function.
And I've just shown in the diagram I've shown values of phi every you know expressed in terms of
you know rational multiples of pi and the cosine and you see in general the cosine the
cosines of the dashed lines they don't line up in any way with the the solid lines and that's a
generic property of Niven's theorem. So a particular example of this is if you took two
Hilbert vectors one would one has the the angle phi in its amplitude and the second has its angle
phi as a phase angle. Then in quantum mechanics these two are related by a unitary Hadamard type
transformation so Hadamard would take you from the first psi one state vector to the second size two
state vector and they would be both well-defined states or points on the block sphere.
However in the proposed discretization and by using this Niven theorem then if psi one
is defined in this discretized approach which means that the cosine square the phi over two is
rational then necessarily psi two is undefined because that then the the phi angle cannot be a
rational multiple of pi. Now this holds no matter how big this discretization is no matter how big
M is and that incidentally raises a question which I think Gerard asked of Sabina which is the
relation to quantum theory. In these types of theories quantum theory arises as a singular limit
the continuum of quantum theory only arises and you literally go somebody's just turned on a microphone
if you could mute I think it would be good.
So M equals in so you rise at the continuum Hilbert space at M equals infinity but any
finite value of M will not give you Hilbert space so this class of theories quantum mechanics
arises as a singular limit and not in any sense is it contained within these theories.
Now I want to say what does this mean in practice this type of discretization result well imagine
you have two imagine you're doing a Mach-Sender experiment and on Monday you you send particles
through the full interferometer and you measure the statistics of the detector outcomes.
Now according to this theory when you're doing these experiments and you can describe the state of
any of the particles by this standard Hilbert vector then the phase angle difference or the
phase path length difference between the two arms of the interferometer phi is such that the cosine
squared phi over two is rational. Now on Tuesday suppose you do an experiment
with the same interferometer but you take out the second half-silvered mirror so then you're
basically asking which way do the particles go around the interferometer. According to the
theory the Hilbert space would Hilbert state would be one where the phase angle now arises
as a in the complex phase and again it's this Hadamard which connects the two. Now what does this
number theoretic property imply it implies that if you ask a counterfactual question so you say
what would I have observed had I performed Tuesday's experiment on Monday's ensemble.
So Tuesday's experiment requires that phi is a rational multiple of pi but Monday's experiment
it's the cosine of phi that is the rational number not the angle itself. So this in this number
theoretic program it means that these types of counterfactual experiments what would I have
observed had I performed the which way experiment when in reality I did the interferometric experiment
is undefined and that's really you know the sort of thing that Bohr of course through his principle
of complementarity focused on in many of the discussions in the early 20th century and is a
kind of a one of the defining features of quantum mechanics and essentially underpins the the non-
commutativity of quantum observables the fact you cannot simultaneously perform say a position
and a momentum measurement that basically means that if in reality you did a position measurement
on a particle and then you ask the question what would I have observed on that same particle had
I performed a momentum measurement. Quantum mechanics tells you that's not possible and this
number theory gives you a in a sense a reason or a theory why that has a reason and the reason is
in the counterfactual experiment you're actually perturbing the state of the real world into one
of these fractal gaps in the invariant set. So exactly the same arguments apply to Bell's theorem
and you know I'm not going to go through this in any detail except to say that so you imagine
you're doing a Bell experiment you have a particle pair represented by hidden variable lambda some
specific value and let's say in reality Alice measures her particle with her detector set in
the zero position this is a standard Bell type experiment where you have each Alice and Bob
can choose between a zero orientation or a one orientation of their of their apparatus. So we
assume in reality that in a particular experiment particular situation with a particular particle
labeled by lambda that Alice does and Bob both do the zero orientation experiment so that has a
non-zero probability. Now according to the this invariant set model and again I'm not going to
say why but it's basically for the same number theoretic reasons it's to do with this you know
the rationality of phi and cosine of phi and it turns out that the probability that the experiment
one one was performed that's a counterfactual uh is actually has got the same probability
according to this theory but the probability that the zero one experiment or the one zero
experiment was performed given that the zero zero experiment was performed those are both
identically zero the zero one and the one zero experiment lie off the invariant set in this fractal
gap so as counterfactual experiments they become non-ontic if you like they are impossible uh
they're not parts of the theory. Now this is precisely the violation of statistical independence
that Sabina talked about um and so but this violation of statistical independence comes
about when you consider the you know these counterfactual alternative experiments that
might have happened but that didn't and this in this particular theory these counterfactual
experiments if they don't lie on this fractal invariant set if they correspond to states in
the gap then they're they they're not consistent with the theory. I just want to make two very
quick points uh on this point without carrying on what is to do with fine tuning um I mentioned
that piadic numbers are a very natural way to represent fractals and for those of you who know
these things there is an associated metric associated with the piadics called the piadic
metric I'm not I'm surprisingly um and it turns out that points which don't lie on the on the fractal
are piadically distant they they they're the distance is at least p and if p is a large number
2 to the m large number then that's a then they're they're distant from the points which lie on the
fractal set so this is my refutation of the fine tuning argument that fine tuning uh you know depends
on a choice of metric and the natural metric to use with these fractals is the piadic and not the
euclidean metric um the second point very quickly is that which is about this um notion of
predictability these fractals are actually technically non-computable which is what the
theory uh what the essay wrote about for foundational questions so in a way you can't
decide you certainly can't decide by algorithm ahead of time whether a particular setting of
Alice and Bob for a given lambda will lie on the invariant set or not so you know you you may
as well I think this is what Gerard was saying you may as well for all practical purposes treat
the outcomes as random but you know ultimately there this is underpinned by a deterministic theory
okay um so because this theory is based on Hilbert vectors albeit ones with these
rational amplitudes and rational phases the theory violates the bell inequality exactly
as does quantum theory but because it violates statistical independence because of these
essentially these counterfactual states have zero probability when they don't lie on the
invariant set um then really this gives rise to a rather novel I think way to interpret bell's
theorem and the violation of bell inequalities without giving up on local causality in other
ways without giving up on the basic principle principle that underpins relativity theory
okay um I I've sort of finished my my talk I'm going to do something which Sabina will hate me for
but which is to give a couple of quotes but this is perhaps useful for two reasons well what one
what two reasons maybe one is to just give the quote but also my own background is I did a phd
many years ago in Oxford under the cosmologist Dennis Sharma but during that time was very strongly
influenced by Roger Penrose and Roger was in fact my internal phd examiner
and I always you know have taken inspiration from some of his critiques of quantum theory
and one that I very strongly agree with was a statement he made some years ago that
to understand quantum non-locality we shall require a radically new theory not just a slight
modification of quantum mechanics but something as different from standard quantum mechanics as
general relativity is from Newtonian gravity would have to be something which has a completely
different conceptual framework and that's totally motivated my my work in this invariance set
type of model and then the second uh oh yeah the second quote Dennis Sharma my supervisor
was actually himself supervised by Paul Dirac so I have some sort of distant affiliation there too
so I'm I'm always pleased to remind people that Dirac was himself a critic of quantum mechanics
despite of course being one of its seminal founding fathers and this is a quote from the
Lynn Chey and Academy he talk he gave in the 70s one can always hope that there will be future
developments which will lead to a drastically different theory drastically different theory
I've I've emphasized from the present quantum mechanical theory and for which there may be a
partial return of determinism I like to think of this idea of sort of non-computable determinism
as being perhaps qualifying for that type of notion that he was talking about so that's my
talk thank you very much thank you team very nice talk so questions it's time for questions
so please pick up if you have one hi I have a general question that
since these theories like black hole physics and general relativity they are better on a better
footing so is like is there any instance where the new theories like super deterministic theories
and your work have been tested on results from black hole physics or you know general relativity
well I speak for myself Sabina may say something I mean I think the great you know the great area
for the future is testing quantum mechanics under condition you know on systems
where whose self-gravitation is not negligible and you know we don't yet have the ability to do that
in in the laboratory and the question about whether things like superposition
and even entanglement still will still hold for such self-gravitating systems I think will be a
a major step forward in our in our experimental capability my own view from this model the
suggestion is that well I don't believe superposition is a is a is a in a sense a fundamental
property of systems and the more the self-gravitation of the system the less the manifestation you will
see of these properties of superposition or indeed entanglement so I'd make that as a prediction
of my particular approach I don't know what Gerard says about that sort of thing but
this is an exciting time I think when we can start to test in the lab
the quantum mechanical properties of self-gravitating systems
right thanks may I say something
with that you assume that gravity somehow is an exceptionally different kind of
force and all the others but on the other hand gravity like any other force is an action
principle so I can also say that since you have a theory based on action and reaction gravity is
very similar to the other forces but I have a more more relevant question and that is when you
talk about the continuum you say the continuum is an essential feature of quantum mechanics
however we can imagine living in a finite universe where everything is finite including the total
number of states you can be in and in fact when it's common practice in many branches of my field
for people to write models where a total Hilbert space is just replaced by a finite factor space
and that you do if you realize that your calculations can never be infinitely precise
for example we have the well-known number alpha to find such a constant one of hundred thirty seven
something it is not known whether that number should be a real number or a rational number
both possibilities are still open and we will never be able to figure out
which is true by any measurement as far as I can imagine but so why do you think that the
distinction between continuous and discrete is so essential I would contradict that I think
there's a natural overlap between the continuous theories and the discrete theories
okay just just come back to the first point of course I mean general relativity is not a theory
of a force it's a theory of gravity and and of course I think the form of the action in general
relativity is in a way quite different from the form of the action say in electromagnetism you know
which is which is why you know the sort of standard spin two versions of quantum of electromagnetism
just don't describe gravity so there's something different about the about the form of gravity
the form of the action that makes it quite distinct from other types of gauge theories for example
and and my own view is that that's the reflection of the fact that gravity is a theory of geometry
and and well in a sense this is what I'm trying to bring to the discussion about
quantum mechanics but albeit fractal geometry as far as the continuum is concerned I mean I'm
talking about the the I'm not talking about the fact that you can't have a finite system
comprising a finite number of particles but the state space is a in quantum mechanics is the
continuum Hilbert space so in other words you use the field of complex numbers to describe your state
space and and the question is can you in some sense start to discretize that complex the complex
field even when you have a finite number of particles and Hardy Lucian I mean I'm referring
really to Lucian Hardy's work where he ends up concluding that the continuity axiom the ability to
to have unitary transformations but continuous unitary transformations between any two states
in this complex Hilbert space even if it's a finite dimensional Hilbert space is an essential
aspect of quantum theory and if you start to violate that then you then your theory is no
longer quantum theory but there I agree so the question is the dimensionality of Hilbert space
whether it's important that that is infinite or can you very well make models of nature
where dimensionality of Hilbert space is finite I agree that any super post states you can choose
any superposition coefficients real or rational whatever you like yeah but that's that's the
question you know the complex number the field of complex numbers seems to seems to play a more
fundamental role in quantum theory than say the field of real numbers does in classical theory
you know you can easily discretize state space in classical theory and you don't really introduce
any fundamental obstacles I don't think but in clarity probabilities which are also of course
continuous classical physics yeah but anyway I mean that that's what I was talking about is how do
you how do you go about um if you like discretizing the continuum of Hilbert space not not the
continuum of spacetime I mean that's that's another issue okay I do have a question actually
so if understood correctly basically you mentioned that time evolution makes you pass from a dense
set like rational numbers once to a different dense set and this is at the basis of the complementarity
and like for instance spelling the quality and so on now my question I'm not familiar with the
details of the theorem but my question is that is there any sort of recurrence time where you go
back to the same set or at least you overlap with the same set and can these give observable consequences
and principle um well I I well um I think if you have a theory which is ultimately based on finite
numbers then there will be a natural recurrence time but that may be of course extremely long and
extremely large so I don't know in practice whether that has any sort of experimental consequence
um what what I'm what I'm saying by the way is is is not that you sort of go from one set of
numbers to another set of numbers um it's that you have these two I mean if you like the the the
angles where the cosine is rational is kind of a discrete set from the from the set of angles
which are themselves rational multiples of pi and so if you're on if you're if your states are
described by one of those number systems uh and then you ask a question which you know which is
inevitably a kind of counterfactual question which would for it to be well defined um have to have
to you'd have to invoke one of the numbers from the other set then because they're dis disjoint
rather they're disjoint slits then you know that that state cannot exist okay thanks so more questions
people can ask Sabina a question as well of course
okay I had a question in principle um you you are talking as I believe also in local causality
but it doesn't mean why you are talking about super determinists because you can have local
causality without talking super determinists in principle your row you are saying it's probability
it's uh but if you're writing row zero one in your example in some sense you describe some
ensemble so you get p zero so there is no super determinists in this respect if you the problem
is super determinists related with bell considerations that something is influences
experimental choices of the settings but but you can have a stat violation of the statistical
independence if lambda depends on the settings and this is natural that lambda can depend on the
settings because instruments are a big chunk of equipment which are governed by their own
parameters so so it can be really explained that it's uh you don't need super determinists to have
setting dependence of the um lambdas and in this case violation of bell inequalities
well look formally I mean there bell bell's uh theorem you can you can formulate as a as a
theorem in mathematics so you know under certain situations um you know the theory uh must
satisfy or must violate bell inequalities now the the usual I mean you know the usual argument
is that um you know if you imagine these particles ensembles of particles um being created
you know hundreds or thousands of years or millions of years whatever you want um before uh you ever
um measured them you know then their whatever their properties are this this would be the
the standard theory it's not the one that I endorse by the way um the standard argument is that the
the hidden variables you know um the properties or probabilities the hidden variables shouldn't
depend on you know the details of what happens in the long distant future um so the you know
the the standard argument is if you if you want to invoke some relationship between the
statistics of the hidden variables and the measurement measurements are settings
then you're introducing something somehow conspiratorial into the into the mix and people
typically don't accept that no this is the false argument because I know it's a false
argument I agree it's a false argument but I'm just trying to tell you why people think this
this is false argument of course because you know okay so I can probably you don't know my paper
anyway so but I can send usually the problem is that the source is producing some some
coordinated signals and they arrive to detectors and in in function of the setting of detectors
there is no predetermination the the the the thing which is broke and broke the
conclusion of the bell is that there is no predetermined outcomes because the outcomes
are not predetermined because the parameters of the detectors in different settings are different
and they are not controlled thank you but I like your lecture yeah
well I think we agree in a sense because if you if you relate the um the hidden variables to the
detector settings then you have some kind of a super deterministic theory I guess we're we're
we're all agreeing that you know this has been a neglected I think a neglected area in the analysis
of the bell theorem and my view is that it it's it's actually you know it underpin it's the reason
why we haven't been able to move on beyond quantum theory because things that you know things like
pilot waves just don't really help you just end up with something much more explicitly non-local
which is um which is very unattractive to people so that's why people have just stuck with quantum
mechanics because the alternatives seem even worse but now I think we're on the cusp you know with
Gerard and yourself and Sabina and myself and others we're on the cusp I think of some quite
radically new thinking where these ideas are exposed to being quite false
okay thank you team I would just select a couple of questions from the chat
and they can be for either speakers so please feel free to intervene either of you so for instance
Albert was asking to Sabina about either of you can answer how do I get the born rule from super
determinism so it's about the born rule who wants to answer that I mean my my view I just
tell you in a nutshell my view is once you use vectors to describe probabilities in other words
you know you have an amplitude times uh one direction plus an amplitude times the second
direction uh then born's rule is a consequence of the Pythagoras theorem there's nothing mysterious
about the born rule so I you know we we I use Hilbert vectors to describe um the the ensemble
statistics of my invariant set and so because I use vectors then necessarily the born rule follows
the born rule being that the squared amplitude gives a probability uh if you do that you'll
violate if you violate the born rule then you're violating Pythagoras theorem so I I see no uh
possibility uh but that the born rule must be satisfied thank you Sabina do you want to add
something yeah so um I think there there are two parts to born's rule one is the particular way
that you calculate the probabilities by taking the square of the amplitude um the other one is
the question how do you get the prefactors to match those that you have in ordinary quantum mechanics
so I think Tim basically answered the first part it's like the moment you have
your theory with vectors in the Hilbert space born's rule is what you get and formally that's
basically what Hardy's proof tells you like this is this is the only probabilistic calculus you can
put on the theory of that type so this leaves the question like where do you get um the the
prefactors like the weights of the different components in your expansion to match those
of quantum mechanics and again you know I can only say that you have to find the right dynamics of
your theory basically I think what it will come down to is that your theory needs to have a mix of
um some kind of attractor mechanism that brings your state into a detector eigenstate
and then of top on top of this you need some kind of randomness that is probably um strongly chaotic
which um reproduces a distribution um that has to match that of quantum mechanics and so I
know that this is wake you know I wish I had an equation uh to fill in the blanks but that's the
best I can do okay thank you Sabina uh a last question from the chat well I try to interpret a
few of them um again for both speakers is super deterministic time reversible or is not clear
yet or not so please it is time reversible um in principle uh you know in practice it's
as not reversible as you have uh it's in statistical mechanics like if you um if you
wanted to reverse it in time you would have to know very very exactly um what state you are in
so it's um it's basically the same thing okay so um it's actually an interesting question maybe I
I should have said something about this but forgot about it if so of course the the collapse
of the wave function is strictly speaking not reversible in time because you have a lot of
initial states that end up being the same outcome so in super determinism you never strictly speaking
do this collapse to one detector ion state it's just that you focus uh your trajectories in these
clusters uh until you can no longer distinguish them but strictly speaking um they never fall
on exactly the same trajectory okay team do you want to add something um I think there must be
some element of irreversibility in the theory as a whole I mean in my particular uh theory or model
where I'm using these uh fractal sets in in in state space um these are these arise from
in classical physics in classical chaotic physics these arise from systems where which have some
degree of irreversibility you know in that you have some dissipation or something like that
which means that volumes in state space get shrunk to zero to zero volume the measure on the uh on
on the fractal invariant set in the euclidean space in which it's embedded is is zero um
but the question is where you know where does this irreversibility actually kind of manifest itself
and it turns out that you actually only need irreversibility in very very small parts of
state space to generate this fractal structure everywhere so you know for example it could be
that you know at the moment of the big bang or in you know spacetime singularities or something
like that that those these are the times and places where some measure of irreversibility occurs
and that leaves an imprint on the whole uh you know fractal structure of the of the system so you
know for all practical purposes um you know we can use Hamiltonian dynamics in laboratory situations
quite well and get get away with it but um the theory has to have some irreversibility I think
at some level at some place but that's a detail which I I don't have complete understanding of yet
okay thank you are there get it maybe you want to yeah I think this is the one
place where uh deterministic theories can be more general than quantum mechanics
because of course in the classical system you may or may not introduce equations of motion which
are reversible but it could also be that two different initial states evolve into getting to one
final state so that from that final state you never know anymore what the initial state was
this is possible but you can all but more likely have something like as an underlying theory something
resembling a van der Waals gas so all these atoms each atom individually obeys a time reversible
equation yet the whole lot uh since it contains so many atoms obeys thermodynamics and you know the
laws of thermodynamics are not time reversible so uh so then it's it's a question on how precisely
you want the theory to be now for me it's very important that if you have an underlying super
deterministic theory that you use that the equation of the theory in principle with infinite
accuracy like in a discretized theories where you just work with numbers which are infinitely
actively defined in that case the question of time reversibility then suddenly becomes very
interesting it may or may not be time reversible it may or may not be that two difficult initial
states evolve to the same identical agents of final state this is possible but looking at nature
it's more likely that we should we should be uh it should suffice to look at models which are time
reversible so this is my favorite today but i'm not changing my mind tomorrow about the underlying
theory being not time reversible it's conceivable okay okay so i would close the session now and
i would thank both the speakers for the nice talks and for answering many many many questions
thanks and i would invite you all to the talk by professor toft tomorrow at the same time on
similar topics but different okay so thanks bye thank you
