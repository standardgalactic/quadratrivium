Hello everyone, I'm Norman Wildberger.
Today we're carrying on with our introduction of diffusion symmetry as an alternative to
usual group theory approach, and I'm hoping to interest physicists and also chemists in
a broad new arena of algebra that really has dramatic implications to the sciences,
I believe.
So today we're going to focus on the key concept of a hypergroup, which from my point
of view has some nice advantages, sort of centrally located, especially with respect
to duality.
And there is going to be a precise definition today, but more importantly, I'm going to
outline five broad classes of examples of hypergroups, so that you will really see that
they come about in a wide variety of different places.
So the subject perhaps officially got going in the 1970s with three papers by Dunkel,
Jouet and Specter, the measure algebra of a locally bounded hypergroup by Dunkel, spaces
with an abstract convolution of measures by Jouet and measures invariant sur les hypergroups
by Specter.
These papers, as you can sort of see by the titles, are somewhat measure theoretic, so
they're mostly concerned with the locally compact or continuous versions of the algebraic
structures that we're looking at today.
But I think it's easier for people getting into the subject to start with the finite
examples, which are very concrete and explicit.
And there's actually a very rich variety of those.
And a reference for that material is this paper of mine, which you won't be able to
see, but it's called Duality and Entropy for Finite Commutative Hypergroups and Fusion
Rule Algebra in the Journal of the London Math Society in 1997, so pretty well 25 years
ago.
So I'll be following somewhat the orientation there.
Okay, so we're going to be defining what these things are more precisely, adopting more or
less a multi-set framework.
The underlying number system is always the rationals, so if I don't state otherwise,
we're talking about the rationals.
And we're aiming towards five key examples, which are really families of examples.
And the last one I think is especially interesting, and I'll go into some detail so you can actually
see some explicit examples for that last category.
All right, so for us, what is a finite commutative hypergroup?
It's a setup involving a number of things, so crucially there is an algebra, actually
a star algebra, and actually a commutative star algebra, okay, over the rational numbers.
And this star algebra also has an identity, the identity is called C0.
And in this algebra, there is a linear basis, okay, the basis is denoted by K, consists
of finite number of elements, C0, C1 up to CN, so C0 is the identity.
And this basis should first of all have the property that it's closed under the star
operation.
So what do we mean by a star algebra?
Well, the star refers to an involution, that is a bijection that if you do it twice, you
get back to where you started, which is a linear transformation of the underlying algebra,
but which respects the multiplication in a precise way.
If you take a product A times B in the algebra, and you take the star of that, then you should
get B star times A star.
So in the non-commutative situation, there's typically a twist that occurs.
In the commutative situation, of course, that doesn't have any effect.
So this star operation should have the property that it permutes the elements of K. So we
take any one of these guys, and then the star of that is going to be another element in
this finite set, okay.
So what makes this entire story a hypergroup?
Well, first of all, we can express the algebra structure in terms of what happens if we multiply
two elements in our basis, okay.
So if we multiply Ci times Cj, then we get a linear combination of these things, say
the summation K equals zero to N of some constants N, some ij super K, times Ck, see these are
the coefficients of the Ck's in the expansion of Ci times Cj, so these are the structure
constants of the algebra.
And we require some special features of the structure constants, okay, three things.
First of all, the structure constants should all be positive rational numbers, okay.
We're working over the rational numbers, we said that, but no negative numbers are allowed
for the structure constants that are appearing here, okay.
All positive and for me, positive means greater than or equal to zero.
Secondly, if we sum all of these coefficients that occur in one of these products, so if
we sum K equals zero to N of NijK for some fixed i and j, then we get one.
That's telling us that this has an interpretation as a probability, because all these coefficients
are positive and they sum up to one.
So in fact, they're all necessarily between zero and one.
And finally, the crucial property that tells us about the star operation or in particle
language, the antiparticle relation between a particle and its antiparticle.
The condition is this, that C i star equals C j, so C i and C j are related by being stars
of each other, precisely when the coefficient of C sub zero, the identity in the product
C i times C j is non-zero, in other words, it's strictly bigger than zero.
So another way of saying this, if we multiply C i, say times, maybe we'll use some other
ones, say C l times C j, okay, you multiply those two together.
And you look at the sum that you get.
The identity element C zero or the photon appears with a non-zero probability, only
if, precisely when, these two objects that you're multiplying together are each other's
stars, okay, that's the only way you get a non-zero probability of getting the photon.
Okay, having done this, we can now introduce some additional terminology, having to do
with weights, okay, this will be useful further as we go on.
The weight of an element C i is one over n i j zero, where we're assuming that the
star of C i is C j, okay, then we know that this particular number n sub i j zero is strictly
bigger than zero, so it's not equal to zero, so we can take its reciprocal, and the reciprocal
is called the weight of C i, that's also the weight of C j.
And then the weight of the entire hypergroup is just the sum of the weights of all of its
elements.
So these are important notions that figure in the harmonic analysis that I will be ultimately
telling you about.
The first of our family of examples is very familiar, it's the class hypergroup of a finite
group.
So we have a finite group G, its conjugacy classes are C zero up to C n where C zero
is the identity, we're thinking of these as multisets from G, and then we normalize these
classes.
So we divide each class C i by the number of elements in it and call that new element
C i small c, okay, now you can think about this in two ways, you can think about it as
an element of the rational group algebra, if you're familiar with some group theory,
but sort of more simple-mindedly, you can just think about this as being a rational multiset,
that is it's a multiset where elements can appear with certain multiplicities, but we're
allowing ourselves the freedom that these multiplicities can not necessarily just be
positive integers, they can also be negative integers, and even fractions, okay, it's like
a portfolio, I have three stocks of this, I have half a stock of that, and I have, well
I owe somebody else three stocks of that.
And the multiplication of these m sets is just induced by the multiplication on the
group.
Okay, so then our class hypergroup becomes the set of these things here, which you can
think of as a basis of the algebra of conjugation invariant or class functions on the group,
if you like, and here are the structure constants, okay, and we might call these the class hypergroup
structure constants.
So given a finite group, I claim that, you know, elucidating the nature of this class
hypergroup is a primary object, and it's not so easy to do, okay, so even for very well
known groups, this hasn't really been done, so there's a fair amount of work, I think,
to be done just from the group theory point of view in terms of bringing out what these
class hypergroups actually explicitly look like.
Our second family of examples is, I think, particularly important to physicists.
It's what we're going to call the character hypergroup of a finite group, G, so G is a
finite group, and now we're looking at the irreducible characters.
Let's call them x0 up to xn, with x0 being just the function which is 1.
So these irreducible characters, which are functions on the conjugacy classes, they are
associated, according to Frobenius, to irreducible representations.
Let's give the irreducible representations names, pi0 up to pin.
Now the point-wise product of these functions, point-wise product, means that you can take
the product of any two of them, and you can express the result as another combination
of the corresponding things with the numbers Nijk, capital Nijk as coefficients.
And this equation is a reflection of the corresponding decomposition of the tensor product of the corresponding
representations.
So this is a very important link.
The representations are harder to deal with, but they do have the advantage that they clarify
the multiplicative structure that's going on here, because taking a tensor product of
representations amounts to somehow taking the tensor product of matrices, first introduced
by Kroniker in the 19th century.
So you have a three-by-three matrix, and you have a four-by-four matrix.
The Kroniker product gives you a 12-by-12 matrix, so it's a way of multiplying these representations,
and then this is a decomposition into blocks.
And because you have a decomposition into blocks, the positive integral nature of these
coefficients is clear.
But at this stage here, we don't have the hypergroup yet, because we haven't renormalized.
So we renormalize by dividing each of these irreducible characters by the corresponding
dimension of the irreducible representation, which can be read off rather easily.
That's just the value of the corresponding character at the identity.
So let's call that Di, then you divide Xi by Di, and you get Chi i.
So these are renormalized, and that's what the character hypergroup of the group is that
it's these things here.
Now this relation here, coming from the characters or coming from the representations, is modified
necessarily because of this renormalization, and we get Chi i times Chi j is the summation
over K of little n i j k Chi k.
And it's not too hard to write down the little n i j k's in terms of the capital n i j k's
and these dimensions.
And now these numbers here are the character hypergroup structure constants, and there
are important things.
And for a physicist, this same story here really is reflected or repeated in the Li group
situation.
Instead of having a finite group, you might have a compact Li group, let's say, like
SO3 or SU2, and you can do something similar, you can still talk about irreducible characters
and corresponding representations, and you can tensor product representations, and those
numbers that are appearing there are quite important for physicists, let's call them
Klebsch-Gordon coefficients in certain contexts.
So this is pretty well directly connecting with things that are quite familiar to physicists.
Now in my first lecture on this subject, I started with the edge graph of an icosahedron.
So our third family of examples generalizes what's going on here.
So the graph theorists have figured out what a nice condition is that will ensure that
this hypergroup-like game that we were able to conjure up from the icosahedron can be
played in other contexts.
So a convenient way of doing that is to consider distance-transitive graphs.
What's a distance-transitive graph?
Well it's a graph which has the following property, that suppose you have two vertices
U and V over here, okay, and you look at the distance between them, so that means distance
along the actual edges of the graph, you know, so there's three edges, the minimal number
of edges to go from this vertex to this vertex is the distance between them.
So U and V have a certain distance, and two other vertices somewhere else, U' and V',
are also of the same distance from each other as these two are.
Then the property that this graph should have is that there should be an automorphism of
the graph.
That means a bijection, okay, a bijection of vertices go to vertices, edges go to edges,
which map this pair to this pair, okay, so that we can realize a global symmetry that
makes the graph from this point of view look exactly as it does from this point of view.
That's called distance-transitive, and that's the condition then that allows us to find
these hypergroups.
These hypergroups are then sort of definable, okay, so what is the hypergroup?
The hypergroup is then the hypergroup of circles about a fixed vertex, so straightened here
with this example.
Here's a famous graph with ten vertices, sort of a pentagon on the outside and a pentagram
on the inside connected like this, it's called the Peterson graph.
It's a source of lots of interesting examples and sometimes counter examples in graph theory.
This thing happens to be distance-transitive, okay, you might stare at that and try to convince
yourself of that.
If we say choose one vertex, say that C0, then the circle of radius one about it will
be this C1, these three vertices here, while the circle of radius two from it, distance
two, will be these ones here, these one, two, three, four, five, six vertices, that's C2.
In this case, the hypergroup that we're talking about has three elements, C0, C1, and C2,
and the multiplication is the same kind of one that I've been telling you about already
and here are the relations and you can check that these really are the relations coming
from this graph.
Okay, so you can convince yourself of that, that's a nice exercise.
So this is a very rich family of examples because graph theorists know lots of distance-transitive
graphs and in fact, the commentatorialists then started asking the question, okay, well,
that's very nice for a distance-transitive, but can we extend, can we enlarge this family?
Maybe there's graphs which are not quite as strongly symmetric as these graphs, but still
have enough structure so that we can define these hypergroup-like things.
Actually the commentatorialists don't often necessarily speak in terms of hypergroups,
they have Bose-Mesner algebras, okay, or association schemes, those are the terms that they're
more likely to use to describe not exactly something like this, but pretty well, pretty
close to it, just differing only by a normalization from this one.
So anyway, they realize that there's a wider family of graphs called distance-regular graphs.
And the distance-regular graphs, maybe about the easiest way of characterizing them is
to say that their graphs, for which this same game of trying to convolve circles from an
arbitrary point, will yield a consistent algebraic structure in exactly the same way, but that's
a little bit more complicated and not so easy and clear to say as the distance-transitive
graphs, this is a very explicit, sort of understandable condition.
So we have a whole wide slew of examples coming from graph theory combinatorics, algebraic
combinatorics that will generate hypergroups for us.
So our fourth family of examples are what we might call double-coset hypergroups or
Galphand pairs, named after famous Russian mathematician, 20th century Russian mathematician,
I am Galphand, very influential and prominent mathematician.
So the story is we're starting again with a finite group in this case, although Galphand's
thinking actually extends certainly beyond that, but let's say a finite group to start
with, and it has a subgroup, say H, so that's a subset which is closed under the operations
of the group and inverses.
So if you have a subgroup of your group, you can take cosets, but actually you can take
left cosets and right cosets, and you can actually do both at the same time.
So you can take double cosets.
So a double coset is something that looks like this, so you take an element x in the
group and you consider all the possible products of the form h1 times x times h2, where h1
and h2 are from our distinguished subgroup.
So this is then a subset of the group called a double coset.
And these double cosets partition the group, so they divide it up into a disjoint union
of these double cosets.
So we could give them some names.
So one of them is just h itself, let's call that h0, okay, and then we can write g as
this disjoint union, that's what these sort of square union signs mean, h0, h1 up to hn.
So now g is a union of these double cosets.
Okay, now we do the same kind of thing we're already familiar with.
First of all, we renormalize.
We divide each of these sets by the number of elements in that set, getting a kind of
an element in the, again, in the rational group algebra of the group.
And then these little hi's, they are then the double coset hypergroup, I haven't actually
stated where it is, okay.
But yeah, the hypergroup then consists of these hi's, and here then there'll be some
structure constants of this kind, hi times hj equals summation k, nijk, hk.
Now but an important provisal here, that in this very general context, we don't always
get a commutative structure for these hypergroups.
After all, if you took h to be just the identity element, then these double cosets would just
be the elements in the group themselves, and this, you know, this object that we're getting
is really the same as the group.
So if the group is non-commutative to begin with, it's not necessarily the case that such
a double coset hypergroup ends up being commutative.
So when it does, that was identified by Gelfand as an interesting condition on the pair, the
pair consisting of g and h.
So they're called a Gelfand pair.
If this double coset structure that we get is actually commutative, and if it is, then
we can see that it's a hypergroup, and then we get another family of examples of hypergroups
coming from group theory.
So our fifth and final example is a very interesting one, and very wide ranging.
I think it's going to be especially interesting, perhaps, even to chemists.
So we are going to call this the automorphism orbit hypergroup, k of g and h.
So g is a finite abelian group, so it's important to know abelian or commutative group, okay?
That's our starting point.
And h is a subgroup of the automorphisms of g.
So a group g has automorphisms.
Those are bijections, maps from the group to itself, which are homomorphisms, and they
respect the multiplication and the inverse operations.
So we're supposing that we have some subgroup of automorphisms, and not all automorphisms,
maybe just a few automorphisms, but they are forming a closed group of automorphisms,
in the sense that when you compose any two of them, you get another one, and the inverse
of one of them is also in the set.
Okay, so under these conditions, the automorphism group h will act on the group g, and we can
talk about orbits of h on g.
So here's the big group g, and we take an element x in it, and we apply the elements
of our automorphism subgroup h.
So h of x, well, you'll get a couple of elements, and that's called the orbit of x under h.
And what happens is that g decomposes into orbits, like any element.
You can apply h to it, and you get some string of elements, and that's sort of a closed little
thing that's going to be separate from the orbits of some other thing, unless they coincide.
So we get this disjoint union of g, not into conjugacy classes, but into these orbits of
h on g.
c0 up to cn, whereas c0, as usual, is just the identity.
That's always going to be an orbit by itself, because that's always mapped to itself under
any automorphism.
Okay, what do we do now?
Well, we replace these sets with corresponding averages in the group algebra, or if you like,
as rational multisets.
So we define ci, little ci, to be 1 over capital ci times the sum of all these elements in
ci.
And then our hypergroup, k of g, say we call it h, is just the set of these ci, c0 up to
cn, the little ones.
And there's going to be some structure constants.
So when you multiply two of these, you can convince yourself that you're going to get
a sum of other orbits, okay?
And there'll be some coefficients, nijk.
So this is really interesting, because this is a very flexible kind of thing that induces
all kinds of examples.
And in particular, one very interesting sort of family of such examples is when you take
a finite group h acting linearly on a vector space over fq.
So if you have a vector space over a finite field, this is a finite field with q elements,
okay, that's a vector space.
What does that mean?
It means, first of all, it's an abelian group, okay?
It's an abelian group that also, you know, has some, you can multiply vectors by elements
in the field.
But in particular, it's an abelian group.
That means you can add vectors, and there's a zero vector.
So that's an abelian group.
And if you have a finite group h acting as linear transformations of this vector space,
then you can think about this h as being essentially acting like automorphisms.
So in other words, we can play this game now by taking the orbits inside this vector space
of this group that's acting linearly.
So the orbits are going to be these basis elements for this hypergroup after we've
renormalized, so they all sort of have average weight, you know, constant one.
So let me show you an example of this.
We're going to take a simplest group that we know pretty well, S3, the symmetric group.
And we're going to look at its two-dimensional representation, so these are two-by-two matrices
that we've already seen before.
I'm going to write them a little bit in a novel way.
But we're going to consider this now as a representation over a finite field.
In fact, pretty well the simplest finite field, the one with three elements, so F3.
So I'm going to introduce a little bit of streamline notation.
Instead of writing minus x, let me write x bar, okay?
I don't like this minus x because it confuses, this minus sign is then being used in two
different ways to denote negation and also to denote an arithmetical operation of subtraction.
And I think that's confusing.
So I don't think this is really optimal.
I think this is better, okay?
So I'm just going to gently steer us in this direction, so allow me to put one bar.
You can read minus one if you like.
Here are the six matrices.
We've seen these before, but now instead of having one, one, zero, minus one, we have
one, one, zero, one bar, okay?
And we're going to think about these six matrices as acting on what?
On a vector space of dimension two over F3.
What does that look like?
Well, it looks like this.
It's basically a grid.
So here is like an x direction.
Here's y direction.
Here's the x coordinates, minus one, zero, one, minus one, zero, one in the y direction,
okay?
So that's the origin, components zero, zero.
This is the point one, zero, et cetera.
So this is a representation of this small two-dimensional vector space over F3.
So it's only nine elements.
And I'm going to think about these elements here as actually row vectors.
So if you have two-dimensional matrices like this, they act on rows, okay, on the right,
or they can act on columns on the left, so you have a bit of a choice to make.
So let's agree that we're talking about row vectors, and these matrices act on the right
to send row vectors to row vectors.
So now what we're interested in is the orbits of this group, this is the group H, okay?
Starting on this abelian group G, which is just this vector space over F3.
So the simplest thing is the origin.
The vector zero, zero gets sent to zero, zero, so its orbit is just itself.
If we look at this vector here, this is the vector one, zero.
If you multiply one, zero by any one of these things, what you're getting is essentially
the top row.
So you're getting one, zero, that's that one there, or you're getting minus one, zero,
that's that one there, what, really one bar, zero, or you're getting one, one, which
is that one there, or you're getting zero, one bar, zero, one bar, that's that one there,
or you're getting one bar, one bar, which is that one there, or you're getting zero,
one, which is that one there.
So the orbit of this element are these six vectors in our little vector space.
And then there's one other orbit, which is the orbit of the point, say, one one bar.
So if you take one one bar and you multiply by all these six, you'll see that you always either get this thing or this thing.
So these are the three orbits.
So now how do we make them into a hypergroup?
Well, we just use the underlying abelian group operation.
Now this is a vector space.
So the operation is like a vector addition.
Okay, so let's see what this means here.
C1 squared.
How do you get that?
So here's C1.
Okay, and what we want to do is we want to multiply it by C1.
So a way of doing that is to pick an arbitrary element of C1, say that one there, the vector one zero,
and multiply each one of these things by one zero.
So multiplication is really addition, so it's really like adding the vector one zero to each one of these.
Geometrically, what that amounts to is just moving it over to the right by one,
remembering that if we're here and we move over to the right, that sort of wraps around there because this is a cyclic aspect here.
So if we move each one of these six elements over by one, what do we get?
Well, this one moved over by one gives us this one.
This one moved over gives us this one.
This one moved over gives us this one.
This one moved over gives us this one.
This one moved over gives us this one.
This one moved over gives us this one.
So what are the results of all these movings?
Well, we get a one, or a two, or a zero, or a one, or a one, or a two.
So there's one of those six possibilities ended up with C0,
and three of them ended up with C1.
That one, that one, and that one.
So there's a three out of six, or one-half chance of getting to C1.
And two of them ended up giving us a C2.
And when you move that one, you get a two, and move that one, you get a two.
So there's a one-third chance of getting C2.
And you can check that C1 times C2 is C1, and that C2 squared is one-half C0 plus one-half C2.
So we get one of these pleasant order three hypergroups, which is quite nice.
And illustrates, I think, the power of this, because this is actually a very simple example.
We could jack this up in lots of ways.
We could just enlarge that field, for example, and play the same game over F5, or over F7, or over F71.
And while we're at it, let me just quickly tell you what the weights are in this case.
So how do you find the weights?
So the weight of C0 is always one, because C0 times C0 is one.
The weight of C1, you look to see what's the inverse of C1, or the star of C1, or the antiparticle of C1.
What do you have to multiply C1 by to get a C0?
Well, multiplying it by C2 does not give you a C0, but multiplying C1 by itself gives you a C0, and the probability is one-six.
We take the reciprocal of one-six, we get six, so the weight of C1 is six.
And the star of C2 is itself C2, and there's the coefficient of C0, that's one-half, take it's reciprocal, we get two.
So the total weight of this hypergroup is one plus six plus two, which is nine,
which is kind of in accord with the fact that we started with nine elements.
But the hypergroup, even though it's sort of lost the information of where it comes from,
it sort of has some recollection, in some sense, of where it came from as represented by these weights.
Okay, so that's some interesting illustration of this fifth family of hypergroups,
and I hope you can see that this theory of hypergroups is actually very interesting,
and it really ought to be a major source of study by algebraists in the 21st century.
It really ought to be a major importance.
And I'm going to convince you that it's also really interesting to physicists and chemists.
I'm Nolan Malberger, thanks for listening.
