Hello everyone, I'm Norman Wahlberger. Thanks for joining me. We're going to be
looking at central poly numbers, which I introduced last time in this context of
M-set arithmetic, very foundational new approach to arithmetic. And I want to
illustrate how these central poly numbers, which arise naturally this way,
actually figure prominently in one of the most important developments in 20th
century mathematics, which is the representation theory of Lie groups in
the simplest non-trivial possible example, which turns out to have huge
consequences for physics in actually a multiple of different areas. Okay, so this
has to do with the characters, particularly of these two very simple
Lie groups, simple, at least in the sense that they're the simplest ones, but
they're still not really very simple, called SL2 and SU2. So we're going to talk
about them and show you how these central poly numbers connect with their
characters. Now this is a highly technical and, you know, large area, which
usually requires all kinds of months and months of preparations and learning all
kinds of different things. So I'm just going to present a very cut-and-dried and
basic approach to these things. But nevertheless, I'm confident that I'm
going to be able to describe these irreducible representations of SL2 in a
way that you probably are not able to find anywhere else. Okay, so I'm going to
give you what I think is a really good way of thinking about these things that
really cuts to the chase. And it's very novel as far as I know, and I've been
working in representation theory on and off for many decades. Today's lecture is
going to be a little bit different from usual, because I'm going to have my
talking on the board like I do here. But I'm also going to augment that with some
hands-on illustration with a computer. So I'm going to take you, like we're in a
coffee shop and we're going to sit down together, and I'm going to explain how to
understand the representations of these groups in a very direct and
concrete way. So we're really going to get our hands right on the objects
themselves. And for that, I'm going to switch to scientific workplace. So you're
going to see me do some manipulations, and it's going to be a more dynamic thing.
If you like that kind of thing, all right, then on the Wildegg Maths YouTube
channel, you can become a member and you can find dozens, many dozens of videos
around a whole range of topics where I get into the details and roll up my
sleeve and start doing some mathematics in front of you. And you can sort of do
that with me. And there will be things for you to do alongside of that.
So the central poly numbers, or maybe central integral poly numbers,
c0 is 1, c1 is alpha to the minus 1 plus alpha,
c2 is alpha minus 2 plus 1 plus alpha squared,
c3 equals alpha minus 3 plus alpha minus 1 plus alpha plus alpha cubed.
And what you should think about is having a sequence of densities spaced
two apart symmetrically arranged around zero on the basically the integral
number line. And in terms of these sort of more elementary symmetric poly
numbers, b sub n, defined by alpha to the minus n plus alpha to the n.
So it's the single delta function here and single delta function there,
symmetric about zero. This is b1, this is 1 plus b2, this is b1 plus b3.
Okay, so those are the the actual algebraic objects that we want to connect with.
And now I'm going to move to a seemingly very different
area of mathematics, this story of Lie groups,
representation theory, and we're going to concentrate on the
the two or three most simplest examples, which are not abelian really,
which are the simplest so-called simple examples.
Okay, so sl2, so for us that's going to be
matrices of the form abcd, so two by two matrices whose determinant is one.
And an important question is what are these numbers here?
Where do they come from? So for us we are not able to say the real numbers because
we don't believe in infinite decimals, we don't believe that there
there are equivalence classes of Cauchy sequences, so being some mystical thing
that takes infinite amounts of time and effort to check.
Okay, that's a fantasy. So we want to ground ourselves, so
the numbers here are going to be rational numbers, that's one story,
but another story which is also really important for the situation
is when we look at complex numbers. And for us a complex number is
something of the form x plus iy, where x and y are
rational numbers, so we might say rational complex numbers,
but I'm just going to say complex numbers because we're not going to consider
these real complex numbers that people like to dream about
when they're talking about algebraically closed fields.
There's no such thing in this context. Okay, so that's sl2. Now a closely related
group, first of all this is a group, in the sense that you can multiply
two of them and you get another one of the same kind. The identity matrix is
obviously this kind and inverses of such things are also contained here.
So this is a group that has a kind of continuous nature
because these rational numbers can be chosen sort of continuously.
Okay, now there's a closely related group called SU2, which is very important.
For example, it's the group that appears in the middle of the standard model story,
the standard model u1 cross SU2 cross SU3, particle physics.
The SU2 there is responsible for symmetries having to do with the weak force.
Okay, that's this SU2. So it's also two by two matrices,
but now the entries are complex numbers. Okay, and there are the form alpha,
beta, minus beta bar, alpha bar, where the bar represents the usual complex conjugate.
But with the condition that the determinant is one, that's what the s stands for,
determinant is one, and that determinant condition is that alpha times alpha bar
plus beta times beta bar equals one. Now, it's quite obvious that this SU2 group can be thought of
as contained in this SL2 group here when considered with complex entries.
Okay, so this thing here is a subgroup of SL2 over the complex numbers.
This thing here has an alternate aspect as the three-sphere. Okay, so it's also considered as the
three-dimensional sphere in four-dimensional space. And it's a compactly group, so it's bounded,
doesn't go off. It's completely contained in some finite region of space, finite in volume.
And so this is a very important group having to do with spherical harmonics.
Yeah, hydrogen atom and of course the standard model. So these are closely connected and we're
going to be interested in the representations of them, so I have to tell you what that means as well.
Okay, so I want to introduce the idea of a representation to you and briefly, you know,
what associated characters are in the context of these relatively simple lead groups,
which are for us groups of matrices. That's my preferred way of thinking about lead groups.
So what is a representation? Well, it's a homomorphism of some group
into matrices. Okay, that seems funny because we're starting with a group of matrices.
Yes, but we're asking, okay, here's this group of matrices. How can we otherwise also
represent it with matrices, perhaps in some quite different way? Okay, so that's not a question that
may, you know, maybe first will strike you if you're starting this subject. But after a while,
you start realizing there's all kinds of reasons why this is a good kind of question to ask.
So I'm going to try to illustrate things by sticking to the example of SL2. Okay, that will
be our basic group, although we'll talk a little bit about GL2 also later. But SL2, group of 2x2
matrices, determinant one condition, and we're flexible at this stage whether we're talking
about rational entries or complex entries. Okay, so what do we mean by a homomorphism? It means
that we want to assign to any one of these things another matrix, say pi of g. And we want that
assignment to have the property that pi of g times h, so you take the product of the element g
and the element h in this group, that's another element of the group, and you consider the pi of
that, that you're getting the product of pi of g times pi of h, because these are all matrices,
right? So you can multiply these two matrices. And hopefully you get that one. That's the basic
homomorphism property. And okay, the identity in the group should go to the identity matrix,
and inverses should go to inverse matrices. Okay, but it's really this property here,
which is the essential one. So here are some examples. So the first three of a whole family of
examples of representations of SL2, the group SL2. And it turns out that up to a pretty natural
notion of equivalence, this is going to be a complete list of the irreducible representations
of this group. Now, I'm maybe not going to say what that means, you can just think about irreducible,
meaning that it doesn't break up in some kind of obvious way as a sum of smaller representations.
There's a way of kind of taking two of representations and sort of putting them together
in a kind of an obvious way. We want to exclude that, all right? So the first representation is
very trivial, called pi zero. It just sends every element in the group to the one by one matrix,
which has a one in it. So the one by one identity matrix. Well, it's kind of obvious that it satisfies
this sort of trivially, but still that's a representation and it plays an important role,
actually. Okay, the next representation is not much harder to come up with. We'll call it pi
one of G. We're going to send this element here to the matrix, which is itself. So pi one of G is A,
B, C, D. This is indeed a matrix. So yes, indeed, that's a homomorphism from SL2 into matrices and
that sort of obviously satisfies this property. Okay, so now we come to the first new non-trivial one,
pi two of G. And it's a three by three matrix. Have a look at it. Okay, so we're starting with this
matrix here and we're going to send this matrix to this matrix. A squared to ABB squared. Okay,
that's just binomial kind of expansion. C squared to CDD squared down the bottom. These two rows
here are pretty simple. Actually, on the sides, pretty easy to A squared, A, C, C squared, B squared,
B, D, D squared. The only curious one is perhaps the central element, which is AD plus BC. And it's
a great exercise to check that this thing here has the property that if you take two matrices
of this kind, okay, G and H, and you look at the corresponding matrices, three by three matrices
for each one of them, that the product of those three by three matrices
will be the matrix corresponding to the product of the two, two by two matrices that you started
with. That's the homomorphism property. Okay, so that's the first sort of non-trivial,
non-obvious representation of SL2. But we're claiming that it's really just the start here
of a family keeping on going, where in fact, the sizes of the matrices increased one by one
as we go. Okay, the next one in the pattern is pi three of G, which is this four by four matrix.
Again, we see sort of binomial expansion here and here. And these ones here on the sides are
also relatively predictable. This one B three, B squared, D, B, D squared, D cubed, you know,
decreasing powers of B, increasing powers of D, etc., same thing over here. And in the middle,
we have these entries here, which are all sort of similar, A squared D plus two ABC, and then
variance of that. And we claim that this particular four by four matrix, if we associated to the
two by two matrix ABCD, that mapping, sending ABCD to this thing here, that's a homomorphism.
That's the key property. Take two, two by two matrices and multiply them, look at the images
of all three of those matrices, then you're going to get a corresponding multiplicative
relationship between the images. So, some obvious questions. How do you get these patterns? Like,
how did I come up with this? I don't want to just present it as a rabbit out of a hat. Here's some
crazy formula and it seems to work sort of magically. So, you know, where do these things come from?
Okay, so that's the first question. The second question is, what's the pattern? In other words,
if we stared at these things and asked, well, what's going to be pi sub four? What's going to be
pi sub 12? Can we predict the form of that thing? Because it seems as if there's some kind of pattern
that after all the top rows are predictable, we know what the top row of the next one's going to be.
What about the whole thing? So, I want to address that and I'm going to give you a
highly novel answer to that, right, which I think is really instructive for students of
Lee theory and students of modern physics. I would have loved that somebody had told me
what I'm going to tell you, shared that with me when I was a first year graduate student. That would
have just like opened my mind to a whole better way of thinking about things. Okay, and a final
question. Well, where are the characters? What do we mean by characters? And where do the
central poly numbers come in? Because after all, we want to connect it with these central
poly numbers. All right, so we're going to do that now. And I'm going to shift to scientific
workplace, roll up the sleeves and sit down with me, have a coffee, and let's dig into the details
here. All right, great. So here is scientific workplace. And we're going to go through some
material. I'm going to repeat a few things that I've already said, and then we're going to strike
out and understand these representations in a really good way. So GL two consists of the
invertible two by two matrices, ABCD, just invertible meaning the determinant is non zero,
there's the identity matrix, and it's a multiplicative group under matrix multiplication.
SL two is a subgroup of GL two consisting of those matrices whose determinant is one.
And we want to discover some other representations of these groups. In other words,
realizations as matrices. And the most trivial one we've already talked about pi zero, where we
send a general matrix ABCD, say in GL two to the matrix, one by one matrix one.
The two by two matrix comes about while one way of thinking about it is that it's really
corresponding to the action of two by two matrices on a vector vector xy. So if you multiply the
column vector xy by ABCD, you get ax plus by cx plus dy. So this two dimensional representation,
the way group theorists sometimes think about this is that it's a case of acting on a two
dimensional space in this case, space of column vectors. Well, in this case, the action is just
giving us the matrix back. So pi one of ABCD is ABCD. Fine. But the beauty of this approach is that
we can think about x y as being linear polynomials on some two dimensional space. Okay, in terms of
poly numbers, alpha and beta the standard poly numbers in the plane. And then what we do is we
think about extending this action to a an action on homogeneous polynomials of a given degree. So we
have an action on linear polynomials x and y. But now we want to bump that up to an action
of the same group on first quadratic polynomials and then cubic polynomials and so on. And it's
exactly that procedure that turns out to give all the irreducible representations for SL2.
So to illustrate that, let's look at the story on quadratic polynomials. So we're talking about
homogeneous quadratic polynomials in the plane. So they are spanned by x squared x y and y squared.
So there are three such quadratic polys in a basis. And the way our group is going to act on
this is going to send this vector to essentially sort of the analog of the what happened to x y.
So x was sent to a x plus b y y is sent to C x plus d y, we're just going to replicate those and
replace x squared x y and y squared with the corresponding expressions in these two quantities
A x plus b y and C x plus d y. So here we have A x plus b y squared on the top. Here's A x plus b y
times C x plus d y in the middle and C x plus d y squared on the bottom. Okay, we expand that out.
We get this. And then we see that the elements here are just linear combinations of x squared x y
and y squared. So for example, this top entry is a squared times x squared plus two AB times x y
plus b squared y squared. That means we can write this as a product of a three by three matrix
times the original column vector x squared x y y squared. Okay, so that illustrates how the three
by three matrix comes about. It comes about because we're acting on quadratic polynomials in the plane
rather than the linear polynomials that we sort of started out with. So we get a three dimensional
representation. We're calling it pi two. You can think of the two as sort of corresponding to the
quadratic degree of the polynomials that we're looking at. And so pi two of A B C D is this three
by three matrix whose entries are all quadratic in the elements A B C and D. And then the exercise
I've already given you to check that this is really a homomorphism that it does really satisfy
this homomorphism property. Now there's a kind of an interesting variant that I'll just mention
as an aside that some of you might like to just pursue. If we chose instead the binomial basis,
just made a slight change of basis by replacing the x y with two x y. That's not unreasonable also
because that's also rather natural basis. So then go through the same procedure and show that the
three by three matrix that you get looks a lot like this one, except that, well,
things are sort of reversed. Instead of having two coefficients up here and here and one coefficients
here and here, the things are reversed. Okay, so let's go one further now to the next higher degree
cubic polynomials. Now we have the basis consisting of x cubed x squared y x y squared and y cubed.
And then we do the same thing, we just replace the x with ax plus by and the y with cx plus dy
and expand all four terms out. Everything is homogeneous, it's all degree three in total.
So each of the elements can be written as a combination of the original
monomials x cubed x squared y x y squared y cubed. And when we write it out as a product
of the matrix, we get this four by four matrix. So this thing here, this top term here, for example,
a cubed x cubed, three a squared b times x squared y three a b squared times x y squared,
and b cubed times y cubed. And the interior ones are a little bit more complicated. So this gives
us then a four dimensional representation that pi three of a b c d equals this this matrix here.
And if you think about it, the homomorphism property is more or less sort of pretty evident
because of the the way that we're acting on on these polynomials, just an extension of the linear
action on the on the plane to corresponding action on a homogeneous polynomial space.
Okay, so now we have four examples. Pi zero was that one there, pi one was just a defining
representation. And then here is pi two acting on quadratic polynomials. And now we've just calculated
pi three acting on cubic polynomials. And the natural question is, how do we how do we extend
this? When we know how to extend it in some sense, we know that the next step will just be that we do
the corresponding thing with quartic polynomials. But can we predict beforehand sort of what the
matrix is that we're going to get, instead of having to make those substitutions and then all
that big expansion? Okay, that's that's the question. And so now I'm going to do something,
which those of you who are members of the wild egg maths channel will have seen me do quite a few
times before it's one of my favorite sort of linear algebraic techniques. It's to perform an LU
factorization, where we try to express a given square matrix as a product of a lower triangular
matrix and an upper triangular matrix. This is a very powerful linear algebraic tool. And it's
also a rational tool, which does not involve any irrationalities, like some other tools do.
So let's let's do that. Okay. Oh, the trace, by the way, the traces of these matrices are also
very interesting. We'll talk a little bit more at the end. But you might, those of you want to
explore, you might see if you can figure out what the traces of these matrices are traces being
the sum of the diagonal elements. Okay, so let us do this LU factorization. So in scientific
work place, I get to do that pretty easily, I go to compute here, matrices, and then PLU decomposition.
Okay, it also gives me a permutation matrix. In general, it's the identity, but not always. So,
okay, but the identity doesn't make any effect. So it's just the product of this lower triangular,
with this upper triangular. So at the first glance, it doesn't look that appealing. So let's
have a look at these two individual components, the lower triangular matrix first. All right.
First thing I'm going to do is I'm going to simplify this. Oh, that was good. So this term here,
which looked a little bit complicated, turns out to be simple. It's just two C over A. Here was C
over A. Here was C over A squared. So this lower triangular piece is very easy and pleasant.
What about this upper triangular one? It's definitely more imposing looking. So let's
have a look at that. So let us first of all simplify this thing.
Okay, that's that's first of all a lot simpler. But crucially, we see that this AD minus BC term
appears in three of the places here, here, and here. So if now, if our matrix G belongs to SL2,
then this simplifies more. Okay, so up till now, we were just taking an arbitrary
invertible matrix and the element of GL2. But now we see the the real possible advantages
in restricting ourselves to the subgroup, the special linear subgroup, because then AD minus
BC equals one. So since AD minus BC equals one, that's the defining property, we get
that this thing here becomes, well, we just get to eliminate these, these things. So that
becomes a one, this thing disappears, and this one squared also disappears. And now we get a
matrix that looks, well, on the face of it, pretty much like this one, it's a triangular matrix,
except it's a little bit more complicated because the diagonal elements are not,
are not one. So I'm going to do something further here, I'm going to decompose this a
little bit further, I want to extract the diagonal part. Okay, so what's the diagonal part?
Let me just make these things zero, delete them, replace them with zero.
And replace this thing with zero. Okay, so that's the diagonal part. So if I take this thing here,
okay, and I sort of multiply by the inverse of the diagonal part momentarily,
I'm going to get this thing. And that tells me that this thing here that I got can now be
rewritten as, because if you look at this thing here, it's the same as this matrix here, I can
now rewrite, bring this to the other side, I can write it as the product of, whoop, undo that,
sorry, copy that, the product of this diagonal matrix with this strictly upper triangular matrix
with ones on the diagonal, a nil potent matrix. Okay, so what have we done? So we have now,
we'll fast forward here a little bit, here is a theorem. If a is not equal to zero, okay,
there's a's in the denominators here, right? So if a is equal to zero, then this is not
going to be valid. But if a is not equal to zero, then with the condition that ad minus bc equals
one, in other words, if we're in the special linear group case, then we can write this thing here,
in terms of an LU factorization, here is the L, and here is the U. We're at simplified things a
little bit by replacing c over a, which appeared, that was, I remind you, that was the term that
appeared here in the lower part, there was a c over a in all three places. So if we call that z,
then I just get one z z squared two z over there, that's the lower part. And then the upper part,
if I replace w, say is b over a, then I get rid of these b over a's, and they get some
simplified thing, we get the diagonal part a squared one one over a squared times this
upper triangular part. Okay, so I claim this is a quite a pleasant story, and it allows us to
see a pattern. Okay, because if we look at this matrix here, we see pretty well something that's
very close to my heart, which is a Harriet Pascal matrix, slightly modified. So if we just look at
the coefficients, we have one one one one two ones, like a binomial sort of expansion matrix, Pascal
matrix, if you like, but I like to call it Harriet Pascal matrix. So this is very close to this Harriet
Pascal story, which seems to be appearing wherever we look. Okay, a lot of my investigations. But in
any case, and so over here, it's sort of something similar, but sort of flipped over on the other
side. So let's have a look to see if the same kind of thing happens with the four dimensional
representation when we're acting on cubic polynomials. So I'm not going to go through the details, but
I strongly urge those of you who are interested to get out your favorite algebraic program. And
maybe you can even do it by hand, actually, it's not impossible at all. And to check that the
following is true, that if we have this condition AD minus BC equals one, and maybe I should just
illustrate that. Okay, let me just do the LU decomposition matrices. Okay, there it is there.
I'm going to get rid of this thing here. That's just the identity. So these are pretty much more
onerous now. So no problem. So let's do that. And let me simplify this. Okay, you see simplification,
we get one, one, one, one, two, one, one, three, three, one, the coefficients here are definitely
Harriet Pascal matrix. Okay, what about this upper triangular thing? Here, this big thing looks more
onerous. Put it here. Okay, you can't see it all neither can I, but I'm going to simplify this thing.
Isn't that a lovely thing? Okay, the simplification again, you see these AD minus BC factors.
So if I just get rid of them by setting, so this is assuming that AD minus BC equals one. So now we
we're supposing that we are in the special linear group here, and all these little terms disappear.
What happened to the 2b? Oh, yes, because I haven't yet divided by the the a cubed a. So if I take
then this a cubed a one over a one over a cubed factor out as I did before, then we end up with
the story that looks like this. Okay, we get pretty well exactly the same thing as I did
previously. This this representation matrix is in a product of a lower triangular,
which is pretty predictable. It's four by four, basically,
one of our Harriet Pascal matrices. And over here, the same kind of story, we've got this
diagonal part, and then sort of a reverse Harriet Pascal thing up here. And the point is that now
we are in a position to to to generalize this, we can see what the pattern is. This LU decomposition
has cleared the air has revealed the underlying structure. Okay, now there's quite a lot more
to be said here about this is connects with a famous decomposition called the NA n bar decomposition
simply groups, etc. But let me just just show you what the the quartic polynomial story looks
like. So there's there's the thing there. And there's the the five by five matrix that appears
there. And you can check that a similar kind of thing is happening as here. And so the the four
dimensional the pi four representation looks like this. And so just to wrap it up now, I want to
bring this now full circle to see where it is that these central poly numbers figure. So let's
deal with this biggest one. Okay, let's this five by five matrix, let's call this matrix p of a b cd.
So this is pi sub four of them, the two by two matrix a b cd. Okay, so we define this thing here.
Okay. And now, I'm going to switch to SU two. So we haven't talked about SU two very much,
but SU two is another subgroup of, well, it's a subgroup of SL two, but a subgroup of SL two,
where the coefficients as I mentioned before, are complex numbers. And the the diagonals in SU
two, so SU two is really like a three sphere. And the diagonal sort of correspond to a circle.
So they correspond to matrices of the form a 00 a inverse, where a is a unit complex number. So
it looks a lot like the unit circle. Okay, so in this case here, we can evaluate what the corresponding
matrix would look like if we just looked at these more special diagonal matrices. So we plug in
a b cd is a 00 a inverse, and we get this very pleasant diagonal matrix with entries a to the
fourth a squared one, a to the minus two, and a to the minus four. And its trace, which is just the
sum of the diagonal elements is exactly our central integral point number C sub four. And that's how
the characters of the representations of SU two or indeed SL two connect directly
with these central integral point numbers C sub n. And that's why the algebra of these things is
particularly important, because this is this whole story that I'm showing you here is really just the
tip of the whole Lee, simple Lie algebra, simple Lie group iceberg. And so there's, you know,
massive generalizations of what's going on here. But this is sort of the simplest and core example
of, well, but we'll see next time is really the vile character formula, which in my view is
really the most important formula in all of 20th century mathematics. I hope you'll join me for
that. I'm the Elmer Lauburger. Thanks for listening.
