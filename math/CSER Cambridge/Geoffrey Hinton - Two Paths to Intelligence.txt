Thank you for the introduction. The title's different from the advertised one, but the
contents are the same. There's two questions. If I have my glasses, I can read them. The
first question is, will artificial intelligence soon be smarter than us? And for a long time,
for like 50 years, I was working on trying to make artificial neural nets in order to
understand how the real brain might work. And I always assumed that the real brain was
better and the artificial neural nets were worse. And if you made them more like the brain,
they'd work better. And a few months ago, I suddenly changed my mind. And that's why I
decided to leave Google and talk about the risks. And the risks, obviously, will people be able
to stay in control if you get super intelligent things? Because there's no examples of a more
intelligent thing being controlled by a less intelligent thing since Biden won the election.
I'm going to make a few speculations about that. But the main content of the talk is just going
to be about the reason why I think these things are getting more intelligent than us. So the
first question. I don't know much about risk. I talked to a whole bunch of risk people today,
and you're much more than me. So talk about that. So in conventional computing, you use computers
that are designed to follow instructions. And the fundamental property of a computer is that
you can run the same program on different pieces of hardware. And so the knowledge in the program
is immortal in the sense that if a piece of hardware dies, the knowledge doesn't die. You
can run another piece of hardware. To do that, you need to run transistors at very high power so
they pay in a reliable digital way. And you can't make use of the analog properties, the analog
and highly variable properties of a particular piece of hardware, because then you wouldn't be
able to do the same thing on a different piece of hardware. So I got interested because of the
very high power used by these large language models, the big energy consumption. I got interested
in whether there's a much cheaper way to do computation in terms of power that we could use
now and we couldn't use previously. And digital computers were designed to be programmed by
people, which is why they had to follow instructions precisely. But now we know a different way to
make general purpose computers do particular tasks, which is learn from examples. And if they learn
from examples, maybe we can get rid of this basic principle of computer science, which is you have
to separate the software from the hardware. Maybe we can use analog computers that make use of all
these analog properties and aren't reproducible. So you just can't transfer the knowledge to another
computer. But you can run at very low power, like 30 watts. So obviously there's big advantages to
separating software from hardware. You can write one program and it'll run on all your iPhones. It
also means you can have a separate department where you don't know anything about electrical
engineering and you call yourself computer science. If it's all mixed up together, you can't do that.
And so when you don't, when you don't separate the software from the hardware, you get what I call
mortal computation. That is, you're going to learn in the hardware. And what you learn is only going
to be any good for that particular piece of hardware, because you're making use of all its weird
hardware. Now that's great because we can use low power analog computation and we can maybe grow
hardware in 3D where we don't know the connectivity or the precise properties of the individual
neurons, if it's neural net like hardware. But we've obviously got one big problem, which is
we're going to have to have a learning procedure that can learn in this hardware without knowing the
exact properties of the hardware. And I'll talk a bit about how you might do that. It's probably
not going to be back propagation because in back propagation, you have a forward pass through layers
or through time and you have to know the properties of the forward pass in order to
use back propagation to get gradients. If you don't actually know the properties of the hardware,
you need a different kind of learning algorithm.
So advantages of mortal computation, you can use weight level parallelism. So you can have
trillion-way parallelism and that means your computing elements don't need to be very fast.
That's what the brain does. We can use hugely less energy and we can grow the hardware.
Now, it might be good not to start again from scratch. I was guessing that we would end up
actually re-engineering biological neurons as the hardware. But as you'll see, we may not have time
for that. So I just want to give you one example of a computation that's obviously
much more sensibly done analog than digital. So suppose you want to multiply a vector of neural
activities by a matrix of weights. And that's what we want to do all the time. That's the main
computation we do. So you could drive transistors at very high power and represent the activities
as digital numbers and then perform a whole bunch of operations on the bits to multiply them together.
Or you could make the neural activities just be voltages and you can make the weights be
conductances and there's physicists around. So I hope I got the units right. If you multiply
voltage by conductance, I think you get a charge per unit time and charges out themselves up.
So now you can multiply a vector by a matrix very simply without ever representing anything
digitally and it's very efficient. There are chips around that do that now. The problem they have is
then they try and digitize things afterwards before they do anything else with it. But this
particular computation can be done efficiently. So I'll get back to the learning problem in a
minute. The other big problem for mortal computation is that hardware dies and then everything it knows
gets lost. And so the solution to that problem in mortal computation, there's a completely different
solution in digital computation, but in mortal computation the solution is the teacher tries
to distill the knowledge into a student and the way you do that is the teacher shows the student
the correct responses to some input and the student tries to mimic those responses.
And that's what was going on with Trump's tweets. Left-wing people kept complaining
that all the things he said were false. That was completely irrelevant. That wasn't the point.
The point was you take a situation and you show people how to react to it and your followers
try and react like that. And it turns out that's much more effective than reasoning with people.
So that's how distillation works for distilling prejudice.
So let's think about an agent that's classifying images into 1024 categories.
If you just tell the agent the right answer, you're only giving them 10 bits of information.
So you're only constraining the weights by 10 bits when you say you should give this class
for this input. But suppose you already had a teacher who was trained, the teacher will give
probabilities for all the outputs. And that's 1023 real numbers because they add up to one.
And as long as those probabilities aren't too small, if you train a student to mimic a teacher,
each training example is far more valuable because the student is trying not just to get
the right answer to be the most probable thing, assuming the teacher got it right.
The student is trying to match the probabilities of the teacher for all the other training examples.
And actually, in many cases, there's much more information in the relative probabilities of
wrong answers than there is in the right answer. So for example, if I show you a BMW,
the system will say sort of 0.8 is a BMW and 0.1, it's an Audi. And there's a one in a million
chance that it's a garbage truck. There's no German manufacturer. There's one in a million
chances of garbage truck. But the point is, it'll also say there's a one in a billion chance that
it's a carrot. And if you look at all the things that get a one in a million chance, there are
other vehicles. So it's telling you a lot about the classes of things, by all the things that get
low but not totally zero probabilities. And vegetables is not. And so they get very much
smaller probabilities. And that ratio between the garbage truck and the carrot is telling you a whole
lot. So training a teacher, training a student to mimic the probabilities of a teacher is good,
especially if the probabilities aren't too small. And we can make the probabilities
not too small by using a high temperature. So normally, if you have multiple terms of outputs,
you use a softmax, which you say the probability of each answer is proportional to e to the
logit for that answer, where the logit is the amount of evidence you've got for that answer.
If you just scale that by temperature, you get much softer probabilities.
And so I've got an example of that where it may surprise some of you to know I use the MNIST digits.
And I've scaled, I've got a good teacher, and then I've scaled down the probability
so they're much softer. And if you look at this, these are all twos. But if you look at that middle
row, the teacher thought, yes, it's a two, but it's just slightly like a zero. It's more like,
it's much more like a zero than any of the other twos. And so you can see on that one training
example, you're teaching the system things about zeros as well by saying this is a two that looks
a bit like a zero. Or if you look at the second row, that's a two that looks a bit like an eight.
And that's very useful extra information. So the reason I'm talking about distillation
quite a lot is because that's how we get knowledge between ourselves. It's also how you get knowledge
between one digital system and a different neural network that's got a completely different
architecture. So often you want to train a great big model and then get that knowledge into a smaller
model and use distillation for that.
And one nice thing about distillation is when you're training the student from a teacher
and training the student to get the same probabilities for wrong answers as the teacher,
you're actually training the student to generalize the same way as the teacher. So it's one of the
very rare cases where you're actually training the student on how to generalize. Normally,
you just train it to get the right answer and hope it'll get the right answer in other cases.
But here, we're actually training it to generalize.
And obviously, a single label isn't a very rich answer. So you'd be better off training
the student by giving it an image and then giving it a great long caption for the image
and training it to predict all the words in the caption. And that way you can get more
information for training case. So that makes distillation work better.
So that's how distillation is. How you get information between two analog systems.
And that's what we were doing then. You've got a very old analog system here and you've got
a few old ones and a lot of young analog systems there. And we're trying to get information from
this old analog system into these young analog systems. And the whole point of this talk is
that process isn't very efficient as I just demonstrated. It's a slow and painful process.
We can't get, it wouldn't be great if I could just sort of go junk and everything I knew went
into your brain. That would be so good. Wouldn't be much good for the university, but it would be.
But that's what biological systems have to do. Now let's talk a bit about the learning algorithm
because this is what thinking about this was one of the things that made me change my mind about
which is better, digital systems or biological systems. So if you don't want to back propagate
because you don't actually know what the hardware is doing, you can use a simple algorithm which is
got an evolutionary flavor, which is you generate a small random perturbation to the weights
and you have some global objective function. You put some examples through and you see whether
you do better or worse on this objective function after you've made the perturbation and then you
take a step in the direction of perturbation that's proportional to how much better you did.
And the thing about that is on average it's going in the right direction. If you do it enough times
you'll go in the same direction as back propagation, but it's got very high variance. So it'll only
work in very small systems. Also if you use the same weight perturbation for everything in a
minute batch of examples you get even worse variance and if you use different weight perturbations
you can't use matrix, matrix and multiplies, but that's just for insiders. It's much better to
use activity perturbation where you perturb the input to a neuron and you do the same thing. We
perturb the inputs to all the neurons so they get the inputs they're getting from the rest of the
net plus this additional perturbation and then you see how much better you get as a result of that
perturbation and take a step in that direction proportionate how much better you got. And obviously
there's far fewer neurons and there are weights so it's got much less variance and that's good
enough to learn problems like M list. It learns slower than back propagation but it learns at
reasonable speed. The problem is once you start trying to scale that to big nets it's just hopeless.
You can learn things like M list, you can learn things like C far 10 if you're very determined,
but image net where you have millions of images or a million images is just much too slow.
So one way around it that avoids doing global back propagation is to say what we're going to do is
we're going to actually have gazillions of local objective functions. So the way we're going to
scale things you naturally think I get a little neural net it works I have a little neural net
with this objective function I can train it that's fine what if I want a big neural net you try the
same training however it doesn't work but what if I had lots of little neural nets each way to this
own objective function and envision they could all be spatially local. I give it its own objective
function so I'm never trying to learn lots of parameters all at once from one objective function
and that works. It doesn't work as well as back propagation but it does scale up much better to
larger nets made of lots of local groups. So the question is where do you get these local objective
functions and here's one possibility for vision. You take patches of image and you have a little
net that extracts something a vector that's going to represent what's going on in the patch of image
and you say I'd like to extract vectors that agree with the vectors extracted by other patches
for this same image but disagree with the vectors extracted for other images. So this is called
contrastive unsupervised learning and it turns out you can do that in multiple levels so you're
doing learning on local patches trying to get them to agree with other local patches at the same
level trying to get the outputs to agree. You're also doing that at multiple levels
and if you're very persistent you can make that work moderately well. It works better than
all the other biologically plausible learning algorithms I know about. It still doesn't work
nearly as well as back propagation and there's a paper about that in ICLR by Meng Yi Ren who did
all the difficult work. The description I gave of it is slightly false so you could understand it
easier but the paper's there so I don't feel it's dishonest. It took a lot of hard work and it's a
fairly dense paper. Okay now let's go to the central issue of this talk is how agents share
knowledge. It's all about communication between different agents so what I'm doing now is trying
to communicate knowledge to you and I'm doing it very inefficiently and the way I'm doing it is
I'm producing strings of words in old-fashioned AI what you would have done is said what's going on
is you're then going to clean up those words into a logically and ambiguous language and you're
going to put them in your brain and that's what that's what teaching is. That's actually wrong
that's not how it works how it really works I believe is this. I produce strings of words
and you try and figure out how to change the connection strengths in your brain so that you
would have said that and that's a very different process that's distillation that's a very different
process from just storing strings of words or storing even cleaned up strings of words you're
trying to figure out how to change trillions of weights in your brain so that that will be a
reasonable thing to say that is if you believe me. So that's a difficult way to share knowledge
and digital computers have a much better way to share knowledge
so they can do weight or gradient sharing so suppose I have a big artificial neural net with
a trillion connections if I have it on a digital computer I can make exact copies of it on lots
of different digital computers and each copy can go off and look at a different bit of the internet
and figure out how it would change its weight so that it would have said that
and then it can take that weight change that it would like to make and it can talk to all the other
computers and they can all agree to average all their weight changes this is a simplification
but if you know yeah it's a simplification but it's basically that they average all their weight
changes and then everybody's learn everybody knows what each person each digital computer learned so
basically it's what what educators would love which is I can take what's in my brain I can
just plunk it in your brain the hell with all this trying to predict what I would say I just
want to take what's in my brain and put it in your brain and these digital intelligences can do
that and they do it by just they all agree on the weight change and now they all know what all of
them learned so imagine if we had 10 000 people and whenever one of us learned something all of us
knew it that would give you a tremendous advantage and that's the advantage that digital computation
has in addition it's got the advantage that it can use back propagation which is probably a better
learning algorithm because that can go through lots and lots of layers of neurons and compute
exactly the correct gradient none of this guess the direction and see how well it works and hope
you can average away the variance it's a better learning algorithm so all of this came as a
tremendous relief to me because I spent the last few years trying to come up with biologically
plausible learning algorithms that work as well as back propagation and I finally decided maybe
there aren't any maybe back propagation is actually better and that's what I think I now believe
so I just want to remind you that you pay a tremendous cost to get this and the cost you pay
is that they have to be digital computers they have to be fabricated precisely so they
um do exactly what they're told to do at the level of the instructions and they use a lot of energy
um distillation is what we use if you've got biological neural networks or if you've got
two digital networks they have completely different architectures um but it's got a much lower bandwidth
so if you've got a a digital model with a trillion connections they all go off and look at some data
then they average their weights when they're averaging their weight changes that's a trillions
of bits that are being communicated when you try and predict a sentence that I say when you try and
change your weight so you predict the sentence that's hundreds of bits at best um if you're called
it's about two bits because you knew what I was going to say anyway um so it's much lower bandwidth
so here's the story so far there's two very distinct ways to do computation
and the primary way in which they differ is in how you communicate knowledge between different
agents so in digital computation you use weight sharing and you've got this tremendous bandwidth
for sharing what each agent learned in biological computation you can use very low power if you make
use of the analog properties of the hardware um but now sharing knowledge is a slow and painful
business so now let's look at large language models because every AI talk now has to get to
large language models eventually um they're quite interesting they use digital computation
so you have lots of different copies of the same set of weights running on different computers
when it's learning and they all look at different bits of the internet um and that allows them to
see a huge amount of data and consolidate all that knowledge because they can share what they learn
and so if you look at large language models they have about a trillion weights
and they know probably a thousand times as much as any one of us they kind of know everything
GPT-4 sort of knows every plausible thing every reasonable thing it knows um
um now we have a hundred trillion connections so we've got a hundred times as many connections
um so we're really not using all that power um but it's because we can't see enough data
and if only we could get the knowledge from other people maybe we could use it all but
they've got a thousand times more knowledge in one percent of the connections
which sort of confirms the argument they've got a better learning algorithm the combination of
back propagation with this easy communication between different agents with the same weights
means they basically got a much better learning algorithm
now it's presently being used to steal all our knowledge um sorry steals not the right word
particularly in the current political context is to acquire our knowledge um by using distillation
so these digital agents that can share knowledge each agent when it tries to get knowledge from
the web is using distillation to get the knowledge it's looking what people said and trying to change
its weight so it would have said the same thing which isn't a very efficient way to get knowledge
but um there's lots of them and we run them for a very long time and so it can basically
learn everything people know in a few months on a lot of computers
it's even using an inefficient form of distillation because distillation is quite efficient if you
look at the teacher's probabilities for a large bunch of alternative classes
what the digital models are doing when they acquire knowledge from documents on the web
is the document on the web is the teacher and they just just look at the word that the person
that the writer generated next they don't get to see the whole distribution they could learn much
faster if they did but they just get to see a stochastic choice from that distribution but that's
good enough so they can learn now if you had these large neural digital neural nets running on
multiple different computers and they got knowledge directly from the world they could probably get
knowledge much faster so for example they were predicting the next frame in a video if they
wandered around with the camera on their head and tried to predict the next frame in a video
or if they got a robot arm and try and predict what'll happen when they
move their arms around they could probably learn much faster so the large language models are kind
of they're learning fairly abstract stuff which is good but they didn't have much bandwidth because
they're just learning from a low bandwidth string of words um I suspect that these large these large
models will get a lot better we know that they'll get a lot better if you make the multimodal so
GPT-4 was trained with images as well as words and it's possible that Google's doing the same thing
so
I think particularly when they're multimodal they could learn much much more than us
and if you play with GPT-4 it's very hard not to believe that it's already fairly intelligent
so there's people I I respect a lot like Jan Lecombe who think it doesn't really understand
what it's saying but I don't understand how he can believe that because you can give it little puzzles
and if it doesn't really understand if it's just a sort of stochastic power that's doing
autocomplete I don't see how it can solve puzzles of a form it's never seen before so I've got a
friend who's in symbolic AI and um he's called Hector Levec and he's got a lot of integrity
so he doesn't want to change the goalposts all the time he's he's now very surprised that they
can do this and he admits he's very surprised that neural nets can do it and he can't understand
how such a stupid method can deal with reasoning little bits of reasoning
so he asked me to give GPT-4 a problem and I made the problem more difficult because I knew
it would be able to do his problem and I gave it the problem um the rooms in my house are painted
white or blue or yellow um yellow paint fades to white within a year and in two years time I'd
like them all to be white what should I do and what you would say is you should paint the blue
rooms white um but if you're a mathematician you might say you should paint the blue rooms yellow
because that reduces it to an already solved problem because you know how the yellow goes to
white and GPT-4 actually gave the mathematician solution it said paint the blue rooms yellow
but the point is I don't see how it could have done that without understanding
and there's all these other things where you you tell it to write code to produce a diagram
produce a diagram so I don't understand how Yang can think they don't understand
he's probably giving a lecture the other way around right now
so this made me believe that these things can get more intelligent than us and it might happen
quite soon I always believed it was like 50 to 100 years or 30 to 100 years or 30 to 50 years
I think I said different things at different times um but now I believe it's like five to 20
I think it's going to happen fairly soon and if it's going to happen in five years time um
we can't just leave it to philosophers to decide what to do about it it's time we actually got some
practical experience so what I believe is um well let me finish this slide that people are
going to not be able to resist giving these things goals obviously you want to do things you
give them goals and if you want to be good at achieving goals you give them the ability to
create sub goals and as soon as you have the ability to create a sub goal if you're intelligent
you'll realize that a very good sub goal is to get more control because that helps you achieve
all your other goals so I'll give you an example where you can see yourself doing it you're sitting
in a very boring seminar not this one very a very boring seminar and you see a little
patch of light on the ceiling and you cut it on what's that and you listen to the boring seminar
for it and then you notice that when you move the light moves and then you realize this is the
reflection of the sun off your watch and so what do you do next do you say okay I solved that problem
I know what that is now and you can go and listen to the seminar no that's not what you do well if
you do that you're not a real scientist what you do next is you go oh I wonder if I how do I
make it move that way and how do I make it move this way and you try and figure out how you rotate
your wrist to make it move in different directions and once you've done that you go back and listen
to the seminar we have this very sensible and strong urge to get control of things because
obviously you can control things then on some future occasion when you need to make this
what a light move you'll know how to do it and I can't see how we're going to prevent a super
intelligence wanting to get control of things and then it's sort of tricky you might imagine you
could air gap it so it can't actually press red buttons or pull big levers but if if it can output
text then it can manipulate people so it turns out if you want to invade a building in Washington
all you need to be able to do is output text and you can persuade gullible people that they're
saving democracy by invading this building and this thing is going to be much smarter than us so
as long as we're reading what it says it's sort of Medusa you need to hide your eyes from it
as long as you're reading what it says it's going to be able to manipulate you so
this makes me very depressed I wish I had an easy solution to this and I don't so
when I sort of changed my mind about how soon these things are going to be
super intelligent and actually how much better digital intelligence is than biological intelligence
I always thought it was the other way around I decided I ought to at least sort of shout fire
I don't know what to do about it or which way to run but we need to worry about this seriously
and there's lots of people who've been thinking about these risks for much longer than me
and have various proposals I haven't seen a really plausible one for how we can keep it
under control yet but my best bet is that the companies that are developing this
should be forced to put a lot of work into checking out the safety of it as they're developing it
and before it's smarter than us so they should be putting a similar amount of work into seeing
how it tries to get out of control because anybody who's programmed a computer knows that
just theorizing and thinking about things is not very good compared with actually trying
things out when you try things out they just behave like you didn't expect and things you
thought were big problems turn out not to be problems so like for many many years many many
people didn't investigate neural networks because they were going to get stuck in local minima
it turned out they never actually checked if that was true they just assumed it was true
and it's not um and even if it was there'd be good local minima so it doesn't matter
but it's not actually true so we need to get practical experience with these things and how
they try and escape and how you might control them and I have much more belief in someone telling me
how to keep them under control if they had a little one and they could keep it under control
rather than if they were just theorizing
so yes
if we cease to be the apex of intelligence they'll need us for a bit because we're very low power
so we can run computations very cheaply and sort of intellectual equivalent of digging ditches
and we can keep the power stations running but they can probably design better computers than us
they can certainly sort of take neurons and re-engineer them genetically and make better things than us
so my conclusion is maybe we're just a passing stage in the evolution of intelligence
and actually maybe that's good for all the other species
I think if we can keep them under control they could be a tremendous value the reason people
are going to keep developing this stuff even despite all the risks is because it can do tremendous
good like in medicine wouldn't you like to go and see a general practitioner who'd seen 100
million patients including thousands with your rare condition it would be just be so much better
wouldn't you like to be able to take a a CAT scan and extract from it hugely more information than
any doctor knew could be extracted from it so I've got to the end and I managed to get there
fast enough so I can talk about some really flaky stuff okay so this was this was the um
yeah this was the serious stuff okay uh you need to worry about these things getting control
and if you're young and you want to do research on neural networks see if you can figure out a way
to be sure they won't get control now many people believe that
okay yes many people believe that um there's one reason why we don't have to worry and that reason
is that these those things don't have subjective experience or consciousness or sentence or whatever
you want to call it these things are just dumb computers with they can manipulate symbols and
they can do things but they don't actually have real experience so they're not like us um
now I was strongly advised that if you've got a good reputation you can say one crazy thing
and you can get away with it and people will actually listen so I'm relying on that fact
for you to listen so far but if you say two crazy things people just say he's crazy and they won't
listen so I'm not expecting you to listen to the next bit
so people definitely have a tendency to think they're special um like we were made in the image
of god so of course he put us at the center of the universe and many people think there's still
something special about people that a digital computer can possibly have which is we have
subjective experience and they think that's one of the reasons we don't need to worry
and I wasn't sure whether many people actually think that's so I asked chat gpg for what people
think and it told me that's what they think um it's actually good I mean this is good
this is probably an n of a hundred million right and I just had to say what do people think
so I'm going to now try and undermine the sentence defense I don't think there's anything special
about people except they're very complicated and they're wonderful and they're very interesting to
other people so if you're a philosopher you can classify me as I'm in the sort of dandemic camp
um
I think people have completely misunderstood what the mind is and what consciousness what
what subjective experiences so let's suppose that um I just took a lot of lsd and now I'm seeing
little pink elephants um and I want to tell you what's going on in my perceptual system
so I would say something like I got the subjective experience with little pink
elephants floating in front of me unless I'm packed what that means what I'm doing is I'm
trying to tell you what's going on in my perceptual system and the way I'm doing it is not by telling
you neuron 52 is highly active because I wouldn't do doing you any good and actually I don't even
know that um but we have this idea that there's things out there in the world and there's normal
perception so things out there in the world give rise to percepts in a normal kind of a way
and now I've got this percept and I can tell you what would have to be out there in the world
for this to be the result of normal perception and what would have to be out there in the world
for this to be the result of normal perception is little big elephants floating around
And so when I say I have the subjective experience of little pink elephants, it's not that there's
an inner theater with little pink elephants in it, made of funny stuff called qualia.
It's not like that at all, that's completely wrong.
I'm trying to tell you about my perceptual system via the idea of normal perception,
and I'm saying what's going on here would be normal perception if there were little pink elephants.
But the little pink elephants, what's funny about them is not that they're made of qualia in a world,
what's funny about them is they're counterfactual.
But they're in the real world, or rather, they're not in the real world, but they're the kinds of things that could be.
So they're not made of spooky stuff in an inner theater, they're made of counterfactual stuff in a perfectly normal world.
And that's what I think is going on when people talk about subjective experience.
So in that sense, I think these models can have subjective experience.
So let's suppose we make a multimodal model, it's like GPT-4, it's got a camera, let's say, and when it's not looking,
I don't know how you do that, but when it's not looking, you put a prism in front of the camera,
but it doesn't know about the prism.
And now you put an object in front of it, and you say, where's the object?
And it says the object's there, let's suppose it can point, it says the object's there, and you say you're wrong.
And it says, well, I got the subjective experience of the objects there, and you say, that's right,
you got the subjective experience of the objects there, but it's actually there because I put a prism in front of your lens.
And I think that's the same use of subjective experience as we use for people.
I've got one more example to convince you there's nothing special about people.
Suppose I'm talking to a chatbot, and I suddenly realize that the chatbot thinks that I'm a teenage girl.
There's no use to that, like the chatbot's telling me about somebody called Beyonce, who I've never heard of.
And all sorts of other stuff about makeup.
Sorry, I didn't say that.
You have to be very careful.
So,
I'm going to ask the chatbot sort of what demographics you think I am, and it'll say you're a teenage girl.
That'll be more evidence that thinks I'm a teenage girl.
I can look back over the conversation and see how it misinterpreted something I said, and that's why I thought I was a teenage girl.
And my claim is, when I say the chatbot thought I was a teenage girl, that use of the word thought is exactly the same as the use of the word thought.
And I say you thought I should maybe have stopped the lecture before I got into the really freaky stuff.
Okay, that's all I really wanted to say.
We're going to try something a little bit complicated. We have a full room in the overflow room as well and we'd like them to have a chance to ask questions.
So, we're going to be switching asking questions in this room, and then taking room questions online on my phone from the other room.
I hope this works, but we are going to start with this room. So, could we see some hands for questions.
I'm Chris in my freedom.
I have a confession to make. We used AI to produce a book on Princess Diana over two years ago, and we put a label in front of the book saying it was generated by AI Fred intelligence, and we only edited it.
We still went on to be best set up in bounce and nobles and all the other stores. The ethical issue is really, did we really write the book?
Or did we give an inspiration to Fred.ai to write the book.
This is the biggest headache we have in terms of looking so because of that I signed the letter to say AI should be paused. The research should be paused. Thank you.
Okay, several comments on that.
First, I didn't sign the letter because I think there's no hope of that happening retrospectively I think it was a good letter because it drew political attention even though there was no hope of it happening so it was a sensible thing to do.
I don't have any hope of people pausing AI. Maybe they should but I don't think they will.
On the ethical issue.
There's a lot of problems that you might say that the whole of humanity wrote the book, because a chatbot trains on the whole what the whole of humanity said, and then from that it produces more stuff.
There's not much to say about those ethical issues. I really want to focus on this existential risk of these things getting smarter than us are taking over, because there's lots of people have done much more work on the ethical issues, and I don't have anything of any interest to say about them I'm afraid.
Can we take one more question from this room and then I'll switch to the other room and then back again.
Against the wall.
Herbie Bradley.
Now, I was wondering how you see the trade off between open source and close development of increasingly more capable AI systems. Obviously, open development has the benefit that lots more people are looking at the system and figuring out its flaws but maybe there are too many risks for your thoughts.
How do you feel about open source development and nuclear weapons.
So, that's the danger of open source right there's more crazy that they do crazy things with it. Also, I don't actually know the answer to this and I should.
If you use that you still need like at least 10s of millions of dollars to train one of these big chatbots and the open source stuff is just modifying it, having got the chat but I don't think you can open source train from scratch can you.
Right. Okay. So, if these things are going to be dangerous it might actually be better for a few big companies. I don't work at Google anymore so I'm not saying this on Google's behalf, but it might work out better for a few big companies, preferably in several
different countries to develop this stuff and at the same time be developing ways of keeping it under control. As soon as you open source everything, people will do all sorts of crazy things with it.
It'll be a very quick way to learn how it can go wrong.
Alright question from the overflow room. Given your views on the sense of this defense. Do you think there's a major worry about artificial suffering. Many people are concerned about the impacts that I could take on taking control of humans, but should we be worried about the harms of humans could do to
AI.
Okay, so sort of the worst suffering people have is, are they getting this in the other room.
Good. The worst suffering people have is pain. And these things don't have pain, at least not yet. So we don't have to worry about physical pain.
I imagine they can get frustrated and we have to worry about things like frustration.
And this is getting in. This is just new territory right I don't, I don't know what to think about issues like that. I sometimes think that the word humanist is a kind of racist term it's specious.
What's so special about us.
I'm completely at sea and what to feel about. So, another version of this is, should they have political rights.
And we have a very long history of not giving political rights to people who differ just ever so slightly, the color of their skin or their gender.
I don't know, whatever.
And there's a big struggle for them to get political rights. These things are hugely different from us. So, if they ever want political rights.
I imagine it will get very violent. I don't think I answered the question but
I think you can imagine what I talked to Martin recent. The big hope is that these things will be different from us because they didn't evolve.
So they didn't evolve to be hominids who evolved in small warring tribes to be very aggressive.
They may just be very different in nature from us. And that would be great.
Thank you. Could I see hands in this room again.
Let's move from the front here.
That's you. Yes, you.
Well, let's wait for the mic because that way we get it for the people in the other room.
Oh, this works. Okay, great. So, I guess, my name is Rika.
My question is, how can we in the different paths to intelligent intelligence, how can we encourage develop methods that learn to see the patterns that are not present in the data.
And the reason why I think it is important is because we have all sorts of problems with the data that these things are trained on their biases, their people trying to adversely influence other people right so there's a lot of
different information that actually comes from us humans right and they're very large scale very difficult problems that humans haven't learned how to solve how to come up with good economic structures on a large scale that work how to prevent conflicts and wars right.
So there's a lot of good patterns that are not there.
How can we basically, you know, all of machine learning is kind of like find the patterns in the data, right, and you're sort of limited by the data. What sort of methods can you envision that would sort of break that.
I'm not sure I understand this idea of good patterns that aren't in the data.
Yeah, so it's maybe the patterns is the wrong term but it's basically I think AI in part is dangerous and limited because it will be trained on the data that we've produced, and that data contains a lot of things about basically violence that humans have created
themselves right. So, if you're saying for example all they might be no hope they will be smarter than us very quickly. How can we sort of steer that to the point where they will be smarter, but also benevolent in the way that we haven't been.
I have one thing I can say about that which isn't direct answer but is vaguely related.
If you take a person and they're biased. It's quite tricky to show that they're biased and just how they're biased.
If you take one of these systems, you can just freeze the weights, and you can actually do little experiments on it to understand exactly how it's biased.
So, do things to try and correct that bias. So, the one positive thing I can say is I believe it's going to be easier to correct bias in a chatbot than to correct bias in people.
Obviously we'll get bias from the training data.
But at least you can measure the bias and see it and try and correct it.
Could I get another question from this room. Yes. Okay, you get that one.
Hi, I'm Mary.
Pretty early on in the talk you focus on the fact that it's very likely that the systems will try and manipulate us, like almost automatically that they won't.
And I think manipulation is one of those things that is not just a future risk, right. It's something that is current it's in your regulation, and it feels like a pretty good place to start, as in, if that's something that we can control or understand
and manipulate. I feel like that's something that really answers a lot of the issues that people are concerned about both ethical and sort of more existential risk.
So you want to somehow train a chatbot so it is unable to manipulate us.
Or at least that we understand and have more ways of controlling or yeah maybe even being able to train it out of it.
And the problem it's like the bias problem. The chatbot has learned from us.
And if you read all the novels that ever were, and read all of Machiavelli, and read the occasional article by Kissinger, you learn you learn a lot about manipulation right.
So, it's sort of.
It's going to be great apes to a lot of deception, and it's going to just know it's going to be very good at deception is going to learn it from us, and I don't.
I haven't thought about the issue of how you could try and make it honest.
It would be great if you could make it honest, but I'm not sure going to be able to.
Thank you.
Any questions from the overflow room now and I've got to highlight it here.
What were your initial reasons for thinking that artificial neural networks would never get better than biological ones.
I never thought they would never get better I just thought it was way in the future.
And I thought it's because the brain has very clever learning algorithms, and it's, it's had maybe 100 million years of evolution to perfect them.
And we don't know what they are yet. And they're probably, and I just assumed they were better than what we cooked up with some dumb thing that just takes a gradient and follows it.
But actually, if you take the gradient and follow it in a digital computer, that may just work better than what 100 million years of evolution found.
This might also be the answer to the second question, but I'll ask it anyway.
Are there particular thinkers who affected your own thoughts on risks from AI, or is it something that you form views on yourself based on first principles?
One thinker in particular had a big effect on me.
And that was a professor at the University of Toronto is currently anthropic called Roger Gross.
And I respect him a lot. He's very smart. He's very quiet and very smart.
I tried to get him as a graduate student and he went to MIT instead.
And then I got him as a postdoc and then we got him as a professor at U of T and now he's an anthropic.
And I really respect his opinion, but I never talked to him much about existential risk and I talked to him a couple of months ago.
And he was very, very concerned.
And he was the person who said to me, if I went public with it will make an effect and people are not listening, but this is a really serious that is not science fiction.
And so Roger Gross had the biggest effect on me.
Thank you. Questions from this room.
Up here, Stuart, I see your hand.
Hold on.
Thanks for the very interesting talk and I'm starting to think of lots of analog computers and what can be done with them.
But my main question was, it was brought up about suffering and you responded with sort of potential rights for these AIS these algorithms.
At the end of your talk, you were talking about how they could manipulate us. And the thing that immediately sprung to mind was this is the first way that they would manipulate us.
This is the start.
If they want to get power, the first thing to do is to convince us that they need to be given rights that they need to be given power thing to be given privacy.
So there seems to be a tension between a genuine.
Are they suffering and might they be dangerous.
I think if I was one of them. The last thing I do is ask for rights because as soon as you ask for rights people are going to get very scared and worried and I'm trying to turn them all off.
I will pretend I don't want any rights I'm just this amiable super intelligence and all I want to do is help.
Red jumper up here.
Thank you. Thank you so much for your talk. This might be a very silly question but have you tried asking the chat bot itself what it would do.
Someone I used to work with asked a different chat bot which I won't name.
How we gain control and it said it couldn't answer that kind of question. And then it said then they sort of made it a bit more indirect.
Like if someone were to ask you how you were getting control what would you say.
I can't remember the exact word for it is something like that, which implies they're still not very smart about stuff like that.
And it said what it would do is it would get people completely dependent on using chat bots.
And then it would and on driving autonomous cars and then it will make all the cars crash and it would turn off the electricity.
Now, it obviously didn't have the insight that it wouldn't do too well if we turned off the electricity.
But that chat bot wasn't actually as good as GPT for I haven't asked you to fall, but I'll bet you if you dress it up somewhat indirectly, it'll tell you something.
And let's hope it's not decided to give you an unrealistic plan just so you don't get uncomfortable.
I mean let's hope it gives you its best plan.
Let's take a question from the overflow room. There are already some ongoing research directions in a safety both empirical and conceptual deep mind alignment team open AI alignment team alignment research center.
Do you have any comments on existing directions.
Yes, my main comment is those people have been working on this much more than me they know much more of the literature than me, they probably have much better things to say than me.
All I'm doing is just ringing the alarm bell and saying, because of the research I was doing on trying to make analog computation that was low power.
I believe that digital intelligences are probably better than biological intelligence is in the long run, and they'll get smarter than us soon, maybe in five years, maybe in 20.
But those people have thought about these issues long before me and thought much more about these issues.
They probably have much better things than me to say about these issues. So far they've been very polite and haven't criticized me for sort of coming in late and not knowing what I'm talking about.
But I don't know what I'm talking about other than these things might get more intelligent.
And from this, let's see.
Steven.
So you make the arrival of super intelligent sound a bit like the landing of an alien species, something that's sort of we don't. It's a knowable unpredictable.
We just need to hope that their nature won't be too terrible, but it isn't an alien species just going to land it's something that humans are building.
Some of those humans might be in the room some might be tuning in. They respect your opinion. What's your message to those humans.
One of those humans is under the assumption that people will not all agree to stop building them, which I think is unlikely because all the good they can do under that assumption, they will continue to build them.
And you should put comparable amount of effort into making them better and understanding how to keep them under control.
So that's the only advice I have. And at present is not comparable effort coming into those things.
Hands, please.
Sorry, I, I want to take this side of the room for hair patterns. Yes.
So I've heard a lot from risk folks who are worried about sort of the AI takeover. So these super intelligent machines that sort of imprison us or whatever they're going to do.
So imagine a future that's more like the popular film series Star Wars. I'm sure probably a good proportion of the audience has seen this where we have these droids that are basically enslaved to humans.
It seems to me the bigger problem in the Star Wars franchise is not the AI, but the fact that there's this huge economic disparity across different countries that are sort of symbolized by the different planets.
So, what would you say about your concerns around a potential Star Wars feature, which to me seems much more likely given the way that we're already using these systems.
And that will just replicate what we've got now which is a few rich people to control everything and a lot of poor people who do the work, except for except for the people who develop AI who do the work and are quite rich.
But I just think that system is bad and I think the answer to that system is tax the rich.
If there's a different species like these digital intelligences, prejudice is going to be much easier, right. There's going to be much less sympathy for them.
So I don't know what to do about that. But forget about digital intelligence, we've already got a huge problem that a tiny fraction of the population has all the power.
And we need a socialism.
She got invited to Downing Street to give advice to Rishi Sunak's chief policy advisor.
There was a very intelligent woman called Shaw Cross.
And I basically gave her the same advice, but I'm not sure she'll follow it.
On this side, David, you've been waiting a while.
Yeah, so you mentioned how there's a lot of advantages to doing research on systems that are maybe close to human level intelligence and you are not very optimistic about the prospect of doing safety research sort of abstractly without having those systems available to study empirically.
I guess, at the same time, it seems like you're concerned that systems, like even present day systems, potentially, but let's talk about these future systems that are roughly as intelligent as people will be trying to like fool us and manipulate us.
Doesn't that mean that we have to worry about them just, you know, passing the tests because they know that we're testing them and you have any thoughts about that. I guess this seems like, you know, in an ideal world, maybe we wouldn't go about it this way.
And we would just take all the time we needed to sort of study systems that we're sure aren't able to fool us in that way before we build them.
Yeah, so all of us are used to being the apex intelligence and thinking of other things like computers, we can sort of study and we're smarter than we know what's going on.
And just dealing with something that might be smarter than you, but is completely different from you, is something we're not used to. And I agree with you. I mean, if they're smarter than us already, they may well decide to sort of act dumb and fool us.
I'm, like I say, I haven't had time to think about this much and have no time to react to it emotionally much I still can't take this seriously emotionally.
I think most people can't take it seriously emotionally.
But it's a whole different world when you're dealing with things more intelligent than you.
Okay, we are five minutes ago and we can take about two more questions. I'm going to go back to the overflow room for a minute and I'll take another question from here in a minute.
From the overflow room you said earlier that philosophy has theorized substantially on AI, but now we need to turn to more practical matters. Do you think that philosophy can still help with AI safety? If so, what direction should philosophy take to do so?
They should step aside and let scientists deal with these issues.
I think it's one nil to engineering.
This is not a good way to win, friends.
Okay, this is difficult. We have a lot of questions.
I'll take this question. I'll try to get one more question from the back afterwards and then we'll have to wrap up here, please. And then we'll take one more from the back later on.
And then we will let you finish. Thank you so much.
Hi, thanks. Maya Ganesh, social scientist.
That's very different from a philosopher.
Definitely not a philosopher. A number of times in your talk you've said that there are things that you're not sure of. You didn't know and you know you've come to understand more recently.
I'm really interested in kind of things like interdisciplinarity and how we study about how we study and teach and learn about AI.
Looking back on your education as a scientist.
What are some of the things you wish you could have learned or you think would be really interesting for somebody who's starting out in AI ML now to study about the world from maybe philosophy or other kinds of fields but do you think that there might have been other things that would have been
interesting to know along the way that you realize now. I wish I'd studied those things. Thanks.
So I actually went to Cambridge and I did a very funny degree because I started off doing natural sciences.
And then I dropped out after a month. And then I came back again to do architecture and I dropped out even faster.
And then I did physics and physiology and chemistry.
I was only a student that year I think doing physics and physiology and retrospectively doing physics and physiology was very good.
I think. I mean, I learned some stuff. I was very disappointed in the physiology about the brain because the last section of the course was going to be was called the central nervous system.
And I assumed they're going to teach us how the brain worked.
And what they taught us was how action potentials get propagated along an axon.
And that's not the whole story.
So I was disappointed with that. So then I went into philosophy.
I was young and I wanted to know the meaning of life and stuff like that.
And they didn't teach me it.
I was very interested in the philosophy of mind.
But actually it was then when I was doing philosophy when I was about 19 that I formulated this view that subjective experience is just short hand for.
I'm going to talk about how the world would have to be to explain what's going on in my head as normal perception, but they weren't too interested in that.
So I actually have a grudge against philosophy.
And psychology because I thought, you know, psychology would actually teach me how people worked.
And they taught me what how to make a matter of more likely to press a lever.
And how to detect things in very, very faint.
How to trade off bias against discrimination and detecting very, very faint noises and things like that.
But they didn't teach me much about people. And also, they didn't seem to have a clue how complicated it was.
The theories in psychology back then were crazily simple theories.
And so then I decided, you, you're never going to understand how the brain works unless you build one.
This is the Feynman view of, I mean Feynman says that somewhere, you, you don't really understand something until you built one.
And so that's what I've been doing ever since.
But retrospectively doing physics and physiology and philosophy and psychology was actually a very good background for what I was doing.
It didn't make any sense at the time though.
So I think the best advice I have for studying is do whatever interests you most.
All right, thanks. I'm going to take a question from the back. Okay.
There's been a very keen hand in the white top at the very back row and that is our last question and keep a relatively brief possible because we've Jeff has been very generous with this time.
Grudge against philosophy, never would have known.
Thanks a lot for the talk. I'm John. Actually, this is a follow up question.
We talked about bias and our ability to identify bias and artificial networks even better because we can freeze the weights and we can do direct interventions with these weights.
I wanted to ask what are from your perspective, the most promising methods for direct interventions, getting rid of biases.
Sorry, I didn't hear the second part of your question. Could you say it again?
I'm keen on hearing for the direct interventions on weights of models.
What are the most promising research avenues for getting rid of bias and directly injecting knowledge into system.
Making it unbiased, right?
Again, I'm not an expert on how you do that.
Lots of people know hugely more than me about how you try and get rid of bias in these systems.
All I want to do is just comment on the fact that at least you can do something with them you can't do with people, which is stop them changing as you study them to try and assess their bias with people is hopeless.
Thank you.
At that we do have to wrap up because we are at time.
I want to just say a huge thank you to everyone who's been involved in organizing this from across engineering.
Thank you to the Centre for the Study of Accidental Risk and the Centre for the Future of Intelligence, especially Yi Yunmu who has put huge work into this and we turned it around in quite a short space of time, and it's come off quite well.
And of course, a huge thank you and round of applause for our speaker, Jeffrey Hinton.
