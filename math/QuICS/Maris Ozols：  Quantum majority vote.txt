We have to dismiss that notification.
Okay, yeah, fantastic. So, so pleasure to do my football is visiting for a couple of days.
So we work together when you're about to be at the University of Waterloo and continue to work together with years and great collaboration.
You know, Mars has done a lot of nice work in, you know, quantum algorithms, but also other areas of, you know, quantum information processing.
He's currently a faculty member at the University of Amsterdam.
He's visiting here for a couple of days and there are key spots open in this schedule. So if you're interested in meeting with campus here, you know, please get in touch.
I'm glad to find the time for you. But without further ado, we're hearing about quantum majority vote.
So yeah, it's, it's nice to be here. It's been a long time since I was here last time.
So I will talk about the quantum majority vote. It's joint work with all these people. And in fact, it's because it's a special opportunity.
It's, you know, by one you get one free so I will talk about this as well, because these are kind of closely related.
I will first talk about the first one and then afterwards the second one is kind of like a generalization of the person.
Okay, so, yes. So to motivate the problem, I will start with a manifesto and it's sort of like, well, anyways, it's like to explain the kind of the main idea.
And get this out of the way.
Yeah, so anyway, so basically, I don't know if I need to preach this to you but you know you are studying quantum computing information.
And so what I'm you know my claim is all information is quantum and that's this kind of obvious because the universe is quantum so you know that everything is quantum basically, but this also means there is no classical information.
So, so it's a myth that there's classical information, and it's a mathematical truth actually that there's no classical information because I can use this to show around so you know the quantum states they form the sphere but the classical states are just two points in the sphere so it's a measure
zero substance which is basically does not exist.
So if people study algorithms what they should really be studying is is CPTP maps or quantum channels completely positive trace preserving maps.
So that's a map from a sphere to a sphere. And, you know, because classical computer science studies only maps from bit strings bit strings or like from single bit to single bit if it's a very simple problem.
You know it's a measure zero subset of all possible maps. And because of that classical computer science is dead.
So instead of, you know, maybe this is too extreme for you.
It's just saying that everything is quantum there's no classical thing. But, you know, in practice we, well, in everyday life we interact with classical information.
So, so this is the water down manifesto so let's say you are interested in purely classical problems you have classical input and classical output.
You have some bit string basically. And, you know, for such problems, it turns out, you know, we know because of Peter sure, if you want to build a device that solves purely classical problems sometimes inside a device you need to do something
And, you know, actually what is happening inside device there's somewhere at the beginning the classical information gets converted from state.
And then there are many steps of processing inside a device and those steps they have a quantum status input and quantum status out.
And so it means that, you know, the algorithmic consists of actually quantum subgroups.
And so, you know, even if you are interested in purely classical problems that has nothing to do with one mechanics.
If you want to solve those problems very well, you should care about fully quantum subgroups basically, because you know, the best way to solve a problem in general will be using quantum algorithm, but quantum algorithm is just a sequence of sort of like
fully quantum steps to have a quantum input and bottom out, but and indeed we have already many examples such as such on some of the things we have a transform we have a great creation spot test and so on.
And quantum majority vote is is sort of like a new primitive that also could be used.
So that's kind of the philosophy so basically, you know, even if you if you don't subscribe to the full manifesto of everything's quantum, just from this purely practical, you know, motivation of making quantum algorithms.
It's nice to have more building blocks because then we can make more quantum models.
Okay, so let's let me explain the problem so the one majority world problem.
Okay, so let's start with a classical majority vote so that's very obvious so it's just you have a bit string of zeros and ones.
And you just want to find the majority base here there are more zeros or the majority zero.
There are so many applications so if you, if you are, I don't know, I'm making randomized algorithms, for example, any randomized algorithm use majority vote at the end basically run the algorithm and then it might produce the wrong answer with some
probability that you just take majority, and you can amplify the success probability from some something bounded from away from half to arbitrarily close to one.
So, you know, has an actual interpretation as an error correction primitive.
So, you know, let's say you have a bit you want to protect it against errors you could make several copies of this bit, and then you send it through some channel and error happens and then at the end to decode you just take majority vote.
And again, it's sort of like a, that's the obvious thing to do for for cleaning up errors and it's a very natural application.
So like in this picture you can think maybe there's some kind of experiment is supposed to produce a single bit. And every time you press the button it's it produces something but it's a, you know, sometimes it was supposed to produce zero but sometimes produces one.
And then this taking majority vote just kind of gets rid of these just these errors and gives you the most likely correct.
Okay, and of course it's also used in some democracies but not in all democracies.
So, so what is one majority vote, it's the obvious generalization to this. So basically we have instead of zeros and ones, we have quantum states.
And so throughout this talk, or at least the first half of the talk I will talk only about qubits.
So basically we have a single qubits, and we can fix some basis like psi and psi perp. And we don't know what this basis is, and we are given a bit stream of these unknown states.
And we have to figure out what is the majority.
Here they're more size so we should output side.
And, you know, just to emphasize this once again, this is a inherently quantum problem. There is, you know, there's a quantum input and there's quantum.
So basically, you know, my goal is, you know, as I said with the manifesto like, you know, I want to build a useful quantum primitive, and I'm not trying to compete with with classical algorithms because classical computer science is dead.
And, you know, there's also there's no meaningful way in which we competing against classical algorithms because the problem is quantum itself.
So, so why is this problem is, you know, we can't just count how many size and how many side perps we have and sort of like, you know, it's not sort of not obvious what to do.
And, you know, if you want to extract some kind of information about what to say, yes, you could measure, but you don't know in what basis to measure because the size and purpose unknown.
And in fact, you know, you kind of you don't want to measure in some sense because if you measure you're going to disturb the state but you know you want to output a quantum states or somehow you could measure something but it should not disturb the state and you don't want to kind of measure
it away should not basically it kind of encodes a direction in space in some sense that you don't want to kind of lose that information.
Okay, so, so here are some simple observations about this problem so it's, it's not too difficult to show that there is no perfect algorithm for this.
Okay, so again, you know, we're not competing against the classical case there is a perfect algorithm in a cluster case you just count how many zeros how many ones and then you'll put the majority.
And this is a different problem it is more more more difficult and it turns out there is no perfect.
You can it would contradict you.
However, there is a trigger algorithm.
You can just, you know, you have this string of states, no size or purpose, just pick one of them at random and just output that. So if there are more size and
there's a good enough probability you just succeed.
So that's it, you know, we're done, I can, I can go home so there's a problem.
Okay, but but so basically we will use this as a kind of a benchmark so we will, you know, we get since there is no classical version we're not going to try to get the quantum speed up doesn't make sense.
So we will try to get a speed up over this trivial algorithm basically let's kind of we do something, some kind of a more, you know, involved processing that does better that somehow globally acts on all these states instead of just picking one of them
Okay, and so you know the the success probability of the trivial algorithm is is just the number of the majority states a number of sites in this case, divided by a number of qubits.
And that's kind of the baseline that we're comparing against.
And by the way, if there are any questions any points, you know, just feel free to interrupt.
All right, so that's that's the problem.
So let me write away, give away the main results, we know how well we can do.
You know, as I said, we cannot solve this problem perfectly importantly, so we interested what is the optimal output fidelity so that you know how close can we produce a stage to do to the optimal step.
And we consider two cases there is a no promise case where basically the number of science I perps are not in any way limited.
So in particular you could be so like this, you are very close to the middle of means you have roughly high, half of the states to be psi and roughly half to be cyber.
So basically, it's going to be a difficult case to find out the majority state. And the promise case is when you promise the number of sites is what you're kind of the way.
These are you have very many sides or very few sides.
In this case, basically there is, there is.
So this promise case will be easier because there will be sort of a bigger difference between the majority and minority.
We're interested in a worse case. So basically, in a no promise case more space is when you're really right next to the half. So let's say and it's even and it's all because if I was given that it's not clear what the majority is.
So let's say and so that basically these two points are the worst case, and I'm here when you are kind of on the edge.
Okay, so the trivial algorithm is basically, you know, if you just pick one state in front of them.
The August penalty will be very close to half, and you will just be there by one.
And just like the size of this gap and basically.
And in the other case, you are promised that like a constant fraction like 506 of the states are the majority states.
And in that case, you know, that's, that's your, that's in the worst case you will have a federal 506.
Okay, so what we show is that the optimal algorithm for this problem has fidelity that goes like this. So basically, I mean, you know, it's not, you know, it's still going to be close to half and that's kind of clear because
half of the states are not decided.
It's going to be a problem. But now here, basically, we are doing a little bit better.
A little bit more than life on the house.
And in a promise case, actually, what you can see is that and is it gets big. And actually this probability that is the stability goes.
Somehow, if you know, basically, in a promise case, you can have more lots of states then then you can actually solve the problem pretty, pretty well.
So this is sort of like a summary of the main results. I don't know is there any question.
Excuse me.
Yep.
This is Nicole younger helper and via zoom. Do you also have any constructions for how you would achieve the optimal results.
So I just sort of gave away the result but I will explain what the algorithm is that that achieves this fidelity.
And I will explain also a little bit about the analysis like how do you pull up now because he we have a type of presentation.
Thanks.
Okay, any other questions.
Great. Okay, so so so so yeah that's the result. But now let's let's see how do we show this right so this this involves representation theory so now I will have like sort of interlude and representation theory.
I don't know if I mean I just think maybe you maybe already know if not then you can learn something new.
Okay, so, so.
All right, so basically so let's let's just consider the following problem so let's say we have some arbitrary matrix and this matrix like the unit three but it doesn't matter it can even be simple.
This is not like the matrix, and you have two copies of this matrix. And, you know, let's say you act on the zero zero state.
So then you're just going to get the first column of this matrix.
And, you know, but the interesting thing is if you can expand this, this first column of this vector as a linear combination like this like if you just write it in a bracket notation.
And, you know, basically it's a combination of these, and I should be pointing here that the online people can see. So it's a linear combination of the hamming weight zero states and hamming weight one state like a superposition or having weight one states, and
hamming weight two states.
Okay, so that's one observation and the other observation is that I mean basically this the state lives in a symmetric subspace so it's just a linear combination of these states that are themselves symmetric.
Okay, so the, and the other observation if you take the singlet state which is on this metric so if you exchange the two qubits you get a minus sign.
Then if you act with two copies of this, of this two by two matrix, you will just get the state back. So it's basically it's an eigenvector.
And, you know, for any two by two matrix if you tensor it twice the single state is an eigenvector and has eigenvalue that's the determinant of this matrix.
Okay, so that that's these are two interesting observations.
But now we can put these observations together, and we can make what's called a short transform on two qubits.
So this is a four by four matrix where we basically we take this singlet state so this on this metric state as a as the first as the first row of the matrix, and then the last three rows are just these.
These are the zero zero states and this is the hamming weight one state uniforms the proposition of having it one, and this is the hamming weight to the one one state.
Right, so now if you make this kind of matrix that just encodes a singlet state and then an or to normal basis for this metric subspace.
And if you do a base change on any two by two matrix, two copies of that matrix.
What you will get is a block matrix that looks like this you have the determinant here, and then you have some kind of a bit more funky three by three block here.
So basically the first block comes from this singlet from the unsymmetric subspace and this three by three block comes from this metric subspace.
And so not only that but also if you take a permutation, there's the only permutation for two systems is just to exchange them.
Then this permutation diagonal ones so basically for the unsymmetric subspace you have eigenvalue minus one, and for this metric subspace you have eigenvalue plus one.
Okay, so what is interesting about these two blocks is that they are homomorphisms for matrix multiplication.
So basically, you know, so this determinant, you know, we know all that determinant is a homomorphism if you take a product of two matrices and you compute determinant.
So I'm just calling this block, any of these blocks and calling it q, you know determine has this this property, but also interestingly this three by three block.
It also has this property. So this is not obvious at all but like this three by three block if you just plug into arbitrary matrices, you can always check this whole.
Okay. All right so basically this is the simplest example of what's what's called true transform.
And you know what you transform does it basically block diagonalize like tensor powers of an arbitrary matrix and it also block diagonalizes permutations.
All right, so now to kind of explain a bit more detail.
I want to show you the most beautiful equation in mathematics or maybe somebody knows the most beautiful equation.
Right, so I assume that people are thinking of this equation, but this is not the most beautiful equation. This is the second most beautiful equation.
Okay, so the most beautiful question is this one, and maybe the people at the back cannot see it. Okay, so, so what is this equation so this equation.
It basically max two by two matrix completely arbitrary two by two matrix was, you can put some symbolic variables, and it outputs a big matrix that's like L plus one so there's like, I don't know if you can see there's so it size L plus one basically.
So these are basically the JK entries. And so this big formula is the JK entry of this of this matrix.
And you know, and this formula, it looks, you know, maybe not very beautiful, but it's, it's very simple you can just write it down in Mathematica and you press enter and it spits out matrix.
And they're just some coefficients with some square roots and factorials and then here there's a monomial that has different powers of ABCD. So the ABCD are the entries of the two by two matrix.
And this is just like basically every entry of the output matrix is just, you know, basically some polynomial of these ABCD.
Okay, so why is this the most beautiful equation mathematics is that this map that it's just an explicit formula for a map which which is a homomorphism under matrix multiplication or this is called a representation basically.
You know, it just maps two by two matrices to matrices of size like L plus one times L plus one. And you know, and you can basically take this two by two matrix and you can blow it up to any size you want.
And the resulting matrix will always be compatible with matrix multiplication in this sense.
And you know, it doesn't matter if this matrix could be, it doesn't have to be unitary can be this arbitrary symbolic entries.
You know, not only that if this is a irreducible representation so it's not the direct sum of representations.
Like one way you could do this is just to like take two copies like a direct sum of your original matrix is itself and that's like a boring way to make a bigger matrix.
That's compatible with matrix multiplication but this is irreducible you kind of break it into blocks and in fact you can show that there any reduced representation of two by two matrix is is basically equivalent to this up to some basis.
All right, so and physics is called Wigner D matrix.
There's a Wikipedia article about this if you want to look it up.
Okay, so now you know this is here's an example of the output of this formula, you know okay the formula was maybe scary but this output is very nice so if you have a one by one matrix then it just outputs the number one and that's that's you know if you multiply one with itself it's going to be a
sum for two by two matrices you just get the matrix back back for three by three you get this one that I showed you before, and then you know you get these bigger and bigger matrices and they have some kind of nice pattern and some kind of combinatorial structure.
And you know I think this is a very good.
Okay, so now.
So, so there is another way to look at this.
It's it's closely related to this metric subspace.
So let's let's let's do the following.
So let's just take. So we have L, L qubits. Let's just take uniform superposition over all standard basis vectors to have some hamming weight w, we just fix, you know, L the number of qubits we fix w.
And then we take uniform superposition for these hamming weight w states and then we normalize.
And so these are called symmetrical with the key states of having the w.
And, you know, another way to describe this matrix I showed you before is basically the entries of that matrix that I showed you.
They are just equal to taking L copies of your two by two matrix, and then expressing it into this metric subspace.
So this metric subspace has dimension L plus one because hamming weight was from zero to L.
And so, you know, you can show that this this basically this is another way to get this this kind of the most beautiful formula mathematics.
Okay, so basically, you know this all all this this QL this this representation is doing is just expressing L copies of the matrix in symmetric subspace and like this M tensor L is of course symmetric under permutations and so so it's kind of natural that
it should have, you know, some kind of a nice action is much space.
Okay.
And, you know, using this you can make actually a quite explicit sure basis for humans. So, you know, like, I mean, it's, it's, it's, it's sort of not so obvious how to make this sure transform in general so I gave you an example on two qubits.
Like for qubits there's actually a pretty explicit construction, and it works as follows so first you take. So we will have these different blocks, and these blocks are parametrized by lambda.
So lambda is a partition of and meaning that it's just, it's just the two numbers that some to end and they are no negative it's like you take a chocolate bar with n squares and you know cut at the middle.
Not in the middle but in two pieces and one is a lens lambda one and the other one is lambda two.
So we're going to define these states that have three components so this is it's like a state on n qubits and we have these parameters.
So the first parameter will be lambda and the second will be humming wait and the third one we just keep zero for now.
And what this is is just we take one of these, the symmetric states I showed you before so has humming wait w and then you know this number of qubits.
And then on the remaining qubits we just put a bunch of single states so this this I minus is just the unsymmetric to state.
And you know what you can show and this is actually it immediately follows from what I said before, if you take n copies now so so now and is going to be bigger than we before we had just held that was kind of smaller.
So if you have n copies of your matrix and you express it sort of in this basis of the states.
You end up because you have this symmetric state you will just get, you know, a matrix of this of this queue like matrix element of this queue.
And because you have these singlets you have lambda two singlets. So for every singlet is an, you know, it's an, if you act with the same matrix and boss qubits of the singlet you get a determinant of the value.
You just get some power of determinant basically which is equal to how many singlets.
And so, and so this basically gives you okay so this is not complete description of the of the sure base for n qubits, but you okay you need to have this other index here, kind of you can extend it in a certain way but I'm not going to talk about that.
So basically, so these states that you obtain that will be you know it will contain these states sometimes some extra states that I did not explain.
They, it ends up forming our normal basis for for n qubits.
And you know anyways and so basically there's a certain number of you know, certain range for w and certain range for I know here explicit formulas but it doesn't matter.
All right, is there any questions about this.
So this is to locate some questions of what what is a sub one minus one, that's like a date on. Yes, yeah, it's on whatever is the subscript. That's how many qubits like lambda one minus two and then w is the handle plate.
Okay.
Wait, sorry. Yep.
Yeah, so yeah because so this state is on lambda one minus lambda two qubits. And this is a singlet state that's on two qubits it's the anti symmetric two qubit state.
So there are two lambda two qubits here, and then there's lambda one minus lambda two qubits here.
Yeah, exactly so then in total there's like, you know, lambda one plus lambda two and that's and that's the total number.
All right. So, you know, and then basically, okay so so here's how the shoe transform is defined more generally. So it basically takes these n qubits, and it's going to so it's going to take this space and decompose it as direct some of these, you know, several blocks.
And within each block you'll have sort of like two registers like a tensor per structure, and these two registers correspond to these these two like this w and I here.
And so this w sort of like it corresponds to the entries of these two by two matrices, and this I has to do something with permutations.
And the shoe transform is basically this this operation so given these states that I defined almost completely before I defined them for I equals zero but anyways.
And then it just maps these states into a direct sum of these tensor parts.
Okay, and so it has nice properties in particular if you have n copies of any two by two matrix. If you apply the shoe transform.
Okay, it's a unit retransform also. So anyway, it will give you, you know, it will give you a block matrix and it has these two registers and in the first register you have one of these these most beautiful matrices that I showed you before, and identity in the other register.
And similarly if you take any permutation pie so the permutation just permutes 10 qubits in some arbitrary way.
And it will have the opposite structure you have the identity in the first register and, and, and some representation of that presentation in the second register.
And you can see that these two matrices commute and that's basically because, you know, if you apply the same operation all qubits, and then permute the qubits that's the same as apply as the same as first permuting the qubits and applying the same operation on all them.
So basically it's kind of obvious that permutations and M tensor and commute.
So basically that's the basis that's kind of reflected by the fact that you have identity here and something here. And here it's the other way around basically.
Okay, and so, you know, from.
So that's kind of mathematically but from the quantum computing perspective is that you can actually implement this and you know there was first was was like our own Harrow's thesis and work by Dave Bacon and John and Harrow, where they show that this can be done in polynomial
and it's just like an explicit paper by Cubian Strauch that have the shows that you can do it in like number of qubits and to the force basically gates.
And they give explicit like a, they even write a program that generates a circuit.
Okay, so that that's that's the shoe transformer and qubits. And so now, you know, Nicole was asking what is the what is the construction so so here is how the algorithm goes.
So you have to use the shoe transform. And you know, again, let me just remind you that, you know, if you have n copies of any matrix, it will block diagonalize into matrix like this, so you'll have some kind of block that depends on your input here, and this will be just an entity.
Okay, so but in a quantity vote, we don't have n copies of the same state we have like, you know, some number of copies of the state and then we have some number of copies of the orthogonal state.
So let's just ignore that let's say we have, you know, lots of states that are the same and then a couple stated are different but you know that it's going to be kind of similar to this.
So let's just kind of pretend that it is actually it looks like this.
Okay, so then what would we do.
Well, okay.
Yeah, well other than just outputting one qubit in front of us. So if we want to make do all these qubits interact somehow we want to entangle them together.
We want to transform and we, you know, we get this this block matrix as output, and then we can measure this lambda so this is a block diagonal matrix, we can just ask in which block are we.
And this is, you know, it doesn't disturb the state. So we kind of get this for free basically. So this is called big sure sampling.
Okay, so once we measure lambda, we kind of end up collapsing on one of these blocks basically just have this cute and the right entity.
And you know this identity is just a massively mixed states up to some normalization. So we don't really care about this it has no information about role.
We can just throw this away basically. So we are just left with this this representation of our state.
And at this point what we have is, is a representation of our state, which the representation itself is an of random dimension, because you know this lambda that we sampled that was, it has some distribution.
Okay, so now we have this representation of the state of a random dimension and we basically want to just we, you know the goal is to produce a qubit as an output that has the majority state.
So we have this this state of a random dimension to just a single qubit.
And, and what we end up doing is as follows, because this this queue lump the role, you know, one way to define it was this this crazy formula.
And the other way was just saying that's the action of you know, certain number of copies of raw in a symmetric subspace.
We can basically apply some isometry to embed this in this metric subspace, and it will be on this number of qubits. And, and then we just, you know, if we would do that, then we will get something that looks like several copies of role like this number of copies of
row. And then we discard all qubits except one and then we should get through back.
So basically like you know if you have any copies of role then this procedure that I described, it should give us rollback.
So that's good because if all the states are the same the majority should be the, you know, the row itself.
But then you can show that you know if, if there are some of the rows are different that they are perpendicular to the original role, then this procedure still gives you the majority state was with fidelity.
And so I'm kind of doing to it well, I mean I basically, you know, it's from what I explained before it's not too hard to see that for raw tensor and this procedure just going to give you raw but, but in general if you have, you know, arbitrary state has some raw
purpose as well then this works to pretty well.
All right so that's that's that was the algorithm are there any questions about the algorithm.
Okay.
All right. So now, so what I'm going to do now I'm going to generalize this problem because like basically this kind of approach can be used to solve more problems and you know not only majority but we can look at other problems.
And then afterwards I will generalize generalized once, once again.
And so the, so the third generalization is basically the following. You know so the original quantum majority world problem was this, we want to find a majority state.
And somehow I think it's useful to think about it also as a, as a computation in an unknown basis like so here we are trying to compute the majority function in an unknown basis that's the science I perp.
Imagine computing some arbitrary function.
So basically we have this scenario we have like some kind of a weak computer that has some data, and it encodes like let's say a bit string it can cause it in zeroes and ones.
And this is a bit of a kind of a crypto setting so maybe, I don't know maybe this has some graphic applications, I just wanted to point out.
Imagine it sends this data to the, to the other big computer on the earth.
But you know, either on purpose, it's maybe trying to hide the input by applying some, some random unit review on every qubit, or maybe it's just you know the atmosphere has some turbulence and you send the photo on polarizing one way.
And when it arrives it's polarized in different ways. Maybe they don't have the same reference. So basically it arrives in some some basis size I perp.
So this computer is able to compute the function so in this case let's say majority function.
Then when you send it back then it you know it traverses the atmosphere the opposite direction and gets you know, the you gets undone, or maybe this this party, you know, was hiding the data on purpose then they can just undo the you because they chose to you.
So you can imagine computing in this kind of fashion not just a majority function but other functions as well.
Because like you're delegating the computation you're trying to hide the data, or you could think that you just maybe because of the misalignment of the reference frames maybe you just don't have the same basis but you still want to somehow delegate the computation to somebody else.
Okay, so then, so so I will call this equivariant computation so basically we are given some arbitrary Boolean function from n bits to one bit.
And you know the sort of the, the obvious way to say what it means to compute this function would be to say that you have a standard basis state on n qubits it has the input string, and you produce one qubit has the output, you know you want to this happen for every x.
If you don't know the basis, if, if you want to compute this in any possible basis like psi psi perp, then you want to implement this transformation for all x and for all you basically.
And so this this mathematics is called equivariance when you basically change the you vary the input and the output varies sort of in the same way.
And so this is, this is a continuous symmetry of the problem. And this is like a, it's a continuous version of discrete symmetry so the discrete symmetry will be just it's called, I will call it equivariance but it's also called self duality.
So it's basically, you know, a Boolean function is self dual. If you negate every bit of the input. It's the same as negating the output.
And the majority function you have like a string of zeros and ones, you know if you replace the zeros by ones, then also in the output you should replace the zeros by ones and that function is yourself do all but there are other functions have this this property.
And so basically, you know, we kind of we want to take a function that has this discrete symmetry the solvability or this equivariance, and we want to extend it to a function that is that has a continuous symmetry basically.
So just a sort of small remark, like these, you know, basically this this equivariance is sort of anyway, it is put some restriction on your function so for example, if you have a function that is equivariant but also symmetric.
So when I say symmetric I mean symmetric under permuting the input bits.
And then, then it from that. So basically that requires them to be on to here's just a small explanation so for example, you know you want to compute your function on a string that has like a humming wait exactly half.
Then you can write this 0011 by negation of 1100.
If your function is equivariant, then you can serve negating the input you can negate the output.
And if your function is also symmetric you can permute these this bit string back into 0011 just exchange the order of the zeros and ones.
And you get a contradiction you get like you know f of something has to be negation of f of that same.
Basically, you know if you have even number of inputs then this equivariance is not compatible with symmetry, but if you have odd number of inputs then then there's no problem.
All right, so so what is the setup so the setup is basically we have an arbitrary function Boolean function from n bits to one bit.
This function is equivariant or because of dual so negating input negates the output.
It is symmetric under permuting the input bits, and this forces them to be odd.
And the task is that we are given you know input string x in an unknown basis every qubit is scrambled up with the same unit tree.
And we want to produce some state that's close to the, the value of the function in the same basis, and this should work for every input and for every, every basis.
And, okay, so we're not going to be able to solve this exactly for the majority we can solve this exactly so we need to have a measure of success and so this, this meaningful measure is the worst case fidelity.
Basically, we, we are going to be optimizing over all quantum channels that's quantum algorithm is a quantum channel.
And we minimizing we look at the worst case over all inputs and all bases, and we just take your our input in, you know, in this unknown basis we apply the channel and then we compare with the right out.
So the channel spits out the density matrix and it takes an intensity matrix in general.
So that's, is there any question about so it's more general problem.
This is Nicole again.
Since you described in the possible motivation for this generalization, how the person performing the computation doesn't know the relevant computational basis or doesn't know something about the data that sounds reminiscent of homomorphic encryption.
But do you know if these results that you've presented have any relationship with algorithms and homomorphic encryption.
Yeah, I don't know so I mean, I just wanted to kind of emphasize this cryptographic flavor, in case somebody who is working on this kind of stuff, you know, wants to have a look at it, but I haven't thought about it myself so I.
I mean, I don't know if this is a useful primitive for some kind of homomorphic computation or not.
Because I mean sort of there's also this question of, you know, when you compute majority vote what is the side information that this computer learns.
So, I mean, in this case, it turns out that there's some like because you perform some kind of measurement so you extract some information we do this week sure sampling.
And you know that gives you basically the ratio of the number of size inside perps. And so there is some information that actually leads to this to this party.
But I don't know what is sort of the trade like.
So yeah, anyway, so even the optimal protocols going to leak some information about the data so that's, you know, that's one problem.
And another problem is I don't know what is the trade off between you know when this, when this guy is trying to cheat like I'm basically in this talk I'm kind of looking at it awkwardly.
I'm just assuming this guy is trying to do the best and sort of not like in particular they're not trying to like measure the state and do some tomography or something, because we want to achieve the best fidelity we don't want to kind of ruin the state.
But yeah, I don't know. Yeah, so anyway the short answer is, I don't know if this is useful for more of a encryption but it has a little bit like flavor like that and I think it's interesting question to investigate.
Okay, thanks.
Thanks for the response actually.
Another question, and some people have been approaching quantum voting from multiple perspectives and one of the perspective that perspective that's gotten a lot of attention is security of voting.
Do you see any ways to make your majority voting protocol secure, or maybe that's an opportunity for future work that you'll talk about at the end.
What is security voting.
Actually, maybe this would be more relevant to say how votes are.
Well, just, I guess the general problem is that when votes are transmitted and processed. We need to ensure that no cheating happens.
Yes.
Yeah, I guess you could like, right. I mean so like, you know, we look at the problem of outputting the majority state and sort of.
Yeah, I don't know. I don't know how you could check that there was not cheating.
Yeah.
Anyway, like, I'm basically, yeah, I, I think these are interesting questions and encourage people to think about them.
Okay.
All right. So yeah so this was, this was a setup for arbitrary function. And so now the question is, you know, what is the algorithm.
And basically the algorithm is essentially the same so this was a previous algorithm so we you know we do all these steps. And basically we just going to modify these last two steps so instead of.
And then we, when we do these steps we end up with this queue lambda pro and then we embedded in symmetric subspace and discard all qubits.
But so we instead of doing that we will basically we need to apply some map from whatever is the dimension of the state to a one qubit, and you know that that map is has to be unitary equivalent.
So it has to kind of commute with unitary and it has one qubit output because that's the output of the function as one bit basically.
And, you know, another question is what which you need to be a current channel should we apply this.
And so here is, here is the, here's the deal. So, basically what you can show is that if you look at these kind of unitary equivalent channels from some dimension.
So here basically what I mean by unitary equivalent is that there is a representation of unitary group acting on the input space.
And, you know, the two by two matrix itself is acting on the output. And what you can show is that there are only two such channels basically and the general channel is a complex combination of these.
And so basically, anyway, so people who know representation theory these two channels correspond to like removing a box from this diagram, either from the first row from the second row.
If you remove from first row then this corresponds to in some sense to partial trace. So this was in the original algorithm basically what I was saying is that you embed the state and symmetric subspace and you throw away all the qubits except except one.
So that that's this partial trace, and the other extremal covariant or unitary variant channel is this, it's called universal not operation so this is basically the problem of you are given like one copy of psi or maybe several copies of
and you want to produce the unique orthogonal state so you are in two dimensions so there's unique internal states I perp, and you just want to have a map that maps a state to the tweets or something.
And this has been studied quite, you know, quite extensively and there's no, you know, there's no exact.
There's no physical operation to do this but you can approximate to a certain extent and this best approximation is this universal.
And so it means that this channel that we need to apply in the final step of the algorithm is just specified by one number so for every lambda we have one number.
And that's just the probability in the convex combination between these two channels.
And so you know if you want to find the best algorithm then for every lambda we need for every partition or every block in the matrix we need to find the best parameters.
And so anyway, and these channels also we show that you can implement them simple.
So, right, so, okay, so let me just then summarize the main results so basically we have the setup when we have a symmetric and a current function and we have both and we want to compute it in an unknown basis.
And we also want to know what's the best fidelity with which we can compute. And so what we show in a paper is that.
So basically that this algorithm is specified like I explained by these parameters is the lambda. So that's for every lambda it's just, you know, one real number between zero and one.
And we show that you can just write down a very small linear program of size that's like n, which is the number of qubits that gives you the optimal choice of these parameters and it also gives you the optimal fidelity so this just have.
In fact, I, I think one can also obtain an explicit solution of this linear program so I haven't had time to work it out but basically we give a linear program but I think when I obtain a closed formal actually.
And, you know, and basically what I showed you before this algorithm that it, it, it implements you can implement this this optimal, optimal.
This is a random channel and and it has complexity that scales like and to the force. And so this complexity comes from the shoe transform basically the most complicated step in the algorithm is sure transform and the rest is, is a little straightforward.
Okay, so, so that's that was the sort of this first paper and the majority vote.
Anyway, so you know we can compute so there's a table here all the feralities of all the, all the seven bit functions.
Actually, maybe I can say for so for people who are interested in query complexity I think what is interesting is that sort of there's some parallels responding very complexity so basically.
So we look at, you know, the very complexity of of symmetric functions has been pretty well understood.
It depends only on certain parameters so if you make the truth table of your function so this this function is symmetric.
So the truth table just depends only on a hamming weight basically of the input and so, and there is certain parameter which is like, you know if you start from the middle of this truth table and you go to one side and see how many times you have a constant like zero.
So the whiz of this interval basically specifies fully the query complexity.
And somehow the same thing happens also is the optimal fidelity in our setting is it turns out is just determined by this so basically the street that I drove through here.
It's you kind of if you start reading this truth table from the middle you see like 001.
So you go like 001. So basically for this function the query complexity it's sorry the optimal fidelity is this number.
So what happens further down and true stable doesn't matter. So now there's something we observe numerically and I think but we haven't proved this rigorously but so.
So it seems like there is some kind of similarity this this one very complexity and I think that's an interesting question to explore.
All right, so there are any questions about this.
I want more generalization.
All right, so okay so you know you can ask even more general question.
You know, so I'm, you know they said I'm interested in quantum algorithms with quantum input and quantum output.
So you can say well you know what is a quantum algorithm it's a quantum channel. It has like you know n qubits is input and m q bits is output in a more general phase.
You know you want to find the best algorithm so you want to optimize of these channels.
And you know that's a completely positive trace measuring map.
And let's assume that we are in the setting when we have the symmetry so it's unitary covariance so meaning if you change the base on all inputs then the same basic change happens in all the outputs.
And okay so I'm not going to be very explicit about about this but we also will assume some extra symmetry so for example that it's symmetric under permuting the inputs and outputs but anyway I'm sweeping stuff under the right but we.
Also assume some kind of the streets entry.
And the question is you know how can we optimize over such channels if we want to find the best channel for certain operation, you know, you know how to how do we do this.
And okay so, so first we need to describe the channel in some some nice way, and the most convenient way is using the so called joy matrix.
The joy matrix is this very simple construct where you basically you take your channel and you apply it onto all standard base matrices so these are matrices have all zeros and then one in a certain position.
And then you just make a big block matrix where you kind of in every block label by IJ you just put the output of the channel basically.
And of course you know this matrix has the full information about the channel because if you have linear combination of these matrices then you can just take in your combination of these blocks and you can recover the channel.
So anyway there's a formula for explicit simple formula how to recover the output using this joy matrix.
But sort of the point is that it's, it's very easy to this to parametrize or sort of to characterize on channels.
Basically what you want is a matrix that's completely positive, and it has certain partial trace input identity so this this.
So the matrix is positive semi definite, which corresponds with channel being completely positive.
And then there's this partial trace constraint that that means that the channel is trace preserve. So basically this is a semi definite constraint and this is linear.
And so it means that naturally if you want to optimize over upon channels you end up having a semi definite program.
And now the question is how can we throw in this this unitary equivariance into the semi definite program. So if we optimize over channels and have this continuous symmetry we want to add this extra constraint.
So you know, okay so what does it mean other so we have this basically this kind of symmetry if you, if you do a base change on input on every input system then the same base change has happened on the every output system.
You know what what is turns out, if you know it's a simple thing to show is that this expressed in terms of the joy matrix will be that it has to commute with all matrices of this form.
So you have like n copies of you, and I'm copies of the complex conjugate of you, and then you tensor everything together, and whatever you you choose your choice matrix has to commute to this.
And so this imposes. Well this imposes infinitely continuously many constraints on your matrix, but since it imposes so many constraints that means there should be lots of parameters that get some sort of symmetrized or that you that you get rid of basically.
Okay, so now, how much time I have so yeah so basically if you want to understand these matrices we understand we need to mathematically understand what is the commute on to this what are the all the matrices that commute with with all the, you know, all these
okay.
And the answer is, is, is, this is described by what's called world Brouwer algebra.
So this has a scary name but all it is it's kind of like a generalization of the symmetric so. So let's say you have like a permutation on n plus m objects.
And, you know, you could draw it as a diagram, where you just connect these these these top objects with the bottom objects, and then here on the right side you flip this this permutation upside down.
Then you gonna obtain a diagram that's not a permutation anymore, because it will have some kind of some strands that go horizontally they connect the bottom to bottom and top to top.
But but these horizontal strands they always go across this middle and this is called wall it sort of separates the two sides.
So this is all it is it's, I call it partially transpose permutation you just you know transpose this side and then you get the presentation.
So because of that the number of these diagrams just the number of permutations which is n plus m factorial.
And you know you can multiply these diagrams in the same way as permutations you just put them together.
And then you, you know you just obtain another diagram, but sometimes you might end up with these loops, and then you just put a constant so that the dimension basically that you have a very system to the power of number of loops basically is going to be the constant.
And this is basically this is the trace of the identity matrix or this diagram is kind of like a trace of identity.
So this this forms algebra you can take linear combinations you can multiply these things.
You can also represent these diagram diagrams by matrices. So, so basically, like for example if you have this this diagram that's like a swap operation.
This is just a swap matrix, it's like a two by two swap gate.
And if you have this, I call it contraction that's, it goes across the wall so it's like these horizontal lines.
This is basically the max length angle state so you would represent it on two qubits by by unnormalized maximum time state.
So for a general mate for a general diagram, you can write some for a similar formula that captures this.
Okay, and so so the basically the, the main point is that so there is an extension of the sure value to these, this is called mixed tensors because they have these you and you bar.
And this basically says that the combatant of this is equal to the matrices the size the map the maps the diagrams matrices is just all the matrices that you obtain from the diagrams basically.
And this is similar to like for the normal sure value when we don't have this extra bar, then the commutant of a few tensor and it's just the permutation matrices that permute the systems.
Okay, and okay so then it means that you know if we have a unit really variant channel, it's the same as the choice matrix commutes with all these matrices. It's the same as the choice matrix is in is a linear combination of these of these diagrams basically.
And there is also a notion of a sure transform which which block diagonalize such matrix. And here it's it's sort of more complicated I don't want to go into details but, but there are also these two registers and one is for the unitary group and there you have identity because it commutes with the
unitary and in the other block you have some some kind of a complicated matrix.
So basically the point of sort of the strategy of optimizing over these channels with the continuous symmetries is that we, you know, instead of having a semi different constraint for this matrix, we can just put a positive semi different constraint for every block of this
matrix. And moreover, if we have additional symmetries which I will not go into like some parentational symmetries, then this block actually becomes diagonal, it gets symmetrized due to shore slammer, and it just becomes a constant times identity matrix basically.
And if it becomes a constant times identity matrix then you don't have a semi different program anymore you just have a linear program.
In cases when you have so much symmetry, you can actually say that your matrix is diagonal in a very specific basis, and then and then your some different program just becomes a linear.
So, so the punchline is that basically what we show is that if you have an arbitrary semi different program with you know when your constraints and inequalities and some different constraints, and you throw in also this disunity
of different variants, then, you know at first this is really scary, some different program because it has infinitely many constraints like continuously infinitely many.
The matrix is a huge size like you know D to the n plus m so there's no way you can solve this naively like for example, if you have like four inputs on one output of your channel and every output has dimension 100, then it has 10 to the 30 variables
like just impossible to solve this on a computer.
But, you know, using this kind of representation theory what you can show is actually this x is block diagonal and you know even maybe even diagonal in some cases.
And for example, you know, you will not be a matrix of with this number of entries, it will just be a linear combination of these diagrams and the number of diagrams is n plus m factorial in this case is 120.
So basically instead of having this huge matrix with 10 to 30 variables, you can just write your matrix as a linear combination of diagrams and there's only 120 variables.
So there's like a huge reduction in the number of items.
And so anyway so we show, we show a general way of taking these some different programs and reducing to much smaller linear programs.
You know that that's, yeah anyways and that that can be used to solve all kinds of problems like this majority vote for example, is an instance of this where we, you know we had a function that has all these kind of symmetries and you can use this approach to solve these kinds of problems.
Okay, because I'm out of time so let me just quickly say the open problems.
One really obvious open problem is that so for the quantum majority vote. So I was assuming the input was just a tensor part of size and side perps.
And the kind of intuition was that we have a device that's supposed to be output inside.
And sometimes it fails and it output side perp instead. But that's a very peculiar failure more so like, you know, if you do experiments, you know if your experiment fails, why should it produce exactly the right answer but you know exactly orthogonal to the answer.
So a nice interesting question is when you have the basic input of the majority vote to be states that are not restricted to any particular basis, they could be just arbitrary states basically.
So you can imagine that maybe there's like a correct state that the experiment was supposed to output but maybe it's rotated by some small angle in some random direction.
And you have several samples of these states that are all different sort of with different perturbations.
And my conjecture is that you should just run the same algorithm but I don't know how well it performs it would be nice to analyze like if you have certain variants of your distribution how does that reflect in the, in the output fidelity.
Yeah, so in a paper we consider only qubits but you can extend this to q dits you can also have multiple outputs you maybe want to have several copies of the site for example.
You know, again so we consider only symmetric functions because then the same different program reduces to linear program, but you know you could consider also non symmetric functions and it's more complicated.
Okay, so there's also this question so like you know the way I explained the algorithm we use a shoe transform but it's not totally obvious to me that it's actually necessary to use a shoe transform.
We also contributed to the most of the complexity was the complexity was and to the four which comes from the shoe transform.
But maybe it can be avoided so like you know, we basically we use this week short sampling, and then we do something to the, to the unitary register on my computer is going to die.
And, oh yeah, but maybe.
Yeah, maybe the shoe transform can be avoided in some way like there's something called generalized phase estimation maybe that can be used with people know it is.
And that I think there's some interesting connections with regular query complexity so basically, you know so so here the measure that we have is not very complexity, but is the output fidelity, but sort of there's some similarities and and you know also.
So in some sense maybe what we are doing with the semidefin program is like the, it's maybe like the adversary method so I like, I think there's also an interesting question of extending the adversary method to like to quantum inputs and quantum outputs basically.
So anyways, that's another and you know as I mentioned application to cryptography so maybe.
So this has kind of a cryptographic flavor so maybe has some applications as well.
So at the moment I'm working with my PhD student to extend this so basically what I, what I very quickly explain was how to reduce the semidefin program to linear program, assuming extra sort of the street symmetries like permutation symmetries.
But now we're working on a more general case when you don't don't have these extra symmetries and then you have a semi different programs.
All right, so that's all I wanted to say.
Thanks a lot.
So.
