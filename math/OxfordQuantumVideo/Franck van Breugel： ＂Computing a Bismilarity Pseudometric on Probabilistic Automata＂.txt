And we'll talk about the complexity of computing a bisoneric pseudometric on probability that's all.
So, as you most probably know, a professor has done a lot of work on bisoneric pseudometric.
So, since we've got some results in this area, it seems only fitting to present them here.
So, it's still a work with the speaker, James.
So, it's to look at the complexity of computing a particular bisoneric pseudometric for the automaton.
Before I come to the pseudometric, let me just briefly review what a probabilistic automaton and what a bisoneric system is.
So, this is a probabilistic automaton. As you can see, step one has these two outgoing edges.
So, if there are two probabilistic transitions and if you choose them, you make a non-automaniastic choice.
So, if you pick, for example, this probabilistic transition over here, what happens is that you end up in state three with probability one.
However, if you pick this one over here, this is a probabilistic transition, so you really have a choice.
So, if you pick this transition that the probability you have over here, the probability you have, you end up in the other state.
These two just make themselves look to themselves.
This one makes a similar transition.
And the colors of the states capture how the states are labeled.
So, these are the total propositions that hold in the unions.
These capture what properties these different states have.
So, I want you to have such an automaton.
The labels on the conditions?
There are no labels on the transitions.
We have just labels on the states.
We can map the one model to the other bisoners, so that doesn't really matter.
Okay, so now I want you to have such a system.
You're interested in which states behave the same.
So, many of these bisoneric notions have been introduced.
So, now look at a slightly different system, because as you can see, three and four are the same color.
So, first of all, we can look at three and four, so we have the same label.
So, in that sense, locally they're the same.
If you look at their behavior, they just make a look with probability one to themselves.
So, that model captures that they behave the same.
So, these two are by similar or equivalent.
So, now let's compare one and two.
So, here we have different choices of non-interminable.
So, you can choose one transition or the other.
So, for example, if this transition makes, sorry, if these states make this transition over here,
you can mimic it by taking this transition over here and it goes through the same state.
Now, when we come to the other one, so we have this transition over here,
I wonder, you know, can we match it by this one over here?
So, the idea here is that you're not interested in what is the probability of going to an individual state,
but you're interested in what is the probability of going into an equivalence class of states.
Now, since I already mentioned that three and four behave the same, they're considered equivalence.
So, they form an equivalence class.
So, as you can see, if I pick this transition, then I go with probability one to this equivalence class
and this can now be matched by this transition over here,
which goes also with probability one to that equivalence class as well.
Okay, so this notion for probabilistic automata was introduced by this person.
So, you recognize maybe who this is?
You do, so.
Yes, this is Robert who introduced this notion.
So, now let's have a look at a slightly different automata.
So, as you can see, the probabilistic transitions from these and these are almost the same,
but what we did is we modified it by a small amount, a positive amount.
So, now, in this particular case, since, by the way, as you can see,
three and four are different colors, so they're not equated, they're not in the state equivalence class.
So, when I want to match this transition with another transition,
well, this is the only other option that we have, but then these two numbers are not exactly the same
and both are separate equivalence classes, so they're not by similar.
So, the problem is that this notion is not robust,
meaning that if you make really small changes to some of the probabilities that are captured in the system,
then states that are by similar might become not or vice versa.
And, furthermore, generally, when we come up with such a model,
these numbers that we find are usually obtained by just looking at the system that belongs to the model.
So, you get them experimentally and they might not be exactly what they capture.
They might not be precisely, might be approximation.
So, this notion is not robust.
I know that some of you have observed, first, by...
So, the collaboration with Jean-Claude NÃ©ju,
small-card showed this in the early 90s.
So, he suggested that rather than using equivalence relations, that's used to the metric.
So, the idea is that one way to review an equivalence relation is just...
In the pair of states, it tells you whether the pair is in the equivalence relation, yes or no.
So, it's a boolean, right? True or false.
So, now you want to...
Sorry, Frank. I was going to be confused.
They're not by similar even for a total equal to zero, right?
So, I'm just going to give you an example.
Because you changed your course.
If that's one of the zero, then they're going to...
If that one is identical, transitions.
Yeah, but three and four are not quite similar, so...
Yeah, but I mean, this one goes with probability.
You have to, the purple state and this one does as well, right?
One and two, I think.
I mean, maybe...
He's talking about by similarity, one and two, yeah.
Ah!
You changed your course.
Sorry, Frank. Thank you.
All right.
Where we are?
Okay, here.
So, okay, so now what we want to do is we want to add a little bit more structure here, because this is not enough.
So, instead what we have is we have a function that, with every pair of states, maps it to, let's say, we pick the unit interval.
The idea is that the distance captures how similar states behave,
where distance zero means that they behave exactly the same,
whereas it is one and they behave completely different.
So, equivalence is captured by distance zero.
So, it gives you a nice quantitative generalization of this notion of behavioral equivalence.
So, now what we'd like to do is come up with a super metric that captures the behavior of equivalence of these automata,
and remember that they have both non-determinism and probabilistic choices as well.
So, all those will split into two.
So, first of all, if we start in non-determinism, usually they're modeled by subsets, right?
So, a natural way is now to look at distance functions on such sets of a particular set.
So, this is one.
And, so just to give you an idea also, what this generalizes,
so again, think of this, so this max is on the unit interval, so you can see that as well as a generalization of conjunction.
Similarly, if you take a max over set, it's more like a universal quantifier,
meaning it's going to be like an existential quantifier.
So, if you look at this formula, what does it say?
It says, for all a, there exists a b, and for all b, there exists an a.
So, hopefully those are any tricks that this might be something which would be quite similar, right?
So, you're just, you know, for every transition that the one system can make,
I can find a transition that matches it and finds the versa.
All right, so, this is what introduced by...
A lot of percussion.
Give one of the other students a chance.
Okay, so this is the one that we're going to use for non-terminism.
So, again, we also have the distance function on probabilistic choices,
which are naturally modeled by probability distribution.
So, we use probability distributions on a particular set,
and so now this is the distance function that we could use.
I'll go into all the details here.
So, we have a metric space over here.
These are all the non-expansive functions,
and then this is more like an integral to look at the difference between the two.
But I'll show you in a minute an example for a much simpler system
and at least hopefully you get a feel for why this notion of distance
is related to probabilistic bias similarity.
The first thing that we will actually introduce is...
So, this function was...
I think they came up with this notion in the early 40s,
but it was reinvented many, many times in the 60s and 70s.
I think even in the 80s, you'll find it under many, many different names.
There was a time metric, it's more or less the same distance function.
The Hodgeson metric is the same distance function and so on.
And it makes it sometimes tricky actually to find results in the literature,
which you have to know which name to look for.
There's also a very nice way of the literature,
because you can actually publish papers about the same thing over and over again.
The algorithm to compute, and then you just put it in the name of whatever you like,
and accept it again.
Alright, I was going to give you a simple example.
So we have the two states, and as you can see there's the uniform distribution,
they're not all determined at the moment, because we're only interested in probabilities.
So you have this situation, and you have this other state over here.
And so now these states are probabilistic by similar,
if you can find a one-to-one mapping, how you reorder them,
so that these pairs over here become probabilistic by similar as well.
So really what it is, is you have to find the permutation,
pi such that s i is corresponding pi i,
the other system, are probabilistic by similar.
So now if we look at the counter-office distance on these two distributions,
then we can characterize it as follows.
So again, as you can see, the permutations are over here.
The existential becomes a minimum.
And here we want that all of them are related,
so that you can do with a weighted sum.
So you get something over here.
And remember that the distance between two states captures their similarity.
So obviously if all these are zero,
then it means that you can find actually a permutation in that sum.
I hope it gives you some idea of how this actually is,
because it's closely related to probabilistic by similarity.
So we have a distance function for non-determinism.
We have a distance function for probability.
So what you do, you can, of course, combine the two.
And that was on pi.
I think some people in the audience might know the answer.
So this is a thought-making three-shot together with some of your colleagues.
So you combine the two distance functions
and then you obtain a distance function on these properties.
And they also proved, I mean, they proved a number of properties in that particular paper,
but also that probabilistic by similarity is nicely characterized by this distribution
in the sense that distance zero if and only if the states are probabilistic by similarity.
So now you have this nice distance function that has nice properties
and captures probabilistic by similarity.
So it might be interesting, can I compute it?
And so the problem that we worked on was how hard is it to actually compute this distance function?
So our result is the following.
So the problem of computing a distance function is in the complexity class called P-PAT,
which is polynomial parity argument in a directive graph.
I hope you do the...
Oh, almost.
So I won't give you the definition.
I'll probably screw it up anyway.
But let me just mention a few problems that are in this distance class as well.
So first of all, computing a natural area of two-layered games is complete for this distance class.
Computing the value of simple stochastic games,
a problem that people have been looking at for quite some time is in this class as well.
For example, compute fixed points of these broader functions is in the complexity class as well.
So what I want to do is in the remainder of the talk, so in the next 10 minutes or so,
at least give you part of the ingredient of how we actually improve this result,
which in itself actually, I think, is a neat result as well.
So for that, we take a few steps back.
So let's look at ordinary label transition systems.
We have just non-determinism.
And of course, they have the classical notion of bisimilarity,
and this notion of bisimilarity can be characterized in different ways.
So first of all, if you all know, there's the Hennessy-Milano logic.
You can characterize it as a great fixed point.
And there's also a game characterization for this notion as well.
So now if we move over to probabilistic automata,
we can look at probabilistic bisimilarity, the ordinary notion,
and then it can be captured by means of a logic.
So again, you can show that two states are probabilistic bisimilar,
if and only if they satisfy exactly the same formula.
And there's a fixed point characterization as well,
which was already given over those pieces.
So now moving from this discrete notion of probabilistic bisimilarity,
you can go to these probabilistic bisimilarity pseudo-metrics.
And again, there's a logical characterization.
This time you have a logic, and when you interpret the formula,
you get numbers, so elements of the interval.
And there's also a fixed point characterization.
This is actually how the distance function was defined.
So what is missing is a game theoretic interpretation of this notion.
So what we've shown is that...
So if you look at the distance of two states of a probabilistic automaton,
then you can actually capture this as the value of a simple stochastic game.
So this is what gives you a game theoretic characterization
of both ordinary probabilistic bisimilarity and also of this pseudo-metric as well.
All right, so now I hope I'll give you some idea
of, first of all, quickly showing what is a simple stochastic game
because you don't know.
And then sketching what is how we can actually capture the distance in such a game.
So this is a simple stochastic game.
So there are two special...
So it's a directed graph.
There are two special vertices.
One is labeled zero, one is labeled one.
And then all the other vertices have two outgoing edges.
And we distinguish three different types.
So we have a max node, a min node, and an average node.
And it's a two-player game.
So let's call them max and min.
So you start in some start vertex.
And then depending on the set, a max node and the max player.
So this side switch of the two to choose.
The min player of the min node and the min player on the side.
And when you're in an average node, it's most random.
So with 50%, you go in one direction and with 50%, you go in the other direction.
Now the objective of the max player is to maximize the probability of reaching this state over here.
And the min player doesn't want to be opposite.
So it either tries to reach state zero or just make sure that the game goes on forever.
So now the next thing that we want to do is see whether we can capture the distance in this particular way.
So let's look at some more details over here.
So the distance, remember, is defined in two things.
This is the house of distance and this is the house of distance.
So the distance between two states is, and this here, hopefully you recognize again, this is max or max min.
So this is the house of distance.
And then here, these are distributions.
So it says that you can go from the state s, you can go to pick a transition,
and on the moment, just pick one to some distribution and this one as well.
So the idea is now is that what we're going to do is we're going to introduce for every pair of states,
we're going to introduce a vertex in our game.
And this is going to be a max node.
And so now what we can do is we can make a, we can go to, let's see that,
so for example, s mu, then a whole bunch of these, and t mu.
So this one over here corresponds to a transition from t to mu,
and this one corresponds to this to mu.
So this, as you can see, and then this node over here should capture the value of this part of the expression.
So this we're now going to make min nodes and then we're going to do the same thing again.
So this may be a min node and this may be a min node as well.
And then we get transitions to, so that's the mu and mu.
So that's the next level that we get.
So now what we have to do still is we have to capture what is the value of this,
which corresponds now of course to this distance between these two distributions,
which is the counter-office distance, which is defined like this.
But this doesn't work really well.
So what we'll do is just use duality.
So we just want to split everything around.
So there's a duality theorem due to counter-office and root instinct.
And so as you can see, the max turns into a min and then things change a little bit.
This is closely related, by the way, to the duality theorem that you find in your program.
All right, so most of it looks okay, but of course this we still need to define.
So what it is, is this is the set of couplings of these two distributions.
And what is it?
Is it coupling more as it's combined?
So what is it?
You have a distribution on S here, another distribution on S,
and how do you combine them in a way?
So you get a joint probability on S times S,
where these are the equation that actually captures.
So now the nice thing about it is this is a convex polytile,
and so of course it will have infinitely many elements,
but luckily it has only finite emitting variables.
So how can we use this now?
Well, so by duality we get this equation over here,
but since this is a linear function, and we know that the linear function
it takes its minimum at the vertices,
so what we need to do is we only have to consider the vertices,
and they're finitely many, so that makes it look much better.
So what we really get here is, as you can see, it's a sum,
and then these are generally, when all the probabilities in the system
that you start with are rational, these will be rational as well,
so you get the sum of this.
So this is what is described by a gadget that consists of all average nodes,
so these are average.
And then as you can see over here, again,
we have the distance on these states again,
so we get back to these vertices B.
So of course, I mean, in graph we have cycles as well,
so ST might actually appear over here as well.
Okay, so this shows that what we can do
is we can now express the distance function
in terms of such a simple stochastic game
where the value is assigned to this particular vertex
actually exactly captures the distance between this, this variable.
You mentioned that there might be an exponential flow locked up,
simply because this over here might be an exponentially larger than what we started.
Alright, so that is pretty much through there.
So some of you might very notice that every time when I throw a picture,
it says that this is not proportional to that.
So this was in, I think it was 2001.
Exactly, that's what I...
So I gave a talk about how to use co-algebra
in order to define certain metrics.
And throughout the talk, I had pictures of this particular person
sometimes looking extremely puzzled.
That was the slide where I said, you know, these are co-algebra.
But towards the end of the talk,
the point sign was that our algorithm was polynomial type.
So as you can see, that was really good
because the algorithm that percussion published a couple of years earlier
was actually not polynomial type.
And ours was, so that was really good.
Now, so what turned out to be the case was that
on the slide, I also mentioned the related work
and Cush's name just ended up right underneath the picture.
He has reminded me of that many, many times.
So just to make sure that I never ever use that slide again,
I might as well give it to you.
So now, obviously, you never ever want to look at that slide again.
And so Joel actually showed a picture of you from a long time ago
and then showed you a picture from yesterday.
But I do a little bit better, you know.
I give you the picture that is just taken.
All right, so that actually concludes my talk.
Thank you very much.
That's our real question.
So if you look at LMPs, instead of probability parameter,
then does your algorithm compare what class it is in here?
I mean, this is the plus sign.
You mean no non-determinism?
There is some non-determinism in here.
Well, there are labels.
Yes.
Oh, yeah.
It should be following your time.
I mean, in the plus aspect, we had a following-time algorithm.
It was a slightly different one.
Probability parameter.
Yeah, the state label.
Yes.
But yeah, this should be.
Yeah, I think I'm not a professional.
Might be.
I had a question about the Kapling.
So Kapling is a keyword also in normal probabilistic literature.
Is there a relationship between your notion of Kapling
and Kapling of Markov chains, for example?
I hope there is something.
So because what you describe is just a span
in the category of probabilistic spaces
with measure-preserving functions.
Right.
I think it's the same notion in the record.
Yeah.
There's also Kapling of Markov chains.
So this is what happens when you write that statement.
Yes.
It's a theory.
I've read some arguments, but I've regretted them.
We said, well, I'm astute methodology
to prove a lot of things about Markov chains.
Yeah, so I think we're going to talk about this here now.
So if it's the same, then why are you trying to minimize on this?
Is that the way that it's trying to prove something
about the two Markov chains from the two different initial states?
Yeah, I think so.
For example, let's say just for any imagination in my question
because it probably doesn't make sense.
Can you relate the mixing times of Tape from SMT?
But if you have a complete Kapling.
So, Vincent, you and I will actually talk about this tomorrow.
We'll get to some Markov later.
We'll get to the gems of MPPs and Kapling together.
And for the disciplines.
OK.
So we'll get to that tomorrow?
I have a question.
So you have two choices.
One is Hausner and the other one is Kantorovich.
So we have any kind of rule of why any of these are the right choices.
Why should we start with Kantorovich's rules
and decide through the right choices
from the point of view of a theory of behaviors?
Why Kantorovich should be a right choice?
It makes the game, right?
But if you put something more than just the distance
which is 0, 5, 7, 8, 7, something else.
Well, I have some partial results that actually show that
if you look at it as a monad
then it has very nice properties
that you can find in other categories as well.
So in that sense, it seems to be
one is the metric analog, for example,
of the probabilistic power domain.
No, but you mentioned at a certain point in the motivation
of the fact that the probabilistic system
is actually something that is an approximation of something.
So one would expect a kind of logical argument
saying that, look, better and better and better
and better and better approximations.
In fact, these things would show that I'm going closer and closer
and closer to the thing I'm approximating.
Right.
So I don't know, can you put this into the thing
that this is not demonstrated?
Possibly, I don't know.
So again, we'll talk about that.
Okay, so it's great.
I'll see that we show up tomorrow.
I have a commentary question to write, essentially.
So you started by robustness
and how exact my similarity doesn't make sense
in the world where the main probability
is anyway the first place.
So do you prove that
if I move in the space of, you know,
transition functions
for some kind of logical notion of moving,
my evaluation of the metric is continuous with respect to this?
Yeah, that's true.
So if you make small changes to some of the probabilities
that the distance will not go all the other way,
it's a nice stage.
And the metric?
Let's say that you put a parameter in your system,
then the distance function will be continuous
with respect to that parameter.
So, okay.
Yeah.
So could you do something patient, then?
So instead of having an exclusive,
completely well-defined marker chain,
having some kind of un-eshrined set of marker chains,
we'll set another probability on top of this
and prove that somehow
all your notions of
behavioral similarities are continuous
with Bayesian updates or things like this?
I'll probably talk about this.
I'm not talking about that,
but I'll be very nice to have.
That's the logical, I mean,
it's actually logical next time,
if you have your vastness.
Very good.
Not too nice.
You have to do that.
So, I mean, why are you attempting
to calculate the distances exactly?
I mean, would the algorithms be better
if you tried to calculate the distances approximately?
I mean, it's the robustness
I'm inviting you back for me.
Yeah.
Well, it seems that we can do some kind of...
There have been some iterative algorithms
that have approximate distances.
It seems that this type of algorithms
might be political in this sense.
But it seems also that
actually the
complexity theory result,
which captures this P-pad,
gives us some hints on how to actually
you might be able to approximate distances.
Yeah, so the algorithms
work by approximating
until you get close enough and then rounding.
So the exact algorithms
are, I guess, approximation
and then do some rounding.
So I guess they're very close.
Okay, let's start again.
APPLAUSE
