In this video, we're going to talk about hidden Markov models, and these are some of the more popular ways to reason about sequences and time series that I know of.
So the video will cover the basic idea behind hidden Markov models, how they extend beyond the Markov models that we saw in the previous video,
and then we'll talk about the forward-backward inference algorithm for hidden Markov models,
which is a way of incorporating evidence from both the past and the future.
In fact, all of the evidence from every time step in our data in a way that is efficient,
and we'll eventually see boils down to an instance of variable elimination.
Then we'll talk about bomb-welch learning, which is a famous algorithm for learning the parameters of hidden Markov model,
which also boils down to something we've seen already, which is a combination of forward-backward and expectation maximization.
So let's get to the basic concept of hidden Markov models.
So the idea that we're going to try to represent with a hidden Markov model as opposed to a Markov model is that we're going to work with the assumption,
the realistic assumption that we don't ever actually get to observe the real state of any real-world sequence or time series.
Instead, what we observe is some other indicator of the true state.
So this happens all over in any application area, because if you think about it,
we don't ever fully observe the state of an entire system that we're measuring data from, except in rare, fully controlled and elaborate environments.
Most of the time, there's something that we don't measure, something that we can't measure that has a strong effect on what our predictions should be for the next thing to happen.
So one cartoon analogy that is consistent with this is a situation where we're controlling a submarine,
and maybe we have sonar available, and we want to detect where other submarines are in the water.
So we send out a sonar ping, and it sends out a pulse, and it bounces back to us,
and from that we can tell that there is some other object that is this far away from us,
and that creates an entire range of possible locations for this other object.
So maybe there's an enemy sub over here, which is on that radius of the circle that describes how far away this object is,
but we don't actually know. It could actually be all the way across from the first estimate.
It could be there, it could be in lots of different places. All these places are about the same distance from our sub.
So we have a lot of uncertainty about the actual state of the world.
We observe this observation, which is how far away the object is from us,
but we don't actually know what the real state of the world is.
But now if we move our sub, if we move down, if we dive a bit, then we send out another sonar ping,
and we get that the other object in the water is now this far away from our own sub,
and that should narrow down the possibilities based on the fact that if we assume that the only thing that changes is we moved around,
then it's unlikely, or let's say we say that it's unlikely that the other sub is also moving.
Maybe that's a faulty assumption, but then it narrows down to just these two possibilities based on the previous possibilities.
Now there's actually a few more, but I didn't draw them on the screen.
But the basic idea here is now because we have some understanding of what this true state of the world can transition from one time step to another,
when we observe the noisy observations, we can then use that to refine our estimate of the true probability distribution of possible states.
Or you can think of it as our beliefs about what the state of the world is.
We update those to be consistent with our observations and what we know about possible state transitions.
So mathematically this ends up being something called the hidden Markov model, where we have two pieces of the hidden Markov model.
The first is the probability of observing some observation given the true state.
So if you know the true state, then you look up a probability function of the y.
So here the x's are the true states and the y's are the observations.
So this is the observation probability and in the analogy with the submarine, this is the sonar noisiness.
This is the noisiness surrounding the sonar reading that we would take.
And then the second piece of a hidden Markov model is the transition probability.
So it's the probability of going from one state to another state as we move forward in the sequence.
And usually we think of this as time.
So in the next time step, given the previous state, what's the probability of our current state?
So this corresponds to the submarine location.
And when you have these pieces and you put them all together,
you get a joint probability distribution over all states in your records and all observations in your records.
And that looks like this.
So you have the probability of x of all the states from time one to time t and all the observations.
And it looks something like that.
I haven't written it all out yet.
But this was basically what we wrote for the Markov model.
I think this is exactly what I wrote actually in the last slides for the Markov model.
So this is just the state transitions.
But then we also add on this piece.
So now we also have the probability of the observations given the states that we've now reasoned about.
And the graphical model for this, if you draw it out, looks something like this.
So we have the Markov chain on top.
And at each time step, we have an observation variable coming off of the Markov chain.
And the reason that this is called the hidden Markov model is because, again,
the usual assumption is that we never get to directly observe x, but we always observe y.
So we see y's, but we need to reason about the y's to understand what they tell us about the x's.
So one common task with hidden Markov models is to infer the hidden state probabilities.
So in this task, you are given all of the y's.
So you observe all of the y's.
They're available for us to look up.
And our goal is to predict the x's.
So maybe one way to write this is you're looking for p of x given y.
But usually we actually care about something a little simpler than that, which is p of x t given y.
So we're interested in the probability at time step t of the hidden states given all of the observations.
Not just the current observation, but all of the observations throughout our entire history of recording data.
And here's how we do this.
And I guess I'm basically giving you a recipe here.
So we compute these alpha quantities, which we can think of as messages, but they actually specifically are defined as these probabilities.
So this is a joint probability between the current state variable x t and all of the observations up until this time.
So you can think of it as what's the joint probability of the current state and all of the historical observations.
So we'll consider these as a key part of our inference procedure.
And then we'll also consider essentially the other piece of the puzzle, which we're going to call them betas,
which are the conditional probability of all future observations.
So it's the probability of y t plus one all the way to y capital T.
So all the future observations given the current state.
And in looking at this, you might start to see why we'd be interested in these two quantities.
And the reason is that if we multiply them together, we get alpha t times beta t of x t, we get this joint probability of x t and all the past observations.
And then the conditional probability of all the future observations given x t, which, you know, if we remember our rules about conditional probabilities,
becomes the joint probability of x t and all of the observations.
And then furthermore, from here, we can normalize this expression and realize that this is going to be proportional to the probability of x t given y.
And we'd have to normalize it because we have to divide out the probability of y, which we can compute by summing over all the x's.
So we normalize this and we get a conditional probability.
And it's important at this point to note that this is a little different from this.
So this is not the same as the joint probability of all x's and y's.
In this case, we've actually, you know, if we can compute these alphas and betas, which I'll show you how we can, the answer is of course we can,
then we're computing the marginal probability of x t given all of the evidence as opposed to the joint distribution.
And the joint distribution we can't do much with because that's actually the whole expression for the Markov model.
So essentially this is a way to, if we can compute alphas and betas, which again we can, what we're doing is we're finding an efficient way to sum out,
to sum over all of the other x's other than t, x t.
Okay, so let's talk about how we do this.
So we do this in two stages and the first stage we're going to compute the alphas and we're going to do that using forward inference.
And this should look very similar to what we did before with the plain Markov models.
So remember our goal is to compute alpha t of x t, which we define as this joint probability of x t and all the evidence so far.
And we can compute that by, well first let's think about how we compute that on the first step of the Markov chain.
So in the first step we just have to consider the probability, the prior probability of the state and the probability of the observation given that state.
So that's by definition alpha one.
And then given this we should think about, well how then can we write down, using the same idea, write down the expression for the second alpha.
And that would be the probability of x two given all the evidence so far, which would be y one and y two.
And we can compute that, well if you think about that what you need to do is you need to sum over all the possible states of the previous time steps, which is x one.
So we have to sum over all the x ones of the probability of x one and y one times the transition probability.
So what's the probability of getting to x two given that we're in each state, each possible setting of x one.
So this p of x two given x one.
And then finally times the observation probability of y two given x two.
And that's the expression for alpha two.
And we can look in that, in that expression we had this p of x one, y one, this joint probability, which we had previously defined as alpha one.
So we can plug that in to write it in a recursive form.
And that should lead you to the fact that in general the expression is the following.
So the probability, the joint probability of the next time step state and all the evidence including the next time step is going to be, that's what we're looking for for alpha t plus one.
That's going to be computed by summing over all the possible states of the current time step.
The probability of, or the joint probability of all the data up till now and xt, the current state, times the transition probability, the probability of getting to the next state, and then times the observation probability.
So you sum those all up and you marginalize out xt and you're left with the probability of xt plus one.
So that's the forward procedure.
And you can see that each time we compute one of these, assuming we already pre-computed the previous alpha, so the way I wrote it is that if we want to compute alpha for t plus one, if we already pre-computed alpha t, then we just have to look that up.
So that's an O of one operation.
And then for variables that have k possible states, then you have to do the summation over all k states.
So it's an O of k operation to compute the next alpha.
And you have to just do that t times, so it's t times k, costs to compute all of the alphas.
So we can do something similar with the backward inference.
And this is a little bit trickier.
It's a little bit stranger in part because right now we're dealing with this conditional probability.
And it's maybe not as obvious how we can do this recursively.
But there's one clever trick, which is that you can start by talking about the probability of, oh, so let's think about what this expression is talking about.
It's always talking about, so beta is always referring to the probability of future evidence.
So there's kind of a weird boundary case when we're dealing with x capital T, the last time step in our data.
And in that situation, the future evidence is null, there's no future evidence.
So the probability of nothing is kind of hard to say, but we're going to define it as one.
We're going to say that when we say that the probability of no event occurring, or when we say what is the probability of no event occurring, we'll just say it's one.
You can argue philosophically about this, this is kind of like the tree falls in the forest kind of thing.
But let's just say it's one, and it makes the math work out.
Okay, so then if we have the last beta expression, by definition it's one, then we might be interested in finding this backward propagation rule.
So if we know beta T, how can we compute beta T minus one?
So beta T minus one is then the probability of all the evidence, including T, little t, all the way up to capital T, given the state of x minus one.
And we can think about the terms in the hidden Markov model expression that have to do with this, and basically it just boils down to this.
We're going to sum over all the possibilities for the state of that next time step, so xT.
Remember we're concerned with, we're trying to compute the beta for T minus one, so the next time step is xT.
So we're going to sum over all the possible settings for xT, and we're going to sum up the probability that we transitioned to xT from xT minus one,
times the probability of all future evidence for us, which is we're living in T minus one, so all the future evidence is going to include T, little t.
So we'll have little t, little t plus one, all the way to capital T.
And we can break that second term apart into the following.
So again, we have the transition probability still, but now we also have the observation probability of time step T, right?
So for yT, we have the probability of yT given xT, and then we have this probability of all the future evidence, future observations, right?
The probability of yT plus one all the way to yT given xT, and that is by definition beta of T.
So we can plug that back in when we get this expression.
So we went through a couple of steps to work this out to see why this is true, but ultimately you just have a simple formula which looks almost,
or very analogous certainly to the alpha update, right?
We're just essentially doing the propagation backwards.
So once we have these, we then fuse the messages, so we have these, these are our definitions for alphas and betas,
and if you multiply those two definitions together, you get a joint probability between, like I said before, xT, the current state,
and all of the evidence, all of the observations, right?
So from y1 to yT, and then yT plus one to y capital T, which is all of y,
and then you can normalize that and get the conditional probability.
And you can also do another thing, which is to, so this is going to be really useful, of course computing P of xT given y,
but we might also be interested in computing this.
So this is probability of xT and xT plus one given y.
And if we think about what that looks like, this is a little bit hard to, at least for me, to wrap my head around,
but it looks like this, right?
So you have the probability of xT and xT plus one, and the probability of all the y's.
And then I broke the y's apart into the three separate pieces.
So the first piece is the probability of y1 to yT.
So that's all of the past and present evidence.
Then yT plus one, which is the next steps evidence, and then yT plus two all the way to the end,
which is going to be the future evidence of the next step.
It's kind of weird.
But the reason that I broke things apart is because this nicely translates into some of the expressions that we've been thinking about so far.
So if we break this apart further or understand the independent structure of the hidden Markov model factorize probability distribution,
then we get these terms to describe these probabilities.
So you have probability of xT, the current time step, the current state of the current time step,
joint with all the past evidence, which roughly corresponds to the y1 through yT in the expression above.
Then we have the transition probability to get to the next state, the probability of getting to the next state from xT.
Then we have another expression, the probability of y from T plus two all the way to capital T,
which is the future evidence of the next step, and then the probability of the evidence from the actual next step, the T plus one.
Of course, the way that this broke down is very convenient because a lot of these terms are things that we've been thinking about already,
and you can just translate it to alpha T, the transition probability, beta of T plus one, and then the observation probability of T plus one.
And then that all gets normalized by, I guess it's a little weird, but the denominator is going to be P of y,
and we can compute P of y from what we've previously computed by considering the alpha at the last time step and summing over all the x's.
And you can think a little bit about why that is. If you look at the equations, you'll see why that is.
I think this is the one slide summary of forward-backward inference, and it's really just a few update equations.
At the very top, we have the alphas, which we compute forward in order, and then the betas that we compute backward in order.
And then once we have those computed, we can write down the expression for probability of x given y, which we can compute by normalizing the joint probability of x, T, and y.
And then we can also write down this probability of transitions, the estimated probability of transitioning from x, T to x, T plus one, all in terms of these alpha and beta messages.
Okay, so this is forward-backward as it's usually written in textbooks and tutorials.
But of course, this is going to get pretty difficult to handle for a computer, because if you think about, let's see, let's go back actually.
So if you look at alpha and beta, you can see that there are these very large joint probabilities.
So there's a joint probability of x, T, which is fine, that's just one variable, but then y1 through y, T, and that could be really large.
We could be looking at a sequence of a million observations, and then y, T, at some point during this computation, there's going to be a million variables.
And that joint probability could be really, really tiny, and for a computer, it often turns to zero.
So what we have to do is we have to normalize some of these to prevent them from underflowing.
So it's just a usual formula for normalization. We basically can compute all of the alpha T's and then replace them with the normalized form of alpha T.
So we sum over all of the possible inputs to alpha T and divide every alpha T by that sum, and same with beta T.
And as an exercise, and I think I'm actually giving this as a homework exercise, you have to work out a little bit in your head, why is this okay?
And you can show the math as to why this is okay, but at least in my view, it gets a little bit less okay first, and then eventually it becomes okay.
So that's the way I view the math. I don't know if that's too vague to give you a hint as to how this works or if it's at all helpful.
But yeah, it's an interesting exercise, and at least for me, it helps understand how this all works.
But the one key is that when we do this normalization, we avoid these things going to zero.
But we end up computing something a little different than what we originally intended.
Okay, so let's put that aside and assume that we can now encapsulate that.
We can say, okay, this forward-backward algorithm, which is world-famous, it allows us to compute p of xt given all of the evidence, given all of the y's.
It also allows us to compute p of xt joint with p of xt plus one, given all of the observations.
So now that we've got that encapsulated, we can tentatively view it as a black box. We can just say it works.
And then let's think about learning. Let's think about how we can use that to do learning.
So the first thing you want to do is we're going to parameterize these things.
So we're usually going to say something like, rather than represent a different probability function for p of xt plus one, given xt, right?
The way we've written on the slide so far, they've been different probability functions.
We can just say it's one function. We can just say it's a single conditional probability table or transition matrix, as people like to call it.
And then similarly, we should have a function that just maps from a hidden state to an observation, or to a conditional probability of an observation.
And we could say that that's, again, the same for every time step.
So what may change between time steps is what state you're in, but what doesn't change is the function that tells you what the probabilities are of transitioning and of observing a variable.
So if all the variables in the hidden Markov model are fully observed, then it's super easy, just like we said in the Markov chain model.
You just count up the probabilities, or you do a simple maximum likelihood if it's a continuous or parameterized distribution family.
But usually that's not the case. Usually we assume that we observe the y's, but we don't know the x's.
So if x is hidden, which is the case most of the time, we treat it as a latent variable, and then we just do expectation maximization.
And that's pretty much it. That's pretty much all you need to know.
So this famous algorithm, Baum-Welch, named after its inventors, is basically just em using forward-backward inference as the e-step.
So that can basically be the entire description of Baum-Welch.
But let me give you some details so we can make this more concrete.
So the details are we compute p of xt given y, and we compute p of xt and xt plus 1 given y using forward-backward.
We already showed you how to do that. You basically go forward with all the forward messages, you compute all the backward messages,
and then you have these formulas for p of xt and p of xt and xt plus 1.
Okay, and then that's the e-step.
And then the em step is we're going to maximize the weighted expected log likelihood.
And I won't give you all the possible equations for this, but basically one way you can do that is you want to, you know,
the things you want to compute in this parameterization are some prior probability over the states.
So one way you might do that is to just average all of the probabilities over all the time steps,
or depending on what your modeling assumptions are, you could just take the probability of the first state.
Or if you have lots of sequences that you're learning from, you can take the average over all the sequences of the first state.
I think the difference between these two forms is whether you believe there is something special about the first state,
or whether you just think that first state is just some arbitrary state where we started measuring it.
And then the transition probabilities, we just estimate that by computing the average transition probability
across all of our time steps, which we computed using forward-backward,
and then dividing out the probability of each state.
So these come right out of our alphas and betas.
Okay, and then the last piece is we need to compute the probability of y given x, right?
And you might have different parameterizations for that.
So for example, if you had a Gaussian emission model or a Gaussian observation model,
then maybe you'd compute the means by taking the weighted average of the observations weighted by the states.
Or you can compute, you know, a table. It could be a table in which case you would take the expectation
indicator of, you know, your observation being in the state that you're computing,
all normalized by the weighted sum of all the probabilities.
So this is all, you know, this should be stuff we've seen before.
I mean, this looks very similar to Gaussian mixture models,
or at least the EM algorithm in Gaussian mixture models,
that's essentially what this is, right?
We're saying that every state is a mixture of all possible hidden states,
or every time step is a mixture of different hidden states,
and our observations come from that mixture model.
But the difference is that now the mixture probabilities have dynamics over the sequence.
Okay, so to summarize, hidden Markov models are graphical models,
and they are factorized probability distributions that specifically represent hidden states.
And the pieces of the factorized probability distribution include
transition probabilities between adjacent states and observations based on the states.
And to work with hidden Markov models, we can use forward-backward inference
to incorporate all evidence into our estimates of the hidden states.
And then to learn the parameters for a hidden Markov model,
we can use expectation maximization, which leads to the Baum-Welch algorithm,
where we just treat the states as latent variables
and run EM pretty much the most standard way possible,
where the E-step is the forward-backward inference.
