BAYESIAN DECISION THEORY 

J. Elder 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

Credits 

2 

Probability & Bayesian Inference 

  Some of these slides were sourced and/or modified 

from: 
  Christopher Bishop, Microsoft UK 
  Simon Prince, University College London 
  Sergios Theodoridis, University of Athens & Konstantinos 

Koutroumbas, National Observatory of Athens 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

3 

Probability & Bayesian Inference 

1.  Probability 
2.  The Univariate Normal Distribution 
3.  Bayesian Classifiers 
4.  Minimizing Risk 
5.  The Multivariate Normal Distribution 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Problems for this Meeting 

4 

Probability & Bayesian Inference 

  Problems 2.1-2.4 
  Assigned Problem: 2.2 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

5 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 1.  Probability 

Random Variables 

7 

Probability & Bayesian Inference 

  A random variable is a variable whose value is uncertain. 

  For example, the height of a randomly selected person in this 
class is a random variable – I won’t know its value until the 
person is selected. 

  Note that we are not completely uncertain about most random 

variables.   

  For example, we know that height will probably be in the 5’-6’ range.   

  In addition, 5’6” is more likely than 5’0” or 6’0”.  

  The function that describes the probability of each possible 

value of the random variable is called a probability 
distribution. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Probability Distributions 

8 

Probability & Bayesian Inference 

  For a discrete distribution, the probabilities over all 

possible values of the random variable must sum to 1. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Probability Distributions 

9 

Probability & Bayesian Inference 

  For a discrete distribution, we can talk about the probability of a particular score 

occurring, e.g., p(Province = Ontario) = 0.36. 

  We can also talk about the probability of any one of a subset of scores occurring, 

e.g., p(Province = Ontario or Quebec) = 0.50. 

 

In general, we refer to these occurrences as events. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Probability Distributions 

10 

Probability & Bayesian Inference 

  For a continuous distribution, the probabilities over all possible values of 

the random variable must integrate to 1 (i.e., the area under the curve must 
be 1). 

  Note that the height of a continuous distribution can exceed 1! 

S h a d e d   a r e a   =   0 . 6 8 3 

S h a d e d   a r e a   =   0 . 9 5 4 

S h a d e d   a r e a   =   0 . 9 9 7 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Continuous Distributions 

11 

Probability & Bayesian Inference 

  For continuous distributions, it does not make sense to talk about the 

probability of an exact score. 
  e.g., what is the probability that your height is exactly 65.485948467… inches? 

Normal Approximation to probability distribution for height of Canadian females 
(parameters from General Social Survey, 1991) 

p
y
t
i
l
i

b
a
b
o
r
P

0.16 
0.14 
0.12 
0.1 
0.08 
0.06 
0.04 
0.02 
0 
55 

µ =

5'3.8"

? 

s

=

2.6"

60 

65 
Height (in) 

70 

75 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
Continuous Distributions 

12 

Probability & Bayesian Inference 

 

It does make sense to talk about the probability of observing a score that falls within a certain 
range 

  e.g., what is the probability that you are between 5’3” and 5’7”? 

  e.g., what is the probability that you are less than 5’10”? 

Valid events 

Normal Approximation to probability distribution for height of Canadian females 
(parameters from General Social Survey, 1991) 

p
y
t
i
l
i

b
a
b
o
r
P

0.16 
0.14 
0.12 
0.1 
0.08 
0.06 
0.04 
0.02 
0 
55 

µ =

5'3.8"

s

=

2.6"

60 

65 
Height (in) 

70 

75 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
Probability Densities 

13 

Probability & Bayesian Inference 

Cumulative distribution (CDF) 

Probability density (PDF) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Transformed Densities 

14 

Probability & Bayesian Inference 

Observations falling within  x + !x

(

)  tranform to the range  y + !y

(

)

! px(x) "x = py(y) "y

! py(y) ! px(x)

"x
"y

Note that in general, !y " !x.

Rather, 

!y
!x

"

dy
dx

 as !x " 0.

Thus py(y) = px(x)

dx
dy

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
   
  
  
  
Joint Distributions 

15 

Probability & Bayesian Inference 

Marginal Probability 

Joint Probability 

Conditional Probability 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Joint Distributions 

16 

Probability & Bayesian Inference 

 Sum Rule 

Product Rule 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Joint Distributions:  The Rules of Probability 

17 

Probability & Bayesian Inference 

  Sum Rule 

  Product Rule 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Marginalization 

18 

Probability & Bayesian Inference 

We can recover probability distribution of any variable in a joint distribution 

by integrating (or summing) over the other variables 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Conditional Probability 

19 

Probability & Bayesian Inference 

  Conditional probability of X given that Y=y* is relative 

propensity of variable X to take different outcomes given that 
Y is fixed to be equal to y* 

  Written as Pr(X|Y=y*) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Conditional Probability 

20 

Probability & Bayesian Inference 

  Conditional probability can be extracted from joint probability 
  Extract appropriate slice and normalize 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Conditional Probability 

21 

Probability & Bayesian Inference 

  More usually written in compact form 

•  Can be re-arranged to give 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Independence 

22 

Probability & Bayesian Inference 

  If two variables X and Y are independent then variable X tells 

us nothing about variable Y (and vice-versa) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Independence 

23 

Probability & Bayesian Inference 

  When variables are independent, the joint factorizes into a 

product of the marginals: 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayes’ Rule 

24 

Probability & Bayesian Inference 

From before: 

Combining: 

Re-arranging: 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayes’ Rule Terminology 

25 

Probability & Bayesian Inference 

Likelihood – propensity for 
observing a certain value of 
X given a certain value of Y 

Prior – what we know 
about y before seeing x 

Posterior – what we know 
about y after seeing x 

Evidence –a constant to 
ensure that the left hand 
side is a valid distribution 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

End of Lecture 2 

Bayesian Decision Theory:  Topics 

27 

Probability & Bayesian Inference 

1.  Probability 
2.  The Univariate Normal Distribution 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 2.  The Univariate Normal Distribution 

The Gaussian Distribution 

29 

Probability & Bayesian Inference 

MATLAB Statistics Toolbox Function:   
normpdf(x,mu,sigma) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Central Limit Theorem  

30 

Probability & Bayesian Inference 

 The distribution of the sum of N i.i.d. random 
variables becomes increasingly Gaussian as N grows. 
 Example: N uniform [0,1] random variables. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Expectations 

31 

Probability & Bayesian Inference 

Condi3onal Expecta3on 
(discrete) 

Approximate Expecta3on 
(discrete and con3nuous) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Variances and Covariances 

32 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Gaussian Mean and Variance 

33 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

34 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 3.  Bayesian Classifiers 

Bayesian Classification 

36 

Probability & Bayesian Inference 

  Input feature vectors 

x = x1,x2,...,xl

!"

T

#$

  Assign the pattern represented by feature vector x 

to the most probable of the available classes 

  !1,!2,...,!M

That is, 

x ! "i : P("i | x) is maximum.

Posterior 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
 
   
   
Bayesian Classification 

37 

Probability & Bayesian Inference 

  Computation of posterior probabilities 

  Assume known 

  Prior probabilities 

P(!1),P(!2)...,P(!M )

   Likelihoods 

(
p x |!i

),

i = 1,2,…,M

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
   
Bayes’ Rule for Classification 

38 

Probability & Bayesian Inference 

(
p !i | x

) =

(
p x |!i
p x(

(
) p !i
)

)

,

where 

p x(

) =

M

"

i=1

(
p x |!i

(
) p !i

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
   
M=2 Classes 

39 

Probability & Bayesian Inference 

39    Given x classify it according to the rule 

If P(!1 x) > P(!2 x)   " !1
If P(!2 x) > P(!1 x)   " !2

  Equivalently:  classify x according to the rule  
)P !2
(
(
)P !1

) > p x |!2
(
(
) > p x |!1

(
If p x |!1
(
If p x |!2

)P !1
(
(
)P !2

) " !1
) " !2

  For equiprobable classes the test becomes 

(
If p x |!1
(
If p x |!2

(
) > p x |!2
(
) > p x |!1

) " !1
) " !2

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
   
   
  
Example: Equiprobable Classes 

40 

Probability & Bayesian Inference 

R

1

(

→

ω

1

)

and

R

2

(

→

ω

2

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
Example:  Equiprobable Classes 

41 

Probability & Bayesian Inference 

41    Probability of error 

  The black and red shaded areas represent 

(
P error | !2
  Thus 

) = p(x !2)dx

x0
$

"#

Pe ! P(error)
(
= P !2
x0
$

=

1
2

"#

(

(
) + P !1
)P error|!2
1
2

p(x !2)dx +

+#

$
x0

p(x !1)dx

) = p(x !1)dx

"

#
x0

and 

(
P error | !1

)P error|!1

(

)

  Bayesian classifier is OPTIMAL:  it 
minimizes the classification error 
probability 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
   
  
  
Example:  Equiprobable Classes 

42 

Probability & Bayesian Inference 

  To see this, observe that 
shifting the threshold 
increases the error rate 
for one class of patterns 
more than it decreases 
the error rate for the 
other class. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

The General Case 

Probability & Bayesian Inference 

  In general, for M classes and unequal priors, the decision rule 

43 

43 

P(!i | x) > P(!j | x)  "j # i $ !i

minimizes the expected error rate. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
   
Types of Error 

44 

Probability & Bayesian Inference 

  Minimizing the expected error rate is a pretty 

reasonable goal. 

  However, it is not always the best thing to do. 
  Example:   

  You are designing a pedestrian detection algorithm for an 

autonomous navigation system. 

  Your algorithm must decide whether there is a pedestrian 

crossing the street. 

  There are two possible types of error: 

  False positive:  there is no pedestrian, but the system thinks there 

is. 

  Miss:  there is a pedestrian, but the system thinks there is not. 
  Should you give equal weight to these 2 types of error? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

45 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 4.  Minimizing Risk 

The Loss Matrix 

47 

Probability & Bayesian Inference 

  To deal with this problem, instead of minimizing error 

rate, we minimize something called the risk. 

  First, we define the loss matrix L, which quantifies the 

cost of making each type of error. 

  Element λij of the loss matrix specifies the cost of 
deciding class j when in fact the input is of class i. 

  Typically, we set λii=0 for all i. 
  Thus a typical loss matrix for the M = 2 case would have 

the form 

L =

"
$
$
#

0 !12
0
!21

%
’
’
&

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Risk 

48 

Probability & Bayesian Inference 

  Given a loss function, we can now define the risk 

associated with each class k as: 

M

$

rk = !ki p x |"k
#
Ri

i=1

(

)d x

  Probability we will decide Class !i  given pattern from Class !k
  where Ri is the region of the input space where we 

will decide ωi. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
Minimizing Risk 

49 

Probability & Bayesian Inference 

  Now the goal is to minimize the expected risk r, 

given by 

r =

M

"

k =1

(
rkP !k

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Minimizing Risk 

50 

Probability & Bayesian Inference 

r =

M

"

k =1

(
rkP !k

)

where 

M

$

rk = !ki p x |"k
#
Ri

i=1

(

)d x

  We need to select the decision regions Ri to minimize the risk r. 
  Note that the set of Ri are disjoint and exhaustive. 
  Thus we can minimize the risk by ensuring that each input x 

falls in the region Ri that minimizes the expected loss for that 
particular input, i.e., 
(
!kip x | "k

Letting li =

)P "k(

#

)
,

M

k=1

we select the partioning regions such that
x $Ri if  li < lj   %j & i

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
   
Example:  M=2 

51 

Probability & Bayesian Inference 

  For the 2-class case: 
(
)P "1
(
l1 = !11p x | "1

(
) + !21p x | "2

(
)P "2

)

   and 

(
l2 = !12p x | "1

(
)P "1

(
) + !22p x | "2

(
)P "2

)

  Thus we assign x to ω1 if 
(
) p x | #2

(
)P #2

(
) < !12 " !11

(
) p x | #1

(
)P #1

)

(
!21 " !22
  i.e., if  
(
p x | !1
(
p x | !2

Likelihood Ratio Test 

)
) >

(
P !2
(
P !1

) "21 # "22
(
(
) "12 # "11

)
) .

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
   
   
   
   
Likelihood Ratio Test 

52 

Probability & Bayesian Inference 

(
p x | !1
(
p x | !2

? 

)
) >

(
P !2
(
P !1

) "21 # "22
(
(
) "12 # "11

)
) .

  Typically, the loss for a correct decision is 0.  Thus the likelihood 

ratio test becomes 
)
) >

(
p x | !1
(
p x | !2

(
P !2
(
P !1

? 

)"21
)"12

.

  In the case of equal priors and equal loss functions, the test 

reduces to 
)
) > 1.

(
p x | !1
(
p x | !2

? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
   
   
Example 

53 

Probability & Bayesian Inference 

  Consider a one-dimensional input space, with features 

generated by normal distributions with identical variance: 

)
p(x !1) ! N µ1,"2
)
p(x !2) ! N µ2,"2

(
(

    where 

µ1 = 0, µ2 = 1, and !2 =

1
2

  Let’s assume equiprobable classes, and higher loss for errors on 

Class 2, specifically: 

!21 = 1,  !12 =

1
2

.

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
    
 
 
Results 

54 

Probability & Bayesian Inference 

  The threshold has shifted to the left – why? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

End of Lecture 3 

Bayesian Decision Theory:  Topics 

56 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 
5.  The Multivariate Normal Distribution 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 5  The Multivariate Normal Distribution 

The Multivariate Gaussian 

58 

Probability & Bayesian Inference 

MATLAB Statistics Toolbox Function:   
mvnpdf(x,mu,sigma) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Geometry of the Multivariate Gaussian 

59 

Probability & Bayesian Inference 

  where ! " Mahalanobis distance from µ to x

Eigenvector equation:  !ui = "iui

MATLAB Statistics Toolbox Function:   
mahal(x,y) 

   where (u i,!i) are the ith eigenvector and eigenvalue of ".

  Note that ! real and symmetric  "  #i real.

See Appendix B for a review of 
matrices and eigenvectors. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Geometry of the Multivariate Gaussian 

60 

Probability & Bayesian Inference 

  ! = Mahalanobis distance from µ to x

where (u i,!i) are the ith eigenvector and eigenvalue of ".

   or y = U(x - µ)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
Moments of the Multivariate Gaussian  

61 

Probability & Bayesian Inference 

thanks to anti-symmetry of z  

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Moments of the Multivariate Gaussian  

62 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

5.1 Application:  Face Detection 

Model # 1: Gaussian, uniform covariance 

64 

Probability & Bayesian Inference 

Fit model using maximum likelihood criterion 

m face 

m non-face 

2

l

e
x
P

i

s  Face 

s  non-face 

Face ‘template’ 

59.1 

69.1 

Pixel 1 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
Model 1 Results 

65 

Probability & Bayesian Inference 

Results based on 200 cropped faces and 200 non-faces from 
the same database.  

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

)
t
i

H
(
r
P

How does this work with a 
real image? 

0.2

0.4

0.6

0.8

1

Pr(False Alarm) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Model # 2: Gaussian, diagonal covariance 

66 

Probability & Bayesian Inference 

Fit model using maximum likelihood criterion 

m face 

m non-face 

2

l

e
x
P

i

s  Face 

s  non-face 

Pixel 1 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
Model 2 Results 

67 

Probability & Bayesian Inference 

Results based on 200 cropped faces and 200 non-faces from 
the same database.  

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

)
t
i

H
(
r
P

More sophisticated 
model unsurprisingly 
classifies new faces 
and non-faces better. 

Diagonal 
Uniform 

0.2

0.4

0.6

0.8

1

Pr(False Alarm) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Model # 3: Gaussian, full covariance 

68 

Probability & Bayesian Inference 

Fit model using maximum 
likelihood criterion 

PROBLEM:  we cannot fit this model.  We 
don’t have enough data to estimate the full 
covariance matrix. 

2

l

e
x
P

i

Pixel 1 

N=800 training images 
D=10800 dimensions 

Total number of measured numbers =  
ND = 800x10,800 = 8,640,000   

Total number of parameters in cov matrix = 
(D+1)D/2  = (10,800+1)x10,800/2 = 
58,325,400  

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
 
 
Transformed Densities Revisited 

69 

Probability & Bayesian Inference 

Observations falling within  x + !x

(

)  tranform to the range  y + !y

(

)

! px(x) "x = py(y) "y

! py(y) ! px(x)

"x
"y

Note that in general, !y " !x.

Rather, 

!y
!x

"

dy
dx

 as !x " 0.

Thus py(y) = px(x)

dx
dy

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
   
  
  
  
Problems for this week 

70 

Probability & Bayesian Inference 

  Problems 2.7 – 2.17, 2.19 – 2.21, 2.23 – 2.27 are 

all good! 
  At least do problem 2.14.  We will talk about this 

Monday. (Hopefully one of you will present a solution!) 

  Also, MATLAB exercises up to 1.4.4 are good. 

  At least do Exercise 1.4.2.  We will talk about this next 

week. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

71 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 

5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 6.   
Decision Boundaries in Higher Dimensions 

Decision Surfaces 

73 

Probability & Bayesian Inference 

  If decision regions Ri and Rj 

are contiguous, deﬁne!
g(x) ! P("i | x) # P("j | x)
  Then the decision surface !

g(x) = 0

separates the two decision 
regions.  g(x) is positive on 
one side and negative on the 
other.!

(
Ri:  P !i | x

(
) > P !j | x

)

(
Rj :   P !j | x

) > P !i | x
(

g(x) = 0

+ 
 - 
)

73 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

!
   
   
  
  
Discriminant Functions 

74 

Probability & Bayesian Inference 

74   

If f (.) monotonic, the rule remains the same if we use: 

x ! "i  if:  f(P("i x )) > f(P("j x))  #i $ j

   

gi(x) ! f(P("i | x))

            is a discriminant function 

 

In general, discriminant functions can be defined in other ways, 
independent of Bayes.   

 

In theory this will lead to a suboptimal solution 

  However, non-Bayesian classifiers can have significant advantages: 

  Often a full Bayesian treatment is intractable or computationally prohibitive. 

  Approximations made in a Bayesian treatment may lead to errors avoided 

by non-Bayesian methods. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
  
   
End of Lecture 4 

Multivariate Normal Likelihoods 

Probability & Bayesian Inference 

  Multivariate Gaussian pdf 

76 

76 

p(x !i) =

1
!

(2")

2 #i

exp $

&
(
’

1
2

1
2

(x $ µ
i

)% #i

$1(x $ µ
i

)

)
+
*

µ
i

= E x,
-

.
/

,
#i = E (x $ µ
-

i

)(x $ µ
i

)%

.
/  

called the covariance matrix 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
77 

77 

Logarithmic Discriminant Function 

Probability & Bayesian Inference 

p(x !i) =

1
!

(2")

2 #i

exp $

&
(
’

1
2

1
2

(x $ µ
i

)% #i

$1(x $ µ
i

)

)
+
*

 ln(!)

        is monotonic.  Define: 
(
(
)P !i
(
gi(x) = ln p x | !i

)

) = ln p x | !i
(

) + lnP(!i)

(x ! µ
i

= !

1
2
where

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

Ci = !

!
2

ln2$ !

1
2

ln "i

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
   
Quadratic Classifiers 

78 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

  Thus the decision surface has a quadratic form. 
  For a 2D input space, the decision curves are 

0.5

quadrics (ellipses, parabolas, hyperbolas or, in 
degenerate cases, lines). 

0.4

0.3

0.2

0.1

0
10

5

0

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

−5

−5

−10

−10

5

0

10

J. Elder 

  
Example:  Isotropic Likelihoods 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

79 

79 

  Suppose that the two likelihoods are both isotropic, but with different means and 

gi(x) = !

variances.  Then 
1
2"i
gi(x) ! gj(x) = 0

2 (x1

2 + x2

  And 

2) +

1
2 (µi1x1 + µi2x2) !
"i

1
2"i

2 (µi1

2 + µi2

(
2 ) + ln P #i

(

)

) + Ci

 will be a quadratic equation in 2 variables. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
   
  
  
Equal Covariances 

80 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

  The quadratic term of the decision boundary is 

given by 

1
2

(
xT !j

"1

"1 " !i

) x

  Thus if the covariance matrices of the two 

likelihoods are identical, the decision boundary is 
linear. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
Linear Classifier 

81 

Probability & Bayesian Inference 

gi(x) = !

1
2

(x ! µ
i

)T " !1(x ! µ
i

) + lnP(#i) + Ci

 

In this case, we can drop the quadratic terms and express the 
discriminant function in linear form: 

T

gi(x) = wi
wi = ! "1µ

i

x + wio

wi0 = lnP(#i) "

T
! "1µ
µ
i
i

1
2

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Example 1: Isotropic, Identical Variance 

82 

Probability & Bayesian Inference 

T

x + wio

Decision  
Boundary 

gi(x) = wi
wi = ! "1µ

i

wi0 = lnP(#i) "

T
! "1µ
µ
i
i

1
2

! = "2I.  Then

T
w

(x # xo) = 0,  where

w = µ
i

# µ
j

,  and

xo =

1
2

(µ
i

+ µ
j

) # "2 ln

P($i)
P($j)

µ
i

# µ
j

2

µ
i

# µ
j

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Example 2: Equal Covariance 

83 

Probability & Bayesian Inference 

T

gi(x) = wi
wi = ! "1µ

i

x + wio

wi0 = lnP(#i) "

T
! "1µ
µ
i
i

1
2

T

gij(x) = w

(x ! x0) = 0

where 

w = ! "1(µ
i

" µ
j

),

(µ
i

+ µ
j

) ! ln

#
%
$

P("i)
P("j)

&
(
’

µ
i

! µ
j

µ
i

! µ
j

2

)!1

,

x0 =

1
2

and

x

T

)!1 * (x

1
2

) !1x)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
  
  
Minimum Distance Classifiers 

84 

Probability & Bayesian Inference 

  If the two likelihoods have identical covariance AND 
the two classes are equiprobable, the discrimination 
function simplifies: 

gi(x) = !

1
2

(x ! µ
i

)T "i

!1(x ! µ
i

) + lnP(#i) + Ci

gi(x) = !

1
2

(x ! µ
i

)T " !1(x ! µ
i

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Isotropic Case 

85 

Probability & Bayesian Inference 

  In the isotropic case, 

gi(x) = !

1
2

(x ! µ
i

)T " !1(x ! µ
i

) = !

1
2#2 x ! µ

i

2

  Thus the Bayesian classifier simply assigns the class 

that minimizes the Euclidean distance de between the 
observed feature vector and the class mean. 

de = x ! µ

i

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
 
General Case:  Mahalanobis Distance 

86 

Probability & Bayesian Inference 

  To deal with anisotropic distributions, we simply classify according 

to the Mahalanobis distance, deﬁned as!

dm = gi(x) = (x ! µ

(

)T " !1(x ! µ
i

)

)1/2

i

  Since the covariance matrix is symmetric, it can be represented as!

 ! = "#"T

    where!

the columns vi  of ! are the eigenvectors of "

    and where!

! is a diagonal matrix whose diagonal elements "i  
are the corresponding eigenvalues.

  Then we have   !
m = (x ! µ

d2

i

)T "T#!1"(x ! µ
i

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

!
!
!
  
  
  
  
General Case:  Mahalanobis Distance 

87 

Probability & Bayesian Inference 

d2

m = (x ! µ

i

)T "T#!1"(x ! µ
i

)

Let  !x = "Tx.  Then the coordinates of  !x  are the projections of x
onto the eigenvectors of #, and we have:

)2

(
!x1 " !µi1
#1

+ ! +

)2

(
!xl " !µil
#l

2

= dm

Thus the curves of constant 

Mahalanobis distance c have ellipsoidal form.

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
   
  
Example: 

88 

Probability & Bayesian Inference 

Given !1, !2 :   P(!1) = P(!2 ) and p(x !1) = N(µ
&
( ,   " =
’

&
( ,   µ
’

0
0

3
3

µ
1

#
%
$

#
%
$

=

=

2

&
(
’

1.1 0.3
0.3 1.9

#
%
$
&
( using Bayesian classification: 
’

,  "), p(x !2 ) = N(µ

2

,  "),

1

classify the vector x =

#
%
$

1.0
2.2

•  !-1 =

#
%
$

0.95 "0.15
"0.15 0.55

&
(
’

•  Compute Mahalanobis dm from µ1, µ2 :

d2

m,1 =

!
"

1.0, 2.2

$ % &1
#

!
’
"

1.0
2.2

#
( = 2.952, d2
$

m,2 =

!
"

&2.0, &0.8

$ % &1
#

!
’
"

&2.0
&0.8

#
( = 3.672
$

•   Classify  x ! "1.  Observe that dE,2 < dE,1

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
 
  
  
Bayesian Decision Theory:  Topics 

89 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 7. Parameter Estimation 

Topic 7.1 Maximum Likelihood Estimation 

Maximum Likelihood Parameter Estimation 

92 

92 

Probability & Bayesian Inference 

Suppose we believe input vectors x are distributed as
p(x) ! p(x;"),  where " is an unknown parameter.
Given independent training input vectors X = x1,x2, ...xN

{

}

we want to compute the maximum likelihood estimate "ML for ".
Since the input vectors are independent, we have

N
p(X;") ! p(x1,x2, ...xN;") = #
k=1

p(xk;")

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Maximum Likelihood Parameter Estimation 

93 

93 

Probability & Bayesian Inference 

N
p(X;!) = "
k=1

p(xk;!)

N
Let L(!) " ln p(X;!) = #
k=1

ln p(xk;!)

The general method is to take the derivative of L
with respect to !,  set it to 0 and solve for ! :

ˆ!ML :   

$L(!)
$(!)

=

N

%

k=1

$ln p(xk;!)
$(!)

= 0

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
Properties of the Maximum Likelihood Estimator 

94 

94 

Probability & Bayesian Inference 

Let !0  be the true value of the unknown parameter vector.
Then
!ML is asymptotically unbiased:  lim
  N"#

E[!ML] = !0

!ML is asymptotically consistent:  lim
N"$

E ˆ!ML % !0

2

= 0

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Example: Univariate Normal 

95 

Probability & Bayesian Inference 

Likelihood func3on 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Example:  Univariate Normal 

96 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Example:  Univariate Normal 

97 

Probability & Bayesian Inference 

Thus !ML is biased (although asymptotically unbiased).

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
End of Lecture 5 

Example:  Multivariate Normal 

99 

Probability & Bayesian Inference 

  Given i.i.d. data                             , the log likeli-

hood function is given by 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Maximum Likelihood for the Gaussian  

100 

Probability & Bayesian Inference 

  Set the derivative of  the log likelihood function to zero, 

  and solve to obtain 

  One can also show that 

Recall:  If x and a  are vectors, then 

"
#$

!
!x

x ta(

) =

!
!x

a tx(

) = a

%
&’

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Topic 7.1 Bayesian Parameter Estimation 

Bayesian Inference for the Gaussian (Univariate Case) 

102 

Probability & Bayesian Inference 

  Assume     is known. Given i.i.d. data 

 !2

                           , the likelihood function for 
   is given by 
µ

  This has a Gaussian shape as a function of   (but it 
µ

is not a distribution over  ). 

µ

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Inference for the Gaussian (Univariate Case) 

103 

Probability & Bayesian Inference 

  Combined with a Gaussian prior over   , 

µ

  this gives the posterior 

  Completing the square over   , we see that 

µ

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Inference for the Gaussian 

104 

Probability & Bayesian Inference 

  … where 

(

).

) has the form Cexp !" 2
Shortcut:  p µ| X(
Get " 2  in form aµ2 ! 2bµ+c = a(µ!b/ a)2 + const and identify
µN = b/ a
1
2 = a
#N

  Note: 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Bayesian Inference for the Gaussian 

105 

Probability & Bayesian Inference 

  Example: 

µ0 = 0
µ = 0.8
!2 = 0.1

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Maximum a Posteriori (MAP) Estimation 

106 

Probability & Bayesian Inference 

In MAP estimation, we use the value of µ that maximizes
the posterior p µ | X(
µMAP = µN.

) :

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Full Bayesian Parameter Estimation 

107 

Probability & Bayesian Inference 

  In both ML and MAP, we use the training data X to 

estimate a specific value for the unknown parameter 
vector θ, and then use that value for subsequent 
inference on new observations x:   

p x | !(

)

  These methods are suboptimal, because in fact we 
are always uncertain about the exact value of θ, 
and to be optimal we should take into account the 
possibility that θ assumes other values. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
Full Bayesian Parameter Estimation 

108 

Probability & Bayesian Inference 

  In full Bayesian parameter estimation, we do not 

estimate a specific value for θ. 

  Instead, we compute the posterior over θ, and then 

integrate it out when computing

(
p x | X

)

 : 

p(x X) = p(x !)p(! X)d!

"
p(X !)p(!)

p(X)

p(! X) =

p(X !)p(!)

p(X !)p(!)d!

=

"

N
p(X !) = #
k=1

p(xk !)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
   
  
Example:  Univariate Normal with Unknown Mean 

109 

Probability & Bayesian Inference 

Consider again the case p(x µ) !N µ,!(
We showed that p µ|X(

(
) ! N µN,!N

2

) , where 

) where ! is known and µ ! N µ0,!0

(

)

In the MAP approach, we approximate p(x X) ! N µN,!2

(

)

In the full Bayesian approach, we calculate p(x X) = p(x | µ)p(µ X) dµ

!

which can be shown to yield p(x X) ! N µN,!2 + !N

2

(

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

    
     
    
  
    
Hints for Exercise 1.4.2 

110 

Probability & Bayesian Inference 

  Here are some MATLAB functions you may find useful in solving 

Exercise 1.4.2 
  mnrnd 
  mvnrnd 
  mvnpdf 
  mean 
  cov 
  squeeze 
  sum 
  repmat 
  inv 
  min 
  max 
  zeros 
  ones 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Problem 1.4.2 

111 

Probability & Bayesian Inference 

function pe=pr142 
%Exercise 1.4.2 from PR Matlab Manual 

m(:,1)=[0 0 0]'; 
m(:,2)=[1 2 2]'; 
m(:,3)=[3 3 4]'; 
S=[0.8 0.2 0.1;0.2 0.8 0.2; 0.1 0.2 0.8]; 
N=1000; 

%Part 1 
ntrain=mnrnd(N,ones(3,1)/3); %Number of training pts generated by each 
class 
ntest=mnrnd(N,ones(3,1)/3); %Number of test pts generated by each class 
test=[]; 
mml=zeros(3,3); 
Smli=zeros(3,3,3); 
for i=1:3 
    train=mvnrnd(m(:,i),S,ntrain(i)); %training vectors from class i 
    test=[test;mvnrnd(m(:,i),S,ntest(i))]; %test vectors from class i 

    mml(i,:)=mean(train); %ML estimate of mean for class i 
    Smli(i,:,:)=ntest(i)*cov(train,1);%weighted ML estimate of 
covariance for class i 
end 

Sml=squeeze(mean(Smli)/N); %ML estimate of common covariance 

%Part 2:  Euclidean distance 
for i=1:3 
    dsq(:,i)=sum((test-repmat(mml(i,:),N,1)).^2,2); 
end 
[m,idx(:,1)]=min(dsq'); 

%Part 3: Mahalanobis distance 
for i=1:3 
    y=test-repmat(mml(i,:),N,1); 
    dsq(:,i)=sum((y*inv(Sml)).*y,2); 
end 
[m,idx(:,2)]=min(dsq'); 

%Part 4:  Maximum likelihood classifier 
for i=1:3 
    p(:,i)=mvnpdf(test,mml(i,:),Sml); 
end 
[m,idx(:,3)]=max(p'); 

%Ground truth classes 
idxgt=[ones(ntest(1),1);2*ones(ntest(2),1);3*ones(ntest(3),1)]; 

for i=1:3 
    pe(i)=mean(idx(:,i)~=idxgt); %Error rate for class i 
end 
!

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
     
  
  
  
  
  
  
Bayesian Decision Theory:  Topics 

112 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Topic 8 Mixture Models and EM 

Motivation 

114 

Probability & Bayesian Inference 

  What do we do if a distribution is not well-

approximated by a standard parametric model? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

8.1 Intuition 

Mixtures of Gaussians 

116 

Probability & Bayesian Inference 

  Combine simple models  
into a complex model: 

Component 

Mixing coefficient 

K=3 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Mixtures of Gaussians 

117 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Mixtures of Gaussians 

118 

Probability & Bayesian Inference 

  Determining parameters 

 µ, ! and "

 using maximum 

log likelihood 

Log of a sum; no closed form maximum. 

  Solution: use standard, iterative, numeric 
optimization methods or the expectation 
maximization algorithm.  

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
End of Lecture 6 

Hidden Variable Interpretation 

120 

Probability & Bayesian Inference 

(
p x | P1,…PJ , µ1,…µJ ,!1,…!J

(
) = PjN x;µj,!j

"

2

J

j=1

)

=

J

"

j=1

(
p(j)p x | j

)

  j = 3

  j = 2

  j = 1

 x

 j

 x

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

  j = 3

  j = 2

  j = 1

 j

J. Elder 

    
Hidden Variable Interpretation 

121 

Probability & Bayesian Inference 

(
p x | P1,…PJ , µ1,…µJ ,!1,…!J
ASSUMPTIONS!

(
) = PjN x;µj,!j

"

2

J

j=1

)

=

J

"

j=1

(
p(j)p x | j

)

•  for each training datum xi there is a hidden variable ji.!
•  ji represents which Gaussian xi came from!
•  hence ji takes discrete values!
OUR GOAL: 

To estimate the parameters !:
The means µj
The covariances " j
The weights (mixing coefficients) Pj
for all J components of the model.

THING TO NOTICE:!

 x

  j = 3

  j = 2

  j = 1

 j

If we knew the hidden variables ji for the training data it would be easy to 
estimate parameters   – just estimate individual Gaussians separately.!

!

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

!
!
  
    
Hidden Variable Interpretation 

122 

Probability & Bayesian Inference 

THING TO NOTICE #2:!

If we knew the parameters     it would very easy to estimate the posterior 
!
distribution over each hidden variable ji using Bayes’ rule:!

(
p j | x,!

) =

(
p x | j,!

J

"

j=1

(
p x | j,!

)Pj
)Pj

)
x
|
j
(
r
P

 x

  j = 3

  j = 2

  j = 1

 j

j=1!

j=2!

j=3!

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

!
!
  
Expectation Maximization 

123 

Probability & Bayesian Inference 

Chicken and egg problem:   

•  could ﬁnd j1...N  if we knew !
!
•  could ﬁnd   if we knew j1...N!
!

Solution:  Expectation Maximization (EM) algorithm 

 (Dempster, Laird and Rubin 1977) 

EM for Gaussian mixtures can be viewed as alternation between 2 steps: 

1. Expectation Step (E-Step) 

•  For ﬁxed    ﬁnd posterior distribution over responsibilities j1...N!

!

2. Maximization Step (M-Step) 

•  Now use these posterior probabilities to re-estimate!

!

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
 
 
  
  
 
 
8.2 Math 

Mixture Model 

125 

Probability & Bayesian Inference 

Let 
xk,k = 1,…N denote the training input observations, assumed to be independent
jk ! [1,…J] indicate the component of the mixture from which the observation was drawn
(Note that xk is observable but jk is hidden.)

Let 
(
!t = "t, Pt

)  represent the unknown parameters we are trying to estimate, where

" represents the vector of coefficients for the component distributions and
P represents the mixing coefficients.

Our mixture model is p xk | !

(

) = Pjk
"

(
p xk | jk; !

)

J

j=1

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
   
   
Q Function 

126 

Probability & Bayesian Inference 

We will iteratively estimate !,  starting with an initial guess !(0) 
and monotonically improving our estimate !(t) at successive time steps t.

For this purpose, we define a Q function
(

)
(
log p xk | jk;"

) = E

Q !; !(t)

#

Pjk

N

$
&
%

k=1

’
)
(

The Q function represents the expected log likelihood of the training data,

given our most recent estimate of the parameters !(t), where the expectation
is taken over the possible values of the hidden labels jk.

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
   
   
Expectation Step 

127 

Probability & Bayesian Inference 

  In the E-Step, we calculate the (expected) log 

probability over the possible parameter values: 

Q !; !(t)

(

) = E

N

#

k=1

$
&
%

Pjk

)
(
log p xk | jk;"

’
)
(

’
()

N

= E Pjk

#

$
%&

)
(
log p xk | jk;"

k=1
N

=

#

k=1

J

#
jk=1

(
P jk | xk; !

)Pjk

)
(
log p xk | jk;"

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
Maximization Step 

128 

Probability & Bayesian Inference 

  In the M-Step, we select for our new parameter 

estimate the value that maximizes this expected log 
probability: 

!(t + 1) = arg max

!

Q !; !(t)

(

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Example:  Mixture of Isotropic Gaussians 

129 

Probability & Bayesian Inference 

(
p xk | j;!

) =

exp $

%
’
’
’
&

xk $ µj
2#j

2

2

(
*
*
*
)

1

(
2"#2
j

!
2

)

  E-Step: 

Q !; !(t)

(

) =

N

J

*

k=1

*

j=1

(
p j | xk; !

$
) "
&
&
%

l
2

log#j

2 "

1
2#j

2 xk " µj

2

+ logPj

’
)
)
(

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

    
   
Example:  Mixture of Isotropic Gaussians 

130 

Probability & Bayesian Inference 

  Responsibilities Update Equation: 

(
P j | xk; !(t)

) =

(
p xk | j;"(t)

J

#

j=1

(
p xk | j;"(t)

)Pj(t)
)Pj(t)

L = 5

2

0

!2

  Parameter Update Equations: 

!2

0

(e)

2

µj(t + 1) =

N

"

k=1

P j | xk; !(t)

(

) xk

N

"

k=1

P j | xk; !(t)

(

)

Pj(t + 1) =

1
N

N

"

k=1

(
P j | xk; !(t)

)

2(t + 1) =

!j

N

$

k=1

P j | xk; "(t)

(

) xk # µj(t + 1)

2

$
l P j | xk; "(t)

(

N

k=1

)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
   
   
   
Univariate Gaussian Mixture Example 

131 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

2-Component Bivariate MATLAB Example 

132 

Probability & Bayesian Inference 

CSE 2011Z 2010W

100

80

60

40

20

)

%

(

e
d
a
r
g
m
a
x
E

0
0

20

40

60

80

100

Assignment grade (%)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
2-Component Bivariate MATLAB Example 

133 

Probability & Bayesian Inference 

%update responsibilities!

    for i=1:k!

        p(:,i)=alphas(i).*mvnpdf(x,mu(i,:),squeeze(S(i,:,:)));!

CSE 2011Z 2010W

    end!

    p=p./repmat((sum(p,2)),1,k);!

%update parameters!

    for i=1:k!

        Nk=sum(p(:,i));!

        mu(i,:)=p(:,i)'*x/Nk;!

        dev=x-repmat(mu(i,:),N,1);!

        S(i,:,:)=(repmat(p(:,i),1,D).*dev)'*dev/Nk;!

        alphas(i)=Nk/N;!

    end!

100

80

60

40

20

)

%

(

e
d
a
r
g
m
a
x
E

0
0

20

40

60

80

100

Assignment grade (%)

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

!
!
 
 
Bivariate Gaussian Mixture Example 

134 

Probability & Bayesian Inference 

1

(a)

0.5

0

1

(b)

0.5

1

(c)

0.5

0

Responsibilities P jk | xk; !(t)

(

)

Samples from p xk, jk | !

(

)

0

Samples from p xk | !

(

)

0

0.5

1

0

0.5

1

0

0.5

1

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

   
   
   
Expectation Maximization 

135 

Probability & Bayesian Inference 

  EM is guaranteed to monotonically increase the 

likelihood. 

  However, since in general the likelihood is non-

convex, we are not guaranteed to find the globally 
optimal parameters. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

8.3 Applications 

Old Faithful Example 

137 

Probability & Bayesian Inference 

2

0

!2

2

0

!2

)
n
i
m

(

n
o
i
t

p
u
r
e

t
x
e
n

o
t

e
m
i
T

2

0

!2

L = 1

2

0

!2

!2

0

(a)

2

!2

0

(b)

2

!2

0

(c)

2

L = 2

L = 5

2

0

!2

L = 20

2

0

!2

!2

0

(d)

2

!2

0

(e)

2

!2

0

(f)

2

Duration of eruption (min) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
 
 
 
Face Detection Example: 2 Components 

138 

Probability & Bayesian Inference 

0.4999 

0.5001 

Prior 

Mean 

Standard 
deviation 

0.4675 

0.5325 

Prior 

Mean 

Standard 
deviation 

Each component is still assumed to 
have diagonal covariance. 

The face model and non-face 
model have divided the data into 
two clusters.  In each case, these 
clusters have roughly equal 
weights.   

The primary thing that these seem 
to have captured is the 
photometric  (luminance) variation.   

Note that the standard deviations 
have become smaller than for the 
single Gaussian model as any 
given data point  is likely to be 
close to one mean or the other. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Face Model 
Parameters 

Non-Face 
Model 
Parameters 

 
 
 
Results for MOG 2 Model 

139 

Probability & Bayesian Inference 

)
t
i

H
(
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Performance improves 
relative to a single 
Gaussian model, 
although it is not 
dramatic. 

We have a better 
description of the data 
likelihood. 

MOG 2 
Diagonal 
Uniform 

0.2

0.4

0.6
Pr(False Alarm) 

0.8

1

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
MOG 5 Components 

140 

Probability & Bayesian Inference 

0.0988 

0.1925 

0.2062 

0.2275 

0.1575 

Prior 

Face Model 
Parameters 

Non-Face 
Model 
Parameters 

Mean 

Standard 
deviation 

0.1737 

0.2250 

0.1950 

0.2200 

0.1863 

Prior 

Mean 

Standard 
deviation 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

MOG 10 Components 

141 

Probability & Bayesian Inference 

0.0075 

0.1425 

0.1437 

0.0988 

0.1038 

0.1187 

0.1638 

0.1175 

0.1038 

0.0000 

0.1137 

0.0688 

0.0763 

0.0800 

0.1338 

0.1063 

0.1063 

0.1263  

0.0900 

0.0988 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Results for Mog 10 Model 

142 

Probability & Bayesian Inference 

)
t
i

H
(
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Performance improves 
slightly more, 
particularly at low false 
alarm rates. 

MOG 10 
MOG 2 
Diagonal 
Uniform 

0.2

0.4

0.6

0.8

1

Pr(False Alarm) 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Background Subtraction 

143 

Probability & Bayesian Inference 

Test Image 

Desired Segmentation 

GOAL : (i) Learn background model  (ii) use this to segment regions 
where the background has been occluded  

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

What if the scene isn’t static? 

144 

Probability & Bayesian Inference 

Gaussian  is  no  longer  a  good 
fit to the data. 

Not  obvious  exactly  what 
probability  model  would  fit 
better. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Background Mixture Model 

145 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

6.8 Applications

111

Figure 6.27 Background subtraction. For each pixel we aim to infer a label

y

0, 1

denoting the absence of presence of a foreground object. a) We

learn a class conditional density model P r(x

y) for the background from

∈{

}

training examples of an empty scene. The foreground model is treated as

|

uniform. b) For a new image we then compute the posterior distribution
x).
using Bayes rule. c) Posterior probability of being foreground P r(y = 1

Background Subtraction Example 

|

146 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Figure 6.28 Background subtraction in deforming scene. a-b) The foliage is

blowing in the wind. c) This distribution of RGB values at this pixel (red

channel only shown) is now bimodal and not well described by a normal

density function. d) A mixture of Gaussians describes this data well. e) We

can now proceed to classify each pixel from a new scene as foreground or

background. f) Results (after post-processing to remove noise).

The results (ﬁgure 6.27) show that individual pixels are sometimes misclassiﬁed

due to the overlap between the distributions:

if there is an unusual amount of

noise at a background pixel it may be classiﬁed as foreground and if the foreground

object is the same colour as the classiﬁer then it can be misclassiﬁed as background.

Moreover, shadows are often misclassiﬁed as foreground. A simple way to remedy

this is to classify pixels based on the hue alone.

End of Lecture 7 

Bayesian Decision Theory:  Topics 

148 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

9.  Nonparametric Methods 

Nonparametric Methods 

150 

Probability & Bayesian Inference 

  Parametric distribution models are restricted to specific 
forms, which may not always be suitable; for example, 
consider modelling a multimodal distribution with a 
single, unimodal model. 

  You can use a mixture model, but then you have to 

decide on the number of components, and hope that 
your parameter estimation algorithm (e.g., EM) 
converges to a global optimum! 

  Nonparametric approaches make few assumptions 
about the overall shape of the distribution being 
modelled, and in some cases may be simpler than using 
a mixture model. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Histogramming 

151 

Probability & Bayesian Inference 

  Histogram methods partition 
the data space into distinct 
bins with widths Δi and count 
the number of observations, ni, 
in each bin. 

•  Often, the same width is used 

for all bins, Δi = Δ. 
•  Δ acts as a smoothing 

parameter. 

 

In a D-dimensional space, using 
M bins in each dimension will 
require MD bins! 

The curse of dimensionality 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Kernel Density Estimation 

152 

Probability & Bayesian Inference 

  Assume observations drawn 
from a density p(x) and 
consider a small region R 
containing x such that 

  If the volume V of R is 

sufficiently small, p(x) is 
approximately constant 
over R and 

  The expected number K out 
of N observations that will 
lie inside R is given by 

  Thus 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Kernel Density Estimation 

153 

Probability & Bayesian Inference 

Kernel Density Estimation: fix V, estimate K 
from the data. Let R be a hypercube centred 
on x and define the kernel function (Parzen 
window) 

It follows  that  

      and hence 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Kernel Density Estimation 

154 

Probability & Bayesian Inference 

To avoid discontinuities in p(x), use a smooth kernel, e.g. a Gaussian 

(Any kernel k(u) such that 

will work.) 

h acts as a smoother. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

KDE Example 

155 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Kernel Density Estimation 

156 

Probability & Bayesian Inference 

  Problem:  if V is fixed, there may be too few points 

in some regions to get an accurate estimate. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Nearest Neighbour Density Estimation 

157 

Probability & Bayesian Inference 

Nearest Neighbour 
Density Estimation: fix K, 
estimate V from the data. 
Consider a hypersphere 
centred on x and let it 
grow to a volume V* that 
includes K of the given N 
data points. Then 

for j=1:np!
     d=sort(abs(x(j)-xi));!

     V=2*d(K(i));!

     phat(j)=K(i)/(N*V);!

end!

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Nearest Neighbour Density Estimation 

158 

Probability & Bayesian Inference 

Nearest Neighbour 
Density Estimation: fix K, 
estimate V from the data. 
Consider a hypersphere 
centred on x and let it 
grow to a volume V* that 
includes K of the given N 
data points. Then 

K=5

K=10

K=100

True distribution
Training data
KNN Estimate

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
 
Nearest Neighbour Density Estimation 

159 

Probability & Bayesian Inference 

  Problem:  does not generate a proper density (for 

example, integral is unbounded on    ) 

  ! D

  In practice, on finite domains, can normalize. 
  But makes strong assumption on tails  

!

"
#$

1
%
&’
x

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Nonparametric Methods 

160 

Probability & Bayesian Inference 

  Nonparametric models (not histograms) require 
storing and computing with the entire data set.  
  Parametric models, once fitted, are much more 
efficient in terms of storage and computation. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

K-Nearest-Neighbours for Classification 

161 

Probability & Bayesian Inference 

  Given a data set with Nk data points from class Ck 

and                      ,  we have 

  and correspondingly 

  Since                    , Bayes’ theorem gives 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

K-Nearest-Neighbours for Classification 

162 

Probability & Bayesian Inference 

K = 3 

K = 1 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

K-Nearest-Neighbours for Classification  

163 

Probability & Bayesian Inference 

  K acts as a smother 
   As              , the error rate of the 1-nearest-

neighbour classifier is never more than twice the 
optimal error (obtained from the true conditional class 
distributions). 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

KNN Example 

164 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Naïve Bayes Classifiers 

165 

Probability & Bayesian Inference 

  All of these nonparametric methods require lots of data 

O N(

)

to work.  If        training points are required for 
accurate estimation in 1 dimension, then 
required for D-dimensional input vectors. 

O ND(

)
   points are 

  It may sometimes be possible to assume that the 
individual dimensions of the feature vector are 
conditionally independent.  Then we have 
(
p xj | !i

(
p x | !i

) =

"

)

D

j=1

  This reduces the data requirements to  

O DN(

).

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
  
   
  
End of Lecture 8 

Bayesian Decision Theory:  Topics 

167 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

10.  Training and Evaluation Methods 

Machine Learning System Design 

169 

Probability & Bayesian Inference 

  The process of solving a particular classification or 
regression problem typically involves the following 
sequence of steps: 
1.  Design and code promising candidate systems 
2.  Train each of the candidate systems (i.e., learn the 

parameters) 

3.  Evaluate each of the candidate systems 
4.  Select and deploy the best of these candidate systems 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Using Your Training Data 

170 

Probability & Bayesian Inference 

  You will always have a finite amount of data on 

which to train and evaluate your systems. 

  The performance of a classification system is often 
data-limited:  if we only had more data, we could 
make the system better. 

  Thus it is important to use your finite data set wisely. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Overfitting 

171 

Probability & Bayesian Inference 

  Given that learning is often data-limited, it is 

tempting to use all of your data to estimate the 
parameters of your models, and then select the 
model with the lowest error on your training data. 

  Unfortunately, this leads to a notorious problem 

called over-fitting. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Example: Polynomial Curve Fitting   

172 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Sum-of-Squares Error Function 

173 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

How do we choose M, the order of the model? 

174 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

1st Order Polynomial 

175 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

3rd Order Polynomial 

176 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

9th Order Polynomial 

177 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Over-fitting 

178 

Probability & Bayesian Inference 

Root‐Mean‐Square (RMS) Error: 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Overfitting and Sample Size 

179 

Probability & Bayesian Inference 

9th Order Polynomial 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Over-fitting and Sample Size 

180 

Probability & Bayesian Inference 

9th Order Polynomial 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Methods for Preventing Over-Fitting 

181 

Probability & Bayesian Inference 

  Bayesian parameter estimation 

  Application of prior knowledge regarding the probable 

values of unknown parameters can often limit over-fitting of 
a model 

  Model selection criteria 

  Methods exist for comparing models of differing complexity 

(i.e., with different types and numbers of parameters) 
  Bayesian Information Criterion (BIC) 
  Akaike Information Criterion (AIC) 

  Cross-validation 

  This is a very simple method that is universally applicable. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Cross-Validation 

182 

Probability & Bayesian Inference 

  The available data are partitioned into disjoint 

training and test subsets. 

  Parameters are learned on the training sets.   
  Performance of the model is then evaluated on the 

test set. 

  Since the test set is independent of the training set, 
the evaluation is fair:  models that overlearn the 
noise in the training set will perform poorly on the 
test set. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Cross-Validation:  Choosing the Partition 

183 

Probability & Bayesian Inference 

  What is the best way to partition the 

data? 
  A larger training set will lead to more accurate 

parameter estimation. 

  However a small test set will lead to a noisy 

performance score. 

  If you can afford the computation time, repeat 

the training/test cycle on complementary 
partitions and then average the results.  This 
gives you the best of all worlds:  accurate 
parameter estimation and accurate evaluation. 

  In the limit:  the leave-one-out method   

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

184 

Probability & Bayesian Inference 

The Univariate Normal Distribution 

1.  Probability 
2. 
3.  Bayesian Classifiers 
4.  Minimizing Risk 

The Multivariate Normal Distribution 
5. 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

10.  What are Bayes Nets? 

Directed Graphical Models and the Role of Causality 

186 

Probability & Bayesian Inference 

  Bayes nets are directed acyclic graphs in which each node represents a 

random variable. 

  Arcs signify the existence of direct causal influences between linked 

variables. 

  Strengths of influences are quantified by conditional probabilities 

where pak  is the set of 'parent' nodes of node k.

  NB:  For this to hold it is critical that the graph be acyclic. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

  
Bayesian Networks 

187 

Probability & Bayesian Inference 

  Directed Acyclic Graph (DAG) 

From the definition of conditional probabilities (product rule): 

In general: 

This corresponds to a complete graph. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Networks 

188 

Probability & Bayesian Inference 

  However, many systems have sparser causal 

relationships between their variables. 

General Factorization 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Generative Models of Perception 

189 

Probability & Bayesian Inference 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Discrete Variables 

190 

Probability & Bayesian Inference 

  General joint distribution: K2 -1 parameters 

  Independent joint distribution: 2(K -1) parameters 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Discrete Variables 

191 

Probability & Bayesian Inference 

  General distributions require many parameters. 
  General joint distribution over M variables:  

KM -1 parameters 

  It is thus extremely important to identify structure in 
the system that corresponds to a sparser graphical 
model and hence fewer parameters. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

192 

19
2 

Binary Variable Example 

Probability & Bayesian Inference 

  S:  Smoker? 
  C:  Cancer? 

  H:  Heart Disease? 

  (H1, H2):  Results of medical 

tests for heart disease 

  (C1, C2):  Results of medical 

tests for cancer 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Discrete Variables 

193 

Probability & Bayesian Inference 

  Example:  M -node Markov chain  
  K -1 + (M -1) K(K -1) parameters 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

194 

19
4 

Using Bayes Nets 

Probability & Bayesian Inference 

  Once a DAG has been constructed, the joint probability can be 
obtained by multiplying the marginal (root nodes) and the 
conditional (non-root nodes) probabilities. 

  Training: Once a topology is given, probabilities are estimated 
via the training data set. There are also methods that learn the 
topology. 

  Probability Inference: This is the most common task that Bayesian 
networks help us to solve efficiently. Given the values of some of 
the variables in the graph, known as evidence, the goal is to 
compute the conditional probabilities for some of the other 
variables, given the evidence. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Inference in Bayes Nets 

195 

Probability & Bayesian Inference 

  In inference, we clamp some of the variables to 

observed values, and then compute the posterior 
over other, unobserved variables. 

  Simple example: 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Example 

196 

Probability & Bayesian Inference 

 a) Suppose x has been measured and its value is 1.  What is 
the probability that w is 0? 
 b) Suppose w is measured and its value is 1.  What is the 
probability that x is 0? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

 
Message Passing 

197 

19
7 

Probability & Bayesian Inference 

  For a), computation propagates from node x to node w, resulting 

in P(w0|x1) = 0.63. 

  For b), computation propagates in the opposite direction, resulting 

in  P(x0|w1) = 0.4. 

  In general, the required inference information is computed via a 
combined process of “message passing” among the nodes of the 
DAG. 

  Complexity: 

  For singly connected graphs, message passing algorithms amount 

to a complexity  linear in the number of nodes. 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

Bayesian Decision Theory:  Topics 

198 

Probability & Bayesian Inference 

1.  Probability 
2.  The Univariate Normal Distribution 
3.  Bayesian Classifiers 
4.  Minimizing Risk 
5.  The Multivariate Normal Distribution 
6.  Decision Boundaries in Higher Dimensions 
7.  Parameter Estimation 
8.  Mixture Models and EM 
9.  Nonparametric Density Estimation 
10.  What are Bayes Nets? 

CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 

J. Elder 

