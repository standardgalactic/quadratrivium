One of the grand challenges in robotics is to get robots to work together as a group
to do things that they couldn't do it alone, call it teamwork, or the sum is greater than
the parts.
Some prefer emergent behavior.
Whatever you choose to call it, the functional benefit of working in a group is the promise
of swarm robotics.
Swarm robotics deals primarily with groups of simple, similar mobile robots, and it's
related to a field of computational modeling known as agent-based modeling.
These are also interested in swarms and have created self-propelled particle theory, where
a particle is an autonomous agent to formalize the behavior of the agents in the swarm.
The agent is any autonomous actor, like a physical robot or an animal, and the core
principle of agent-based models and self-propelled particle theory is that simple rules enacted
by each agent independently generate complex behavior of the group as a whole.
When we apply this principle in swarm robotics, we frame it as a matter of creation and control.
How do we create and then control a swarm?
Let's start with a single autonomous mobile robot, like this nerd robot right here.
The nerds controller, I peel this on the inside, the nerds controller is a small computer located
just here inside the robot.
On to that computer, we've loaded information to take, loaded a code that takes information
from these eye spots here, which are photoresistors, and converts it into instructions to send
to the servo motor here, which controls the tail and propels it back and forth.
Now, in terms of controlling the behavior of the nerd, this controller does only part
of the job.
Now, let me show you what it looks like when we put the nerd into the tank to explain what
I'm talking about here.
All right, inside our tank, we actually have one, two, three, four, five nerds, and we
have some aqua bots here from hex bugs swimming around.
So, we've got a swarm of swarms, two different kinds of swarms here, and actually these
hex bugs don't have any way of communicating with each other, and so they're doing their
own thing, and right now the nerds are not communicating with each other either.
So, I'm going to turn on this one nerd that I have here, and I've got it programmed, so
what I actually have to do is put it in the dark for a moment, and that's its signal to
start operating.
All right, so this nerd is going in the tank, and you can see the tail is just moving back
and forth, and we've got those two eye spots, and what I want to show you is I can get it
to turn by shading the eye spot.
It might be easier to actually show you out of the water here.
So right now in this light, the robot would actually be turning to the right, circling
in the right.
Now, if I put my hand over this eye spot, you see what happens there?
Okay, let the light come again, shade over here, right, the robot's now going to go
straight and start heading towards, but there's an even tighter turn in that direction.
So this shows you the connection between the sensor and the motor, and what results is
actually light seeking behavior.
Now, what's really cool about robots is its behavior is not just controlled by the computer
program that's connecting up the actuator and the sensor.
Let me show you what I've done on another robot here, which is I've played with its
mind by actually changing its body.
I've rotated here the position of the eye sensors, so that one is in back and the other
one is up front.
Now I would turn this on for you, but what happened when I just rotated this is I pulled
one of the wires, so I can't turn it on.
But now instead of reacting to shade back here by turning to the left in what it's doing,
so what is going to happen is it's going to have a response to light from behind and
the light in front, so we're going to get a different kind of behavior than we would
if the eyes were rotated those 90 degrees.
So what's important about this is that what we see happening is the robot interacts with
the world through its body, and as that body or the world change, so too does the specific
pattern of that interaction.
And that interaction of the robot with the world is what we call behavior.
Ironically, and importantly, what we've just shown is the behavior is only partially controlled
by what we call that computer controller, right?
So control is distributed throughout the robot world system, each part has a role to play.
Let's call this principle distributed control of behavior, and while we've just applied
this principle to a single agent interacting in the world, one of the very cool things
about it is we also see it in action in swarms.
A colony of leaf cutter ants are a great example of a group doing things that no individual
could do alone.
Leaf cutter ants are farmers.
They grow fungus to feed their larvae and themselves, and to grow that fungus they have
to harvest fresh leaves.
They cut those leaves and then transport them back to the nest.
The nest has to be cleaned of debris, and most importantly, cleaned of pests or molds
that could harm the fungus that they're trying to harvest.
This is an extensive farming operation.
A typical mature nest may have a mound with a radius of several hundred feet and contain
eight million workers.
Some casts tend the fungus, and some casts tend the larvae.
Others patrol the foraging columns and defend the nest.
Others still forage and then cut and transport leaf materials.
Here's the best thing.
No single individual ant is in charge.
Instead, each ant responds to the behavior of other ants it interacts with directly.
The ants out foraging also communicate indirectly by virtue of leaving a chemical trail as they
move that other ants can smell.
As more ants travel a path, that path gains more odorant.
A simple rule then for an ant at a fork in the road could be, follow the smelliest trail.
That rule results in the line of foragers marching in a column that is typical of fire
ants or, the ants we're talking about, leaf cutter ants.
This algorithm of swarm navigation has been implemented in robotic ants.
Simon Garnier and his colleagues at the Swarm Lab at the New Jersey Institute of Technology
have built small behavior-based robots that follow simple rules.
Each robot has an IR sensor oriented laterally to avoid hitting objects, and then two light
sensors point it up.
In a maze, a robot heads out from a starting point and blunders its way around, eventually
finding a path.
It leaves not a chemical trail, but a trail of light on the ground, thanks to a special
set of overhead spotlights.
Now the next robot that comes along uses its light sensors to detect the path laid down
by that first robot, and it also leaves a trail of light that makes for an even brighter
path to the next robot, and that makes for faster navigation.
Garnier's ant robots are individual agents that use simple rules to enhance behavior,
in this case, enhance the navigation, and it fixes and helps the whole group figure out
where to go.
So a diametrically opposed approach to the control of swarms is to have agents be centrally
controlled by a single system.
And Jo Fong Foy at Stanford Research Institute takes this centralized approach with tiny
construction robots.
These microbots, which can vary in size from 1 to 10 millimeters in length, are magnetic
and are propelled by changes in the electromagnetic field on the printed circuit board over which
they hover.
As they use magnetic forces and can manipulate objects, these microbots have been called
diamagnetic micromanipulators.
Each microbot can carry a different set of arms so that they can have specialized worker
casts like ants that can populate that swarm.
One cast can carry fibers, another cast can carry superglue.
Programmed to work together, they can construct truss-like systems.
This work has been funded by DARPA as part of their initiative to miniaturize manufacturing
processes and take advantage of the unique properties of structures assembled at the
nano-scale.
The SRI microbots are very interesting not only because they can be programmed to build
things on a tiny scale, but also because they do so by violating the core principle
of agent-based models and swarms, simple rules enacted by each agent independently generate
complex behavior of the group as a whole.
Instead, the rules, while they may be simple, are enacted by the central control program
that moves all the magnets around the circuit board at the same time.
Another way to think about this is that each microbot is not an autonomous agent.
They don't act on their own.
I love it when principles get violated by the way.
When it happens, it's usually for good reason and it's often very instructive.
Santa Claus, why?
Why are you taking our core principle?
Well, the core principle was discarded with microbots for the simple, practical reason
that they are too small to carry an energy supply, a computer controller, or even sensors.
In techno-speak, all of those parts and processes are offloaded to a smart and centrally controlled
environment.
Once you do this, the limit to the size of the microbots is only limited by your ability
to control the magnetic field that drives them.
So this is really a terrific innovation in swarm robotics that has the potential to change
micromanufacturing.
But we can't always create the highly structured world required for pre-programmed and centralized
control of a group of agents.
Almost anything we might want a swarm of robots to do outside will be working in an unstructured
world that is rapidly changing.
So let's bring back our core principle that we just discarded and look for a behavior-based
form of control.
Leandro Marcolini at the University of Southern California has been conducting experiments
on small mobile robots.
A group of six identical epoch robots are attracted to a goal, namely to move from one
side of the arena to the other.
What makes that task challenging is that there are two groups moving in the opposite direction,
so they have traffic to navigate.
In addition to being programmed with the goal to reach the other side, each robot also has
an avoidance reflex built in.
So when a robot hits or approaches another closely, it will stop, back up, and then set
off in a slightly different direction towards its goal.
Marcolini calls this first process uncoordinated in the sense that each agent is acting independently.
And both groups are able to make it past each other eventually.
However, the time to navigate past the other group is reduced by 50% with coordination
among the individuals.
What's wonderful is that the coordination algorithm is simple.
The first robot to detect congestion up ahead signals a warning to the rest of the team.
The whole group then switches to the state of deviating from the congestion at a preset
angle as indicated by LEDs that illuminate.
The result is a beautiful display of well-behaved robotic traffic.
Marcolini's traffic experiments point out the usefulness of communication among agents
in a group.
Using a similar process of nonverbal communication, fish swimming in a school may avoid obstacles
or threatening circumstances.
Kauno's rays are famous for swimming together in formations.
A single school of thousands can migrate together into and out of the Chesapeake Bay each year.
And what's important to keep in mind is that the schooling behavior of rays, or the flocking
of birds for that matter, is not pre-planned.
And remember, there's not some kind of group leader here.
That is, even though one individual may be in front of the formation, different individuals
take up that position all the time.
There's no bow-peep for the sheep, no general for the troops.
How do we know?
When we study the schooling of Kauno's rays, we do so by carefully tracking their motions
with an overhead video camera as they swim in a big tank.
At the tank, we would wait until the rays were just milling around and not schooling.
Then one of the members of my research team would randomly select an individual, what
we call a focal individual, and try to grab it.
Which reactors you or I would, to try to be grabbed, they startle and want to swim away.
By measuring the movements of all the individuals at the same time, we could see something really
cool happen.
The first individual to respond to that startle of the focal ray was its nearest neighbor.
They would speed up, move towards the startle ray, and realign its motion.
Then the nearest neighbor to that second ray would do the same thing, not in response to
the first ray, but in response to the second ray.
This is fascinating.
What we see in Kauno's rays is a chain reaction, with the startle behavior of a single individual
spreading to each nearest neighbor in turn.
If you think of a startle behavior as a warning or a danger signal, that signal propagates
through the school.
The school of rays has just come together on high alert, and they will stick together
for a bit, swimming in tight formation.
We see other species of fish drawn together when a threat looms, like a large predator.
Just as that predator swims through the school, the fish closest to the predator turn away
and others follow, a chain reaction that can result in the beautiful coordination of the
entire school as it wheels and twirls away.
The fact that schools of fish and flocks of bird can coordinate movements with nonverbal
signals suggests the possibility of simple algorithms.
In 1987, a computer scientist by the name of Craig Reynolds was looking for a way to
realistically simulate how individuals move in a group.
Agent-based modeling has its origins in computer-based artificial intelligence simulations, and Reynolds'
work was a typical example.
He proposed three very simple rules for swarming, what I'll call attraction, avoidance, and
alignment.
These rules guide the movement of each individual in his group simulation.
Alignment is the force that makes you want to move towards other individuals.
Avoidance is the force that repels you from those other individuals and keeps you from
running into them.
And alignment is the force that turns you to move in the same direction as others.
And if you have all three rules, work at the same time.
What happens?
Well, just like in our rays, Reynolds' swarm algorithm creates a chain reaction, coordinating
the motions of the whole group.
And what's amazing about Reynolds' three simple rules is that they allow anyone to
make realistic computer simulations of swarms.
So Reynolds did this using little agents that had become known as BOIDS.
So successful were these three simple rules in creating realistic animations that Reynolds
actually formed his own company, Reynolds Engineering and Designing, to work with entertainment
companies.
You've probably seen movies with CGI of autonomous agents inspired by BOIDS.
The BOIDS model inspired Steven Regulus to make the multi-agent software known as Massive,
which allowed director Peter Jackson to put tens of thousands of individual CGI orcs and
other soldiers on the battlefield in the Lord of the Rings movies.
Massive is also used to simulate crowds.
Now one of the deviations from the simple BOIDS model is that with Massive you can have your
crowds follow the terrain or head towards a common goal.
The agents can also be of different sizes and carry different weapons.
But here's the apparent punchline, from cow-nose rays to BOIDS.
Swarming requires that you be aware of what your neighbors are doing.
Now we humans might naturally think of neighbor awareness as something visual, but it needn't
be.
For example, when fish swim without any available light, they can still school using a sensory
system called the lateral line, which is an ear-like series of external hairs that detect
changes in flow and how the water is hitting around the body.
The closer you swim to another fish, the more that your flow will be disturbed by their
wake.
This is like car racing, where a driver can tail another in order to draft, using the
wake of the lead car to change the flow around their own.
But vision-based swarming works too, as we saw with our cow-nose rays.
Vision also works for robot swarms, as the Kokoro robots have shown.
A lily robot moves underwater and uses blue lights on its body to signal other lily robots.
Each lily also has photo detectors to pick up those signals.
Just like with the cow-nose rays, lily robots are detecting what their nearest neighbors
are doing.
If a neighbor's blue lights are on and blinking, then the lily robot that detected that signal
starts blinking its own blue light, then the next neighbor detects the blinking lights
and starts blinking and so on.
Chain reaction.
In the case of this group of lily robots, the chain reaction of blinking allows information
to be sent along the physical chain of robots.
In the situation that you see in this video clip, the message is this.
We have detected a robot with blinking blue lights.
To keep the lily robots organized, the Kokoro project used a swarm algorithm based on the
behavior of honeybees clustering.
They called it B-clust.
Robots start by moving randomly.
If a robot detects an obstacle ahead, it stops and listens for a signal that indicates the
object is another robot.
If another robot is detected, the focal robot measures the intensity of the light.
The higher the light intensity, the longer it waits before returning to random movement.
B-clust is a very simple algorithm for swarming, and it works in the water, with lily robots
and on land with jasmine robots.
The Kokoro project envisions using swarms of underwater robots to search more efficiently
for things like sunken treasure, downed airliners, or leaks in underwater oil pipes.
I think that these search and detect tasks are exactly the right kind of thing for swarm
robots.
You simply can't search far and wide very quickly with a single robot.
When we build robots at Vassar College, we, like Reynolds and Kokoro, are always interested
in pushing the limits of simplicity.
So we wondered if we could create a swarm of robots that used even simpler rules than
those suggested by Reynolds.
Crazy as it sounds, we wanted to try a swarm that gets rid of sensing other agents.
No neighbor awareness.
Let me go back to the tank here, because I want to talk about the robots you've already
met, the nerds.
These were used in our experiments.
So nerds stand for neuro-evolving robotic device, and it's maybe the best robot name
ever.
I can say that because I didn't come up with it.
Another member of our team, Marianne Porter, did.
Now, because nerds cannot, by design, sense other nerds, we call them asocial.
Other swarming agents, like the Boyds or the Kaunos Rays, are social in that sense,
in that they're able to sense the other agents.
At the same time, nerds have some sensory capacity, as you've seen.
They wouldn't be autonomous robots if they didn't, so they're able to sense the direction
of light using this pair of photoresistors.
Now, each nerd has a simple task, swim towards the light, and all nerds share this goal.
If we give the nerds about a minute to swim around with the light on in a very large tank
at my laboratory at Vassar College, sometimes something really cool happens.
Sometimes the nerds will end up swimming in a circle around the light.
This is coordinated movement, and it's not programmed into the nerds, and they can't
detect each other because they're asocial.
So what in the world is going on?
Well, we ran some experiments to find out.
The first thing we did was to turn off the shared light-seeking behavior and just let
the nerds wander randomly about even though the light was on.
That's what you can see in this video clip.
In other words, these were asocial nerds who also had no ability to detect anything in
the world.
They were without senses of any kind.
We would call these agents open-loop agents, and in an open loop, the functional loop of
robotics is broken.
The agent has no feedback between what's going on in the world and what is happening
with the controller and the actuators.
Another way to talk about the functional loop is to think about it in terms of agency.
With a broken functional loop, the machine has 0% agency.
With a closed functional loop, the machine has 100% agency and is now an autonomous robot.
So when we turn nerds into open-loop machines, they don't school, they just mill around.
Let's give the nerds 100% agency what happens then.
Starting from different positions on the perimeter of that large round tank, the nerds all swim
towards the light.
They should, right?
Those photoresistors are 4.
And near the light, what happens?
They start to bump into each other.
They bump and deflect, always trying to get towards the light, like kids scrambling for
the candy that's just burst forth from a pinata.
Voila.
You can see that this group of 10 nerds is now swimming in a coordinated conga line around
the light.
The key result here then is that you need agency, that closed loop, that shared goal
to get coordinated movement.
What the nerds show us is that, at least in some cases, you don't need to sense other
agents in order to swim in a coordinated manner.
Sharing a goal and colliding with each other are all you need to create teamwork.
I know it sounds crazy, right?
But this work on nerds helps us understand that sharing a goal is a key principle in
swarm robotics.
But goals can be shared in a swarm without all of the robots having to act in the same
way at the same time.
And this allows more complicated tasks to be accomplished, like mapping a novel environment
and then navigating through it.
Justin Werfel and his colleagues at the Weiss Institute for Biological Inspired Engineering
at Harvard University have built termite robots that share a goal of building a structure,
have decentralized control, and work like termites to build a structure much larger than themselves.
Each robot carries with it a set of traffic laws, if you will, tell it where to go based
on where it is on the structure and how much of the structure has already been built.
If you think of this as a set of navigational rules, then the plans for the building are
an ever-changing map.
The traffic laws allow each robot to pick up a brick, carry it, and then place it where
it is needed to grow the structure.
The termite robots can sense a brick, the path along the structure, and the number and position
of bricks in that structure.
And they can also sense each other's presence and can adjust their behavior accordingly.
They sense the path using infrared sensors.
With an accelerometer, they detect their own tilt angle when climbing, and they sense the
structure in other robots using sonar.
For actuators, the robots have hybrid wheel legs called waggs, which work well for climbing.
They have an arm that can collect and place a brick also.
And each robot has a set of simple behavioral modules from which its continuous behavior
is built.
Pick up a brick, add a brick to the structure, detect nearby robots, move forward on the
structure, turn on the structure, or circle the structure when on the ground.
All the instructions for each robot are identical, fixed, and held on board by each robot, and
this is decentralized construction without a supervisor.
The key to this whole process is that the current state of the structure itself gives
information to each individual about what to do next.
The termites and the termite robots communicate with each other by changing the world.
When insects do this, we call the process Stigmergy.
Not only is Stigmergy fun to say, it's also fun to understand.
It comes from the Greek roots stigma, meaning mark or sign, and ergon, meaning work.
So Stigmergy is literally the mark or sign of work done so far.
This concept was actually invented in 1959 when thinking about termites.
Think of Stigmergy in this way.
Every time you change something in the world, you are leaving instructions for other agents,
including your future self.
You do not need to go tell other agents what to do or give them special signals.
The change in the world contains the information.
You are coordinating the group indirectly.
What we've seen in this lecture is that some set of behaviors and tasks, coordinating groups,
and working together to build structures can be accomplished by robots.
Robots help us know and understand exactly how to create the behaviors and carry out
the tasks.
Some robotics is a field barely in its infancy, but it has vast potential.
Let me just mention three areas to expect future developments.
First, scaling up to large swarms.
In 2014, a Harvard team demonstrated the first robot swarm with over a thousand robots,
which they called kilobots.
A surprise finding from that work was how often robots fail and do other things not
captured in swarm intelligence simulations run solely on a computer.
Swarms of large robots is another example of something that's coming down the road.
These are big robots, right?
So the individual robots in a swarm do not have to be of small size.
Research is usually done that way on small robots because it's cheaper.
But in 2014, the US Navy and the Coast Guard ran a trial using a dozen full-sized boats
fitted with sensors and software to act as a protective outer line of defense to swarm
against attacks such as the Yemen bombing of the USS Cole in the year 2000.
Another thing on the horizon, diverse swarms.
The members of a swarm can be but do not have to be all alike.
Just like the software agents generated for Peter Jackson's movies, agents don't have
to be identical.
In fact, virtually any robot we've discussed in this course might ultimately be adapted
become part of a multifunctional swarm.
And what would it take to have that happen?
We've seen the success of the core principle of swarm robotics.
Simple rules enacted by each agent independently generate that complex behavior of the group
as a whole.
This means that we don't necessarily need a robot to be completely simple only that
is able to enact those simple rules.
We've also seen the importance of having the agents share goals and to be physically
together so that they can interact with each other and communicate.
So there's literally no reason in principle that we can't have robots of different sizes,
different bodies and different complexities all using the same set of swarm algorithms
to work together as a team.
Any robot can join a swarm.
