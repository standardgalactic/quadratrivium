We are now entering the Heartland of Science Fiction, Rossum's Universal Robots, Maria,
the Machine and Mench in Metropolis.
Isaac Asimov really got the robots rolling, so to speak, with characters such as Robbie,
Speedy, Cutie, Herbie, and Steven Byerly.
In Japan, an atomic-powered character translated into English as Astro Boy was a huge influence.
Star Trek, the next generation, had Lieutenant Commander data.
These are androids and personoids, replicants, terminators, cyborgs, and cylons.
In science fiction, we ask, how much are humanoid robots just like us?
Are we there yet?
We are getting close in terms of artificial intelligence.
IBM's Watson cognitive computing system can take in complex natural language, estimate
the intended meaning of each sentence, carry out a related deep search task, and then output
as natural language and informative response.
So traditionally, there has been a huge lag between AI and humanoid robotics.
This lag was famously observed by Hans Moravec in 1988 in what has been called Moravec's
paradox.
Quote, it is comparatively easy to make computers exhibit adult-level performance in solving
problems on intelligence tests or playing checkers, and difficult or impossible to give
them the skills of a one-year-old human when it comes to perception and mobility.
And yet, since 1988, we have made considerable progress, if not in closing the gap, at least
in advancing what robots can do in the physical world.
The DARPA Robotics Challenge of 2015 was all about building skillful, capable, and intelligent
humanoid robots for disaster response operations.
To achieve its goal, the challenge aimed, quote, to advance the current state of the
art in perception and decision-making, mounted and dismounted mobility, dexterity, strength,
and platform endurance.
Now, note the emphasis on perception, mobility, and dexterity.
We're still dealing with Moravec's paradox three decades after he stated it.
And as we've seen throughout this course, most of the advances in robotics have come
from building a body for a specific set of tasks in a given workplace.
Often, the workplace is modified to help the robot.
The science fiction approach to humanoids presumes we are trying to build robots just
like us, that is, we are designing robots without thinking of specific tasks or workplaces.
By contrast, projects like the DARPA Robotics Challenge take an intermediate approach, urging
us to consider explicitly defined tasks in a human environment that has not been modified
to help the robot, such as the site of a disaster.
Now, DARPA's 2015 Challenge specified several tasks for robots to negotiate during a disaster.
They had to be able to go up and down stairs, hold tools designed for humans, turn wheels,
knobs, and cranks also designed for humans.
Now, if you can't modify an environment built for humans, then give the robot humanoid features
to meet the environment halfway.
Each body part of a humanoid provides its own engineering challenges.
Hands turn out to be very complex to engineer, in part because each one has at least 15 degrees
of freedom, three for each finger.
Each degree of freedom needs to be under independent control, yet it's difficult to fit all the
electric motors, or pneumatic muscles, into the confined space of a human-sized hand.
Speaking of pneumatics, the Raphael robotic hand is an air-powered robot hand introduced
in 2009 at Virginia Tech.
The name Raphael is an acronym for robotic air-powered hand with elastic ligaments.
The use of compressed gas is what provides the energy and pressure to move the fingers,
allowing the hand to hold onto light bulbs or heavier objects with carefully controlled
levels of force.
Dexterity can also be achieved with miniature DC motors embedded in the hand itself.
In 2012, the German company Schunk released a commercial five-fingered hand.
Each finger has three degrees of freedom.
The motions are driven by small, geared DC motors embedded in the palm of the hand and
in the finger itself.
General control of these motors and feedback from pressure sensors on the surface of the
skin allows the Schunk hand to hold a key or a pin, can also shake hands safely with
the human.
Shadow Robot Company sells a Dexterous hand with 129 sensors.
Now this is a hand that has 20 actuated degrees of freedom, plus four additional joints that
are indirectly actuated.
The thumb has extra degrees of freedom, five, and the pinky has four.
So in combination, the hand has much greater dexterity.
In 2014, Shadow also began testing the addition of a 3D camera on the hand.
The idea is that visualize data about 3D shape of objects right at the level of the hand
and then use a 3D software model of what the hand is seeing and so then to model how the
hand should adjust to conform to the shape it is seeing in order to get a better grip.
The Dexterous hand can operate as an autonomous robot with actuators either air powered muscles
or electric motors.
But the hand can be attached to other robots and just as the hand can become a module to
an arm, fingertips can become modules to a hand.
Module fingertips have been designed by Cintouch, a company started at the University of Southern
California to have the sensitivity of a human finger.
Compared to other robotic hands, the biotack fingertips have four sensors whose impedance
changes in response to deformations of an elastic skin layer and a fluid layer below
the skin.
They have vibration sensors that can detect objects sliding across the surface and they
have thermistors that can transduce temperature and heat flow.
But as we always see in robotics, there are always trade-offs.
For many tasks, the only important aspect of the human hand is the opposable thumb and
finger that's needed for grasping.
We see a much simpler approach to the finger and opposable thumb in Baxter.
Let's take a look at Baxter's parallel grippers.
I've got these fingers here on the table that are part of his parallel grippers that we
take and then actually install on Baxter's hand.
So this is the end effector here and it has one degree of freedom which I'll show you
in translation, out and in.
So this may be all you need for picking up different types of objects.
So with Baxter, you can swap out different sizes of fingers as I mentioned.
They can vary in their length and in their width.
So the size of things that can be gripped this way and also how deep you can go into
something like a cup in order to grasp it.
Moreover, a great thing about robots is that we don't need to limit their workings to
whatever humans happen to have.
A great example of breaking out of the human box is the vacuum cup gripper.
It uses a vacuum cup and a negative pressure that comes in here in order to be able to
suck on to things.
In other words, it sucks, I know, but in a good way.
So factory managers have long known that a vacuum cup gripper can grasp things that are
sometimes really hard for fingers to manage, flat surfaces or smooth shapes.
But we haven't been putting grippers like that on humanoid robots until much more recently.
Also, you may have noticed that Baxter sits on a pedestal rather than on legs.
This is a design decision made by Rethink Robotics based on the following reasoning.
First, if you have legs, you have to balance on them, right?
And that takes energy and control.
And second, the balancing is active process that requires a lot more going on and a lot
more weight.
So by putting Baxter up on a pedestal, so to speak, Rethink is making sure Baxter is
stable.
Robotic legs that mimic the gait of a human walk with incredible bio-realistic functioning
were introduced in 2012 by researchers at the University of Arizona.
Now humans have a neural network in the lumbar region of the spinal cord that controls and
produces rhythmic neural signals for the muscles responsible for routine walking.
So the researchers created a simple robotic version of the system where feedback from
load sensors on the legs provided information to a central pattern generator which varied
the rhythm of its response and hence the frequency of walking.
Now the earliest humanoid robots relied on something called static walking, which was
more of a shuffle than a walk.
A nice example of static walking is Robo sapien, one of the most famous of humanoid robot toys.
It's made by the Huawei company and millions of these have been sold since their introduction
in 2004.
Robo sapien is controlled remotely and it's also famous for a new autonomous mode where
it can move around the house on its own avoiding objects.
The Robo sapien class of robots was invented by Mark Tilden, who is known for developing
robots with solid state electronics that don't need an embedded computer.
This sensory motor circuit right here is an example of a robotic system with sensor and
actuator that doesn't have a computer and the controller.
Let me just turn it on and get it going.
This was made by my colleague Larry Doe.
So if you look up close here, you can actually see right here there is a photo resistor.
I'm getting in the way.
And if I put my finger on the photo resistor, you can see what I'm actually doing is covering
it up so there's not much light on it.
You can see that system, the whole thing, turns towards this part of the laboratory.
I let light fall on that photo resistor and it turns back.
So there's a simple set of connectors in here and I'm going to turn this off and let you
see the circuit board that drives this.
There's no computer in here, right?
So there's simply input from that photo resistor that goes into that circuit and that circuit
then converts that photo resistor signal into control signal for the turning of the motor.
Now Tilden's approach to balance RoboSapien was to start with robots that were actually
stable statically so they could stand without having to be actively controlled.
Their feet are shaped like shallow cups to allow for the rocking that gives clearance
for the leg to swing forward.
So like this, right?
So this is a very simple and elegant solution to the problem of bipedal locomotion but it
hasn't been scaled up for larger robots.
The problem with static walking is that involves a lot of rocking side to side.
In larger humanoids, this movement sideways takes up energy better spent moving forward,
right?
So rocking is energetically inefficient.
Also rocking creates instability that is liable to make your robot fall over.
Well that's fine for a toy, it's very dangerous for a large, heavy humanoid.
In fact, fear of falling is why you see so many androids actually tethered from above
when they're in prototype stages.
So the key to stable walking is to keep your body's center of mass between the support
provided by your feet.
Let me show you what I mean.
If I move sideways while I walk, each step I risk tipping over sideways, right?
There's this critical position where I am balanced.
Go a smidge further and I'm in trouble.
Now what's really cool for robotics is that just like my feet have sensors, to let me
know when I'm tipping over, we can put pressure sensors on the feet of our humanoids.
Now I'm going to show you the bottom of my shoe, forgive me.
So if we think about putting pressure sensors on my shoe, we would want to make sure that
there were three pressure sensors on the inside, middle, and outside here across the ball of
my foot.
And we would want to make sure that the highest pressure read was never on that outside because
then we know we're tipping over.
So that's how we might design a smart foot for our robot.
So if we felt like, uh-oh, pressure on the outside, we have to do something, right?
And I just did it there.
I went, whoa, I put out my arms to begin to balance.
So when we are actually walking, we use our legs as inverted pendulums.
And so what this means is from the time our heel strikes, till the time we toe off, I'm
using my leg to pivot my mass up and over my foot.
And you can think of this as a pendulum now, an inverted pendulum, over which I tip.
Boom, like that.
So the key to the dynamics of this motion is to coordinate the transfer of momentum from
one leg to the other.
You may have heard walking is falling and catching yourself.
So we always have to keep the center of pressure inside the supports of our feet.
The control of the center of pressure in walking was first described in a series of papers
during 1968 to 1972 by a Russian mathematician, Mirmir Vukobratovich.
Because he was focusing on gravity and inertia for legs of various lengths, he called the
critical position of when you head stability the zero-moment point.
The word moment here refers to the product of force and the length of a lever like a
leg.
So by getting us to think about walking as the physics of transferring momentum between
inverted pendulums, Vukobratovich provided a method for calculating how to control the
overall motion of the body.
And not just the legs, right?
Part of the balancing act that takes place is to swing your arms and move your torso
side to side.
Now you can actually feel what your stomach muscles do and how they help in the control
and balance of your body if you put your hands on the sides of your belly and walk.
So I can do this and I can feel the muscle contractions alternate.
So the biomechanical take-home message is that even though we think about walking as
something we do with our legs, walking involves the whole body.
So the robot that's often considered to be the world's very first self-propelled bipedal
humanoid is Wabot 1, who stepped into robotic stardom in 1973.
Now Wabot was a shuffling robot that didn't use the zero-moment point concept to help
with balance.
But researchers led by Ichiro Kato at Waseda University in Tokyo built a series of Wabots
over the years and by 1984, Wabot model WL-10RD did use zero-moment points and in so doing
became the first robot to achieve dynamic walking.
Dynamic walking is a balancing act.
The robot Atlas, produced by Boston Dynamics in 2013, does an amazing job of balance.
While Atlas is balancing on one leg, no mean feat in and of itself, it gets hit with a
20-pound weight from the side and it doesn't fall over.
Atlas is able to maintain its karate kid pose as a stable inverted pendulum by virtue of
making rapid adjustments to its center of mass and in turn to that center of pressure
on the foot.
Arms outstretched are the key.
When the body's center of mass is displaced by the transfer of momentum from the ball
to the body, the arms react to move the center of mass back when it's came.
The important functional point is that to balance standing on one leg or even two is
an active process.
I'm constantly using my muscles, sensing when I start to fall, sensing the position
of my joints and the tension in my muscles.
If I were to stop doing all of that work, I would just crumble to the ground.
So human-style standing and walking are statically unstable modes for self-supporting humanoids
with jointed legs.
Now that's a technical way of saying that biped robots moving like a human will fall
down unless they actively balance.
You can think of the problem like this.
It's like trying to balance on a bicycle that is not moving.
One solution for the problem of a statically unstable structure is to put it in motion.
Passive dynamic walkers show this principle in action.
Using only gravity, a jointed linkage system will walk on its own if it's arranged just
right on a tilted surface.
Passive dynamic walking is dynamically stable because one leg catches the falling system.
Atlas, our Karate Kid balance master, also has impressive stability when walking.
Just like when we saw Atlas balancing, it can sense changes in the center of pressure
on its feet and make rapid adjustments to its center of mass.
So Atlas can keep its balance while walking with obstacles put on its path that it has
to step over.
And more impressive is its ability to keep its balance while walking across a very uneven
surface of good sized rocks.
But how was Atlas walking?
It was using what we call groucho walking in human biomechanics.
Let me give you that demonstration.
With a groucho walk named after the famous comedian, you keep your center of mass level
and you never pivot up and over your planted foot.
Your knee never straightens.
We see groucho walk in Honda's Asamo humanoid robot too.
Now why the groucho walk?
Because it's very stable.
The robot is squatting all the time, so it has a lower center of mass.
Also, a groucho walk allows you to move with precision that you don't normally have with
an inverted pendulum walk.
And this is also what you find in Tai Chi where you learn to stand and move with your
legs bent.
Again, the idea here is all about control and stability.
Let's think about hands, arms, and legs and what they need to do in the context provided
by the DARPA robotic challenge.
First responders in a disaster.
A challenge is that you need to get inside a burning building, so you need to open a
door.
Notice you don't need to have five fingers to do this, but it helps if your end effector
can grasp the handle with opposable fingers and then twist.
Now twisting a handle is something that we do with our wrist and our arm.
To twist a round doorknob, you need one degree of freedom rotation in your wrist.
To rotate a door with a handle, you need another degree of freedom in your elbow to do that.
Another challenge, you need to get to the source of the fire, but you encounter stairs.
If you have the Tai Chi Groucho Walk, stairs can be traversed and you can make it down.
Now if you've ever broken a wrist or had a bum knee, you know that doors and stairs can
be real challenges.
The fact that we humanoids, who can handle so to speak these challenges, shows just how
far we've come to closing the gap between brains and physical skill identified by Moravex
Paradox.
Another frontier in humanoid robotics focuses on how the body looks.
Looks turn out to be very important for how we react emotionally to robots.
While the promise of humanoid robots is that robots can fit immediately into our workplaces
and our homes, we won't want them around unless we feel comfortable with them.
Imagine if a robot scowled back at you or was hideous, or what about a robot clown, can't
sleep clown, lead me.
Now some folks really get freaked out by clowns.
If your affinity for a robot is low, indifferent, or negative, that eventually means no attention.
No attention means no communication.
Professor Hiroshi Ishiguro of Osaka University in Japan addresses the problem of affinity
head on.
He's working with a company called Kokoro to build highly realistic humanoids they
call actroids.
This actroids model name is Geminoid and is built to gaze, blink, turn, gesture, and interact
verbally with people.
What you can't see here is the woman behind the curtain, just like in Oz.
Geminoid is remotely controlled by a human.
When Geminoid speaks, it's the voice of the human in the control room.
Now in many ways these first generation Geminoids are like animatronic figures that you may
have seen at museums and amusement parks.
Actroids are meant to be interactive though, in a way that animatronics are not.
Professor Ishiguro is interested in using actroids to probe deep questions like what
is the essence of human being.
One of Ishiguro's research areas is to have a human be remotely present with another
human through one of the actroids.
Now when Ishiguro talks about the human inhabiting the remote android, this sounds a lot like
the Avatar movie.
Actroids have come in many different models and more recent ones like actroid-sit have
conversational autonomy.
Actroid-sit is getting better at social communication thanks to such features as interuptability.
Researchers found that humans tended to interrupt or to try to redirect a conversation 25% of
the time.
They became more successful in conversation once it could stop its current remarks and
transition immediately to the new topic.
Researchers also refined Actroid's repertoire of 18 gestures to adapt to the location of
the human speaker.
They called this motion parameterization.
The motion of the robots have parameters that change based on where the speaker is located.
Additional refinements might be possible based on the height, posture, and even body language
of the speaker.
Actroids are also being used as actors in stage performances, cool or creepy.
Different people react differently depending on the robot.
In 1970, a robotics professor, Dr. Masahiro Mori, working at the Tokyo Institute of Technology,
created a hypothesis that captured what turns out to be a complex relationship between the
level of our affinity for a robot and how human-like it is.
He proposed that as a robot becomes more in appearance, our affinity for it grows at first.
But as soon as the robot gets very close, but not quite to human likeness, what you
might see in a scary mask, for example, then the appeal of the robot plummets quickly to
creepy.
Now, Professor Mori called this small region of disgust and creepiness the uncanny valley.
Fictional robots such as Wall-E or Robbie the Robot from Forbidden Planet, a landmark
movie in the history of cinema, by the way, do not look very human and are clearly machines,
not creepy.
But in the 1927 German film Metropolis, a human who sees Maria the Robot for the first
time recoils.
How do you read his expression?
Is that disgust?
Is it fear creeped out?
This is clearly the uncanny valley for him.
So Mori, all the way back in 1970, knew that affinity would matter if we wanted humans
to accept robots and be willing to work together with them or to purchase them.
However, some researchers now think that the location of the uncanny valley might actually
change over time or depend on the culture.
Maria the Robot may have been uncanny back in 1927, but some humans now regard Maria
as the most beautiful robot in the world.
And apparently, Maria inspired C-3PO, a good robot in Star Wars.
Let's compare Kismet and Jibo, two real robots created by Dr. Cynthia Brazil, one of the founders
of the field of social robotics.
Kismet is rather close to the uncanny valley.
Jibo, by contrast, is very far from the uncanny valley.
It's small, and the sleek design does not have humanoid features.
And yet its movement and style of interaction are intended to create a friendly set of emotional
reactions for most humans.
So what next for humanoid robotics?
Smooth transitions from one kind of movement to another from walking to climbing stairs,
for example, changing pace, stopping on a dime.
Those will be important.
Another important thing will be allowing the arms to actually carry objects or perform
other tasks while the legs are continuing moving the entire body in various ways.
A third area of work will be thinking fast and moving fast.
Those are two related challenges we see being addressed at the annual RoboCup, the robotic
soccer contests that are held around the world.
Now humanoid robots made by the Aldebaran Company, the maker of the Social Robot Pepper,
are a favorite among robotic soccer teams.
And now comes with two cameras, an inertial measurement unit, touch sensors, and four
directional microphones, and two sonar rangefinders.
Now is 58 centimeters tall and has 25 degrees of freedom.
When now teams play soccer, they are moving autonomously.
They use cameras to detect the ball and teammates, and that allows them to take on the opposition
doing the same thing.
So their inertial measurement unit measures acceleration so they can keep track of their
balance and their position.
Teams of humanoids are another focus.
Working with humanoids that look a lot like Baxter, General Motors is working not on soccer,
but on coordinating the reaching and grabbing, good old fashioned pick and place of multiple
humanoids working together.
This problem is a lot like playing soccer or doubles and tennis.
You have to be able to know where you are, where you need to be, where others are, and
where they're likely to be.
Again, think fast and move fast.
These are two essential ingredients for humanoid robots.
Better sensors, better actuators, and better controllers will all be needed and will continue
to be the foci of research and innovation.
Krasheim proposes three imitation games as a way to judge where we are with humanoid robotics.
The first of these is the communication imitation test, and this is the classic Turing test.
But with advances in our understanding that communication includes the non-verbal, the
emotional, and the gestural, we should broaden this to be a physically enacted test.
Test number two, the physical imitation test.
Does the humanoid look and feel like a real human?
Does it have the dexterity of a human?
Test number three, the autonomous behavior test.
Does the humanoid behave on its own?
Does it learn and adjust as new circumstances arise?
While it's useful to have these categories, we can bring them together to give what Stephen
Harnad has described as the total Turing test.
We can distinguish a robot with all it does and its physical embodiment from a human.
If we cannot, then that humanoid will have passed the total Turing test, and we will
truly have taken science fiction and made it science fact.
Science fiction dreams about robots being just like us have inspired many roboticists,
and we can expect the skill of humanoid robots to improve in challenging physical tasks,
perhaps even closing the gap of more of X-paradox identified between their capacity for artificial
intelligence and their physical skill.
But there are always trade-offs, even with humanoid robots.
Let's remember three points.
First, humanoid robots don't need all human abilities in order to accomplish valuable tasks.
A humanoid rescue robot in a disaster situation may be able to save lives even if it only
does a few things well.
Two, humanoid robots don't need human biology to do what humans do.
For example, humans have the power of speech thanks to a descended larynx in a throat
with 100 different muscles and a descended hyoid bone below the tongue.
But that doesn't mean that our robots need those particular structures in order to communicate
with us.
Point number three, humanoid robots can be given other body parts and abilities that
humans don't have, like vision in our wrists or suction cups for hands.
In short, many of the advances in humanoid robotics may come from not attempting to replicate
humans.
