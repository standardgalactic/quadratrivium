You
you
Dr. Patel is a professor of psychology at Tufts University.
He received his Ph.D. in Organismic and Evolutionary Biology from Harvard University.
The author of the award-winning book Music, Language and the Brain, Dr. Patel is active
in education and outreach, having given more than 70 scientific lectures and colloquia
and more than 20 educational and popular talks.
His major contributions have included research on music-language relations, the processing
of musical rhythm, cross-species comparisons, and relations between musical training and
neuroplasticity.
For many of us, music is one of the great joys of life.
It seems to tap into something deep inside us.
It's found in every human culture, from the largest societies to the smallest tribes.
It can have powerful effects on our emotions.
People of every age respond to it.
Babies are soothed or excited by it.
Young adults dance for hours to it, and older adults can be mentally transported back to
their youth by the vivid memories it evokes.
It's part of our most important rituals, including those marking birth, weddings, and death.
For the past few centuries, it's been the medium for some of humanity's greatest works
of art.
In 2009, archaeologists reported that world's oldest known musical instrument, a bone flute,
found in a cave in southwest Germany that dates to about 40,000 years ago.
This is not long after modern humans first reached southern Europe from Africa.
The flute was made from the wing bone of a griffin vulture, and the last person to play
it was an Ice Age hunter-gatherer who lived at a time when there were still two human
species alive in Europe, our own species, and the Neanderthals.
The descendants of this flute player wouldn't develop agriculture for another 30,000 years.
This flute actually predates the famous cave paintings of southern Europe.
Yet when those cave paintings were still fresh, this flute was already a 5,000-year-old archaeological
relic, as distant in time from the cave painters as the builders of the ancient pyramids of
Egypt are from us today.
And here's the really remarkable thing.
When archaeologists looked closely at this flute, they realized that making it required
a complex series of steps and a lot of time.
This flute wasn't a new technology.
Musical behavior probably dates back much further in time.
Most musical behavior, like singing, doesn't leave any trace.
Music could date back to the origins of cognitively modern humans in Africa some 60,000 years
ago before they spread out to colonize the globe.
So music is universal, powerful, and ancient.
Yet even though it's intimately woven into the fabric of human life, music remains mysterious
in many ways.
How and why did musical behavior originate?
Are we musical today because music had some survival value in our species past?
How can mere tones produced by mechanical devices like flutes or pianos or guitars come
to sound so alive, so full of human thought and feeling?
Why does music with a beat give us the urge to move and dance?
How much of our sense of music is inborn versus acquired by experience?
New perspectives on old questions like these come from the young field of cognitive neuroscience,
which integrates the study of human mental processes with the study of the brain.
The past 20 years have seen incredible growth in this field, thanks to new tools for exploring
the mind, including non-invasive brain imaging, which allows us to study brain structure and
activity without opening up the skull.
Cognitive neuroscience has not only helped us address old questions, it has allowed us
to ask new ones.
For example, do the brains of musicians differ from non-musicians?
Can musical training change the structure of the brain?
What is the relationship between music and other mental abilities, such as language?
Can listening to music or making music improve brain function in patients with brain damage?
Can musical training significantly impact the cognitive and neural development of children?
For each of these questions, there are now enough theories and data to create a course
like this, something that would simply have not been possible a mere two decades ago.
Studying music from the standpoint of neuroscience immediately runs up against a serious challenge.
Music is a human universal, but it's also tremendously diverse in its structure and meaning
across cultures and across time.
My favorite example of this combination of universality and diversity comes from a golden
record that was attached to the two Voyager spacecraft, which were launched in the 1970s
to explore our solar system.
Scientists knew that these two spacecraft one day leave our solar system, run out of
fuel, and drift into the cosmic ocean that separates the stars.
A team led by Carl Sagan and Ann Drewian, who later created the TV series Cosmos, put
together a collection of recordings to represent important universal aspects of human life,
a message in a bottle to any alien species that might come across these spacecraft.
These recordings included 90 minutes of music from around the world.
If you listen to those recordings, which you can now do on the internet, you'll get a small
sample of how diverse human music can be.
You'll hear Indonesian gamelan, a wedding song from Peru, Bach's Brennenberg Concerto
No. 2, a Pygmy Girls initiation song from Zaire, and Chuck Berry's Johnny B. Good, just
to name a few.
So human music is very diverse in how it sounds across cultures.
The field of ethnomusicology has documented this diversity in great detail and has shown
how music is made and what it means to listeners can be very different from one culture to
the next.
These studies have put to rest the 19th century notion that non-Western musics are primitive
and represent a more rudimentary stage of development compared to Western art music.
The rhythmic structure of West African music, for example, can be much more complex than
the rhythms we hear in most Western classical and popular music.
Music is not only diverse across cultures, it is also diverse across time.
Western art music provides a spectacular example of this.
Imagine if we could bring Mozart back to life today and take him to hear a string quartet.
Let's imagine the first piece he hears is a piece composed by Haydn, dating from 1790,
which sounds like this.
This type of music would sound quite familiar to Mozart.
He and Haydn were friends, and the piece was composed when Mozart was alive.
Let's now imagine that the next piece Mozart hears is a string quartet composed in 2012.
This is a piece by the American composer, Jason Carl Rosenberg, who earned his PhD in
music at the University of California, San Diego, and studied with the Pulitzer Prize-winning
composer, Roger Reynolds.
Rosenberg's string quartet sounds like this.
Imagine the look on Mozart's face.
Would he be thrilled?
Would he be horrified?
We don't know.
Just as modern visual art is wildly different from the paintings Mozart was familiar with,
contemporary art music is wildly different from the music he grew up with.
These facts about the cultural and historical diversity of music mean that music is a moving target,
and this presents a challenge for the study of music in the brain.
Neuroscience typically doesn't deal with behaviors that vary so dramatically across cultures and across time.
Brain science usually focuses on phenomena which are culturally and historically stable.
Diseases are a good example.
Parkinson's disease in America is a lot like Parkinson's disease in China, or India, or New Guinea.
And Parkinson's disease today is a lot like it was in Mozart's day.
For that matter, it's probably a lot like it was when our ancestors were playing bone flutes 40,000 years ago.
Music just isn't like this.
It's really different in different cultures and time periods.
So how then can the study of music in the brain make any progress when music itself is so changeable?
Fortunately, there is a way that brain science can acknowledge the great diversity of music
and still move forward in a way that leads to interesting discoveries about music in the mind.
This way involves making a key conceptual distinction,
one that has been emphasized by Professor Henkian Honig of the University of Amsterdam.
This is the distinction between music and musicality.
Music, like other arts, is a social and cultural construct that strongly reflects the historical context in which it's created.
Musicality is the set of mental processes that underlie musical behavior and perception,
and these are much more stable across place and time.
Let me give you an example of musicality.
One of the things humans do when we perceive music is recognize the similarity of melodies when they are transposed,
that is, shift it up or down in pitch.
Let's listen to a melody, followed by a transposed version of that melody,
so you can perceive how similar these melodies sound to us.
Here's the melody.
And here's the transposed version.
Those two melodies didn't have a single pitch in common,
yet for most of us, they're the same melody, just played in different registers, that is, transposed.
We recognize that similarity without any conscious effort,
just as you would instantly recognize a familiar tune, like the Happy Birthday song,
if it was played on a piccolo or a double bass,
even if you had never heard it played that high or low before.
Recognition of transposed melodies is a component of musicality,
one of the mental processes involved in music perception.
We have no reason to believe that this ability varies radically across cultures or historical eras.
Many types of music, across culture and time, rely on transposition in creating musical patterns.
The ability to recognize transposed melodies develops spontaneously in humans.
It doesn't require any special training in music.
One study has shown that even six-month-old infants have this ability.
Judy Plantinga and Laurel Trainor showed that after a six-month-old hears a melody over and over,
they prefer a new melody and grow bored with the old one.
And that reaction is the same, whether the old melody is presented at its original pitch level
or at a new pitch level transposed up or down.
This means that for infants, just as for adults,
an important part of the identity of a melody is not the absolute pitch of the notes,
but the relative pitch pattern that the notes create.
That is the pattern of upward and downward pitch movements across the note sequence.
This pattern stays the same when a melody is transposed.
So here we have a component of musicality,
the ability to recognize melodies on the basis of relative pitch patterns.
We take this ability for granted.
To us, it seems unremarkable, just business as usual for a human brain.
But two lines of research on this ability have revealed surprising and interesting things
about this aspect of human musicality.
The first line comes from comparative psychology,
the study of mental processes in other species,
In the 1980s, the psychologist Stuart Hulse and colleagues at Johns Hopkins
began a series of experiments with European starlings
to determine if they could recognize the similarity of transposed melodies.
He chose them because they use complex sound sequences in their own songs,
and they're also good mimics,
which suggested they had the ability to process artificial sound patterns.
Hulse trained the starlings to discriminate between tone sequences that rose or fell.
They were rewarded for pecking a key when they heard a rising series of pitches,
and for not pecking when they heard a falling series of pitches.
The birds were trained on this discrimination using several ascending and descending patterns
in a particular frequency range.
For example, they were taught to peck when they heard this,
or this,
and not peck when they heard this,
or this.
The birds succeeded in learning this discrimination.
Hulse then tested to see whether the songbirds could generalize this discrimination,
that is, apply this learning to new stimuli.
These were rising and falling to a certain level,
and these apply this learning to new stimuli.
These were rising and falling tone sequences that were presented outside of the training frequency range.
These new sequences preserved the pattern of relative pitch between notes,
but presented them at novel absolute pitches higher or lower than heard in training.
For example, a rising pattern like this
would have been higher than the birds ever heard during training,
and a falling pattern like this
would be lower than the birds ever heard in training.
If they recognized ascending and descending patterns on the basis of relative pitch,
they should have pecked for the novel rising patterns and not pecked for the novel falling patterns.
How did the birds do?
To quote Hulse, the Starlings failed utterly when required to transfer the rising falling pitch discrimination
to frequencies outside the training range.
Later research showed that other songbirds, like mockingbirds,
also do not recognize transposed melodies, even though humans easily do.
One reason that these findings are so interesting
is that these animals make and perceive complex sound patterns as part of their natural behavior,
so it's not that they have a primitive auditory system or can't remember sound sequences.
Also, studies have shown that they are able to make generalizations
about sound patterns on the basis of other kinds of relations than relative pitch.
This research shows that relative pitch perception doesn't just automatically emerge
in a brain that does complex sound processing.
A certain kind of auditory system is needed.
Could it be that it requires a primate auditory system?
A primate brain is much larger than a bird brain,
so you might expect that it could do more complex processing.
Also, the auditory system of monkeys is thought to be very similar
to humans in terms of basic neuroanatomy in neurophysiology.
Yet when other researchers took Hulse's experimental design
and tried it with Capuchin monkeys,
they found the monkeys were like the starlings.
They did not recognize a tone sequence when it was transposed out of a training range.
A more recent study with Rhesus monkeys suggested that they may have a limited ability
to recognize transposed melodies.
So unlike the bird research where multiple studies point to the same result,
with monkeys there are only two studies of relative pitch perception
with contradictory results.
Research on relative pitch perception in monkeys is too young
to draw any firm conclusions.
At this point, however, it's distinctly possible that our spontaneous tendency
to use relative pitch and melody perception is uniquely human.
One thing I love about cross-species studies
is that they help restore our sense of wonder with the familiar.
Before Hulse and colleagues did their experiments,
few researchers would have suspected that perceiving the similarity of transposed melodies
would be difficult for songbirds or for any non-human primate.
The Belgian surrealist painter Magritte once said,
too often by a twist of thought we tend to reduce what is strange to what is familiar.
Comparative psychology reminds us that what's familiar to us
in terms of music perception may actually be quite strange
from the perspective of other species.
This effortless ability we have to recognize transposed melodies
may be quite odd when viewed in an evolutionary perspective.
Why would we have such an ability?
Why would our brain have been modified over evolutionary time
to make relative pitch so natural to us?
I'd like to suggest that it may stem from an unusual feature of our bodies.
Once again, it's a feature we're all so familiar with that we take it for granted,
and it's only a cross-species perspective that reveals how strange it is.
This feature is the sex difference between men and women in the pitch of the voice.
In humans, male voice pitch lowers dramatically during puberty.
Stimulated by testosterone, male vocal folds become longer and thicker
so that they tend to vibrate more slowly, thus producing lower pitches.
This is reflected by the growth in the male's Adam's apple during puberty,
which is the cartilage that covers the larynx or voice box.
The Adam's apple sticks out more in men than in women
because the vocal folds have grown larger.
The resulting difference in average voice pitch is remarkable.
Due to the changes in puberty, adult male voices end up being about 50% lower than females,
way out of proportion with our body size difference, which is only about 8%.
This sex difference in vocal anatomy is universal in humans
and very unusual among primates,
and it may have set the stage for our facility with relative pitch perception.
Why is that?
The big difference in voice pitch between men and women
means that when we communicate with each other,
any pitch patterns we make with our voice are going to be far apart in absolute frequency.
If I say, birds make good pets,
you know I'm asking a question because my voice pitch goes up at the end of the sentence.
If I drop my voice pitch at the end, the question turns into a statement.
Birds make good pets.
If a woman says the same sentence as a statement or a question,
her voice pitch will go up or down at the end too,
but her entire pitch pattern will be much higher than mine.
In order to recognize the similarity of her pitch pattern to mine,
when I ask a question or make a statement,
I need to process the pattern in terms of relative pitch, not absolute pitch.
So one scenario for the evolution of relative pitch perception
is that it emerged due to the need to recognize pitch patterns
coming from individuals whose average voice pitch is very different from ours.
This could explain why birds and monkeys may not have this ability.
Many do use pitch patterns to communicate,
but they don't have big differences between individuals in average voice pitch.
Music
Earlier I mentioned that two lines of research have revealed surprising
and interesting things about how humans process relative pitch,
which is a core component of musicality.
The first line of research came from comparative psychology,
and the surprise was that this ability is not typical
of how other species process pitch patterns.
The second line of research comes from cognitive neuroscience
and has taught us surprising things about the brain circuits
that process relative pitch.
What brain regions support this ability?
The auditory system is very complex.
There are multiple neural processing centers between the ears and the cerebral cortex.
These brainstem and midbrain centers are very similar in anatomy
and function between humans and other mammals.
They are involved in basic functions like sound localization
and integrating auditory and visual signals.
So if we're looking for specializations of brain structure
or processing in humans,
these are areas that are probably not good candidates.
When neuroscientists study the auditory cortex,
which resides in the temporal lobes on the left and right side of the brain,
they find multiple regions which have been named core, belt, and parabelt.
In general, the further one gets from the core region,
the more complex the processing that takes place.
For example, a neuron in the core region might respond
to a particular frequency as heard,
while a neuron in the belt or parabelt might only respond
if a particular frequency is followed by another particular frequency.
Neurons in these higher order regions are more interested in combinations of features
than in single features.
Neuroscientists know this because they are able to measure
the response of single neurons in the brains of animals.
This is generally not possible with humans.
While following this logic, one might expect neurons in the belt
and parabelt regions to be involved in relative pitch perception.
Since relative pitch perception is not just about which frequencies are heard,
but about relations between frequencies,
whether pitch goes up or down, and by how much.
In the year 2000, Robert Zittore and his colleagues
at the Montreal Neurological Institute
published a paper which supported this idea.
But their work contained a real surprise.
They studied patients who had part of their temporal lobes
surgically removed as a treatment for epilepsy.
In some of these patients, the surgery had removed portions
of the higher order auditory regions,
either on the left side of the brain or the right.
When the patients were asked to listen to two tones
and judge if they were the same or different,
a simple discrimination task,
they did as well as normal individuals.
But when they were asked to judge if the pitch went up or down,
which is a relative pitch judgment,
then the patients had trouble.
The surprise was that it was only the patients with right auditory surgery
who had trouble.
Patients with surgery on the left side were completely fine.
Thus, it seems that relative pitch involves a brain specialization
in the right auditory cortex of humans.
This finding lined up with other studies that Zittore and colleagues had done
suggesting that in humans, the right side of the brain
is particularly important for musical pitch processing.
You may have heard the idea that music is a more right brain activity,
while language is a more left brain activity.
Studies like this show there is a grain of truth
to this idea, though it's important to remember
we're just talking about one component of music cognition,
relative pitch perception.
As we'll see later in the course,
other aspects of music cognition do rely on both sides of the brain,
so it's a mistake to think that music as a whole is a right brain function.
This study with surgical patients,
which relied on people whose brains had been impacted by disease,
is an example of a classic method in neuropsychology
which far predates modern brain imaging.
Such studies can tell us if a brain region is critical to a particular mental ability
by asking if patients with damage to that region still have that ability.
In the last 20 years or so,
this traditional method has been complemented by new methods
of non-invasive brain imaging,
which allow us to look at brain structure and function in healthy normal individuals.
For example, functional MRI, or fMRI,
uses magnetic fields to measure increases and decreases in blood flow
in different regions of the brain.
When blood flow to a particular region increases,
we infer that there is more neural activity in that region,
because neurons in that region are consuming metabolic resources
that the blood is delivering.
In 2010, Zittorian colleagues used fMRI to study relative pitch perception
and got another surprise.
In this study, listeners heard pairs of short melodies
and had to judge if they were the same or different.
The members of a pair could be at the same pitch level,
or the two melodies could be at different pitch levels.
In the second case, listeners had to use relative pitch to recognize the similarity.
Listeners had their brains imaged while they did these melodic tasks.
Brain activations to the relative pitch task
were compared to the activation for the simpler task
to look for brain regions specifically involved
in perceiving melodic similarity on the basis of relative pitch.
The surprising result was that one of the key regions activated by the relative pitch task
was far outside of the auditory cortex.
It was a region in the right parietal lobe known as the intra-parietal sulcus.
Why would this region be involved in recognizing the similarity between transposed melodies?
This is a brain region that is known to be involved in visual spatial processing
and visually guided spatial tasks like reaching and grasping.
The researchers had not predicted this area to be involved in relative pitch processing,
but the evidence couldn't be ignored.
One thing that made this evidence so compelling
is that not only did this region light up
when people were doing the relative pitch task,
but the degree to which it lit up in an individual
correlated with how well she or he did on the task.
This strongly implies that activity in this brain region
is related to the ability to do the task.
Of course, it's not the only region that's involved.
Remember that the studies with patients showed that higher-order auditory regions
on the right side of the brain are important
for perceiving the direction of pitch change between two tones.
The fMRI study required people to judge relative pitch relations
across sequences of tones.
So relative pitch perception of melodic phrases
likely involves a network involving right auditory cortex,
right parietal cortex, and perhaps other regions.
This illustrates an important point about the brain basis of cognition.
Any reasonably complex cognitive task engages a network of brain regions,
not just a single brain area.
But again, why would a visual spatial brain processing area
be involved in relative pitch processing?
And in monkeys and humans, this area is involved
in integrating information from different senses
and in visually-guided grasping.
This is a great example of brain imaging revealing something unexpected.
But how do we make sense of it?
Interestingly, another visual task that activates this region,
the intraparietal sulcus of the parietal lobe, is mental rotation,
looking at two three-dimensional objects
and determining if one is a rotated version of the other.
Like relative pitch perception, this involves interpreting a sensory pattern
in terms of the relations between elements.
In vision, this could be important for programming
how you would reach out and grasp an object.
Thus, this brain area may support the ability to recognize patterns
that are transformed but which still retain their relational properties.
In humans, it seems that pitch processing has become connected to this ability,
likely via strong neuroanatomical connections
between auditory regions and this region.
This illustrates something very important about musicality.
Musicality involves brain networks that extend well outside of auditory regions.
We'll see this again and again in this course,
and it has deep implications for how music interacts
with other aspects of cognition.
For example, the fact that relative pitch perception recruits brain regions
also involved in spatial processing might suggest why musical sound
so often has multimodal associations.
In 2006, the psychologists Zohar Eitan and Ronnie Grunow
found that listeners could easily associate the rises and falls
of musical pitch contours with imagined physical movements
and that there were consistencies in what people imagined.
For example, listeners associated pitch rises with running or walking
and pitch falls with a falling motion.
The more general point is that from the brain's perspective,
music perception is not just about the auditory system.
It's about connecting sound processing to other things that brains do,
like moving, planning, remembering, imagining, and feeling.
I think music gets some of its great power from its ability
to link the auditory system to other brain systems
spread widely across the brain.
I think this power to co-activate many systems is one reason
music yields such rich mental experiences.
Those experiences were important to that Ice Age human
that played a bone flute 40,000 years ago.
And they'll still be important to humans
when the Voyager spacecraft and its golden record of our music
finally reaches the vicinity of another star,
which is estimated to happen about 40,000 years from now.
Music has always been and will always be part of the human condition.
The ability to process and enjoy it seems effortless,
instinctive, even primal.
But brain science suggests that this is all an illusion.
Behind the curtain of consciousness,
musical experience depends on a sophisticated mental machinery
with many parts, some of which are relatively new
in terms of brain evolution.
In this lecture, we've focused on one component of this machinery,
our capacity for relative pitch perception.
It's just one in a larger set of mechanisms
that underlie human musicality.
