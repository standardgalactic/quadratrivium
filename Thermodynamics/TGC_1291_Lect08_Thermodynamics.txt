Hello and welcome.
Today we're going to talk about the second law of thermodynamics.
We've already introduced the concept of entropy several times in previous lectures,
but today we're going to learn the thermodynamic law that deals directly with entropy,
and we'll discuss the important consequences of this law.
The key concepts I want you to feel comfortable with by the end of this lecture include
entropy itself, ordered and disordered states,
the idea of irreversibility, which is something we touched on in the last lecture,
and of course, the second law and what it means.
We begin this lecture with the very nature of the word begin itself.
That is, the nature of time.
What is the connection of time to various processes in nature?
I think that most of us would agree time is unidirectional, but why is this?
Take the following examples.
An egg drops to the floor accidentally and breaks apart.
Ice is placed into a coffee to make iced coffee.
The ice melts while the coffee gets colder.
A piece of paper catches fire and burns, turning into ashes.
What is common to all of these processes?
Do we ever observe these processes happening in reverse?
What we know intuitively is that in all of these examples, the reverse never happens.
The broken egg does not suddenly reassemble into an unbroken shell.
The coffee would never become warm while ice cubes form spontaneously in the glass.
And the burnt ashes would never unburn to turn into paper.
But why is this the case?
So time is unidirectional and intuitively related to all of these processes that occur naturally.
But the question we are really asking is, can we connect the unidirectionality of time to some property of these processes that occur?
The answer lies in the concept of irreversibility.
As I mentioned in the last lecture, all natural processes are to some extent irreversible
due to the presence of dissipative forces in nature.
No process is perfectly ideal or reversible.
But take the egg example.
In that case, would the process occurring in reverse, that is, the shattered egg reassembling,
would that violate any laws of physics?
Would it violate the first law of thermodynamics?
The answer is no.
The reverse process does not violate the laws of physics,
and the reverse process does obey the energy conservation of the first law.
In fact, as far as we currently know, all physical laws are time-reversible,
meaning that when viewed in reverse, no laws of physics are broken.
From our day-to-day experience, though, time definitely flows only in one direction.
As we'll learn today, the reverse processes do not happen
because they are statistically improbable.
We will need to use entropy to understand such statistics.
Entropy is our only measure for time moving forward.
Entropy serves as nothing less than the arrow of time
and explains the unidirectionality of all of these processes.
We can in fact think about the property called entropy,
which remember we write with the letter S in thermodynamics,
as the property that measures the amount of irreversibility of a process.
And here is an important postulate about entropy.
If an irreversible process occurs in an isolated system,
the entropy of the system always increases.
For no process in an isolated system does the entropy ever decrease, ever.
Let's think about the difference here with energy.
In an isolated system, remember, that would be a system that has boundaries
through which nothing can pass, not energy nor heat nor matter.
We know that energy will be conserved.
But we just stated that entropy may not be conserved in such a system
and it can only stay the same or increase.
This is a postulate for now,
which I'm going to return to more rigorously into a law shortly.
But first, let's take a look at a mathematical definition of entropy,
which I briefly alluded to in our last lecture.
Now, I gave those examples of irreversibility
in my discussion of the natural direction of time a moment ago.
As a quick reminder, the concept of reversibility
refers to a process for which the system is always in equilibrium.
There are no dissipative forces occurring.
In other words, no energy is lost to the surroundings.
There's no friction, no viscosity, electrical resistance, and so on.
And a reversible process can occur both forward and backward
with absolutely no change in the surroundings of the system.
And remember that when I talked about reversible processes,
I also mentioned that no such process exists.
It's only a theoretical concept
since all real processes dissipate some amount of energy.
But reversibility is an extremely useful theoretical concept
as we're about to see.
That's because if a process is fully reversible,
then I can define a change in entropy during that process as the following.
dS equals squiggly dQ divided by T.
So the change in entropy is the change in heat flow over the temperature.
Now, we can see from this definition
that the units of entropy are energy divided by temperature.
So in standard units, this would be joules per Kelvin.
Remember to always be aware of the units for any variable you're working with.
As we said in the last lecture,
we use the squiggly d to remind us that the variable under consideration is path dependent.
Also, standard notation is that we put the letters R-E-V as a subscript on the squiggly d
to denote that this relationship holds only if the path is reversible.
So if we want to know the change in entropy for a whole process,
then we simply integrate both sides of this equation.
The integral of the left side gives the final entropy minus the initial entropy.
Or we can just write it as delta S.
Here I use the capital Greek letter for delta
signifying to our mathematically inclined friends
that this is not a differential, but rather a full difference between some initial and final state.
Now, remember what it means when the integral over the differential of a thermodynamic variable
is just equal to taking the difference between the final and initial values of that variable?
You got it. It means the variable is a state function.
Part of what the second law does is to confirm the very existence of entropy as a state function.
Remember that heat is not a state function since it's path dependent.
Well, effectively, entropy is a way to change heat into a quantity such that it is a state function
and therefore no longer path dependent.
The origins of entropy go back to Rudolph Clausius.
Back in the day, that would be the late 1800s, scientists were trying to figure out a way
to understand and combine the ideas of Kelvin, Joule, and Carnot.
That is, they were trying to figure out why energy is conserved in thermodynamic processes
and why it is that heat always flows downhill in temperature.
That is, heat always flows from a hot body to a colder one.
Clausius defined the idea of entropy as I just described above.
Namely, the change in entropy for a process is the ratio of the heat exchanged in the process
with the absolute temperature at which that heat is exchanged.
Now, what this does is give a mathematical framework to the idea that heat always flows from higher to lower temperature.
This, by his new concept and definition of entropy, Clausius showed when an amount of heat Q
flows from a higher temperature object to a lower temperature object,
the entropy gained by the cooler object during the heat transfer is greater than the entropy lost by the warmer one.
You can see this from the math.
This Q over T-cold is greater than Q over T-hot since T-hot is larger than T-cold.
This was a really powerful and important result.
It allowed him to state that what drives all natural thermodynamic processes is the fact
that any heat transfer results in a net increase in the combined entropy of the two objects.
We know that heat flows this way simply from our own experience,
but with entropy we have a rigorous variable that establishes quantitatively the direction that natural processes proceed.
Before I move on, let me give a brief analogy to help visualize the role of dividing by temperature in our definition of entropy.
Imagine that out of the blue you decide to scream at the top of your lungs.
Let's say you scream, hey you.
Now, if you did that in a quiet library, then every single person, including perhaps a quite irritated librarian, would turn and look at you.
But if you did that in, say, a football stadium, right when a touchdown is scored by the home team,
in that case no one would even notice.
Tying this back to our thermal, the change in heat is our scream.
And the temperature is related to whether we're in a library or a football game or somewhere in between.
Dividing by temperature gives a kind of relative impact that the heat has on the system.
Okay, back to our natural processes.
All natural processes occur in such a way that the total entropy of the universe increases.
The only heat transfer that could occur and leave the entropy of the universe unchanged is one that occurs between two objects which are at the same temperature.
But we know that's not possible, since no heat would transfer in such a case.
All processes, that is all real processes, have the effect of increasing the entropy of the universe.
And that is the second law of thermodynamics.
We have that a change in entropy for a given process, call it delta S, which equals S final minus S initial,
is equal to the integral from some initial state to the final state of D squiggly Q divided by temperature for a path that is reversible.
Now, this equation allows us to write the first law of thermodynamics in a new way.
The first law states that the change in internal energy during a process is equal to the heat flow, plus all work done during that process.
Written in differential form, this amounts to D U equals squiggly D Q plus squiggly D W.
Now, as you can see, we're in a position to make a substitution here, since the second law gives us a relationship for squiggly D Q.
For a reversible process, it's equal to T D S.
And let's say the only type of work being done during this process is mechanical work, so pressure volume work.
Well, as we've learned, the work term for that is minus P D V.
In this case, we can now write out the first law as the following. D U equals T D S minus P D V.
Then again, remember, this holds for a process that is reversible.
If no work was being done at all, then we'd have the direct relationship between entropy and internal energy.
D U equals T times D S.
As a side note, check the units here to see that they work out properly. You should get energy on both sides of the equality.
And one of the important consequences of entropy being a state property is that since it's path independent, a change in its value can be obtained by artificially constructing any path.
Let's dig into this a bit more, since I know all this talk of reversibility and path dependence can be confusing the first time or two you hear it.
Don't worry, my students at MIT are just as confused. It usually takes thinking about this at least two or three times and see it used in specific examples before it all starts to make sense.
In essence, the second law of thermodynamics is about predictions. It's a law that predicts what processes will occur.
Let me repeat that crucial fact. The second law of thermodynamics makes predictions about what processes will occur.
There are actually a number of ways of stating the second law. I'll let you know when I come to other ones, but for starters, let's go with the most common and also simplest form of the second law, which goes as follows.
We consider a system and we call the rest surroundings. We could then say that the universe consists of the system plus the surroundings, since we said the surroundings was everything else.
Now, we can write the entropy of the universe as the entropy of the system plus the entropy of the surroundings.
The second law of thermodynamics states that the entropy of the universe, that is the system plus surroundings, always increases in an irreversible process and remains a constant in a reversible process.
That means that delta s for the universe is greater than or equal to zero for all processes. When the change in entropy is exactly equal to zero, then the system must be in equilibrium.
When it's greater than zero, it tells us the directionality of the process, that concept we discussed in the beginning of this lecture. That's the second law.
Let's put this law into action for a very simple system to determine equilibrium. Suppose I have a system of two blocks, call them block A and block B, which are in contact with one another but isolated from everything else and held at constant volume.
Now suppose that block A initially has a temperature Ta and block B has a different temperature, call it TB. When will heat transfer stop between these two blocks? When is the system at equilibrium?
Of course, we know intuitively that heat transfer stops when the temperature of the blocks are equal, but how can we prove this from thermodynamics?
In order to do that, the question we ask is as follows. What is the condition of blocks A and B when the point of maximum entropy is reached?
According to the second law, this will be the point at which the system is no longer changing its entropy, so the point where the derivative is zero.
Let's start with the fundamental equation for the change in entropy for a reversible process. Remember, this comes from the first law of thermodynamics assuming only work of expansion.
To get this, I simply take the equation I just showed you for du and divide through by temperature. This gives us ds equals du divided by t plus p times dv divided by t.
We can write this equation for each block, so we get an expression for the change in entropy of block A and a similar one for block B.
Now, I mentioned these blocks are not going to change their volume, so we can eliminate the dv terms. That's nice.
And that leaves us with ds for block A equals duA over ta and ds for block B equals duB over TB.
From the second law, when the change in entropy of the universe is equal to zero, the system will be in equilibrium.
Since the system is isolated, we know there cannot be any change in entropy of the surroundings, and we're left with ds universe equals ds system.
So, ds universe equals ds for block A plus ds for block B, which from the equation above equals du of block A over ta plus du of block B over TB.
And again, we take advantage of the knowledge that the system is isolated to apply a constraint, namely that the total internal energy must equal a constant for any process.
So, the change in total internal energy must equal zero, meaning that du of block A equals minus du of block B.
Plugging that in, we get that the change in entropy of the universe for this system is equal to du for block A times the quantity 1 over ta minus 1 over TB.
And there we have it. This quantity can only be zero if ta equals TB.
The second law of thermodynamics told us exactly how to find the point of equilibrium.
Now, I know this seems a bit trivial because, again, it's kind of intuitively obvious what will happen for this problem.
By the way, it's also intuitively obvious for the examples I gave in the beginning of the lecture, like the egg smashing apart, the burning paper turning to ash, or the ice melting in hot coffee.
Those examples and this one were by design intuitive because I want you to feel the intuitive nature we all have for the flow of time and the irreversibility of processes.
However, I also want to point out that the reason we need a thermodynamics-based approach for determining rigorously which direction a process flows is that in many, many cases, the answer is not at all intuitive.
As you can imagine, the more complex a process is, say, with many different thermodynamic driving forces acting at once, the more difficult it is to predict whether a given process will occur by intuition alone.
And even when only one driving force is present, such as the chemical potential and how it directs chemical reactions, we often have very little natural insight into what will happen.
The second law of thermodynamics provides that guidance.
Before I move on, I want to clarify an important distinction.
Some of you may be remembering our zeroth law of thermodynamics and thinking, wait a minute, doesn't that kind of explain this problem of two blocks of different temperatures next to each other?
In fact, didn't I use this example of temperature blocks to explain the zeroth law itself?
If that's what you're thinking, then great memory. That's exactly right. But there's an important distinction here.
You see, the zeroth law of thermodynamics in this case would tell us the following statement.
If heat stops flowing between the two blocks, then the temperature of one block must equal that of the other.
The second law, however, tells us that this is also the point of equilibrium.
And the second law is extremely general. You may be thinking, but that example was kind of a special case, wasn't it?
I mean, it was for an isolated system where the transfer of heat, work, or molecules at the boundaries are not allowed.
In such constant internal energy systems, it's straightforward to directly apply the fundamental equation for the entropy.
And the second law to calculate equilibrium properties of the system.
However, controlling heat and work flow at the boundaries of a real system is often not a simple task.
And in fact, it's unnecessary for most experimental procedures one would be interested in performing.
The second law can be applied to define the conditions for equilibrium for any arbitrary system.
And as we'll see in a later lecture, the Gibbs free energy is another state function that is extremely useful for determining equilibrium
in the most common type of experimental system, namely one where the pressure and temperature are maintained at constant value.
But for now, back to entropy.
So the clausius definition of entropy, which says that ds equals squiggly dq over t for reversible process is based on the macroscopic variables temperature and heat.
There's also a very important microscopic definition of entropy, which I'd like to explain now.
Remember how we talked about the fact that temperature and pressure are macroscopic variables that average over a very wide range of microscopic behaviors?
For temperature, this is an average over the kinetic energy of the particles.
While for pressure, it's an average over their collision frequency and speed.
For the case of entropy, which is also a macroscopic thermodynamic variable, the microscopic quantity that matters is the number of ways in which these particles can be arranged,
or the number of microstates of the system.
The definition for entropy that connects s to this microscopic property comes from statistical mechanics,
and it states that the entropy of a system is equal to a constant times the natural logarithm of the number of available states the system has.
The constant, written as case of b, is known as the Boltzmann constant.
And as you may have guessed, it has this name because this definition of entropy is due to him.
Now, what exactly do I mean by the number of available microstates in a system?
Let's take dice as an analogy, since it lets us do some simple counting.
Suppose I roll three dice together.
Then, how many ways could I roll a three?
Well, only one. The only way to get a three is by rolling three ones.
But how many ways are there to roll a twelve?
Well, quite a number of ways.
I could roll a four, four, four, or a three, three, six, or a four, five, three, and so on.
So the number of states available to rolling a twelve is much higher than the number available to rolling a three.
So let's now think about the microstates for a material. How do we count those?
Well, as you might imagine, it has to do with the number of different ways in which the atoms or molecules can move around and be arranged.
It has to do with the atomic scale degrees of freedom.
Take the example of a perfect crystal.
In that case, the atoms are all locked in to rigid positions in a lattice.
So the number of ways they can move around is quite limited.
In a liquid, the options expand considerably, and in a gas, the atoms can take on many more configurations.
This is why the entropy of a gas is higher than a liquid, which in turn is higher than a solid.
Just to give you a sense of how many actual ways I'm talking about here, take one mole of water.
In that amount of water at room temperature, the number of possible states we have is about ten raised to the number two with 24 zeros after it.
That's an almost unimaginable number of microstates.
Let me give one more example to illustrate the power of thinking of entropy as a counting of states.
And I'll use a very common example in thermodynamics.
That is the expansion of an ideal gas into a vacuum.
Suppose I have this chamber, and just to keep it simple, I'll make it have an adiabatic wall around it.
That means that no heat can pass across the boundaries.
But I'll also put a little membrane down the middle, and I start with the gas all on just one side of that membrane.
On the other side, there's nothing.
So it's a vacuum.
What happens when I remove this membrane?
Well, intuitively, we all know what will happen, right?
The gas will expand into the vacuum.
But hang on.
Why does this happen?
If you think about it, the temperature of the gas does not increase.
That's because when I remove the membrane, now the gas particles can travel over to this side over here where the vacuum was.
But they're not going to travel any faster or slower on average.
And since the temperature is related to the average kinetic energy of the gas, it won't change.
And no heat can transfer in or out of the system since it's an adiabatic wall.
What about work?
Is any work done during the expansion?
That's sometimes tricky, since most students who see this at first will assume that work is done,
because the volume of the gas changes, and we know that work is equal to minus pressure times the change in volume.
But hang on a minute.
During the expansion, the gas has zero pressure since it's expanding into a vacuum.
So actually, there cannot be any work done during the process.
So we have no work done, no heat transferred, no change in temperature, or internal energy.
Seems kind of like nothing much of anything is going on.
What is it that governs the behavior?
It all comes back to counting.
Suppose I slice up the container into a bunch of boxes.
Let's say m boxes on the left where the gas starts.
And if I have n particles, then the number of ways I can arrange these particles,
that is, the number of possible microstates for them to be in, would be equal to m raised to the nth power.
But once I remove the membrane wall, there are now two m boxes.
And the number of possible ways to arrange n particles over those two m boxes is going to be two m raised to the nth power.
So a whole lot more.
It's only because of that, only because of that reason that the gas expands into the vacuum.
There are more ways in which the gas can be, just like rolling a 12 instead of a 1 with a 3 dice.
Whenever there is a way for a system to have more ways to be arranged, it tends towards that state.
Again, I know that some of this stuff is a bit confusing the first time you see it.
Don't worry, you are not alone.
If nothing else, as you think about the concepts I've discussed in this lecture, start from this very simple statement.
The first law of thermodynamics does not distinguish between heat transfer and work.
As we learned, it considers them as equivalent.
The distinction between heat transfer and work comes about by the second law of thermodynamics.
Now, before I conclude, I want to bring up one more important consequence of entropy.
That is, it tells us something about the usefulness of energy.
Suppose I have an axle that can rotate, and it has a weight attached to one end by a string.
Now, when I put energy into rotating the axle, which in turn lifts the weight,
all the molecules of the rod are moving in the same direction.
This helps to keep the energy organized in a sense, so that I can perform the useful work of lifting up the weight.
And here's an important observation.
If I assume that there is no dissipation, then there would also be no entropy transferred associated with the work the rod is doing on the weight.
Of course, I always do have some degree of dissipation, like from friction,
but I could try to get pretty close to the ideal case, where no additional disorder is produced by moving the weight up in this way.
Any process that does not produce net entropy is reversible,
and therefore the process described here can be reversed by lowering the weight.
The energy is not degraded during the process, and the potential to do work is maintained.
But now, instead of using this axle to raise and lower a weight, let's do something different.
Let's put the axle into a box filled with gas molecules.
And instead of a weight on a string, I'll put a fan on the end of it.
Now, I can put the same amount of energy into turning the rod,
and the fan will rotate, which in turn will heat up the gas.
The hotter the gas gets, the more disorder it has, and the higher its entropy will be.
Notice how different this process is compared to raising a weight.
We're converting this organized form of energy into a highly disorganized form of energy.
That is, rotation of the rod is converted into higher internal energy of the gas.
But I cannot convert this energy back to the rod as the rotational kinetic energy.
It cannot be reversed.
No matter how much energy those gas molecules have, they're not going to spin the fan.
Or in other words, they will not do work on the fan.
This is because the energy of this gas is in no way organized.
It's hard to extract useful work directly from disorganized energy.
Maybe if I wanted, I could use the heat in a heat engine to make the rod turn.
But as we'll see in a later lecture on Carnot, I can never do this with 100% efficiency.
So, energy is degraded during this process.
The ability to do work is reduced, disorder is produced, and associated with all this is an increase in entropy.
The quantity of energy is always preserved during an actual process.
That's the first law.
The quality is likely to decrease.
That's the second law.
This decreasing quality is always accompanied by an increase in entropy.
