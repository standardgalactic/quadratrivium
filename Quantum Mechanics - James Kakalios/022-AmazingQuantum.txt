Chapter 21 Seriously, where's my jet pack? As mentioned
at the very start of our narrative, science fiction pulp writers anticipated that the future
would bring a new era in energy production and storage. Instead, it was data manipulation
that underwent a profound transformation, enabled by the discoveries of quantum mechanics.
Why is a new type of energy delivery system needed before jet packs and flying cars become
commercially viable? Let's stipulate that we are not invoking any violations of the
laws of physics, such as the discovery of caverite or some other miraculous material
with anti-gravity properties. Thus, our jet pack must provide a downward thrust equal
to a person's weight in order to lift the person off the ground. Consider how much energy
it takes to lift a 180-pound person, 330 feet, one-sixteenth of a mile, up in the air. Just
to get up there, neglecting any energy needed to jet from place to place would require an
energy expenditure of a little over 80,000 joules, which is equivalent to 0.5 trillion
trillion electron volts. Recall that nearly every chemical reaction involves energy transfers
on the order of an electron volt. Thus, to lift a person over a 20-story building involves
roughly a trillion, trillion molecules of fuel. But that's not actually as much as it
seems, for there are approximately that many atoms in 50-100 cubic centimeters of any solid.
Recall that a cubic centimeter is about the size of a sugar cube. A gallon of fuel contains
nearly 4,000 cubic centimeters, capable of producing 40 trillion, trillion electron volts
of energy. If this is the case, why are we still driving to work? The problem is, what
goes up must come down. As soon as our jet pack stops expending energy to maintain our
large potential energy above ground, back to earth we must return. Thus, every second
we spend in the air, we must continue to burn through our stored chemical energy. The rate
at which we use up fuel will depend on the particular mechanism by which we achieve an
upward thrust. But for most energy supplies, our trip will be over in a minute or two. We
can indeed take jet packs to work, provided we live a block from our office. Note that
the largest expenditure in energy is overcoming gravity. It takes more than 80,000 joules
to get us up in the air. Flying at 40 miles per hour in contrast calls for a kinetic energy
of only 13,000 joules, neglecting the work we must do to overcome air resistance. This
is why we don't have flying cars. Your gas mileage would be nonexistent if the vast majority
of the fuel you carried went toward lifting you up off the ground, with hardly any left
over to get you to your destination. Sort of defeats the whole purpose of a car, flying
or otherwise. Now there have been improvements in the energy content of stored fuel, and
prototype jet packs have been able to keep test pilots aloft for more than a minute. But
ultimately the longer the flight, the more fuel needed, and the heavier the jet pack
will be. Of course there are alternatives to chemical fuel reactions to achieve thrust
and lift. One could use a nuclear reaction, which as we saw in section three, yields roughly
a million times more energy per atom than chemical combustion. But the idea of wearing
even a licensed nuclear power plant on your back is less than appealing. In the 2008 film
Iron Man, Tony Stark designs a suit of armor that contains a host of high-tech gadgets,
all of which are within the realm of physical plausibility. With one big exception, the one
miracle exception from the laws of nature that the film invokes is the arc reactor that
powers Stark's high-tech exoskeleton. This device is a cylinder about the size of a hockey
puck and is capable of producing three gigawatts of power, sufficient to keep a real-world
jet pack aloft and flying for hours. Sadly, we have no way of producing such compact,
lightweight, high-energy content power cells. Note, this is for the first version of the
arc reactor, built in a cave with a bunch of scraps. Later models had even higher power
outputs, though the exact specs are classified proprietary information of Stark enterprises.
Had the revolution in energy anticipated by the science fiction pulp magazines indeed occurred,
and we employed personal jet packs to get to work or the corner grocery store, powered
by some exotic energy source, the need for conventional fossil fuels would of course
be dramatically reduced, with a concurrent dramatic shift in geopolitical relations.
There is one important use of potential jet pack technology that does not involve transportation,
but rather thirst quenching, that would have an immediate beneficial impact. According
to the World Health Organization, as of 2009, 40% of the world's population suffers from
a scarcity of potable fresh water. The most straightforward method to convert seawater
to fresh water involves boiling the salt water, converting the liquid water to steam, which
leaves the salts behind in the residue. This is, after all, what occurs during evaporation
from the oceans, which is why rainwater is salt free.
The amount of energy needed to boil a considerable amount of water is not easily provided by
solar cells, but if one had a power supply for a fully functioning jet pack, the lives
of more than 2 billion people would be profoundly improved, even if everyone's feet stayed planted
firmly on the ground. Can quantum mechanics help in the production of energy, so that
jet pack dreams of the 1930s can be at long last realized? Possibly. Global consumption
of energy, which in 2005 was estimated to be 16 trillion watts, will certainly increase
in the future, with many experts projecting that demand will grow by nearly 50% in the
next 20 years. One strategy to meet this additional need involves the construction of a new power
plant, capable of producing a gigawatt of power at the rate of one new facility every
day for the next two decades. This does not seem likely to happen. Another approach is
to tap the vast amount of energy, that is, for the most part, ignored by all nations.
Sunlight. The surface of the earth receives well over 100,000 trillion watts of power,
more than 6,000 times the total global energy usage, and more than enough to meet the world's
energy needs for decades to come. As described in Chapter 16, the simple diode, comprised
of a junction between one semiconductor with impurities that donate excess electrons, and
a second semiconductor with impurities that donates holes, can also function as a solar
cell. When the diode absorbs a photon, an electron is promoted into the upper band, leaving
a mobile hole in the lower filled band. These charge carriers feel a force from the strong
internal electric field at the PN junction, and a current can be drawn out of the device
simply as a result of exposing it to sunlight. Work is underway to improve the conversion
efficiency of these devices, that is, to maximize the current that results for a given intensity
of sunlight. But even using current cells with conversion efficiencies of only 10%, that
is, 90% of the energy that shines on the solar cell does not lead to electrical power. We
could provide all of the electricity needs of the United States with an array of solar
cells of only 100 miles by 100 miles. The problem is, we don't have that many solar
cells on hand to cover a 100 mile by 100 mile grid, and at the present production capacity,
it would take more than 50 years to fabricate these devices. Moreover, even if the solar
cells existed, we would need to get the electrical power from bright sunny locales to the glues
loomy cities with large population densities. Here again, quantum mechanics may help.
In Chapter 13, we saw that in certain metals, electrons can form bound pairs through a polarization
of the positive ions in the metal lattice. Electrons have intrinsic angular momentum of
H divided by 2, and individually obey Fermi-Dirac statistics, Chapter 12, that stipulate that
no two electrons can be in the same quantum state. When the electrons in a metal at low
temperature pair up, they create composite charge carriers that have a net total spin
of zero. These paired electrons obey Bose-Einstein statistics, and as the temperature is lowered,
they condense into the lowest energy state. If the temperature of the solid is low enough,
then for moderate currents, there is not enough energy to scatter the electrons out of this
lowest energy state, and they can thus carry current without resistance. This phenomenon,
superconductivity, is an intrinsically quantum mechanical effect, and is observed only in
metals at extremely low temperatures, below negative 420 degrees Fahrenheit. At least,
that was the story until 1986. In that year, two scientists, Johannes Bednoors and Karl
MÃ¼ller, at the IBM Research Laboratory in Zurich, Switzerland, reported their discovery
of a ceramic, that is, an insulator, that became a superconductor at negative 400 degrees
Fahrenheit. That's still very cold, but at the time, it set a record for the highest
temperature in which any material, let alone an insulator, would exhibit superconductivity.
Once the scientific community knew that this class of materials containing copper, oxygen,
and rare earth metals, could exhibit superconductivity, the race was on, and research labs around
the world tried a wide range of elements and a host of combinations. A year later, a group
of scientists from the University of Houston and University of Alabama discovered a compound
of yttrium, barium, copper, and oxygen, that formed an insulating ceramic at room temperature,
but became a full-fledged superconductor at a balmy minus 300 degrees Fahrenheit. Liquid
nitrogen, used in many dermatologists' offices for the treatment of warts, is 20 degrees colder
than this, at a temperature of minus 321 Fahrenheit. These materials are referred to as high-temperature
superconductors, as their transition into a zero-resistant state can be induced using
a refrigerant found in many walk-in medical clinics. There is no definitive explanation
for how these materials are able to become superconductors at such relatively toasty
temperatures, and their study remains an active and exciting branch of solid state physics.
The most promising models to account for this effect involve novel mechanisms that quantum
mechanically induce the electrons in these solids to form a collective ground state.
High-temperature superconductors would be an ideal material to transmit electricity generated
from a remote bank of solar cells or windmills to densely populated regions where the power is
needed. While they would need to be kept cool, liquid nitrogen is easy to produce, and when
purchased for laboratory needs it is cheaper than milk, and certainly cheaper than bottled water.
Unfortunately, to date, challenging material science issues limit the currents that can be
carried by these ceramics, such that if we were to use them for transmission lines, they would
cease to become superconductors, and would in fact have resistances higher than those of ordinary
metals. If these problems are ever solved, then in addition to transmitting electrical power,
these innovations may help transportation undergo a revolution as well.
As discussed in Chapter 13, in addition to carrying electrical current with no resistance,
superconductors are perfect diamagnets, completely repelling any externally applied magnetic field.
The material sets up screening currents that cancel out the external field trying to penetrate
the superconductor, and as there is no resistance to current flow, these screening currents can
persist indefinitely. If high-temperature superconductors can be fabricated that are able to support
high enough currents to block out large enough magnetic fields, then high-speed magnetically
levitating trains are possible, where the major cost involves the relatively cheap and safe liquid
nitrogen coolant. Bednoors and Muller won the Nobel Prize in Physics just one year after they
published their discovery of high-temperature superconductivity in ceramics. However, more
than 20 years later, the trains still do not levitate riding on rails composed of novel
copper oxide compounds. Unlike giant magneto resistance and the solid state transistor,
both of which went from the research lab to practical applications in well under a decade,
there are no pre-existing consumer products for which raising the transition temperature of a
superconductor would make a significant difference. Nevertheless, research on these materials continues,
and some day we may have high-temperature superconductors overhead in our transmission lines
and under foot on our rail lines. Another untapped source of energy that quantum
mechanics-based devices may be able to exploit in the near future involves waste.
I speak here not of garbage, but waste heat generated as a byproduct of any combustion process.
Why is heat wasted under the hood of your car? Heat and work are both forms of energy. Work,
in physics terms, involves a force applied over a given distance, as when the force exerted by
the collisions of rapidly moving gas molecules lift a piston in a car engine. Heat in physics
refers to the transfer of energy between systems having different average energy per atom.
Bring a solid where the atoms are vigorously vibrating in contact with another where the
atoms are slowly shaking, and collisions and interactions between the constituent atoms
result in the more energetic atoms slowing down while the sluggish atoms speed up.
We say that the first solid initially had a higher temperature,
while the second had a lower temperature, and that through collisions they exchange heat
until they eventually come to some common temperature. We can do work on a system and
convert all of it to heat, but the second law of thermodynamics informs us that we can never,
with 100% efficiency, transform a given amount of heat into work. Why not?
Because of the random nature of collisions. Consider the molecules in an automobile piston,
right before the ignition spark and the compression stroke cause the gasoline and oxygen
molecules to undergo combustion. They are zipping in all directions, colliding with each other
in the walls and bottom and top of the cylinder. The pressure is uniform on all surfaces in the
cylinder. Following combustion, the gas-oxygen mixture undergoes an explosive chemical reaction,
yielding other chemicals and releasing heat. That is, the reaction products have greater
kinetic energy than the reactants had before the explosion. The greater kinetic energy leads to a
greater force being exerted on the head of the piston as the gas molecules collide with it.
The larger force raises the piston and, through a clever system of shafts and cams,
converts this lifting to a rotational force applied to the tires. But the higher gas pressure,
following the chemical explosion, pushes on all surfaces of the cylinder, though only the force
on the piston head results in useful work. The other collisions wind up warming the walls and
piston of the cylinder, and from the point of view of getting transportation from the gasoline,
this heat is wasted. When heat is converted to work, the second law of thermodynamics quantifies
how much heat will be left over. In an automobile, in the best-case scenario, one can expect to extract
only one-third of the initial chemical energy from the gas into energy that moves the car,
and very few auto engines are even that efficient. There's a lot of energy under the hood that is
not being effectively utilized. Similarly, cooling towers for power plants eject vast quantities of
heat into the atmosphere. It is estimated that more than a trillion watts of energy are wasted
every year in the form of heat, not completely converted to work. This situation may change in
the future, thanks to solid state devices called thermoelectrics. These structures convert temperature
differences into voltages, and are the waste-heat version of solar cells, also known as photovoltaic
devices, that convert light into voltages. Thermoelectrics make use of the same physics that
enables solid state thermometers to record a temperature without glass containers of mercury.
Consider two different metals brought into contact. We have argued that metals can be viewed as
lecture halls, where only half of the possible seats are occupied, so that there are many available
empty seats that can be occupied if the electrons absorb energy from either light or applied voltages
or heat. Different metals will have different numbers of electrons in the partially filled
lower bands. Think about two partially filled auditoriums, each with different numbers of
people sitting in the seats, separated by a removable wall, as in some hotel ballrooms.
One auditorium has 200 people, while the other has only 100. Now the wall separating them is
removed, creating one large auditorium. As everyone wants to sit closer to the front,
50 people from the first room move into vacant seats in the other, until each side has 150 people
sitting in it. But both metals were electrically neutral before the wall was removed. Adding 50
electrons to the small room creates a net negative charge, while subtracting 50 electrons from the
first room yields a net positive charge. A voltage thus develops at the juncture between the two
metals, just by bringing them into electrical contact. If there are significant differences
in the arrangements on the rows of seats in each side, then as the temperature is raised,
the number of electrons on each side may vary, leading to a changing voltage with temperature.
In this way, by knowing what voltage measured across the junction corresponds to what temperature,
this simple device, called a thermocouple, can measure the ambient temperature.
Thermoelectrics perform a similar feat using a nominally homogenous semiconductor.
If one end of the solid is hotter than the other, then the warmer side will have more electrons
promoted from the full lower band up into the mostly empty conducting band, then will be found
at the cooler end. For some materials, the holes that are generated in the nearly filled lower
energy orchestra will move much slower than the electrons in the higher energy balcony,
so we can focus only on the electrons. The electrons promoted at the hot side will diffuse
over to the cooler end, where they will pile up, creating a voltage that repels any additional
electrons from moving across the semiconductor. This voltage can then be used to run any device
acting as a battery. To make an effective thermoelectric device, one wants a material that
is a good conductor of electricity, so that the electrons can easily move across the solid,
but a poor conductor of heat, so that the temperature difference can be maintained across
the length of the solid. Research in developing materials well suited to thermoelectric applications
is underway at many laboratories. Commercially viable devices could find application in, for
example, hybrid automobiles, taking the waste heat from the engine and converting it into
a voltage to charge the battery. In the world of the future, thanks to solid state devices made
possible through our understanding of quantum mechanics, the cars may not fly, but they may
get much better mileage. Finally, we ask, can quantum mechanics do anything to develop small
lightweight batteries to power a personal jetpack? The answer may lie in the developing field of
nanotechnology. Nano comes from the Greek word for dwarf, and a nanometer is one billionth of a meter,
equivalent to approximately the length of three atoms placed end to end.
First, let's see how normal batteries operate, and then I'll discuss why
nanoengineering may lead to more powerful energy storage devices. In an automobile engine,
the electrical energy from the spark plug induces the chemical combustion of gasoline and oxygen.
Batteries employ a reverse process where chemical reactions are used to generate voltages.
In an electrolysis reaction, an electrical current passes through reactants,
often in liquid form, and provides the energy to initiate a chemical reaction.
For example, one way to generate hydrogen gas that does not involve the burning of fossil fuels
is to break apart water molecules. To do this, we insert two electrodes in a beaker of water
and attach them to an external electrical power supply, passing a current through the fluid.
One electrode will try to pull electrons out of the water. Pure water is a very good electrical
insulator, while the other will try to shove them in. The input of electrical energy overcomes
the binding energy holding the water molecule together, and positively charged hydrogen ions,
H positive, are attracted to the electrode trying to give up electrons, while the negatively charged
hydroxides, OH negative units, move toward the electrode trying to accept electrons.
The net result is that H2O molecules break into gaseous hydrogen and oxygen molecules.
In a battery, making use of essentially a reverse electrolysis process,
different metals are employed for the electrodes, such as nickel and cadmium.
They are chosen specifically because they undergo chemical reactions with certain liquids,
leaving the reactant either positively or negatively charged.
Where the metal electrode touches the chemical fluid, though batteries can also use a porous
solid or a gel between the electrodes, electrical charges are either taken from the metal or added
to it, depending on the chemical reaction that proceeds. Just as in our discussion of solid
state thermometers a moment ago, the metal electrode and the contacting fluid represent
two partially filled auditoriums, which will have electrons move from one room to the other,
depending on which room had the higher concentration of electrons.
A barrier is placed between the two electrodes, preventing the fluid from moving from one
electrode to the other, so that negative charges, that is electrons, pile up on one electrode,
and an absence of electrons, equivalent to an excess of positive charges, accumulates at the other.
The only way the excess electrons on one electrode, which are repelled from each
other and would like to leave the electrode, can move to the positively charged electrode,
is if a wire is connected across the two terminals of the battery. The stored electrical charges
can then flow through a circuit and provide the energy to operate a device. In an alkaline battery,
once the chemical reactants in the fluid are exhausted, the device loses its ability to
charge up the electrodes. Certain metal fluid chemical reactions can proceed in one way when
current is drawn from the battery, and in the reverse direction with the input of an electrical
current, as in the water electrolysis example earlier, restoring the battery to its original
state. Such batteries are said to be rechargeable, and it is these structures that have exhibited
the greatest increases in energy storage capacity of late. There have been great improvements in
the energy content and storage capacity of rechargeable batteries, driven by the need for
external power supplies and consumer electronics. In a battery, the electrodes should be able to
readily give up or accept electrons. Examination of the periodic table of the elements shows that
lithium, similar in electronic structure to sodium and hydrogen, has one electron in an
unpaired energy level, similar to figure 31c, that it easily surrenders, leaving it positively charged.
Batteries that make use of these lithium ions with a lithium cobalt oxide electrode, though
other chemical compounds are employed depending on the battery requirements, and with the other
electrode typically composed of carbon, produce nearly twice the open circuit voltage of alkaline
batteries. These batteries are lighter than those that use heavy metals as the electrodes,
and a lithium ion battery weighing 8 ounces can generate more than 160,000 joules of energy,
compared to 80,000 joules from a comparable weight nickel metal hydride battery, or 20,000 joules from
a half pound lead acid battery. These lightweight, high energy capacity rechargeable batteries
are consequently ideal for cell phones, iPods, and laptop computers. As all of the electrochemical
action in a battery takes place when the electrolyte chemical comes into physical contact with the
electrode surface, the greater the surface area of the electrode, the more available sites for
chemical reactions to proceed. One way to increase the surface area is to make the electrodes larger,
but this conflicts with the desire for smaller and lighter electronic devices.
Another way to increase the capacity of these batteries is to structure the electrodes differently.
Nanotextured electrodes are essentially wrinkly on the atomic scale, dramatically increasing
the surface area available for electrochemical reactions without a corresponding rise in
electrode mass. Recent research on electrodes composed of silicon nanoscale wires finds that
they are able to store 10 times more lithium ions without appreciable swelling than carbon
electrodes. While not quite in the league of Iron Man's arc reactor, the ability to fabricate
and manipulate materials on these nanometer length scales is yielding batteries with properties
worthy of the science fiction pulse. This nano structuring is also helping out with the laundry.
Nanoscale filaments woven into textiles yield fabrics that are wrinkle resistant and repel
staining. In addition to giving us whiter whites, nanotechnology is helping keep us healthy.
A 5 nanometer crystal contains only 3300 atoms, and such nanoparticles are excellent platforms
for highly refined pharmaceutical delivery systems, able to provide, for example, chemotherapy drugs
directly to cancerous cells while ignoring healthy cells. We are only beginning to exploit the
quantum mechanical advantages of nanostructured materials. There are 92 stable elements in the
periodic table, and the specific details of the configuration of their electrons determines
their physical, optical, and chemical properties. The fact that neon, for example, has 10 protons
in its nucleus and 10 electrons in various energy levels, described by the Schrodinger equation,
is what determines that neon will have a prominent optical emission of red light,
that is, photons of energy roughly 2 electron volts in energy. Similarly, crystalline silicon has a
separation between its lowest filled states and first empty states of about one electron volt,
and if you want a semiconductor with a different energy gap, you must choose a different chemical
element. Many technological applications would become possible, or would be improved if the
energy separation in crystalline silicon could be adjusted at will without alloying it with other
chemicals that may have unintended deleterious effects on the material's properties. Recent
research indicates that we can indeed make silicon a tunable semiconductor, provided we make it tiny.
Whether the energy separation between the filled orchestra and the empty balcony in our auditorium
analogy is 1 electron volt, 2 electron volts, or 10 electron volts is determined by the elements
that make up the solid and the specific details of how each atom's quantum mechanical wave function
overlaps and interacts with its neighbors. In large crystals, big enough to see with the naked eye or
with an optical microscope, the electrons leaving one side of the solid will suffer many scattering
collisions, so any influence from the walls of the crystal on the electron's wave function
will have been washed out by the time the electron makes it to the other side.
If the size of the solid is smaller than the extent of the electron's de Broglie wavelengths,
then the electrons in the small crystal, in essence, are able to detect the size of the solid in
which they reside. The smaller the box confining these electrons, the smaller the uncertainty in
their location, and thanks to Heisenberg, the larger the uncertainty in their momentum.
Consequently, nanocrystals can have an energy band gap that is determined primarily by the
size of the solid and that we can control, freeing designers of solid state devices from the tyranny
of chemistry. The discoveries by a handful of physicists back in the 1920s and 1930s,
explicating the rules that govern how atoms interact with light and each other,
continue to shape and change the world we live in, today and tomorrow.
