Electrical, electronic. We're surrounded by devices bearing these names. Electrical
appliances, electrical water heaters, electronic ignitions, and some more electronics. Both
we know are somehow related to electricity, but what's the difference between saying
something is an electrical versus an electronic device? When we say something is electrical,
that means the device is powered by electricity, that it gets the energy from either a battery
or being connected to the grid, and here with an electric motor and an elevator. In contrast,
an electronic device is something that is controlled by an electric current, and to do so means
passing that current through special circuits that use vacuum tubes, transistors, or integrated
circuits. For example, modern jet planes use electronic controls to manage the flaps and
the rudder to adjust the speed of the engines, but the jet engines supply the power, and
the electronics control what that power does. While in US engineering schools, we teach
power and electronic engineering together in the electrical engineering curriculum, British
engineering schools make a clear distinction. Students either focus on heavy current engineering,
and that covers power distribution in homes, factories, and mines, or light current engineering,
which is electronics and telecommunications. In previous lectures, we've talked about power
engineering. In lecture 18, we discussed the invention of two fundamental sources of electricity,
the battery and the generator, and in lecture 20, we investigated inventions related to
power consumption, the electric light and the AC motor. Hence, in this lecture, we'll
talk about the rise of electronics. Across the 20th century, electronics has grown up
around three key inventions. First, there was the vacuum tube invented by Leda Forrest
in 1907. Next, there was the transistor invented by John Bardeen, William Shockley, and Walter
Bratton at Bell Labs in 1948. And finally, there was the integrated circuit where the
chip that was perfected by Jack Hilby and Robert Noyce in 1957. Notably, as we'll see, each
of these inventions was a dramatic step forward, building on ideas and practices that came from
the previous device. Because electronic components can perform several simple functions, such
as generating and detecting radio waves, amplifying weak signals, and operating as switches, it
becomes possible to build radios, televisions, computers, and cell phones. Because we have
come to rely so heavily on electronic devices like television and cell phones, it's safe
to say that the vacuum tube, transistor, and chip are literally the building blocks of
the 20th century. So what came before electronics? While three of these building blocks, the
vacuum tube, transistor, and chip are devices that allow us to control other machines or
systems. They were certainly not the first controlled devices. James Watt's steam engine,
for example, had a fly ball governor that controlled the speed of the engine, and engineers
in the late 19th century started using electromechanical relays to control machines. For instance,
if you got into an elevator and pushed the button for a certain floor, the button would
have completed a circuit to a relay that in turn would have switched on the power to the
big motors that make the elevator move. Electromechanical relays are still used in all sorts of
applications, as they are well understood and they are reliable. Until the 1970s, many
telephone exchanges relied on huge banks of relays that permitted people to automatically
dial telephone numbers. And these banks of relays were why the telephone building, centrally
located in many towns and cities, had to be so big. Indeed, some of the first computers,
such as Harold Akin's Mark I at IBM and Harvard, were built using thousands of relays. However,
it quickly becomes difficult to create circuits that use lots and lots of relays, and so control
engineers were keen to develop alternatives. Those alternatives in particular needed to
have no moving parts, and this led people to look at how to control the behavior and
movement of electrons in a variety of situations, in semiconductors, in a vacuum, or in a gas.
We've already encountered our first building block, the vacuum tube, when we talked about
the development of radio in lecture 27. As late as 1912, while Marconi had made many
improvements in his equipment, he was still using a crude receiver to detect incoming
signals, a coherer that consisted of iron filings in a tube. Incoming radio signals made the
filings in the tube line up, and the filings then could conduct an electric current to
a sounder which produced either a long or a short click, a dot or a dash, in the radio
man's headphones. After each incoming signal, the coherer would have to be cleared by gently
tapping it with a tiny hammer. Now, I wish I could tell you that Paul McCartney was referring
to this tiny hammer when he wrote the song, Maxwell Silver Hammer, and that was written
for the Beatles, but that's not what McCartney was thinking about. To replace this complicated
detector used by Marconi, two inventors, John Ambrose Fleming, and an American, lead of
forest, experimented with modified incandescent lamps. Several decades earlier, Edison had
inserted a metal plate inside one of his bulbs, and he noticed that when a current was run
through the bulb's filament, another current was induced in the metal plate. Now, Edison
didn't do anything with this phenomenon, but it was soon dubbed in the scientific literature
the Edison effect. After designing the transmitting station that Marconi used to send his first
signal across the Atlantic in 1901, Fleming went back to his academic research as an engineering
professor, and he began studying the Edison effect. Fleming found specially designed lamps,
which he called valves, could function as a switch, since the current would have only
flow in one direction through them, making them what today we would call a diode. Equally,
Fleming noticed that his valves could also be used to detect radio waves that they might
be used as a replacement for Marconi's coher. Following up on Fleming's work, DeForest added
a third element to his bulb, a small wire grid that could carry its own current. DeForest
found that his bulb could operate not only as a wave detector, but also as an amplifier.
DeForest called his bulb the Audeon. One of the first companies to take a license from
DeForest for the Audeon was AT&T, and that company used vacuum tubes to experiment with
radio telephony before World War I. As we noted in the radio lecture, Edwin Howard Armstrong
studied vacuum tubes and invented several practical circuits that were able to detect,
tune, and amplify radio signals, so it was possible to have a convenient radio receiver
in the home. Hence, the vacuum tube was at the heart of the radio broadcasting boom of
the 1920s. From the 1920s to the 1950s, vacuum tubes were high-tech, and they could be found
in the most amazing devices of those decades, including radar. Again, engineers tried to
use them to build a large-scale computer. At the same time that Aiken was building the
Mark I with relays, John Mockley and Jay Presper Eckert at the University of Pennsylvania
used over 17,000 vacuum tubes to construct ENIAC in 1945, the first general-purpose electronic
computer. Mockley and Eckert had to use so many vacuum tubes because they were trying
to overcome some of the problems posed by this technology. Vacuum tubes, like light bulbs,
have a relatively short service life, and they often fail. Vacuum tubes also are fragile
like light bulbs, and they use a lot of current and they generate a great deal of heat. Hence,
a substitute was needed, a device that could function, much like a vacuum tube, but at
the same time, something that didn't have all these problems.
The substitute for the vacuum tube was the transistor, invented at Bell Labs, the research
arm of AT&T. To overcome the limitations of the vacuum tube, engineers and physicists
at Bell Labs began investigating alternatives as early as the 1930s. Bell Labs scientists
were prompted to do so by their leader, Mervyn Kelly, who did an economic analysis of the
growth of the Bell system in the coming decades, and he concluded that before long, it would
be physically impossible to build exchanges using only electromechanical relays. AT&T
would have to move to electronic switching, and Kelly didn't want to rely on vacuum tubes.
Intent on eliminating vacuum tubes and mechanical relays from AT&T's telephone network, Kelly
encouraged his researchers to investigate non-mechanical, non-thermal ways to control
electric currents. Physicists at Bell Labs focused on how semiconductor materials, such
as germanium or silicon, responded to electric currents as the currents were passed through
them. As the name suggests, a semiconductor has electrical properties that fall between
a conductor, like copper in which an electric current flows freely, and an insulator in
which the current will not flow. Semiconductors are often crystals in structure with holes
in their crystalline lattice that allow the current to flow in one particular direction.
One of Bell Labs' leading researchers on semiconductors was William Shockley, but he was initially
unable to come up with a working device. Instead, in 1947, John Bardeen and Walter Bratton
realized you could control the currents at the point where two kinds of semiconductors
met. But to make an actual device was a genuine challenge, since you were going to need to
put two contacts there at the sandwich in order to get the readings that they wanted,
and that those contacts had to be no more than two-thousandths of an inch apart.
Since the tiniest wires available were three times that size, Bratton attached a small
strip of gold foil over the point of a plastic triangle and then carefully sliced through
the gold at the triangle's tip. Using this arrangement, the team observed that this device
worked well as an amplifier, boosting the strength of a current that was fed into it.
For me, this is one of the great examples of how even 20th century inventions still rely
or still rely on close observation and great hands-on skill.
Jealous that he had to share credit for the invention of the transistor with Bardeen and
Bratton, and all three of them subsequently received the Nobel Prize in 1956, Shockley
went on to invent in 1949 an improved transistor, the junction transistor, that consists of
two different semiconductor materials sandwiched together. In particular, engineers at Bell
Labs had to develop ways to add impurities to silicon so they could get exactly the right
kind of semiconductors with the desirable properties.
Bell Labs patented both transistor designs and then sold licenses to other electronic
companies. By 1953, transistors could be found in hearing aids and miniature radios. In 1957,
the first transistorized computers were introduced by Philco and Univac, that's the company that
was founded by Eckerd and Mockley.
As important as the transistor was, it was eclipsed by another breakthrough in semiconductor
electronics, the integrated circuit or the chip. The idea was first investigated by Jack
St. Clair Kilby at Texas Instruments, who demonstrated in 1958 that you could make a
miniature circuit using only germanium.
Kilby was concerned by what electronics engineers referred to at that time as the tyranny of
numbers. They could see how hundreds or thousands of transistors could be used to create computers
and other complex devices, but the engineers were frustrated by the fact that to create
these circuits, workers had to connect each component in the circuit by hand soldering.
The tyranny of numbers referred to the reality that the more complex the circuit, the harder
it was to fabricate. Although engineers eliminated some of this hand work by creating printed
circuit boards and automatic soldering machines, the reliability of circuit boards was still
very low and electronic devices as a result remained expensive.
To overcome the tyranny of numbers, Kilby thought, if you could create a substitute
for the vacuum tube using a semiconductor material, why not create a semiconductor substitute
for the entire electronic circuit?
Because Kilby was a new employee at Texas Instruments and hadn't incurred any vacation
time, Kilby spent a month in the summer of 1958 thinking about and testing this idea.
And Kilby made the first integrated circuit by soldering tiny pieces of germanium together.
He made his next circuit out of a single piece of germanium.
By making all of the parts out of the same block of material and adding the metal needed
to connect them as a layer on top, there was no more need for individual discrete components.
No more wires and components had to be assembled manually. The circuits could be made smaller
and the entire manufacturing process could be automated.
Texas Instruments promptly filed patents in 1959 to cover Kilby's invention. In 1961,
Texas Instruments built the first computer using integrated circuits for the United States
Air Force and Kilby went on to help create the first electronic calculator in 1967.
This was the first consumer product that utilized integrated circuits or chips.
By working on techniques to mass produce chips, Texas Instruments was able to drive down the
price of chips in the 1970s. One illustration of how fast the price for chips dropped is
that the first electronic calculators cost several hundred dollars, but by the early
1980s, ten years later, banks and department stores were giving calculators away as gifts
when people signed up for credit cards. At the same time, I remember well how the first
calculator bought by my father around 1971 could add, subtract, multiply and divide,
but I also remember how proud I was to acquire five years later a Texas Instrument scientific
calculator that was loaded with advanced math functions.
Meanwhile in California, the chip was undergoing parallel development. After being passed over
for senior management position at Bell Labs, Shockley quit and moved to California in 1955
to start his own company, the Shockley Semiconductor Laboratory, and he built the company in Mountain
View, California, which is one of the towns today that makes up Silicon Valley. Shockley
attracted a team of very bright engineers and physicists to his new company, but his
arrogance and his erratic behavior soon put them off.
In 1957, Shockley became furious when a secretary cut her thumb and he was convinced that it
was a malicious act. Find the culprit, Shockley demanded that all employees be subjected to
lie detector tests. It was later determined that the cut was caused by a broken thumb
tack on an office door. Nevertheless, eight of his best people subsequently quit and created
their own semiconductor company.
Supported by Sherman Fairchild, who had built a successful camera company and had strong
contacts with the military, the traitorous eight, as they like to call themselves, set
up Fairchild Semiconductor. Heading up the company was Robert Noyce, who advocated that
the new company focus on developing new transistors using silicon.
To do so, another member of the eight, Jean Hornie, created a planar process whereby semiconductor
materials could be laid down layer by layer. Using photographic techniques, a pattern could
be put down on each layer and an acid then used to eat away or etch the undesirable material.
As Hornie's process took form, Noyce realized that she could use it not only to make transistors,
but also entire circuits. He came to this realization about the same time as Kilby at
Texas Instruments was doing his work.
Hence, both Kilby and Noyce are regarded as co-inventors of the chip, with Kilby contributing
the basic design and Noyce contributing the production techniques.
Because chips were lightweight, highly reliable, and consumed very little power, integrated
circuits were taken up by the U.S. military and NASA for use in missiles and in spacecraft.
While the Soviets developed large rockets for launching heavy payloads into space, the
U.S., in contrast, concentrated on smaller rockets, making miniature electronics a necessity
for the American space program.
For all the same reasons, IBM decided to first use ceramic modules that contain several
discrete electronic components and then integrated circuits to build a large-scale general-purpose
computer, what we'd now call a mainframe. In 1964, after investing $5 billion in the
project, IBM introduced the System 360.
By providing extensive technical support and a wide range of printers and accessories,
known as peripherals, IBM Systems 360 came to dominate the computer industry.
By the mid-1960s then, electronic engineers were enthusiastic about the possibility that
came with integrated circuits. Integrated circuits were small, fast, and reliable. Engineers
dreamed of using them not only as replacements for cumbersome relays, but for entirely new
applications.
Along with Texas Instruments and IBM, the radio manufacturer Motorola was quick to see
great possibilities in the chip. As one of their executives, Daniel E. Noble, wrote an
electronics magazine in April 1965, and I quote, in the future, we can expect electronics
to find new applications for the consumer in communications, comfort, convenience, computation,
and credit. To these, we can add fun and games, health, and education.
Noble even saw the possibility of e-commerce, shopping on the web, predicting that, and
I quote again, the housewife will sit at home and shop by dialing the selective store for
information about the merchandise wanted.
But in the same issue of electronics came an even more amazing prediction. Another one
of the traders, Gordon Moore, wrote an article in which he hoped he could promote the adoption
of integrated circuits by talking about how quickly they were improving. To do so, Moore
predicted that over the coming years, every 18 months would see the chip density, that
is to say, the number of transistors found on each chip to double while the price of
the chip shrunk by one half. This prediction has come to be known as Moore's Law, and remarkably,
the chip industry has been able to deliver on this prediction for nearly 50 years.
The industry has done so by using less and less real estate on the chip for each transistor
or other component, and by developing increasing sophisticated techniques of layering the silicon
and other materials used to fabricate the chips.
One illustration of just how many transistors are now being crammed onto a chip is that
the memory chip in an 8 gigabyte flash drive has about 32 billion transistors on it.
Just as the Traders 8 had left Shockley Semiconductor in 1957, so talented people, sometimes known
as the Fair Children, left Fair Child Semiconductor in the 60s to create many of the companies
that constitute Silicon Valley. Among the last to leave were Noison Moore, who set up their
own business, Intel, in 1967.
Casting around for customers, Intel entered into an agreement with a Japanese company,
Busycom, to develop an electronic calculator that was intended to compete with Texas Instruments'
new handheld model.
Assigned to design the necessary chips for the Busycom calculator was a young engineer,
Marcian Ed Hoff, but he's always called Ted Hoff.
Disturbed by the undue complexity of the Busycom's chip design, Hoff instead proposed a noise
that Intel designed a series of standard chips that could perform the basic functions of
a computer.
There would be chips for read-only memory, or ROM, another for random access memory,
or RAM, and a final one for doing the actual calculations, the microprocessor.
These three chips might well do everything that existing mini-computers were doing at
that time, mini-computers that cost thousands of dollars.
Hoff and his associates at Intel soon saw that a microprocessor and memory chips represented
a giant change for their industry.
Up to then, they had functioned like a traditional factory, using machines and workers to turn
out several hundred or several thousand chips that were custom designed for other manufacturers
or for the military.
Now, if they could come up with a standardized, flexible design, they could shift the entire
industry from this batch mode to mass production, and do just what Henry Ford had done with
the Model T. Intel could manufacture and sell millions of chips and drive down the cost
per chip.
They could make Moore's law a reality.
Needless to say, it took Hoff and his colleagues several years to convince Noyce and others
at Intel to see that this business model would work.
It also took some serious effort to match the circuitry in the chip with the software
that would have to run on it.
And it took a competitive scare from Texas Instruments, in the form of a patent and prototype
of a similar device that was patented by Gary Boone in 1970-71.
But Hoff pushed ahead, and Intel started shipping in 1971 the first commercial microprocessor,
the 4004, which contained the equivalent of 2,300 transistors.
The 4004 had the same computing power as ENIAC, but instead of taking up 3,000 cubic feet
of space, the chip was 1 eighth inch wide by 1 sixth inch long.
By placing an entire computer on a chip, known as a microprocessor, it became possible not
only to design personal computers, but to integrate computers into a wide variety of
other applications, such as automobile controls, cameras, answering machines, medical monitors,
and microwave ovens.
But even more than just improving existing technologies, the chip has created entirely
new products.
Without chips, there would be no CD or MP3 players, global positioning system devices,
or above all, cell phones.
To get a sense of how powerful the chips are in the modern smartphone, consider that by
one estimate, the smartphone has more computing power than Apollo 11 had when it landed a man
on the moon.
The chip industry, which continues to be led by Intel in the United States, is far larger
than the steel industry was at the close of the 19th century.
Indeed, the technical and social implications of electronics are so great that we will devote
four of the next five lectures to looking at different inventions that have been built
around the chip, satellites, cell phones, the personal computer, the internet, and social
media.
