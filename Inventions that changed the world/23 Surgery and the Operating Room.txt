I've suggested throughout this course, technology is not simply about machines and factories,
but indeed it is about things that are intertwined with all aspects of culture.
As we have seen, inventions have reshaped how we travel, how we eat, how we entertain
ourselves.
In this lecture, we look at another important area of daily life, an area that inventions
have also changed, medicine and in more particularly surgery.
We take it for granted today, in the 21st century, that medicine and technology are closely
intertwined, but these two areas only really began affecting each other beginning in the
mid 19th century.
And of the various branches of medicine, the most important field that was changed by technology
was surgery.
As a branch of medicine, surgery dates back to ancient times, but in the 1700s, a growing
understanding of anatomy allowed surgeons in Europe to undertake a wider range of procedures
in response to disease and pain.
Moreover, surgery was facilitated by a shift in medical theory.
Rather than viewing disease as an imbalance of the humors within the body, physicians
developed a pathological view that disease was caused as the result of specific problems
with organs or different parts of the body.
Nevertheless, surgery before the end of the 19th century was not for the faint hearted.
With no anesthesia, patient had to be restrained by the surgeon's assistance, and the surgeon
prided himself on being able to have the ability to ignore the screaming and to have the skill
to exercise his scalpel and saw with lightning speed.
And once the procedure was over, there was little that either the patient or the surgeon
could do to prevent infection from causing further death or suffering.
In response to this typical situation, surgeons and doctors introduced a host of technological
innovations in the second half of the 19th century, and they were trying to make surgery
safer.
In combining a host of improvements, doctors created a remarkable new space, the operating
room, where procedures could be performed in a sterile environment while the patient
was under anesthesia.
As we will see, the modern operating room may seem obvious to us, but the process of
creating this new space involved many challenges, and doctors were not always in favor of every
single innovation.
One reason why doctors were hesitant to take up all of the new innovations that we're going
to talk today is that these innovations often altered the nature of their expertise and
authority.
Now for the purposes of this lecture, we can talk about the innovations that changed surgery
around three big categories, pain, bleeding, and infection.
These were three areas that challenged both patients and doctors and needed to be addressed
before you could have the modern operating room.
Before the invention of anesthesia, which is the one that we're going to talk about
first, surgery was dramatically painful, and surgeons had to be as swift as possible to
minimize patient suffering.
For centuries, physicians and patients searched for various remedies to minimize pain, and
in surgery they sought to find a way to put the patient in a suspended state, what they
call sleep, a state that would not kill or harm the individual.
Herbal potions were often used.
Some were based on hemlock, mandrake, or other plants, and all of these plants are quite
poisonous, and physicians found that it was better to have the patient smell rather than
eat them.
Indeed, the origin of Snow White's poison apple was probably the fact that these potions
were often put on a sleep apple, which the patient was supposed to smell just before
going under.
Among the other approaches that were tried in addition to the sleep apple for alcohol,
opium, and even knocking the patient out with a swift blow to the jaw.
Needless to say, all of those have their drawbacks.
During the 1700s, Anton Mesmer developed sophisticated methods of hypnosis, which some doctors hoped
could be used to put patients into a state where they would be unaware of pain.
But the physicians were not able to use hypnotism with any degree of consistency.
Hypnosis instead, via Sigmund Freud, went on to play an important role in psychiatry.
Such was the state of affairs that prompted the invention of anesthesia in the 1840s.
And this took place in both Europe and America.
Over the previous several decades, chemists had discovered how to isolate a variety of
gases, and these included oxygen, hydrogen, nitrogen, nitrous oxide, and the vapors of
diethyl ether.
Now, in the spirit of experimentation, these men of science included prominent scientists
such as Joseph Priestley and Sir Humphrey Davy.
And again, in the spirit of science, these men often inhaled the gases thinking that
they might have a healthy benefit.
But in the case of nitrous oxide and ether, they found that the gases made them feel giddy
and carefree, and nitrous oxide became known as laughing gas.
Before long, teenagers throughout Europe and America were inhaling ether in order to have
what they called, in the 19th century, etherfrolix.
Now, because surgeons prided themselves in being able to swiftly execute complicated
procedures because they took the view that pain was God's punishment for sin, traditional
doctors were not especially interested in using laughing gas in their procedures.
Dentists, however, were generally self-trained in the 19th century, and they were keen on
developing ways to attract new patients and to do so they needed to make their procedures
less painful.
One pair of Boston dentists keen on getting new patients were Horace Wells and William
Morton, partners in what they called mechanical dentistry.
Mechanical dentistry was the specialty that they were putting together to replace rotting
teeth with dentures.
In 1844, the day after he attended a lecture where he saw a demonstration of the remarkable
properties of laughing gas, one of these two dentists, Wells, had the lecturer come to
his office.
And there, Wells had the gas administered to him, and while Wells was under, one of
his dental students extracted a tooth, and he did so painlessly.
Wells subsequently tried to convince other doctors and dentists to use nitrous oxide
laughing gas in their operations, and when he failed, Wells became depressed, addicted
to sniffing ether, and he ultimately killed himself in 1848.
His partner Morton had slightly better luck.
Hoping to make a transition from being a dentist to being a medical doctor, Morton signed on
as a private pupil with Dr. Charles Thomas Jackson of Boston.
Jackson introduced Morton to toothache drops that could be applied topically to teeth before
extraction.
And a key ingredient in the drops was ether.
Determined to have a painkiller that he could use in his dental practice, Morton took two
steps.
First, Morton decided that he could get a more controlled effect by having the patients
inhale ether vapors rather than consuming them.
Think back here to the sleeping apple.
Second, Morton decided that he needed to purify the ether vapors to a high level, leading
him to create what he called lethion gas.
In 1846, Morton used his new gas and inhalation apparatus on a patient, while a surgeon was
removing a large tumor from the patient's neck.
Patient rested quietly, and the procedure went smoothly.
Morton patented his lethion gas, but in doing so, he was challenged by his old mentor, Dr.
Jackson, who had insisted that he had come up with the idea of using ether as an anesthetic.
If that was not enough, the same time the Massachusetts Medical Society was horrified
that Morton, a quack dentist, had secured a patent and was planning to make a profit
on his special gas.
Things came to a head when Morton offered to provide lethion to be used on a young girl
in an amputation case where it seemed clear that anesthesia was needed to overcome the
girl's anxiety.
Since the medical society's code of ethics did not allow the use of secret remedies unless
they were revealed, the doctors at the hospital would not permit Morton to assist in the surgery
unless he told them the secret formula for lethion.
Morton promised to disclose the formula, and the amputation went forward, and again the
procedure was successful.
During the Civil War, Morton served in the Union Army and there administered ether to
over 2,000 wounded soldiers.
Nevertheless, financial success continued to elude Morton, and he died in 1868 still fighting
with Dr. Jackson over who was to be the true inventor of ether-based anesthesia.
Morton was buried in Mount Auburn Cemetery in Cambridge, Massachusetts, and there his
supporters erected a tombstone with an inscription describing Morton as the inventor of anesthetic
inhalation.
Legend has it that his nemesis, Dr. Jackson, wandered into the cemetery while he was drunk,
saw Morton's tombstone and flew into a rage.
As a result, he had to be subsequently hospitalized and he was declared hopelessly insane.
The ether is poisonous and highly flammable, and often it induces vomiting in patients
after surgery.
His complications prompted the professor of midwifery, the University of Edinburgh in
Scotland, a physician named James Young Simpson, to investigate alternatives.
After inhaling acetone, benzene and other reagents, Simpson found in 1847 that a dense
colorless liquid, known as chloroform, induced a sense of euphoria while depressing the central
nervous system.
Although surgeons continued to question the efficacy and safety of anesthesia, for which
there were good reasons, the surgeons nonetheless also challenged anesthesia because anesthesia
meant a redefinition of their profession.
Rather than being an armed savage who overpowered the patient and performed the necessary procedure,
surgeons now needed to be scientifically trained to know about these new gases and these new
techniques, and they needed to develop a more humane bedside manner.
Nevertheless, since anesthesia came to be valued by the growing number of middle class
patients, it came to be an accepted part of medical care by the middle of the 19th century.
Perhaps the cultural turning point for anesthesia came in 1853 when chloroform was administered
by Dr. John Snow, who was the physician to Queen Victoria, and the queen received chloroform
during the birth of her eighth child.
Unlike Wells or Morton or Dr. Jackson, all of whom died penniless, Simpson, the inventor
of chloroform, died rich and was knighted by the queen.
Besides inhalation, anesthesia can be administered by injection.
In the beginning of the 1870s, physicians tried barbiturates, which were injected intravenously,
but the first really effective intravenous anesthetic evapan did not appear until the
1930s.
For minor operations, such as those that you have in dentistry, cocaine was topically
administered.
And since cocaine is highly addictive and can be toxic, chemists developed artificial
substitutes such as lignocaine and what we often have today, novocaine.
Because it's essential to track the patient's state while they're under anesthesia, the
anesthesiologist, who could be either a doctor or nurse, relies on the stethoscope, which
was invented in 1816, to listen to the heartbeat.
And over the years, additional monitoring devices were introduced into the operating
room, including the electrocardiograph to monitor the heart, the blood pressure cuff,
pulse oximetry, and most recently, brainwave monitors.
Let's turn now and talk about the second of the three challenges that surgeons faced,
bleeding.
Once an operation is underway, the surgeon has to deal with numerous vessels and arteries
that are in the wound and try to prevent the patient from bleeding to death.
From ancient to modern times, the usual method of dealing with all of these arteries and
these vessels was to sear them with a red hot iron.
And often that failed and the patient's blood to death.
Another approach, first described by the Roman physician Galen, was to tie the vessels off
with a thread, a technique that surgeons call ligature.
Now, ligature was forgotten during the Middle Ages, but it was revived by the French surgeon
Ambroise Perret in the 1500s when he developed new techniques to treat the wounds that were
being caused by this another great invention, guns.
Later on in the 1800s, another French surgeon, Jules-Emile Pion, further perfected this technique
of ligature by using a clamp, which what doctors call a hemostat, that is used to hold the
vessels closed while they are tying it off.
In the 20th century, blood also was dealt with by developing new techniques for transfusion.
Transfusion techniques were perfected so that patients could be given blood that was lost
during an operation.
Now attempts at transfusion date all the way back to the 1600s, but they weren't really
very feasible until the discovery of blood types by the Austrian physician Karl Landsteiner
in the early 1900s.
Mixing blood from individuals who have incompatible blood types can lead to an immune response
and the destruction of red blood cells can release hemoglobin into the bloodstream and
that can have fatal consequences.
In the 1910s, it was also discovered that blood could be kept for longer periods by
refrigerating it and adding an anticoagulant.
The first blood banks were established in the Soviet Union in the early 1920s where research
was undertaken into transfusions with the hope of bringing back the great Bolshevik
leader V.I. Lenin back from the dead.
Inspired by these curious Soviet developments, Bernard Fontis at the Cook County Hospital
in Chicago established the first blood bank in the United States and he did so in 1937.
By 2006, 15 million units of blood were used for transfusions every year in the United States.
So we've talked about pain, we've talked about bleeding, let's talk about the third
challenge infection.
Once the surgery is complete, the patient may still die of infection.
For centuries prior to the discovery of the germ theory of disease, surgeons took no precautions
to prevent infection and the mortality rate after a normal successful procedure was often
more than 50%.
They inceled and washed their hands before seeing patients and often wore the same frock
coat, the same dress coat while performing operations, coats that one person reported
were, quote, stiffed and glazed with blood.
Indeed, a dirty coat was viewed as a sign of a surgeon's expertise and knowledge.
First step toward preventing infection came in 1847 when the Austrian doctor Ignez Simmelweis
noticed that medical students fresh from the dissecting room were causing more deaths in
the maternity ward than the midwives who assisted births but did not go to the dissecting
room.
In the face of ridicule, Simmelweis insisted that everyone wash their hands before childbirth
or other surgical procedures.
Further development of antiseptic techniques came with Joseph Lister.
Born into a Quaker family, Lister's father had invented an improved lens for a microscope.
The younger Lister studied medicine first at the University of London and then at the
Edinburgh Royal Infirmary.
In Edinburgh, Lister married the daughter of his professor and then promptly took her
on a three-month honeymoon to Europe where they visited all the major hospitals and medical
schools.
Fortunately, his wife became fascinated by medical research and served as Lister's assistant
and partner for the rest of her life.
When they came back from that honeymoon, Lister took up a position as professor of surgery
at the University of Glasgow.
While there, he read a paper by another scientist, Louis Pasteur, that explained how gangren
was caused by anaerobic, that is to say non-air-breathing, bacterium.
Inspired by Pasteur, Lister decided to investigate how aerobic air-breathing germs might be
the cause of infection and he sought to develop antiseptic techniques to kill those microorganisms.
To kill germs, Lister started by using carbolic acid that's made from creosote.
Carbolic acid is seen promising since creosote is used to treat wood and make sure that it
doesn't rot.
Creosote had also been used to treat sewage as well as to fight parasites in cattle plague.
In 1865, Lister swabbed the wound around a compound fracture in the leg of an 11-year-old
boy and Lister was pleased to find that no infection developed around the wound.
Lister reported the results in the medical journals and he proceeded to invent a pump
which would not only spray carbolic acid on surgical instruments but also in the air
of the operating room.
A practice that, as you might be surprised, not surprised to learn, surgeons and patients
found most unpleasant.
Finding Lister's carbolic, missed annoying, other surgeons followed the advice of the great
German microbiologist Robert Koch who recommended using steam to sterilize their instruments.
Sterilization is the basic technique that is used for aseptic surgery and aseptic surgery
is where you avoid introducing germs into either the operating environment or the wounds.
This led surgeons to begin wearing sterilized white gowns, masks and gloves and to insist
on a sterile operating room free of unnecessary visitors.
Surgical gloves, one of the major things used in Asepsis technique, came about in 1890.
The Johns Hopkins Medical School in Baltimore, William Stuart Halstead was promoting aseptic
techniques and using carbolic acid to sterilize both his hands as well as those of his nurse.
Since his nurse had skin that was too sensitive for repeated washing in carbolic acid, Halstead
asked the Goodyear Tire and Rubber Company if they could make a rubber glove that could
be dipped into carbolic acid.
While we live in a world that today assumes that germs are everywhere, then that leads
us to buy all sorts of antibacterial products.
It's important to remember that not everyone in the medical profession immediately accepted
germs as a cause for infection and disease.
Indeed it took decades from Lister and Pasteur's first work in the 1860s until the 1900s for
antiseptic and aseptic techniques to become fully accepted as part of medical practice.
There's no obvious connection between the sort of test tube experiments that Pasteur
performed to study how microorganisms caused the fermentation of wine or the spoiling of
milk or caused infection in patients.
In part, the slow acceptance reflects the fact that full aseptic techniques in hospitals
required cooperation not only by the physicians and the surgeons but also by nurses, orderly
and other staff who needed to be committed to maintaining a high degree of cleanliness
and sterile conditions.
In other words, you can only have aseptic sterility if you get the organizational or
social conditions right.
Procedures need to be established, people have to be trained, and you need to design
the hospital building in a way that facilitates maintaining a sterile environment.
However, another reason that asepsis required surgeons and doctors to change their way of
practice was that they had to reimagine their expertise.
For centuries, surgeons had been respected for their ability to perform difficult operations
by holding patients down and being lightning quick with the scalpel.
Skill was at the heart of their professional identity.
Now with anesthesia and aseptic techniques, the expertise of the surgeon was grounded
in science and it took time for surgeons to adapt to this new notion of what made them
experts.
Put it another way, doctors went from being experts based on their surgical skill to
their knowledge of science.
This was not necessarily a quick or easy transition.
Doctors today fight infections not only using sterile techniques, but they also can rely
on a remarkable class of pharmaceuticals known as antibiotics.
The first antibiotic, penicillin, was developed between World War I and World War II by Alexander
Fleming and Howard Florey.
Fleming was a Scottish physician who conducted research in bacteriology at St. Mary's Hospital
in London.
In 1928, Fleming went off on a vacation, but before leaving, he forgot to clean up his
laboratory bench and he left out a petri dish on which he had been culturing some bacteria.
When Fleming returned, he saw that an especially disgusting greenish-yellow mold had sprouted
in the petri dish and that there was a zone around the mold for all the bacteria had been
killed off.
Fleming investigated and found that the mold was penicillin notatum and that the spores
of this mold had floated up from a laboratory on the floor below his.
Fleming isolated the bacteria-killing compound produced by the mold and he named it penicillin.
Although Fleming reported the results of his investigation to a medical journal, nobody
followed up on his discovery until 1938.
In that year, an Australian scientist at Oxford University, Howard Florey, was leading
a team that was studying antibacterial drugs.
In 1941, Florey treated a patient with a severe infection on his face and the penicillin cleared
it up overnight.
Florey's team was keen to test penicillin further, but they were limited by the difficulty
in culturing the mold and producing large quantities of the drug.
Consequently, they investigated large-scale techniques for producing penicillin and they
devised ways of efficiently extracting the active ingredient.
Thanks to Florey's work, by 1945, penicillin production was at an industrial level and
for the Allies, it saved an estimated 6 million lives of wounded soldiers.
In recognition for their work, Fleming and Florey shared the 1945 Nobel Prize in Medicine.
After World War II, both university researchers and pharmaceutical companies, such as Merck,
launched major R&D programs in the area of antibiotics and began developing a range of
drugs.
Some of the drugs that were produced over the next few years were Neomyosin, which appeared
in 1949, Spiromyosin in 1955, and also tetracycline, same year, 1955.
All of these drugs have become an essential part of modern medicine.
Just as the inventions necessary for anesthesia and asepsis altered surgery in the 19th century,
so technology continues to bring changes to this field.
One example is lacroscopic or keyhole surgery, which takes advantage of fiber optics and
video cameras.
Because patients often recover more quickly when the surgeon only has to make a very small
incision.
Researchers and engineers developed techniques in the 1970s, whereby procedures can be done
by making three, maybe four incisions that are only one or two centimeters in diameter.
Using these holes, a surgeon inserts a fiber optic light source in one hole, a remote controlled
video camera in a second hole, and uses the third or fourth hole in order to insert long
handled instruments.
By watching on the video monitor, the surgeon can see what's going on inside the body and
thus guide the instruments and perform the procedure.
Overall as the medical profession conquered the basic challenges of pain, bleeding and
infection, surgery became a central branch of medicine.
As the techniques I've described here took hold, surgeons were able to perform an increasing
array of new surgical procedures.
The first apotectomy was performed in Iowa in 1886, the first heart surgery in Germany
in 1896, the first cornea transplant in 1905, and plastic surgery began in 1917, and that's
just to name a few of the procedures that we now have available to us today.
Indeed, the careful coordination of all aspects of surgery permitted the breakthrough of organ
transplants beginning in the 1950s.
And even more remarkable is that these transplants are viewed as being more or less routine.
In general, we've seen how a range of inventions created the modern operating room, permitting
surgeons in the words of the historian of medicine, Louis Magner, to quote, transform
the operating room from a doorway of death into an arena of quiet routine.
