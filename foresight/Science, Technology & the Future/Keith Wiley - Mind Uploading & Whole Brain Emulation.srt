1
00:00:00,000 --> 00:00:08,840
Today everybody, thanks for joining us. We've got Keith Wiley here today who's a board member

2
00:00:08,840 --> 00:00:14,920
of Carbon Copies and has written extensively on the concepts of mind uploading and whole

3
00:00:14,920 --> 00:00:21,120
brain emulation. He's the author of a Taxonomy and Metaphysics of Mind Uploading as well

4
00:00:21,120 --> 00:00:27,480
as the author of a new book which is coming out very soon, which is a novel this time,

5
00:00:27,480 --> 00:00:34,600
our fiction Contemplating Oblivion which should be coming out sometime this month. So welcome

6
00:00:34,600 --> 00:00:40,280
and it's great to have you here again. Thank you, it's great to be back. Now I guess mind

7
00:00:40,280 --> 00:00:45,760
uploading has a concept that's been kicked around for a while now. Why don't you think

8
00:00:45,760 --> 00:00:52,280
that mind uploading has enjoyed as much wide acceptance within the philosophical community,

9
00:00:53,200 --> 00:01:00,840
as it has in the futurist community? Well, I guess I don't know exactly why people are

10
00:01:00,840 --> 00:01:06,200
answering questions the way they do, because they tend to be rather terse interviews that

11
00:01:06,200 --> 00:01:14,240
don't really have a lot of follow-up. But I think that the, you know, the sort of really

12
00:01:14,280 --> 00:01:24,600
contentious questions of the right around this whole body identity concern and the copy problem,

13
00:01:24,600 --> 00:01:31,800
which I actually consider to be a category or are really hard to get away from. And in fact,

14
00:01:31,800 --> 00:01:41,760
I think the recent sort of, you know, popularity of the discussion has probably made people think

15
00:01:41,800 --> 00:01:47,160
about it just enough to convince themselves that it doesn't work, but not to really think past

16
00:01:47,160 --> 00:01:53,600
that point. So there is a certain sort of initial reaction, I think, where you're presented with

17
00:01:53,600 --> 00:02:00,120
the idea of a non-destructive uploading scenario. It could be a transporter-like scenario from

18
00:02:00,120 --> 00:02:05,960
Star Trek or just a more sort of direct mind uploading scenario, but it's presented in a

19
00:02:05,960 --> 00:02:09,960
non-destructive fashion and people think about it for a minute. They sort of initially have this

20
00:02:10,000 --> 00:02:15,560
cut reaction that something about it must not have worked because of the famous copy problem.

21
00:02:15,560 --> 00:02:25,440
And, you know, the bulk of my writing has been an attempt to chip away at that sort of instinctive

22
00:02:25,440 --> 00:02:31,480
reasoning and, you know, really work through it beyond an intuitive sort of feeling and see

23
00:02:31,480 --> 00:02:39,920
where a logical analysis really lands. So there are these various reasons that support

24
00:02:40,160 --> 00:02:46,280
the initial intuition. People often raise concerns about breaking the stream of consciousness or

25
00:02:46,280 --> 00:02:51,160
good old-fashioned body identity in which sort of they feel like their identity has to stick to

26
00:02:51,160 --> 00:02:56,800
their atoms or what I call space-time worm identity, which sort of says that, you know,

27
00:02:56,800 --> 00:03:02,960
it seems a little odd that your identity might discontinuously jump around in space. You have

28
00:03:02,960 --> 00:03:09,600
to sort of go past the sort of the click-off-the-cuff reaction to sort of ask whether that

29
00:03:09,680 --> 00:03:15,840
actually makes any sense. And, you know, I personally have always landed pretty squarely on

30
00:03:15,840 --> 00:03:21,920
the psychological model of identity, which is memory-based. You know, identity is indicated by

31
00:03:22,560 --> 00:03:28,160
some unique conglomeration of a lifetime of memories. And just to be clear, I use the word

32
00:03:28,160 --> 00:03:37,120
memory in a broader sense than what most people might think. I don't just mean episodic memories

33
00:03:37,200 --> 00:03:43,840
of events from your life. For me, the word memory means the totality of neural encodings in the

34
00:03:43,840 --> 00:03:51,360
brain. So there's a lot of subconscious personality traits. There's actually a lot of muscle

35
00:03:51,360 --> 00:03:56,240
memory. You know, sort of the ability to play the piano is a memory, even though it's a completely

36
00:03:56,240 --> 00:04:01,920
subconscious memory stored primarily in the cerebellum, presumably. So by memory, I just mean

37
00:04:01,920 --> 00:04:07,360
neural encodings within the brain. But nevertheless, my preferred model of identity

38
00:04:07,360 --> 00:04:15,520
pins identity to that feature. And that leads one towards certain conclusions. First of all,

39
00:04:15,520 --> 00:04:23,040
that uploading works because it's all memory-based. So if you sort of, if you consider a thought

40
00:04:23,040 --> 00:04:28,800
experiment in which those memories remain intact through the uploading procedure, then by definition

41
00:04:28,800 --> 00:04:33,200
it worked because that was the premise. In order to reject that, you have to sort of

42
00:04:33,840 --> 00:04:37,520
figure out why you reject it. You're obviously going to reject it on the basis of some,

43
00:04:38,640 --> 00:04:43,920
you know, not full commitment to the memory basis of identity. You're looking at some other

44
00:04:43,920 --> 00:04:51,120
component. And I just don't do that. I just always found that the psychological memory theory works best.

45
00:04:51,440 --> 00:04:59,200
Okay. So, I mean, it's interesting that, like, if it's totally based on memory, surely you'd have

46
00:04:59,200 --> 00:05:06,480
to have some sort of mechanism to be able to process consciousness, valence, awareness,

47
00:05:08,080 --> 00:05:16,880
and the memory as well in order to have an agent feel as though it's got some sort of stream of,

48
00:05:17,760 --> 00:05:21,760
or persistence of identity and enduring metaphysical ego.

49
00:05:22,560 --> 00:05:28,640
Yes, yes. There is this, right. So you've got the concept of identity and you have the concept of

50
00:05:28,640 --> 00:05:34,480
consciousness. And they're not necessarily the same thing, which is where the famous thought

51
00:05:34,480 --> 00:05:40,240
experiments of philosophical zombies come from. You could have agents, you know, cognitively

52
00:05:40,240 --> 00:05:47,280
processing in the world and, you know, encoding memories of a form, although it's hard to imagine

53
00:05:47,280 --> 00:05:50,960
what reflecting on a memory consists of, if you have no consciousness, I'm not quite sure what

54
00:05:50,960 --> 00:05:55,760
that comes down to. But nevertheless, you can sort of separate these components and you can ask,

55
00:05:56,480 --> 00:06:04,160
what would it be like to be an unconscious but yet memory fulfilling system? I think it's mostly

56
00:06:04,240 --> 00:06:10,640
philosophical whimsy. I don't actually think that philosophical zombies are a practical reality,

57
00:06:10,640 --> 00:06:18,720
they're just sort of a philosophical exploration. Well, as David Chalmers brought up the thought

58
00:06:18,720 --> 00:06:24,000
experiment is if you had an atom for an atom reconstruction of a human that, you know, the

59
00:06:24,000 --> 00:06:28,800
first human had consciousness and the second human didn't have consciousness, although all

60
00:06:28,800 --> 00:06:36,160
their atoms and physically they were exactly the same, wouldn't that imply dualism? So, but I mean,

61
00:06:36,160 --> 00:06:41,520
people take the thought experiment to mean something else and it's been used to describe

62
00:06:41,520 --> 00:06:48,000
artificial intelligence without the architecture or the phenomenology of consciousness as well.

63
00:06:48,000 --> 00:06:54,560
So yeah, so we're, this interview is evolving quickly, we're getting into some other topics, but

64
00:06:55,280 --> 00:07:00,960
yes, there is a huge question in the last three years, give or take,

65
00:07:01,840 --> 00:07:09,760
you know, what the implications are of the huge LLM models? What does that actually have to say,

66
00:07:09,760 --> 00:07:15,920
not only about artificial intelligence, but also about consciousness? You know, are these things

67
00:07:15,920 --> 00:07:20,800
conscious? Do they actually have a sentient sort of set of rights and things like that?

68
00:07:21,440 --> 00:07:27,120
A lot of people, I think with pretty good reason, think that the LLMs are not satisfying those

69
00:07:27,120 --> 00:07:31,920
requirements. They're doing something else, but they're probably not conscious in the way that

70
00:07:31,920 --> 00:07:37,200
we're particularly concerned about. I don't think that they have a lot of recurrent

71
00:07:38,960 --> 00:07:44,560
wiring in their layers, they tend to be pretty feed forward heavy. And there's, you know, I'm

72
00:07:44,560 --> 00:07:47,600
definitely one of those people who thinks that recurrence is doing something, I don't quite

73
00:07:47,680 --> 00:07:54,800
know what, but I think recurrence is a crucial component of consciousness in some fashion,

74
00:07:55,520 --> 00:08:02,160
sort of sense of kind of looking back on on itself in a sort of a loop of sort of experiential

75
00:08:02,160 --> 00:08:07,360
self-learn looking. We don't know metaphysically like why, you know, what is it about that?

76
00:08:07,360 --> 00:08:12,400
That's critical to consciousness, but a lot of people seem to agree that some kind of recurrence

77
00:08:13,200 --> 00:08:19,520
should be part of the design. Short of that, you know, something that's always bothered me about

78
00:08:19,520 --> 00:08:27,040
the modern AI systems is that they're all sort of on and off and then they're in the dark. You

79
00:08:27,040 --> 00:08:33,600
query them with your question, they go off and they dig through their neural net, they come up

80
00:08:33,600 --> 00:08:39,760
with a result and they produce for you a paragraph of text or image or a digitized voice, and then

81
00:08:39,760 --> 00:08:45,280
they go dark. That's it. Like they're not sitting there daydreaming, they're not musing on the

82
00:08:45,280 --> 00:08:49,680
conversation you're having with them going back over, you know, the previous dialogue from last

83
00:08:49,680 --> 00:08:57,120
minute. They're not doing, hey, they're dead while the prompts, if they're blinking until you issue

84
00:08:57,120 --> 00:09:04,960
your next query. So is that, can a system like that be conscious? This is an interesting question,

85
00:09:04,960 --> 00:09:11,040
and are these systems conscious on the basis of that sort of strange operation, which is just

86
00:09:11,040 --> 00:09:16,240
very not lifelike. It's just not what we think of. It's not what we do with people. You know, a person

87
00:09:16,240 --> 00:09:21,600
doesn't sort of sit there like a rock until you go up to them and sort of, you know, you query them

88
00:09:21,600 --> 00:09:27,440
and then they do something and they emit and then they die until you interact with them again. Like

89
00:09:27,440 --> 00:09:33,920
people don't work that way, animals don't work that way. So natural brains seem to be in a constant

90
00:09:33,920 --> 00:09:39,200
flux. That's what it means to be a dynamic living brain. And modern AI systems don't do that.

91
00:09:40,000 --> 00:09:44,400
By definition, it's just not what they do. So I don't think they're conscious. I think it's pretty

92
00:09:44,400 --> 00:09:52,000
clear that they're lacking these major features that we would generally assume we expect before

93
00:09:52,000 --> 00:09:57,840
we're going to put those labels on it. I don't know. Yeah, what a time to be alive where we

94
00:09:57,840 --> 00:10:04,800
beginning to be able to, I guess, empirically investigate some of these questions in more,

95
00:10:04,800 --> 00:10:12,240
with more precision, and also with a whole lot more data and we can test assumptions in artificial

96
00:10:12,880 --> 00:10:18,480
sort of substrates instead of just, you know, trying to probe directly the human mind.

97
00:10:19,440 --> 00:10:24,480
Not averse to doing the, the, the latter though, I think it's really interesting where brain

98
00:10:24,480 --> 00:10:34,640
computer interfaces could go. But given the, yeah, so what, what kind of resolution or fidelity of

99
00:10:35,520 --> 00:10:45,200
memory do you think is required for persistence of, in order to either reconstitute or persist

100
00:10:46,160 --> 00:10:54,480
an identity or a sense of self? Yeah, I'm not. How do we approach that question? Yeah, maybe it's a

101
00:10:54,480 --> 00:10:59,040
better question. Well, I think the first approach is to bring in the professionals of which I am

102
00:10:59,040 --> 00:11:04,000
not. I think that there are, you know, psychologists, psychiatrists and neurologists probably have,

103
00:11:05,360 --> 00:11:09,680
there's probably a lot of very established science in sort of how memory evolves over the course of

104
00:11:09,680 --> 00:11:18,880
one's life. And what degrees of memory loss imply, you know, sort of noticeable losses of

105
00:11:18,880 --> 00:11:26,000
personality or, or sort of the, the external third, third party perception that a person is

106
00:11:26,000 --> 00:11:31,040
sort of being lost over the course of dementia, you know, we can say, you know, if you lose your

107
00:11:31,040 --> 00:11:36,720
memory at a certain slow steady rate, it's not dementia and you're not really losing your

108
00:11:36,720 --> 00:11:42,480
personality. But if it happens faster, or if it happens more discontinuously, then we as external

109
00:11:42,480 --> 00:11:48,400
observers sort of recognize that there's some sort of identity collapse sort of in process, you know.

110
00:11:48,400 --> 00:11:58,960
So I don't, I don't claim to, you know, beholding the professional sort of state of that. But it's

111
00:11:58,960 --> 00:12:03,440
something like that, you know, it certainly can't be a requirement of perfect fidelity because

112
00:12:03,440 --> 00:12:08,400
we are, this is, this is Thomas Reed's experiment from, from the 1700s that people are, you know,

113
00:12:08,400 --> 00:12:13,760
over the course of their lives, always constantly steadily shedding their old memories. And yet

114
00:12:13,760 --> 00:12:18,320
we consider, consider it to be a persistence of identity so long as the shift is, you know,

115
00:12:19,280 --> 00:12:26,400
you know, smooth below some threshold. But, you know, to give you an actual answer to that question,

116
00:12:26,400 --> 00:12:34,560
I don't know. I do think it does imply, though, that when we try to create

117
00:12:34,560 --> 00:12:40,240
mind uploads, you know, and things of that nature, there's going to be a lot of wiggle room. You know,

118
00:12:40,240 --> 00:12:46,880
people who demand that before a mind upload can be bonafide, it has to preserve every quantum state

119
00:12:46,880 --> 00:12:52,480
in the brain or some complete nonsense like that are, I just think they're off the map. I mean,

120
00:12:52,560 --> 00:12:58,160
it can't possibly require anywhere near that level of fidelity for us to judge the result

121
00:12:58,160 --> 00:13:04,080
to be a successful preservation of identity. And that's just ridiculous. But again, I don't

122
00:13:04,080 --> 00:13:07,200
actually know what the threshold is going to be. But I think we're going to have a lot of leeway.

123
00:13:07,200 --> 00:13:09,760
I think there's going to be a lot of flexibility once we figure out how to do this.

124
00:13:11,040 --> 00:13:18,240
Right. And previously in an interview in the past, you've mentioned the idea of a child

125
00:13:18,800 --> 00:13:25,040
had drowned in very, very cold waters underneath ice or something like that. And in upwards of an

126
00:13:25,040 --> 00:13:31,840
hour after drowning or being out of it for an hour, they've been able to be revitalized.

127
00:13:33,760 --> 00:13:41,200
So this cuts to a very specific argument. A common argument that comes up in

128
00:13:41,520 --> 00:13:51,360
debates about whether or not a mind upload preserves your identity is a concern that people have

129
00:13:51,360 --> 00:13:56,080
about whether or not it preserves your stream of consciousness. There is this perception that you

130
00:13:56,080 --> 00:14:05,120
have this permanent lifelong and never ending ongoing dynamic stream of processing. And if

131
00:14:05,120 --> 00:14:10,800
that breaks and restarts, such as you go into some sort of, you know, mind uploading operation in

132
00:14:10,800 --> 00:14:15,600
your entire brain is vaporized, but it's scanned in the process and then it's revitalized in a

133
00:14:15,600 --> 00:14:20,960
whole brain emulation, a computational model. That breakage marks a loss of identity and the

134
00:14:20,960 --> 00:14:26,800
person dies by definition. And then a new doppelganger basically springs up out of the ether

135
00:14:26,800 --> 00:14:32,480
and takes the place. So that's the basis of that argument. And I've written not one, but in fact,

136
00:14:32,480 --> 00:14:39,600
two papers on this very specific question. They have very similar titles there. And I don't know,

137
00:14:39,600 --> 00:14:49,600
I think they're both worth reading, but there is a proof positive argument that that whole line

138
00:14:49,600 --> 00:14:57,680
of reasoning doesn't work. So, you know, the way a sort of an exploratory debate will usually approach

139
00:14:57,680 --> 00:15:02,000
this is like, well, when you fall asleep, you sort of lose a little bit of consciousness. Does that

140
00:15:02,000 --> 00:15:05,200
break your stream of identity? And then you say, yeah, but you still got a lot of neural processing

141
00:15:05,200 --> 00:15:10,880
going on. So that doesn't count. Okay, well, you go into the ER, you go into general anesthesia.

142
00:15:10,880 --> 00:15:15,200
That's really deep. That's really, you know, that's really turning the brain down a lot. Maybe,

143
00:15:15,760 --> 00:15:19,040
you know, does that undo the argument? They say, well, no, even in general anesthesia,

144
00:15:19,040 --> 00:15:22,160
you know, you still got a lot of neural processing going on. So now that doesn't count.

145
00:15:22,960 --> 00:15:25,920
There's just sort of a lot of knockdown arguments here that you're trying to sort of

146
00:15:26,720 --> 00:15:32,960
work your way through. And eventually you get to the fact that there are real world medical cases

147
00:15:33,920 --> 00:15:40,480
of people under two different scenarios, one accidental and one intentional sort of an OR

148
00:15:40,480 --> 00:15:48,240
procedure that basically disprove this whole thing. So there is this accident tragedy,

149
00:15:48,240 --> 00:15:55,040
usually called rapid frigid drowning. It's occurred a few times. It either occurs when

150
00:15:55,040 --> 00:16:00,400
someone falls into a frozen lake or occasionally in a dry example, they just fall into a snow drift,

151
00:16:00,400 --> 00:16:04,160
essentially. These are these are sort of real medical cases that have come up.

152
00:16:05,520 --> 00:16:10,000
And they are found several hours later, seven, seven, eight hours later.

153
00:16:12,080 --> 00:16:14,960
And in a few, you know, most in most cases, like this person dies.

154
00:16:16,480 --> 00:16:23,760
In a few rare medical cases, they dropped in temperature quickly enough that instead of

155
00:16:23,760 --> 00:16:28,960
killing them, it essentially preserved them the same way a freezer preserves food from spoiling.

156
00:16:31,120 --> 00:16:38,240
And in a few rare medical cases, those people have been miraculously brought to an emergency room

157
00:16:38,240 --> 00:16:42,080
and they had they basically shot from the hip. It's not like this happens often enough that

158
00:16:42,080 --> 00:16:46,080
they know what to do. But, you know, the doctors basically went through the motions of trying to

159
00:16:46,080 --> 00:16:50,240
slowly warm a person up so that they don't go into shock and various things of that nature.

160
00:16:50,240 --> 00:16:54,880
And there have been cases of recoveries. Okay, what's the point of all this? The point is,

161
00:16:55,440 --> 00:17:00,480
well, first of all, the point is not that their bodies and their brains froze solid,

162
00:17:00,480 --> 00:17:06,640
but you know, this is not an example sort of natural accidental. Oh, and so that was the accidental

163
00:17:06,640 --> 00:17:14,160
case. Then there's an OR procedure called hypothermic preservation. I can't remember exactly what the

164
00:17:14,160 --> 00:17:19,440
term is. It's usually used for cardiac surgeries. It's not used for neural surgery. It's used for

165
00:17:19,440 --> 00:17:26,960
cardiac surgery. You take the patient's brain down in temperature in order to buy more time to

166
00:17:26,960 --> 00:17:32,000
perform the cardiac surgery. Okay, while you're working on the heart, you're not pumping blood

167
00:17:32,000 --> 00:17:37,040
through the rest of the body, including the brain. You take the brain down in temperature on purpose

168
00:17:37,040 --> 00:17:42,960
so that the brain will survive longer, hopefully long enough to finish a heart surgery. Okay,

169
00:17:42,960 --> 00:17:49,600
so those are the two cases. So then coming back to my point, sometimes people sort of

170
00:17:49,600 --> 00:17:53,920
overinterpret this. They're like, oh, this is an example of cryonics. We know cryonics works.

171
00:17:53,920 --> 00:18:00,320
That's actually not true. In none of these realized medical cases, did the person remain

172
00:18:01,840 --> 00:18:05,840
in a cold environment long enough to literally freeze? That's not what these medical cases

173
00:18:05,840 --> 00:18:13,840
involve. But they do involve a drop in temperature to, okay, the temperatures are in those papers

174
00:18:13,840 --> 00:18:18,560
that I wrote. So go to them for the exact numbers I don't remember. But somewhere on the order of

175
00:18:19,440 --> 00:18:25,520
15 degrees Fahrenheit, I think, so they're well above freezing. But the point is that

176
00:18:27,600 --> 00:18:34,720
there is a temperature below which the brain does not operate anymore. It does not fire

177
00:18:34,720 --> 00:18:43,200
action potentials between neurons anymore. Neurons do not operate just because they're above freezing

178
00:18:43,200 --> 00:18:48,080
or something. There's a critical temperature at which they are able to operationalize

179
00:18:48,080 --> 00:18:55,360
being a neuron, primarily operationalizing action potentials, which is the fundamental basis of

180
00:18:55,360 --> 00:19:03,520
all sort of neurology theory. The propagation of action potential is how we expect the brain works.

181
00:19:05,120 --> 00:19:13,120
And that temperature is higher than some of these medical cases. So just I'm speaking

182
00:19:13,120 --> 00:19:17,440
in sort of long interwoven sentences here. What it comes down to is there are medical cases where

183
00:19:17,440 --> 00:19:27,280
a patient has had a brain not fire any action potentials for many, many hours, which coming back

184
00:19:27,280 --> 00:19:31,920
to the original point means that their stream of consciousness was turned off like a water tap.

185
00:19:31,920 --> 00:19:38,160
You cannot have a stream of consciousness if you don't have a brain sending signals through

186
00:19:38,160 --> 00:19:42,080
itself from one neuron to the next. There's no stream of consciousness if the brain is not

187
00:19:42,080 --> 00:19:48,000
processing. So these are patients. Stream of consciousness ceased. And when they were revived,

188
00:19:48,640 --> 00:19:54,400
nobody called them a copy or a doppelganger or a body snatcher or whatever these ridiculous like

189
00:19:55,120 --> 00:20:00,320
everyone just treated them as a revived recovered medical patient.

190
00:20:03,120 --> 00:20:06,960
And that's it. That's the disproof of the entire stream of consciousness

191
00:20:06,960 --> 00:20:12,000
counter argument against preservation of identity via mind uploading.

192
00:20:13,120 --> 00:20:18,240
On the basis of it sort of involving some cessation of processing during the uploading

193
00:20:18,240 --> 00:20:25,440
procedure, it's just a that's the whole thing. I think the paper is going a little more detail,

194
00:20:25,440 --> 00:20:30,320
but really, I don't think there's a whole lot more there. So that's you sort of brought that up.

195
00:20:30,320 --> 00:20:33,280
And I just thought I'd walk through that. That's what that was all about.

196
00:20:35,040 --> 00:20:38,720
Is whole brain emulation needed for blind uploading?

197
00:20:39,280 --> 00:20:43,600
Right. I think whole brain emulation,

198
00:20:47,360 --> 00:20:50,000
short answer, yes, I mean, I don't really see how you do it without it.

199
00:20:50,000 --> 00:20:55,920
There are some interpretations of mind uploading that sort of lean away from that. There's this

200
00:20:55,920 --> 00:21:04,160
concept of training a, what did they used to be called classically? They were called expert

201
00:21:04,160 --> 00:21:08,720
systems. This is what they're called in the 80s and 90s. These expert systems, which have basically

202
00:21:08,720 --> 00:21:14,560
now become bots and the recent LLMs and the AIs, if you train one of these things on the corpus

203
00:21:14,560 --> 00:21:20,080
of a person's available lifetime of media, some people provide more media than others,

204
00:21:21,360 --> 00:21:29,440
but everything they wrote, all their emails, any videos, home videos, everything you can find

205
00:21:29,440 --> 00:21:36,000
that person ever produced, if you train one of these neural net AI systems that are sort of

206
00:21:37,680 --> 00:21:44,240
all above these days against that, you can produce a bot that basically in a dialogue acts

207
00:21:45,360 --> 00:21:50,400
a lot like that person. So the theory, and there have been companies that have made an

208
00:21:50,400 --> 00:21:55,360
entire business model around sort of ostensibly preserving you in this way, or at least preserving

209
00:21:55,360 --> 00:22:05,920
like this sort of post-mortem puppet for other people to interact with. So the idea is you produce

210
00:22:05,920 --> 00:22:11,040
the system, and in those cases where there's a rich enough set of data, you can produce a bot

211
00:22:11,040 --> 00:22:17,520
that sort of passes for the person, and with that count is mind uploading. So that's a version of

212
00:22:17,520 --> 00:22:22,320
mind uploading that doesn't involve whole grain emulation, but there aren't too many examples

213
00:22:22,320 --> 00:22:28,640
like that. In most cases, mind uploading is a whole grain emulation concept, and the way I

214
00:22:28,640 --> 00:22:36,160
describe it is that whole grain emulation is just a technical term. It's the process of

215
00:22:36,720 --> 00:22:42,720
modeling and emulating the neural processing of a brain. Doing that is whole grain emulation.

216
00:22:42,720 --> 00:22:47,920
Mind uploading is not actually a technical term. Mind uploading the way I see it is

217
00:22:47,920 --> 00:22:54,240
it is a philosophical interpretation of the consequences of whole grain emulation. If you

218
00:22:54,240 --> 00:22:59,600
choose to interpret whole grain emulation as a preservation of a person, a preservation of a

219
00:22:59,600 --> 00:23:05,120
person's identity, then the label for that is mind uploading. Mind uploading is the

220
00:23:05,120 --> 00:23:10,000
interpretation of whole grain emulation as a preservation of identity. You have uploaded

221
00:23:10,000 --> 00:23:15,760
the mind. I don't actually hear too many people phrase it that way, but that's that's how I've

222
00:23:15,760 --> 00:23:21,040
always thought of mind uploading as being this interpretation of the technical process of whole

223
00:23:21,040 --> 00:23:31,760
grain emulation. So I mean, I guess the question comes to me is, do you think you'd need to upload

224
00:23:31,760 --> 00:23:41,120
all the exact brain of a particular identity in order to reproduce that identity in the machine,

225
00:23:41,120 --> 00:23:46,720
or are there some parts of the brain that are rather generic that you'd be able to scaffold on

226
00:23:46,720 --> 00:23:57,200
more templated emulations of like a generic brain, certain salient features of the brain,

227
00:23:57,200 --> 00:24:02,640
which are more important to preserve personal identity and during metaphysical in go?

228
00:24:03,200 --> 00:24:08,400
Yeah, I mean, there's one really big one, which is that something like 80% of our brain

229
00:24:08,400 --> 00:24:12,640
by neuron count is the cerebellum. And you could probably just throw the whole thing away.

230
00:24:13,200 --> 00:24:18,080
You might, you know, not bring your expert piano playing skills along with you.

231
00:24:19,360 --> 00:24:23,760
And you know, you might because the muscle memory is often encoded in the cerebellum.

232
00:24:24,720 --> 00:24:28,480
And you know, whether or not you could borrow someone else's or well, or you could relearn it

233
00:24:28,480 --> 00:24:32,800
again, right? But if we're trying to sort of simplify the process of the emulation, because

234
00:24:32,800 --> 00:24:36,880
it turns out to be technically really hard, you're trying to find ways to cut corners.

235
00:24:38,880 --> 00:24:45,280
You know, it's it's sort of debatable whether a professional musician or it could be any

236
00:24:45,280 --> 00:24:49,600
professional athlete, because some sort of sort of muscle memory task,

237
00:24:49,600 --> 00:24:52,880
would they consider that to be a critical component of their identity? If they lost that,

238
00:24:52,880 --> 00:25:00,880
would they not be the same person? You know, that's a very sort of emotional question to ask.

239
00:25:00,880 --> 00:25:07,840
But nevertheless, it really doesn't impact the cortical, I mean, by which I mean cortex,

240
00:25:08,880 --> 00:25:15,280
the cortex encodings that we generally associate with all the rest of personality traits.

241
00:25:15,280 --> 00:25:21,600
All of that is in the cortex, not the cerebellum, you know, not the alamus, maybe the hyper alamus,

242
00:25:21,600 --> 00:25:25,600
because that's doing some short term memory work, you know, not the amygdala. There's all sorts of

243
00:25:25,600 --> 00:25:30,960
stuff in there that doesn't seem to be particularly pertinent to what we consider to be human identity.

244
00:25:32,160 --> 00:25:36,240
Which isn't too surprising, because if you look at it from an evolutionary perspective,

245
00:25:36,880 --> 00:25:45,360
the human cortex and neocortex is just this 500,000 year veneer, you know, sort of thinly

246
00:25:45,360 --> 00:25:51,040
layered on top of 4 billion years of stuff that we don't care about. We don't care about the

247
00:25:51,040 --> 00:25:55,920
aspects of our identity back when we were amphibians. So why would we consider that important?

248
00:25:55,920 --> 00:26:02,960
What we really are interested in is just the last 500,000 years of neurological evolution.

249
00:26:02,960 --> 00:26:09,280
That's the stuff we really need to successfully carry over to preserve human identity. Now,

250
00:26:09,280 --> 00:26:14,560
I'm not saying we have to just sort of cut, you know, cut it off that early. You know, it might

251
00:26:14,560 --> 00:26:21,120
be nice to try to get a richer, more complete emulation. I'm not against that. But the question is,

252
00:26:22,000 --> 00:26:27,920
is it possible to achieve, you know, a pretty good passing whole brain emulation and

253
00:26:27,920 --> 00:26:34,400
mind upload, you know, at some fraction of the total brain? And I think we could. I think we could

254
00:26:34,400 --> 00:26:42,640
make a good enough version one with way less than the entire brain. And over time, we'll probably

255
00:26:42,640 --> 00:26:46,240
be able to do the rest of it anyway. So it's probably going to be a question 10 years later.

256
00:26:46,240 --> 00:26:48,560
But in theory, I don't think we need a lot of it.

257
00:26:49,440 --> 00:26:58,960
Okay, so therefore, if we don't need to emulate the whole brain, then are we approaching the

258
00:26:58,960 --> 00:27:04,000
compute required to be able to achieve this if we knew how? Do you think?

259
00:27:05,440 --> 00:27:10,160
So there are very wide estimates. You know, there are different, there are different sort of

260
00:27:10,800 --> 00:27:17,600
dimensions, you know, processing power, memory capacity. I think at a recent video or presentation

261
00:27:18,880 --> 00:27:22,480
sort of the speed of communications in between the units was considered.

262
00:27:25,360 --> 00:27:31,600
We, I think the expectation is that we have almost gotten there now with some of our supercomputers,

263
00:27:32,240 --> 00:27:39,760
at least at the lower estimates of human processing capability. But this sort of begs the question

264
00:27:39,760 --> 00:27:46,720
of whether mere number of computes per second is actually the trick. Or, you know, I'm a big

265
00:27:46,720 --> 00:27:52,000
believer in neuromorphic computing in which you actually create a massively parallel system

266
00:27:52,560 --> 00:27:59,760
that is connected to itself and wired against itself in ways that closely mirror the architecture

267
00:27:59,760 --> 00:28:05,440
of the brain. And you know, one might ask, well, who cares, you know, and this comes back to a

268
00:28:05,440 --> 00:28:12,000
point made earlier, which is that I strongly suspect there is something about the recurrent

269
00:28:12,080 --> 00:28:19,440
self-referential, you know, self-looping signal processing structure of the brain

270
00:28:20,000 --> 00:28:25,920
that is important. I don't know why this is an open question. We don't understand consciousness

271
00:28:25,920 --> 00:28:32,720
well enough to understand why certain network architectures seem to work and others don't.

272
00:28:32,720 --> 00:28:37,520
So the wiring diagrams in the cerebellum have essentially nothing to do with your consciousness.

273
00:28:37,520 --> 00:28:45,840
If you remove a person's cerebellum in surgery, their consciousness, their sense of who they are

274
00:28:45,840 --> 00:28:51,840
is all left completely intact. Again, there can be sort of some damage to their muscle memory,

275
00:28:51,840 --> 00:28:58,880
but their sense of who they are is not harmed. And their conscious experience is not affected.

276
00:28:58,880 --> 00:29:05,440
So the way that the cerebellum is wired is not the right way. And the way the cortex is wired

277
00:29:05,520 --> 00:29:10,960
is the right way. What is that way? We don't know. It's the 21st century. What a time to be alive.

278
00:29:10,960 --> 00:29:19,440
I don't know. But I think that I think that we need to get, I think we need to solve that.

279
00:29:19,440 --> 00:29:24,000
I think we need to figure that out. We need to figure out what it is about the wiring of the

280
00:29:24,000 --> 00:29:30,640
cortex that is creating this amazing experience. And that's what we need to make sure that we

281
00:29:30,640 --> 00:29:37,840
recapture in an emulation. Do people know what parts of the brain were destroyed

282
00:29:37,840 --> 00:29:44,880
when Phineas Gage got a big rod through his brain? They definitely do. I don't recall if I've

283
00:29:44,880 --> 00:29:51,200
talked my head. So it went sort of right behind his eye, which means that it hit a lot of his

284
00:29:51,200 --> 00:29:59,600
prefrontal cortex, which is, you know, relative to a prandola pram. It hit parts of the brain

285
00:29:59,600 --> 00:30:05,280
that are salient to sort of emotion control or something, which is why after the accident,

286
00:30:05,280 --> 00:30:11,200
he became very sort of fly off the handle, kind of sort of an angry kind of person. He just sort

287
00:30:11,200 --> 00:30:16,720
of didn't regulate his emotions probably. So something about that sort of, and it wasn't like

288
00:30:16,720 --> 00:30:22,640
an amygdala, you know, sort of emotional response. I understand, I really have to go read it. I'm

289
00:30:22,640 --> 00:30:28,960
sort of shooting from the hip here. But my understanding is that our, you know, very advanced

290
00:30:28,960 --> 00:30:34,800
intellectual cortex level ability to regulate our emotions to basically just think through

291
00:30:34,800 --> 00:30:40,240
a situation instead of flying off the handle. That's all, you know, that's not the kind of

292
00:30:40,240 --> 00:30:46,960
carefully reasoned stuff that, say, a mouse does, you know, a mouse more or less does actually kind

293
00:30:46,960 --> 00:30:54,320
of respond emotionally to what happens around it. Humans can feel the emotions, but also think

294
00:30:54,320 --> 00:30:57,920
through it and say, okay, I'm not going to act directly in accordance with my emotions. I'm

295
00:30:57,920 --> 00:31:02,560
going to act in this intellectually rationalized way instead, because I have a cortex that enables

296
00:31:02,560 --> 00:31:09,280
me to do that. And that's precisely what got blasted out of Cage's head. But no, I don't remember

297
00:31:09,280 --> 00:31:13,840
exactly which, if you look it up, it'll tell you exactly which parts of his brain were damaged.

298
00:31:14,080 --> 00:31:21,120
Do you think quantum computing is required for whole brain emulation or mind uploading?

299
00:31:21,760 --> 00:31:24,240
Or if it's not, could it be desirable?

300
00:31:24,800 --> 00:31:29,520
Yeah. So of course, Penrose and Haneroff made this famous.

301
00:31:32,560 --> 00:31:33,520
My God, you be all right.

302
00:31:33,520 --> 00:31:39,600
The most honest answer is that I don't know. Of course, people tend to have hunches and I have a

303
00:31:39,600 --> 00:31:48,000
hunch. What's your hunch? I would be surprised if the quantum mechanical properties of neurons

304
00:31:48,800 --> 00:31:55,440
had anything to do with sort of animalian evolved behavior. Just why would evolution

305
00:31:56,320 --> 00:32:02,560
need to go there? It seems like all it really needs to do is get the action potential sputtering

306
00:32:02,560 --> 00:32:09,520
around well enough that the animal can satisfy the four Fs of survival. Why do you need quantum

307
00:32:09,520 --> 00:32:18,720
mechanics to do that? I don't see the need. I don't get it. I just never found the quantum

308
00:32:18,720 --> 00:32:25,760
mechanical argument for consciousness convincing. That's my take. But I'm also fairly open-minded

309
00:32:25,760 --> 00:32:29,440
about it. There's a lot we don't understand about quantum mechanics. There's a lot we don't

310
00:32:29,440 --> 00:32:37,120
understand about consciousness. Some people see that as an excuse to pin them together, implicitly.

311
00:32:37,120 --> 00:32:40,880
People like Deepak Chopra are like, well, we don't understand consciousness and we don't understand

312
00:32:40,880 --> 00:32:46,160
quantum mechanics. That means they're connected. So that's garbage reasoning. Two things aren't

313
00:32:46,160 --> 00:32:52,480
related just because they're both mysterious. But at the same time, they are both mysterious.

314
00:32:52,480 --> 00:32:58,480
We don't actually know. So I'm open to someday discovering what there's a connection there.

315
00:32:59,440 --> 00:33:05,760
But I don't see a need for it now. Forgive the cat. The cat is forgiven.

316
00:33:09,680 --> 00:33:16,320
Once we can upload brains onto a computer, there's going to be an economic issue too

317
00:33:16,320 --> 00:33:23,280
about how to afford to do all this and how to do it in a cost-effective way. Such is a good

318
00:33:23,280 --> 00:33:29,920
ratio between the amount of minds that can be uploaded and the amount of power which is consumed.

319
00:33:31,840 --> 00:33:37,360
The trade-offs will become more friendly over time as technology gets better. But what's the

320
00:33:37,360 --> 00:33:45,680
role of the type of resolution that we might want to capture and the ability to do compression

321
00:33:45,760 --> 00:33:50,880
to reduce the amount of space a mind might occupy?

322
00:33:52,320 --> 00:34:00,400
I do have some numerical answers to that question. But more to the point, everyone's sort of used to

323
00:34:00,400 --> 00:34:08,400
these claims about the rate of advance of technology and it's true. So we can just barely scrape

324
00:34:08,400 --> 00:34:13,840
together a supercomputer that might emulate the human brain today. It's millions and millions

325
00:34:14,080 --> 00:34:18,640
of dollars and it will only run one brain. And it's not wired correctly to do that anyway. So we

326
00:34:18,640 --> 00:34:22,400
couldn't do it even if we wanted to. But in theory, the architecture is there. So we're just,

327
00:34:22,400 --> 00:34:27,280
you know, that's where we are now. So we're down the road. We'll be able to do it

328
00:34:27,280 --> 00:34:32,480
expensively but successfully. And then 10 years after that, we'll be able to do it cheaply. You

329
00:34:32,480 --> 00:34:38,560
know, you know, the human brain uses as much electricity as a light bulb. So we're going to

330
00:34:38,560 --> 00:34:42,800
get it, you know, at some point, the power requirements won't be relevant. You know, it's not

331
00:34:42,800 --> 00:34:48,240
like you're going to need a supercomputer or even anything as inefficient as a desktop computer.

332
00:34:48,240 --> 00:34:52,480
You'll need something as efficient as a brain. And you know, how cheap are brains? There are

333
00:34:52,480 --> 00:34:57,760
brains everywhere. Like brains don't take much energy. The world's full of brains. So once we

334
00:34:57,760 --> 00:35:05,440
are able to create those kinds of computers at that level of energy efficiency, the efficiency

335
00:35:05,440 --> 00:35:09,520
equation, and therefore the cost equation, it's all just going to go out the window. You know,

336
00:35:09,520 --> 00:35:15,440
so long as your brain, so long as your emulated brain can get, you know, as much energy as,

337
00:35:15,440 --> 00:35:20,800
you know, a salad, then it'll be able to stay alive the same way we can. Right? So it's just,

338
00:35:20,800 --> 00:35:26,320
that whole problem is going to go away. I hate to say that it's a question of patience, but it is.

339
00:35:27,440 --> 00:35:31,120
People really chomp at the bit about futurism stuff sometimes. They're really like,

340
00:35:31,120 --> 00:35:34,640
gosh, you know, this stuff really needs to happen in the 2040s or it's no good at all.

341
00:35:35,280 --> 00:35:38,880
I mean, I'm sorry, but some stuff's just not going to be available in the 2040s,

342
00:35:38,880 --> 00:35:43,040
but that doesn't mean that it's not going to happen, you know, early in the next century.

343
00:35:45,120 --> 00:35:49,040
Or later in the next century. I don't know when, but I'm not particularly concerned about that,

344
00:35:49,040 --> 00:35:54,640
because when it does happen, I just think it's going to be practically preordained. I don't

345
00:35:54,640 --> 00:36:00,400
think it can be stopped. I also don't think it can really be expedited too much. I mean,

346
00:36:00,400 --> 00:36:08,560
mostly we want to create a society and a culture that sort of broadly favors science and knowledge

347
00:36:08,560 --> 00:36:15,440
and openness, you know, and sort of inquisitiveness and curiosity. If we just sort of create a society

348
00:36:15,440 --> 00:36:22,400
that creates and supports these general ideas, the science will just happen. Now,

349
00:36:23,520 --> 00:36:27,360
I can get into some minor specifics. You were talking about sort of resolution and compression

350
00:36:27,360 --> 00:36:35,920
and things about that, things like that. So where we currently stand on sort of producing

351
00:36:35,920 --> 00:36:43,440
whole brain emulations is, well, in short, we can't, but there are sort of multiple steps in

352
00:36:43,440 --> 00:36:50,560
the process. So the most likely way to produce a whole brain emulation is to preserve the brain.

353
00:36:50,560 --> 00:36:57,920
That means essentially embedded in a resin, and then you deli slice it, and then you take pictures

354
00:36:57,920 --> 00:37:03,280
of each slice. You can't use, like, a camera, because the features, the neurological features

355
00:37:03,280 --> 00:37:07,520
you're trying to capture are smaller than the wavelengths of visible light. You can't take

356
00:37:07,520 --> 00:37:11,840
a picture of something that's smaller than a wavelength of light. So we use electron microscopy

357
00:37:11,840 --> 00:37:20,320
to do this. And the resolution that we're currently able to achieve is a z-axis resolution. That means

358
00:37:20,320 --> 00:37:29,360
slice to slice. We can physically diamond knife slice brains at about a 40 nanometer thick resolution.

359
00:37:30,320 --> 00:37:35,440
And then when we take an electron microscope image of a slice, we can achieve about a four

360
00:37:35,440 --> 00:37:42,400
nanometer xy resolution. So each image has 10 times as much resolution, 10 times the fineness of

361
00:37:42,400 --> 00:37:48,560
resolution as the slice to slice steps. So in the three dimensions, two of the dimensions are finer

362
00:37:48,560 --> 00:37:54,000
by a factor of 10 than the third. And the question is, is that good enough? And the answer is,

363
00:37:55,440 --> 00:38:02,640
according to current models and theories of how the brain works, you know, the major paradigm

364
00:38:02,640 --> 00:38:11,840
of action potentials propagating along axons, triggering synapses, and then propagating secondary

365
00:38:11,840 --> 00:38:20,720
action potentials and downstream neurons, we can image synapses at that resolution. And of course,

366
00:38:20,720 --> 00:38:25,680
we can image all the rest of the neural components, which are larger than the synapses. So we can

367
00:38:25,680 --> 00:38:31,120
actually capture the necessary resolution to produce a whole brain emulation. We can do it today.

368
00:38:31,760 --> 00:38:41,440
What we can't capture is the scale. So the data sets that we are scanning these days are, you know,

369
00:38:41,520 --> 00:38:46,880
what sort of cubic section of a brain can we kind of scoop up and actually perform this on?

370
00:38:47,600 --> 00:38:56,320
And it's on the order of sort of fractions of a cubic millimeter. One full millimeter is just,

371
00:38:56,320 --> 00:39:05,200
you know, sort of coming around the bend as sort of a 2020s project. So, you know, human brain is a

372
00:39:05,200 --> 00:39:12,560
little bit bigger than that. So we've got a ways to go. I think a whole mouse brain of approximately

373
00:39:12,560 --> 00:39:18,880
a cubic centimeter, which is a thousand times bigger than a millimeter, is one of the projects

374
00:39:18,880 --> 00:39:26,080
that is being sort of put together now to be achieved over the next many years. And when that

375
00:39:26,080 --> 00:39:31,040
is done, all we will have is a brain of a mouse. All we will have is one centimeter. We're going

376
00:39:31,040 --> 00:39:39,200
to need to go another factor of 10 in all three directions to get to the human. So

377
00:39:41,520 --> 00:39:48,160
it's going to take a while. But the interesting thing is that while it's going to take a while

378
00:39:48,160 --> 00:39:54,240
on the calendar to get there, there's a very little that actually appears to stand in the way.

379
00:39:54,960 --> 00:40:02,480
We just need to keep turning the crank for a few decades. And there's no real obvious like place

380
00:40:02,480 --> 00:40:08,160
where we think it's just technically like we're up against a wall. When we look at it today,

381
00:40:08,160 --> 00:40:12,880
there are things we don't know how to do yet. Getting the perfusion of the chemicals deep

382
00:40:12,880 --> 00:40:19,280
into a large brain evenly is very hard to do. We don't really know exactly how to do it. How do

383
00:40:19,280 --> 00:40:24,960
you, you know, we can slice up a slice of a brain on the scale of microns, but can we create, you

384
00:40:24,960 --> 00:40:33,520
know, a cross section of a brain that's only 40 nanometers thick, but is five inches across, right?

385
00:40:33,520 --> 00:40:40,000
Like how do you even handle that, right? So there are major technical questions, but these don't

386
00:40:40,000 --> 00:40:44,800
feel like things are going to be like some fundamental wall in physics that we cannot get

387
00:40:44,800 --> 00:40:49,520
past. It seems like we just need to keep going and we will get there.

388
00:40:51,760 --> 00:40:58,080
Do you think that there's a particular, okay, so you mentioned plastination, at least that's

389
00:40:58,080 --> 00:41:04,000
what I think you're referring to. Do you think that's the state of art and cryopreservation today?

390
00:41:05,360 --> 00:41:10,880
Or brain preservation today? Or would you suggest that it's something else? Or

391
00:41:11,840 --> 00:41:16,800
yeah. And what about in the future? Do you think where the state of the art

392
00:41:16,800 --> 00:41:20,640
plastination techniques that we have today are the best that they'll ever be?

393
00:41:21,600 --> 00:41:26,400
So correct me if I'm wrong. I think plastination refers to room temperature preservation techniques.

394
00:41:27,440 --> 00:41:34,800
Correct. Yeah, I don't really know where they stand. So you've got two things. You've got the cryopreservation

395
00:41:35,520 --> 00:41:41,200
and then there is the preparation of a brain sample for slicing, which is not cryo.

396
00:41:42,400 --> 00:41:47,520
It's just room temperature and it's a resin. I don't really know what the chemical structure is,

397
00:41:47,520 --> 00:41:56,000
but it's essentially you first you flush out a lot of colorful and opaque things so that you

398
00:41:56,000 --> 00:42:01,120
can physically see through the brain. And once you flush all that stuff out, you embed it in

399
00:42:01,120 --> 00:42:07,280
some kind of resin. I'm not a chemist. I don't know. And now you've got like this solid block with a

400
00:42:07,280 --> 00:42:13,760
transparent brain inside it. Then you slice that and you take pictures of it. Now preservation,

401
00:42:14,560 --> 00:42:18,240
you know, especially sort of preservation for the purpose of longevity, sort of cryonics or

402
00:42:19,280 --> 00:42:24,800
the next generation of cryonic like preservation, which would be these sort of the aldehyde

403
00:42:24,800 --> 00:42:30,240
stabilized cryopreservation that the Brain Preservation Foundation has awarded,

404
00:42:31,760 --> 00:42:37,920
which is probably, you know, definitely the next generation in preservation if we can get it

405
00:42:37,920 --> 00:42:47,440
sort of normalized, popularized. That's for a different task. That's for the task of preserving

406
00:42:47,440 --> 00:42:57,680
people, keeping them alive for one of two reasons. Many proponents of sort of classic cryonics want

407
00:42:57,680 --> 00:43:03,440
to be kept alive just to sort of get to that point in the future where the technology can revive them

408
00:43:03,440 --> 00:43:10,880
biologically again. The other reason to go into preservation of some form though is to just get

409
00:43:10,880 --> 00:43:15,520
to that point in the future, not where you can be biologically revived, but where mind uploading

410
00:43:15,520 --> 00:43:20,240
technology has gotten to the point where it can upload your preserved brain and you would not come

411
00:43:20,240 --> 00:43:25,520
back biologically, you come back as an upload. Now as you know, I don't want to completely distract,

412
00:43:25,520 --> 00:43:28,560
we can get into like, you know, why would you want one? Why would you want the other? What's the

413
00:43:28,560 --> 00:43:32,080
benefit? You know, why would you choose one if the other's available? There's a whole conversation

414
00:43:32,080 --> 00:43:38,240
there, but I just I think it would sort of take us off into the weeds. But there is never there's

415
00:43:38,240 --> 00:43:43,520
this issue of preservation for longevity and there is this issue of preserving and preparing a brain

416
00:43:43,520 --> 00:43:49,840
sample for very contemporary and real, you know, sort of slicing and scanning that we do today

417
00:43:50,480 --> 00:43:59,920
in labs today all over the world. What about inspirations from biology? And it's been mentioned

418
00:43:59,920 --> 00:44:07,360
before about the visual cortex, how that's very much like the way a convolutional neural network

419
00:44:07,360 --> 00:44:14,880
might work. There's hierarchies there. They've got logical receptive fields and shared weights.

420
00:44:16,000 --> 00:44:24,480
Do you think this is important to sort of mimic the way that biology works in the

421
00:44:24,480 --> 00:44:33,120
way that we approach developing a whole brain emulation? Or can it be a completely wildly,

422
00:44:33,120 --> 00:44:41,280
even wildly different computing power time? Yeah. So yeah, modern convolutional neural

423
00:44:41,280 --> 00:44:47,760
networks were intentionally inspired by the visual cortex. It's actually not the brains

424
00:44:47,760 --> 00:44:53,840
resemble convolutional nets. Convolutional nets were sort of built on the previous four decades of

425
00:44:54,480 --> 00:44:59,040
research into how vision works. We actually learned a whole lot about how vision works. And we said,

426
00:44:59,040 --> 00:45:03,200
wait a minute, what if we code that up to a computer and see if we can see? And lo and behold,

427
00:45:03,200 --> 00:45:09,440
it's actually worked incredibly well. So one thing that that story tells is that

428
00:45:10,080 --> 00:45:16,640
biomimicry is a very good strategy. If you figure out how nature does something and then emulate

429
00:45:16,640 --> 00:45:21,760
that with engineering, that's very likely to actually get you pretty far down the road or

430
00:45:21,760 --> 00:45:32,000
whatever task you're trying to solve. For whole brain emulation, there's an interesting opportunity

431
00:45:32,000 --> 00:45:38,320
to cut some really major corners at the possible expense of like a major philosophical or metaphysical

432
00:45:38,320 --> 00:45:49,120
failure. So what do I mean by that? At the lowest level, we could create a whole brain emulation

433
00:45:49,200 --> 00:45:56,960
of neurons and synapses, individually firing action potentials down their axons across the

434
00:45:56,960 --> 00:46:04,480
synapses into the dendrites through the full complex web of neurons. And we would expect

435
00:46:04,480 --> 00:46:09,360
that to certainly be functionally successful. You know, there's a whole big debate about whether

436
00:46:09,360 --> 00:46:13,360
it's successful in some other way, but it's, you know, because some people think it has to be a

437
00:46:13,360 --> 00:46:17,760
biological system to be conscious or it has to be whatever, but but it would functionally replicate

438
00:46:17,760 --> 00:46:23,760
the action potential propagation paradigm. But that's actually pretty computationally like

439
00:46:23,760 --> 00:46:30,960
inefficient. Like why, why recreate a thousand neurons that fire in some complex arrangement

440
00:46:30,960 --> 00:46:37,360
to perform multiplication? If you can just stick a multiplication module in there that has the same

441
00:46:37,360 --> 00:46:43,120
black box behavior of the neurons. If you've got a set of neurons, you've got a set of neurons

442
00:46:43,120 --> 00:46:49,120
and you send input signals in and you read the output signals out, you could replace that set

443
00:46:49,120 --> 00:46:55,680
of neurons with what we with what we call a black box. Ted Berger has been doing this with

444
00:46:55,680 --> 00:47:03,440
hypothalamus. And if you basically produce a prosthesis effectively that takes in neural

445
00:47:03,440 --> 00:47:07,680
signals and emits neural signals like the system that's replacing the set of neurons that's

446
00:47:07,680 --> 00:47:15,360
replacing, you can use a potentially much simpler computational piece of hardware to process those

447
00:47:15,360 --> 00:47:21,200
signals and yet get the same functional propagation, right? So it's an interesting question. How far

448
00:47:21,200 --> 00:47:29,040
that could go? How much of the brain could we abstract away to some sort of algorithm? You know,

449
00:47:29,040 --> 00:47:32,560
one of the one of the biggest ones would be like cortical columns. The cortical columns are these

450
00:47:32,560 --> 00:47:37,200
sets of about a hundred neurons that seem to sort of process kind of as a singular unit, you know,

451
00:47:37,200 --> 00:47:41,440
really know exactly what they're doing or how they're doing it, but there's this repeated

452
00:47:42,800 --> 00:47:49,520
cortical column pattern. If we could solve the cortical column algorithm, we might just black

453
00:47:49,520 --> 00:47:56,080
box the whole thing away. What if you just make a single computer chip that emulates, you know,

454
00:47:56,080 --> 00:48:01,280
the hundreds of neurons and the thousands upon thousands of synapses in a single cortical column

455
00:48:01,280 --> 00:48:05,600
just turn the whole thing into some little computer that may not work the same way at all,

456
00:48:05,600 --> 00:48:10,640
but it will take the same neural signals and it will critically, it will emit the same signals,

457
00:48:10,640 --> 00:48:16,080
so it has perfectly replaced that behavior, but in a completely different way, right?

458
00:48:16,800 --> 00:48:20,560
And then the question is, well, what if you did that with every cortical column in the entire brain?

459
00:48:21,200 --> 00:48:28,080
So a whole lot of the neural structure that we consider to be extremely human in its architecture

460
00:48:28,720 --> 00:48:35,040
would be completely replaced with some sort of computational abstraction. The same signals

461
00:48:35,040 --> 00:48:42,160
would be going through, but would it actually metaphysically be the same sort of mind?

462
00:48:43,120 --> 00:48:52,240
And we don't know, we don't know where that cutoff is, you know, where the sort of level of

463
00:48:52,240 --> 00:49:01,120
fidelity of your emulation loses some sort of metaphysical thing that is deemed important.

464
00:49:01,200 --> 00:49:07,920
We don't know where that threshold is. I suspect that we might find it, you know, because,

465
00:49:08,880 --> 00:49:14,720
you know, as you produce these things, as you produce these prostheses, these modules,

466
00:49:14,720 --> 00:49:20,000
and emulate them, if you get some sort of surprising behavior out of it, then you're

467
00:49:20,000 --> 00:49:26,480
probably abstracting too far. I'm not sure how that would happen, because if it's producing,

468
00:49:26,480 --> 00:49:31,920
if it's reproducing the same functions, there isn't an opportunity for it to sort of behave

469
00:49:31,920 --> 00:49:36,400
differently, like that's the whole point of the functional paradigm. So I'm not really sure where

470
00:49:36,400 --> 00:49:40,800
it could break down, but if it was going to break down, presumably we would discover it. I mean,

471
00:49:40,800 --> 00:49:44,080
otherwise you're just back into philosophical zombie territory, you'd be able to replicate

472
00:49:44,080 --> 00:49:49,600
all the functionality, but you wouldn't know, but then I just find that all very hard to buy.

473
00:49:49,840 --> 00:49:57,760
Yeah. What do you think, look, I mean, there's the issue of like the fire in the equations when

474
00:49:57,760 --> 00:50:03,120
like Hawking spoke about, like, what is understanding of a mathematical formula?

475
00:50:04,000 --> 00:50:08,640
What is the, you know, what's the important, what's the important difference between the way

476
00:50:08,640 --> 00:50:14,080
or computers at the time when you wrote this subject would be able to calculate and the way

477
00:50:14,080 --> 00:50:19,440
a human would be able to calculate? And he spoke about fire in the equations. I think it was Hawking

478
00:50:19,440 --> 00:50:24,960
anyway. Maybe it's got something to do with like something like that and embodied understanding,

479
00:50:25,600 --> 00:50:30,880
maybe a feeling. I don't know a valence of it. It's going to be one of the big questions of

480
00:50:30,880 --> 00:50:39,280
neuroscience in coming decades. Sure. Well, okay. Another big question that I thought to bring up

481
00:50:39,280 --> 00:50:47,360
was why is whole, a form of whole brain emulation of your preserved brain, probably you, and you've

482
00:50:47,360 --> 00:50:51,040
written directly about this, but I thought I might bring this up during the interview.

483
00:50:51,040 --> 00:50:57,520
Why do you think whole brain, a whole brain emulation of Keith Wiley would be Keith Wiley,

484
00:50:57,520 --> 00:51:04,080
even if the original was still alive? Well, that's that last sentence really. Okay. Where

485
00:51:04,080 --> 00:51:10,480
that's where people start the big debates, right? Well, let's step back. Let's, let's exclude that

486
00:51:10,480 --> 00:51:17,600
last, that last sentence then, because we will touch on identity branching. Yeah. Better. I mean,

487
00:51:17,600 --> 00:51:23,040
that has been the bulk of my writing is defending the branching interpretation. But yeah, why would

488
00:51:23,040 --> 00:51:31,040
it be me? So there are different theories of metaphysical personality. For example, you have

489
00:51:31,040 --> 00:51:35,920
the body theory, which basically says that your identity is attached to the matter that you're

490
00:51:35,920 --> 00:51:41,520
built out of. You have a stream of consciousness theory, which is that it's some sort of continual

491
00:51:41,520 --> 00:51:45,520
stream of consciousness. You have a space-time worm identity, which is that this thing sort of

492
00:51:45,520 --> 00:51:51,040
weaves its way through four-dimensional space-time, because it's sort of continuously located in

493
00:51:51,040 --> 00:51:56,960
sort of a smooth path. The identity is sort of following the sort of worm through time,

494
00:51:57,840 --> 00:52:02,400
but it doesn't discontinuously jump around. And then you have psychological identity. There are

495
00:52:02,400 --> 00:52:08,480
there others as close as continual identity. Close as continual identity says that the thing

496
00:52:08,480 --> 00:52:15,920
that is you now is that which most closely resembles the thing that was you, you know, very

497
00:52:15,920 --> 00:52:20,880
recently. And you have to get into the thought experiments to sort of feel sort of where that

498
00:52:20,880 --> 00:52:27,360
question comes up. But another theory is psychological identity, which is a catchall term

499
00:52:27,360 --> 00:52:32,640
for, by psychological, what we mean is the memories. Your identity is indicated by

500
00:52:33,360 --> 00:52:38,640
your psychological traits, your personality traits, your memories, your all the sort of cognitive,

501
00:52:39,600 --> 00:52:46,960
emotional, intellectual sort of senses of your, that's your identity. Every single one of these

502
00:52:47,680 --> 00:52:52,960
theories can be subjected to sort of colorful thought experiments that try to sort of break them

503
00:52:52,960 --> 00:53:01,600
apart and undo them and find some sort of paradox that leaves them. And the interesting thing about

504
00:53:01,600 --> 00:53:11,440
the psychological theory is that it only breaks down if you declare at the outset that you're

505
00:53:11,440 --> 00:53:18,480
going to reject branching interpretations. So you basically have to say, well, branching is just

506
00:53:18,480 --> 00:53:22,160
going to cause a lot of trouble for my thought experiments. So I'm just going to say it's excluded

507
00:53:22,160 --> 00:53:27,040
and we're not allowed to consider it. And if you say that, then you can come up with a thought

508
00:53:27,040 --> 00:53:32,160
experiment where psychological identity sort of runs into problems for the same way all the other

509
00:53:32,160 --> 00:53:37,520
identity problems models can run into problems. But if you don't cut yourself off with the knees at

510
00:53:37,520 --> 00:53:41,440
the beginning, if you say, well, wait a minute, branching identities on the table, let's see

511
00:53:41,440 --> 00:53:49,760
where that takes us. Then psychological identity is the most robust theory that I have yet found.

512
00:53:52,480 --> 00:53:58,560
Well, okay, so you're going to be talking a lot about this in your up and coming novel,

513
00:53:58,560 --> 00:54:05,280
contemplating oblivion. So anybody who's interested in these topics would like to explore worlds in

514
00:54:05,280 --> 00:54:13,200
which they are covered, yeah, watch this space or watch out for contemplating oblivion.

515
00:54:14,240 --> 00:54:19,680
So but yeah, would you want to just give us a breakdown of some of the writings you've

516
00:54:20,400 --> 00:54:29,520
covered identity branching in? It's your favorite topic. This is what you've covered the most.

517
00:54:30,160 --> 00:54:38,880
Yeah. So it all started actually started slightly before my 2014 book. I was inspired to

518
00:54:38,880 --> 00:54:44,400
write an article in which I sort of thought I would lay out my basic sort of theory of branching

519
00:54:44,400 --> 00:54:49,680
identity. In early 2014, I'm sort of going through my own list of writing here.

520
00:54:50,960 --> 00:54:55,520
Was this the H plus magazine? Let me think here. It was

521
00:54:59,200 --> 00:55:04,320
Oh, in March of 2014, it was actually a response to a article by Susan Schneider.

522
00:55:05,760 --> 00:55:11,120
I remember. Yes. And I wrote an article for H plus. Yes. I think you might have emailed me about

523
00:55:11,120 --> 00:55:16,800
that and got me to put that in there when I was on the board. Yeah. Yeah. And I'm looking at the

524
00:55:16,800 --> 00:55:20,560
so if that was March 2014, that means that I basically wrote that article. And then

525
00:55:21,280 --> 00:55:28,160
I must have come off of that article with ideas for a book just boiling because I started it

526
00:55:28,160 --> 00:55:37,920
immediately. I worked over I worked on the book that summer of 2014. So the larger work I produced

527
00:55:37,920 --> 00:55:41,840
is my book attacks on me and metaphysics of mind uploading.

528
00:55:44,480 --> 00:55:51,440
And then from there, I just continued to write. So I've I produced a paper in the Journal of

529
00:55:51,440 --> 00:55:58,400
Consciousness Studies with Randall Conan, which tackles one very specific question, which is,

530
00:56:00,000 --> 00:56:04,480
you know, we taught I've been talking about the slicing and scanning approach

531
00:56:05,200 --> 00:56:12,240
to whole green emulation. Another thought experiment or scenario that comes up in a lot

532
00:56:12,240 --> 00:56:19,760
of philosophical musing is what we call gradual replacement, where you perfuse the brain with

533
00:56:19,760 --> 00:56:28,000
billions of nanobot prosthetic neurons, some sort, and they they learn the input output function

534
00:56:28,000 --> 00:56:33,760
of the neurons. And then once they've got that function figured out, they kill the neuron and

535
00:56:33,760 --> 00:56:38,160
like attached to all of its synapses and slowly over time, the whole brain becomes mechanical.

536
00:56:39,520 --> 00:56:47,520
I think it's all complete and total nonsense. It's it's a fun thought experiment. It makes

537
00:56:47,520 --> 00:56:52,640
maybe potentially good science fiction. I don't I don't know. But it's not practical.

538
00:56:54,400 --> 00:57:01,280
It's sort of it's very sobering to read sort of online debates about the stuff in which people

539
00:57:02,240 --> 00:57:08,480
you know, with great emotional commitment, you know, say that they really require like a gradual

540
00:57:08,480 --> 00:57:14,080
replacement process in order to be uploaded. Well, I got news for you. It's not going to happen.

541
00:57:14,720 --> 00:57:22,320
That is so technically challenging that I don't know if we'll have it in a thousand years. It is

542
00:57:23,040 --> 00:57:32,000
beyond incomprehensibly challenging, whereas scanning slices of preserved brains is a 20th

543
00:57:32,000 --> 00:57:37,760
century tech. We're doing it now. So we're talking about the trade-offs between a technology that exists

544
00:57:37,760 --> 00:57:43,200
now and a technology that I'm not sure if it will exist in a thousand years. That is the that is the

545
00:57:43,200 --> 00:57:51,920
chasm between these two approaches. So Randall and I wrote a paper that argues not only, you know,

546
00:57:51,920 --> 00:57:58,080
is gradual placement just, you know, technically just bonkers. But from from a philosophical

547
00:57:58,080 --> 00:58:02,320
standpoint, we don't have to worry about it. It doesn't matter. It actually isn't a critical

548
00:58:02,320 --> 00:58:07,600
need anyway. The whole point of the paper is to argue that metaphysically and interpretively,

549
00:58:08,160 --> 00:58:14,320
both of these two procedures have the same outcome from sort of the perspective of personal identity.

550
00:58:14,320 --> 00:58:18,240
So we don't have to worry about the impossible gradual placement process, the process, the

551
00:58:18,240 --> 00:58:24,000
scanning of a preserved brain will work just fine. So that paper was in the I was in JOCS.

552
00:58:25,440 --> 00:58:29,920
I've written several others. I wrote sort of an all-encompassing paper that was sort of a broad

553
00:58:29,920 --> 00:58:38,320
sort of life philosophy paper with the tongue-in-cheek title of mind uploading in the question of life,

554
00:58:38,320 --> 00:58:46,080
the universe and everything, obviously an homage to Douglas Adams. And the the thrust of that paper

555
00:58:46,080 --> 00:58:53,840
is sort of a step-by-step almost theorem like layout of an argument that argues that

556
00:58:55,360 --> 00:59:07,520
developing mind uploading is the most important priority ever. It is the most important thing

557
00:59:07,520 --> 00:59:12,000
for us to work on. And the reason why it is the most important thing is the entire sort of line of

558
00:59:12,000 --> 00:59:16,080
reasoning laid out in the paper. One might immediately sort of knee-jerk response that

559
00:59:16,080 --> 00:59:20,800
well that's just ridiculous. How can I possibly say that? Well, read the paper. The whole point

560
00:59:20,800 --> 00:59:27,360
of the paper is to explain why I think that's so critically important. And then I've written two

561
00:59:27,360 --> 00:59:35,440
papers that get into this argument that a breakage in the stream of consciousness implies a death

562
00:59:36,080 --> 00:59:44,320
and then some sort of invoked doppelganger in the output. I found that such as such an important

563
00:59:44,320 --> 00:59:49,680
topic I actually ended up writing two papers on it. I don't know, I probably have others too, but

564
00:59:51,280 --> 00:59:53,360
yeah, I sort of keep keep writing about it.

565
00:59:56,480 --> 01:00:02,400
Well, I mean, do you think, how do you think people will react? I mean, when push comes to shove,

566
01:00:02,480 --> 01:00:08,880
do people really care if there's a doppelganger or if what they feel is a doppelganger out there?

567
01:00:08,880 --> 01:00:14,960
Do they mind if there's two identities that are metaphysically exactly the same, at least on

568
01:00:14,960 --> 01:00:21,040
invocation? I think a lot of people are uncomfortable with this idea. I don't know if the concern is

569
01:00:21,040 --> 01:00:28,080
whether there's either one or two of them. The concern is that by being some sort of a copy,

570
01:00:28,080 --> 01:00:32,480
it represents that they actually died. They didn't really survive the procedure.

571
01:00:32,480 --> 01:00:37,120
They died. The entire goal of surviving through the procedure into the upload failed,

572
01:00:37,840 --> 01:00:40,640
and the person living on as the upload is just some other person.

573
01:00:42,000 --> 01:00:47,840
And therefore, the entire goal of using this procedure as a method of survival has failed.

574
01:00:47,840 --> 01:00:55,600
So that's the concern. That's why people sort of get very knotted up about whether or not it's a copy.

575
01:00:58,800 --> 01:01:05,040
Okay. Well, was there any interesting points that you wanted to... There was a recent meeting

576
01:01:05,040 --> 01:01:10,640
you had, like, I guess, you know, is there anything that you discussed at this meeting,

577
01:01:10,640 --> 01:01:18,000
at the Hold By An Emulation meeting with Anders Sandberg, Randall Kruhner, and yourself and others?

578
01:01:18,000 --> 01:01:24,800
Robin Hansen was there. Yeah. Is there any points that you brought up that you thought you might be

579
01:01:24,800 --> 01:01:31,440
able to reveal? Well, okay, so the topic of that meeting for the audience, the topic was

580
01:01:31,440 --> 01:01:40,640
the ethics surrounding Hold By An Emulation. So it got into several different questions. It got into

581
01:01:41,920 --> 01:01:45,760
I'm just sort of reading out of the notes here, the future of work. You know, what will it mean,

582
01:01:45,760 --> 01:01:52,400
what will work mean when to do work consists of spinning off a Hold By An Emulation and having

583
01:01:52,400 --> 01:01:58,080
it do the work and then shutting down the emulation when the work is done? What does that say about,

584
01:01:58,640 --> 01:02:02,400
you know, people who are always concerned about technology taking jobs? So if we're just spinning

585
01:02:02,400 --> 01:02:07,200
off WBEs, you know, Hold By An Emulations, then how are we ever going to get a paycheck ever again?

586
01:02:07,200 --> 01:02:13,360
But then if Hold By An Emulations are, you know, really sort of verging on conscious, you know,

587
01:02:13,360 --> 01:02:18,960
human minds, there's a whole other question about work, which is, are we making, you know, an entire

588
01:02:19,040 --> 01:02:26,000
society of slaves, right? So there's a whole, instead of that came up. Then we talked about

589
01:02:26,000 --> 01:02:30,160
just sort of good old fashioned existential risk. It's a topic that always comes up everywhere.

590
01:02:30,880 --> 01:02:36,880
I don't actually remember exactly where the conversation went. I don't tend to have like

591
01:02:36,880 --> 01:02:41,600
a crystal clear like immediate short term memory. I'm the kind of person you have to look at the

592
01:02:41,600 --> 01:02:46,080
notes afterwards to remember and I don't have the notes from the front of me. But I can go

593
01:02:46,080 --> 01:02:50,560
through the topics. We talked about, oh, an interesting one was, you know, once Hold By

594
01:02:50,560 --> 01:02:55,520
An Emulation is sort of common, what are the implications in terms of neuro security and

595
01:02:55,520 --> 01:03:00,560
mental privacy? So this is a concern not for us, but a concern for the well-being of Hold

596
01:03:00,560 --> 01:03:07,280
By An Emulations. What rights should they have to the security of their brains and minds against

597
01:03:07,280 --> 01:03:13,280
hacking? And what rights do they have to privacy? You know, once, you know, a Hold By An Emulation

598
01:03:13,360 --> 01:03:20,080
can probably be interrogated more easily. So it'll basically be more difficult for it to lie,

599
01:03:20,080 --> 01:03:26,640
you know, because does it have the right to lie? We talked about access and inequality, democratization

600
01:03:26,640 --> 01:03:31,520
of the technology. This is sort of a, you know, a very common concern that the rich elites will

601
01:03:31,520 --> 01:03:36,160
not only get it first, but we'll get it first and then somehow lock it down so that, you know,

602
01:03:36,160 --> 01:03:39,760
instead of just coming along 10 years later at the scale of sort of technological advancements,

603
01:03:39,760 --> 01:03:47,840
somehow everyone else will get sort of locked out. We talked about sort of the ethics of using

604
01:03:47,840 --> 01:03:53,600
animals for research and development and then what the implications are for using increasingly

605
01:03:54,720 --> 01:04:01,600
verisibiltudeness emulations in sort of, in lieu of animals. You know, if there's an ethics question

606
01:04:01,600 --> 01:04:06,880
about using an animal in an experiment, is there an ethics question about using a really,

607
01:04:06,880 --> 01:04:11,840
really good emulation of an animal? I mean, so good that the emulation might be conscious.

608
01:04:12,720 --> 01:04:16,960
You know, what sort of ethical concerns are there about using a system like that experimentally?

609
01:04:18,640 --> 01:04:22,560
Once we get into population ethics and emulations, I'm not sure what that refers to.

610
01:04:24,480 --> 01:04:28,080
Yeah, I don't know. We just, we talked about a lot of stuff, but it was all sort of from an

611
01:04:28,080 --> 01:04:30,720
ethical perspective. That was the point of that panel.

612
01:04:31,360 --> 01:04:39,600
Yeah, so I mean, this probably comes up before a lot of people. So as futurists with some sort of

613
01:04:40,560 --> 01:04:47,920
idea of how mind uploading might work, we might be concerned about friends or family who have a

614
01:04:47,920 --> 01:04:57,600
short time to live. And the difficulty in discussing this with people who have not got an interest

615
01:04:57,600 --> 01:05:04,000
in futurology. But have you got any advice on how to speak to people about the possibility of

616
01:05:04,000 --> 01:05:08,560
survival beyond the flesh? You know, I sort of find myself having this conversation a lot. I'm

617
01:05:08,560 --> 01:05:12,160
just sort of, you know, telling people about my book. And I have to, you know, break through that

618
01:05:12,160 --> 01:05:17,840
initial barrier of people just sort of having to sort of an initial shock that my book is about,

619
01:05:17,840 --> 01:05:26,560
what? That's ridiculous. So I have that conversation a lot. People are actually

620
01:05:27,280 --> 01:05:32,800
generally receptive to the idea of it. I'm sure there's an entire population contingent that just

621
01:05:32,800 --> 01:05:36,800
sort of has this very powerful religious objection. And I just, you know, don't tend to cross paths

622
01:05:36,800 --> 01:05:44,960
with those sorts of people in my life. But people are just sort of broadly curious about technology,

623
01:05:44,960 --> 01:05:50,160
but maybe they don't make it their entire daily life, and sort of think about these things. But

624
01:05:50,160 --> 01:05:55,520
most of their exposure is through rather poorly written science fiction, mostly written science

625
01:05:55,520 --> 01:06:01,040
fiction is often pretty good. Hollywood is generally terrible. So, you know, if your exposure to

626
01:06:01,040 --> 01:06:08,000
these ideas is mostly movies coming out of Hollywood, then you're probably, you know, not getting the

627
01:06:08,000 --> 01:06:15,840
richest philosophical presentation of the ideas. So, you know, what do I do? I talk to people about

628
01:06:15,840 --> 01:06:26,000
it. I just try to sort of not be overly prescriptive, you know, sort of telling people sort of what I

629
01:06:26,000 --> 01:06:32,560
think they ought to believe. It's more a question of whether or not this ought to be available as

630
01:06:32,560 --> 01:06:37,920
an option for people who would choose it without putting some sort of prohibition against it.

631
01:06:37,920 --> 01:06:42,880
And then people who are uncomfortable with it, I don't have to do it. I mean, nobody's forcing them.

632
01:06:43,440 --> 01:06:49,360
But, you know, sort of try to convince people that it's just a typical sort of let's let people

633
01:06:49,360 --> 01:06:57,280
make decisions for their own sake kind of issue. If you can get there, then that's usually, you

634
01:06:57,280 --> 01:07:06,240
know, a pretty good conversation. I think people are sort of resistant, you know. People are always

635
01:07:06,240 --> 01:07:10,080
trying to figure out whether or not the government needs to prohibit something. I don't entirely

636
01:07:10,080 --> 01:07:16,640
sure why, but there seems to be something people worry about a lot. So, I think they may just have

637
01:07:16,640 --> 01:07:21,760
to evolve through a few generations. You know, with each passing generation, they don't have to

638
01:07:21,760 --> 01:07:28,960
actually shed their prior sort of biases from childhood because they're just sort of born into

639
01:07:28,960 --> 01:07:33,360
an increasingly technological world. They will be born into a world of increasingly

640
01:07:35,120 --> 01:07:40,160
sort of versatile AI systems with which they interact. You know, their teddy bears are going

641
01:07:40,160 --> 01:07:48,000
to be increasingly lifelike with each passing generation. And by the time anything, even vaguely

642
01:07:48,000 --> 01:07:53,120
resembling an Ulbrian emulation is possible, that generation will have been interacting

643
01:07:53,840 --> 01:07:59,920
with computers that are so smart that they will already seem lifelike anyway. And the whole

644
01:07:59,920 --> 01:08:05,600
concept is just going to be relatively comfortable to that future generation. We just, I don't know,

645
01:08:05,600 --> 01:08:09,440
I would, I don't really worry too much about our generation because it's not going to happen in our

646
01:08:09,440 --> 01:08:14,320
time anyway. We just need to keep the research money flowing. You know, so long as there isn't a

647
01:08:14,320 --> 01:08:21,680
general sort of hatred of science, then we're doing okay. Do you think choir preservation for

648
01:08:21,680 --> 01:08:30,640
yourself is an option to reach for your, for yourself to reach a time in which whole brain

649
01:08:30,640 --> 01:08:38,960
emulation is possible? So the Brain Preservation Foundation has multiple sort of goals. One is to

650
01:08:38,960 --> 01:08:43,680
sort of advance the actual technology of preservation to sort of actually get the thing

651
01:08:43,680 --> 01:08:49,600
available, make it technically possible. But then there's this sort of advocacy component of, you

652
01:08:49,600 --> 01:08:57,840
know, you know, how can this procedure become part of sort of gender mill society, just in

653
01:08:57,840 --> 01:09:03,200
general, procedure available in hospitals or something like that. I think those gears turn

654
01:09:03,200 --> 01:09:10,000
pretty slowly. That's a lot of politics. It's a lot of red tape and sort of a lot of regulation.

655
01:09:11,520 --> 01:09:16,880
You know, I just, I tend to be pretty tempered about this stuff. You know, other people just get

656
01:09:16,880 --> 01:09:23,760
really excited about it. They think everything's going to happen in 20 years. And I don't know

657
01:09:23,760 --> 01:09:34,880
if I'm cynical or just realistic. And I just, I would be absolutely amazed if the technology

658
01:09:34,880 --> 01:09:46,560
of human brain preservation was cheap enough and available enough to serve me. But maybe if

659
01:09:46,560 --> 01:09:51,360
something happens very rapidly in the next few decades, then maybe, but I don't know.

660
01:09:52,160 --> 01:09:57,200
But we're probably only missing it by one or two generations. It's pretty sad to think we might be

661
01:09:57,200 --> 01:10:03,440
one of the last generations ever that sort of has to contend with this. But I mean, I don't know,

662
01:10:03,440 --> 01:10:08,080
it's more exciting than missing it by a thousand years. I mean, at least we get to sort of understand

663
01:10:08,080 --> 01:10:12,400
it, right? You know, a thousand years ago, no one even understood any of this. So that's pretty,

664
01:10:12,400 --> 01:10:18,880
that's pretty, that's good enough. It'll have to be. Well, who knows? I mean, like we get AI

665
01:10:18,880 --> 01:10:24,400
powered scientists and there's some claim that would that could really fast forward a lot of the

666
01:10:25,120 --> 01:10:30,960
science. Maybe there's a chance. I don't know. Yeah. You mentioned that there's a lot of written

667
01:10:30,960 --> 01:10:37,520
science fiction, which you are more, I guess, amenable to, which gives better descriptions of

668
01:10:37,520 --> 01:10:44,880
how whole brain emulations might work. What science fiction is that? What's been your favorite?

669
01:10:44,880 --> 01:10:51,920
So a few years ago, I sort of sent myself on a mind uploading voyage, trying to find, you know,

670
01:10:51,920 --> 01:10:58,400
all the good sci-fi on it. Of course, I'm sure I missed, you know, 90% of it. But Greg Egan's

671
01:10:58,400 --> 01:11:06,160
permutation city is famous, Arthur C. Clark's city and the stars. Alter carbon, which of course

672
01:11:06,160 --> 01:11:13,360
was a Netflix hit. There were others because I had like four, at least five of these books I sort

673
01:11:13,440 --> 01:11:19,760
of rattled through. So I'm missing a few. But it's definitely a topic that's been presented and

674
01:11:19,760 --> 01:11:30,880
covered pretty well in some cases. All right. Well, yes. Is there any points that you wanted

675
01:11:30,880 --> 01:11:35,600
to talk about that you, that I haven't yet asked you that you thought might be worth bringing up?

676
01:11:36,960 --> 01:11:40,240
Well, I think you and I will talk about my upcoming book in another part

677
01:11:41,200 --> 01:11:47,840
session. So I'll leave that to then. So yeah, it was nice to have an opportunity to sort of

678
01:11:47,840 --> 01:11:52,880
talk about these ideas again. It's been a while. Yeah, it has. Thanks. Thanks very much, Keith.

679
01:11:52,880 --> 01:11:57,520
It's really good to talk to you again. And yeah, I look forward to our conversation about your

680
01:11:57,520 --> 01:12:12,800
up and coming book contemplating oblivion. Thank you very much. Cheers.

