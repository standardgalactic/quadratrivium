start	end	text
0	8840	Today everybody, thanks for joining us. We've got Keith Wiley here today who's a board member
8840	14920	of Carbon Copies and has written extensively on the concepts of mind uploading and whole
14920	21120	brain emulation. He's the author of a Taxonomy and Metaphysics of Mind Uploading as well
21120	27480	as the author of a new book which is coming out very soon, which is a novel this time,
27480	34600	our fiction Contemplating Oblivion which should be coming out sometime this month. So welcome
34600	40280	and it's great to have you here again. Thank you, it's great to be back. Now I guess mind
40280	45760	uploading has a concept that's been kicked around for a while now. Why don't you think
45760	52280	that mind uploading has enjoyed as much wide acceptance within the philosophical community,
53200	60840	as it has in the futurist community? Well, I guess I don't know exactly why people are
60840	66200	answering questions the way they do, because they tend to be rather terse interviews that
66200	74240	don't really have a lot of follow-up. But I think that the, you know, the sort of really
74280	84600	contentious questions of the right around this whole body identity concern and the copy problem,
84600	91800	which I actually consider to be a category or are really hard to get away from. And in fact,
91800	101760	I think the recent sort of, you know, popularity of the discussion has probably made people think
101800	107160	about it just enough to convince themselves that it doesn't work, but not to really think past
107160	113600	that point. So there is a certain sort of initial reaction, I think, where you're presented with
113600	120120	the idea of a non-destructive uploading scenario. It could be a transporter-like scenario from
120120	125960	Star Trek or just a more sort of direct mind uploading scenario, but it's presented in a
125960	129960	non-destructive fashion and people think about it for a minute. They sort of initially have this
130000	135560	cut reaction that something about it must not have worked because of the famous copy problem.
135560	145440	And, you know, the bulk of my writing has been an attempt to chip away at that sort of instinctive
145440	151480	reasoning and, you know, really work through it beyond an intuitive sort of feeling and see
151480	159920	where a logical analysis really lands. So there are these various reasons that support
160160	166280	the initial intuition. People often raise concerns about breaking the stream of consciousness or
166280	171160	good old-fashioned body identity in which sort of they feel like their identity has to stick to
171160	176800	their atoms or what I call space-time worm identity, which sort of says that, you know,
176800	182960	it seems a little odd that your identity might discontinuously jump around in space. You have
182960	189600	to sort of go past the sort of the click-off-the-cuff reaction to sort of ask whether that
189680	195840	actually makes any sense. And, you know, I personally have always landed pretty squarely on
195840	201920	the psychological model of identity, which is memory-based. You know, identity is indicated by
202560	208160	some unique conglomeration of a lifetime of memories. And just to be clear, I use the word
208160	217120	memory in a broader sense than what most people might think. I don't just mean episodic memories
217200	223840	of events from your life. For me, the word memory means the totality of neural encodings in the
223840	231360	brain. So there's a lot of subconscious personality traits. There's actually a lot of muscle
231360	236240	memory. You know, sort of the ability to play the piano is a memory, even though it's a completely
236240	241920	subconscious memory stored primarily in the cerebellum, presumably. So by memory, I just mean
241920	247360	neural encodings within the brain. But nevertheless, my preferred model of identity
247360	255520	pins identity to that feature. And that leads one towards certain conclusions. First of all,
255520	263040	that uploading works because it's all memory-based. So if you sort of, if you consider a thought
263040	268800	experiment in which those memories remain intact through the uploading procedure, then by definition
268800	273200	it worked because that was the premise. In order to reject that, you have to sort of
273840	277520	figure out why you reject it. You're obviously going to reject it on the basis of some,
278640	283920	you know, not full commitment to the memory basis of identity. You're looking at some other
283920	291120	component. And I just don't do that. I just always found that the psychological memory theory works best.
291440	299200	Okay. So, I mean, it's interesting that, like, if it's totally based on memory, surely you'd have
299200	306480	to have some sort of mechanism to be able to process consciousness, valence, awareness,
308080	316880	and the memory as well in order to have an agent feel as though it's got some sort of stream of,
317760	321760	or persistence of identity and enduring metaphysical ego.
322560	328640	Yes, yes. There is this, right. So you've got the concept of identity and you have the concept of
328640	334480	consciousness. And they're not necessarily the same thing, which is where the famous thought
334480	340240	experiments of philosophical zombies come from. You could have agents, you know, cognitively
340240	347280	processing in the world and, you know, encoding memories of a form, although it's hard to imagine
347280	350960	what reflecting on a memory consists of, if you have no consciousness, I'm not quite sure what
350960	355760	that comes down to. But nevertheless, you can sort of separate these components and you can ask,
356480	364160	what would it be like to be an unconscious but yet memory fulfilling system? I think it's mostly
364240	370640	philosophical whimsy. I don't actually think that philosophical zombies are a practical reality,
370640	378720	they're just sort of a philosophical exploration. Well, as David Chalmers brought up the thought
378720	384000	experiment is if you had an atom for an atom reconstruction of a human that, you know, the
384000	388800	first human had consciousness and the second human didn't have consciousness, although all
388800	396160	their atoms and physically they were exactly the same, wouldn't that imply dualism? So, but I mean,
396160	401520	people take the thought experiment to mean something else and it's been used to describe
401520	408000	artificial intelligence without the architecture or the phenomenology of consciousness as well.
408000	414560	So yeah, so we're, this interview is evolving quickly, we're getting into some other topics, but
415280	420960	yes, there is a huge question in the last three years, give or take,
421840	429760	you know, what the implications are of the huge LLM models? What does that actually have to say,
429760	435920	not only about artificial intelligence, but also about consciousness? You know, are these things
435920	440800	conscious? Do they actually have a sentient sort of set of rights and things like that?
441440	447120	A lot of people, I think with pretty good reason, think that the LLMs are not satisfying those
447120	451920	requirements. They're doing something else, but they're probably not conscious in the way that
451920	457200	we're particularly concerned about. I don't think that they have a lot of recurrent
458960	464560	wiring in their layers, they tend to be pretty feed forward heavy. And there's, you know, I'm
464560	467600	definitely one of those people who thinks that recurrence is doing something, I don't quite
467680	474800	know what, but I think recurrence is a crucial component of consciousness in some fashion,
475520	482160	sort of sense of kind of looking back on on itself in a sort of a loop of sort of experiential
482160	487360	self-learn looking. We don't know metaphysically like why, you know, what is it about that?
487360	492400	That's critical to consciousness, but a lot of people seem to agree that some kind of recurrence
493200	499520	should be part of the design. Short of that, you know, something that's always bothered me about
499520	507040	the modern AI systems is that they're all sort of on and off and then they're in the dark. You
507040	513600	query them with your question, they go off and they dig through their neural net, they come up
513600	519760	with a result and they produce for you a paragraph of text or image or a digitized voice, and then
519760	525280	they go dark. That's it. Like they're not sitting there daydreaming, they're not musing on the
525280	529680	conversation you're having with them going back over, you know, the previous dialogue from last
529680	537120	minute. They're not doing, hey, they're dead while the prompts, if they're blinking until you issue
537120	544960	your next query. So is that, can a system like that be conscious? This is an interesting question,
544960	551040	and are these systems conscious on the basis of that sort of strange operation, which is just
551040	556240	very not lifelike. It's just not what we think of. It's not what we do with people. You know, a person
556240	561600	doesn't sort of sit there like a rock until you go up to them and sort of, you know, you query them
561600	567440	and then they do something and they emit and then they die until you interact with them again. Like
567440	573920	people don't work that way, animals don't work that way. So natural brains seem to be in a constant
573920	579200	flux. That's what it means to be a dynamic living brain. And modern AI systems don't do that.
580000	584400	By definition, it's just not what they do. So I don't think they're conscious. I think it's pretty
584400	592000	clear that they're lacking these major features that we would generally assume we expect before
592000	597840	we're going to put those labels on it. I don't know. Yeah, what a time to be alive where we
597840	604800	beginning to be able to, I guess, empirically investigate some of these questions in more,
604800	612240	with more precision, and also with a whole lot more data and we can test assumptions in artificial
612880	618480	sort of substrates instead of just, you know, trying to probe directly the human mind.
619440	624480	Not averse to doing the, the, the latter though, I think it's really interesting where brain
624480	634640	computer interfaces could go. But given the, yeah, so what, what kind of resolution or fidelity of
635520	645200	memory do you think is required for persistence of, in order to either reconstitute or persist
646160	654480	an identity or a sense of self? Yeah, I'm not. How do we approach that question? Yeah, maybe it's a
654480	659040	better question. Well, I think the first approach is to bring in the professionals of which I am
659040	664000	not. I think that there are, you know, psychologists, psychiatrists and neurologists probably have,
665360	669680	there's probably a lot of very established science in sort of how memory evolves over the course of
669680	678880	one's life. And what degrees of memory loss imply, you know, sort of noticeable losses of
678880	686000	personality or, or sort of the, the external third, third party perception that a person is
686000	691040	sort of being lost over the course of dementia, you know, we can say, you know, if you lose your
691040	696720	memory at a certain slow steady rate, it's not dementia and you're not really losing your
696720	702480	personality. But if it happens faster, or if it happens more discontinuously, then we as external
702480	708400	observers sort of recognize that there's some sort of identity collapse sort of in process, you know.
708400	718960	So I don't, I don't claim to, you know, beholding the professional sort of state of that. But it's
718960	723440	something like that, you know, it certainly can't be a requirement of perfect fidelity because
723440	728400	we are, this is, this is Thomas Reed's experiment from, from the 1700s that people are, you know,
728400	733760	over the course of their lives, always constantly steadily shedding their old memories. And yet
733760	738320	we consider, consider it to be a persistence of identity so long as the shift is, you know,
739280	746400	you know, smooth below some threshold. But, you know, to give you an actual answer to that question,
746400	754560	I don't know. I do think it does imply, though, that when we try to create
754560	760240	mind uploads, you know, and things of that nature, there's going to be a lot of wiggle room. You know,
760240	766880	people who demand that before a mind upload can be bonafide, it has to preserve every quantum state
766880	772480	in the brain or some complete nonsense like that are, I just think they're off the map. I mean,
772560	778160	it can't possibly require anywhere near that level of fidelity for us to judge the result
778160	784080	to be a successful preservation of identity. And that's just ridiculous. But again, I don't
784080	787200	actually know what the threshold is going to be. But I think we're going to have a lot of leeway.
787200	789760	I think there's going to be a lot of flexibility once we figure out how to do this.
791040	798240	Right. And previously in an interview in the past, you've mentioned the idea of a child
798800	805040	had drowned in very, very cold waters underneath ice or something like that. And in upwards of an
805040	811840	hour after drowning or being out of it for an hour, they've been able to be revitalized.
813760	821200	So this cuts to a very specific argument. A common argument that comes up in
821520	831360	debates about whether or not a mind upload preserves your identity is a concern that people have
831360	836080	about whether or not it preserves your stream of consciousness. There is this perception that you
836080	845120	have this permanent lifelong and never ending ongoing dynamic stream of processing. And if
845120	850800	that breaks and restarts, such as you go into some sort of, you know, mind uploading operation in
850800	855600	your entire brain is vaporized, but it's scanned in the process and then it's revitalized in a
855600	860960	whole brain emulation, a computational model. That breakage marks a loss of identity and the
860960	866800	person dies by definition. And then a new doppelganger basically springs up out of the ether
866800	872480	and takes the place. So that's the basis of that argument. And I've written not one, but in fact,
872480	879600	two papers on this very specific question. They have very similar titles there. And I don't know,
879600	889600	I think they're both worth reading, but there is a proof positive argument that that whole line
889600	897680	of reasoning doesn't work. So, you know, the way a sort of an exploratory debate will usually approach
897680	902000	this is like, well, when you fall asleep, you sort of lose a little bit of consciousness. Does that
902000	905200	break your stream of identity? And then you say, yeah, but you still got a lot of neural processing
905200	910880	going on. So that doesn't count. Okay, well, you go into the ER, you go into general anesthesia.
910880	915200	That's really deep. That's really, you know, that's really turning the brain down a lot. Maybe,
915760	919040	you know, does that undo the argument? They say, well, no, even in general anesthesia,
919040	922160	you know, you still got a lot of neural processing going on. So now that doesn't count.
922960	925920	There's just sort of a lot of knockdown arguments here that you're trying to sort of
926720	932960	work your way through. And eventually you get to the fact that there are real world medical cases
933920	940480	of people under two different scenarios, one accidental and one intentional sort of an OR
940480	948240	procedure that basically disprove this whole thing. So there is this accident tragedy,
948240	955040	usually called rapid frigid drowning. It's occurred a few times. It either occurs when
955040	960400	someone falls into a frozen lake or occasionally in a dry example, they just fall into a snow drift,
960400	964160	essentially. These are these are sort of real medical cases that have come up.
965520	970000	And they are found several hours later, seven, seven, eight hours later.
972080	974960	And in a few, you know, most in most cases, like this person dies.
976480	983760	In a few rare medical cases, they dropped in temperature quickly enough that instead of
983760	988960	killing them, it essentially preserved them the same way a freezer preserves food from spoiling.
991120	998240	And in a few rare medical cases, those people have been miraculously brought to an emergency room
998240	1002080	and they had they basically shot from the hip. It's not like this happens often enough that
1002080	1006080	they know what to do. But, you know, the doctors basically went through the motions of trying to
1006080	1010240	slowly warm a person up so that they don't go into shock and various things of that nature.
1010240	1014880	And there have been cases of recoveries. Okay, what's the point of all this? The point is,
1015440	1020480	well, first of all, the point is not that their bodies and their brains froze solid,
1020480	1026640	but you know, this is not an example sort of natural accidental. Oh, and so that was the accidental
1026640	1034160	case. Then there's an OR procedure called hypothermic preservation. I can't remember exactly what the
1034160	1039440	term is. It's usually used for cardiac surgeries. It's not used for neural surgery. It's used for
1039440	1046960	cardiac surgery. You take the patient's brain down in temperature in order to buy more time to
1046960	1052000	perform the cardiac surgery. Okay, while you're working on the heart, you're not pumping blood
1052000	1057040	through the rest of the body, including the brain. You take the brain down in temperature on purpose
1057040	1062960	so that the brain will survive longer, hopefully long enough to finish a heart surgery. Okay,
1062960	1069600	so those are the two cases. So then coming back to my point, sometimes people sort of
1069600	1073920	overinterpret this. They're like, oh, this is an example of cryonics. We know cryonics works.
1073920	1080320	That's actually not true. In none of these realized medical cases, did the person remain
1081840	1085840	in a cold environment long enough to literally freeze? That's not what these medical cases
1085840	1093840	involve. But they do involve a drop in temperature to, okay, the temperatures are in those papers
1093840	1098560	that I wrote. So go to them for the exact numbers I don't remember. But somewhere on the order of
1099440	1105520	15 degrees Fahrenheit, I think, so they're well above freezing. But the point is that
1107600	1114720	there is a temperature below which the brain does not operate anymore. It does not fire
1114720	1123200	action potentials between neurons anymore. Neurons do not operate just because they're above freezing
1123200	1128080	or something. There's a critical temperature at which they are able to operationalize
1128080	1135360	being a neuron, primarily operationalizing action potentials, which is the fundamental basis of
1135360	1143520	all sort of neurology theory. The propagation of action potential is how we expect the brain works.
1145120	1153120	And that temperature is higher than some of these medical cases. So just I'm speaking
1153120	1157440	in sort of long interwoven sentences here. What it comes down to is there are medical cases where
1157440	1167280	a patient has had a brain not fire any action potentials for many, many hours, which coming back
1167280	1171920	to the original point means that their stream of consciousness was turned off like a water tap.
1171920	1178160	You cannot have a stream of consciousness if you don't have a brain sending signals through
1178160	1182080	itself from one neuron to the next. There's no stream of consciousness if the brain is not
1182080	1188000	processing. So these are patients. Stream of consciousness ceased. And when they were revived,
1188640	1194400	nobody called them a copy or a doppelganger or a body snatcher or whatever these ridiculous like
1195120	1200320	everyone just treated them as a revived recovered medical patient.
1203120	1206960	And that's it. That's the disproof of the entire stream of consciousness
1206960	1212000	counter argument against preservation of identity via mind uploading.
1213120	1218240	On the basis of it sort of involving some cessation of processing during the uploading
1218240	1225440	procedure, it's just a that's the whole thing. I think the paper is going a little more detail,
1225440	1230320	but really, I don't think there's a whole lot more there. So that's you sort of brought that up.
1230320	1233280	And I just thought I'd walk through that. That's what that was all about.
1235040	1238720	Is whole brain emulation needed for blind uploading?
1239280	1243600	Right. I think whole brain emulation,
1247360	1250000	short answer, yes, I mean, I don't really see how you do it without it.
1250000	1255920	There are some interpretations of mind uploading that sort of lean away from that. There's this
1255920	1264160	concept of training a, what did they used to be called classically? They were called expert
1264160	1268720	systems. This is what they're called in the 80s and 90s. These expert systems, which have basically
1268720	1274560	now become bots and the recent LLMs and the AIs, if you train one of these things on the corpus
1274560	1280080	of a person's available lifetime of media, some people provide more media than others,
1281360	1289440	but everything they wrote, all their emails, any videos, home videos, everything you can find
1289440	1296000	that person ever produced, if you train one of these neural net AI systems that are sort of
1297680	1304240	all above these days against that, you can produce a bot that basically in a dialogue acts
1305360	1310400	a lot like that person. So the theory, and there have been companies that have made an
1310400	1315360	entire business model around sort of ostensibly preserving you in this way, or at least preserving
1315360	1325920	like this sort of post-mortem puppet for other people to interact with. So the idea is you produce
1325920	1331040	the system, and in those cases where there's a rich enough set of data, you can produce a bot
1331040	1337520	that sort of passes for the person, and with that count is mind uploading. So that's a version of
1337520	1342320	mind uploading that doesn't involve whole grain emulation, but there aren't too many examples
1342320	1348640	like that. In most cases, mind uploading is a whole grain emulation concept, and the way I
1348640	1356160	describe it is that whole grain emulation is just a technical term. It's the process of
1356720	1362720	modeling and emulating the neural processing of a brain. Doing that is whole grain emulation.
1362720	1367920	Mind uploading is not actually a technical term. Mind uploading the way I see it is
1367920	1374240	it is a philosophical interpretation of the consequences of whole grain emulation. If you
1374240	1379600	choose to interpret whole grain emulation as a preservation of a person, a preservation of a
1379600	1385120	person's identity, then the label for that is mind uploading. Mind uploading is the
1385120	1390000	interpretation of whole grain emulation as a preservation of identity. You have uploaded
1390000	1395760	the mind. I don't actually hear too many people phrase it that way, but that's that's how I've
1395760	1401040	always thought of mind uploading as being this interpretation of the technical process of whole
1401040	1411760	grain emulation. So I mean, I guess the question comes to me is, do you think you'd need to upload
1411760	1421120	all the exact brain of a particular identity in order to reproduce that identity in the machine,
1421120	1426720	or are there some parts of the brain that are rather generic that you'd be able to scaffold on
1426720	1437200	more templated emulations of like a generic brain, certain salient features of the brain,
1437200	1442640	which are more important to preserve personal identity and during metaphysical in go?
1443200	1448400	Yeah, I mean, there's one really big one, which is that something like 80% of our brain
1448400	1452640	by neuron count is the cerebellum. And you could probably just throw the whole thing away.
1453200	1458080	You might, you know, not bring your expert piano playing skills along with you.
1459360	1463760	And you know, you might because the muscle memory is often encoded in the cerebellum.
1464720	1468480	And you know, whether or not you could borrow someone else's or well, or you could relearn it
1468480	1472800	again, right? But if we're trying to sort of simplify the process of the emulation, because
1472800	1476880	it turns out to be technically really hard, you're trying to find ways to cut corners.
1478880	1485280	You know, it's it's sort of debatable whether a professional musician or it could be any
1485280	1489600	professional athlete, because some sort of sort of muscle memory task,
1489600	1492880	would they consider that to be a critical component of their identity? If they lost that,
1492880	1500880	would they not be the same person? You know, that's a very sort of emotional question to ask.
1500880	1507840	But nevertheless, it really doesn't impact the cortical, I mean, by which I mean cortex,
1508880	1515280	the cortex encodings that we generally associate with all the rest of personality traits.
1515280	1521600	All of that is in the cortex, not the cerebellum, you know, not the alamus, maybe the hyper alamus,
1521600	1525600	because that's doing some short term memory work, you know, not the amygdala. There's all sorts of
1525600	1530960	stuff in there that doesn't seem to be particularly pertinent to what we consider to be human identity.
1532160	1536240	Which isn't too surprising, because if you look at it from an evolutionary perspective,
1536880	1545360	the human cortex and neocortex is just this 500,000 year veneer, you know, sort of thinly
1545360	1551040	layered on top of 4 billion years of stuff that we don't care about. We don't care about the
1551040	1555920	aspects of our identity back when we were amphibians. So why would we consider that important?
1555920	1562960	What we really are interested in is just the last 500,000 years of neurological evolution.
1562960	1569280	That's the stuff we really need to successfully carry over to preserve human identity. Now,
1569280	1574560	I'm not saying we have to just sort of cut, you know, cut it off that early. You know, it might
1574560	1581120	be nice to try to get a richer, more complete emulation. I'm not against that. But the question is,
1582000	1587920	is it possible to achieve, you know, a pretty good passing whole brain emulation and
1587920	1594400	mind upload, you know, at some fraction of the total brain? And I think we could. I think we could
1594400	1602640	make a good enough version one with way less than the entire brain. And over time, we'll probably
1602640	1606240	be able to do the rest of it anyway. So it's probably going to be a question 10 years later.
1606240	1608560	But in theory, I don't think we need a lot of it.
1609440	1618960	Okay, so therefore, if we don't need to emulate the whole brain, then are we approaching the
1618960	1624000	compute required to be able to achieve this if we knew how? Do you think?
1625440	1630160	So there are very wide estimates. You know, there are different, there are different sort of
1630800	1637600	dimensions, you know, processing power, memory capacity. I think at a recent video or presentation
1638880	1642480	sort of the speed of communications in between the units was considered.
1645360	1651600	We, I think the expectation is that we have almost gotten there now with some of our supercomputers,
1652240	1659760	at least at the lower estimates of human processing capability. But this sort of begs the question
1659760	1666720	of whether mere number of computes per second is actually the trick. Or, you know, I'm a big
1666720	1672000	believer in neuromorphic computing in which you actually create a massively parallel system
1672560	1679760	that is connected to itself and wired against itself in ways that closely mirror the architecture
1679760	1685440	of the brain. And you know, one might ask, well, who cares, you know, and this comes back to a
1685440	1692000	point made earlier, which is that I strongly suspect there is something about the recurrent
1692080	1699440	self-referential, you know, self-looping signal processing structure of the brain
1700000	1705920	that is important. I don't know why this is an open question. We don't understand consciousness
1705920	1712720	well enough to understand why certain network architectures seem to work and others don't.
1712720	1717520	So the wiring diagrams in the cerebellum have essentially nothing to do with your consciousness.
1717520	1725840	If you remove a person's cerebellum in surgery, their consciousness, their sense of who they are
1725840	1731840	is all left completely intact. Again, there can be sort of some damage to their muscle memory,
1731840	1738880	but their sense of who they are is not harmed. And their conscious experience is not affected.
1738880	1745440	So the way that the cerebellum is wired is not the right way. And the way the cortex is wired
1745520	1750960	is the right way. What is that way? We don't know. It's the 21st century. What a time to be alive.
1750960	1759440	I don't know. But I think that I think that we need to get, I think we need to solve that.
1759440	1764000	I think we need to figure that out. We need to figure out what it is about the wiring of the
1764000	1770640	cortex that is creating this amazing experience. And that's what we need to make sure that we
1770640	1777840	recapture in an emulation. Do people know what parts of the brain were destroyed
1777840	1784880	when Phineas Gage got a big rod through his brain? They definitely do. I don't recall if I've
1784880	1791200	talked my head. So it went sort of right behind his eye, which means that it hit a lot of his
1791200	1799600	prefrontal cortex, which is, you know, relative to a prandola pram. It hit parts of the brain
1799600	1805280	that are salient to sort of emotion control or something, which is why after the accident,
1805280	1811200	he became very sort of fly off the handle, kind of sort of an angry kind of person. He just sort
1811200	1816720	of didn't regulate his emotions probably. So something about that sort of, and it wasn't like
1816720	1822640	an amygdala, you know, sort of emotional response. I understand, I really have to go read it. I'm
1822640	1828960	sort of shooting from the hip here. But my understanding is that our, you know, very advanced
1828960	1834800	intellectual cortex level ability to regulate our emotions to basically just think through
1834800	1840240	a situation instead of flying off the handle. That's all, you know, that's not the kind of
1840240	1846960	carefully reasoned stuff that, say, a mouse does, you know, a mouse more or less does actually kind
1846960	1854320	of respond emotionally to what happens around it. Humans can feel the emotions, but also think
1854320	1857920	through it and say, okay, I'm not going to act directly in accordance with my emotions. I'm
1857920	1862560	going to act in this intellectually rationalized way instead, because I have a cortex that enables
1862560	1869280	me to do that. And that's precisely what got blasted out of Cage's head. But no, I don't remember
1869280	1873840	exactly which, if you look it up, it'll tell you exactly which parts of his brain were damaged.
1874080	1881120	Do you think quantum computing is required for whole brain emulation or mind uploading?
1881760	1884240	Or if it's not, could it be desirable?
1884800	1889520	Yeah. So of course, Penrose and Haneroff made this famous.
1892560	1893520	My God, you be all right.
1893520	1899600	The most honest answer is that I don't know. Of course, people tend to have hunches and I have a
1899600	1908000	hunch. What's your hunch? I would be surprised if the quantum mechanical properties of neurons
1908800	1915440	had anything to do with sort of animalian evolved behavior. Just why would evolution
1916320	1922560	need to go there? It seems like all it really needs to do is get the action potential sputtering
1922560	1929520	around well enough that the animal can satisfy the four Fs of survival. Why do you need quantum
1929520	1938720	mechanics to do that? I don't see the need. I don't get it. I just never found the quantum
1938720	1945760	mechanical argument for consciousness convincing. That's my take. But I'm also fairly open-minded
1945760	1949440	about it. There's a lot we don't understand about quantum mechanics. There's a lot we don't
1949440	1957120	understand about consciousness. Some people see that as an excuse to pin them together, implicitly.
1957120	1960880	People like Deepak Chopra are like, well, we don't understand consciousness and we don't understand
1960880	1966160	quantum mechanics. That means they're connected. So that's garbage reasoning. Two things aren't
1966160	1972480	related just because they're both mysterious. But at the same time, they are both mysterious.
1972480	1978480	We don't actually know. So I'm open to someday discovering what there's a connection there.
1979440	1985760	But I don't see a need for it now. Forgive the cat. The cat is forgiven.
1989680	1996320	Once we can upload brains onto a computer, there's going to be an economic issue too
1996320	2003280	about how to afford to do all this and how to do it in a cost-effective way. Such is a good
2003280	2009920	ratio between the amount of minds that can be uploaded and the amount of power which is consumed.
2011840	2017360	The trade-offs will become more friendly over time as technology gets better. But what's the
2017360	2025680	role of the type of resolution that we might want to capture and the ability to do compression
2025760	2030880	to reduce the amount of space a mind might occupy?
2032320	2040400	I do have some numerical answers to that question. But more to the point, everyone's sort of used to
2040400	2048400	these claims about the rate of advance of technology and it's true. So we can just barely scrape
2048400	2053840	together a supercomputer that might emulate the human brain today. It's millions and millions
2054080	2058640	of dollars and it will only run one brain. And it's not wired correctly to do that anyway. So we
2058640	2062400	couldn't do it even if we wanted to. But in theory, the architecture is there. So we're just,
2062400	2067280	you know, that's where we are now. So we're down the road. We'll be able to do it
2067280	2072480	expensively but successfully. And then 10 years after that, we'll be able to do it cheaply. You
2072480	2078560	know, you know, the human brain uses as much electricity as a light bulb. So we're going to
2078560	2082800	get it, you know, at some point, the power requirements won't be relevant. You know, it's not
2082800	2088240	like you're going to need a supercomputer or even anything as inefficient as a desktop computer.
2088240	2092480	You'll need something as efficient as a brain. And you know, how cheap are brains? There are
2092480	2097760	brains everywhere. Like brains don't take much energy. The world's full of brains. So once we
2097760	2105440	are able to create those kinds of computers at that level of energy efficiency, the efficiency
2105440	2109520	equation, and therefore the cost equation, it's all just going to go out the window. You know,
2109520	2115440	so long as your brain, so long as your emulated brain can get, you know, as much energy as,
2115440	2120800	you know, a salad, then it'll be able to stay alive the same way we can. Right? So it's just,
2120800	2126320	that whole problem is going to go away. I hate to say that it's a question of patience, but it is.
2127440	2131120	People really chomp at the bit about futurism stuff sometimes. They're really like,
2131120	2134640	gosh, you know, this stuff really needs to happen in the 2040s or it's no good at all.
2135280	2138880	I mean, I'm sorry, but some stuff's just not going to be available in the 2040s,
2138880	2143040	but that doesn't mean that it's not going to happen, you know, early in the next century.
2145120	2149040	Or later in the next century. I don't know when, but I'm not particularly concerned about that,
2149040	2154640	because when it does happen, I just think it's going to be practically preordained. I don't
2154640	2160400	think it can be stopped. I also don't think it can really be expedited too much. I mean,
2160400	2168560	mostly we want to create a society and a culture that sort of broadly favors science and knowledge
2168560	2175440	and openness, you know, and sort of inquisitiveness and curiosity. If we just sort of create a society
2175440	2182400	that creates and supports these general ideas, the science will just happen. Now,
2183520	2187360	I can get into some minor specifics. You were talking about sort of resolution and compression
2187360	2195920	and things about that, things like that. So where we currently stand on sort of producing
2195920	2203440	whole brain emulations is, well, in short, we can't, but there are sort of multiple steps in
2203440	2210560	the process. So the most likely way to produce a whole brain emulation is to preserve the brain.
2210560	2217920	That means essentially embedded in a resin, and then you deli slice it, and then you take pictures
2217920	2223280	of each slice. You can't use, like, a camera, because the features, the neurological features
2223280	2227520	you're trying to capture are smaller than the wavelengths of visible light. You can't take
2227520	2231840	a picture of something that's smaller than a wavelength of light. So we use electron microscopy
2231840	2240320	to do this. And the resolution that we're currently able to achieve is a z-axis resolution. That means
2240320	2249360	slice to slice. We can physically diamond knife slice brains at about a 40 nanometer thick resolution.
2250320	2255440	And then when we take an electron microscope image of a slice, we can achieve about a four
2255440	2262400	nanometer xy resolution. So each image has 10 times as much resolution, 10 times the fineness of
2262400	2268560	resolution as the slice to slice steps. So in the three dimensions, two of the dimensions are finer
2268560	2274000	by a factor of 10 than the third. And the question is, is that good enough? And the answer is,
2275440	2282640	according to current models and theories of how the brain works, you know, the major paradigm
2282640	2291840	of action potentials propagating along axons, triggering synapses, and then propagating secondary
2291840	2300720	action potentials and downstream neurons, we can image synapses at that resolution. And of course,
2300720	2305680	we can image all the rest of the neural components, which are larger than the synapses. So we can
2305680	2311120	actually capture the necessary resolution to produce a whole brain emulation. We can do it today.
2311760	2321440	What we can't capture is the scale. So the data sets that we are scanning these days are, you know,
2321520	2326880	what sort of cubic section of a brain can we kind of scoop up and actually perform this on?
2327600	2336320	And it's on the order of sort of fractions of a cubic millimeter. One full millimeter is just,
2336320	2345200	you know, sort of coming around the bend as sort of a 2020s project. So, you know, human brain is a
2345200	2352560	little bit bigger than that. So we've got a ways to go. I think a whole mouse brain of approximately
2352560	2358880	a cubic centimeter, which is a thousand times bigger than a millimeter, is one of the projects
2358880	2366080	that is being sort of put together now to be achieved over the next many years. And when that
2366080	2371040	is done, all we will have is a brain of a mouse. All we will have is one centimeter. We're going
2371040	2379200	to need to go another factor of 10 in all three directions to get to the human. So
2381520	2388160	it's going to take a while. But the interesting thing is that while it's going to take a while
2388160	2394240	on the calendar to get there, there's a very little that actually appears to stand in the way.
2394960	2402480	We just need to keep turning the crank for a few decades. And there's no real obvious like place
2402480	2408160	where we think it's just technically like we're up against a wall. When we look at it today,
2408160	2412880	there are things we don't know how to do yet. Getting the perfusion of the chemicals deep
2412880	2419280	into a large brain evenly is very hard to do. We don't really know exactly how to do it. How do
2419280	2424960	you, you know, we can slice up a slice of a brain on the scale of microns, but can we create, you
2424960	2433520	know, a cross section of a brain that's only 40 nanometers thick, but is five inches across, right?
2433520	2440000	Like how do you even handle that, right? So there are major technical questions, but these don't
2440000	2444800	feel like things are going to be like some fundamental wall in physics that we cannot get
2444800	2449520	past. It seems like we just need to keep going and we will get there.
2451760	2458080	Do you think that there's a particular, okay, so you mentioned plastination, at least that's
2458080	2464000	what I think you're referring to. Do you think that's the state of art and cryopreservation today?
2465360	2470880	Or brain preservation today? Or would you suggest that it's something else? Or
2471840	2476800	yeah. And what about in the future? Do you think where the state of the art
2476800	2480640	plastination techniques that we have today are the best that they'll ever be?
2481600	2486400	So correct me if I'm wrong. I think plastination refers to room temperature preservation techniques.
2487440	2494800	Correct. Yeah, I don't really know where they stand. So you've got two things. You've got the cryopreservation
2495520	2501200	and then there is the preparation of a brain sample for slicing, which is not cryo.
2502400	2507520	It's just room temperature and it's a resin. I don't really know what the chemical structure is,
2507520	2516000	but it's essentially you first you flush out a lot of colorful and opaque things so that you
2516000	2521120	can physically see through the brain. And once you flush all that stuff out, you embed it in
2521120	2527280	some kind of resin. I'm not a chemist. I don't know. And now you've got like this solid block with a
2527280	2533760	transparent brain inside it. Then you slice that and you take pictures of it. Now preservation,
2534560	2538240	you know, especially sort of preservation for the purpose of longevity, sort of cryonics or
2539280	2544800	the next generation of cryonic like preservation, which would be these sort of the aldehyde
2544800	2550240	stabilized cryopreservation that the Brain Preservation Foundation has awarded,
2551760	2557920	which is probably, you know, definitely the next generation in preservation if we can get it
2557920	2567440	sort of normalized, popularized. That's for a different task. That's for the task of preserving
2567440	2577680	people, keeping them alive for one of two reasons. Many proponents of sort of classic cryonics want
2577680	2583440	to be kept alive just to sort of get to that point in the future where the technology can revive them
2583440	2590880	biologically again. The other reason to go into preservation of some form though is to just get
2590880	2595520	to that point in the future, not where you can be biologically revived, but where mind uploading
2595520	2600240	technology has gotten to the point where it can upload your preserved brain and you would not come
2600240	2605520	back biologically, you come back as an upload. Now as you know, I don't want to completely distract,
2605520	2608560	we can get into like, you know, why would you want one? Why would you want the other? What's the
2608560	2612080	benefit? You know, why would you choose one if the other's available? There's a whole conversation
2612080	2618240	there, but I just I think it would sort of take us off into the weeds. But there is never there's
2618240	2623520	this issue of preservation for longevity and there is this issue of preserving and preparing a brain
2623520	2629840	sample for very contemporary and real, you know, sort of slicing and scanning that we do today
2630480	2639920	in labs today all over the world. What about inspirations from biology? And it's been mentioned
2639920	2647360	before about the visual cortex, how that's very much like the way a convolutional neural network
2647360	2654880	might work. There's hierarchies there. They've got logical receptive fields and shared weights.
2656000	2664480	Do you think this is important to sort of mimic the way that biology works in the
2664480	2673120	way that we approach developing a whole brain emulation? Or can it be a completely wildly,
2673120	2681280	even wildly different computing power time? Yeah. So yeah, modern convolutional neural
2681280	2687760	networks were intentionally inspired by the visual cortex. It's actually not the brains
2687760	2693840	resemble convolutional nets. Convolutional nets were sort of built on the previous four decades of
2694480	2699040	research into how vision works. We actually learned a whole lot about how vision works. And we said,
2699040	2703200	wait a minute, what if we code that up to a computer and see if we can see? And lo and behold,
2703200	2709440	it's actually worked incredibly well. So one thing that that story tells is that
2710080	2716640	biomimicry is a very good strategy. If you figure out how nature does something and then emulate
2716640	2721760	that with engineering, that's very likely to actually get you pretty far down the road or
2721760	2732000	whatever task you're trying to solve. For whole brain emulation, there's an interesting opportunity
2732000	2738320	to cut some really major corners at the possible expense of like a major philosophical or metaphysical
2738320	2749120	failure. So what do I mean by that? At the lowest level, we could create a whole brain emulation
2749200	2756960	of neurons and synapses, individually firing action potentials down their axons across the
2756960	2764480	synapses into the dendrites through the full complex web of neurons. And we would expect
2764480	2769360	that to certainly be functionally successful. You know, there's a whole big debate about whether
2769360	2773360	it's successful in some other way, but it's, you know, because some people think it has to be a
2773360	2777760	biological system to be conscious or it has to be whatever, but but it would functionally replicate
2777760	2783760	the action potential propagation paradigm. But that's actually pretty computationally like
2783760	2790960	inefficient. Like why, why recreate a thousand neurons that fire in some complex arrangement
2790960	2797360	to perform multiplication? If you can just stick a multiplication module in there that has the same
2797360	2803120	black box behavior of the neurons. If you've got a set of neurons, you've got a set of neurons
2803120	2809120	and you send input signals in and you read the output signals out, you could replace that set
2809120	2815680	of neurons with what we with what we call a black box. Ted Berger has been doing this with
2815680	2823440	hypothalamus. And if you basically produce a prosthesis effectively that takes in neural
2823440	2827680	signals and emits neural signals like the system that's replacing the set of neurons that's
2827680	2835360	replacing, you can use a potentially much simpler computational piece of hardware to process those
2835360	2841200	signals and yet get the same functional propagation, right? So it's an interesting question. How far
2841200	2849040	that could go? How much of the brain could we abstract away to some sort of algorithm? You know,
2849040	2852560	one of the one of the biggest ones would be like cortical columns. The cortical columns are these
2852560	2857200	sets of about a hundred neurons that seem to sort of process kind of as a singular unit, you know,
2857200	2861440	really know exactly what they're doing or how they're doing it, but there's this repeated
2862800	2869520	cortical column pattern. If we could solve the cortical column algorithm, we might just black
2869520	2876080	box the whole thing away. What if you just make a single computer chip that emulates, you know,
2876080	2881280	the hundreds of neurons and the thousands upon thousands of synapses in a single cortical column
2881280	2885600	just turn the whole thing into some little computer that may not work the same way at all,
2885600	2890640	but it will take the same neural signals and it will critically, it will emit the same signals,
2890640	2896080	so it has perfectly replaced that behavior, but in a completely different way, right?
2896800	2900560	And then the question is, well, what if you did that with every cortical column in the entire brain?
2901200	2908080	So a whole lot of the neural structure that we consider to be extremely human in its architecture
2908720	2915040	would be completely replaced with some sort of computational abstraction. The same signals
2915040	2922160	would be going through, but would it actually metaphysically be the same sort of mind?
2923120	2932240	And we don't know, we don't know where that cutoff is, you know, where the sort of level of
2932240	2941120	fidelity of your emulation loses some sort of metaphysical thing that is deemed important.
2941200	2947920	We don't know where that threshold is. I suspect that we might find it, you know, because,
2948880	2954720	you know, as you produce these things, as you produce these prostheses, these modules,
2954720	2960000	and emulate them, if you get some sort of surprising behavior out of it, then you're
2960000	2966480	probably abstracting too far. I'm not sure how that would happen, because if it's producing,
2966480	2971920	if it's reproducing the same functions, there isn't an opportunity for it to sort of behave
2971920	2976400	differently, like that's the whole point of the functional paradigm. So I'm not really sure where
2976400	2980800	it could break down, but if it was going to break down, presumably we would discover it. I mean,
2980800	2984080	otherwise you're just back into philosophical zombie territory, you'd be able to replicate
2984080	2989600	all the functionality, but you wouldn't know, but then I just find that all very hard to buy.
2989840	2997760	Yeah. What do you think, look, I mean, there's the issue of like the fire in the equations when
2997760	3003120	like Hawking spoke about, like, what is understanding of a mathematical formula?
3004000	3008640	What is the, you know, what's the important, what's the important difference between the way
3008640	3014080	or computers at the time when you wrote this subject would be able to calculate and the way
3014080	3019440	a human would be able to calculate? And he spoke about fire in the equations. I think it was Hawking
3019440	3024960	anyway. Maybe it's got something to do with like something like that and embodied understanding,
3025600	3030880	maybe a feeling. I don't know a valence of it. It's going to be one of the big questions of
3030880	3039280	neuroscience in coming decades. Sure. Well, okay. Another big question that I thought to bring up
3039280	3047360	was why is whole, a form of whole brain emulation of your preserved brain, probably you, and you've
3047360	3051040	written directly about this, but I thought I might bring this up during the interview.
3051040	3057520	Why do you think whole brain, a whole brain emulation of Keith Wiley would be Keith Wiley,
3057520	3064080	even if the original was still alive? Well, that's that last sentence really. Okay. Where
3064080	3070480	that's where people start the big debates, right? Well, let's step back. Let's, let's exclude that
3070480	3077600	last, that last sentence then, because we will touch on identity branching. Yeah. Better. I mean,
3077600	3083040	that has been the bulk of my writing is defending the branching interpretation. But yeah, why would
3083040	3091040	it be me? So there are different theories of metaphysical personality. For example, you have
3091040	3095920	the body theory, which basically says that your identity is attached to the matter that you're
3095920	3101520	built out of. You have a stream of consciousness theory, which is that it's some sort of continual
3101520	3105520	stream of consciousness. You have a space-time worm identity, which is that this thing sort of
3105520	3111040	weaves its way through four-dimensional space-time, because it's sort of continuously located in
3111040	3116960	sort of a smooth path. The identity is sort of following the sort of worm through time,
3117840	3122400	but it doesn't discontinuously jump around. And then you have psychological identity. There are
3122400	3128480	there others as close as continual identity. Close as continual identity says that the thing
3128480	3135920	that is you now is that which most closely resembles the thing that was you, you know, very
3135920	3140880	recently. And you have to get into the thought experiments to sort of feel sort of where that
3140880	3147360	question comes up. But another theory is psychological identity, which is a catchall term
3147360	3152640	for, by psychological, what we mean is the memories. Your identity is indicated by
3153360	3158640	your psychological traits, your personality traits, your memories, your all the sort of cognitive,
3159600	3166960	emotional, intellectual sort of senses of your, that's your identity. Every single one of these
3167680	3172960	theories can be subjected to sort of colorful thought experiments that try to sort of break them
3172960	3181600	apart and undo them and find some sort of paradox that leaves them. And the interesting thing about
3181600	3191440	the psychological theory is that it only breaks down if you declare at the outset that you're
3191440	3198480	going to reject branching interpretations. So you basically have to say, well, branching is just
3198480	3202160	going to cause a lot of trouble for my thought experiments. So I'm just going to say it's excluded
3202160	3207040	and we're not allowed to consider it. And if you say that, then you can come up with a thought
3207040	3212160	experiment where psychological identity sort of runs into problems for the same way all the other
3212160	3217520	identity problems models can run into problems. But if you don't cut yourself off with the knees at
3217520	3221440	the beginning, if you say, well, wait a minute, branching identities on the table, let's see
3221440	3229760	where that takes us. Then psychological identity is the most robust theory that I have yet found.
3232480	3238560	Well, okay, so you're going to be talking a lot about this in your up and coming novel,
3238560	3245280	contemplating oblivion. So anybody who's interested in these topics would like to explore worlds in
3245280	3253200	which they are covered, yeah, watch this space or watch out for contemplating oblivion.
3254240	3259680	So but yeah, would you want to just give us a breakdown of some of the writings you've
3260400	3269520	covered identity branching in? It's your favorite topic. This is what you've covered the most.
3270160	3278880	Yeah. So it all started actually started slightly before my 2014 book. I was inspired to
3278880	3284400	write an article in which I sort of thought I would lay out my basic sort of theory of branching
3284400	3289680	identity. In early 2014, I'm sort of going through my own list of writing here.
3290960	3295520	Was this the H plus magazine? Let me think here. It was
3299200	3304320	Oh, in March of 2014, it was actually a response to a article by Susan Schneider.
3305760	3311120	I remember. Yes. And I wrote an article for H plus. Yes. I think you might have emailed me about
3311120	3316800	that and got me to put that in there when I was on the board. Yeah. Yeah. And I'm looking at the
3316800	3320560	so if that was March 2014, that means that I basically wrote that article. And then
3321280	3328160	I must have come off of that article with ideas for a book just boiling because I started it
3328160	3337920	immediately. I worked over I worked on the book that summer of 2014. So the larger work I produced
3337920	3341840	is my book attacks on me and metaphysics of mind uploading.
3344480	3351440	And then from there, I just continued to write. So I've I produced a paper in the Journal of
3351440	3358400	Consciousness Studies with Randall Conan, which tackles one very specific question, which is,
3360000	3364480	you know, we taught I've been talking about the slicing and scanning approach
3365200	3372240	to whole green emulation. Another thought experiment or scenario that comes up in a lot
3372240	3379760	of philosophical musing is what we call gradual replacement, where you perfuse the brain with
3379760	3388000	billions of nanobot prosthetic neurons, some sort, and they they learn the input output function
3388000	3393760	of the neurons. And then once they've got that function figured out, they kill the neuron and
3393760	3398160	like attached to all of its synapses and slowly over time, the whole brain becomes mechanical.
3399520	3407520	I think it's all complete and total nonsense. It's it's a fun thought experiment. It makes
3407520	3412640	maybe potentially good science fiction. I don't I don't know. But it's not practical.
3414400	3421280	It's sort of it's very sobering to read sort of online debates about the stuff in which people
3422240	3428480	you know, with great emotional commitment, you know, say that they really require like a gradual
3428480	3434080	replacement process in order to be uploaded. Well, I got news for you. It's not going to happen.
3434720	3442320	That is so technically challenging that I don't know if we'll have it in a thousand years. It is
3443040	3452000	beyond incomprehensibly challenging, whereas scanning slices of preserved brains is a 20th
3452000	3457760	century tech. We're doing it now. So we're talking about the trade-offs between a technology that exists
3457760	3463200	now and a technology that I'm not sure if it will exist in a thousand years. That is the that is the
3463200	3471920	chasm between these two approaches. So Randall and I wrote a paper that argues not only, you know,
3471920	3478080	is gradual placement just, you know, technically just bonkers. But from from a philosophical
3478080	3482320	standpoint, we don't have to worry about it. It doesn't matter. It actually isn't a critical
3482320	3487600	need anyway. The whole point of the paper is to argue that metaphysically and interpretively,
3488160	3494320	both of these two procedures have the same outcome from sort of the perspective of personal identity.
3494320	3498240	So we don't have to worry about the impossible gradual placement process, the process, the
3498240	3504000	scanning of a preserved brain will work just fine. So that paper was in the I was in JOCS.
3505440	3509920	I've written several others. I wrote sort of an all-encompassing paper that was sort of a broad
3509920	3518320	sort of life philosophy paper with the tongue-in-cheek title of mind uploading in the question of life,
3518320	3526080	the universe and everything, obviously an homage to Douglas Adams. And the the thrust of that paper
3526080	3533840	is sort of a step-by-step almost theorem like layout of an argument that argues that
3535360	3547520	developing mind uploading is the most important priority ever. It is the most important thing
3547520	3552000	for us to work on. And the reason why it is the most important thing is the entire sort of line of
3552000	3556080	reasoning laid out in the paper. One might immediately sort of knee-jerk response that
3556080	3560800	well that's just ridiculous. How can I possibly say that? Well, read the paper. The whole point
3560800	3567360	of the paper is to explain why I think that's so critically important. And then I've written two
3567360	3575440	papers that get into this argument that a breakage in the stream of consciousness implies a death
3576080	3584320	and then some sort of invoked doppelganger in the output. I found that such as such an important
3584320	3589680	topic I actually ended up writing two papers on it. I don't know, I probably have others too, but
3591280	3593360	yeah, I sort of keep keep writing about it.
3596480	3602400	Well, I mean, do you think, how do you think people will react? I mean, when push comes to shove,
3602480	3608880	do people really care if there's a doppelganger or if what they feel is a doppelganger out there?
3608880	3614960	Do they mind if there's two identities that are metaphysically exactly the same, at least on
3614960	3621040	invocation? I think a lot of people are uncomfortable with this idea. I don't know if the concern is
3621040	3628080	whether there's either one or two of them. The concern is that by being some sort of a copy,
3628080	3632480	it represents that they actually died. They didn't really survive the procedure.
3632480	3637120	They died. The entire goal of surviving through the procedure into the upload failed,
3637840	3640640	and the person living on as the upload is just some other person.
3642000	3647840	And therefore, the entire goal of using this procedure as a method of survival has failed.
3647840	3655600	So that's the concern. That's why people sort of get very knotted up about whether or not it's a copy.
3658800	3665040	Okay. Well, was there any interesting points that you wanted to... There was a recent meeting
3665040	3670640	you had, like, I guess, you know, is there anything that you discussed at this meeting,
3670640	3678000	at the Hold By An Emulation meeting with Anders Sandberg, Randall Kruhner, and yourself and others?
3678000	3684800	Robin Hansen was there. Yeah. Is there any points that you brought up that you thought you might be
3684800	3691440	able to reveal? Well, okay, so the topic of that meeting for the audience, the topic was
3691440	3700640	the ethics surrounding Hold By An Emulation. So it got into several different questions. It got into
3701920	3705760	I'm just sort of reading out of the notes here, the future of work. You know, what will it mean,
3705760	3712400	what will work mean when to do work consists of spinning off a Hold By An Emulation and having
3712400	3718080	it do the work and then shutting down the emulation when the work is done? What does that say about,
3718640	3722400	you know, people who are always concerned about technology taking jobs? So if we're just spinning
3722400	3727200	off WBEs, you know, Hold By An Emulations, then how are we ever going to get a paycheck ever again?
3727200	3733360	But then if Hold By An Emulations are, you know, really sort of verging on conscious, you know,
3733360	3738960	human minds, there's a whole other question about work, which is, are we making, you know, an entire
3739040	3746000	society of slaves, right? So there's a whole, instead of that came up. Then we talked about
3746000	3750160	just sort of good old fashioned existential risk. It's a topic that always comes up everywhere.
3750880	3756880	I don't actually remember exactly where the conversation went. I don't tend to have like
3756880	3761600	a crystal clear like immediate short term memory. I'm the kind of person you have to look at the
3761600	3766080	notes afterwards to remember and I don't have the notes from the front of me. But I can go
3766080	3770560	through the topics. We talked about, oh, an interesting one was, you know, once Hold By
3770560	3775520	An Emulation is sort of common, what are the implications in terms of neuro security and
3775520	3780560	mental privacy? So this is a concern not for us, but a concern for the well-being of Hold
3780560	3787280	By An Emulations. What rights should they have to the security of their brains and minds against
3787280	3793280	hacking? And what rights do they have to privacy? You know, once, you know, a Hold By An Emulation
3793360	3800080	can probably be interrogated more easily. So it'll basically be more difficult for it to lie,
3800080	3806640	you know, because does it have the right to lie? We talked about access and inequality, democratization
3806640	3811520	of the technology. This is sort of a, you know, a very common concern that the rich elites will
3811520	3816160	not only get it first, but we'll get it first and then somehow lock it down so that, you know,
3816160	3819760	instead of just coming along 10 years later at the scale of sort of technological advancements,
3819760	3827840	somehow everyone else will get sort of locked out. We talked about sort of the ethics of using
3827840	3833600	animals for research and development and then what the implications are for using increasingly
3834720	3841600	verisibiltudeness emulations in sort of, in lieu of animals. You know, if there's an ethics question
3841600	3846880	about using an animal in an experiment, is there an ethics question about using a really,
3846880	3851840	really good emulation of an animal? I mean, so good that the emulation might be conscious.
3852720	3856960	You know, what sort of ethical concerns are there about using a system like that experimentally?
3858640	3862560	Once we get into population ethics and emulations, I'm not sure what that refers to.
3864480	3868080	Yeah, I don't know. We just, we talked about a lot of stuff, but it was all sort of from an
3868080	3870720	ethical perspective. That was the point of that panel.
3871360	3879600	Yeah, so I mean, this probably comes up before a lot of people. So as futurists with some sort of
3880560	3887920	idea of how mind uploading might work, we might be concerned about friends or family who have a
3887920	3897600	short time to live. And the difficulty in discussing this with people who have not got an interest
3897600	3904000	in futurology. But have you got any advice on how to speak to people about the possibility of
3904000	3908560	survival beyond the flesh? You know, I sort of find myself having this conversation a lot. I'm
3908560	3912160	just sort of, you know, telling people about my book. And I have to, you know, break through that
3912160	3917840	initial barrier of people just sort of having to sort of an initial shock that my book is about,
3917840	3926560	what? That's ridiculous. So I have that conversation a lot. People are actually
3927280	3932800	generally receptive to the idea of it. I'm sure there's an entire population contingent that just
3932800	3936800	sort of has this very powerful religious objection. And I just, you know, don't tend to cross paths
3936800	3944960	with those sorts of people in my life. But people are just sort of broadly curious about technology,
3944960	3950160	but maybe they don't make it their entire daily life, and sort of think about these things. But
3950160	3955520	most of their exposure is through rather poorly written science fiction, mostly written science
3955520	3961040	fiction is often pretty good. Hollywood is generally terrible. So, you know, if your exposure to
3961040	3968000	these ideas is mostly movies coming out of Hollywood, then you're probably, you know, not getting the
3968000	3975840	richest philosophical presentation of the ideas. So, you know, what do I do? I talk to people about
3975840	3986000	it. I just try to sort of not be overly prescriptive, you know, sort of telling people sort of what I
3986000	3992560	think they ought to believe. It's more a question of whether or not this ought to be available as
3992560	3997920	an option for people who would choose it without putting some sort of prohibition against it.
3997920	4002880	And then people who are uncomfortable with it, I don't have to do it. I mean, nobody's forcing them.
4003440	4009360	But, you know, sort of try to convince people that it's just a typical sort of let's let people
4009360	4017280	make decisions for their own sake kind of issue. If you can get there, then that's usually, you
4017280	4026240	know, a pretty good conversation. I think people are sort of resistant, you know. People are always
4026240	4030080	trying to figure out whether or not the government needs to prohibit something. I don't entirely
4030080	4036640	sure why, but there seems to be something people worry about a lot. So, I think they may just have
4036640	4041760	to evolve through a few generations. You know, with each passing generation, they don't have to
4041760	4048960	actually shed their prior sort of biases from childhood because they're just sort of born into
4048960	4053360	an increasingly technological world. They will be born into a world of increasingly
4055120	4060160	sort of versatile AI systems with which they interact. You know, their teddy bears are going
4060160	4068000	to be increasingly lifelike with each passing generation. And by the time anything, even vaguely
4068000	4073120	resembling an Ulbrian emulation is possible, that generation will have been interacting
4073840	4079920	with computers that are so smart that they will already seem lifelike anyway. And the whole
4079920	4085600	concept is just going to be relatively comfortable to that future generation. We just, I don't know,
4085600	4089440	I would, I don't really worry too much about our generation because it's not going to happen in our
4089440	4094320	time anyway. We just need to keep the research money flowing. You know, so long as there isn't a
4094320	4101680	general sort of hatred of science, then we're doing okay. Do you think choir preservation for
4101680	4110640	yourself is an option to reach for your, for yourself to reach a time in which whole brain
4110640	4118960	emulation is possible? So the Brain Preservation Foundation has multiple sort of goals. One is to
4118960	4123680	sort of advance the actual technology of preservation to sort of actually get the thing
4123680	4129600	available, make it technically possible. But then there's this sort of advocacy component of, you
4129600	4137840	know, you know, how can this procedure become part of sort of gender mill society, just in
4137840	4143200	general, procedure available in hospitals or something like that. I think those gears turn
4143200	4150000	pretty slowly. That's a lot of politics. It's a lot of red tape and sort of a lot of regulation.
4151520	4156880	You know, I just, I tend to be pretty tempered about this stuff. You know, other people just get
4156880	4163760	really excited about it. They think everything's going to happen in 20 years. And I don't know
4163760	4174880	if I'm cynical or just realistic. And I just, I would be absolutely amazed if the technology
4174880	4186560	of human brain preservation was cheap enough and available enough to serve me. But maybe if
4186560	4191360	something happens very rapidly in the next few decades, then maybe, but I don't know.
4192160	4197200	But we're probably only missing it by one or two generations. It's pretty sad to think we might be
4197200	4203440	one of the last generations ever that sort of has to contend with this. But I mean, I don't know,
4203440	4208080	it's more exciting than missing it by a thousand years. I mean, at least we get to sort of understand
4208080	4212400	it, right? You know, a thousand years ago, no one even understood any of this. So that's pretty,
4212400	4218880	that's pretty, that's good enough. It'll have to be. Well, who knows? I mean, like we get AI
4218880	4224400	powered scientists and there's some claim that would that could really fast forward a lot of the
4225120	4230960	science. Maybe there's a chance. I don't know. Yeah. You mentioned that there's a lot of written
4230960	4237520	science fiction, which you are more, I guess, amenable to, which gives better descriptions of
4237520	4244880	how whole brain emulations might work. What science fiction is that? What's been your favorite?
4244880	4251920	So a few years ago, I sort of sent myself on a mind uploading voyage, trying to find, you know,
4251920	4258400	all the good sci-fi on it. Of course, I'm sure I missed, you know, 90% of it. But Greg Egan's
4258400	4266160	permutation city is famous, Arthur C. Clark's city and the stars. Alter carbon, which of course
4266160	4273360	was a Netflix hit. There were others because I had like four, at least five of these books I sort
4273440	4279760	of rattled through. So I'm missing a few. But it's definitely a topic that's been presented and
4279760	4290880	covered pretty well in some cases. All right. Well, yes. Is there any points that you wanted
4290880	4295600	to talk about that you, that I haven't yet asked you that you thought might be worth bringing up?
4296960	4300240	Well, I think you and I will talk about my upcoming book in another part
4301200	4307840	session. So I'll leave that to then. So yeah, it was nice to have an opportunity to sort of
4307840	4312880	talk about these ideas again. It's been a while. Yeah, it has. Thanks. Thanks very much, Keith.
4312880	4317520	It's really good to talk to you again. And yeah, I look forward to our conversation about your
4317520	4332800	up and coming book contemplating oblivion. Thank you very much. Cheers.
