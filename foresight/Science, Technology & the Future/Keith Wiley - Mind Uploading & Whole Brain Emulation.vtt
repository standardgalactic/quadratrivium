WEBVTT

00:00.000 --> 00:08.840
Today everybody, thanks for joining us. We've got Keith Wiley here today who's a board member

00:08.840 --> 00:14.920
of Carbon Copies and has written extensively on the concepts of mind uploading and whole

00:14.920 --> 00:21.120
brain emulation. He's the author of a Taxonomy and Metaphysics of Mind Uploading as well

00:21.120 --> 00:27.480
as the author of a new book which is coming out very soon, which is a novel this time,

00:27.480 --> 00:34.600
our fiction Contemplating Oblivion which should be coming out sometime this month. So welcome

00:34.600 --> 00:40.280
and it's great to have you here again. Thank you, it's great to be back. Now I guess mind

00:40.280 --> 00:45.760
uploading has a concept that's been kicked around for a while now. Why don't you think

00:45.760 --> 00:52.280
that mind uploading has enjoyed as much wide acceptance within the philosophical community,

00:53.200 --> 01:00.840
as it has in the futurist community? Well, I guess I don't know exactly why people are

01:00.840 --> 01:06.200
answering questions the way they do, because they tend to be rather terse interviews that

01:06.200 --> 01:14.240
don't really have a lot of follow-up. But I think that the, you know, the sort of really

01:14.280 --> 01:24.600
contentious questions of the right around this whole body identity concern and the copy problem,

01:24.600 --> 01:31.800
which I actually consider to be a category or are really hard to get away from. And in fact,

01:31.800 --> 01:41.760
I think the recent sort of, you know, popularity of the discussion has probably made people think

01:41.800 --> 01:47.160
about it just enough to convince themselves that it doesn't work, but not to really think past

01:47.160 --> 01:53.600
that point. So there is a certain sort of initial reaction, I think, where you're presented with

01:53.600 --> 02:00.120
the idea of a non-destructive uploading scenario. It could be a transporter-like scenario from

02:00.120 --> 02:05.960
Star Trek or just a more sort of direct mind uploading scenario, but it's presented in a

02:05.960 --> 02:09.960
non-destructive fashion and people think about it for a minute. They sort of initially have this

02:10.000 --> 02:15.560
cut reaction that something about it must not have worked because of the famous copy problem.

02:15.560 --> 02:25.440
And, you know, the bulk of my writing has been an attempt to chip away at that sort of instinctive

02:25.440 --> 02:31.480
reasoning and, you know, really work through it beyond an intuitive sort of feeling and see

02:31.480 --> 02:39.920
where a logical analysis really lands. So there are these various reasons that support

02:40.160 --> 02:46.280
the initial intuition. People often raise concerns about breaking the stream of consciousness or

02:46.280 --> 02:51.160
good old-fashioned body identity in which sort of they feel like their identity has to stick to

02:51.160 --> 02:56.800
their atoms or what I call space-time worm identity, which sort of says that, you know,

02:56.800 --> 03:02.960
it seems a little odd that your identity might discontinuously jump around in space. You have

03:02.960 --> 03:09.600
to sort of go past the sort of the click-off-the-cuff reaction to sort of ask whether that

03:09.680 --> 03:15.840
actually makes any sense. And, you know, I personally have always landed pretty squarely on

03:15.840 --> 03:21.920
the psychological model of identity, which is memory-based. You know, identity is indicated by

03:22.560 --> 03:28.160
some unique conglomeration of a lifetime of memories. And just to be clear, I use the word

03:28.160 --> 03:37.120
memory in a broader sense than what most people might think. I don't just mean episodic memories

03:37.200 --> 03:43.840
of events from your life. For me, the word memory means the totality of neural encodings in the

03:43.840 --> 03:51.360
brain. So there's a lot of subconscious personality traits. There's actually a lot of muscle

03:51.360 --> 03:56.240
memory. You know, sort of the ability to play the piano is a memory, even though it's a completely

03:56.240 --> 04:01.920
subconscious memory stored primarily in the cerebellum, presumably. So by memory, I just mean

04:01.920 --> 04:07.360
neural encodings within the brain. But nevertheless, my preferred model of identity

04:07.360 --> 04:15.520
pins identity to that feature. And that leads one towards certain conclusions. First of all,

04:15.520 --> 04:23.040
that uploading works because it's all memory-based. So if you sort of, if you consider a thought

04:23.040 --> 04:28.800
experiment in which those memories remain intact through the uploading procedure, then by definition

04:28.800 --> 04:33.200
it worked because that was the premise. In order to reject that, you have to sort of

04:33.840 --> 04:37.520
figure out why you reject it. You're obviously going to reject it on the basis of some,

04:38.640 --> 04:43.920
you know, not full commitment to the memory basis of identity. You're looking at some other

04:43.920 --> 04:51.120
component. And I just don't do that. I just always found that the psychological memory theory works best.

04:51.440 --> 04:59.200
Okay. So, I mean, it's interesting that, like, if it's totally based on memory, surely you'd have

04:59.200 --> 05:06.480
to have some sort of mechanism to be able to process consciousness, valence, awareness,

05:08.080 --> 05:16.880
and the memory as well in order to have an agent feel as though it's got some sort of stream of,

05:17.760 --> 05:21.760
or persistence of identity and enduring metaphysical ego.

05:22.560 --> 05:28.640
Yes, yes. There is this, right. So you've got the concept of identity and you have the concept of

05:28.640 --> 05:34.480
consciousness. And they're not necessarily the same thing, which is where the famous thought

05:34.480 --> 05:40.240
experiments of philosophical zombies come from. You could have agents, you know, cognitively

05:40.240 --> 05:47.280
processing in the world and, you know, encoding memories of a form, although it's hard to imagine

05:47.280 --> 05:50.960
what reflecting on a memory consists of, if you have no consciousness, I'm not quite sure what

05:50.960 --> 05:55.760
that comes down to. But nevertheless, you can sort of separate these components and you can ask,

05:56.480 --> 06:04.160
what would it be like to be an unconscious but yet memory fulfilling system? I think it's mostly

06:04.240 --> 06:10.640
philosophical whimsy. I don't actually think that philosophical zombies are a practical reality,

06:10.640 --> 06:18.720
they're just sort of a philosophical exploration. Well, as David Chalmers brought up the thought

06:18.720 --> 06:24.000
experiment is if you had an atom for an atom reconstruction of a human that, you know, the

06:24.000 --> 06:28.800
first human had consciousness and the second human didn't have consciousness, although all

06:28.800 --> 06:36.160
their atoms and physically they were exactly the same, wouldn't that imply dualism? So, but I mean,

06:36.160 --> 06:41.520
people take the thought experiment to mean something else and it's been used to describe

06:41.520 --> 06:48.000
artificial intelligence without the architecture or the phenomenology of consciousness as well.

06:48.000 --> 06:54.560
So yeah, so we're, this interview is evolving quickly, we're getting into some other topics, but

06:55.280 --> 07:00.960
yes, there is a huge question in the last three years, give or take,

07:01.840 --> 07:09.760
you know, what the implications are of the huge LLM models? What does that actually have to say,

07:09.760 --> 07:15.920
not only about artificial intelligence, but also about consciousness? You know, are these things

07:15.920 --> 07:20.800
conscious? Do they actually have a sentient sort of set of rights and things like that?

07:21.440 --> 07:27.120
A lot of people, I think with pretty good reason, think that the LLMs are not satisfying those

07:27.120 --> 07:31.920
requirements. They're doing something else, but they're probably not conscious in the way that

07:31.920 --> 07:37.200
we're particularly concerned about. I don't think that they have a lot of recurrent

07:38.960 --> 07:44.560
wiring in their layers, they tend to be pretty feed forward heavy. And there's, you know, I'm

07:44.560 --> 07:47.600
definitely one of those people who thinks that recurrence is doing something, I don't quite

07:47.680 --> 07:54.800
know what, but I think recurrence is a crucial component of consciousness in some fashion,

07:55.520 --> 08:02.160
sort of sense of kind of looking back on on itself in a sort of a loop of sort of experiential

08:02.160 --> 08:07.360
self-learn looking. We don't know metaphysically like why, you know, what is it about that?

08:07.360 --> 08:12.400
That's critical to consciousness, but a lot of people seem to agree that some kind of recurrence

08:13.200 --> 08:19.520
should be part of the design. Short of that, you know, something that's always bothered me about

08:19.520 --> 08:27.040
the modern AI systems is that they're all sort of on and off and then they're in the dark. You

08:27.040 --> 08:33.600
query them with your question, they go off and they dig through their neural net, they come up

08:33.600 --> 08:39.760
with a result and they produce for you a paragraph of text or image or a digitized voice, and then

08:39.760 --> 08:45.280
they go dark. That's it. Like they're not sitting there daydreaming, they're not musing on the

08:45.280 --> 08:49.680
conversation you're having with them going back over, you know, the previous dialogue from last

08:49.680 --> 08:57.120
minute. They're not doing, hey, they're dead while the prompts, if they're blinking until you issue

08:57.120 --> 09:04.960
your next query. So is that, can a system like that be conscious? This is an interesting question,

09:04.960 --> 09:11.040
and are these systems conscious on the basis of that sort of strange operation, which is just

09:11.040 --> 09:16.240
very not lifelike. It's just not what we think of. It's not what we do with people. You know, a person

09:16.240 --> 09:21.600
doesn't sort of sit there like a rock until you go up to them and sort of, you know, you query them

09:21.600 --> 09:27.440
and then they do something and they emit and then they die until you interact with them again. Like

09:27.440 --> 09:33.920
people don't work that way, animals don't work that way. So natural brains seem to be in a constant

09:33.920 --> 09:39.200
flux. That's what it means to be a dynamic living brain. And modern AI systems don't do that.

09:40.000 --> 09:44.400
By definition, it's just not what they do. So I don't think they're conscious. I think it's pretty

09:44.400 --> 09:52.000
clear that they're lacking these major features that we would generally assume we expect before

09:52.000 --> 09:57.840
we're going to put those labels on it. I don't know. Yeah, what a time to be alive where we

09:57.840 --> 10:04.800
beginning to be able to, I guess, empirically investigate some of these questions in more,

10:04.800 --> 10:12.240
with more precision, and also with a whole lot more data and we can test assumptions in artificial

10:12.880 --> 10:18.480
sort of substrates instead of just, you know, trying to probe directly the human mind.

10:19.440 --> 10:24.480
Not averse to doing the, the, the latter though, I think it's really interesting where brain

10:24.480 --> 10:34.640
computer interfaces could go. But given the, yeah, so what, what kind of resolution or fidelity of

10:35.520 --> 10:45.200
memory do you think is required for persistence of, in order to either reconstitute or persist

10:46.160 --> 10:54.480
an identity or a sense of self? Yeah, I'm not. How do we approach that question? Yeah, maybe it's a

10:54.480 --> 10:59.040
better question. Well, I think the first approach is to bring in the professionals of which I am

10:59.040 --> 11:04.000
not. I think that there are, you know, psychologists, psychiatrists and neurologists probably have,

11:05.360 --> 11:09.680
there's probably a lot of very established science in sort of how memory evolves over the course of

11:09.680 --> 11:18.880
one's life. And what degrees of memory loss imply, you know, sort of noticeable losses of

11:18.880 --> 11:26.000
personality or, or sort of the, the external third, third party perception that a person is

11:26.000 --> 11:31.040
sort of being lost over the course of dementia, you know, we can say, you know, if you lose your

11:31.040 --> 11:36.720
memory at a certain slow steady rate, it's not dementia and you're not really losing your

11:36.720 --> 11:42.480
personality. But if it happens faster, or if it happens more discontinuously, then we as external

11:42.480 --> 11:48.400
observers sort of recognize that there's some sort of identity collapse sort of in process, you know.

11:48.400 --> 11:58.960
So I don't, I don't claim to, you know, beholding the professional sort of state of that. But it's

11:58.960 --> 12:03.440
something like that, you know, it certainly can't be a requirement of perfect fidelity because

12:03.440 --> 12:08.400
we are, this is, this is Thomas Reed's experiment from, from the 1700s that people are, you know,

12:08.400 --> 12:13.760
over the course of their lives, always constantly steadily shedding their old memories. And yet

12:13.760 --> 12:18.320
we consider, consider it to be a persistence of identity so long as the shift is, you know,

12:19.280 --> 12:26.400
you know, smooth below some threshold. But, you know, to give you an actual answer to that question,

12:26.400 --> 12:34.560
I don't know. I do think it does imply, though, that when we try to create

12:34.560 --> 12:40.240
mind uploads, you know, and things of that nature, there's going to be a lot of wiggle room. You know,

12:40.240 --> 12:46.880
people who demand that before a mind upload can be bonafide, it has to preserve every quantum state

12:46.880 --> 12:52.480
in the brain or some complete nonsense like that are, I just think they're off the map. I mean,

12:52.560 --> 12:58.160
it can't possibly require anywhere near that level of fidelity for us to judge the result

12:58.160 --> 13:04.080
to be a successful preservation of identity. And that's just ridiculous. But again, I don't

13:04.080 --> 13:07.200
actually know what the threshold is going to be. But I think we're going to have a lot of leeway.

13:07.200 --> 13:09.760
I think there's going to be a lot of flexibility once we figure out how to do this.

13:11.040 --> 13:18.240
Right. And previously in an interview in the past, you've mentioned the idea of a child

13:18.800 --> 13:25.040
had drowned in very, very cold waters underneath ice or something like that. And in upwards of an

13:25.040 --> 13:31.840
hour after drowning or being out of it for an hour, they've been able to be revitalized.

13:33.760 --> 13:41.200
So this cuts to a very specific argument. A common argument that comes up in

13:41.520 --> 13:51.360
debates about whether or not a mind upload preserves your identity is a concern that people have

13:51.360 --> 13:56.080
about whether or not it preserves your stream of consciousness. There is this perception that you

13:56.080 --> 14:05.120
have this permanent lifelong and never ending ongoing dynamic stream of processing. And if

14:05.120 --> 14:10.800
that breaks and restarts, such as you go into some sort of, you know, mind uploading operation in

14:10.800 --> 14:15.600
your entire brain is vaporized, but it's scanned in the process and then it's revitalized in a

14:15.600 --> 14:20.960
whole brain emulation, a computational model. That breakage marks a loss of identity and the

14:20.960 --> 14:26.800
person dies by definition. And then a new doppelganger basically springs up out of the ether

14:26.800 --> 14:32.480
and takes the place. So that's the basis of that argument. And I've written not one, but in fact,

14:32.480 --> 14:39.600
two papers on this very specific question. They have very similar titles there. And I don't know,

14:39.600 --> 14:49.600
I think they're both worth reading, but there is a proof positive argument that that whole line

14:49.600 --> 14:57.680
of reasoning doesn't work. So, you know, the way a sort of an exploratory debate will usually approach

14:57.680 --> 15:02.000
this is like, well, when you fall asleep, you sort of lose a little bit of consciousness. Does that

15:02.000 --> 15:05.200
break your stream of identity? And then you say, yeah, but you still got a lot of neural processing

15:05.200 --> 15:10.880
going on. So that doesn't count. Okay, well, you go into the ER, you go into general anesthesia.

15:10.880 --> 15:15.200
That's really deep. That's really, you know, that's really turning the brain down a lot. Maybe,

15:15.760 --> 15:19.040
you know, does that undo the argument? They say, well, no, even in general anesthesia,

15:19.040 --> 15:22.160
you know, you still got a lot of neural processing going on. So now that doesn't count.

15:22.960 --> 15:25.920
There's just sort of a lot of knockdown arguments here that you're trying to sort of

15:26.720 --> 15:32.960
work your way through. And eventually you get to the fact that there are real world medical cases

15:33.920 --> 15:40.480
of people under two different scenarios, one accidental and one intentional sort of an OR

15:40.480 --> 15:48.240
procedure that basically disprove this whole thing. So there is this accident tragedy,

15:48.240 --> 15:55.040
usually called rapid frigid drowning. It's occurred a few times. It either occurs when

15:55.040 --> 16:00.400
someone falls into a frozen lake or occasionally in a dry example, they just fall into a snow drift,

16:00.400 --> 16:04.160
essentially. These are these are sort of real medical cases that have come up.

16:05.520 --> 16:10.000
And they are found several hours later, seven, seven, eight hours later.

16:12.080 --> 16:14.960
And in a few, you know, most in most cases, like this person dies.

16:16.480 --> 16:23.760
In a few rare medical cases, they dropped in temperature quickly enough that instead of

16:23.760 --> 16:28.960
killing them, it essentially preserved them the same way a freezer preserves food from spoiling.

16:31.120 --> 16:38.240
And in a few rare medical cases, those people have been miraculously brought to an emergency room

16:38.240 --> 16:42.080
and they had they basically shot from the hip. It's not like this happens often enough that

16:42.080 --> 16:46.080
they know what to do. But, you know, the doctors basically went through the motions of trying to

16:46.080 --> 16:50.240
slowly warm a person up so that they don't go into shock and various things of that nature.

16:50.240 --> 16:54.880
And there have been cases of recoveries. Okay, what's the point of all this? The point is,

16:55.440 --> 17:00.480
well, first of all, the point is not that their bodies and their brains froze solid,

17:00.480 --> 17:06.640
but you know, this is not an example sort of natural accidental. Oh, and so that was the accidental

17:06.640 --> 17:14.160
case. Then there's an OR procedure called hypothermic preservation. I can't remember exactly what the

17:14.160 --> 17:19.440
term is. It's usually used for cardiac surgeries. It's not used for neural surgery. It's used for

17:19.440 --> 17:26.960
cardiac surgery. You take the patient's brain down in temperature in order to buy more time to

17:26.960 --> 17:32.000
perform the cardiac surgery. Okay, while you're working on the heart, you're not pumping blood

17:32.000 --> 17:37.040
through the rest of the body, including the brain. You take the brain down in temperature on purpose

17:37.040 --> 17:42.960
so that the brain will survive longer, hopefully long enough to finish a heart surgery. Okay,

17:42.960 --> 17:49.600
so those are the two cases. So then coming back to my point, sometimes people sort of

17:49.600 --> 17:53.920
overinterpret this. They're like, oh, this is an example of cryonics. We know cryonics works.

17:53.920 --> 18:00.320
That's actually not true. In none of these realized medical cases, did the person remain

18:01.840 --> 18:05.840
in a cold environment long enough to literally freeze? That's not what these medical cases

18:05.840 --> 18:13.840
involve. But they do involve a drop in temperature to, okay, the temperatures are in those papers

18:13.840 --> 18:18.560
that I wrote. So go to them for the exact numbers I don't remember. But somewhere on the order of

18:19.440 --> 18:25.520
15 degrees Fahrenheit, I think, so they're well above freezing. But the point is that

18:27.600 --> 18:34.720
there is a temperature below which the brain does not operate anymore. It does not fire

18:34.720 --> 18:43.200
action potentials between neurons anymore. Neurons do not operate just because they're above freezing

18:43.200 --> 18:48.080
or something. There's a critical temperature at which they are able to operationalize

18:48.080 --> 18:55.360
being a neuron, primarily operationalizing action potentials, which is the fundamental basis of

18:55.360 --> 19:03.520
all sort of neurology theory. The propagation of action potential is how we expect the brain works.

19:05.120 --> 19:13.120
And that temperature is higher than some of these medical cases. So just I'm speaking

19:13.120 --> 19:17.440
in sort of long interwoven sentences here. What it comes down to is there are medical cases where

19:17.440 --> 19:27.280
a patient has had a brain not fire any action potentials for many, many hours, which coming back

19:27.280 --> 19:31.920
to the original point means that their stream of consciousness was turned off like a water tap.

19:31.920 --> 19:38.160
You cannot have a stream of consciousness if you don't have a brain sending signals through

19:38.160 --> 19:42.080
itself from one neuron to the next. There's no stream of consciousness if the brain is not

19:42.080 --> 19:48.000
processing. So these are patients. Stream of consciousness ceased. And when they were revived,

19:48.640 --> 19:54.400
nobody called them a copy or a doppelganger or a body snatcher or whatever these ridiculous like

19:55.120 --> 20:00.320
everyone just treated them as a revived recovered medical patient.

20:03.120 --> 20:06.960
And that's it. That's the disproof of the entire stream of consciousness

20:06.960 --> 20:12.000
counter argument against preservation of identity via mind uploading.

20:13.120 --> 20:18.240
On the basis of it sort of involving some cessation of processing during the uploading

20:18.240 --> 20:25.440
procedure, it's just a that's the whole thing. I think the paper is going a little more detail,

20:25.440 --> 20:30.320
but really, I don't think there's a whole lot more there. So that's you sort of brought that up.

20:30.320 --> 20:33.280
And I just thought I'd walk through that. That's what that was all about.

20:35.040 --> 20:38.720
Is whole brain emulation needed for blind uploading?

20:39.280 --> 20:43.600
Right. I think whole brain emulation,

20:47.360 --> 20:50.000
short answer, yes, I mean, I don't really see how you do it without it.

20:50.000 --> 20:55.920
There are some interpretations of mind uploading that sort of lean away from that. There's this

20:55.920 --> 21:04.160
concept of training a, what did they used to be called classically? They were called expert

21:04.160 --> 21:08.720
systems. This is what they're called in the 80s and 90s. These expert systems, which have basically

21:08.720 --> 21:14.560
now become bots and the recent LLMs and the AIs, if you train one of these things on the corpus

21:14.560 --> 21:20.080
of a person's available lifetime of media, some people provide more media than others,

21:21.360 --> 21:29.440
but everything they wrote, all their emails, any videos, home videos, everything you can find

21:29.440 --> 21:36.000
that person ever produced, if you train one of these neural net AI systems that are sort of

21:37.680 --> 21:44.240
all above these days against that, you can produce a bot that basically in a dialogue acts

21:45.360 --> 21:50.400
a lot like that person. So the theory, and there have been companies that have made an

21:50.400 --> 21:55.360
entire business model around sort of ostensibly preserving you in this way, or at least preserving

21:55.360 --> 22:05.920
like this sort of post-mortem puppet for other people to interact with. So the idea is you produce

22:05.920 --> 22:11.040
the system, and in those cases where there's a rich enough set of data, you can produce a bot

22:11.040 --> 22:17.520
that sort of passes for the person, and with that count is mind uploading. So that's a version of

22:17.520 --> 22:22.320
mind uploading that doesn't involve whole grain emulation, but there aren't too many examples

22:22.320 --> 22:28.640
like that. In most cases, mind uploading is a whole grain emulation concept, and the way I

22:28.640 --> 22:36.160
describe it is that whole grain emulation is just a technical term. It's the process of

22:36.720 --> 22:42.720
modeling and emulating the neural processing of a brain. Doing that is whole grain emulation.

22:42.720 --> 22:47.920
Mind uploading is not actually a technical term. Mind uploading the way I see it is

22:47.920 --> 22:54.240
it is a philosophical interpretation of the consequences of whole grain emulation. If you

22:54.240 --> 22:59.600
choose to interpret whole grain emulation as a preservation of a person, a preservation of a

22:59.600 --> 23:05.120
person's identity, then the label for that is mind uploading. Mind uploading is the

23:05.120 --> 23:10.000
interpretation of whole grain emulation as a preservation of identity. You have uploaded

23:10.000 --> 23:15.760
the mind. I don't actually hear too many people phrase it that way, but that's that's how I've

23:15.760 --> 23:21.040
always thought of mind uploading as being this interpretation of the technical process of whole

23:21.040 --> 23:31.760
grain emulation. So I mean, I guess the question comes to me is, do you think you'd need to upload

23:31.760 --> 23:41.120
all the exact brain of a particular identity in order to reproduce that identity in the machine,

23:41.120 --> 23:46.720
or are there some parts of the brain that are rather generic that you'd be able to scaffold on

23:46.720 --> 23:57.200
more templated emulations of like a generic brain, certain salient features of the brain,

23:57.200 --> 24:02.640
which are more important to preserve personal identity and during metaphysical in go?

24:03.200 --> 24:08.400
Yeah, I mean, there's one really big one, which is that something like 80% of our brain

24:08.400 --> 24:12.640
by neuron count is the cerebellum. And you could probably just throw the whole thing away.

24:13.200 --> 24:18.080
You might, you know, not bring your expert piano playing skills along with you.

24:19.360 --> 24:23.760
And you know, you might because the muscle memory is often encoded in the cerebellum.

24:24.720 --> 24:28.480
And you know, whether or not you could borrow someone else's or well, or you could relearn it

24:28.480 --> 24:32.800
again, right? But if we're trying to sort of simplify the process of the emulation, because

24:32.800 --> 24:36.880
it turns out to be technically really hard, you're trying to find ways to cut corners.

24:38.880 --> 24:45.280
You know, it's it's sort of debatable whether a professional musician or it could be any

24:45.280 --> 24:49.600
professional athlete, because some sort of sort of muscle memory task,

24:49.600 --> 24:52.880
would they consider that to be a critical component of their identity? If they lost that,

24:52.880 --> 25:00.880
would they not be the same person? You know, that's a very sort of emotional question to ask.

25:00.880 --> 25:07.840
But nevertheless, it really doesn't impact the cortical, I mean, by which I mean cortex,

25:08.880 --> 25:15.280
the cortex encodings that we generally associate with all the rest of personality traits.

25:15.280 --> 25:21.600
All of that is in the cortex, not the cerebellum, you know, not the alamus, maybe the hyper alamus,

25:21.600 --> 25:25.600
because that's doing some short term memory work, you know, not the amygdala. There's all sorts of

25:25.600 --> 25:30.960
stuff in there that doesn't seem to be particularly pertinent to what we consider to be human identity.

25:32.160 --> 25:36.240
Which isn't too surprising, because if you look at it from an evolutionary perspective,

25:36.880 --> 25:45.360
the human cortex and neocortex is just this 500,000 year veneer, you know, sort of thinly

25:45.360 --> 25:51.040
layered on top of 4 billion years of stuff that we don't care about. We don't care about the

25:51.040 --> 25:55.920
aspects of our identity back when we were amphibians. So why would we consider that important?

25:55.920 --> 26:02.960
What we really are interested in is just the last 500,000 years of neurological evolution.

26:02.960 --> 26:09.280
That's the stuff we really need to successfully carry over to preserve human identity. Now,

26:09.280 --> 26:14.560
I'm not saying we have to just sort of cut, you know, cut it off that early. You know, it might

26:14.560 --> 26:21.120
be nice to try to get a richer, more complete emulation. I'm not against that. But the question is,

26:22.000 --> 26:27.920
is it possible to achieve, you know, a pretty good passing whole brain emulation and

26:27.920 --> 26:34.400
mind upload, you know, at some fraction of the total brain? And I think we could. I think we could

26:34.400 --> 26:42.640
make a good enough version one with way less than the entire brain. And over time, we'll probably

26:42.640 --> 26:46.240
be able to do the rest of it anyway. So it's probably going to be a question 10 years later.

26:46.240 --> 26:48.560
But in theory, I don't think we need a lot of it.

26:49.440 --> 26:58.960
Okay, so therefore, if we don't need to emulate the whole brain, then are we approaching the

26:58.960 --> 27:04.000
compute required to be able to achieve this if we knew how? Do you think?

27:05.440 --> 27:10.160
So there are very wide estimates. You know, there are different, there are different sort of

27:10.800 --> 27:17.600
dimensions, you know, processing power, memory capacity. I think at a recent video or presentation

27:18.880 --> 27:22.480
sort of the speed of communications in between the units was considered.

27:25.360 --> 27:31.600
We, I think the expectation is that we have almost gotten there now with some of our supercomputers,

27:32.240 --> 27:39.760
at least at the lower estimates of human processing capability. But this sort of begs the question

27:39.760 --> 27:46.720
of whether mere number of computes per second is actually the trick. Or, you know, I'm a big

27:46.720 --> 27:52.000
believer in neuromorphic computing in which you actually create a massively parallel system

27:52.560 --> 27:59.760
that is connected to itself and wired against itself in ways that closely mirror the architecture

27:59.760 --> 28:05.440
of the brain. And you know, one might ask, well, who cares, you know, and this comes back to a

28:05.440 --> 28:12.000
point made earlier, which is that I strongly suspect there is something about the recurrent

28:12.080 --> 28:19.440
self-referential, you know, self-looping signal processing structure of the brain

28:20.000 --> 28:25.920
that is important. I don't know why this is an open question. We don't understand consciousness

28:25.920 --> 28:32.720
well enough to understand why certain network architectures seem to work and others don't.

28:32.720 --> 28:37.520
So the wiring diagrams in the cerebellum have essentially nothing to do with your consciousness.

28:37.520 --> 28:45.840
If you remove a person's cerebellum in surgery, their consciousness, their sense of who they are

28:45.840 --> 28:51.840
is all left completely intact. Again, there can be sort of some damage to their muscle memory,

28:51.840 --> 28:58.880
but their sense of who they are is not harmed. And their conscious experience is not affected.

28:58.880 --> 29:05.440
So the way that the cerebellum is wired is not the right way. And the way the cortex is wired

29:05.520 --> 29:10.960
is the right way. What is that way? We don't know. It's the 21st century. What a time to be alive.

29:10.960 --> 29:19.440
I don't know. But I think that I think that we need to get, I think we need to solve that.

29:19.440 --> 29:24.000
I think we need to figure that out. We need to figure out what it is about the wiring of the

29:24.000 --> 29:30.640
cortex that is creating this amazing experience. And that's what we need to make sure that we

29:30.640 --> 29:37.840
recapture in an emulation. Do people know what parts of the brain were destroyed

29:37.840 --> 29:44.880
when Phineas Gage got a big rod through his brain? They definitely do. I don't recall if I've

29:44.880 --> 29:51.200
talked my head. So it went sort of right behind his eye, which means that it hit a lot of his

29:51.200 --> 29:59.600
prefrontal cortex, which is, you know, relative to a prandola pram. It hit parts of the brain

29:59.600 --> 30:05.280
that are salient to sort of emotion control or something, which is why after the accident,

30:05.280 --> 30:11.200
he became very sort of fly off the handle, kind of sort of an angry kind of person. He just sort

30:11.200 --> 30:16.720
of didn't regulate his emotions probably. So something about that sort of, and it wasn't like

30:16.720 --> 30:22.640
an amygdala, you know, sort of emotional response. I understand, I really have to go read it. I'm

30:22.640 --> 30:28.960
sort of shooting from the hip here. But my understanding is that our, you know, very advanced

30:28.960 --> 30:34.800
intellectual cortex level ability to regulate our emotions to basically just think through

30:34.800 --> 30:40.240
a situation instead of flying off the handle. That's all, you know, that's not the kind of

30:40.240 --> 30:46.960
carefully reasoned stuff that, say, a mouse does, you know, a mouse more or less does actually kind

30:46.960 --> 30:54.320
of respond emotionally to what happens around it. Humans can feel the emotions, but also think

30:54.320 --> 30:57.920
through it and say, okay, I'm not going to act directly in accordance with my emotions. I'm

30:57.920 --> 31:02.560
going to act in this intellectually rationalized way instead, because I have a cortex that enables

31:02.560 --> 31:09.280
me to do that. And that's precisely what got blasted out of Cage's head. But no, I don't remember

31:09.280 --> 31:13.840
exactly which, if you look it up, it'll tell you exactly which parts of his brain were damaged.

31:14.080 --> 31:21.120
Do you think quantum computing is required for whole brain emulation or mind uploading?

31:21.760 --> 31:24.240
Or if it's not, could it be desirable?

31:24.800 --> 31:29.520
Yeah. So of course, Penrose and Haneroff made this famous.

31:32.560 --> 31:33.520
My God, you be all right.

31:33.520 --> 31:39.600
The most honest answer is that I don't know. Of course, people tend to have hunches and I have a

31:39.600 --> 31:48.000
hunch. What's your hunch? I would be surprised if the quantum mechanical properties of neurons

31:48.800 --> 31:55.440
had anything to do with sort of animalian evolved behavior. Just why would evolution

31:56.320 --> 32:02.560
need to go there? It seems like all it really needs to do is get the action potential sputtering

32:02.560 --> 32:09.520
around well enough that the animal can satisfy the four Fs of survival. Why do you need quantum

32:09.520 --> 32:18.720
mechanics to do that? I don't see the need. I don't get it. I just never found the quantum

32:18.720 --> 32:25.760
mechanical argument for consciousness convincing. That's my take. But I'm also fairly open-minded

32:25.760 --> 32:29.440
about it. There's a lot we don't understand about quantum mechanics. There's a lot we don't

32:29.440 --> 32:37.120
understand about consciousness. Some people see that as an excuse to pin them together, implicitly.

32:37.120 --> 32:40.880
People like Deepak Chopra are like, well, we don't understand consciousness and we don't understand

32:40.880 --> 32:46.160
quantum mechanics. That means they're connected. So that's garbage reasoning. Two things aren't

32:46.160 --> 32:52.480
related just because they're both mysterious. But at the same time, they are both mysterious.

32:52.480 --> 32:58.480
We don't actually know. So I'm open to someday discovering what there's a connection there.

32:59.440 --> 33:05.760
But I don't see a need for it now. Forgive the cat. The cat is forgiven.

33:09.680 --> 33:16.320
Once we can upload brains onto a computer, there's going to be an economic issue too

33:16.320 --> 33:23.280
about how to afford to do all this and how to do it in a cost-effective way. Such is a good

33:23.280 --> 33:29.920
ratio between the amount of minds that can be uploaded and the amount of power which is consumed.

33:31.840 --> 33:37.360
The trade-offs will become more friendly over time as technology gets better. But what's the

33:37.360 --> 33:45.680
role of the type of resolution that we might want to capture and the ability to do compression

33:45.760 --> 33:50.880
to reduce the amount of space a mind might occupy?

33:52.320 --> 34:00.400
I do have some numerical answers to that question. But more to the point, everyone's sort of used to

34:00.400 --> 34:08.400
these claims about the rate of advance of technology and it's true. So we can just barely scrape

34:08.400 --> 34:13.840
together a supercomputer that might emulate the human brain today. It's millions and millions

34:14.080 --> 34:18.640
of dollars and it will only run one brain. And it's not wired correctly to do that anyway. So we

34:18.640 --> 34:22.400
couldn't do it even if we wanted to. But in theory, the architecture is there. So we're just,

34:22.400 --> 34:27.280
you know, that's where we are now. So we're down the road. We'll be able to do it

34:27.280 --> 34:32.480
expensively but successfully. And then 10 years after that, we'll be able to do it cheaply. You

34:32.480 --> 34:38.560
know, you know, the human brain uses as much electricity as a light bulb. So we're going to

34:38.560 --> 34:42.800
get it, you know, at some point, the power requirements won't be relevant. You know, it's not

34:42.800 --> 34:48.240
like you're going to need a supercomputer or even anything as inefficient as a desktop computer.

34:48.240 --> 34:52.480
You'll need something as efficient as a brain. And you know, how cheap are brains? There are

34:52.480 --> 34:57.760
brains everywhere. Like brains don't take much energy. The world's full of brains. So once we

34:57.760 --> 35:05.440
are able to create those kinds of computers at that level of energy efficiency, the efficiency

35:05.440 --> 35:09.520
equation, and therefore the cost equation, it's all just going to go out the window. You know,

35:09.520 --> 35:15.440
so long as your brain, so long as your emulated brain can get, you know, as much energy as,

35:15.440 --> 35:20.800
you know, a salad, then it'll be able to stay alive the same way we can. Right? So it's just,

35:20.800 --> 35:26.320
that whole problem is going to go away. I hate to say that it's a question of patience, but it is.

35:27.440 --> 35:31.120
People really chomp at the bit about futurism stuff sometimes. They're really like,

35:31.120 --> 35:34.640
gosh, you know, this stuff really needs to happen in the 2040s or it's no good at all.

35:35.280 --> 35:38.880
I mean, I'm sorry, but some stuff's just not going to be available in the 2040s,

35:38.880 --> 35:43.040
but that doesn't mean that it's not going to happen, you know, early in the next century.

35:45.120 --> 35:49.040
Or later in the next century. I don't know when, but I'm not particularly concerned about that,

35:49.040 --> 35:54.640
because when it does happen, I just think it's going to be practically preordained. I don't

35:54.640 --> 36:00.400
think it can be stopped. I also don't think it can really be expedited too much. I mean,

36:00.400 --> 36:08.560
mostly we want to create a society and a culture that sort of broadly favors science and knowledge

36:08.560 --> 36:15.440
and openness, you know, and sort of inquisitiveness and curiosity. If we just sort of create a society

36:15.440 --> 36:22.400
that creates and supports these general ideas, the science will just happen. Now,

36:23.520 --> 36:27.360
I can get into some minor specifics. You were talking about sort of resolution and compression

36:27.360 --> 36:35.920
and things about that, things like that. So where we currently stand on sort of producing

36:35.920 --> 36:43.440
whole brain emulations is, well, in short, we can't, but there are sort of multiple steps in

36:43.440 --> 36:50.560
the process. So the most likely way to produce a whole brain emulation is to preserve the brain.

36:50.560 --> 36:57.920
That means essentially embedded in a resin, and then you deli slice it, and then you take pictures

36:57.920 --> 37:03.280
of each slice. You can't use, like, a camera, because the features, the neurological features

37:03.280 --> 37:07.520
you're trying to capture are smaller than the wavelengths of visible light. You can't take

37:07.520 --> 37:11.840
a picture of something that's smaller than a wavelength of light. So we use electron microscopy

37:11.840 --> 37:20.320
to do this. And the resolution that we're currently able to achieve is a z-axis resolution. That means

37:20.320 --> 37:29.360
slice to slice. We can physically diamond knife slice brains at about a 40 nanometer thick resolution.

37:30.320 --> 37:35.440
And then when we take an electron microscope image of a slice, we can achieve about a four

37:35.440 --> 37:42.400
nanometer xy resolution. So each image has 10 times as much resolution, 10 times the fineness of

37:42.400 --> 37:48.560
resolution as the slice to slice steps. So in the three dimensions, two of the dimensions are finer

37:48.560 --> 37:54.000
by a factor of 10 than the third. And the question is, is that good enough? And the answer is,

37:55.440 --> 38:02.640
according to current models and theories of how the brain works, you know, the major paradigm

38:02.640 --> 38:11.840
of action potentials propagating along axons, triggering synapses, and then propagating secondary

38:11.840 --> 38:20.720
action potentials and downstream neurons, we can image synapses at that resolution. And of course,

38:20.720 --> 38:25.680
we can image all the rest of the neural components, which are larger than the synapses. So we can

38:25.680 --> 38:31.120
actually capture the necessary resolution to produce a whole brain emulation. We can do it today.

38:31.760 --> 38:41.440
What we can't capture is the scale. So the data sets that we are scanning these days are, you know,

38:41.520 --> 38:46.880
what sort of cubic section of a brain can we kind of scoop up and actually perform this on?

38:47.600 --> 38:56.320
And it's on the order of sort of fractions of a cubic millimeter. One full millimeter is just,

38:56.320 --> 39:05.200
you know, sort of coming around the bend as sort of a 2020s project. So, you know, human brain is a

39:05.200 --> 39:12.560
little bit bigger than that. So we've got a ways to go. I think a whole mouse brain of approximately

39:12.560 --> 39:18.880
a cubic centimeter, which is a thousand times bigger than a millimeter, is one of the projects

39:18.880 --> 39:26.080
that is being sort of put together now to be achieved over the next many years. And when that

39:26.080 --> 39:31.040
is done, all we will have is a brain of a mouse. All we will have is one centimeter. We're going

39:31.040 --> 39:39.200
to need to go another factor of 10 in all three directions to get to the human. So

39:41.520 --> 39:48.160
it's going to take a while. But the interesting thing is that while it's going to take a while

39:48.160 --> 39:54.240
on the calendar to get there, there's a very little that actually appears to stand in the way.

39:54.960 --> 40:02.480
We just need to keep turning the crank for a few decades. And there's no real obvious like place

40:02.480 --> 40:08.160
where we think it's just technically like we're up against a wall. When we look at it today,

40:08.160 --> 40:12.880
there are things we don't know how to do yet. Getting the perfusion of the chemicals deep

40:12.880 --> 40:19.280
into a large brain evenly is very hard to do. We don't really know exactly how to do it. How do

40:19.280 --> 40:24.960
you, you know, we can slice up a slice of a brain on the scale of microns, but can we create, you

40:24.960 --> 40:33.520
know, a cross section of a brain that's only 40 nanometers thick, but is five inches across, right?

40:33.520 --> 40:40.000
Like how do you even handle that, right? So there are major technical questions, but these don't

40:40.000 --> 40:44.800
feel like things are going to be like some fundamental wall in physics that we cannot get

40:44.800 --> 40:49.520
past. It seems like we just need to keep going and we will get there.

40:51.760 --> 40:58.080
Do you think that there's a particular, okay, so you mentioned plastination, at least that's

40:58.080 --> 41:04.000
what I think you're referring to. Do you think that's the state of art and cryopreservation today?

41:05.360 --> 41:10.880
Or brain preservation today? Or would you suggest that it's something else? Or

41:11.840 --> 41:16.800
yeah. And what about in the future? Do you think where the state of the art

41:16.800 --> 41:20.640
plastination techniques that we have today are the best that they'll ever be?

41:21.600 --> 41:26.400
So correct me if I'm wrong. I think plastination refers to room temperature preservation techniques.

41:27.440 --> 41:34.800
Correct. Yeah, I don't really know where they stand. So you've got two things. You've got the cryopreservation

41:35.520 --> 41:41.200
and then there is the preparation of a brain sample for slicing, which is not cryo.

41:42.400 --> 41:47.520
It's just room temperature and it's a resin. I don't really know what the chemical structure is,

41:47.520 --> 41:56.000
but it's essentially you first you flush out a lot of colorful and opaque things so that you

41:56.000 --> 42:01.120
can physically see through the brain. And once you flush all that stuff out, you embed it in

42:01.120 --> 42:07.280
some kind of resin. I'm not a chemist. I don't know. And now you've got like this solid block with a

42:07.280 --> 42:13.760
transparent brain inside it. Then you slice that and you take pictures of it. Now preservation,

42:14.560 --> 42:18.240
you know, especially sort of preservation for the purpose of longevity, sort of cryonics or

42:19.280 --> 42:24.800
the next generation of cryonic like preservation, which would be these sort of the aldehyde

42:24.800 --> 42:30.240
stabilized cryopreservation that the Brain Preservation Foundation has awarded,

42:31.760 --> 42:37.920
which is probably, you know, definitely the next generation in preservation if we can get it

42:37.920 --> 42:47.440
sort of normalized, popularized. That's for a different task. That's for the task of preserving

42:47.440 --> 42:57.680
people, keeping them alive for one of two reasons. Many proponents of sort of classic cryonics want

42:57.680 --> 43:03.440
to be kept alive just to sort of get to that point in the future where the technology can revive them

43:03.440 --> 43:10.880
biologically again. The other reason to go into preservation of some form though is to just get

43:10.880 --> 43:15.520
to that point in the future, not where you can be biologically revived, but where mind uploading

43:15.520 --> 43:20.240
technology has gotten to the point where it can upload your preserved brain and you would not come

43:20.240 --> 43:25.520
back biologically, you come back as an upload. Now as you know, I don't want to completely distract,

43:25.520 --> 43:28.560
we can get into like, you know, why would you want one? Why would you want the other? What's the

43:28.560 --> 43:32.080
benefit? You know, why would you choose one if the other's available? There's a whole conversation

43:32.080 --> 43:38.240
there, but I just I think it would sort of take us off into the weeds. But there is never there's

43:38.240 --> 43:43.520
this issue of preservation for longevity and there is this issue of preserving and preparing a brain

43:43.520 --> 43:49.840
sample for very contemporary and real, you know, sort of slicing and scanning that we do today

43:50.480 --> 43:59.920
in labs today all over the world. What about inspirations from biology? And it's been mentioned

43:59.920 --> 44:07.360
before about the visual cortex, how that's very much like the way a convolutional neural network

44:07.360 --> 44:14.880
might work. There's hierarchies there. They've got logical receptive fields and shared weights.

44:16.000 --> 44:24.480
Do you think this is important to sort of mimic the way that biology works in the

44:24.480 --> 44:33.120
way that we approach developing a whole brain emulation? Or can it be a completely wildly,

44:33.120 --> 44:41.280
even wildly different computing power time? Yeah. So yeah, modern convolutional neural

44:41.280 --> 44:47.760
networks were intentionally inspired by the visual cortex. It's actually not the brains

44:47.760 --> 44:53.840
resemble convolutional nets. Convolutional nets were sort of built on the previous four decades of

44:54.480 --> 44:59.040
research into how vision works. We actually learned a whole lot about how vision works. And we said,

44:59.040 --> 45:03.200
wait a minute, what if we code that up to a computer and see if we can see? And lo and behold,

45:03.200 --> 45:09.440
it's actually worked incredibly well. So one thing that that story tells is that

45:10.080 --> 45:16.640
biomimicry is a very good strategy. If you figure out how nature does something and then emulate

45:16.640 --> 45:21.760
that with engineering, that's very likely to actually get you pretty far down the road or

45:21.760 --> 45:32.000
whatever task you're trying to solve. For whole brain emulation, there's an interesting opportunity

45:32.000 --> 45:38.320
to cut some really major corners at the possible expense of like a major philosophical or metaphysical

45:38.320 --> 45:49.120
failure. So what do I mean by that? At the lowest level, we could create a whole brain emulation

45:49.200 --> 45:56.960
of neurons and synapses, individually firing action potentials down their axons across the

45:56.960 --> 46:04.480
synapses into the dendrites through the full complex web of neurons. And we would expect

46:04.480 --> 46:09.360
that to certainly be functionally successful. You know, there's a whole big debate about whether

46:09.360 --> 46:13.360
it's successful in some other way, but it's, you know, because some people think it has to be a

46:13.360 --> 46:17.760
biological system to be conscious or it has to be whatever, but but it would functionally replicate

46:17.760 --> 46:23.760
the action potential propagation paradigm. But that's actually pretty computationally like

46:23.760 --> 46:30.960
inefficient. Like why, why recreate a thousand neurons that fire in some complex arrangement

46:30.960 --> 46:37.360
to perform multiplication? If you can just stick a multiplication module in there that has the same

46:37.360 --> 46:43.120
black box behavior of the neurons. If you've got a set of neurons, you've got a set of neurons

46:43.120 --> 46:49.120
and you send input signals in and you read the output signals out, you could replace that set

46:49.120 --> 46:55.680
of neurons with what we with what we call a black box. Ted Berger has been doing this with

46:55.680 --> 47:03.440
hypothalamus. And if you basically produce a prosthesis effectively that takes in neural

47:03.440 --> 47:07.680
signals and emits neural signals like the system that's replacing the set of neurons that's

47:07.680 --> 47:15.360
replacing, you can use a potentially much simpler computational piece of hardware to process those

47:15.360 --> 47:21.200
signals and yet get the same functional propagation, right? So it's an interesting question. How far

47:21.200 --> 47:29.040
that could go? How much of the brain could we abstract away to some sort of algorithm? You know,

47:29.040 --> 47:32.560
one of the one of the biggest ones would be like cortical columns. The cortical columns are these

47:32.560 --> 47:37.200
sets of about a hundred neurons that seem to sort of process kind of as a singular unit, you know,

47:37.200 --> 47:41.440
really know exactly what they're doing or how they're doing it, but there's this repeated

47:42.800 --> 47:49.520
cortical column pattern. If we could solve the cortical column algorithm, we might just black

47:49.520 --> 47:56.080
box the whole thing away. What if you just make a single computer chip that emulates, you know,

47:56.080 --> 48:01.280
the hundreds of neurons and the thousands upon thousands of synapses in a single cortical column

48:01.280 --> 48:05.600
just turn the whole thing into some little computer that may not work the same way at all,

48:05.600 --> 48:10.640
but it will take the same neural signals and it will critically, it will emit the same signals,

48:10.640 --> 48:16.080
so it has perfectly replaced that behavior, but in a completely different way, right?

48:16.800 --> 48:20.560
And then the question is, well, what if you did that with every cortical column in the entire brain?

48:21.200 --> 48:28.080
So a whole lot of the neural structure that we consider to be extremely human in its architecture

48:28.720 --> 48:35.040
would be completely replaced with some sort of computational abstraction. The same signals

48:35.040 --> 48:42.160
would be going through, but would it actually metaphysically be the same sort of mind?

48:43.120 --> 48:52.240
And we don't know, we don't know where that cutoff is, you know, where the sort of level of

48:52.240 --> 49:01.120
fidelity of your emulation loses some sort of metaphysical thing that is deemed important.

49:01.200 --> 49:07.920
We don't know where that threshold is. I suspect that we might find it, you know, because,

49:08.880 --> 49:14.720
you know, as you produce these things, as you produce these prostheses, these modules,

49:14.720 --> 49:20.000
and emulate them, if you get some sort of surprising behavior out of it, then you're

49:20.000 --> 49:26.480
probably abstracting too far. I'm not sure how that would happen, because if it's producing,

49:26.480 --> 49:31.920
if it's reproducing the same functions, there isn't an opportunity for it to sort of behave

49:31.920 --> 49:36.400
differently, like that's the whole point of the functional paradigm. So I'm not really sure where

49:36.400 --> 49:40.800
it could break down, but if it was going to break down, presumably we would discover it. I mean,

49:40.800 --> 49:44.080
otherwise you're just back into philosophical zombie territory, you'd be able to replicate

49:44.080 --> 49:49.600
all the functionality, but you wouldn't know, but then I just find that all very hard to buy.

49:49.840 --> 49:57.760
Yeah. What do you think, look, I mean, there's the issue of like the fire in the equations when

49:57.760 --> 50:03.120
like Hawking spoke about, like, what is understanding of a mathematical formula?

50:04.000 --> 50:08.640
What is the, you know, what's the important, what's the important difference between the way

50:08.640 --> 50:14.080
or computers at the time when you wrote this subject would be able to calculate and the way

50:14.080 --> 50:19.440
a human would be able to calculate? And he spoke about fire in the equations. I think it was Hawking

50:19.440 --> 50:24.960
anyway. Maybe it's got something to do with like something like that and embodied understanding,

50:25.600 --> 50:30.880
maybe a feeling. I don't know a valence of it. It's going to be one of the big questions of

50:30.880 --> 50:39.280
neuroscience in coming decades. Sure. Well, okay. Another big question that I thought to bring up

50:39.280 --> 50:47.360
was why is whole, a form of whole brain emulation of your preserved brain, probably you, and you've

50:47.360 --> 50:51.040
written directly about this, but I thought I might bring this up during the interview.

50:51.040 --> 50:57.520
Why do you think whole brain, a whole brain emulation of Keith Wiley would be Keith Wiley,

50:57.520 --> 51:04.080
even if the original was still alive? Well, that's that last sentence really. Okay. Where

51:04.080 --> 51:10.480
that's where people start the big debates, right? Well, let's step back. Let's, let's exclude that

51:10.480 --> 51:17.600
last, that last sentence then, because we will touch on identity branching. Yeah. Better. I mean,

51:17.600 --> 51:23.040
that has been the bulk of my writing is defending the branching interpretation. But yeah, why would

51:23.040 --> 51:31.040
it be me? So there are different theories of metaphysical personality. For example, you have

51:31.040 --> 51:35.920
the body theory, which basically says that your identity is attached to the matter that you're

51:35.920 --> 51:41.520
built out of. You have a stream of consciousness theory, which is that it's some sort of continual

51:41.520 --> 51:45.520
stream of consciousness. You have a space-time worm identity, which is that this thing sort of

51:45.520 --> 51:51.040
weaves its way through four-dimensional space-time, because it's sort of continuously located in

51:51.040 --> 51:56.960
sort of a smooth path. The identity is sort of following the sort of worm through time,

51:57.840 --> 52:02.400
but it doesn't discontinuously jump around. And then you have psychological identity. There are

52:02.400 --> 52:08.480
there others as close as continual identity. Close as continual identity says that the thing

52:08.480 --> 52:15.920
that is you now is that which most closely resembles the thing that was you, you know, very

52:15.920 --> 52:20.880
recently. And you have to get into the thought experiments to sort of feel sort of where that

52:20.880 --> 52:27.360
question comes up. But another theory is psychological identity, which is a catchall term

52:27.360 --> 52:32.640
for, by psychological, what we mean is the memories. Your identity is indicated by

52:33.360 --> 52:38.640
your psychological traits, your personality traits, your memories, your all the sort of cognitive,

52:39.600 --> 52:46.960
emotional, intellectual sort of senses of your, that's your identity. Every single one of these

52:47.680 --> 52:52.960
theories can be subjected to sort of colorful thought experiments that try to sort of break them

52:52.960 --> 53:01.600
apart and undo them and find some sort of paradox that leaves them. And the interesting thing about

53:01.600 --> 53:11.440
the psychological theory is that it only breaks down if you declare at the outset that you're

53:11.440 --> 53:18.480
going to reject branching interpretations. So you basically have to say, well, branching is just

53:18.480 --> 53:22.160
going to cause a lot of trouble for my thought experiments. So I'm just going to say it's excluded

53:22.160 --> 53:27.040
and we're not allowed to consider it. And if you say that, then you can come up with a thought

53:27.040 --> 53:32.160
experiment where psychological identity sort of runs into problems for the same way all the other

53:32.160 --> 53:37.520
identity problems models can run into problems. But if you don't cut yourself off with the knees at

53:37.520 --> 53:41.440
the beginning, if you say, well, wait a minute, branching identities on the table, let's see

53:41.440 --> 53:49.760
where that takes us. Then psychological identity is the most robust theory that I have yet found.

53:52.480 --> 53:58.560
Well, okay, so you're going to be talking a lot about this in your up and coming novel,

53:58.560 --> 54:05.280
contemplating oblivion. So anybody who's interested in these topics would like to explore worlds in

54:05.280 --> 54:13.200
which they are covered, yeah, watch this space or watch out for contemplating oblivion.

54:14.240 --> 54:19.680
So but yeah, would you want to just give us a breakdown of some of the writings you've

54:20.400 --> 54:29.520
covered identity branching in? It's your favorite topic. This is what you've covered the most.

54:30.160 --> 54:38.880
Yeah. So it all started actually started slightly before my 2014 book. I was inspired to

54:38.880 --> 54:44.400
write an article in which I sort of thought I would lay out my basic sort of theory of branching

54:44.400 --> 54:49.680
identity. In early 2014, I'm sort of going through my own list of writing here.

54:50.960 --> 54:55.520
Was this the H plus magazine? Let me think here. It was

54:59.200 --> 55:04.320
Oh, in March of 2014, it was actually a response to a article by Susan Schneider.

55:05.760 --> 55:11.120
I remember. Yes. And I wrote an article for H plus. Yes. I think you might have emailed me about

55:11.120 --> 55:16.800
that and got me to put that in there when I was on the board. Yeah. Yeah. And I'm looking at the

55:16.800 --> 55:20.560
so if that was March 2014, that means that I basically wrote that article. And then

55:21.280 --> 55:28.160
I must have come off of that article with ideas for a book just boiling because I started it

55:28.160 --> 55:37.920
immediately. I worked over I worked on the book that summer of 2014. So the larger work I produced

55:37.920 --> 55:41.840
is my book attacks on me and metaphysics of mind uploading.

55:44.480 --> 55:51.440
And then from there, I just continued to write. So I've I produced a paper in the Journal of

55:51.440 --> 55:58.400
Consciousness Studies with Randall Conan, which tackles one very specific question, which is,

56:00.000 --> 56:04.480
you know, we taught I've been talking about the slicing and scanning approach

56:05.200 --> 56:12.240
to whole green emulation. Another thought experiment or scenario that comes up in a lot

56:12.240 --> 56:19.760
of philosophical musing is what we call gradual replacement, where you perfuse the brain with

56:19.760 --> 56:28.000
billions of nanobot prosthetic neurons, some sort, and they they learn the input output function

56:28.000 --> 56:33.760
of the neurons. And then once they've got that function figured out, they kill the neuron and

56:33.760 --> 56:38.160
like attached to all of its synapses and slowly over time, the whole brain becomes mechanical.

56:39.520 --> 56:47.520
I think it's all complete and total nonsense. It's it's a fun thought experiment. It makes

56:47.520 --> 56:52.640
maybe potentially good science fiction. I don't I don't know. But it's not practical.

56:54.400 --> 57:01.280
It's sort of it's very sobering to read sort of online debates about the stuff in which people

57:02.240 --> 57:08.480
you know, with great emotional commitment, you know, say that they really require like a gradual

57:08.480 --> 57:14.080
replacement process in order to be uploaded. Well, I got news for you. It's not going to happen.

57:14.720 --> 57:22.320
That is so technically challenging that I don't know if we'll have it in a thousand years. It is

57:23.040 --> 57:32.000
beyond incomprehensibly challenging, whereas scanning slices of preserved brains is a 20th

57:32.000 --> 57:37.760
century tech. We're doing it now. So we're talking about the trade-offs between a technology that exists

57:37.760 --> 57:43.200
now and a technology that I'm not sure if it will exist in a thousand years. That is the that is the

57:43.200 --> 57:51.920
chasm between these two approaches. So Randall and I wrote a paper that argues not only, you know,

57:51.920 --> 57:58.080
is gradual placement just, you know, technically just bonkers. But from from a philosophical

57:58.080 --> 58:02.320
standpoint, we don't have to worry about it. It doesn't matter. It actually isn't a critical

58:02.320 --> 58:07.600
need anyway. The whole point of the paper is to argue that metaphysically and interpretively,

58:08.160 --> 58:14.320
both of these two procedures have the same outcome from sort of the perspective of personal identity.

58:14.320 --> 58:18.240
So we don't have to worry about the impossible gradual placement process, the process, the

58:18.240 --> 58:24.000
scanning of a preserved brain will work just fine. So that paper was in the I was in JOCS.

58:25.440 --> 58:29.920
I've written several others. I wrote sort of an all-encompassing paper that was sort of a broad

58:29.920 --> 58:38.320
sort of life philosophy paper with the tongue-in-cheek title of mind uploading in the question of life,

58:38.320 --> 58:46.080
the universe and everything, obviously an homage to Douglas Adams. And the the thrust of that paper

58:46.080 --> 58:53.840
is sort of a step-by-step almost theorem like layout of an argument that argues that

58:55.360 --> 59:07.520
developing mind uploading is the most important priority ever. It is the most important thing

59:07.520 --> 59:12.000
for us to work on. And the reason why it is the most important thing is the entire sort of line of

59:12.000 --> 59:16.080
reasoning laid out in the paper. One might immediately sort of knee-jerk response that

59:16.080 --> 59:20.800
well that's just ridiculous. How can I possibly say that? Well, read the paper. The whole point

59:20.800 --> 59:27.360
of the paper is to explain why I think that's so critically important. And then I've written two

59:27.360 --> 59:35.440
papers that get into this argument that a breakage in the stream of consciousness implies a death

59:36.080 --> 59:44.320
and then some sort of invoked doppelganger in the output. I found that such as such an important

59:44.320 --> 59:49.680
topic I actually ended up writing two papers on it. I don't know, I probably have others too, but

59:51.280 --> 59:53.360
yeah, I sort of keep keep writing about it.

59:56.480 --> 01:00:02.400
Well, I mean, do you think, how do you think people will react? I mean, when push comes to shove,

01:00:02.480 --> 01:00:08.880
do people really care if there's a doppelganger or if what they feel is a doppelganger out there?

01:00:08.880 --> 01:00:14.960
Do they mind if there's two identities that are metaphysically exactly the same, at least on

01:00:14.960 --> 01:00:21.040
invocation? I think a lot of people are uncomfortable with this idea. I don't know if the concern is

01:00:21.040 --> 01:00:28.080
whether there's either one or two of them. The concern is that by being some sort of a copy,

01:00:28.080 --> 01:00:32.480
it represents that they actually died. They didn't really survive the procedure.

01:00:32.480 --> 01:00:37.120
They died. The entire goal of surviving through the procedure into the upload failed,

01:00:37.840 --> 01:00:40.640
and the person living on as the upload is just some other person.

01:00:42.000 --> 01:00:47.840
And therefore, the entire goal of using this procedure as a method of survival has failed.

01:00:47.840 --> 01:00:55.600
So that's the concern. That's why people sort of get very knotted up about whether or not it's a copy.

01:00:58.800 --> 01:01:05.040
Okay. Well, was there any interesting points that you wanted to... There was a recent meeting

01:01:05.040 --> 01:01:10.640
you had, like, I guess, you know, is there anything that you discussed at this meeting,

01:01:10.640 --> 01:01:18.000
at the Hold By An Emulation meeting with Anders Sandberg, Randall Kruhner, and yourself and others?

01:01:18.000 --> 01:01:24.800
Robin Hansen was there. Yeah. Is there any points that you brought up that you thought you might be

01:01:24.800 --> 01:01:31.440
able to reveal? Well, okay, so the topic of that meeting for the audience, the topic was

01:01:31.440 --> 01:01:40.640
the ethics surrounding Hold By An Emulation. So it got into several different questions. It got into

01:01:41.920 --> 01:01:45.760
I'm just sort of reading out of the notes here, the future of work. You know, what will it mean,

01:01:45.760 --> 01:01:52.400
what will work mean when to do work consists of spinning off a Hold By An Emulation and having

01:01:52.400 --> 01:01:58.080
it do the work and then shutting down the emulation when the work is done? What does that say about,

01:01:58.640 --> 01:02:02.400
you know, people who are always concerned about technology taking jobs? So if we're just spinning

01:02:02.400 --> 01:02:07.200
off WBEs, you know, Hold By An Emulations, then how are we ever going to get a paycheck ever again?

01:02:07.200 --> 01:02:13.360
But then if Hold By An Emulations are, you know, really sort of verging on conscious, you know,

01:02:13.360 --> 01:02:18.960
human minds, there's a whole other question about work, which is, are we making, you know, an entire

01:02:19.040 --> 01:02:26.000
society of slaves, right? So there's a whole, instead of that came up. Then we talked about

01:02:26.000 --> 01:02:30.160
just sort of good old fashioned existential risk. It's a topic that always comes up everywhere.

01:02:30.880 --> 01:02:36.880
I don't actually remember exactly where the conversation went. I don't tend to have like

01:02:36.880 --> 01:02:41.600
a crystal clear like immediate short term memory. I'm the kind of person you have to look at the

01:02:41.600 --> 01:02:46.080
notes afterwards to remember and I don't have the notes from the front of me. But I can go

01:02:46.080 --> 01:02:50.560
through the topics. We talked about, oh, an interesting one was, you know, once Hold By

01:02:50.560 --> 01:02:55.520
An Emulation is sort of common, what are the implications in terms of neuro security and

01:02:55.520 --> 01:03:00.560
mental privacy? So this is a concern not for us, but a concern for the well-being of Hold

01:03:00.560 --> 01:03:07.280
By An Emulations. What rights should they have to the security of their brains and minds against

01:03:07.280 --> 01:03:13.280
hacking? And what rights do they have to privacy? You know, once, you know, a Hold By An Emulation

01:03:13.360 --> 01:03:20.080
can probably be interrogated more easily. So it'll basically be more difficult for it to lie,

01:03:20.080 --> 01:03:26.640
you know, because does it have the right to lie? We talked about access and inequality, democratization

01:03:26.640 --> 01:03:31.520
of the technology. This is sort of a, you know, a very common concern that the rich elites will

01:03:31.520 --> 01:03:36.160
not only get it first, but we'll get it first and then somehow lock it down so that, you know,

01:03:36.160 --> 01:03:39.760
instead of just coming along 10 years later at the scale of sort of technological advancements,

01:03:39.760 --> 01:03:47.840
somehow everyone else will get sort of locked out. We talked about sort of the ethics of using

01:03:47.840 --> 01:03:53.600
animals for research and development and then what the implications are for using increasingly

01:03:54.720 --> 01:04:01.600
verisibiltudeness emulations in sort of, in lieu of animals. You know, if there's an ethics question

01:04:01.600 --> 01:04:06.880
about using an animal in an experiment, is there an ethics question about using a really,

01:04:06.880 --> 01:04:11.840
really good emulation of an animal? I mean, so good that the emulation might be conscious.

01:04:12.720 --> 01:04:16.960
You know, what sort of ethical concerns are there about using a system like that experimentally?

01:04:18.640 --> 01:04:22.560
Once we get into population ethics and emulations, I'm not sure what that refers to.

01:04:24.480 --> 01:04:28.080
Yeah, I don't know. We just, we talked about a lot of stuff, but it was all sort of from an

01:04:28.080 --> 01:04:30.720
ethical perspective. That was the point of that panel.

01:04:31.360 --> 01:04:39.600
Yeah, so I mean, this probably comes up before a lot of people. So as futurists with some sort of

01:04:40.560 --> 01:04:47.920
idea of how mind uploading might work, we might be concerned about friends or family who have a

01:04:47.920 --> 01:04:57.600
short time to live. And the difficulty in discussing this with people who have not got an interest

01:04:57.600 --> 01:05:04.000
in futurology. But have you got any advice on how to speak to people about the possibility of

01:05:04.000 --> 01:05:08.560
survival beyond the flesh? You know, I sort of find myself having this conversation a lot. I'm

01:05:08.560 --> 01:05:12.160
just sort of, you know, telling people about my book. And I have to, you know, break through that

01:05:12.160 --> 01:05:17.840
initial barrier of people just sort of having to sort of an initial shock that my book is about,

01:05:17.840 --> 01:05:26.560
what? That's ridiculous. So I have that conversation a lot. People are actually

01:05:27.280 --> 01:05:32.800
generally receptive to the idea of it. I'm sure there's an entire population contingent that just

01:05:32.800 --> 01:05:36.800
sort of has this very powerful religious objection. And I just, you know, don't tend to cross paths

01:05:36.800 --> 01:05:44.960
with those sorts of people in my life. But people are just sort of broadly curious about technology,

01:05:44.960 --> 01:05:50.160
but maybe they don't make it their entire daily life, and sort of think about these things. But

01:05:50.160 --> 01:05:55.520
most of their exposure is through rather poorly written science fiction, mostly written science

01:05:55.520 --> 01:06:01.040
fiction is often pretty good. Hollywood is generally terrible. So, you know, if your exposure to

01:06:01.040 --> 01:06:08.000
these ideas is mostly movies coming out of Hollywood, then you're probably, you know, not getting the

01:06:08.000 --> 01:06:15.840
richest philosophical presentation of the ideas. So, you know, what do I do? I talk to people about

01:06:15.840 --> 01:06:26.000
it. I just try to sort of not be overly prescriptive, you know, sort of telling people sort of what I

01:06:26.000 --> 01:06:32.560
think they ought to believe. It's more a question of whether or not this ought to be available as

01:06:32.560 --> 01:06:37.920
an option for people who would choose it without putting some sort of prohibition against it.

01:06:37.920 --> 01:06:42.880
And then people who are uncomfortable with it, I don't have to do it. I mean, nobody's forcing them.

01:06:43.440 --> 01:06:49.360
But, you know, sort of try to convince people that it's just a typical sort of let's let people

01:06:49.360 --> 01:06:57.280
make decisions for their own sake kind of issue. If you can get there, then that's usually, you

01:06:57.280 --> 01:07:06.240
know, a pretty good conversation. I think people are sort of resistant, you know. People are always

01:07:06.240 --> 01:07:10.080
trying to figure out whether or not the government needs to prohibit something. I don't entirely

01:07:10.080 --> 01:07:16.640
sure why, but there seems to be something people worry about a lot. So, I think they may just have

01:07:16.640 --> 01:07:21.760
to evolve through a few generations. You know, with each passing generation, they don't have to

01:07:21.760 --> 01:07:28.960
actually shed their prior sort of biases from childhood because they're just sort of born into

01:07:28.960 --> 01:07:33.360
an increasingly technological world. They will be born into a world of increasingly

01:07:35.120 --> 01:07:40.160
sort of versatile AI systems with which they interact. You know, their teddy bears are going

01:07:40.160 --> 01:07:48.000
to be increasingly lifelike with each passing generation. And by the time anything, even vaguely

01:07:48.000 --> 01:07:53.120
resembling an Ulbrian emulation is possible, that generation will have been interacting

01:07:53.840 --> 01:07:59.920
with computers that are so smart that they will already seem lifelike anyway. And the whole

01:07:59.920 --> 01:08:05.600
concept is just going to be relatively comfortable to that future generation. We just, I don't know,

01:08:05.600 --> 01:08:09.440
I would, I don't really worry too much about our generation because it's not going to happen in our

01:08:09.440 --> 01:08:14.320
time anyway. We just need to keep the research money flowing. You know, so long as there isn't a

01:08:14.320 --> 01:08:21.680
general sort of hatred of science, then we're doing okay. Do you think choir preservation for

01:08:21.680 --> 01:08:30.640
yourself is an option to reach for your, for yourself to reach a time in which whole brain

01:08:30.640 --> 01:08:38.960
emulation is possible? So the Brain Preservation Foundation has multiple sort of goals. One is to

01:08:38.960 --> 01:08:43.680
sort of advance the actual technology of preservation to sort of actually get the thing

01:08:43.680 --> 01:08:49.600
available, make it technically possible. But then there's this sort of advocacy component of, you

01:08:49.600 --> 01:08:57.840
know, you know, how can this procedure become part of sort of gender mill society, just in

01:08:57.840 --> 01:09:03.200
general, procedure available in hospitals or something like that. I think those gears turn

01:09:03.200 --> 01:09:10.000
pretty slowly. That's a lot of politics. It's a lot of red tape and sort of a lot of regulation.

01:09:11.520 --> 01:09:16.880
You know, I just, I tend to be pretty tempered about this stuff. You know, other people just get

01:09:16.880 --> 01:09:23.760
really excited about it. They think everything's going to happen in 20 years. And I don't know

01:09:23.760 --> 01:09:34.880
if I'm cynical or just realistic. And I just, I would be absolutely amazed if the technology

01:09:34.880 --> 01:09:46.560
of human brain preservation was cheap enough and available enough to serve me. But maybe if

01:09:46.560 --> 01:09:51.360
something happens very rapidly in the next few decades, then maybe, but I don't know.

01:09:52.160 --> 01:09:57.200
But we're probably only missing it by one or two generations. It's pretty sad to think we might be

01:09:57.200 --> 01:10:03.440
one of the last generations ever that sort of has to contend with this. But I mean, I don't know,

01:10:03.440 --> 01:10:08.080
it's more exciting than missing it by a thousand years. I mean, at least we get to sort of understand

01:10:08.080 --> 01:10:12.400
it, right? You know, a thousand years ago, no one even understood any of this. So that's pretty,

01:10:12.400 --> 01:10:18.880
that's pretty, that's good enough. It'll have to be. Well, who knows? I mean, like we get AI

01:10:18.880 --> 01:10:24.400
powered scientists and there's some claim that would that could really fast forward a lot of the

01:10:25.120 --> 01:10:30.960
science. Maybe there's a chance. I don't know. Yeah. You mentioned that there's a lot of written

01:10:30.960 --> 01:10:37.520
science fiction, which you are more, I guess, amenable to, which gives better descriptions of

01:10:37.520 --> 01:10:44.880
how whole brain emulations might work. What science fiction is that? What's been your favorite?

01:10:44.880 --> 01:10:51.920
So a few years ago, I sort of sent myself on a mind uploading voyage, trying to find, you know,

01:10:51.920 --> 01:10:58.400
all the good sci-fi on it. Of course, I'm sure I missed, you know, 90% of it. But Greg Egan's

01:10:58.400 --> 01:11:06.160
permutation city is famous, Arthur C. Clark's city and the stars. Alter carbon, which of course

01:11:06.160 --> 01:11:13.360
was a Netflix hit. There were others because I had like four, at least five of these books I sort

01:11:13.440 --> 01:11:19.760
of rattled through. So I'm missing a few. But it's definitely a topic that's been presented and

01:11:19.760 --> 01:11:30.880
covered pretty well in some cases. All right. Well, yes. Is there any points that you wanted

01:11:30.880 --> 01:11:35.600
to talk about that you, that I haven't yet asked you that you thought might be worth bringing up?

01:11:36.960 --> 01:11:40.240
Well, I think you and I will talk about my upcoming book in another part

01:11:41.200 --> 01:11:47.840
session. So I'll leave that to then. So yeah, it was nice to have an opportunity to sort of

01:11:47.840 --> 01:11:52.880
talk about these ideas again. It's been a while. Yeah, it has. Thanks. Thanks very much, Keith.

01:11:52.880 --> 01:11:57.520
It's really good to talk to you again. And yeah, I look forward to our conversation about your

01:11:57.520 --> 01:12:12.800
up and coming book contemplating oblivion. Thank you very much. Cheers.

