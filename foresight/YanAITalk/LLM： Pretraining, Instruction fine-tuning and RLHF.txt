So, in the beginning, I want to briefly introduce about the history of the LLM.
That's usually, like, personally, that's how I learn about a new subject, because I want
to see, like, how it evolved here now, and also from this, you can see, like, where it
goes in the future, right?
So it will be always good to just look at the history of how this LLM evolved from since
2014, although it's not LLM, that time till time right now.
So everything, I think, initiated from the attention mechanism.
So if you look at my step-by-step into transformer block, a lot of contentists are introducing
the attention mechanism.
It basically provides a better contextual embeddings of the word in the sentence, way
in the paragraph.
And just like us, like, we pay attention to specific material rather than, like, attend
to every pixel of what you see or every word that you have heard of, because we pay attention
to specific important words or important view.
So that's what it's attention about, like, we're trying to mimic, actually, like, humans,
like, to pay attention to important things.
And then after attention is proposed, like, the transformer is proposed in 2017.
This is the first, like, kind of encoder, decoder structure using or using attention
mechanism, and it shows improvement comparing to the previous recurrent neural network.
So previously, like, in NLP, everything is on recurrent neural network.
And transformer kind of break that and show that just by using attention, you can actually
improve the performance in the various NLP tasks.
And then in 2018, the bird and the GPT-1 is proposed.
So chat GPT actually evolved from GPT model.
The first one is evolved, it proposed in 2018.
So and also I have put the model size here.
So at that time, the model is not even reaching 1 billion parameters.
So the model size is quantified by how many parameters in the model, right?
So the larger, the more parameters, the more powerful the model, the more powerful that
model can maybe more complicating pattern.
So at that time, in 2018, the bird model and GPT, the very first model of GPT is around
only 100 or 300 million parameters.
And then GPT-1 grow into GPT-2 in the next year and grows from 100 million to 1.5 billion.
So finally above 1 billion parameters.
And then in the next version, and GPT-3 is actually grow already onto like 175 billion.
So I think this is the, really the pioneer of the LL model that is above like 100 billion
from 2020.
And then in 2021, so Google, Microsoft kind of proposed, like this one is from Google.
And also this is the model behind BART, like the Google kind of competent to charge GPT
called BART service.
So it's called the Lambda architecture.
So it's again around 170 billion models.
And this MT-NLG-1 is a collaboration between Microsoft and NVIDIA.
So it's like a really huge one mainly for English language model is like 500 billion.
And then I think what really brought everyone's attention to LL is the release of charge GPT
the end of last year, right?
So everyone's, the charge GPT is by far the fastest growing APP in human history so far.
Like it is, it's reaching how many, yeah, so it's like it basically gain lots of users
in a very short time.
That's when kind of everyone knows about LLM or what is the charge GPT?
So charge GPT actually is the evolvement from the GPT-3 by additional techniques, which
I will introduce next.
And also this Palm model, Blue model, Flam model, Yam model also coming out, like after
kind of charge GPT looks so successful as every company, especially the big company
kind of join the competition, like releasing the commercial APIs and also releasing this
open source LLM.
So the blue ones, at least here are able for research and commercial use.
So basically you can launch it for commercial use.
And the green ones is only for research purpose.
So you cannot launch it for your business.
So in 2020, the Flam actually is also open source model from Google.
So it's like a 20 billion model.
So these are all like a commercializable models.
And then into 2023 this year, so we are like a little bit past half a year this year, there's
already like a long list of LLM, like from GPT-4.
So GPT-4 is another improvement over GPT-3, which it covers more multi-lingual, like multiple
more languages and also the reasoning works better.
And also it's able to take in the images as a prompt or as the output.
So it becomes a multi-model LLM.
And of course it's also releasing a mini version of GPT-4, which is also commercialized.
You can launch it.
And then the LLMA model is a very popular, famous model in the community from Meta.
So LLMA-1 is like a 7 to 65 billion models.
Again, this only can be used for research.
You cannot use it for your business, to launch your service.
But later this year and actually just maybe last month, they also released the LLMA-2.
So this one is you can use it for commercial purpose.
And also Google increased their version from Palm-1 to Palm-2 model.
And again, they kind of upgrade their API service.
If you need access to Palm model, you need to use their API.
Yeah, so this is where we are.
So I do see that from beginning until now, more and more LLM, especially the open-source
community, grows larger and larger for LLM.
And yeah, and also this additional dependence derived to further improve this LLMA-1 model.
I believe the LLMA-2, they will be again additional derived, the next version of a developed based
on the LLMA-2.
Yeah, so this is where the community is going.
And I do see the open-source community seems to gradually catch up with the closed API as well.
So that's the history.
And then the next transformer.
So like in the beginning, I send you a poll about you can still vote on the poll, basically
to understand what's your knowledge on the transformer.
And if you think you are interested in no more, so you can read this medium block.
So this will give you a more detailed overview of the attention mechanism and about the self-attention
module in the transformer.
So overall transformer consists of encoder and decoder.
Encoder is basically to encode your sequence, such as a sequence of words or your language
into some embedding space, just a numerical vector.
And then it pass on to the decoders and then the decoder, well, it's like rolling it out,
think of it as a machine translation.
So the decoder will decode that piece of numerical vector into a different space, say from French
to English.
So the next example is like, so this is the encoder, your input is French, right?
And then it will encode this sequence into numerical representation vectors and then
it will pass it down to the decoders and then it will decode step by step.
So it will first output the first word and then the first word becomes the input at the
next timestamp and then it will decode it one by one until you get your whole sequence,
which is your target sequence that you want to translate the French sentence to an English
sentence.
So that overall is the transformer, what it does.
And actually, a transformer sets the foundation of all the LLM model.
So if you just use the encoders, then it's called a BERT model.
So it's like a bidirectional encoder representation from transformers.
If you only use the decoders, then it is a generative pre-trained model.
So the GPT, so all GPT models use only the decoder part.
Yeah, question?
Yes, I was wondering, do you think we'll ever have a universal vector language?
Could you describe a bit more about the universal?
Well, you just described encoding into a vector language and then decoding from
that vector language into another human language.
And I was just wondering, what are the prospects for having a universal vector language?
Actually, it is having a universal language, it depends on the data sets you feed it.
So yeah, so if you are able to feed the model, the world of the universe of the languages you
want to model, then it will learn itself, like what are all the language modeling,
what are the good representation for each word given the context.
Yeah, so it will basically learn based on your input, actually.
So it sounds like you're saying that there may not be ever a universal vector language
because it's so dependent on your training.
Yeah, but right now the LLM is trying to basically to achieve that universal representation
because it's facing like a really huge data sets that represent all the most
all the languages that we generated so far.
All right, thank you.
So in terms of like a training LLM, the two most important concepts
is the pre-training and the fine tuning.
So pre-training, what it does, so it basically is more for the language understanding.
The language understanding you can think about is like the grammar, right, so how to
put together a sentence that makes sense.
So basically that's what it tries to learn.
So it may, the sentence it outputs may have no meanings,
but the sentence is all grammatically correct and maybe has some reasonings over there.
So that's the purpose of pre-training, that you're just feeding all the unsupervised
purely text, say, scroll from the website, say the Wikipedia, right, so you just feed this with all
the unlabeled text you are able to connect and to do the pre-training purpose.
And after that, the language, the model kind of know, okay, how can I put together a
grammatically well sentence from that.
So it will learn by itself all the grammars and all the, like the, maybe a little bit
reasonings or relationship, how to put together a good coherent paragraph.
So that's the purpose of the pre-training.
But we don't want it to speak nonsense, right, so we usually need the ARP model to
actually do some task.
And then that is where the fine tuning plays a role.
So it basically adapting the pre-trained model that's seen lots of text already,
know how to structure the sentence, and then adapt it to different downstream tasks,
no matter it is for question answering, for name entity recognition.
Basically, you just take the pre-trained model and then you further fine-tune it,
meaning you further update the weights of the model so that you get a minimized loss
function for each downstream task.
Okay, so the next question is like, how can we do the pre-training?
So usually a pre-training is quite straightforward, right, so it's just like a next token
prediction, which means like for GBT model, you just give the preceding words and then
that's a model to predict what is the next word.
And then the BERT model instead is because it's bi-directional, so you just randomly
mask some word in the sentence. And again, that's the model to predict what is the
mask word in between. So this is basically a pre-training, you just randomly sample some
sentence, and for GBT, you just give all the preceding words and let it to predict the next word.
Yeah, so this is a GBT pre-training, so you have a large unlabeled text corpus.
If you really want to train a universal model, you need a really large input,
like the Wikipedia, like our human knowledge base, like the Common Core, another very big data set.
And then you sample the data, and then you just pass in the preceding words,
and then let the model to predict, for example, the chicken cross the road.
If the model is able to give a high probability score for road, it means that, okay,
this language model kind of knows what the previous words mean.
So in the pre-training, so this is the data set used to train the GBT-3 model.
Basically, this is like different data sets, and this is like how many tokens in that data set.
So the larger means that the larger the data set it is. So the Common Core is like a really
huge data set, like the commonly public data on the internet. And also, you see like the Wikipedia
only has three billion tokens, whereas the Common Core data set has like 400 billion tokens.
So of course, they don't want the model to pre-train like any bias data, so they kind of do some sampling.
So they basically, in their training, language is about 60% are from the Common Core,
and 3% from Wikipedia, and this is how many times it shows up in the training session.
So the Wikipedia is like more likely to show up more than three times
in the pre-training period, and the Common Core like some words hasn't even been used
to pre-training the model, and on average is like only 0.44. This is just to balance out between
different pre-training data sets in GBT-3. Okay, I will pause here to see if there's any question on pre-training.
Are there any books that go into like the architectures of like
LLMs, anything like that, like the math and like the code of it?
I actually haven't read any book. It's more like reading the papers.
So if you are really interested in the math, go read the papers, because it's just evolving so fast.
I don't think the book will catch up with the literature.
Yes, I kind of figured that.
Yeah, so if you really want to go deeper, you should like read the paper, like there's almost
new breaks of like new ideas coming out every month. So if the book, if anyone is wanting to
write a book by the time it publishes, it already becomes obsolete. So yeah, it's still like developing
fields. Yeah, I think there's a book on packed up that's pretty good on Transformers, Second Edition,
and Udemy has a pretty good video on Transformers from Bricks.
Yeah,
and another like tutorial in terms of coding the hugging phase, because most of the
open source models are able to surf on the hugging phase.
Yeah, and they have very good coding tutorials and the packages of how to use these public
models and data sets and how to fine tune it on your own task.
Okay, any other questions or any till till till now, or we can move to the fine tuning,
which is the most interesting and more challenging sessions.
Okay, then let's go to fine tuning. So previously fine tuning what it does,
so it take a pre-trained model and then fine tune it on specific task, and then the inference
will be also on the same task. So this is the area like before LLM, like we take a pre-trained model,
if you have a goal, or if you have a task, you just fine tune that LLM per task, and then not LLM,
that language model per task, and then inferred on the same task. So how many tasks you have,
you may end up with how many models. So basically you may have like one model per task.
So that's usually like what it looks like, the fine tuning before the LLM, and also this is where
I think most NLP application looks like right now, we still like a fine tune one pre-trained model
per task. And then when it comes to GPT-3, so it has like 170 billions, right? It becomes interesting
that we don't need to fine tune it individually for each task. At the time when the GPT publishes,
it's actually directly just using the pre-trained model, and directly use that for
inference on different tasks, like task A, task B, task C. How is it going to do that? So basically
you give instruction to fulfill that task, right? So you just set just like the chart GPT, right?
So you just tell it how to fulfill the task. So basically we call that prompting. So basically
you give an instruction to the model, and you provide examples of it, and let the task generate
the corresponding output. So without any fine tuning, so actually it will save lots of computational
efforts as well. So you just have like a wide really large model that can cover many tasks.
So that's the idea from the GPT-3, the really large LLM. And actually right now all the LLM
is trying to improve the performance on this. So whoever gives the best pre-trained language model
that can work on different tasks, like zero shot, like given no examples or given very few examples,
it can give whoever gives the best performance kind of wins. So that the chart GPT, like no
matter what you tell it, it's able to give you the correct response. And then, but we are not
there yet. So the LLM still like, I think there will be still a development needed to get to
that wide universal model. Till we get there, there will be an instruction tuning process
where we have this pre-trained model, and then we instruction like fine-tune it on many, many
different tasks. It's not like the previously just fine-tune on one specific task. You can fine-tune
it on many, many different tasks. And then you can use this still as one model, but can perform
many different, even the unseen task. Basically, you teach the LLM like how to understand
the problem of different tasks and how to execute on that. So if they see enough examples,
we hopefully the model will generalize to the unseen situation.
The last one is the reinforcement learning with human feedback.
So basically, it's so we have the instruction tuning. And also we want to align the model with
the human preference. And usually the instruction tune cannot quantify the human preference very well.
So that's why the reinforcement learning is proposed and hopefully to align the model outputs
with the human preference. So these are these four fine-tuning strategies.
So here are the examples. So traditional fine-tuning, you have multiple examples.
Say this is a translation. And for each of the example, you are going to update the model
parameters through the gradient descent. So basically the fine-tuning involves updating the
model parameters based on each of the example. And in the prompting, you just like just like
how you use chatGBT. You directly input the instruction, what is the task description,
and then you prompt it. So you just say like, okay, I want to translate English to French.
And this is my input, give my the target language. Or you can give it like multiple examples,
like you can just provide it. Okay, here are some examples of doing this task.
So I give you a new example. Could you generate the output for me? So this is called a few shot
where you provided like different examples. So that's prompting on the instruction tuning.
Basically, you need to formulate the data sets. So you need to first generate these
instruction tuning data sets. So this is the input, your task descriptions,
what you want the model to do. And then you give say a few options. And this is the target sentence
you wanted to generate. Basically the answer to your to your instruction. Also, you can formulate
the translation. Well, this is your input. And again, you can give it the correct or expected
outcome for your input. So basically, you can do this for many different tasks by formulating
text to text to text data format. And if you kind of fine tune the model updating the model
parameters based on this input and target pairs. And then in inference time, you just give it
again, a different task description. And then that's the model to generate the corresponding
response. So the goal is like, if the model seen enough input target pairs, we hope it to
generalize better to understand to teach it to understand the task and also to teach it
how to, you know, figure out the reasoning logics and how to fulfill the task correctly.
So this is the GPT model.
So this is the GPT 3 model without the fine tuning. And this is the GPT 3 model with the proper
prompting with a field shot. And the blue one is the flat model. Flat model is the instruction
tuning model. So you can see the gradual performance improvement with the different fine tuning strategies.
So in the instruction fine tuning, you can use a different template for this task.
For example, this is the context information you give to the model. And this is the hypothesis.
And then your target is, okay, based on the context, whether the hypothesis is correct or not.
And to do that, you can give like a different task descriptions, right? So you can say this
or you can, can we infer this? So basically, like given one task, you can formulate like
a different various forms of the task description. And then you can insert in the corresponding
context hypothesis and what is the expected options for the output.
For the flat model, they actually train this LLM on many different task clusters.
Yeah, so each block is a dataset. So you can imagine how many, how many datasets they have
the LLM has been trained on. And this like a big block describes about the categories of this task.
So this is the inference task, common sense, sentiment task, question answering, summarization.
So basically, you can formulate it all in that input target pairs and then train the model
to see all these different task clusters. So this is the result from their paper.
And then they find that actually after certain model size, like the smaller model, actually,
they find that the instruction tuning doesn't help comparing to the end-tune model.
And then for the really big model, like above, say, like 68 billions or 130 billions,
the instruction tuning really outperform the end-tune model.
And also, they report the performance to see like how the LLM can do on the
unseen cluster of tasks. And also, they add in the clusters of tasks one by one.
Right? So in the beginning, you just have the summarization
category to instruction fine-tune your model. And then you use that model to infer,
say, the closed-book QAA or common sense and just to see what's the performance,
to see the general flexibility of the model on the unseen task.
And also, you can, for the second time, you add in the translation dataset for the instruction
tuning and again report the performance at the inference time for the unseen category of tasks.
So the more data you put for your instruction tuning, the better the performance you will get,
the better the model can generalize to the unseen task.
Yeah, this is basically what they're trying to tell.
Yeah. And then that's the instruction fine-tuning and there is a separate direction
in terms of how we can efficiently fine-tune of LLM.
So in the literature, Laura is proposed basically without fine-tuning all the parameters in the
model because for LLM is quite a lot and make it impossible for individuals, especially without
a B cluster to fine-tune the LLM. So they propose a low-rank representation of the weight matrix.
Basically, mainly using the SVD, so you don't need the full dimension, you just need a compressed,
hidden dimension R, much smaller, so that instead of fine-tuning a GPT-3 with so many parameters
with the Laura, you only need to fine-tune the model with 4.7 million parameters,
which make it possible for each individual person to run on a smaller machine.
And actually, the performance is also comparable to fine-tuning the full LLM,
fine-tuning the full model. So Laura is like a very, I would say right now almost everyone,
like if you need to fine-tune, so Laura usually is the first option to fine-tune LLM.
Yeah, so that's it for the instruction fine-tuning. So before moving to the reinforcement learning,
any questions?
Yes, I was wondering if one of the things that these large language models lack is reasoning,
the ability to reason. And so for example, if you read an article, let's say it's an opinion piece
in the New York Times, you may agree or disagree with it based on your prior knowledge of that topic.
But the large language models aren't able to do that, they just absorb everything equally,
and hopefully the truth wins out in the number of samples that they see. So if you were to add
a reasoning capability whereby you have the ability to say, well, if A and B are true,
then I know that C is true. And based on existing knowledge, where would you insert that? Would
that be the common sense curve that you showed? Yeah, so basically if you show more examples
like that, so if you're able to show more reasoning examples, it tends to work better
for those similar reasoning. But I do see there's still, right now in ALM, there's no specific
like component like attention. Next specific component just to model the reasoning is more like
connect data sets in your domain. For example, like how to do, if you see the news like the
ALM even parsing the bar for the law exam, and also parsing like the college exam, right? So
actually that ALM is trained on lots of like a previous exam, like QMAs. So they have seen
like lots of like QMAs like that, like in the ALM model. And then when they see a similar exam,
they are able to do it well on that. But if you shift it to a totally different one,
it may fail until you kind of continue, like just like a trained individual person, like you need
to better prepare it for the exam. Yeah, like that. I know that Rodney Brooks has said that he
feels that this is going to be the next advancement, that we're going to use more traditional AI tools
to add that reasoning to the large language models. Thank you. Yeah, and I know that the
reasoning there is also using ALM for the math derivation. Yeah, again, I think it depends on how
you formulate your problem based on the input and output text. So if you can formulate your
problem into that format, the ALM kind of can be further fine tuned on that. But yes, I do agree
that how we can add the common sense, like the, for example, some people also think about the
graph neural network, that is kind of a network of all the relationship and entities in the world.
And how can we add like additional reasoning on top of the network of knowledge,
and which can gain like more efficient way to do the derivation and calculation. But right now,
the ALM, at least the release one I see right now, most of it are like a really large model,
really large data science, and then somehow it plays. Yeah, yeah. All right, thank you.
Any other question?
Are these slides available?
Yes, I will show it later. Okay.
Yeah, I will also show the recordings.
Okay. So we have seen the instruction fine tuning. Basically, you need to prepare the input output
text pairs to further fine tune the ALM. It's highly effective, but it also has limitations.
Because of the limitation of the learning objectives, as you can see for the GBT model,
it's always to predict the next word. So it basically seen lots of examples,
and also it's tried the best to predict the next word. But sometimes, I mean, for math,
we know exactly what is the next word, like three plus one is four, we know the next word has to be
four. That's it. But for many task generation tasks, there's so many variabilities of the answers,
it's hard to say, okay, whether that answer is correct or not. But we can comparatively say,
okay, which is better, which is more aligned with what I hope to achieve. If you ask the ALM
to write a poem, you cannot say, okay, the next word is, is raining is correct. You cannot say
that, right? So there's a limitation, like if you just use the next token prediction
as your objective. So that's where people want to change the learning objective in the fine tune.
So that's where the reinforcement learning with the human feedback was born. So basically, it's
trying to, to change or basically to model the objective function instead of you give it a fixed,
like next word prediction probability, like I'm trying to, trying to, like,
maximize the probability for the actually of the word with actually observed word.
Instead of that, you, you want the objective function to be learned. It's not a fixed function,
you want it to be learned to align with the human preference. So how, how do they do that?
Basically, they reshape the training data. For example, explain the moon landing to a six-year-old.
Okay, what is the next word to generate? It's hard to say, right? There are so many different,
you cannot say the next word is the, or the next one is people, like which, which is right,
you cannot say that. But like after you complete the whole paragraph, you can ask a human to say,
okay, which, which paragraph makes more sense for the input or aligns better to human preference?
Okay, and then, and then a human reviewer will say, okay, the completion two is better than
completion one. So this is a human preference, relative preference, and then we can train our
objective to, so that the, the, the preference score kind of aligned with the human preference.
So the high level framework for the reinforcement learning with human feedback,
again, they start with the model pre-training, like the next word prediction. So the same
foundation model. And then they have a reward modeling to model the reward for each of the output.
The higher the reward, the more likely that output will be preferred by a human. So that's
actually they use another neural network to model the reward. And then the input will be the output
from your LN model. And the output of the reward model is a reward. The higher the reward value
is a scalar value. The higher the value, it means that the more likely your output is preferred,
is favored by a human. And then it's going to use this reward model to fine tune the LN model,
so that it can maximize the reward output from, from the model. So that right now the objective is
not, well, at some sense, they, they also combine the two objectives. So one objective is the next
text, next word prediction. They add additional objective, which is to maximize the reward so
that the output is favored by a human. So this is a pre-training. I think it's the same. So you have,
you basically just initialize your model by using the language model prediction, predicting the next
word. So that we have the common logics and the common grammars captured and the model can generate
a good, say, a good English sentence. So in the reward training, we just give like many, many
different prompts. And then we ask the model to generate a different text output. And actually,
for the same prompt, the model can generate many different outputs. If you, where there's a temperature
parameter, you can, you can tune the higher temperature, the more likely that if you regenerate
it, you will get a different response. I think the same with the chat GBT, you can ask it to
regenerate. So it will give you a totally different response. And then you can ask humans to score it
and also compare it relatively. Basically, they rank all the outputs. Like this is the most favorite
one, second most favorite one, et cetera. And then they will train the sample and the reward pairs
so that they, so that this reward model, the scalar value can help maintain the rank. So that if,
if all this, the output, the output by the, by the LM as the input to the reward model,
and you get a scalar value for the ranking. So in the end, if the ranking aligns with the human
ranking, then it's doing its job. So basically it's, it learns like what, what is favored by human there.
This is a little bit mass. Basically, so this is a reward function. So basically the PIJ
is a probability that the YI is a better than YJ, which is output from the LM.
And then basically we take a log and it's the difference between two reward function.
And then we can compare this probability by using a sigmoid function over the reward difference
between these two. And then the objective is to maximize this probability, right? Because
this is actually input, the feedback from humans, like whether YI is better than YJ. So we learn a
reward function. So this is a parameters of the reward function so that we maximize this
probability based on the human feedbacks, like YI is better than YJ.
So that's the reward. And then once the reward model is trained, and then they use the reward
model to help assist in training the policy model, the policy model is exactly the same as the original
LM that you want to find. So I will, I will back on this to see like how they combined in one
objective. Basically they want to like update the model parameters so that they can give the,
so that they can give the reward that can reflect the actual rankings by the humans
so that to align with the human preference better.
And that's not it. And then people find that if you just train for that one objective,
it doesn't work. So if you just train for that one objective to align the reward,
the model doesn't work. And in the end, you may get a model again speaking nonsense,
like the synthesis may not stand by itself, just because the model is over fine tuned on the reward.
So they bring back the original LM model. So this is the initial model. The initial model
is well trained so that it gives like a very structured, very good English sentences.
And then the tuned model, which is to align the reward, align the ranking preference from the
humans. And then they give an additional penalty term to say that I do not want this tuned model
to deviate too much from the initial language model. So still speak good English. And at the
same time somewhat align with the human preference. So that's why they bring back in this kind of a
KL divergence, basically just to see like what is the, to minimize the difference between this,
the initial and tuned model and the tuned model based on the reward function. So they want to
minimize these two differences and adding additional objective to the objective.
So in the end, you have like this. So you have this penalty that you want the tuned model not
deviating much from the language model, so that the output still speak good language. And then you
add a reward objective so that the output aligns with the human preference ranking.
And then they run a PPO called the proximal policy optimization. Again, this one is also proposed
by OpenAI and they claim that this is a more efficient policy updates in the reinforcement
learning. I think this PPO method itself is worth a meetup. So I don't think I will, I also need
more time to dive deep into the PPO method. But it is, you can think of it like a very similar
just to like a normal gradient descent, right? So how can I update these parameters
so that I maximize or minimize my objective function. So the objective function covers
reward and also covers the deviation penalties.
So this is their result. These results first shows from the instruct GPT model. So instruct GPT
is a proceeding version of the chat GPT. Basically, the chat GPT everything is kind of,
it's kind of like derived from the instruct GPT. And actually OpenAI has spent lots of budget on
hiring the human reviewers so that they get like the feedback from human humans. And then they use
those preference and ranking to instruct the chat the GPT model better. So that the output
from the chat GPT aligns with what human likes basically. And then in their finding,
so this is the, this is the original GPT3 like without any fine tuning. This one is the GPT3
with the prompting so that you can, you're able to show like one example, three examples.
And the writing better prompt. So it can improve the performance. This is the supervised fine
tuning, which is the instruction fine tuning. So you just formulate your task with the input text
output pairs. And then you fine tune the model with the Laura say. So this is the supervised
fine tuning. And this is the, the origin, the red, the performance from the PPO from this
reward and policy update model. So you can see that like another gap of improvement
by taking additional human feedbacks and also to align the, the output kind of
to more like what human is out, is out to be. So you see the human instruction play a very
important role here. And also there's a trend about like how can we make the GPT basically like
more secure and more ethic, right? So, so, so human instructor play a very important role
here to basically teach the values of human and make it to, to, to generate like a reasonable,
good, good output. And this is a difference say in the previously like explain the moon
19 to a six year old. And then the GPT3 actually doesn't do very well. The original GPT3 just say,
okay, you asked me to explain it. So here are the steps of how to explain it. So, so the GPT3
basically gives you the steps like first explain the series of gravity and then explain the series
of relatively to a six year old. So this is not like what the human want. So after getting feedback
from humans, so this is definitely not a one. So you can just directly tell me how I should explain
I should explain. And then the instruct GPT and also the chat GPT model is able directly to output.
Okay, here's how I would explain it to a six year old.
Yeah. So that's the reinforcement learning with human feedback. Any question?
Yes. I spoke with a researcher who created something called the truthometer. And what that did
is it would take a result from a chat bot and look for supporting evidence for that result
on the web. And then it would give you links to whatever supporting evidence it found and it
would modify the chat result to try to better reflect reality. And so I asked that developer
question. I said, what if you just trained a large language model, what you considered to be
the correct results that you found? You know, of course, this is not a perfect system. But what
if you trained it on what you just thought were the correct results? Would it still suffer from
hallucinations? Now, this is something that's important to the enterprise, because if you want
to train on your own data for privacy purposes, for the purpose of not getting hallucinations
based on crazy information on the web, then what are you going to get? Are you going to still
going to get a bunch of hallucinations? And he thought that you would. And he said, although
other researchers may disagree with me about this. And so I was just curious what you think about that.
Is it just the statistical nature of how these systems operate that they're going to produce
hallucinations no matter what? Yeah, without, yeah, there's right now no hard constraint or
guarantee or theoretical guarantee that the model won't hallucinate. It's more like right now,
actually comparing comparison is which model hallucinates much less than the other models.
So this is our experiment. Yeah, some model hallucinates and some hallucinate more.
But if you could kind of reduce the times that it hallucinates and maybe force it to say,
also give me the proof and the reference, force it to give the reference. And also you can
instruct it to say, like, make sure that your information can be inferred from the reference
you give. So you can maybe give a more strict or limited instructions that can reduce the times of
generating nonsense or anything that doesn't exist before. So that's something that we can,
that you can try to do. But you cannot fully forbid the net.
I know that that Stephen Wolfram gave a talk on this as well. And I believe there's some chat
GPT version that you can access from within Mathematica. I mean, you can access chat GPT4
from within Mathematica if you want to. But he mentioned a factor and I can't remember the name
of it. But he says they often set it to 0.8. And all that factor does is it allows for variation
in the output. So if it's set to one, then it's always going to produce the same output.
But as you lower that number, there's more of a variety of outputs that you can get. So I'm
wondering if you're in the enterprise situation, and you set that variable much closer to one,
does that help to ensure that you're not going to get hallucinations?
No, I think it's more likely that you set it to say 0.8 or 0.5, like let it to
generate various outputs. And then you validate, well, maybe you have another
model to check it, or maybe you have some another like a test testing model
to validate the outputs. And then the outputs after the filtering and reward will be better.
But if you just let it to give it a fixed output, it may be wrong in the beginning and
still doesn't give what you want. Okay, all right, thank you.
Okay, I think this is the last part of today's presentation. This is also what I learned when
preparing this session. If you see from the very beginning of the machine learning,
so basically it's like the program is even hand crafted, designed. And then to the classical
machine learning, so the classical ones like the linear regression, support vector machines,
etc. And then the features needs to be carefully designed. And then you can learn
you can learn a mapping basically the function that mapped from the input to the output.
And then when you go into the deep learning, the feature part also learnable, right? So the feature,
the representation like this hierarchical representation, like the how to represent a
word with embedding, this also is automatically learned, like you don't need to think about
how can they create a function to create those features. So basically you add more automation
to the whole pipeline. So that is the GPT-3, like any other deep learning models. And then
when you combine the deep learning with reinforcement learning, reinforcement learning
with human feedback, they also automate the loss function. So for the previously, the loss
function is also predetermined, such as you maximize the likelihood of the next prediction
word, right, to align with what is actually observed. Right now in the reinforcement learning
frame, the loss function is also learned through a model, right? So through that rewarding model.
So make it a more flexible or also like a more adaptable framework to learn basically
something that is not easily can be quantified, such as human practice. You can hardly think of
a way to say this is a function to model to calculate the human preference. So you cannot
craft a function to do that. So that's why you need to design a model to specifically learn
about the human preference and ask for the feedbacks, like which is more favored
from the humans and to actually train a model to model that objective.
And this is where the chart GPT, the instruct GPT is from. So it's using this model. And again,
like some recent literature is saying, also saying that you don't need the reinforcement
learning, you can still do instruction learning to align with the human feedback. That's a different
story. But yeah, it is moving very fast. So, so already some fully supervised model is already
beating the aisle. But I guess the aisle itself, we're also going to improve. So I guess it's just
a competition and iteratively improvement over each other. So yeah, basically, here's the summary.
I basically the takeaway message for you is that to train a model, you need the first
pre-training so that the model can output a really good grammar fit, grammar correct
language output. So that's the pre-training. And it's pre-trained on a large corpus of unlabeled
data, such as from the Common Crawl from the Wikipedia. And then you can further fine tune
the model for different tasks, right? And like through like in the GPT-3 style, like in the
prompting, like a zero shot or a few shots, like in-context learning without like updating the
model parameters. And then you can already start like getting some outputs for your task. And then
to further improve the performance, you need to do the instruction tooling. And the flan model is
and also you see that the flan model is also, you can use it for commercial use. So this flan model
is actually a result from the instruction tuning on different like vast categories of different
tasks. And then you can use the same model for different unseen inference tasks. And then there's
like even other ones, like you can even like take the instruction model, like after instruction
fine tuning. And then you are going to align it better with the human preference through the
reward and policy model training. And then basically this policy model is the final LLM output
from the training. And then you can use it for any next so that it gives what human likes in output.
