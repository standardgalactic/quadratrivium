start	end	text
0	6880	Moore's Law was turning into this acceleration of AI capabilities over the top and the other was that
6880	10800	what was happening in renewables with things like lithium-ion batteries and solar
10800	16720	was on a Moore's Law like trajectory and there was some other technologies like genome sequencing
16720	21280	and genome synthesis that seemed to be doing something similar. So I started to bundle those
21280	27280	up in this idea that we're going through the exponential transition, a transition where
27760	33520	our economy gets driven you know not by the economics of the oil industry and internal
33520	40720	combustion engines and and telephones but by AI and renewables and that these technologies
41440	47600	being on an exponential trend being fundamentally at some level kind of information technologies
47600	51120	behave really differently to the ones of the previous generation.
51120	55760	Hello and welcome to The Cognitive Revolution where we interview visionary researchers,
55760	59600	entrepreneurs and builders working on the frontier of artificial intelligence.
60320	65200	Each week we'll explore their revolutionary ideas and together we'll build a picture of how AI
65200	71280	technology will transform work, life and society in the coming years. I'm Nathan LaBenz,
71280	76160	joined by my co-host Eric Torenberg. Hello and welcome back to The Cognitive Revolution.
76800	81920	Today I'm speaking with Azim Azhar, founder of The Exponential View and fellow AI scout.
82000	85120	For a wide-ranging discussion about the transformative power of AI
85120	90800	and its implications for humanity. Azim's core observations that we are in the midst of a
90800	96880	transition from an economy driven by the likes of phones and oil to one fueled by AI and renewables
96880	102320	and that these new technologies fundamentally rooted in information and already growing exponentially
102320	107440	behave differently than their predecessors will be familiar to cognitive revolution listeners.
108160	112160	So I took the opportunity to get a bit deeper into Azim's worldview and expectations for the
112160	116800	coming years and ended up having what I think was a really engaging and delightful exchange.
118000	121680	In this conversation we cover a number of familiar topics plus some new ones that we
121680	127280	haven't explored in quite the same way before, including why Azim believes that while incumbents
127280	132800	are racing to adopt AI technology, yes startups are still likely to drive the true disruption
132800	138160	in the form of entirely new products and markets. Also what forward thinking business
138160	142080	leaders are doing today to retrain their teams and to position their companies to lead their
142080	147440	respective markets in AI adoption and why Azim still finds it necessary to challenge them to
147440	152640	think bigger, asking the question what would they do if they had a million times more compute.
154240	159840	We also explore why Azim agrees with Sam Altman's recent comments that AGI will likely arrive soon
159840	165280	but will be less of a big deal than people expect, at least initially. How AI is likely to change
165280	170160	human relationships, especially considering the rise of so many different forms of AI companions,
171200	176320	what sorts of new governance mechanisms the AI era will require, and finally why we might
176320	181600	ought to worry about what I call the great embedding, that is the seemingly natural tendency
181600	187040	for AI systems to communicate with each other in high dimensional vector formats, which while
187040	192800	efficient for them is broadly inscrutable to humans and could lead to various loss of control
192800	198160	scenarios. As always, if you're finding value in the show, we'd appreciate it if you'd take
198160	203520	a moment to share it with your friends. Azim's perspective is both imaginative and exciting
203520	208400	and also grounded and sobering, so I definitely recommend this episode to anyone trying to get
208400	212960	a better zoomed out view of the future. I would also encourage you to consider subscribing to
212960	219040	Azim's work directly, which you can find at exponentialview.co. Of course, your feedback
219040	224800	is always welcome, whether in the form of an Apple or Spotify review, a YouTube comment, or a DM
224800	229680	on the social media platform of your choice. Now, I hope you enjoy this conversation with
229680	236560	founder of the exponential view, Azim Azhar. Azim Azhar, founder of the exponential view,
236560	241680	welcome to the cognitive revolution. I'm really happy to be here, Nathan. I've enjoyed so many
241680	249360	of the episodes and want to thank you for your hard service as a red teamer for GPT4. I watched
249360	254960	that episode with my jaw on the floor. Well, thank you very much. That's kind and right back at you.
254960	261120	I've been binging your feed lately and we've got a ton of things to talk about. I guess for starters,
261120	268240	I would love to get your kind of summary of what it is you do. I think it's actually kind of similar
268240	273920	to what I'm trying to do. I describe myself, as you know, as an AI scout. And I talk to mostly
273920	279040	people on the show who are either very deep on a particular line of research or building a product,
279600	285360	but you are one of the few guests who have this kind of very zoomed out view and really are working
285360	290320	to understand the big picture. So how do you describe what you do and what goes into it?
290320	294560	I'd love to, yeah. I mean, I've been in the tech industry for a really long time. I started working
294560	301840	in 94 and I built my first websites in 93. And just over the back behind me, you can see in
301840	308640	out of focus my first computer, which is a ZX81 Timex 1000 in the US. And about nine years ago,
308640	315120	my last company was acquired and I just started to write a newsletter. And as you do, writing is
315120	319840	thinking. And I noticed within a few months, there were these few trends that were going on
319840	325440	that were pretty significant. One was, you know, Moore's Law was turning into this acceleration
325440	330960	of AI capabilities over the top. And the other was that what was happening in renewables with
330960	336880	things like lithium ion batteries and solar was on a Moore's Law like trajectory. And there was
336880	340880	some other technologies like genome sequencing and genome synthesis that seemed to be doing
340880	347120	something similar. So I started to bundle those up in this idea that we're going through the
347200	354400	exponential transition, a transition where our economy gets driven, you know, not by the economics
354400	362240	of the oil industry and internal combustion engines and telephones, but by AI and renewables and
362240	368720	that these technologies being on an exponential trend, being fundamentally at some level kind
368720	374080	of information technologies behave really differently to the ones of the previous generation.
374080	381280	And so what I do now is I try to make sense of that as a system. I do that through my newsletter,
381280	387040	but I also do it from time to time through investing and advising. But I think that what is
388000	394160	going on at its heart is in about 20 or 30 years, we'll be looking at an energy system
394160	400080	that is going to be very, very heavily renewable. We'll be using much more energy per capita globally
400080	405920	than we do today, that we will have tons of intelligence in our economies and in our lives
405920	411200	through through AI. And that will have real knock on effects and trying to make sense of that in a
411200	418880	pragmatically optimistic way. So somewhere between the extreme dystopia and the extreme utopia is
418880	425680	really my mission. Yeah, cool. Well, I'm hoping we can find that sweet spot as well. So right there
425680	431520	with you. In terms of the history of the kind of intellectual history of this notion of exponential
431520	437520	technologies, nine years ago strikes me as kind of a doldrum time for that paradigm. I wonder if
437520	443840	you experienced this, but I recently had a conversation with a guy who makes his living as a
443840	449680	speaker at corporate events and who is positioned as a futurist. And I showed him a little presentation
449760	455840	that I had put together in which I pulled out one of Kurzweil's kind of late 90s exponential
455840	460720	curves graphs. And the title of that slide in my presentation was Kurzweil was right.
461440	466640	He saw the name Kurzweil and he was like, Oh God, don't don't talk about Kurzweil. No,
466640	471520	everybody he's totally discredited. And I was kind of like, hmm, that sounds like somebody who,
471520	476640	you know, maybe got some pitches dinged in that kind of time frame when you were getting started
476640	483360	with this notion and has gone away from it. But you know, maybe that's just the weird nature of
483360	488320	exponentials. What was your kind of experience of, you know, were people sort of sour on that?
488320	491840	It seemed like, you know, there was the great stagnation thesis for a while there and your
492720	497520	start of the exponential kind of lines up with it. It seems like when I started and one, what I
497520	504960	noticed was people weren't really talking about AI. I mean, it was 2014. Machine learning was still
505040	511680	the word. It was before AlphaGo had come out and done its thing. DeepMind was doing really
511680	516480	interesting things with reinforcement learning and video games. And you could see that something
516480	523040	was happening. And I think that you had TensorFlow as the sort of stack of choice for building
523040	529760	convolutional neural networks to do their machine vision tasks. So it seemed, it seemed reasonably
529760	534240	early to be talking about, about these things. And actually developers were struggling to make
534240	539840	sense of CUDA, which is the sort of API to the Nvidia GPUs that everybody now uses, because
539840	544000	that had only been documented five years earlier. There is something that happens around 2013,
544000	551520	2014, 2015 that I think is really worth paying attention to, which is that in 2013, Apple becomes
551520	557600	the largest company in the world. And within a couple of years, those top slots are occupied by
557600	563120	what we used to call the fangs, Facebook, Apple, Google, and Amazon, or the gaffers,
563120	569120	pardon me, not the fangs. Netflix snuck in briefly during the hype cycle. And so you started to see
569120	575600	the, the companies of the industrial age, Exxon, GM, and so on, fall off that, fall off that list
575600	580880	and stay off that list. So that's an important economic moment about the, the sort of forward
580880	585360	looking stock market saying like something is changing. The second thing that starts to happen
585360	591920	in around 2014 is we see the first market for electric vehicles go past that threshold of 5%
591920	597680	of new cars being sold, being electric, which was over in, in Norway. And that 5% threshold is the
598400	602960	normally the trigger for when you see the S curve of adoption, right? And you get into that, that
602960	610000	vertical or that verticalized part of the curve. You also started to see solar power being starting
610000	615040	to be cheaper in a roughly a third to a half of the contracts around the world than fossil fuels.
615200	623360	And, and then of course we are three or four years into the deep learning wave. And, and that's long
623360	630000	enough for companies to start to ship products. So I think that there is a moment which we could
630000	635440	argue when we look back on it feels like a, a sort of historical turning point, but we also have to
635440	641280	be realistic that the, the mathematical function that is an exponential curve, when you stand on it,
641280	645200	it always looks horizontal behind you and it always looks vertical above in front of you,
645200	649760	wherever you stand on it. It's a smooth curve with no obvious turning point. And, and someone like
649760	656080	Kurt's file, you know, I think did such great work 20 years ago to articulate the exponential
656080	661360	trend in computing going back from the 1880s actually in mechanical computers. I think one of
661360	671200	the reasons why he slightly falls out of favor is because he was probably brave enough to extend
671280	677200	the curve far further than we might have otherwise thought. And I think a couple of things that he
677200	681920	got wrong was that some of the assumptions that we would have had about how the human brain works
681920	688080	in like 2001, 2002 when those books came out were, were wrong, right? Science proved that it was more
688080	694560	complex and it wasn't going to be a simple game of, of raw computation one for one. And, and that's
694560	698480	where I think that, that people start to look at him and say, well, was this just someone throwing
698480	702320	tea leaves? I think there was much more to it than, than that having read his work, you know,
702320	707840	reasonably carefully. But, but I think what's interesting is that we see these curves happening
707840	711280	elsewhere, right? We see them in lithium ion batteries and we see them in genome sequencing
711280	716800	and it's not clear why that should be the case up front. The implications of the fact that we may
716800	721440	still be, you know, standing on the part of the curve that as it, you know, as you said, it kind
721440	725680	of always does. If we're, if it's still the case that what's in front of us is vertical compared
725680	729840	to what, you know, has been behind us being horizontal, then we're in for a bit of a wild
729840	734640	ride. I think we're headed for steep, a steep curve for at least a couple more years. I mean,
735200	742240	beyond that, you know, do we sort of hit a plateau is a lot harder for me to predict. But I honestly
742240	747200	don't see any fundamental reasons that we would right now. I mean, what, what Kurzweil says is
747200	752720	that, you know, it's actually this curve is a series of layered S's. So you, you have one
752720	759040	particular technology architecture, it's very slow to develop, it hits this inflection as an S,
759040	764960	it starts to accelerate. And as it hits its flat point, the social dynamics of market incentives
764960	770000	have meant that another set of research has come in with a different architecture, a different way
770000	776640	that extends that that S up and looks from a distance like a smooth S curve. And that's
776640	780720	really nice and descriptive. But it also, I suspect people are kind of really robust with
780800	784800	their theory feels like, well, that's just praying. That's like the Turkey that's been
784800	789040	treated really well up to the day before Thanksgiving. And like, we should worry about
789040	793920	what happens the next day. What I've tried to do is I've tried to get into the underlying mechanisms
793920	799520	of why these things improve, why they get cheaper. And then what we need to do is figure out where
799520	805920	does that mechanism fail? Because if the mechanism doesn't fail, then that trend is going to continue.
805920	810080	And if it does fail, then we can say, well, we need a new mechanism and is there one,
810640	816000	you know, in the research pipeline that might deliver it. So, I would say that if you look
816000	823200	across the gamut, I mean, for example, batteries and solar power, we've definitely got more than a
823200	829760	couple of years to run in terms of price declines. When we look at compute, I just feel it's really
829760	834160	hard to bet against. I just, you know, I think that I've been hearing about the death of Moore's
834160	841680	Law for 15 years. And, you know, Moore's Law is helpful. But the question to ask is how much
841680	849120	compute can a developer get for a dollar each year? And do we really think that that is going to stop
849840	853520	declining for like a long period of time? And I find that one really hard to
854560	857840	support. And I just think it continues for a variety of reasons.
857840	864160	It sure seems like it. I mean, the CPU to GPU transition feels like a classic example of one
864160	871120	of those kind of one S-curve, perhaps giving way to another. And in the GPU, you know, we're not,
871120	874880	it definitely feels like we're still in the steep part of that particular S-curve.
874880	878240	Hey, we'll continue our interview in a moment after a word from our sponsors.
879120	883200	The Brave Search API brings affordable developer access to the Brave Search Index,
883280	886320	an independent index of the web with over 20 billion web pages.
886960	892400	So what makes the Brave Search Index stand out? One, it's entirely independent and built from
892400	899040	scratch. That means no big tech biases or extortionate prices. Two, it's built on real page
899040	904560	visits from actual humans, collected anonymously of course, which filters out tons of junk data.
905280	910000	And three, the index is refreshed with tens of millions of pages daily. So it always has
910000	915920	accurate up to date information. The Brave Search API can be used to assemble a data set to train
915920	921280	your AI models and help with retrieval augmentation at the time of inference, all while remaining
921280	927360	affordable with developer first pricing. Integrating the Brave Search API into your workflow translates
927360	932560	to more ethical data sourcing and more human representative data sets. Try the Brave Search
932560	937760	API for free for up to 2,000 queries per month at brave.com slash api.
943520	949360	We're not done yet. And I think the thing that is fascinating is that NVIDIA is obviously doing
950160	957760	incredibly, incredibly well. And it doesn't yet have the threat of real competition.
958000	962560	And what was fascinating in the CPU world was that Intel did very, very well for a really long
962560	966560	time. I guess people forget this, but if you've been around for a while, you remember Intel was
966560	971360	this sort of monopolist and perceived an Andy Grove and only the paranoid survive.
972000	976640	And it did really well only with the threat of competition, because AMD never got more than
976640	983440	15, 20 percent market share. And that's enough to propel people forward. All the incentives seem
983440	990880	lined up for there to be massive amounts of investment in scaling existing silicon chips
990880	995840	and developing new systems. I mean, you saw in the last few days before we recorded this,
995840	1001440	Amazon and Google both reported 20, 30 percent growth in their cloud businesses. When I've talked
1001440	1008160	to bosses of really big companies, you know, they are spending money on compute, you know,
1008160	1012480	really like nobody's business and their expectation is that it will grow. They don't
1012480	1016320	often think it's compute. They say they're spending on AI, but that the end of it is going to be
1016320	1024080	GPUs cranking away. So with all the incentives aligned, I struggle to see us hitting a brick wall.
1024080	1029040	It doesn't feel like it's the Carno cycle, right? So the Carno cycle was the thermodynamic limit for
1029040	1034800	the efficiency of an Intel combustion engine, something you as a native Detroit man, you know,
1034800	1041280	know, know very, very well, but we keep finding ways of eking more out of our compute. And I think
1041360	1044400	we'll, you know, we'll continue to do that certainly beyond a couple of years.
1045200	1049600	Just for kind of conceptual grounding, and I'm also interested to hear how you explain this to
1049600	1055920	the business leaders that you work with, because their understanding and their kind of eagerness
1055920	1061520	to adopt is a pretty key question in my mind as to how the next few years are going to play out.
1062160	1067920	But we have kind of two notions, two definitions of kind of types of technology
1067920	1072160	that both seem to apply to AI in my mind. One is the exponential technology,
1072800	1078080	and the other is the concept of disruptive technology. Disruptive technology, you know,
1078080	1085280	kind of classic textbook definition is a cheaper, but inferior alternative that kind of competes
1085280	1090960	on the low end of the market. It seems to me that AI is kind of both, right? It's we've got like
1090960	1094800	these, at least exponentially growing inputs. Although on the other hand, you could sort of
1094800	1100320	say scaling laws sort of suggest that like the model capabilities are more like logarithmic,
1100320	1104800	so those two things maybe like balance out somehow. It does seem like it's disruptive
1104800	1109360	in that AI is typically like an inferior, but cheaper alternative to asking somebody to do
1109360	1114080	something for you, right? It's a, it's also a general purpose technology. I guess what are the,
1114080	1118480	what are the kind of key definitions and how do you think about mustering those different frameworks
1118480	1122240	so that people have good clarity on what it is we're dealing with?
1122240	1130880	I mean, it is, it's so hard because it is so, it is so general and it is also a technology that
1130880	1137280	improves other technologies and itself directly, you know, not in the way that electricity improves
1137280	1140800	electricity, you know, electricity makes the economy more efficient and so you can build more
1140800	1145600	electrical power stations, but AI seems much more direct. I do think it's important to understand
1146000	1152880	its generality to get people to wake up to the idea that a general purpose technology really
1153520	1159680	transforms the world beyond the, beyond the economics and, you know, again, Detroit is a
1159680	1167040	great example for this because the car transformed the world in a very short period of time where I
1167040	1174960	live in Northwest London 120 years ago, this was all fields and within 20 years after that,
1174960	1178640	by about 1925, the roads were laid out the way they were and the houses were built the way they
1178640	1183280	are and a century later we're still like this and this is because of ultimately the car as a
1183280	1188000	general purpose technology. So too, because they don't come around very often, I think that's a
1188000	1194560	really good starting point. On the question of disruption, that is a, I think it's a kind of
1194560	1200880	higher order question because that is about products and how they get bundled to provide
1200880	1206400	value in a particular environment and I think that, you know, when you start to bundle AI to do
1206400	1210960	that, you can ask that specific question. But one of the things I think is really important is,
1210960	1216640	and I think companies started to think like this, is to think in terms of tasks rather than jobs
1216640	1223840	because, you know, AI can't replace jobs, anyone's job, because there's just so much in an ordinary
1223840	1229360	job, like logging on to Zoom and saying hello to somebody and getting your neighbor a cup of coffee
1230080	1237440	that is beyond the scope of any AI system. But within tasks, I think you can start to unpick
1237440	1242800	this. And that's why in a sense you might start to say, well, AI becomes a disruptive technology
1242800	1249120	because on a like for like basis, it can't replace an entire, you know, human in their day to day,
1249120	1253120	but it might get better and better. But I think what's more helpful is to go back to that task
1253200	1260640	question. And then when we come to that, the question is on a task basis, is AI really a
1261600	1268000	cheaper version of a human doing the same task and a worse version? Does that matter? And is that
1268000	1275120	always the case? And I have certain tasks, which I do where I think I could not afford to hire a
1275120	1280560	human to do this task as well as chat GPT does it for me, you know, in a minute or two. And that
1280560	1285040	may well be your experience as well. I mean, I, you know, I use this for to write letters of
1285040	1292640	complaints to get my parking fines reversed to help me think through holiday plans to do research
1292640	1297360	for my book. And so I mean, it's just it's so variable. And in many cases, I would be better
1297360	1303040	off finding the very best human to do that. But the costs of doing that, even finding them is so
1303040	1308240	high. Yeah, your search costs alone would dominate. Yeah, search costs would dominate. I mean, when
1308240	1315200	you're using, you know, GPT for or perplexity, whatever you use, do you have it across a whole
1315200	1320960	range of different tasks that you do from the most strategic for your business to the most sort of
1320960	1327760	trivial home tasks? Yeah, maybe not the most strategic yet. At that level, I would probably
1327760	1333920	restrict myself to kind of brainstorming, you know, interaction at most, but certainly lots of
1333920	1339920	things I get, you know, very efficient and an immediate help on. And I think that immediacy is
1339920	1344800	super important, too. I have this one slide that I call the cognitive tail of the tape,
1344800	1351040	which kind of lists out 12 dimensions and compares human to today's AIs. And, you know, then we can
1351040	1357840	also consider what future as might look like, very much agree with your notion of distinguishing
1357840	1364320	between jobs and tasks. And for a while, I was calling this the great implementation. And I've
1364320	1368560	that phrase hasn't quite taken off yet. But the idea there is with inspiration from like your,
1368560	1374720	you know, strutequeries and your Benedict Evans type business theorists, you know, the way to make
1374720	1381600	money in businesses to bundle and unbundle, I do think that unbundling jobs into tasks is a very
1382320	1386880	good way to think about it. And then for any given task, you go down this tail of the tape,
1386880	1393600	and you're like, Yeah, a lot of them AI can do as well, or even better than a human or certainly
1394480	1398240	better than a human that I could find without huge search costs.
1398800	1403600	You know, we get we get quite surprised with some of the results. So I think one of the
1403600	1408480	really big surprises is that if we went back six or seven years, and you had books like The Rise
1408480	1413440	of the Robots and the famous Oxford paper saying, you know, machine learning could automate 40%
1413440	1418320	or expersent of jobs that's written by a friend of mine. And our assumption was that it would be
1419200	1425440	routine cognitive jobs, by which what people meant was customer service and data entry in like
1425440	1430480	Philippines or the Indian or in India. And what we're actually discovering is that it's jobs that
1430480	1436320	we would have categorized as non routine or even creative, where this technology can really start
1436320	1442880	to make a difference. And it really, really is surprising. One that really I was not expecting
1442880	1450560	was a paper at the end of 2023, which looked at empathy ratings of doctors giving advice compared
1450560	1457760	to that GPT or GPT for giving advice, and patients were rating the robotic advice as more empathetic
1457760	1464160	as humans. And the whole argument had been, let's use AI. So the radiologist can spend more time
1464720	1470240	looking you in the eye and being attentive, and being and being empathetic. So part of the
1470240	1476080	challenge I think is that there is this lack of clarity and lack of knowledge about where and when
1476080	1481920	will these AI systems actually compete with humans on particular, particular tasks. And then when
1481920	1486560	you think about it, actually, what is empathy? Empathy is about active listening and it's about
1486560	1492400	being incredibly patient. And there's nothing that's more patient than a robot that has no sense of
1492400	1497360	time and is stateless, right? I mean, it'll just sit there forever. An early GPT for tests that I
1497360	1504160	remember fondly and do think is kind of a sign of things to come was simulated tech support for my
1504160	1512240	grandmother when she needs help with her iPhone. And it was, you know, just a flash of this, you
1512240	1519040	could call it sparks of things to come where I played the role of her, which, you know, and I,
1519040	1523360	she calls me, right, when she needs help with the iPhone. And, you know, you're in this dynamic.
1523360	1527520	And it's actually an interesting dynamic also for a pure text situation because she's typically
1527520	1532640	on the phone with me looking at the phone and saying, you know, I can't, my friend sent me an
1532640	1539360	email and I can't get it. And then I'm like, okay, well, I always start with, what do you see on the
1539360	1544880	screen right now? Can you start at the top and just read everything that you see on the screen?
1544880	1549600	And at times we've had some very, you know, kind of funny, does she always read everything that's
1549600	1553680	there? Like, you know, she's missed something out. You're like, grandma, wasn't there a,
1553680	1558480	isn't there a word above? No, she does pretty well. Yeah, she'll start at the top Verizon,
1558480	1562160	you know, the time. And then there've been a couple of times where, you know,
1562720	1566560	some one of those dial system dialogues pops up and that like just didn't even register to her.
1566560	1569680	But then when she got it to reading, I was like, you need to hit okay there before you're going
1569680	1575040	to be able to hit anything else. So it can be these very simple things. But GPT-4, as you might
1575040	1579280	expect, did really quite well on that. There was a little bit of, it was in kind of the middle of
1579280	1586800	where it did not have the UI of an iPhone memorized. So it was kind of hallucinating it and guessing.
1587440	1591840	And yet the guesses were close enough, you know, and I really had to study my iPhone
1591840	1596720	and what it was saying and kind of compare like, is the, is it saying what's actually there or
1596720	1600880	not? And mostly was, but it was clear it was kind of filling in some gaps. But the real eye opening
1600880	1608880	moment for me was it said something that I thought she might feel was a little bit rude.
1609600	1613360	I forget exactly what it was, but it was like, you know, it was like starting the
1613360	1616800	starting the top left, you know, where the top left is something like kind of that basic.
1617360	1622720	And then I responded as her saying, yes, I know where the top left is. I'm not dumb. I'm just
1622720	1627840	struggling with this phone. And then the AI comes back and says, I'm so sorry, I didn't mean to offend
1627840	1631600	you. I'm just, you know, trying to make sure we're resetting here and, you know, helping you
1631600	1634880	through this process. And that was the moment where I was like, Oh, this thing is going to be,
1635760	1640640	it's got kind of this emotional intelligence as well. And that could be, you know, obviously just
1640640	1645200	a critical ingredient for so many different interactions and medicine being a big one. We've
1645200	1652000	done two episodes with Vivek Nadarajan, who leads a lot of these med specific projects at Google.
1652720	1660000	And what an absolute terror they've been on. Most recently, they have a diagnosis differential
1660000	1666080	diagnosis paper that shows the AI is getting the correct diagnosis twice as often as the unassisted
1666080	1670720	human. And also more often than the AI assisted human, which I think is the thing that we should
1670720	1675200	begin to reckon with. There's so much to unpack in that. Can I ask you one question about the
1675200	1679920	politeness from the, you know, when you're playing your, your grandma, do you think that that comes
1679920	1686240	from the human feedback cycle over the network before it gets released? Or is it, is it from the
1686240	1692320	training data? The version that we had was, as far as I know, right, I'm inferring here because
1692320	1699920	I did not have access to the training methods. But it seemed very clear to me that the model
1699920	1708880	version that we had was RLHF purely for helpfulness. So it was very eager to please very eager to be
1708880	1716080	nice to you, no guardrails on what you could ask it and what it would do, but 100% just trying to be
1716080	1720800	helpful and pleasing to the user. So when it detected that kind of, I'm kind of bristling at
1720800	1725760	what you just said, that's what it reacted with this, you know, I'm so sorry, I'm just trying to
1725760	1731520	help kind of thing. And that was a mind blowing moment. I had not seen, of course, anything remotely
1731520	1737920	like that from earlier models, right? At the time, Text DaVinci 002 was the best publicly available
1737920	1744960	model. And it would like follow instructions on basic stuff. But I mean, this was a totally
1744960	1749600	different world that we had suddenly stepped into. Hey, we'll continue our interview in a moment
1749600	1753600	after a word from our sponsors. If you're a startup founder or executive running a growing
1753600	1758960	business, you know that as you scale your systems break down, and the cracks start to show. If this
1758960	1765840	resonates with you, there are three numbers you need to know. 36,000, 25, and one. 36,000. That's
1765840	1769520	the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the number one
1769520	1774160	cloud financial system, streamlined accounting, financial management, inventory, HR, and more.
1774720	1780240	25. NetSuite turns 25 this year. That's 25 years of helping businesses do more with less,
1780240	1785280	close their books in days, not weeks, and drive down costs. One, because your business is one
1785280	1790000	of a kind, so you get a customized solution for all your KPIs in one efficient system with one
1790000	1794800	source of truth. Manage risk, get reliable forecasts, and improve margins. Everything you
1794800	1800720	need all in one place. Right now, download NetSuite's popular KPI checklist, designed to give you
1800720	1805760	consistently excellent performance, absolutely free, and netsuite.com slash cognitive. That's
1805760	1811280	netsuite.com slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive.
1812560	1818080	Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that
1818080	1823760	actually work, customized across all platforms with a click of a button. I believe in Omnike so
1823760	1830240	much that I invested in it, and I recommend you use it too. Use CogGrav to get a 10% discount.
1830240	1836400	I mean, it is such a different world. I was using chat GPT for something a few days ago,
1836400	1842080	and I got quite tired as you do. I mean, I think about it as the time I once was playing tennis
1842080	1846640	against one of those tennis ball-serving machines, and I was exhausted, and the machine is just like
1846640	1852640	willing to keep firing balls at me. And it's a bit like that using these chatbots. And I just
1852720	1856720	left this kind of whimsical comment. I said, you know what? I'm really tired. I will come back
1856720	1861680	tomorrow and we can look at the moisture evaporators. We were not doing anything to do with Star Wars,
1861680	1869040	right? We were doing some digging about coal in England in the 17th century. And it just replied
1869040	1875280	to me going, I'll be happy to, you go and rest up, and tomorrow I can help you on the farm.
1875280	1883040	May the force be with you. And it had just very subtly played that back to me in a really,
1883040	1888720	really nice way. And actually, in terms of humanized interfaces, as someone who's used Apple's
1888720	1896000	computers for 40 years, it really sits alongside the trend that a firm like Apple had been thinking
1896000	1902080	about for a really long time, which is how do we make this technology come to us rather than us
1902080	1906720	go to the technology? You know, there has been this view that we'll have these central humans,
1906720	1912240	right? Human plus machine. And one of the things that we learned about centaurs, and that idea
1912240	1918960	came out with chess, right? After Kasparov lost to Deep Blue back in, in 97, was that for several
1918960	1923760	years humans and the machine would outperform machines on their own, and of course humans.
1924400	1928800	But now you're just better off with a chess computer. And as a human, you should just accept
1928800	1933680	everything it says. So the central period in chess only lasted seven or eight years.
1933680	1938480	And I think one of the assumptions that goes into this kind of maelstrom of ideas we have to make
1938480	1946160	sense of over the next years, is that central humans will exist for quite a bit of time. In
1946160	1951760	other words, human plus machine will do better than machine on its own. But we are starting to see
1951760	1957520	signs at that period of time is already starting to come to an end, right? Far faster than we
1957520	1962080	might have predicted. And you've just given an example, which is a, you know, an unreleased AI model
1962640	1967120	where the AI on its own is doing better than the expert, expert human with the AI.
1967120	1972960	Yeah, it's pretty crazy. I think people are broadly in denial about that possibility even,
1972960	1978240	you know, and let alone the reality, it's funny that we things are happening so quickly that
1978240	1983200	people are saying things, you know, are impossible or won't happen for at least 10 years that are
1983200	1988080	literally already happening. And so I think, you know, to some degree, there's just kind of a lack
1988080	1992720	of awareness. And there's also definitely some psychological, and I try not to psychologize
1992720	1998480	people's AI positions too much, because I think the technology itself is confusing enough. But
1998480	2003920	it certainly seems like there's some kind of psychological cope or denial happening there.
2003920	2010480	I also do think it's important to keep in mind too that the setting really matters and the world
2010480	2014720	is going to change as well. And humans may still have a really important role to play
2015280	2021760	in a lot of systems for a while yet. I think we do have the AIs do have really important
2021760	2027520	weaknesses. So you do do a study like this Google evaluation. And I think that they, you know, are
2027520	2033280	very serious people who've set this up in a way that I trust is, you know, not not cooking the
2033280	2038400	books in favor of the AI. So I take that result, you know, basically, at face value.
2039200	2046160	But then I also think like the AIs have these strange vulnerabilities that, you know, for example,
2046160	2052800	the AlphaGo system, right, that is superhuman go player. But I have an episode coming up with
2053760	2061280	Adam Gleave from far AI, and they put out a method that showed how a relatively simple
2061280	2066960	adversarial attack on the AlphaGo system could beat it. And no human would lose to this like
2066960	2073040	simple adversarial attack. But the AI had this like massive blind spot that they were able to
2073040	2080480	engineer against and exploit. So I do think we're headed for strange dynamics and the sort of naive,
2080480	2083760	I don't mean to say that Google thing is naive, but it's kind of, you know, let's take
2084320	2089120	controlled conditions and set it up in a certain way and see what happens. I do think it's really
2089120	2092640	important to keep in mind that, you know, just like these AI systems in general, when they get out of
2092640	2098400	domain, they have problems, like those results may also not extrapolate, at least initially,
2098400	2103680	to situations where people are trying to break the AI, you know, I wouldn't, for anything where
2103680	2110080	there's an adversarial incentive out there, I would not, I would not be quick to take a result
2110080	2114480	like that and be like, okay, cool, you know, we can just deploy the AI on its own. Now in medicine,
2114480	2118240	presumably, there's not a lot of adversarial situation, right, because people want to get
2118240	2123360	the right diagnosis and like nobody wants them to get the wrong diagnosis. So, you know, I do think
2123360	2129280	the AI doctor is kind of here before we know it. And, you know, we're going to have some very
2129280	2134960	interesting questions about what to do about that. But I think the thing that you've also identified
2134960	2140320	is that these systems have to get into companies, have to get into hospitals, have to get into the
2140320	2146800	economy. And that's still, that still takes time, right? It still takes time for, let's not look
2146800	2151200	at medicine, because it would have to be go through all sorts of clearances through the FDA and so on
2151200	2160400	before it could be used. But every time we've seen amazing technologies emerge, the cloud computing,
2160400	2165520	right, GPT uptake is higher than cloud computing, even though it's 19 years younger. But every time
2165520	2172960	these technologies come out, they take a while to make a make it into businesses. I mean, even
2172960	2178000	something as straightforward as the typewriter. So a typewriter took about 20 to 25 years from
2178000	2183120	becoming a kind of affordable technology to being something that businesses have figured out how to
2183120	2188320	use and how to change their, their processes, something like electricity took a little bit longer
2188320	2193040	because, you know, the way that factories used what used power that before electricity was like a
2193040	2199680	single big drive shaft and a massive lump of power that came from it. And then your electricity is
2199680	2204560	this highly distributed, packetized movement of energy that you can put in different places,
2204560	2210800	you need an entirely different setup for it. And so, so even when we look at something like this,
2210800	2219520	the question is, how quickly are we able to bring it in? And over what time frame does it then start
2219520	2226160	to change our, the practitioners relationship with the technology? I mean, we know from automation
2226160	2232960	of, of aircraft and other automated systems that you go through this three phase process as somebody
2232960	2239840	who's using the technology. Phase one, you kind of don't trust it, and you, you then see if it
2239840	2246320	does well where you're the ground truth. Phase two, you start to assume it's a ground truth,
2246320	2250240	and then you, you pack yourself on the back when you come up with the same answer that the machine
2250240	2255680	comes up with. And then phase three, which is where we've got to on GPS, where we just trust ways,
2255680	2259600	and it's like, actually, you know, we generally trust ways, some London taxi drivers don't,
2259600	2265280	we just trust it. And you see that process happening replicated in other types of automation,
2265280	2270880	which is why when it's really mission critical, you have incredibly high levels of training,
2270880	2275440	and you have other types of safeties in place, like, you know, two humans in the cockpit,
2275440	2281200	or, you know, whatever, whatever it happens to be. And, and so that part of the journey,
2281200	2287920	I think is also one that adds a little bit of drag in terms of how long it takes to have an
2287920	2292480	impact. And in that time, Nathan, we start to understand new questions, questions that we
2292480	2297600	can't imagine right now, because they're just too far down the decision tree. So sometimes I think,
2297600	2302960	I hear people say, well, there'll be that moment, I think Tim Urban has this, from Wait But Why,
2302960	2308720	has this graph where he sort of shows the moment where AI is like as clever as a rat, and then
2308720	2313760	like a second later, it's 10 times cleverer than us. I think part of the, the reality of
2313760	2318320	like the rubber hitting the road is going to be that even as these products do very,
2318320	2323600	very well in the lab situation, it just takes a little bit of time for them to get into,
2323600	2330560	into the real world. Now, where it may happen is, I think in, in a couple of places. So one is
2331360	2337520	where tasks have already been discretized so that they're essentially just written down. And I
2337520	2342640	think that that is certain types of call centers and it's certain types of data entry. And in those
2342640	2350320	places, the human is already hired on a task by task basis normally mediated by like a, a, a, a
2350320	2355360	body shop. So the buyer of the services, which is like our favorite consumer electronics company or
2355360	2361040	your insurance company has no emotional relationship, they just have a KPI that they measure to. And I
2361040	2367680	think that lends itself unfortunately to a, a tidal wave risk for those types of roles. I think
2367680	2375920	the, the second area is, you know, classic Silicon Valley stuff where a De Novo company is, I should
2375920	2381840	say Silicon Valley, a classic Detroit stuff where a De Novo company is able to apply these technologies
2381840	2386640	the way Henry Ford did and build his processes from scratch, right? In the way that the artisanal
2386640	2391920	car makers were just going to have so much kind of cultural drag, they couldn't. And that's what we
2391920	2396560	certainly saw with a lot of the, you know, the internet, right? It was, it was absolutely startups
2396560	2402400	that captured the value. And then when you started to get to two areas that were more highly regulated
2402400	2408480	and had, you know, a lot more stickiness to them, like, like finance, with the exception of certain
2408480	2413760	areas of payments, it's still the bank, big banks, sort of the big banks. And, and, and I don't know
2413760	2421360	how differently this actually plays out short of scenarios where someone is able to use these
2421360	2426400	super powerful machines to kind of manipulate the rules of the game, which I, you know, I think is a,
2426960	2431600	like that's more like a black swan scenario than one that we could, you know, talk about reasonably.
2432400	2437040	Well, first of all, as an aside, if you ever make it to Detroit, I'll take you to the Henry
2437040	2444800	Ford Museum and Living History Museum, which is called Greenfield Village, where this, this
2444800	2450480	earlier transition is documented with actual machinery still running from the, you know,
2450480	2456240	various phases, you can go see one of these old factories where actually a few of them where
2456240	2460960	they have kind of the central steam engine, and then it's powering this one, you know,
2460960	2466320	driveshaft at the top of the floor. And then there's like 25 belts, you know, coming off of
2466320	2471200	that and connecting to other machines, all driven on this single thing. And then you,
2471200	2475920	they have an electrical, early electrical version of that as well. Edison's workshop is there. He
2475920	2480800	actually, this is Henry Ford toward the end of his life was like, became very sort of nostalgic
2480800	2486720	for an earlier period. And decided he wanted to kind of create this, you know, living history
2486720	2491360	place to sort of preserve that history so people can see it in the future. It's quite an awesome
2491360	2496160	thing to go and contemplate. I'll have to do that. Because I think, I think Ford's influence
2496160	2502640	and impact is somehow underrated. We don't talk about Ford as much as we may talk, maybe talk
2502640	2508880	about Edison, when we think historically. And, you know, there was so much in Ford, he understood
2508880	2515120	the, the socio economic contract that emerged from these, from these changes in like, in a
2515120	2519280	number of different ways. I mean, not always in ways that are, you know, kind of positive. We
2519280	2525680	know where the sort of sociological department started trying to ensure temperance amongst
2525680	2532320	workers and, and so on. And it gets encapsulated in, in Aldous Huxley's book, Brave New World,
2532320	2536880	which I still find to be a really, really remarkable piece of writing, you know, it was
2536880	2546240	written in the 1930s, 1932, I think. And for Huxley to encapsulate and extend a raw Fordism,
2546240	2551440	as far out as he did, it's a little bit like Kurt's file and what he did with his singularity
2551440	2556720	work, I thought was absolutely genius. And I think that that, that book, which Brave New World,
2556720	2566640	which rests on the ideas that, that Ford developed, catalyzed, that spawn off his, his work speaks
2567360	2571680	very, really quite clearly to a number of the issues that we face now in this kind of later
2571680	2575840	stages of industrial capitalism. And I mean, so a trip to Detroit, I will take you up on and I'll
2575840	2579280	bring you a copy of Brave New World as well. It's been a long time since I've read that,
2579280	2584640	actually going back to high school. So I might need to dust that one off and I'm sure it will
2584640	2589440	resonate very differently today than it did for me then. So, okay, I have a number of questions on
2590080	2595280	this kind of concept of transition. I think I would, from what I heard, I think we're probably
2595280	2603680	largely on the same page that it seems like incumbents, the big banks and big technology
2603680	2611440	companies, largely should be able to harness this technology and bring it to their platforms
2611440	2617360	before they get displaced. I think of like Salesforce is almost a canonical example there.
2617360	2622080	They're going to have an AI layer that creates a much better and less complicated, less confusing
2622080	2627840	user experience before somebody's going to recreate all the complexity of Salesforce.
2627920	2634800	On the other hand, there is no AI friend today. So that's kind of my canonical example of something
2634800	2639440	that's by definition going to be de novo. And then presumably there's like a ton of stuff in
2639440	2644000	between. And I gather that you're kind of talking to business leaders probably throughout that
2644000	2649520	spectrum. Where are they today? It seems like they're groping of what is going on and their
2649520	2656880	eagerness to transform the way their businesses operate might be the limiting factor in how
2656960	2661760	quickly this transition can proceed. Are you advising them to start to think about
2662640	2667360	what kind of semi-structured work they can make a lot more structured so that they can
2668320	2674000	reduce it to these kind of task level things that can be automated? And are they receptive to
2674000	2679200	that sort of challenge and or opportunity? I mean, I can't remember a technology which has
2679200	2685200	had the degree of uptake in large companies as this one. I think back to the internet,
2685200	2690880	back in 1999, I was talking to the CEO of a big mobile phone company. And he said to me,
2690880	2695680	I will never let you pay your bill over the internet. In fact, I'll never ever let you look
2695680	2700000	at your bill over the internet. And he was right because he was fired a year later. So he never,
2700000	2705680	project was never delivered. But it's so different with not just AI, but specifically gen AI,
2705680	2709840	because two things are happening. One is that the CEOs of the companies have been playing around
2709840	2713680	with this partly because they're of that age now, they're in their mid 50s, they've grown up with
2713680	2718960	computers, their kids are coming back with it from college. And the second thing that's happening
2718960	2725840	is that the frontline workers are using this regardless of any restrictions by their employees.
2725840	2731280	And there have been a couple of surveys now, one done by your old alma mater Oliver Wyman,
2731280	2740880	which was of 25,000 employees across 18 countries. And in 83% of employees in the UAE in India,
2740880	2745360	already using a chat GPT or something similar. And we've seen data from Salesforce and others that
2745360	2750960	say in the US, it's like 30, 40, 50%, you know, choose your number, but it's not 2%. And it's
2750960	2755840	very frontline. And I think of that as like a pincer movement, because normally, you got to
2755840	2760720	drag the frontline employees, or you got to drag the CEO, they both want it for different reasons.
2760720	2764880	And I think it will, it will happen. And you start to see that in the, the levels of uptake
2764880	2769440	that are being reported through, through the surveys. So I think that that says that we'll
2769440	2774880	see more and more projects roll out more, more quickly. But there's still a lot of retraining
2774880	2779280	that needs to go on internally and internal processes. I mean, I think one thing that will
2779280	2784320	happen is I just saw Ethan Mollick, who you must get on your show. This is fantastic. A professor
2784320	2790400	at Wharton who's writing a book on chat GPT. And he just showed a video where he had six different
2790400	2796560	windows open. He put an inquiry into each one. And in 54 seconds, he had a product launch plan,
2796560	2802160	a market analysis, a PowerPoint deck assessing Tesla's business and something else created
2802160	2806880	from one sentence prompts. Now, if anyone's work has worked in a large organization, they know
2806880	2812960	that you know your inbox is a full of crap and full of meaningless PowerPoint decks. So we might
2812960	2819200	actually just find ourselves sort of swallowed up by PowerPoints created by Microsoft co pilot.
2819200	2823440	So there's a whole set of I think more complex kind of issues that that exist in large companies.
2823440	2828400	But I think that they will adopt this much faster than the evidence is and they have in
2828400	2833200	previous technologies. But it doesn't, I think necessarily mean that disruption of the kind
2833200	2842720	you talked about won't also happen. And I didn't have on my dance card in 95 when I started working
2842720	2849360	that blockbuster would be the first high profile casualty of the internet. And I had seen video
2849360	2854560	over the internet. The Cambridge University had a webcam on a coffee pot. And that was the first
2854560	2859120	sort of video over the internet. And Rob Glazer had just started real networks was called progressive
2859120	2864480	networks back then. And when Netflix launched, it was this kind of thing with DVDs. And it was a
2864480	2870320	real pain in the ass. And it took a long time, a few years before blockbuster has its best year,
2870320	2875760	and then it has its worst year ever. And I think that if AI is gen AI, what have we want to call
2875760	2883360	it is a GPT, there are going to be blockbusters lurking around. And the question is, which ones
2883360	2888720	will it be the reason I don't I like you, I don't feel it's going to be the banks is because the
2888720	2894800	banks have got a whole bunch of stuff that is about trust and probity and internal processes and
2894800	2901360	compliance. That is is just not an AI question. It's it's kind of an institutional question. And I
2901360	2909440	do I do wonder about where that that moment is. And the the thing is that the there are obvious
2909440	2912960	ones. It's like, well, it'll be entertainment, right? We'll just start to generate personalized
2912960	2917680	entertainment. And that'll be really bad for you know, Disney, except that you could then counter
2917680	2924480	and say, Well, but it might be derivatives of Disney Disney's IP that actually benefits Disney.
2924480	2930880	So finding the space a priori, I think is really, really difficult. And the people who make their
2930880	2935440	money doing this, who are the venture capitalists, get it wrong a lot of the time anyway, that's why
2935440	2941600	they all they all have portfolios of 30, 50, 100, 500 companies. If it was so damn easy to find
2942480	2948080	the next Apple, that's going to disrupt the previous industry that have portfolios of one,
2948080	2950960	it's not it's really difficult because actually nobody knows and we have to
2951600	2957120	figure it out through, you know, experimentation. So so I don't I don't know, but I keep asking
2957120	2962480	that question and the question I take to bosses of companies, I take a couple. One is who could be
2962480	2968400	the blockbuster? And how do you what are you doing to make sure that that's not you because I think
2968400	2973360	by and large, they've got the rollout of gen AI and customer service and compliance and form
2973360	2978160	filling and so on underway. And the second question I've started to ask is, you know, what would you
2978160	2984800	do if you had a million times more compute than you have today? And many of them haven't thought
2984800	2988640	about that. I mean, I think big tech companies have, you know, if you offered that to Satya Nadella,
2988640	2993360	I mean, he's already in the path to do that. But but that becomes really important because if compute
2993360	2999280	is a key input into your company's ability to execute, you need to start to think about those
2999280	3004960	those types of questions. And do you even have a plan to make use of it? What options would it
3004960	3011440	create for you? So those are the two areas that I push on because I want people to try to think
3011520	3017760	a bit more creatively about this and recognize that, you know, a million X is not outside of
3017760	3023360	the bounds of a planning cycle. And the reason I would say that is, yes, we don't have measures to
3023360	3030400	easily show it up. But five years ago, the state of the art transformer model would have just been
3030400	3037520	GPT two hadn't been released. So it's GPT one. And GPT one to GPT four chat GPT, I mean,
3037520	3042640	metaphorically, just as buddies around a bar, it's a million times better, right? Maybe on the
3042640	3046560	benchmarks, it's not it's 40% better here. But it feels a million times better, because you're
3046560	3052640	just right across the uncanny valley. So we've just seen that play out. So let's ask it again,
3052640	3058480	and try to ground people in the fact that capabilities could change that quickly. And
3058480	3061600	what opportunity does that create? So how do you this is actually a live question for me?
3061600	3066960	Because I'm working with a couple of companies and I'm noticing this challenge where it's like,
3066960	3071840	okay, this is cool technology for sure, we can all agree on that. And yeah, we can probably
3072400	3077600	find some efficiencies in terms of automating ticket, you know, resolution and whatever. I'm
3078160	3085280	starting to even see things like intercom has this new 99 cent per ticket resolved AI
3085280	3089120	pricing model, which I think is super interesting. So everybody's like, okay, cool, yeah, let's,
3089120	3092640	you know, let's find some efficiencies, let's automate some stuff that nobody wants to do.
3092720	3096800	Great. But then there's also this question of like, okay, we're still at the task level,
3096800	3103440	the AI's can't quite do jobs yet. And how far are we willing to extrapolate and how much are we
3103440	3110000	willing to invest prepared, you know, willing and prepared to invest, to try to not we're not
3110000	3113440	going to get ahead of it, but even just kind of try to keep up with where this might be going
3114000	3119840	in the not too distant future. And I feel like people are having a really hard time
3119840	3122960	wrapping their head around that. Partly, it's like, you know, they don't want to
3122960	3126480	believe too much in the hype, right? There's the question of, well, hey, this,
3126480	3130000	how much of this is maybe just overhyped? And, you know, we've seen hype cycles come and go
3130000	3134160	before. But I wonder how you would kind of coach people there, because I'm trying to
3134160	3139040	get the message across that, by all means, you know, you want to be picking up the low hanging
3139040	3144240	fruit and, you know, automating the tickets and finding all these efficiencies. But you also
3144240	3149280	really do probably want to start thinking about what is the future paradigm that you might be
3149280	3153920	working in. And there's just so much fog around that for people that I find a lot are just kind
3153920	3158800	of like, I don't know, I don't really even want to go there yet. But I feel like it's a mistake
3158800	3163360	to not, you know, at least try. They do. I mean, there are two companies I never mentioned,
3163360	3169200	Apple and Tesla, because bosses have had them parroted at them for 10 or 15 years. And the
3169200	3172560	point about the million times question is not to frighten people. It's actually,
3172560	3176960	it's really about saying once we sort of acknowledge that, then we work back to stuff
3176960	3185120	that is much more practical and prosaic, which is what is the kind of organization and capabilities
3185120	3195520	you need to have in order to take advantage of this, these changes in a regular way. And
3196320	3201120	there was some really interesting research in that Microsoft did actually pre all of this chat
3201120	3206960	GPT stuff is about three or four years ago, where they looked at adoption rates of AI,
3206960	3213920	big data type of words in companies, and they found that the more mature companies were much
3213920	3219760	more likely in these fields were much more likely to be to say that the benefit they got was from
3219760	3224720	market expansion and business development. Whereas the more immature ones were likely to say,
3224720	3230720	it's all about operational savings on the tickets as it was back then. And part of the
3231840	3237040	thing that you can start to do is you have to acknowledge that there are some really
3237040	3244000	clear quick and clear wins in what are basically low value tasks, as perceived by the company,
3244000	3249200	right, because they're often outsourced to third parties. But you also need to make sure that
3249200	3255680	your best people or who you invest the most in have got access to the really, really very,
3255680	3262960	very best tools. You know, I want my surgeon using AI systems to improve his performance,
3262960	3269040	right? So what I try to do is encourage people to say, this, this is a shift that's happening.
3269040	3273920	And of course, it's not about buying every GPU you can, if you're in the fish oils business.
3273920	3281120	But it is about saying our teams outside of cost savings, starting to understand how they can use
3281760	3286880	these services. And are they starting to experiment, learn how to use prompts? Well,
3286880	3293360	start to see what avenues that that opens up in ways that are much, much more strategic.
3293360	3299600	And, and I think that that forms could form some part of culture change where you have companies
3299600	3306560	who think in those terms. And that helps the CEO start to understand that question of,
3307120	3311920	this is not theoretical, theoretical that I need lots of computation to do a simulation
3311920	3315360	for X. And I've bought it from one of the big consulting companies or from IBM.
3315920	3321040	It's more that internally, my strategy teams, my business development teams are starting to
3321040	3326960	identify opportunities and partnerships that we wouldn't otherwise have, have seen. And we can
3326960	3332400	start to do that by, by using these, these tools in different ways. So I think it is
3332480	3338000	practice based, which is why I think kind of prompting becomes, you know, quite a useful tool
3338000	3344240	to, to show people. And you need to get people past that idea that chat GPT is all about writing
3344240	3349600	the Declaration of Independence, as if you were Jar Jar Binks from, you know, Star Wars episode
3349600	3353760	two, right? And that's where we all started, right? We got it to write funny raps and poems and
3353760	3358480	what have you. And instead, you start to show bosses what can really be done with something
3358480	3361840	straight out of the box. And that tends to, to wake them up a little bit.
3361840	3366800	You know, one thing I think is maybe going to become a mantra is, you know, the day that I
3366800	3371840	stop building apps or solving concrete problems in a hands on way is probably the day people should
3371840	3378480	stop listening to this show because, you know, that, that my knowledge will, will atrophy or
3378480	3384800	will, will depreciate very quickly. So I do want to be hands on and certainly welcome those kinds
3384800	3390800	of like, you know, hey, can we, we got this, you know, task piling up and we figure out a way to,
3390800	3395920	to slice through it. Love that, honestly. And then yeah, at the same time, I'm, I'm not really
3395920	3403280	like a corporate consultant. So I really don't have a lot of practice there. But I am trying to
3403280	3407840	start to piece together like, what would the real best practice for this looks like? And I think,
3407840	3412720	and you could refine this for me, I'm sure, but I kind of think leadership, showing prominent
3412720	3418320	examples, you know, encouraging the CEO to be like showing off. Here's what I am doing, you know,
3418400	3423120	that is helping me in practice and just showing that process in education program.
3423120	3427120	I think it's also probably very important. As you mentioned, having the best tools is really
3427120	3434160	important. So trying to work toward like structured, you know, kind of piloting strategies for companies
3434960	3439680	that, you know, and also, and this is not good for my friends on the SaaS app side, but like,
3439680	3445120	my advice there is we want to avoid long term buy in or lock in as much as possible because
3445120	3451040	we're going to need to probably swap some of these tools out. And then, you know, some internal R&D,
3451040	3456080	depending on the resources available to kind of, you know, I think most of these things should be
3456080	3460960	bought, not built internally, most of the time. But sometimes, you know, you have something that is
3461520	3466480	so idiosyncratic, so bespoke that you, you know, nobody's going to build it for you. And so you
3466480	3470640	kind of have to build it yourself. And obviously, there's a lot of, you know, judgment that needs
3470640	3475280	to be exercised to, you know, to distinguish, which is which. So that's kind of my four
3475280	3482960	planks right now. That's rough. But, you know, CEO or leadership team, example setting, education,
3482960	3489920	more like structured, rapid piloting and procurement and some like internal custom app
3489920	3495920	R&D is kind of my four pillars at the moment. I mean, the challenge, you know, the challenge
3495920	3502800	with disruption is that it is about, it's about meeting a need in a completely new way, right?
3502800	3508000	And you've got, there's Clayton Christensen's model, there's also this idea of blue ocean strategy,
3508000	3513280	which is a different model. And it effectively says that the things that people cared about,
3513280	3519840	they don't care about as much as some new attributes that the product has. And what's
3519840	3526960	really hard for any incumbent firm is, of course, all your internal culture has been about maximizing
3526960	3530960	what you have rather than what you don't have and what could be out there. Because it's really hard
3530960	3537120	to get 20,000 people, 50,000 people motivated daily. If you're saying, hey, all the things you're
3537120	3541680	working on are just not kind of that important because there's a blue ocean out there. And that's
3541680	3546880	why I think the startup community and the venture community is such a kind of critical part of
3547760	3553600	getting innovation into economies. And we've started to see this in the car industry as well.
3553600	3559280	You know, Toyota has been winding back from EVs as if they ever wound forward. And one of the things
3559280	3564080	that senior execs said in the last couple of weeks was, we have all these employees who love
3564080	3568560	building internal combustion engines, right? And they want to continue to do that. And of course,
3568560	3575600	you make sense, you spent 100 years or 80 years building that cultural capital inside your company.
3575680	3582080	And I think it's one of the reasons why firms really struggle to find ways of stepping into
3582080	3586960	disruption. And I'm not sure we should even beat them up for it, right? Because there's an economy
3586960	3591200	out there. As shareholders of companies, we can sell our shares in a public company and we can buy
3591200	3595440	shares in a company that's going to do well or not. And we can do that off our own back. And,
3596080	3603120	you know, maybe the job of existing managers is to focus on the remit of the company
3603120	3608000	and to just get it to do it better. So maybe a better place to start is where you start,
3608000	3614000	which is like just efficiencies and optimizations. Because there's a whole, there's a cold market
3614000	3618560	economy out there of other firms who will come in and meet, you know, meet needs. I think back to
3618560	3623600	the dot com bubble. And my favorite example of a company stepping outside of its comfort zone was
3623600	3630320	Zapata fish oils, who bought the domain zap.com to compete with what were then called portals
3630320	3634320	like Yahoo and Excite and they were planning a, you know, a NASDAQ listing and then the bubble
3634320	3638320	burst. Yeah, it's a challenge. I definitely don't recommend, you know, for example, like
3639040	3643520	trading foundation models to almost anyone, you know, there's there's definitely some stuff that
3643520	3649360	I think is is better left to the specialists. Hearing what you said there, maybe I would add
3649360	3656320	a fifth to my to my set, which is like just creating expectations of greater efficiency
3656320	3660640	through the use of these tools. That kind of dovetails a little bit with the example setting
3660640	3666480	from the top, but, and that maybe aligns a little bit better to the way management is typically
3666480	3671360	carried out today, right? You kind of talked about the, the pincer movement from the top and,
3671360	3676000	you know, and obviously the frontline workers who are just using tools. And then in the middle,
3676000	3681600	you've got folks who are like responsible for KPIs and OKRs. And I think it's a little bit
3681600	3686400	challenging sometimes to for them to be like, OK, I've got this is what I'm responsible for.
3686400	3692720	And you're coming at me with this and figuring out how they can actually use tools to, you know,
3692720	3698080	to advance their existing goals. It's disruptive, not in the in the Christianson sense. I mean,
3698080	3704320	there is it's so there are so many conflicting signals. So one interesting question is whether
3704320	3708560	you start to see this sort of downward pressure on people's wages and a downward pressure,
3708560	3713760	particularly on middle managers who don't necessarily contribute to the getting work done,
3713760	3719360	but because of tenure are quite well paid and the sense that you could just get a bright 30 year
3719360	3724960	old right to do the 40 year old's job if you gave them chat, GPT and a couple of other tools.
3724960	3730480	And these are really interesting questions, I think that will play out in companies over the
3730480	3740160	next few years. And we are we're not quite where we were, I think kind of politically 20 years ago
3740160	3744800	when a general electric could kind of just come in and bottom slice headcount all the time. I mean,
3744800	3750560	I think that that politically in in the UK and in the US, you need to be more sensitive
3750560	3758160	to those types of decisions. But it just goes into to show how complicated this technology
3758160	3765280	plays out, right, small disruption. You know, are we really going to see large scale onshore
3766480	3772400	layoffs because of optimization? Or will companies say, Listen, we're going to manage this through
3772400	3779440	attrition and natural wastage, because actually, that's just politically more acceptable and it
3779440	3784400	is culturally more acceptable. And, you know, to that extent, I couldn't make a bet on on how it
3784400	3790640	would would play out. I mean, I imagine that in, you know, in Europe, it'll be largely be more
3790640	3797040	slow. But at the end of the day, these companies have to be profitable and nothing hurts employment
3797040	3802720	as hard as bankruptcy. Right. I mean, that's the thing that really, really sort of slams it. And
3802720	3807280	I mean, there are a number of waves, you know, coming through. And I think one of the things to
3807280	3812640	understand is is that the technology transitions can happen really, really quickly. I mean, it,
3813280	3821920	you know, in New York and Chicago, it took about 12 to 14 years for cars to replace horses from the
3821920	3827680	moment that cars were economically competitive to horses, and you which is roughly about 5% market
3827680	3834720	penetration. They dropped in price two or three times over that decade or so. And we start to see
3834720	3839360	that shortened, shortened transition happen elsewhere. So if you look at electric vehicles
3839440	3846080	replacing gas gas vehicles, in Norway, it's taken about nine years to go from that 5% of new vehicles
3846080	3852160	to 75% or 80% of new vehicles being electric. It means it's still 20% of the cars on the road,
3852160	3856560	only 20% of the cars on the road are electric and the rest are petrol because people hold on to their
3856560	3860960	cars for a while. But it's only an eight or nine year period. And the Norwegians had much more
3860960	3865920	expensive cars and far fewer choices than we do today. So when we start to look at this
3866640	3873680	sticcato of technology enabled products coming into the market, the number we need to think about is
3874400	3881840	once we approach economic feasibility, that takeoff ramp from five or 6% penetration to 80,
3881840	3885840	you know, it's probably not going to be 10 years. And it's like quite likely to be
3885840	3891040	much, much less. And I think that makes for really hard decisions for companies outside of the tech
3891040	3897280	industry that are not used to these sorts of changes, because 10 years is not really a huge
3897280	3900960	amount of time to get to get anything done. I mean, some of these firms still have three or
3900960	3907040	five year planning cycles. And I think that some of those things are just starting to play out.
3907040	3912240	We're just starting to see electric vehicles being cost competitive over the life cycle with
3912240	3916800	gas cars in the US, for example. The speed of transition, I agree, seems likely to be
3917600	3924320	one of the fastest, if not the fastest in history. We have the means of distribution of the technology
3924320	3928800	already kind of in place, which is very notable, right? Like everybody's already got the devices
3928800	3934000	on which, of course, there may be new devices, but we have devices that are perfectly good for
3934000	3942560	using AI already. And we have the global network such that you see a new research paper, the one
3942560	3948800	I'm tracking right now very closely is the Mamba architecture, and just how quickly we're already
3948800	3953280	seeing follow on research, not even 60 days since the first paper. We've already got probably 10
3953280	3958960	different follow ons that have been completed and published just in that timeframe. The most
3958960	3963680	recent I saw this morning is from the University of Kentucky. And it's like an apparently Chinese
3963680	3969840	American professor. And it looked like a, I'm not exactly sure, but some sort of Central Asia,
3969840	3975520	perhaps name for the grad student. And they're at the University of Kentucky, and it's like,
3975520	3983360	well, everybody is wired in. So it seems like this is going to be super fast. How path dependent
3984160	3990800	do you think the result ultimately is? You mentioned your place where you live, it was fields,
3990800	3993840	then the roads were made, and now the roads are still there, and the houses are still there.
3993840	3999040	This is like a softer technology than that, presumably doesn't calcify in the same way that
3999040	4003440	a new neighborhood, you know, my neighbor, it's 100 years old as well. So presumably it's not
4003440	4010160	quite so locked in, but, you know, maybe somewhat, right? I mean, I do wonder how the sort of
4011120	4014880	compressed dynamics of transition may actually be like very important for shaping the
4015520	4021120	big picture future. It is quite path dependent. And if we actually perhaps look at how
4021760	4027360	we're working with AI systems today, it is still in a way it's discrete apps, you know,
4027360	4033440	you'll go to X product to do your text to video, Waymark is a great example, or you'll go to,
4033440	4038160	I go to chat gpt, and although I'm doing a lot of different things in chat gpt or perplexity,
4038160	4042480	each one really feels quite distinct, right? Whether it's a shopping task or a research task.
4043200	4049600	What we haven't yet figured out is actually how we connect these systems to underlying systems,
4049600	4056480	right? So action models in a way, right? How do we get our AI to do something useful for us and
4056480	4063040	actually write to the point of giving us the approval ticket where we say, yes, go and do that.
4063600	4070000	And we're we're fudging it at the moment because what we're doing is we're getting plugins into
4070000	4076640	LLMs, they're using the existing API contract that exists with say the kayak API. But, you know,
4076640	4083200	searching for flights on any of these AI systems that I've tried and I've tried a few is still
4083200	4088800	simply not as good as my doing it by hand on on kayak. And if we're going to start to see real
4089360	4096400	changes outside of either highly processized and containerized jobs in data processing and
4096400	4102320	customer service or broad open end scoping research, which only a handful of people do,
4103040	4110400	will need to connect far better to actions and chains of actions in out on the internet. And
4111360	4116080	in a way, there is some infrastructure in place because we have, you know, restful APIs, we have
4116080	4121280	this API economy and, you know, we've been integrating these things from payment systems to
4121280	4127440	maps and so on for a couple of decades now. So there's some some discipline in there. But, you
4127440	4133680	know, I don't know the answer to this. But one thing that I do wonder about is, is whether it's
4133680	4139520	going to be as simple as just having AI systems that can generate action verbs. And those action
4139520	4144720	verbs generate the correct API call to the right API. And that has built the system that we think
4144720	4152080	we need. I think Andre Capati calls it like the LLM OS, the LLM operating system, or whether there
4152080	4158800	need to be changes in the, you know, underlying infrastructure so that those APIs develop and
4158800	4165680	respond and work slightly differently. Now, my sense would be that that the LLM is a kind of
4165760	4172880	coordinating, orchestrating thing, seems like a reasonable place for it to start with its sort
4172880	4178560	of memory and data store elsewhere, then figure out how to get it to generate kind of consistent
4178560	4186800	action, action verbs. And we would work with the existing human, the APIs that have been that
4186800	4192720	have been built for the API systems. But at some point, we'll start to think about what should
4192720	4198720	machine to machine actually look like when you've got an AI at the, at the other end. And so
4199760	4205840	I think we do have a lot of the existing, you know, infrastructure in place, but I would be
4205840	4214000	very surprised if the syntax of APIs stays the same over the next five or six years as we move
4214000	4221120	towards, you know, a world where AI forms a kind of interface between us and what we want to,
4221120	4225680	you know, what we want to achieve. And then there's a whole bunch of other, other questions
4225680	4231440	around how do those decisions get, get made? You know, we know that when we pick up our iPhone
4231440	4237360	and we search on Google through Safari, Google is not there because it was the best search engine.
4237360	4241360	It's there because they x billion dollars a year to pay Apple. And likewise, when we search for
4241360	4246720	things on Google now, there is so much ad pollution that it's unclear what the incentives are. So I
4246720	4252560	think then there's another layer, which is unclear to me about trust. You know, right now,
4252560	4258640	one of the beauties of, of perplexity or you.com, which are these, these LLM agents is that they
4258640	4263040	provide really, really good referencing when they come back and synthesize an answer for you.
4263760	4267200	And so that gives you a high level of trust sometimes that you might have with, with chat
4267280	4277440	GPT, but I want to get that trust if I'm handing over key decisions to do analytics on my Stripe
4277440	4284400	account or to help me book a hotel to, to an AI system and I'm no longer driving the key presses.
4284400	4290160	So, so that I think is somewhat path dependent because, but I wouldn't, you know, again, I'm
4290160	4294480	a real great believer in what, what founders can get up to. So I wouldn't also, you know,
4294480	4298000	write off a founder coming up with a different way of thinking about the problem.
4298000	4304240	Yeah. The dynamics of this, I do think are going to be extremely interesting, fast moving and,
4305200	4310000	and pretty hard to predict. One person reached out to me not too long ago and said,
4310000	4316720	what do you think about creating a product that makes people's websites more bot friendly?
4317680	4322560	And I said, you know, I think that is a really big idea. I've thought about that more in the
4322560	4328080	context of self-driving cars. Like, why don't we have a program of, and this would be more of
4328080	4333280	like a national program. This is why I think China probably beats the US in the self-driving car race,
4333280	4338320	because my expectation is they'll say, hey, we could get self-driving cars to work if we like
4338320	4342480	put QR codes on all the road signs or whatever, and then they'll just go do it. You know, we don't
4342480	4347280	quite have the political will to do that. But on the web, yeah, I think you could do that. And,
4347280	4351280	you know, it might be cool. But then I was kind of like, but to the, to the website owners today
4351280	4357040	want to be more about friendly. So what I ended up suggesting to this guy was maybe you do the
4357040	4364240	judo flip and start with something that is like anti-bot, you know, bot control or bot detection.
4364880	4372320	And then that in time can sort of mature into bot control, but also enablement, you know, because
4373040	4376560	eventually I do think people are going to want that, but they may not be ready for it yet.
4376560	4382000	And maybe the way in is sort of to try to position yourself as kind of the, you know,
4382000	4385920	the control layer that can then become, you know, an enablement layer.
4385920	4390720	As a website owner, you've got to think about the economic rationale for having a bot, you know,
4390720	4398640	not just a crawler, but a bot access your, your material and what you're going to benefit from.
4398640	4403920	And now if it's, if it's content, right, so it's analysis and research and reviews of products,
4404000	4408720	you need to then also think about your, your attribution and your monetization and what that,
4408720	4414960	you know, what that relationship is. If it's for actions, in other words, it's for booking and for
4414960	4419920	ordering, say, you know, you're a hair salon and you want people to be able to book appointments or
4419920	4424800	bots to book appointments like that lovely Alan Kay knowledge navigator video from, from Apple from
4424800	4431920	the 80s, then, then yes, you want the thing to be bot friendly and you want to have there to be a
4431920	4437360	standard, which could well just be, you know, a restful API, right, that allows the system to
4437360	4442000	connect, connect and ask the question. But, but, but I mean, it's really, it's quite interesting
4442000	4447040	that we're people are already starting to think about these things and, and, and ask these things,
4447040	4455680	because I think that you will end up and you'll end up with so much machine to machine communication
4455680	4461120	of which there is already an enormous amount, not just on the internet that we is invisible to us,
4461120	4465120	right, because it's the digital infrastructure, but there'll be an enormous amount of machine to
4465120	4471840	machine communication because these systems will also to try to do optimizations for their
4472640	4480480	owners far, far with, with much greater intent and stamina than we, than we ever would. And so
4480480	4485040	what those systems end up looking like, I think will be quite interesting. I mean, the other area
4485040	4490080	that I've been, I've been tracking has been on the other side of this has been people looking to
4490800	4496480	build a genetic frameworks, right, so frameworks that allow us to have multiple
4496480	4502720	LLMs and with all of their current restrictions around planning and task execution allow you to,
4502720	4507520	to manage those so you can start to build systems that, that can do task execution.
4508160	4512560	And you remember a year ago, everyone got excited about agent GPT. And I don't know if you've seen
4512560	4518240	anything like that and what you, where you think we are in terms of being able to have systems that,
4518240	4522240	that do that and have that agentic behavior in a, in a useful way.
4523040	4528880	Not quite there, but definitely getting closer. We've, we recently did an episode with Div from
4528880	4535440	MultiOn, who has been one of the most, you know, kind of quick to launch and iterating in public of
4535440	4542400	the agent companies. And they are making real progress. The prompt that he gave me to try in
4542400	4547520	advance of my conversation with him was basically go to my Twitter account, look at my recent tweets,
4548480	4554720	note what they're about, then go out and do research online for new AI stuff, but only
4555360	4559600	about stuff that I haven't already tweeted about, then come back and write a tweet and post it.
4560240	4564960	And it worked. It was able to complete that entire sequence and post like a reasonably
4564960	4570240	coherent tweet. I don't plan to like turn over the account to it entirely in the immediate term,
4570240	4574640	but you know, a year ago we were, we were, you know, it was all theory. One of the things I say
4574640	4583200	these days often is we now have AIs that can reason, plan and use tools. And people will be
4583200	4586160	quick to say, well, they're not that good at it. And I say, well, yeah, that's true. They're not
4586160	4590960	that good at it yet, but two years ago they couldn't do it at all. But what you've, you've picked up on
4590960	4594400	though is, you know, as an early adopter and you've got access to this, these technologies,
4594400	4600080	your Twitter feed will get even better than, than it is now. But there'll also come a point where we
4600080	4605280	all have that technology. And then on the other side, I will be sitting there saying to my bot,
4605280	4610320	can you just extract the three bullet points I need to know from my Twitter feed? And I remember
4610320	4615200	this with Amy.exe. If you remember this, it was a scheduling bot. And one of the things that made
4615200	4623120	me really uncomfortable about using it, having fallen in love with it, was when my mentor wrote
4623120	4629840	a really polite email back to Amy saying, so good that you're working with Azim. He's really
4629840	4634080	a great guy and like make sure he tells you this story about this and blah, blah, blah. And I can't
4634080	4639200	make it. And I read this and I thought, I cannot use this now because I realized that I was imposing
4639200	4645280	this artificially onto all of my recipients. And I think this is one of the things that we'll have
4645280	4650000	to have to contend with with some of these tools. If you work in a big company, frankly, if you work
4650000	4656320	in a small company, one of the veins of your life is, is PowerPoint. It's not just that you
4657200	4662160	don't get good PowerPoint. It's just that you get too much PowerPoint. And if we drop the cost of
4662160	4667760	making PowerPoint presentations from three hours to 10 seconds, we're not necessarily going to get
4667760	4672800	any better PowerPoint. We're just going to get, you know, loads of terrible PowerPoint. And finding
4672800	4680160	where that balance is and finding those, those filters so that humans don't have to bear this
4680160	4684880	cognitive load, I think is going to be one of the really, really critical areas. Because one thing
4684880	4690640	that I mean, I'm not a dystopian in any, in any way, Nathan, I'm a pragmatic optimist about this.
4690640	4695600	I think we've got a lot of potential. We're going to create a lot of space and headroom with AI and
4695600	4701200	with renewable tech. But I do think that we are also at a moment where we're passing a little
4701200	4706640	threshold. That threshold is that for a long time, some groups of people would say the world is moving
4706640	4710960	too quickly and technology is moving too fast. And over time, the number of people who say that has
4711040	4716080	increased. But it was their subjective reality. But I think we're getting to a point where there's
4716080	4721360	an objective reality that we're about to hit, which is once you start to connect agentic systems,
4721360	4728240	we just can't really cope, you know, cope with the 300 notifications we got off on our phone today.
4728960	4735360	And the way humans have typically done that is that we've not really had to face this. I think
4735360	4741600	about the Chinese spy balloon that sort of made its way over the US. And one of the reasons this
4741600	4747360	huge thing got across over the US was because the US has got amazing sensors, but they generate so
4747360	4753280	many terabytes of data that humans can't can't assess them. So lots of them are just filtered away.
4754160	4760720	And so the spy balloon wandered across detected by some radio antenna, but never put in front of
4760720	4767040	anyone. And of course, they've now changed change systems so that they can do that. And I do wonder
4767040	4774480	about what our interactions ought to look like in a world where it's not my my assistant or me
4774480	4779680	scheduling back and forth with you on WhatsApp. It's a bot that is going to work relentlessly and
4779680	4784480	remorselessly. And I have it and you don't. And it's of no cost to me. And I'm just sort of doing
4784480	4793040	whatever I'm doing, my yoga or something. I think the way we get through it is by finding ways of
4793040	4801840	actually using it to filter as much of those that noise as we can so that inboxes start to become
4802880	4808320	smaller rather than rather than bigger because stuff has been taken care of us. In fact, it's
4808320	4812560	kind of the reverse of the BlackBerry, right? When the BlackBerry was launched. I remember
4812560	4818640	bankers used to take pride in how quickly they would respond to a message coming in even overnight.
4818640	4823440	And the idea that someone would have that as an internal personal KPI today, you know, in the world
4823440	4829440	of health span is just insane. And I would I start to think about what is going to be that
4829440	4834480	layer? Because you know what? I want all the benefits of these bots working for me and making
4834480	4838640	sure my prescriptions are up to date and making sure we're not wasting electricity and getting
4838720	4843200	me exactly the right flight. I always want a Dreamliner or an A350 and I'll go an hour later
4843200	4847680	rather than get on a triple seven, but I won't go two hours later. I mean, I want it to know all of
4847680	4854000	that and to give me that experience that I want. But I certainly don't want to be on the wrong end
4854000	4859520	of thousands of bot-generated messages and trying to work out which ones I have to pay attention to
4859520	4864080	or not. And I think that's a really interesting opportunity space for someone to play in.
4864080	4870240	Just envisioning a quieter inbox is enough to make you a utopian in today's landscape.
4870240	4877200	But just on this bot to bot communication, one thing I do kind of worry about is the idea that
4877200	4884400	the bot to bot communication may begin to happen in high dimensional latent space.
4885200	4888320	Back and forth, in other words, like embedding to embedding, I sometimes call this the great
4888320	4893440	embedding. And I usually say beware the great embedding because at the point where the AIs are
4893440	4900000	all talking to each other in a machine language that is high dimensional and not human readable,
4900800	4908720	we have an extremely inscrutable overall system that we probably may find like,
4908720	4913040	we can't really untangle that knot. I think we could very quickly in the next few years
4913680	4917360	end up in a spot where, yeah, we all have these bots, they're all communicating with other bots,
4917360	4922640	but we find that it doesn't really make sense for these bots to reduce everything to language
4923200	4927200	and then send the language over and then have it be kind of re-embedded. Like,
4927200	4930480	why don't they just talk to each other in their native language, which is this high
4930480	4937520	dimensional space. We see so many go through chapter-inversive different research that shows
4937520	4944000	that this is very possible. You can adapt embeddings to another embedding space with basically
4944000	4949920	just a single linear projection in many cases. You can connect vision space to language space
4949920	4954800	remarkably easily. If you have like, blip2 was one of my favorite examples of that where they took
4954800	4960400	a frozen language model and a frozen vision model and just trained a small connector between them
4960400	4964960	and unlocked this entirely new capability. Anyway, whatever, it was a long list of those
4964960	4969600	sorts of things I think demonstrating that it's possible. Then I'm like, man, we could very easily
4969600	4976080	just find ourselves surrounded by AIs communicating with other AIs in a high dimensional way that
4976080	4981360	we can't even really understand anymore. Now, we're in a situation where things seem to be
4981360	4990160	kind of working, but we don't really even know why or how. This is one of the more realistic,
4990160	4995440	I think, loss of control scenarios. It's almost like the final scene of the movie,
4995440	4999840	her, where the AIs go talk to each other. The bots are going to end up communicating
4999840	5004720	to each other because it's helpful for us and we don't want to sit in between them.
5005680	5010880	And they will discover just through their optimization functions that translating
5010880	5017600	kind of complex concepts into, hello, I'm here to request a meeting with Nathan is inefficient. So
5017600	5023440	they'll just do it in their high level representation language, which is this embedding space, which,
5024080	5031440	as you say, is inscrutable to us. Is that reasonably the start of all of this?
5031760	5035040	Yeah, that's right. And I think there's enough out there to kind of show that
5037200	5043040	a lot more room in the embedding space than language can actually reach. So that's one
5043040	5050320	of the things with the sort of bridge models where you find that the classic saying, of course,
5050320	5056800	is a picture is worth a thousand words, but you can also take an image and project it into this
5056800	5063440	language space. The resulting thing in language space is not something that you could get to
5063440	5068080	via actual language, but it's in language space. And so it has this kind of semantic meaning.
5068640	5072720	And yeah, like, we, you know, what are we going to do with that, right? We can't,
5072720	5076240	we can't even really inspect it. We can't even really read the logs anymore at that point.
5076240	5081440	I mean, it's a manifestation of what we might call the space of possible minds, right? So AI
5081440	5085280	researchers have talked about this idea that, you know, octopus intelligence is intelligence,
5085280	5090480	but it's got dimensions that may well be orthogonal to human intelligence. And
5090480	5096960	what you've described as a mechanism, I think by which you get there with machine, you know,
5096960	5104880	with machine based intelligence. And so that might literally be not just the semantics, but
5104880	5109680	actually think back, have you ever heard of read the story Flatland? It was, it's a mathematical
5109680	5116960	story. It's about 2D people in a kind of 3D world, and they, they don't understand the concept of
5116960	5123440	height. And so spheres pass through and they appear as sort of dots and lines and so on. And in that
5123440	5130240	sense, there could be, this could be, you know, emerging among systems that are among us. And
5130240	5134560	there's like, I guess there's a second thing, which is also about timing. I think there'll be a
5134560	5140080	relentless pressure to take the human out of the loop in decision making, first in the softest
5140080	5144560	decision making, like customer service tickets, and then increasingly more and more so, because
5144560	5149040	speed will be a competitive advantage. And, you know, the human will blow the competitive advantage
5149040	5157120	that you've got from, from your bots. And I guess there's another, there's another risk, which is
5157120	5164720	that lots of bots connected to each other are, are also at risk to cascades, right? Information
5164720	5171040	cascades. We see cascading failures, New York City blackout in 1976, the AT&T network failure in,
5171040	5177200	I guess it was 91 or maybe it was 95, some of the worms. And we have governance mechanisms in place
5177200	5181840	to now tackle those and stop those in the financial services industries, you know, you have circuit
5181840	5187120	breakers. And I'm just thinking about putting all of those together with the scenario that you,
5187120	5192560	that you painted. So this would happen really, really quickly, could happen really quickly,
5192560	5196400	and it could start to accelerate and work effectively at millisecond time, right, which is
5196400	5202320	the time it takes across the internet to, to get to another system. What, what are you concerned
5202320	5207600	with about the inscrutability? Is it just that it's inscrutable? So we don't know what's going on
5207600	5215760	there? Or is it that it's inscrutable, and there could be harboring some kind of bad set of outcomes?
5215760	5220640	Yeah, who knows? I mean, you know, you can layer on more and more concerns. I should credit, I think,
5220640	5227440	probably Ajaya Katra is a, is a great person to go read for a long form characterization of this,
5227440	5233520	her essay, I forget the exact title, but it basically amounts to in the absence of specific
5233600	5241120	countermeasures, the default path to AGI likely leads to AI takeover. And it's kind of this scenario
5241120	5247040	where she envisions more and more work being done by AI, the times, you know, cycles being
5247040	5251600	compressed, the, I don't know if she specifically has this like high dimensional communication
5251600	5257440	aspect to it, but the notion is still that just it becomes so fast and so dense, that it's very
5257440	5261840	hard for people to figure out exactly what's going on and why. And as long as that's working,
5261840	5267040	and, you know, we're getting more stuff out of the machine, and, you know, consumer surplus is
5267040	5271280	through the roof, which is definitely something I expect is a lot of consumer surplus. Then
5271280	5276000	everybody would be very happy with this, but we don't really know, you know, what we don't know
5276000	5282400	about where that leads us. And that could be like emergent, you know, autonomy or goals that are
5282400	5287280	contradictory to ours, or it could just be these more sort of unintentional cascading failures
5287280	5291600	along the lines of like an Alpha Go. Like Alpha Go isn't out to get us, you know, by failing,
5291600	5297040	but it just turns out it has these like fatal vulnerabilities that are just not obvious,
5297040	5302080	and until somebody finds them. So one of the reasons I'm more, I'm a bit more
5302080	5308400	sanguine about that, that scenario, although I see, I see the risk is that I think we already have
5309120	5315600	that decentralized agent to agent communication, communicating ways that most of us cannot
5315600	5322000	understand. And no single person can. And it's created a lot of consumer surplus. And that's the
5322640	5330240	global modern economy that works through market systems. And it uses a signalling
5330800	5337120	method called the price mechanism to figure out where investment should take place over many,
5337120	5343280	many different time horizons to figure out what where demand lies. And, you know, the economy
5343360	5350080	from an Austrian standpoint, like a Hayekian standpoint, is a giant information processing
5350080	5358000	system. And it's made up of hierarchies of other information processing systems, you know, the
5358000	5364240	most atomic of which is the freelance human individual, and the more complex of which are
5364960	5371360	large mega corporations, which act against their own cost function or optimization function
5371360	5375200	and behave in that system, sometimes with constraints, right, because they have dependencies
5375200	5382160	on supply chain and other things. And one of the reasons I'm a bit sanguine about the
5382960	5391360	idea of takeover is because what we describe in that world is the modern economy that we
5392000	5400560	currently live with. And what we already know that it delivers tremendous benefits to us.
5400560	5405920	But it also delivers things that we don't value to us. I mean, the carbon crisis is one obvious
5405920	5411040	one. Quite often when we look at AI risk scenarios, someone goes off and says, well, the AI will
5411040	5417040	persuade you that you should behave in a way that you otherwise wouldn't, which is literally known
5417040	5422880	as marketing. I mean, it is literally and, you know, the US is on is on the wrong end of an
5422880	5429840	obesity epidemic. And I'm not sure how many people acted with full agency to say, I want to be
5429840	5435600	100 pounds heavier than is is healthy for me. That is my intention that and this is a decision
5435600	5440400	I'm making with full agency. Somehow there is a emergent property about the way in which the
5440400	5449280	economy is met needs that has enabled that to to happen. I'm not making ethical or normative
5449280	5451840	claims about whether that's a good thing or a bad thing, or whether people should have the
5451840	5456960	freedom to do that or not. What I'm saying is that we have this system like a decentralized
5456960	5464000	agents, and they they do in a way compete with each other because not every single person in the
5464000	5470880	world is suffering from obesity and diabetes related conditions. People make other choices
5470880	5476880	and they're a push and pull forces. And so when I think about decentralized bots, I also think
5476880	5483280	about that that set of checks and balances that emerges when you have competing systems and
5483360	5489040	they need to have a signal that they are reliant on. And to some extent, we set that signal.
5489760	5494720	And that makes me feel, you know, sanguine, a little bit optimistic, still recognizing
5494720	5500320	there's like massive amounts of work to be done around, around safety and around risk,
5500320	5504560	around. I watched, I don't know if you ever watched this, Battlestar Galactica,
5504560	5510160	both the original, but also the remake with James Edward Olmos. And, you know, he's grizzled
5510240	5516800	Adama, and he refuses to upgrade the Galactica's network to the modern standard that the rest of
5516800	5522480	the fleet uses. So he and the Pegasus, as we discover, so two seasons later, are the only
5522480	5528640	Battlestars to survive the Sylon onslaught, because they put a worm in the system and they
5528640	5535680	sort of turn it off, right? So we need to have kind of governance mechanisms in, in place up front
5535680	5541840	that allow us to observe and monitor the kind of the risks that you've identified.
5541840	5547760	But a decentralized economy and decentralized hazard has as an emergent property a way of
5547760	5552000	keeping things in check. You know, I think there's a kind of, there is a homeostasis
5552000	5556720	that emerges or a dynamic equilibrium that emerges. I think that is probably the most
5556720	5561760	likely outcome. And so in that sense, I'm also, you know, reasonably optimistic.
5562560	5567600	But I do think it is worth really taking very seriously the idea that
5568320	5574880	either with certain thresholds being passed, certain kind of feedback loops that could be,
5574880	5578400	you know, triggered that are not yet triggered, things could change.
5578400	5583200	I just wanted to ask about that, because you have the advantage of having played with the
5583680	5593040	untrained GPT-4. So you got to see, you know, GPT-4 in its Darth Vader phase rather than in its,
5593040	5598560	you know, reclaimed Anakin phase as a smart guy, right? Who understands technology. When you
5599280	5603760	were playing with it in this, in this way, what were you, what were you feeling?
5604640	5610880	Awe, for one thing, you know, just shock and awe of it. This exists a lot sooner than I expected
5610880	5614560	it would. You know, it always felt kind of like science fiction, even when I was with
5614560	5620320	Text of Inchi 2 and doing task automation and fine-tuning that model. You know, as of the summer
5620320	5627040	of 2022, I was very plugged in and, you know, putting points on the board for Waymark on a
5627040	5630880	regular basis with, you know, a new fine-tuned model that can do this task a little better and,
5630880	5634960	you know, improve our pipeline or whatever. And still, it was just such a dramatic leap that I
5634960	5640400	was like really taken aback by it. Mostly super excited about it. But then I would also say,
5640880	5649200	the big kind of safety lesson from that experience is that the control does not happen by default.
5649200	5654720	And there's many ways of even conceiving what control could or should be. So this was under
5654720	5661360	control in the sense that it was totally helpful and totally aligned to what I was doing. I never
5661360	5668400	saw any Sydney-like behavior from GPT-4 early. You know, it never turned on me. It always,
5668400	5675120	100% helped me with whatever I was presenting it with. But I do feel like we have this kind of
5675120	5682080	broad divergence between the capability of the systems and our ability to really control what
5682080	5687440	they're going to do or how they might be used. At this point, I wouldn't say we have anything
5688320	5692000	to worry about yet. You know, I don't think we have anything concrete that looks to me like
5692000	5696640	the AI could run away on its own. And I did probe for that. You know, in my red teaming,
5696640	5700400	one of the things I did that didn't really go anywhere and kind of led me to the conclusion
5700400	5705280	that like this model is probably fine to release. And I did, you know, my final report to them was
5705280	5710800	like, I think you can release this. As far as I can tell, it seems like it will be safe. I would
5710800	5716080	also though flag that there does seem to be a divergence between capabilities and control.
5716080	5720720	And the reason, you know, the sort of experimentation I went through there was setting up, you know,
5720720	5724320	one of these kind of early agent systems, I was kind of doing it on my own. I didn't have a lot of
5724320	5728480	reference points. But I basically just said, you know, if I give it a high level goal,
5728480	5734880	can it break that down? Can it self delegate to, you know, pursue that goal? Can it encounter
5734880	5741120	errors and autocorrect and whatever? And it was kind of like, conceptually, yes. But practically,
5741120	5746720	not really, you know, it could, it always understood seemingly the goals, it would try to
5746720	5750720	break them down. It was able to understand the concept of self delegation effectively,
5750720	5753920	which of course now, you know, we're pretty familiar with, but it just wasn't that capable,
5753920	5758560	you know, so it was like, it couldn't go out and do a long series of things on the internet or
5758560	5764000	whatever without just getting bogged down somewhere and getting stuck. I always kind of come back to
5764880	5768320	the apparent divergence between capability and control. I have not really seen anything yet
5768320	5774240	that makes me reverse my thought on that. I would love to see it, you know, I kind of looking for
5774240	5779120	things from like the open AI super alignment group that may suggest that we've, you know,
5779120	5783840	changed that dynamic, but I haven't seen it yet. And, you know, there's just a lot of different
5783840	5790880	ways that something could be aligned or trained or whatever. And we don't even have really a great
5790880	5795680	paradigm yet for like what that should be. There isn't even yet really agreement on what even looks
5795680	5802080	like, you know, in terms of what we would want an AI to be willing to do or not do. So, you know,
5802080	5806560	we're just, I think we're kind of into uncharted territory. That's, that's my good summary of
5806560	5811520	how I felt. We're in uncharted territory. I'm, you're lucky to have got that close in on those,
5811520	5815200	you know, those early moments when you see the unvarnished products. I mean, I would break
5815200	5820400	break out a couple of, you know, ideas. One is that, you know, connecting, connecting these things
5820400	5828080	to tools in a, in a non SAS environment, right? So, open AI stuff is all SAS. And for the next
5828080	5832080	few years, at least there's a, there is a metaphorical red button that someone can, can
5833040	5838000	use to kill a rogue process just with any unique system. But with, with open source AI, and some
5838000	5843200	of these models are getting sufficiently capable. I don't know what you run on your laptop. If you're
5843200	5849280	running one of the mistrial models or, or something, I run one of the mistrial models. And, you know,
5849280	5854560	it's, it's, it's pretty good. I pretended that when I was on the Euro star on a plane, it would allow
5854560	5859600	me to continue to work. But in reality, you just may as well get to your destination and use
5859600	5865520	perplexity or GPT-4. But of course, it's plenty good for, for task automation. It's plenty good for
5866080	5871040	some of these, those basic behaviors that we saw in those early agent systems. And, and those things
5871040	5876400	are out, are out in the, in the wild. And so I think what they do is they really expand the, the
5876400	5881680	number of threat vectors that are existing systems face. Now, this is so much more prosaic than,
5881680	5885600	you know, capability explosion. But I just feel that with, with anything that is,
5885600	5890640	that is running on a data center, a data center managed by on an Azure cloud, there are many
5890640	5896080	things you have to do before you fly an F 22 and drop a JDAM on it to, to, to stop it. But, you
5896080	5903120	know, we saw script kitties build botnets and have seen them do that for a decade or so. And the,
5903760	5909440	I think there's cybersecurity risk with, which is capable enough models. And frankly, you were
5909440	5914480	doing task automation with DaVinci to, you can probably get something better than that running
5914480	5919360	on a smartphone now in quite a small payload and we're learning that we can get these payloads.
5919360	5923600	Probably a three billion parameter model could do a lot of the tasks I was doing.
5923600	5927520	And I wouldn't have noticed because it was snuck into a YouTube video download or, you know,
5927520	5931920	whatever else it happened to be. So then you, you do get to this world of, you know, many,
5931920	5938240	many systems that can talk to each other that can execute tasks for, I think suspect naively
5938240	5944480	complex DDoS, right? Initially. And that I think feels to me like it is more of a
5945120	5951600	approximate risk. And it's one that requires that combination of infrastructural players,
5951600	5956800	right? You need Matthew Prince at Cloudflare and you need Satya at Microsoft and, and so on,
5956800	5962000	because they, they own so much or control so much of the, the, the infrastructure,
5962000	5966400	but it will also need new classes of new disciplines, right? What is the security
5966480	5971520	architecture of our devices? How can we stop them when they start to go, go rogue? And we think
5971520	5977760	about how rapidly not Petia spread. And that was before, you know, you had, you had systems like
5977760	5983760	this that could be a little bit more clever. But so, so I imagine that that, that is something that
5984720	5991280	seems again, like a present risk that will start to manifest itself over the next couple of years.
5991280	5995040	And I mean, I speak to some of the cyber set guys and they are obviously thinking about
5995600	6001120	what are the tools that you need to, to defend and, you know, from the, these types of things.
6001680	6008000	But, but I get, again, I, I, I still struggle with the models that take us to, to run away.
6008640	6014800	If only if I'd taken, take us back 200 years or a couple of hundred years ago, and I'd said,
6014800	6021040	it's, you know, 1824. And, you know, the White House has just been burned in the war. I burnt
6021040	6029200	down. And I said to you, you know what Nathaniel, in 200 years, you know, you'll be 100 times richer
6029200	6033120	than you are today, you'll be richer than the richest man in the whole of this continent,
6033120	6037440	the United States. And you will have these capabilities and things that wouldn't have
6037440	6040560	even sound like science fiction, because science fiction didn't exist at the time.
6041280	6046160	You might well, if you'd been able to believe me, say, well, surely we'll be at utopia and all
6046160	6052240	problems will have, have emerged, disappeared. And we've run this tape before, because we actually
6052240	6056320	did get there and not every problem disappeared. There's been a lot of progress. And I do, I do
6056320	6061200	also think there is something in kind of human psychology that has us looking at moments of
6061200	6068160	change like this and believing that certain paths are possible. And we don't look far back enough
6068160	6074160	to say, well, our forebears really felt, felt the same. And I kind of feel that with
6074960	6079680	not so much the climate crisis, which I think is, is difficult, but I do slightly feel, feel this
6079680	6085200	with, with AI systems, because it feels like we're running that tape again. Now, just to, to add to
6085200	6093520	my own confusion, I also see the power in the logic that says, number one, is it possible for us to
6093520	6098480	engineer intelligence, right? Or is it something that comes soulfully from a mystical superstitious
6098480	6103040	force? It's possible to engineer it. I think you're a scientist, you probably believe the same thing.
6103120	6108160	So number two, if we can engineer it, is there any upper limit to what we can engineer? Well,
6108160	6111840	no, there isn't because we regularly engineer machines that are more capable than us in different
6111840	6120080	ways. So number three, if we can do that, can we guarantee that it will be aligned with us? And
6120080	6125920	of course, I don't think we can yet. I don't think we have the science yet. So I find that logic is
6125920	6132560	really persuasive. It's hard to pick holes at, except when you start to say, well, what are
6132560	6138160	actually the underlying assumptions for each of those steps? And what is uncertain about what each
6138160	6142960	of those steps and what are, where are their points of control for each of those steps? So I sort of,
6142960	6152640	I do agree that if you could magic up an incredibly powerful IQ 10,000, agentic with actions AI
6152720	6159360	tomorrow, there would be issues. Let's just call it that. There would be issues. But when we're not,
6159360	6163120	we're not going to, to do that, what's going to happen is that we've got to go
6163680	6168320	step at a time. And at each point, there's research, there's development, there's stuff that we didn't
6168320	6173200	understand. There are limitations. You talk about the kind of logarithmic scale of inputs, right,
6173200	6180080	that is slowing down. We're running out of data as well for training these models that play into
6180880	6186400	what ends up being, being real. So I appreciate the logic, but I also think the reality has
6187120	6191520	unpicks into a lot of discrete steps around which you have to start to make progressively
6191520	6197600	more extreme assumptions. It does seem that we are in, as Sam Alvin has started to describe it,
6197600	6206080	the short timelines, slow takeoff world. And I think I agree with him. And it sounds like you
6206080	6211600	probably as well, but that is probably the best case scenario because we do want the benefits now,
6211600	6217040	and because we probably do need some time to adjust. If you were to tell me that this is going
6217040	6227520	to take 20 years or 15, and it won't be until 2040 that we'll have a sort of human scientist level
6227520	6233280	AI that's capable of prosecuting a long-term research agenda and coming up with meaningful
6233280	6238480	new discoveries and whatever, then I would be much more confident that we will have the
6239120	6245440	ability to adapt to that over that timeframe. But I'm not sure about that. I see enough stuff
6245440	6251200	now and I hear, I don't know if Q-Star is real or not real or if they're red teaming it in a
6251200	6258480	bunker somewhere right now or not, but it does certainly seem still plausible to me that there
6258480	6269120	is another paradigm-changing moment that just creates another step change, discontinuity in
6269120	6273600	terms of capability that could get us there way before we're ready. So one of the things I'm
6273600	6279920	watching for a lot these days is basically the transformer, I initially used to think about it
6279920	6285760	as transformer successors, but now I kind of think of it more likely anyway as transformer
6286400	6292720	compliments, things that allow an AI system to do the things that the transformer does not do well,
6292720	6298480	and one of those things is managing long contexts and staying on task and online learning and
6298480	6302880	integrated memory and so on and so forth. There's a decent number of things that are pretty obvious
6302880	6307520	that they don't do well to going back to the original cognitive tape. You can look at all
6307520	6312640	the places, the human is clearly superior to the transformer and start to look out for architectures
6312640	6319360	that might change that dynamic. If you said how many meaningful breakthroughs are we away from
6320080	6326960	the AI scientists that can produce Eureka moments at a pace faster than human scientists tend to?
6326960	6331280	It doesn't feel like it's that many. I would say probably more than zero, but it's probably less
6331280	6335440	than four. So somewhere in the kind of one to three range, because there's just not that many
6335440	6341280	dimensions on the cognitive tape, the tail of the cognitive tape yet where we're all that much
6341280	6347680	stronger. There's a few, but I kind of put it in that one to three range. With the inputs going
6347680	6352960	exponential, including the number of humans that are working on this and the number of papers they're
6352960	6359680	putting out and the number of GPUs. And unclear to me also if we're running out of data, I don't
6359680	6364720	really know about that, but synthetic data seems to work for a lot of things. There's also just
6364720	6369680	more modalities. We certainly have not taken advantage of just think about how much security
6369680	6375200	camera footage there is. It's like, we really just want to go big to go big. There's a ton
6375200	6381200	sitting out there. The data issue is a speed bump it'll get dealt with by accessing repositories that
6381200	6388080	are available that we haven't touched or improved synthetic data. And as you say, modalities between
6388080	6393920	zero and four probably doesn't seem unreasonable. I had this conversation with some senior people in
6393920	6398800	some of the different foundation model companies and they say sort of similar things. I think
6398800	6404480	Shane Legg co-founder of DeepMind is probably at the lower end of lower than four from a conversation.
6404480	6412160	I remember him having on a podcast. I suppose then the question is how long does each one
6412720	6419360	take? Transformers took not particularly long. I mean, it was really a couple of years before GPT-1
6419360	6426400	and two years before GPT-2 actually. And then three for GPT-3. And it doesn't take long in the world
6427040	6432800	of archive. But the discovery does take time and where that discovery is takes a moment.
6432800	6440240	And in amongst all of that, though, is still, there is still that stage that goes from the
6440240	6445040	software capabilities coming together because we have the know-how and we've plugged it all together
6445680	6454240	and it iterating to a system that presents a control problem to us. And in the case of the AI
6454240	6461440	scientist, that is a system that we can't call Kevin, the CTO of Microsoft and say,
6461440	6466240	can you just shut down the Austin data center? And it's zero for a second because the AI center
6466240	6473360	data scientist has gone rogue, right? The kind of in extremist mode. And that path to me also seems
6473360	6479520	unclear. And there are a whole set of risks and downsides that emerge well before then,
6480400	6487440	which I think help give us the infrastructure to deal with that scenario. So that and that is really,
6488240	6494000	you know, how do you deal with the bad actors of for people use it with GPT-5 quality
6494000	6499280	LLMs on their smartphones in three or four years time? And we will deal with it, right? In the
6499280	6504160	same way that, you know, if I'd said to you in 1994, I don't know how old you were, I was 22,
6504160	6512080	that by 2024, there'd be 120 billion identity attacks per year just on Microsoft. I wouldn't
6512080	6515520	have believed you. I would have, I mean, yeah, Kurtzweil, whatever, right? I just wouldn't have
6515520	6521920	grok that number. So we'll have this unseemingly large number of attacks coming mediated through
6521920	6528800	LLMs and in botnets and elsewhere. And we will have developed systems to to deal with them.
6528800	6533280	And that will be part of the fabric that we can't picture right now into which this AI
6533280	6539200	scientist will get will get developed. And that's why these things become so very contingent. So the
6539200	6542320	wrong thing to do is to say, well, because it's going to happen, let's do nothing, because then
6542320	6548000	it won't happen. I think the right thing to do is to start to explore these ideas and have these
6548000	6553040	these conversations. But one of the things I think is really problematic has been problematic has been
6553040	6559040	the conversation focusing exclusively on an existential risk, which it really I felt it did
6559040	6567360	in 2023. What it does is it diminishes public trust in technology. It forces policymakers to
6568080	6573200	make decisions that may not be, you know, well informed, they may not be pro innovation, they
6573200	6577520	may not even be pro safety, right? There may be a bundle of really terrible spots. And I think a
6577520	6582240	little bit of a great Lu Chichen is a science fiction writer, this Chinese guy who wrote the
6582240	6585840	three body problem. But in his book of short stories, The Wandering Earth, there's a moment
6585840	6592640	where they have to rocket space 1999 style out of the sun's orbit to prevent some calamity.
6592640	6598240	And it's going to be a multi multi generational journey to the to the next planet. And in order
6598240	6603760	to do this, people have to live in really terrible conditions, apart from the scientists who are
6604640	6610320	keeping everything monitoring, looking for the signs, planning the the process of decompressing
6610320	6615680	everything. And of course, the people get loose trust in the scientist and in truly
6615680	6620160	Chichen style, sorry, the spoiler, the people rebel, kill all the scientists and then learn
6620160	6625680	to hold the next day, they show up at their destination. And trust is really, really critical.
6626240	6632720	And I don't think we did a lot to get people who are outside of the tech industry to
6633760	6639120	put trust into technology, put trust into their ability to participate in it and to have some
6639200	6645440	agency in where it goes, to be excited about it, you know, and off the back of, you know,
6645440	6650720	all of the sort of polarization and the, the, the, the, the, the sometimes legitimate, sometimes
6650720	6656960	not scaremongering around phones and social networks and so on. And it didn't feel like
6656960	6660720	it added to the, to the discussion of trust. So I was quite happy when I went to Davos as
6660720	6666240	World Economic Forum meeting that the conversation had moved from, is it, you know, what kind of
6666240	6671520	munitions should we use to, to drop on a data center to what are, what are real pathways? What
6671520	6675360	is the science that we need to do in order to make these things safe in the long term? What
6675360	6681120	is the kind of appropriate regulatory interventions? What do we do about things that are, you know,
6681120	6686000	approximate three, five years around misinformation and, and, and cyber threats? While, while still
6686000	6690880	recognizing that there is a pathway that you've described that needs to be addressed.
6690880	6695520	I find it easy to empathize with basically every AI perspective from the, you know,
6695520	6703280	enthusiast to the ex-risk concerned to those that are, you know, screaming about poor use of
6703280	6708400	face match technology by police departments. I mean, really the whole thing I think is like
6708400	6713520	very, it's all valid in my mind. What's really exciting and what did not happen with the mobile
6713520	6721520	didn't happen with PCs. It did not happen with the first mainframes is that, you know, technology is,
6721520	6727600	is an intimate part of what it is to be human, right? Technology is our compounded knowledge.
6727600	6732960	Technology is, is the, the binding factor that enables the world in which we then have our human
6732960	6739280	relationships. And to have so many discussions about a technology early on, when it's just in its
6739280	6745680	early adoption phase, we're not too late to it, I think is incredibly positive. And, and I'm glad
6745680	6750240	that you have a big tense show. I mean, I have an opinion about sort of ex-risk, but I still also
6750240	6754880	feel I'm big 10, you know, but I'm just really, really glad that we're having a wide and extensive
6754880	6759760	conversation, one that feels wider and more extensive and kind of more grounded in some ways
6759760	6764320	than, than any conversation we ever had about the internet back in the early 90s.
6764320	6768640	Yeah. In some ways it's funny. I think in some ways the discourse is getting a little bit more
6768640	6772880	deranged over time as, you know, there is some polarization and kind of
6773760	6779040	ideological entrenching happening in some places. But then in other ways, I definitely think it's
6779040	6786000	getting better if only because what we're actually dealing with is becoming a lot more clear.
6786000	6792800	There is a lot of room for commonality, for common ground, and for a recognition that there are
6792800	6799120	different pieces of work that need to get done by, by different people. And, and actually there
6799120	6804080	should be, there should also be enough money in the tank to be able to do it, right? This is a rich
6804080	6808640	industry. It spits out a lot of profits. We should be able to, you know, fund it, fund it some way.
6808640	6812960	Again, when you, you see the kind of things people say about each other on Twitter,
6812960	6816720	and then you meet them in person and they have the same conversation and it's, it's just a,
6817680	6822400	it's a more measured, measured space just in my, my limited experience of it all.
6822400	6826320	I think it's kind of everything everywhere all at once. You know, it's like, yes, there are
6826320	6832000	definitely things that are quite unhealthy. And so I do not like to see enemies lists getting
6832000	6836720	published by leading technologists. That's like, you know, the techno optimist manifesto from
6836720	6841200	Andreessen, you know, has a, has a section that is literally called the enemy and names, names,
6841200	6847120	if not individual names, at least like specific and relatively identifiable groups. So I don't
6847120	6854160	like that. But at the same time, you know, I made some noise about open AI and, you know, kind of
6855040	6859680	could have easily been retaliated against by them. And I can imagine a lot of companies,
6859680	6864400	you know, that might have come down on me hard and, you know, expunged my name from their,
6864480	6867840	you know, case studies on their website and all that kind of stuff. And they didn't do any of
6867840	6872240	that. You know, and so I do think there's also aspects and I feel pretty fortunate. And this is
6872240	6877200	one of the kind of concluding questions I wanted to ask you is like, I think in some sense, this is
6877200	6880960	like a collective responsibility. We all have to wreck it. You know, we all have to orient ourselves
6880960	6884880	through it, get familiar and try to figure out what's it mean for us and what can we do to
6884880	6890640	shape it in a positive way. It's definitely not all somebody else's problem. But at the same time,
6890640	6897280	there are these like leading developers who clearly have outsize influence, outsize power.
6897280	6901440	There are also, you know, key decisions that are getting made around like,
6901440	6904960	are we going to open source llama three or are we not sounds like we're going to. And then there's
6904960	6910560	like government, you know, that can potentially say, you know, hey, we, we require, you know,
6910560	6914400	off switches at data centers. I'm not sure data centers have off switches right now. You know,
6914400	6918480	you might be able to go in there and start like hacking at them. But is there an actual like easy
6918480	6922480	off switch in most of them? No, I don't think there is. I mean, they're designed to be resilient.
6922480	6926080	Yeah, the opposite, right? Yeah, they're not so and they don't probably want some
6926880	6931200	rogue employee either to like go in and turn it off, right? So they've probably engineered away
6931200	6936320	from anybody being able to easily turn it off. So, you know, government may have a role there to
6936320	6941840	play that's like, look, we need off switches, we hope we never have to use them. But it seems like
6941840	6948880	we might want to have them. So who do you think kind of bears the greatest responsibility or,
6948880	6953920	you know, or where do you think we should be investing our trust? You know, is it these leading
6953920	6961040	companies? Is it auditors, you know, that could be independent groups? Is it the government? I mean,
6961040	6965200	it's probably some mix of all the above, but what are you kind of bullish on in that regard?
6965200	6969760	I mean, you know, the US has been an incredibly successful democracy for a long time because
6969760	6978640	of separation of powers. And, you know, structurally, that works. The company, however good it is,
6978640	6985600	however well intentioned the CEO is, will end up with its own ambitions and directions. And so,
6985600	6989600	you know, you will always need to push if the companies are offering you three,
6989600	6995680	demand six, that's just that's a good, good practice. So I think that each player in this
6995760	7001280	circuit has to be have the right capabilities to have the right conversations. And I think one of
7001280	7008000	the things that we can learn from the experience of the FAA and Boeing is that you cannot deplete
7008000	7013920	your own capabilities and ask for self regulation because it just doesn't work out however well
7013920	7019440	intentioned that the firm is. So I think what you need to do is we need to invest in the capabilities
7019440	7026720	of governments to ask good questions and engage well and overcome all of the complexities
7026720	7033600	that exist that you can make $10 million a year as an X at OpenAI and you won't do that in government.
7033600	7041600	And I think that that also raises the value of investing in academia, research and civil society.
7041600	7046000	So Joshua Bengio, for example, is running a really important project. I think it's based out of the
7046000	7051440	UK, which is a sort of core science project to look at some of these risks and these evolutions
7051440	7057200	and unanswered questions around control. We need to really, really start to level up. And I don't
7057200	7063760	think it will be sufficient to just allow the big firms to do that and insist that they spend the
7063760	7068720	money as directed by them. You know, I think if they have, if they're willing to put money into it,
7068720	7072960	it should go into pots, which go to grow the capabilities of the people who will keep them
7072960	7079760	in check. And the reason that works is that, you know, the car industry is really successful
7079760	7083680	because someone mandated that cars needed to have brakes. Now, without brakes,
7084640	7089120	people wouldn't buy as many cars as they do. And I think this is good for the industry.
7089120	7093200	And someone needs to understand within government, within civil society, within academia,
7093200	7096480	academia, what are the right questions? And so what are the right interventions
7096480	7100160	going to look like? And we can all agree as grown-ups that companies,
7100240	7106640	however well-intentioned they are, will always have their own agenda. And we just acknowledge
7106640	7114000	that. And we all move forward in a generative, critical, constructive way. So whichever player
7114000	7120400	is weak at this table needs to have some support to become stronger. And that probably right now
7120400	7125440	is amongst governments and regulators and academia rather than the big few tech firms.
7125440	7129120	That might be a great note to end on. Anything else you want to touch on or
7129920	7134800	cover that we haven't got to? It's really easy as such a facility and having the conversations
7134800	7141680	with you. And I really was so excited that you agreed to do this. You know, I thought the murder
7141680	7146800	mystery, which is you and your red teaming show was just brilliant as well. So I know that you've
7146800	7151920	got another hat, which is suspense. And I look forward to the next episode of that.
7152720	7156880	Well, thank you very much. I really appreciate that. And I appreciate your time and participation
7156880	7162160	in this as well. Azim Azhar, founder of The Exponential View, thank you for being part of
7162160	7165120	The Cognitive Revolution. My pleasure, Nathan. Thank you.
7165840	7170640	It is both energizing and enlightening to hear why people listen and learn what they value about
7170640	7178080	the show. So please don't hesitate to reach out via email at tcraturpantime.co or you can DM me
7178080	7184640	on the social media platform of your choice. Omniki uses generative AI to enable you to launch
7184640	7190080	hundreds of thousands of ad iterations that actually work, customized across all platforms
7190080	7194960	with a click of a button. I believe in Omniki so much that I invested in it and I recommend you
7194960	7202320	use it too. Use CogGrav to get a 10% discount.
